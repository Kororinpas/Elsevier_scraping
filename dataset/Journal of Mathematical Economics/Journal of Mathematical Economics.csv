name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Benita Francisco,Nasini Stefano,Nessah Rabia","Engineering Systems and Design, Singapore University of Technology and Design, 8 Somapah Road, 487372, Singapore,IESEG School of Management, Univ. Lille, CNRS, UMR 9221 -LEM - Lille Economie Management, F-59000 Lille, France","Received 14 June 2022, Revised 20 October 2022, Accepted 31 October 2022, Available online 23 November 2022, Version of Record 24 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102789,Cited by (0)," not only allows to numerically characterize a budget partitioning that balances fairness and efficiency in decentralized investment, but also gives rise to a computational approach that facilitates its numerical solvability.","In the modern theory of portfolio optimization, there has been an increasing interest in the decentralized investment problem, where an individual investor delegates the construction of a portion of global investment to financial intermediaries, who take decisions rationally and independently. These intermediaries are frequently affiliates of large organizations and implement investment strategies in disjoint sets of assets, under regulations from a common headquarter. They can also act as expert advisors for asset allocation and compete for budget resources (Greenbaum et al., 2015), in line with the fund investment selection in well-known asset management firms (such as ====, ====, or ====).====Efficiency and fairness are natural objectives that a central headquarter may have in mind when assigning its fixed budgets to financial intermediaries. The first objective refers to the maximization of the headquarter payoff, with no direct consideration for balancing gains and losses at the intermediary level. The construction of efficient portfolios (from the headquarter viewpoint) is traditionally studied in the decentralized portfolio optimization literature, motivated by the strong incentives’ alignments (in terms of returns maximization and risk minimization) between the headquarter and the intermediaries’ payoffs (Bhattacharya and Pfleiderer, 1985, Ou-Yang, 2003, Nagurney and Ke, 2006, Stracca, 2006, Maug and Naik, 2011, Benita et al., 2019, Leal et al., 2020, Nasini et al., 2022). In this regard, existing contributions primarily focus on uncovering incentive mechanisms driving the actions of decentralized intermediaries. For instance, an economic oriented strain of research has focused on compensation contracts between the individual investor and intermediaries, as well as their implications for asset pricing (Bhattacharya and Pfleiderer, 1985, Ou-Yang, 2003, Nagurney and Ke, 2006, Stracca, 2006, Maug and Naik, 2011).==== Other studies have addressed the optimal design of delegation rules in the principal-intermediary-agent hierarchy (Liang, 2013), as well as on the role of the topology of principals–agents linkages among market participants (Fainmesser, 2019).====This being said, despite this growing interest in decentralized investment problems, when multiple intermediaries are involved, incentives’ alignments are no longer sufficient to guarantee that all participants’ payoffs are homogeneously balanced by the headquarter decisions. In this context, specialized bargaining models might provide valuable tools to address this quest for fair budget allocations.====Yet although different definitions of fairness have been adopted in the bargaining literature, it is much less obvious how to express fairness mathematically. Varian (1976) regards an allocation to be fair iff it is Pareto efficient and no agent prefers the allocation of any other agent to its own. Fairness in the distribution of a surplus has also been tacked by axiomatic approaches, establishing a collection of desirable properties that a distribution must satisfy (Nash Jr., 1950, Van Damme, 1986, Conley and Wilkie, 1996, Kalai and Smorodinsky, 1975, Roth, 1979, Akin et al., 2011).====Despite the increasing relevance of equitable budget allocations in large asset management firms (where multiple intermediaries come into place), these alternative notions of fairness have not taken part to the portfolio optimization literature. A recent opening towards this direction is the work of Rocheteau et al. (2021), who analyze the implication of bargaining concepts into models of decentralized investment with gradual negotiations.====Our article proposes a cooperative bargaining model==== and focuses on the Nash bargaining solution (hereafter referred to as NB solution) and the Kalai–Smorodinsky bargaining solution (hereafter referred to as KSB solution), to provide an efficient and fair distribution of a fixed investment budget into ==== portions. Each portion is assigned to one decentralized financial intermediary aiming at minimizing risk or disutility measures.====When it comes to level fairness and efficiency, the relevance of the NB solution relates to its fundamental property of being the unique scale-invariant solution for which intermediaries’ payoffs lie between the minimum and maximum assigned by the egalitarian and utilitarian solutions. Specifically, the four Nash bargaining axioms, which are allegedly the result of multilateral negotiations to build an equilibrium portfolio,==== have specific economic implications.====(i) The ==== axiom implies that risk or disutility measures behave as preference representations over budget allocations; (ii) the ==== axiom entails the lack of bias or propensity for specific intermediaries; (iii) the ==== axiom implies that the assigned budget must be used, when it gives rise to an increase in the utility at the intermediary level; (iv) the ==== axiom has two major implications in the portfolio optimization context. Firstly, it ensures that including financial constraints and regulations to the budget allocation does not impact the equilibrium portfolio, as long as the latter remains feasible under these constraints. Secondly, it suggests that intermediaries’ preferences over pre-existing investment options remain unchanged when new financial alternatives are made available.====Focusing on an alternative consideration of fairness, the KSB solution replaces the ==== axiom with a ==== axiom. This states that increasing the bargaining set size in a direction favorable to a specific intermediary must always benefit that intermediary. Precisely, in view of intermediaries as imperfect substitutes having access to idiosyncratic investment opportunities, the KSB budget partitioning amounts to include the opportunity cost of unilaterally beneficial investment options into the equilibrium allocation. Hence, this can be used to address applications where additional resources must be assigned to local markets handling more important sets of investment options.====With a view to translating the search for a decentralized portfolio satisfying the NB axioms or the KSB axioms into treatable optimization problems, we characterize these cooperative bargaining solutions as bilevel optimization models. Next, we introduce the class of ==== risk or disutility measures, that generalizes the portfolio moments, the conditional value at risk of portfolio losses (hereafter referred to as CVaR), as well as other well-known disutility measures. For its direct bearing on the closed-form equilibrium characterization, the notion of ==== constitutes the theoretical cornerstone of the proposed methodology.====While decentralized investment problems have been shown to be both theoretically (Stoughton, 1993, Stracca, 2006, Liang, 2013) and computationally challenging (Benita et al., 2019, Leal et al., 2020, Nasini et al., 2022), we prove that for the class of quasi-homogeneous risk or disutility functions, a form of duality between the aforementioned bilevel optimization model and convex knapsack problems can be established. Specifically, we show that the quasi-homogeneity condition allows for the computational characterization of the NB solution by a convex separable knapsack problem. Analogously, it allows for the computational characterization of the KSB solution by a convex non-separable knapsack problem. This provides a contribution to the algorithmic solvability of decentralized investment problems, in line with the works of Thi et al. (2012), Benita et al. (2019), Leal et al. (2020), and Nasini et al. (2022). Further, building on the quasi-homogeneity, we establish a closed-form comparison between the NB solution and the KSB solution, highlighting a necessary and sufficient condition for their equivalence in the proposed decentralized investment problem.====On the empirical side, building on the ability to explicitly characterize the cooperative bargaining solutions in the decentralized portfolio problem, we assess the correctness and computational solvability of the proposed methodology on large-scale portfolio instances of 7,256 U.S. listed enterprises within the period 1999–2014 available from the Center for Research in Security Prices (CRSP). We consider a collection of ==== industries, classified by the grouped-industry code from the Standard Industrial Classification (hereafter referred to as SIC), where intermediaries operate. Therefore, the resulting bargaining consists in partitioning a fixed budget among the ==== industries, while disutility levels are set as a best-response correspondence of intermediaries to budget partitions, resulting in a nested (bilevel) decision setting. By solving the aforementioned knapsack reformulation, we observe that the notion of quasi-homogeneity not only allows us to numerically characterize a budget partitioning that balances fairness and efficiency in decentralized investment, but also gives rise to a computational approach that solves the vast majority of large-scale investment problems in less than a minute.====The rest of this paper is organized as follows. Section 2 introduces a bargaining game among ==== financial intermediaries and a characterization of the class quasi-homogeneous measures. Section 3 establishes the equivalence between NB equilibrium partitioning and a convex separable knapsack problem. Similarly, Section 4 establishes the equivalence between KSB equilibrium partitioning and a convex non-separable knapsack problem. Section 5 contains a closed-form analysis of four illustrative cases, that allows deducing stylized figures about the obtained equilibrium partitioning. Section 6 proposes an extension to the problem of multi-objective intermediaries. Section 7 presents numerical tests to provide an empirical grounding of our results, using real financial data. Section 8 concludes this paper.",A cooperative bargaining framework for decentralized portfolio optimization,https://www.sciencedirect.com/science/article/pii/S030440682200115X,23 November 2022,2022,Research Article,0.0
"Albeverio Sergio,Mastrogiacomo Elisa","Institut für Angewandte Mathematik Abteilung Wahrscheinlichkeitstheorie, Endenicher Allee 60 53115 Bonn, Germany,HCM Rheinische Friedrich-Wilhelms-Universität Bonn, Endenicher Allee 60 53115 Bonn, Germany,Dipartimento di Economia, Insubria University, Via Monte Generoso, 21100 Milano, Italy","Received 7 September 2021, Revised 15 October 2022, Accepted 25 October 2022, Available online 9 November 2022, Version of Record 19 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102784,Cited by (0),In this paper we study a spatially structured economic growth model on a finite network in the presence of a Wiener noise acting on the system. We consider an extension of the Solow’s model under the assumption of Lipschitz type for the production function and uniform ,"The Solow’s model (Solow, 1956) represents one of the milestones of neoclassical endogenous growth literature; despite its relative simplicity, it provides a dynamic model that is still used in today’s macroeconomic theory. Solow’s purpose was to develop a model to describe the dynamics of the growth process and the long-term evolution of the economy, ignoring short-term fluctuations. Sometimes Solow’s model is called Solow–Swan models since Swan (1956) developed it independently. In the last decades, a new approach has significantly changed the economic view on economic growth, called the Geographic Economics. In the models developed in this approach, the accumulation of growth driving factors is not considered as a purely time-depending process anymore, but also spatial agglomeration effects are taken into account. We identify this research area as one of “spatial growth”; see, e.g., the papers (Boucekkine et al., 2009, Briani et al., 2008, Brito, 2004, Boucekkine et al., 2013, Boucekkine et al., 2019, Brock and Xepapadeas, 2009, Xepapadeas and Yannacopoulos, 2016, Camacho and Zou, 2004, Camacho et al., 2008), and Mossay (2003).====In this framework, the spatial capital dynamics is driven by differences in access to productivity factors and economic variables and a more general mathematical approach to spatial growth is represented by operator theory. More precisely, capital spatio-temporal dynamics in one (closed) economy market with only one final good, which can be signed to consume or invest, are argued to be driven by a partial differential equation (PDE) of the form: ====Here ==== is the capital storing at location ==== at time ====, ==== stands for the total factor productivities at ====, and ==== is the production function. Moreover, ==== is the depreciation rate of the capital (assumed, for simplicity, to be constant, i.e., homogeneous in time, space and capital), while ==== is saving rate of household ==== at time ====.====Some generalization of Solow’s model are remarkable. The neoclassical theory of capital accumulation and growth has been extended to a stochastic framework to take into account noises affecting many parameters of the model such as growth rates, competition coefficient, and so on. Several papers in the economics and mathematics literature have considered economic growth problems in time-space where the state equation is an infinite dimensional stochastic differential equation (SDE). For instance, the one-sector neoclassical growth model under uncertainty of Solow-type has been considered by Merton (1975) and Bourguignon (1974) in two by now classical papers. Among other things, they study in the stochastic setting the existence and the functional form of a steady state which is then identified with the stationary distribution of the diffusion generated by the stochastic differential equation satisfied by the capital-labor ratio. Later Mirman, 1972, Mirman, 1973 studied these models from a Markovian point of view, while Schenk-Hoppé and Schmalfuss (2001) extended the analysis to sample-path stability by applying random dynamical systems theory. See also, among others, Acemoglu (2004, Chapter 17) for discrete time and the books (Malliaris and Brock, 1982 Chapter 3) and (Morimoto, 2010 Chapter 9) and the papers (Bismut, 1975, Obstfeld, 1994, Turnovsky, 1995) for continuous time. We also mention very recent papers concerned with infinite dimensional control problems for economic growth in time-space; see, e.g., the papers (Boucekkine et al., 2021, Calvia et al., 2021, Gozzi and Leocata, 2022).====A related research direction investigates asymptotic properties of the solutions. Several papers deal with the limiting behavior of stochastic economic growth models, both from a theoretical and a numerical point of view. In the mathematical literature, key papers include Fleming (1971) and Fleming and Souganidis (1986) who prove the uniform convergence of policy functions in continuous time models when the noise goes to zero. In economics literature, Prandini (1994) used the large deviation results of Azencott (1980) to analyze a stochastic Solow growth model in a continuous time framework, while Williams (2004) derives a functional central limit theorem, a large deviation principle, and a moderate deviation principle in a discrete-time framework. Judd, 1998, Judd and Guu, 1993, Judd and Guu, 1997 present numerical methods based on Taylor expansions to analyze stochastic and deterministic growth models, respectively.====The objective of this paper is two-fold. First, we develop a stochastic Solow’s model for a spatial network, which is intended to be a simplified model for a large geographical area including village centers along highways and rural countryside. In this approach it is suggested that in a geographic area there are interactions between the different towns in the regions with particular economic significance (marked by nodes of a planar graph). Cities are connected by streets and other linear transport routes (considered as edges) incorporating areas which have a very modest population, or a suburban or even mostly rural nature, and thus are characterized by different social and economic impacts on the lives of its inhabitants. Hence, we propose to identify every node with a city while edges are relatively isolated locations and neighborhoods; we allow a rather general nonlinear production function in each edge equation, which includes, in particular, approximations of the Cobb–Douglas production function. The network approach pervades economic studies today (see, e.g. Ding et al., 2019 and references therein) because it can describe and identify the linkages, as well as shock transmission mechanisms among the economic agents (households, banks, firms, etc.), which influence the individual behaviors or strategic decisions. From a mathematical perspective, this means that we study a system of nonlinear diffusion equations on a graph with ==== edges and ==== nodes (thus, a geometrical object) in the presence of a Wiener noise acting on the system. Among the papers dealing with diffusions on networks, let us mention a series of recent papers by Cardanobile and Mugnolo, 2007, Mugnolo and Romanelli, 2007, where the well-posedness of isolated systems is studied. A second objective of our study is the analysis of the small noise asymptotics of the system and taking limits as the “stochastic shocks” become small. This is motivated by recent research characterizing the stochastic properties of the model economy both in terms of its average behavior and its occasional large cyclical fluctuations.====The main results in the present work can be summarized as follows.====First, we study in the stochastic setting the existence and uniqueness of a mild solution for a diffusion process of type (1.1) on networks. We assume that the production function ==== is Lipschitz while the diffusion coefficient is multiplicative and bounded. We formulate the problem in an abstract setting, so that it can be handled by applying results concerning the well-posedness of the infinite-dimensional dynamical system. As a general reference we consider the monograph (Da Prato and Zabczyk, 1992 Chapter 7). The hypotheses on the coefficients are somehow restricting. Anyway, in forthcoming work, we shall treat more realistic situations and weaker regularity assumptions. For instance, we intend to study diffusions processed on graphs characterized by more general drifts, e.g., only Hölder continuous or having polynomial growth and not necessarily bounded, so to include a larger class of production functions. In addition, we plan to discuss models with weaker regularity on the diffusion coefficient, e.g., by dropping the assumption of boundedness and just assuming global Lipschitz continuity.====Then, we provide analytic asymptotic results which characterize the average behavior of the stochastic growth model and its occasional large fluctuations. Precisely, we can estimate the probability that the economic activity along the network (considered in the model) deviates from the deterministic dynamics by a given amount. We will show that this probability decays exponentially with the intensity of the noise. We refer to Williams (2004) and references therein for empirical evidences about the US market and economic applications. To our knowledge, this reference appears to be the first attempt to apply this type of convergence results for the stochastic growth model on networks. The results provide analytic, theoretically justified approximations for stochastic models with small noise. We focus on a relatively simple and standard model, but the methods we develop can be applied to more general nonlinear models which may provide, if implemented numerically, a closer match to certain features of suitable time-series (e.g. the technology growth rate and standard deviation coming from different geographical areas, see, e.g. the construction proposed in Stock and Watson, 1999).====The paper is organized as follows: the next section is devoted to preliminaries, it gives necessary notations and assumptions, and result from the theory of infinite dimensional stochastic differential equations. In this section we also give a synthesis of the main results of the paper. In Section 2.3 we analyze the stochastic Solow’s growth model on spatial networks, and we give technical tools and proofs necessary to solve the system of stochastic differential equations under investigation. In Section 3, in particular, we derive the large deviation principle for our model.",Large deviation principle for spatial economic growth model on networks,https://www.sciencedirect.com/science/article/pii/S0304406822001100,9 November 2022,2022,Research Article,1.0
"Ha-Huy Thai,Nguyen Thi Tuyet Mai","Université Paris-Saclay, Univ Evry, EPEE, 91025, Evry-Courcouronnes, France,TIMAS, Thang Long University, Viet Nam,Thuongmai University, Hanoi, Viet Nam","Received 12 August 2019, Revised 14 January 2022, Accepted 24 October 2022, Available online 30 October 2022, Version of Record 15 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102783,Cited by (0),"This article studies an inter-temporal optimization problem under a criterion that is a combination of Ramsey’s and Rawls’s criteria. A detailed description of the saving behavior through time is provided. The optimization problem under the ====-==== criterion is also considered, and the optimal solution is characterized.",None,Saving and dissaving under ,https://www.sciencedirect.com/science/article/pii/S0304406822001094,30 October 2022,2022,Research Article,2.0
Tsasa Jean-Paul K.,"Department of Economics, Université du Québec, C.P. 8888 Succursale Centre-ville, H2X 3X2, Montreal, Quebec, Canada","Received 8 November 2021, Revised 29 September 2022, Accepted 17 October 2022, Available online 25 October 2022, Version of Record 5 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102776,Cited by (0),I show that including a nonlinear production function with physical capital in the Diamond–Mortensen–Pissarides model lowers the ==== of labor market tightness with respect to labor productivity by about 11.5% under the Hagedorn–Manovskii’s calibration. My finding is robust against different ====.,"Building on the recent work of Ljungqvist and Sargent, 2017, Ljungqvist and Sargent, 2021 and Atolia et al. (2018) (hereafter AGM), this paper contributes to the macroeconomics-labor literature by providing tractable analytical proof that including a nonlinear production function with physical capital input in a Diamond–Mortensen–Pissarides search-and-matching model (DMP model henceforth) reduces the volatility in labor market variables.====The DMP model has been criticized for failing to generate enough volatility in labor market variables in response to measured productivity shocks that are quantitatively consistent with the data (see, e.g., Shimer, 2005, Costain and Reiter, 2008). This critique is known as the Shimer puzzle. A number of papers in the macroeconomics-labor literature have proposed alternative solutions to the Shimer puzzle, including the Hagedorn and Manovskii (2008) (HM) calibration strategy.==== While Shimer (2005) proposes a calibration strategy that implies the DMP model generates only a small volatility in labor market variables, HM suggest an alternative calibration strategy that allows the DMP model to generate volatility in labor market variables that is consistent with evidence from data. The main difference between Shimer and HM lies in how the two central parameters of the DMP model (i.e., the workers’ value of nonmarket returns and the workers’ relative bargaining power) are identified. Shimer (2005) sets the value of nonmarket returns to match the average replacement rate of unemployment insurance and fixes the workers’ relative bargaining power to ensure the Hosios (1990) condition for efficiency. In contrast, HM use empirical evidence from the costs of posting vacancies and the elasticity of wages with respect to productivity to identify the two parameters of interest. This alternative calibration helps the DMP model to have a strong amplification mechanism for labor market variables.====However, by using computational methods based on calibration and simulations,==== AGM show that while the HM calibration can resolve the Shimer puzzle in the extended RBC search model when preferences are linear, as they are in the textbook DMP model (see Pissarides ch. 1 2000), this is not the case once the framework exhibits nonlinearities in the utility and production functions. Thus, AGM argue that the success of the HM calibration is driven primarily by the assumption of linear preferences embedded in the DMP model.====AGM do not provide a mathematical proof to support their results analytically. This paper attempts to complement their study by providing tractable analytical proof that the presence of nonlinearities in the production function reduces labor market volatility using HM calibration. To achieve this goal, I use Ljungqvist and Sargent’s (2017) protocol. I carefully derive the analytical expression of fundamental surplus for a fully specified RBC search model,==== which is a search-and-matching framework that includes a nonlinear production function such as a standard Cobb–Douglas specification, and a variety of commonly used utility specifications. After specifying the model, I derive a set of analytical expressions for the elasticity of unemployment with respect to labor productivity in the steady-state. I calibrate these expressions to US data and compare the implied elasticities to a basic DMP model without capital stock. I find that including nonlinear production reduces the implied elasticity by 11.5% using HM calibration. I show that my baseline results are robust against alternative specifications of RBC search models, including setups with an endogenous labor supply (Hansen, 1985), with a fiscal policy (Christiano and Eichenbaum, 1992), and with money in the utility function (Cooley and Hansen, 1991). My results are also robust against alternative values of key model parameters that drive labor market volatility, including the workers’ relative bargaining power, nonmarket activity (replacement ratio), and matching-function elasticity.====AGM find that the HM calibration fails to resolve the Shimer puzzle under a standard preference specification for an RBC search model with risk-averse agents.==== My baseline results show that nonlinearities in the utility function do not affect the elasticity of labor market tightness with respect to labor productivity in a deterministic steady state. It is important to note that my analytical experiments on the sensitivity of elasticity are conducted in a deterministic steady state, as in Ljungqvist and Sargent, 2017, Ljungqvist and Sargent, 2021, whereas AGM’s computational experiments are implemented by simulating the search model outside of a steady state. On the other hand, in my framework, the curvature of the utility function does not matter per se in a deterministic steady state. What matters is the nonlinearity of the production function and the presence of physical capital.====Finally, my results show that nonlinearities in the production function affect labor market volatility through a channel that is distinct from the fundamental surplus. While the recent macroeconomics-labor literature basically focuses on the fundamental surplus to solve the Shimer puzzle (for further discussions, see Ljungqvist and Sargent, 2017, Ljungqvist and Sargent, 2021, Kokonas, 2022, among others), my finding analytically highlights the existence of an alternative channel through which nonlinearities implied by a production function with physical capital affect labor market volatility.====The paper is related to a large literature that stresses the importance of considering search-and-matching frictions for understanding labor market dynamics (see, e.g., Diamond, 1982a, Diamond, 1982b, Mortensen, 1982a, Mortensen, 1982b; Pissarides, 1979, Pissarides, 1985; Mortensen and Pissarides 1994, and the subsequent literature).==== Analytically, the paper stays close to the quantitative tradition of the early literature that extends the standard RBC model by introducing labor market search-and-matching frictions (Merz, 1995, Andolfatto, 1996, and subsequent vast macro-labor literature). Specifically, I follow the RBC literature (Kydland and Prescott, 1982, Kydland and Prescott, 1995, Long and Plosser, 1983) to model both household decisions and the economy’s production sector. I explicitly build in a frictional labor market with firms posting vacancies and the unemployed searching for jobs. I use the resulting framework to conduct analytical experiments. Finally, as mentioned, my calibration strategy is built on the strand of literature concerning the search-and-matching model that aims to solve the labor market puzzle presented by Shimer (2005) and Costain and Reiter (2008), among others, mainly using the HM calibration strategy, as in AGM.====The remainder of the paper is organized as follows. Section 2 briefly discusses the model economy. Section 3 presents analytical results and discusses the implications of my finding for future research. Section 4 proceeds with a sensitivity analysis, and Section 5 concludes.",Labor market volatility in a fully specified RBC search model: An analytical investigation,https://www.sciencedirect.com/science/article/pii/S0304406822001021,25 October 2022,2022,Research Article,3.0
"Liu Taoxiong,Liu Zhuohao","Institute of Economics, School of Social Sciences, Tsinghua University, Beijing 100084, China","Received 24 April 2022, Revised 3 September 2022, Accepted 7 October 2022, Available online 23 October 2022, Version of Record 12 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102774,Cited by (0),"We propose a new approach to deal with continuous time dynamic model with discontinuity, by which we incorporate gradual innovations and technological leaps, as well as endogenous cycles and long-run growth into one framework. We show that the optimal or equilibrium growth path is a cyclic growth path (CGP) and we develop the mathematical technique to prove the existence and uniqueness of the solution. We provide two basic setups to include the technological leaps in the R&D sector or goods production sector respectively. The growth paths under these two setups show different cyclic features and abundant economic dynamics. Our approach can be easily utilized to evaluate the ==== of the timing of the technological revolution. In the decentralized economy, the technological breakthrough is exogenous to families and always happens too late. The social planner’s problem clarifies two fundamental effects on the optimal growth path, i.e., the “dividend effect” and the “swot-up effect”, induced by discontinuous technological change. Our framework can also serve as a workhorse to address dynamic economic problems with discontinuity.","In macroeconomics, technological progress has been regarded as a fundamental driver of long-term economic growth (Solow, 1956). We agree with Schumpeter (1927) that the occurrence and diffusion of innovations should be viewed as a very uneven process that is sometimes explosive and sometimes gradual. Schumpeter (1974) further argued that the economic growth and business cycle of capitalists should be explained by ====Growth theory studies that seriously examine technological revolution are in general lacking in the literature. As an important attempt, Helpman and Trajtenberg (1994) built a model to explore the effects of new GPTs and GPTs diffusion, while treating GPTs advances as exogenous. However, the critical point lies exactly in how and when the technological revolutions occur and what determines them, which can only be explained by endogenizing technological revolutions in the model where the occurrence and timing of technological revolutions are determined by the decisions of agents.==== We conjecture that one reason for the absence of a relevant theoretical model is the lack of a satisfactory tool to deal with that, mathematically. The existing dynamic framework in macroeconomics, especially for growth models, is suitable for incremental technological progress, where usually a balanced growth path (BGP) is obtained as the solution path. However, if we want to introduce saltatorial technological breakthroughs into the continuous time framework, we need to allow path jumps, which is not permitted in the existing main dynamic framework in economics.====The purpose of this article is to address these gaps and provide a concise framework to tolerate discontinuous growth path so as to endogenize both the gradual progress and radical revolution of technology in one growth model. Although there are some related techniques in cybernetics, they have not been well included in economic models. Hull (2013) introduced the optimal control theory with deterministic discontinuities in detail and gave the corresponding Erdmann–Weierstrass condition. Moreover, for the need for a growth model, it is necessary to allow the technology to break through successively and thereafter the growth path to jump infinitely many times. This is the most difficult part of our task, which has no ready-made answers in the cybernetics literature, and is much more complicated than finitely multiple jumps, as in Liu and Zhou (2015) and Boucekkine et al., 2016, Boucekkine et al., 2013. Under this circumstance, the solution path inevitably is not a BGP. A natural question thus arises: Is there an alternative solution path that is reasonable and if so when does it exist? Fortunately, in our framework, we find that another type of equilibrium growth path (or the optimal growth path in the social planner problem), which fluctuates cyclically, does exist, and we successfully clarify the conditions for its existence and uniqueness.====Intuitively, since the technological revolution shows an inherent property of unevenness or discontinuity, it naturally induces the growth path to fluctuate. Schumpeter (1927) believed that long cycles such as Kondratiev cycles are generated by technological revolutions (Ayres, 1990). Nevertheless, there have been some attempts to connect endogenous growth with cycles by introducing innovation cycles. For most of the studies, such as in Canton (2002), Martimort and Verdier (2004), Pavlov and Weder (2017), and Wälde (2005), also including the one-sector model in the seminal paper Aghion and Howitt (1992), the cycles are aroused by stochastic bursts of innovations, and there is no ex-ante path discontinuity in the meaning of expectations. Thus, since the timing of cycles and the cyclic length are stochastic, there are no stationary waves and little room to explore the cyclic behavior or the transitional dynamics before the next burst of innovation. In addition, the technological revolution is the result of the long-term accumulation of knowledge and technology, especially research in the basic scientific field, so it is predictable to a certain extent. For example, with the evolution of mobile communication technologies, before the application of 4G or 5G technology, even before their inventions in laboratories, people had discussed the technologies for a long time. Moreover, it is also practically important to model innovation cycles or discontinuous innovation under deterministic circumstances because under deterministic circumstances we can obtain the stationary timing of the cycles and explore the transitional dynamics transparently.====Therefore, our approach incorporates technological leaps, endogenous technological cycles, and long-run growth into one framework. We classify technological progress into two basic scenarios: incremental progress and technological breakthroughs or technological revolutions. We use the discontinuous optimal control technique in continuous time to establish and solve the model. Our baseline model is founded with the classical approach of expanding varieties. Different from the classical growth model, our approach unifies long-term growth and economic fluctuation into one framework, which is driven by leapfrog technological revolution, together with continuous technological progress. We introduce the concept of the cyclic growth path (CGP) for the solution. On the CGP, the economy shows stable growth in the meaning of a time average in long term. Furthermore, the growth rates of major economic variables fluctuate periodically. The model shows abundant interesting economic dynamics and indicates the underlying mechanism. For example, in the equilibrium solution of the baseline model, the output is continuous, the consumption ratio jumps downward and the investment ratio jumps upward while crossing the border of technological dynasties. In addition, within a cycle, the growth rate, the saving rate, and the consumption rate change monotonically while the consumption-assets ratio does not. An even more fundamental result is that the social planner prefers a different path and different technological revolution timing compared to the market equilibrium because the households in the market treat the timing of technological revolution as given while the social planner endogenizes it. The solution of the social planner’s problem clarifies two fundamental effects induced by the technological revolution, the “dividend effect” after the revolution and the “swot-up effect” before that. Thus, correct policies can be designed to improve social welfare by balancing these two effects better. We provide two basic setups (one of which is placed in the appendix), which include technological revolutions in the R&D sector for the first setup and technological revolutions in the goods production sector for the second. The growth paths in these two setups show different cyclic features, such as that the output is continuous and the consumption is discontinuous in the first setup while it is the opposite in the second.====This paper contributes to the existing literature from three perspectives. First of all, we propose a general approach to deal with continuous time dynamic model with discontinuity, by which we incorporate gradual innovations and technological breakthroughs, as well as endogenous cycles and long-run growth into one framework. To the best of our knowledge, this is the first attempt to endogenize these two basic forms of technological progress in one model and show how their interaction induces luxuriant types of economic dynamics. By the advantage of our approach, it is very convenient to compare the social planner problem and the market equilibrium, especially to evaluate the optimality of the timing of technological revolution under these two circumstances, which is obviously of essential importance given the profoundness and extensiveness of the technological revolution in reality. We also show that the drastic innovations occur too late in the market equilibrium compared to the social planner problem because the social planner favors a different path of consumption and investment around a drastic innovation. We illustrate how the proper policy can restore social optimality and how it needs to carefully address the economic behavior around the time point of technological breakthrough.====Second, we suggest a tool to incorporate both endogenous cycles and long-run growth into one framework. Instead of the concept of the balanced growth path (BGP) for the modern growth model, we propose the concept of the CGP as the solution path in which both long-term growth and business cycles are rooted. The most technical part of our work is to prove the existence and uniqueness of the solution path, as a CGP. We provide related mathematical techniques to address this type of model and successfully conquer the main difficulties by the power of the Poincare map. Different from Freeman et al. (1999), where to obtain a cyclical path the economy must start with a given level of capital stock, our model has a cyclical solution with any reasonable starting initial condition.====Third, our framework can also be used as a basic tool to address dynamic economic problems with discontinuity. Our framework tolerates various forms of technological fluctuations and can fit different settings of production functions or economic environments. The framework is flexible enough to address some other dynamic macroeconomic problems with abrupt changes, including discontinuous policy design, such as the birth into being of new industrial sectors, the saltatory change after some threshold such as new infrastructure opening up based on long-term investment, etc. From the technical perspective, our approach is most closely related to the literature treating optimization problems with regime transitions. Boucekkine et al. (2013) present the general FOCs regarding the regime changes under economic circumstances. Liu and Zhou (2015), Boucekkine et al. (2016), and Haunschmied et al. (2021) propose some applications of discontinuous control techniques on economic models. Our approach extends the techniques treating regime transitions in these papers to incorporate infinite times of regime transitions and accommodate decentralized equilibrium. As far as we know, there are several papers involving discontinuous growth paths, such as Freeman et al. (1999) and Francois and Lloyd-Ellis (2003), etc. We believe that the present approach has the potential to be applied to these models, especially to explore the behavior around the discontinuous point on the path. Our approach could also benefit the literature on political economy, since political regime changes are such an important topic there (Acemoglu and Robinson, 2001, Boucekkine et al., 2016, Haunschmied et al., 2021).====The remainder of this article is arranged as follows. Section 2 preliminarily explores the representative household problem under a discontinuous environment. Section 3 present the baseline model. Section 4 establishes the concept of the CGP, obtains the equilibrium solution path, the CGP, and probes the economic dynamics on the CGP. Section 5 contributes by solving the social planner problem and focuses on the model’s policy implications. Then, we conclude the main findings in the final section. Most of the mathematical proofs and a summary of the method for the discontinuous optimal control problem are offered in Appendix A and Appendix C. Appendix B provides an extension of the model. Appendix C shows the properties of the CGP by some numerical examples.",A growth model with endogenous technological revolutions and cycles,https://www.sciencedirect.com/science/article/pii/S0304406822001008,23 October 2022,2022,Research Article,4.0
"Li Xinmi,Zheng Jie","Fundamental Science of Mathematics and Physics, Department of Physics, Tsinghua University, 100084, Beijing, China,School of Economics and Management, Tsinghua University, 100084, Beijing, China","Received 10 March 2022, Revised 12 August 2022, Accepted 24 September 2022, Available online 7 October 2022, Version of Record 22 October 2022.",https://doi.org/10.1016/j.jmateco.2022.102771,Cited by (0),"We consider a large class of 2-contestant Colonel Blotto games, for which the budget and valuation are both asymmetric between players and the contest success functions are in Tullock form with battle-specific discriminatory power in ","Competition can be multi-dimensional in many economic, political and military activities, and one of the foundational game theoretical models to study such a type of competition is the Colonel Blotto game, which was originally formulated by Borel (1921). In a typical Colonel Blotto game, two competitors, each under a budget constraint, strategically allocate resources to multiple battles to optimize their own objective. Their own objective depends on the outcomes of battles, each of which is determined by their resource allocations in the corresponding battle. Such a model has been applied to many competing scenarios, including the marketing contest of a product (Friedman, 1958), the electoral campaign (Laslier and Picard, 2002), and the attacker–defender conflict (Powell, 2009), among many others.====In this paper, we consider a class of two-contestant Blotto games with Tullock contest success functions under a very general setting, where (1) players are asymmetric in terms of resource budget, battle valuation, and lobbying effectiveness; (2) battles are heterogeneous in terms of discriminatory power, battle valuation, and lobbying effectiveness. We focus on the pure strategy Nash Equilibrium (hereafter referred to simply as equilibrium) of the game, and thus restrict our attention to the cases where the discriminatory powers can only take the values greater than ==== and no greater than ====. We show that the equilibrium of the game always exists and we completely characterize all the equilibria of the game. Although a closed-form solution is generally unavailable, we are able to examine some important properties of the equilibrium, including rent dissipation, range of values for strategies and utilities, payoff comparison across different equilibria, and the effect of resource budget in case of a unique equilibrium. Furthermore, we provide two sets of sufficient conditions, under each of which there is a unique equilibrium of the game. The set of sufficient conditions assuming symmetric valuations incorporates many existing results as special cases. More importantly, to the best of our knowledge, the set of sufficient conditions with asymmetric valuation provides a novel uniqueness result to the literature. Our study thus contributes to a better understanding regarding both the existence and the uniqueness, as well as characterization and properties, of the equilibria in Blotto games.====Many studies on Colonel Blotto games have focused on deterministic contest success functions (hereafter CSFs) (Hillman and Riley, 1989, Baye et al., 1996), also known as the all-pay auction CSFs, in which the player who allocates the most resource into a given battle becomes the winner of that battle for sure (Borel, 1921, Gross and Wagner, 1950, Roberson, 2006, Macdonell and Mastronardi, 2015, Kovenock and Roberson, 2021, Boix-Adserà et al., 2021). Our research belongs to the Blotto game literature that studies stochastic CSFs, in which the player who allocates more resource has a higher chance to win the battle compared to the player who allocates less resource.==== The CSFs of these imperfectly discriminating contests (Skaperdas, 1996) can take a ratio form (Cornes and Hartley, 2005), and typically follow the generalized Tullock setup (Tullock, 1980, Nti, 1999, Stein, 2002). Within this string of stochastic Blotto game literature, some studies consider the scenarios where players’ allocations of resource are sequential across the battles (Klumpp et al., 2019, Anbarcı et al., 2020, Li and Zheng, 2021) or the scenarios where players’ objectives are more complex than the maximization of winning valuation (Duffy and Matros, 2015, Klumpp et al., 2019, Anbarcı et al., 2020, Li and Zheng, 2021, Morales and Thraves, 2021). Many studies including ours follow the original setup of the Blotto game by assuming that every player simultaneously allocates the resource among all the battles to maximize the expected sum of valuations from the battles where the player is the winner (Friedman, 1958, Robson, 2005, Osorio, 2013, Duffy and Matros, 2015, Xu and Zhou, 2018, Kim et al., 2018, Kovenock and Rojo Arjona, 2019). Recently, there are a few studies combining contest theory with network structure theoretically or experimentally (Xu et al., 2021, Li and Zheng, 2022, Cortes Corrales and Rojo Arjona, 2022).====Our work is closely related to the following several studies, and Table 1 summarizes the main features of model setup in these works. Friedman (1958) considers a two-contestant lottery Blotto game==== with heterogeneous battle valuation, where players have asymmetric resource budget but symmetric battle valuation and lobbying effectiveness. It is shown that the equilibrium of the game is unique and exhibits the proportionality feature: for every player, the resource allocated to any given battle is proportional to the valuation of that battle.==== Duffy and Matros (2015) show that the uniqueness result and the proportionality feature still hold when there are more than two players in Friedman (1958)’s framework.==== Robson (2005) generalizes the setting of Friedman (1958) by extending the standard lottery CSF to Tullock CSFs where the discriminatory powers are battle-specific and in the range of ====, and provides an equilibrium characterization of the game. Xu and Zhou (2018) further prove that the equilibrium in Robson (2005) is indeed unique for discriminatory powers in the range of ==== and a Blotto game with no pure strategy Nash equilibrium can always be constructed if the discriminatory power is greater than ==== in a battle.====All of the above mentioned four studies focus on the scenario where battle-specific valuations are symmetric across players, while there are many real-world examples where competitors hold different views toward the valuation of competition.==== We investigate the existence and uniqueness of equilibrium of the Blotto game under such a more general setting, studied also by Osorio, 2013, Kim et al., 2018, Kovenock and Rojo Arjona, 2019. Specifically, Osorio (2013) considers a two-contestant Blotto game with player-specific budget and battle-specific discriminatory power, in which the battle valuation and lobbying effectiveness are both player-specific and battle-specific. Regarding the existence of equilibrium, Osorio (2013) provides a necessary condition, which is equivalent to solving for a system of ==== equations with ==== unknowns, where ==== is the number of battles. Kim et al. (2018) consider a simplified setting where each battle is a standard lottery contest allowing for more than two contestants. They prove the existence of equilibrium for such a multi-contestant lottery Blotto game. In their setting, for the case of two contestants, they provide a characterization of the equilibrium, which is equivalent to solving for an equation with one unknown. A more recent development in this literature is Kovenock and Rojo Arjona (2019), who fully characterize the best response functions of the Blotto game in a two-contestant setting of Kim et al. (2018). However, the understanding regarding the uniqueness condition of the equilibrium of the game so far is very limited. Although both Osorio (2013) and Kim et al. (2018) have provided numerical examples where there exist multiple equilibria, there is no result regarding the conditions under which the equilibrium is unique being provided in the literature. Furthermore, when there exist multiple equilibria, the properties regarding the comparison across these equilibria are also unclear. This is partly due to the fact that the equilibrium characterization method adopted in the literature does not provide much insight regarding equilibrium properties.====In this paper, we follow the framework in Osorio (2013), which is so far the most general setting for two-contestant lottery Blotto game, and adopt a different approach to prove the existence of equilibrium. This approach allows us to characterize the equilibrium, which is equivalent to solving an equation with one unknown, similar to the approach in Kim et al. (2018). Unlike Kim et al. (2018), our approach directly delivers the existence result by applying the well-known Intermediate Value Theorem, provides further insights regarding equilibrium rent dissipation and equilibrium value ranges, allows us to identify the effect of resource budget when there is a unique equilibrium, and also enables us to compare players’ payoffs across equilibria when there are multiple equilibria. Furthermore, we are able to provide two sets of sufficient conditions under which the equilibrium is guaranteed to be unique.====Our work contributes to the Blotto games literature mainly in three aspects:====Firstly, through explicit characterization, we show that a pure strategy Nash equilibrium always exists for any two-contestant Blotto game with Tullock CSFs of discriminatory powers in ====. Our existence result can be viewed as a generalization of those in Friedman, 1958, Robson, 2005, Duffy and Matros, 2015, Xu and Zhou, 2018 from symmetric valuations to asymmetric valuations, and can also be viewed as an extension of Kim et al. (2018) from standard lottery CSF to battle-specific Tullock CSFs of discriminatory powers in ==== with battle-and-contestant-specific lobbying effectiveness. The setting we have considered, to the best of our knowledge, is the most general one in the literature for two-player Blotto games to guarantee the existence of pure strategy Nash equilibrium.====Secondly, thanks to our equilibrium characterization approach, we are able to deliver some insights on equilibrium properties, which are new to the literature. Our proposition on equilibrium rent dissipation ratio is a generalization of the well-known proportionality result from symmetric valuations to asymmetric valuations. We provide the lower bound and upper bound for allocation ratio between contestants and across battles, as well as equilibrium utility of contestants, which intuitively suggests what an equilibrium may look like. Furthermore, in the presence of multiple equilibria, we show that contestants always have the opposite preferences regarding the payoffs across difference equilibria, hence a payoff dominance relation is impossible between any two distinct equilibria.====Thirdly, and perhaps most significantly, our uniqueness results shed light on the understanding toward the relationship between game structure and uniqueness of equilibrium. The first set of conditions applies to the setting of symmetric valuations, and incorporates many existing uniqueness results in the literature (Friedman, 1958, Robson, 2005, Duffy and Matros, 2015, Xu and Zhou, 2018) as special cases. Furthermore, the unique equilibrium is fully characterized with a closed-form solution. More importantly, the second set of conditions applies to the setting of asymmetric valuations, in which there may exist multiple equilibria in general. We are the first to provide a set of reasonably weak, simple and intuitive conditions, under which the equilibrium is guaranteed to be unique under asymmetric valuations. Our result suggests that: (1) the more symmetric the battle valuations are, the more likely the equilibrium is unique; (2) the more symmetric the resource budgets are, the less likely the equilibrium is unique; (3) the more discriminatory the battle is (the greater ==== is), the less likely the equilibrium is unique.====The rest of the paper is structured as follows. In Section 2, we introduce the model setting. In Section 3, we show our main results about the existence and uniqueness of equilibrium, as well as results about equilibrium properties. Section 4 concludes.",Pure strategy Nash Equilibrium in 2-contestant generalized lottery Colonel Blotto games,https://www.sciencedirect.com/science/article/pii/S0304406822000970,7 October 2022,2022,Research Article,5.0
"Hashimoto Ken-ichi,Im Ryonghun,Kunieda Takuma,Shibata Akihisa","Graduate School of Economics, Kobe University, Rokko-dai 2-1, Kobe 657-8501, Japan,School of Economics, Kwansei Gakuin University, 1-155 Uegahara Ichiban-cho, Nishinomiya 662-8501, Hyogo, Japan,Institute of Economic Research, Kyoto University, Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan","Received 15 March 2022, Revised 4 August 2022, Accepted 24 September 2022, Available online 5 October 2022, Version of Record 17 October 2022.",https://doi.org/10.1016/j.jmateco.2022.102772,Cited by (0),"This paper uses a dynamic general equilibrium model to examine whether financial innovations destabilize an economy in which a representative firm operates one production sector with a neoclassical production function. Entrepreneurs receive individual-specific productivity shocks in each period. Entrepreneurs who draw higher productivity shocks become capital producers and borrow production resources in the financial market, whereas entrepreneurs who draw lower productivity shocks become lenders. Capital producers face financial frictions and can borrow only up to a certain proportion of their funds. Under an empirically plausible ==== of substitution between capital and labor, as financial frictions are mitigated, the economy loses stability, and a flip bifurcation occurs at a certain level of financial frictions. In some cases, a cycle with more than two periods and complex dynamics arise as period-doubling bifurcations repeatedly occur. In any case, the amplitude of fluctuations increases as financial frictions faced by capital producers are mitigated and is maximized when they are eliminated. These outcomes imply that financial innovations are likely to destabilize an economy.","The development of modern monetary and financial systems began in the early ====th century. In the financial development process, unceasing financial innovations have enabled us to trade considerable amounts of assets in the financial market. Under these circumstances, people have repeatedly witnessed large swings of boom-bust fluctuations, which are occasionally accompanied by credit market booms and collapses (see Boissay et al., 2016). As such, researchers and policymakers often assert that financial innovations destabilize economies (e.g., Loayza and Ranciere, 2006, Guillaumont Jeanneney and Kpodar, 2011, Rubio and Carrasco-Gallego, 2014). The purpose of this paper is to explore whether financial innovations destabilize an economy by applying a dynamic general equilibrium model with infinitely lived agents.====In our study, we assume that financial innovations directly mitigate financial frictions and that the mitigation of financial frictions softens the borrowing constraints that entrepreneurs face. From the perspective of the 21st century, one may imagine that this kind of financial innovation is caused by financial technology combined with information technology and/or artificial intelligence. In particular, due to the development of financial technology in recent years, the amount of total debt and credit in the private sector has ballooned far beyond the scale of gross domestic product in some countries. In this paper, we investigate how the mitigation of financial frictions affects economic fluctuations.====The economy in our model is inhabited by three types of economic agents: firms, workers, and entrepreneurs. The representative firm produces general goods from capital and labor with a neoclassical production technology that exhibits positive and diminishing marginal products and is homogeneous of degree one with respect to capital and labor. Workers inelastically supply one unit of labor to the representative firm to earn wage income in each period. They cannot borrow or save in the financial market, and thus, they consume all their earnings in each period, i.e., they are hand-to-mouth consumers. Entrepreneurs receive idiosyncratic productivity shocks in each period. Entrepreneurs who draw higher productivity shocks borrow in the financial market and undertake an investment project to produce capital. Those who draw lower productivity shocks store their wealth by lending in the financial market without engaging in an investment project. Because of idiosyncratic productivity shocks, investors and lenders appear endogenously in each period. Entrepreneurs face financial frictions, and they can borrow only up to a certain proportion of their net wealth. The mitigation of financial frictions promotes capital accumulation because the allocation of production resources is improved, and thus macroeconomic productivity increases.====An increase in the capital stock has two conflicting effects on capital income. On the one hand, it places upward pressure on capital income because the source of capital income (i.e., the principal) becomes greater. On the other hand, it places downward pressure on capital income because the decrease in the marginal product of capital lowers the interest rate. Therefore, an increase in the capital stock does not necessarily raise capital income. In this paper, we assume that the elasticity of the marginal product of capital in the production technology increases as the capital stock increases. In this case, at the early stage of capital accumulation, the positive effect of the increase in the capital stock on capital income surpasses the negative effect, whereas the negative effect becomes stronger than the positive effect as capital accumulates. Then, capital income increases at the early stage of capital accumulation and starts to decline when capital accumulation attains a certain threshold level. In other words, the net effect that capital accumulation has on capital income has an inverted-U shape. In our model, this inverted-U shaped effect of capital accumulation on capital income is crucial to producing endogenous business cycles.====To study how the mitigation of financial frictions affects economic fluctuations, we produce bifurcation diagrams with respect to the extent of borrowing constraints faced by capital producers under the uniform distribution of entrepreneurs’ productivity shocks. The key factor in determining the configuration of bifurcation diagrams is the elasticity of substitution between capital and labor. The recent study by Gechert et al. (2022) performs a meta-analysis by using 3186 observations of the elasticity of substitution from 121 prior studies and reports that the mean of the elasticity of substitution should be 0.3. Then, we examine three plausible cases in which the elasticity of substitution is equal to 0.40, 0.29, and 0.25 (the mean of which is 0.31). Our findings are as follows. When the elasticity of substitution is 0.40, as financial frictions are mitigated, the economy loses stability at a certain level of financial frictions with a flip bifurcation occurring, and then a period-two cycle arises. When the elasticity of substitution is 0.29, whereas a period-four cycle occurs under severe borrowing constraints, period-doubling bifurcations repeatedly occur as financial frictions are mitigated, and eventually the economy exhibits complex dynamics. When the elasticity of substitution is 0.25, the economy exhibits complex dynamic behaviors under severe borrowing constraints, but if borrowing constraints are sufficiently softened, the economy converges to a period-three cycle. In any case, the amplitude of business fluctuations increases as financial frictions faced by capital producers are mitigated and is maximized when they are eliminated. Therefore, financial innovations are likely to destabilize the economy.====The current paper is related to the literature on (deterministic) endogenous business cycles in the dynamic general equilibrium model, which have been theoretically studied by many researchers over the past thirty years, and the plausibility of which has recently been empirically supported by Beaudry et al., 2015, Beaudry et al., 2017, Beaudry et al., 2020. Examples of works that employ the overlapping generations model are Benhabib and Day (1982), Grandmont (1985), Farmer (1986), Reichlin (1986), Benhabib and Laroque (1988), Bertocchi and Wang (1995), Grandmont et al. (1998), and Rochon and Polemarchakis (2006). As in these works, Yokoo (2000) demonstrates that endogenous business cycles can occur in an overlapping generations model with outside money. As in Yokoo’s model, the elasticity of the marginal product of capital plays a crucial role in the occurrence of endogenous business cycles in our model, but our model is inhabited by infinitely lived agents and does not assume the presence of outside money. Benhabib and Nishimura (1985), Boldrin and Deneckere (1990), and Nishimura and Yano (1995) employ an infinitely lived agent model with two production sectors in which endogenous business cycles are derived. Furthermore, Airaudo (2017) derives endogenous cycles in an otherwise standard Lucas tree model (Lucas, 1978) in which households extract their utility not only from direct consumption but also from their wealth (Max Weber’s spirit of capitalism), and Sorger (2018) studies a one-sector growth model with elastic labor supply and obtains endogenous cycles in equilibrium. Lazaryan and Lubik (2019) investigate the dynamic behavior of a standard search and matching model and derive endogenous business fluctuations. Deng et al. (2021) investigate the phenomenon of ==== periodicity of any order by applying the two-sector Robinson–Shinkai–Leontief model of economic growth.====None of the abovementioned studies, however, explicitly consider financial frictions. Examples of papers that derive endogenous business cycles with financial frictions are Azariadis and Smith, 1996, Azariadis and Smith, 1998, Matsuyama, 2007, Matsuyama, 2013, Kikuchi (2008), Kikuchi and Stachurski (2009), Gokan (2011), Favara (2012), Myerson, 2012, Myerson, 2014, Matsuyama et al. (2016), and Asano and Yokoo (2019). All these studies employ the overlapping generations model. In contrast, Woodford (1989) constructs a dynamic general equilibrium model of infinitely lived capitalists and workers and derives endogenous business cycles. In his model, workers consume in a hand-to-mouth manner, not being allowed to borrow and lend in the financial market, and capitalists are homogeneous, so neither borrowing nor lending occurs among them. Although hand-to-mouth workers are also assumed in our model, entrepreneurs can borrow up to the limit of borrowing constraints, as previously explained. In our model, the extent of financial frictions plays a crucial role in the instability of the economy. Aghion et al. (2004) and Pintus (2011) also derive endogenous business cycles in a model of infinitely lived agents with financial frictions, but they assume a small open economy where the world interest rate is exogenously given.==== Although Kunieda and Shibata (2017) investigate how financial development affects endogenous business cycles in a dynamic general equilibrium model of infinitely lived agents, their economy does not exhibit endogenous business cycles when financial constraints are fully softened. To the best of our knowledge, no studies in the existing literature obtain the result that the amplitude of endogenous business cycles increases as financial frictions faced by capital producers are eliminated.====The remainder of this paper is organized as follows. In the following section, we develop the model and obtain the optimality conditions of entrepreneurs and the representative firm. Section 3 derives an equilibrium in which a one-dimensional dynamical system with respect to capital is obtained. In Section 4, we investigate the patterns of dynamic behaviors in the economy and observe the appearance of endogenous business cycles. In Section 5, we find that the mitigation of financial frictions destabilizes the economy. In this section, we also produce bifurcation diagrams with respect to the extent of financial frictions. In Section 6, we conclude the paper.",Financial destabilization,https://www.sciencedirect.com/science/article/pii/S0304406822000982,5 October 2022,2022,Research Article,6.0
"Bayrak Halil İbrahim,Dalkıran Nuh Aygün","Department of Industrial Engineering, Bilkent University, 06800 Ankara, Turkey,Department of Economics, Bilkent University, 06800 Ankara, Turkey","Received 24 June 2021, Revised 5 August 2022, Accepted 14 September 2022, Available online 30 September 2022, Version of Record 14 October 2022.",https://doi.org/10.1016/j.jmateco.2022.102773,Cited by (0),"A freelancer with a time constraint faces offers from multiple identical parties. The quality of the service provided by the freelancer can be high or low and is only known by the freelancer. The freelancer’s time cost is strictly increasing and convex. We show that a pure-strategy equilibrium exists if and only if the preferences of the high-type freelancer satisfy one of the following two distinct conditions: ==== the high-type freelancer does not prefer providing his services for a price equal to the expected quality at the no-trade point; ==== the high-type freelancer prefers providing his services for a price equal to the expected quality at any feasible trade point. If ==== holds, then in equilibrium, the high-type freelancer does not trade, whereas the low-type may not trade, may trade efficiently, or may exhaust all of his capacity. Moreover, the buyers make zero profit from each of their traded contracts. If ==== holds, then both types of the freelancer trade at the capacity in equilibrium. Furthermore, the buyers make zero expected profit with cross-subsidization. In any equilibrium, the aggregate equilibrium trades are unique. Our results extend to the case where the freelancer has more than two types if the buyers are restricted to offering concave tariffs.","Consider a freelancer who has limited working hours either due to legal obligations (e.g., 48 h/week) or natural constraints (e.g., 24 h/day) and can serve multiple parties by allocating his time accordingly.==== Suppose the freelancer values the leisure time that he can spare from his working hours. Hence, working an extra minute gets more costly as the allocated time for work gets higher (convex cost). On the other side of the market, multiple parties can benefit from the services of the freelancer but have limited information regarding the quality of the service (adverse selection). Furthermore, no buyer can pose limits on the freelancer regarding the contract deals made with the other buyers (nonexclusivity). In modern labor markets, nonexclusivity becomes more and more the rule. A real-life example is a consultant who faces multiple firms seeking his expertise. What kind of trades shall we expect to arise in such a setup?====In this paper, we characterize the equilibrium trades for this problem under the following setting: There are at least two buyers interested in the services of the freelancer. The freelancer has private information regarding the quality of his service that can be either low or high. The buyers share a common prior regarding the quality of the service provided by the freelancer. The buyers have linear preferences for quality and compete through offering contracts that specify a quantity (number of working hours) and a transfer (payment to the freelancer).==== The freelancer observes the offers and chooses the contracts that maximize his payoff. The preferences of (each type of) the freelancer are quasilinear: They are linear in the aggregate payment and display strictly increasing convex cost in the aggregate quantity.====In this context, we characterize the freelancer’s aggregate trades in any pure-strategy equilibria. Our results can be summarized as follows: We provide two distinct conditions either of which is sufficient for the existence of a pure-strategy equilibrium. These conditions are also necessary so that there is no pure-strategy equilibrium if both fail to hold. Furthermore, they depend only on the preferences of the high-type freelancer:====If condition ==== holds, then the high-type freelancer does not trade in equilibrium while the aggregate trade of the low-type depends on his preferences. In such equilibria, the buyers make zero profit from each of their traded contracts. On the other hand, if condition ==== holds, then both types trade at the capacity and, there is cross-subsidization in equilibrium. In all of these equilibria, aggregate equilibrium trades are unique.====Our results contribute to the literature on competition under adverse selection. There are two classical papers in this literature: Akerlof (1970) considers a market where the sellers are privately informed about the quality of their goods. The goods are non-divisible, and all trades take place at the same price. Because uninformed buyers do not consider trading at a price above the average quality of the goods, sellers of high-quality goods end up not trading in equilibrium. On the other hand, Rothschild and Stiglitz (1976) considers a similar setup where uninformed buyers compete through contract offers for a divisible good. By offering different quantities at different unit prices, the buyers can screen the quality of the goods through sellers’ contract choices. Rothschild and Stiglitz (1976) allow only for exclusive competition, i.e., each seller can only trade with at most one buyer. They show that, when an equilibrium exists, low-quality sellers trade efficiently while high-quality sellers trade a non-zero, but sub-optimal quantity.====Attar et al., 2011, Attar et al., 2014 are the first to bring nonexclusive competition together with adverse selection. They observe that in many real-life market situations sellers simultaneously and secretly trade with several buyers. In their words, “nonexclusivity is the rule rather than the exception” in many markets. This is also true for the modern labor markets: many firms are simultaneously and secretly seeking the expertise of a freelancer. Hence, our work is complementary to Attar et al., 2011, Attar et al., 2014. These two papers differ from our work in two dimensions concerning the seller (the freelancer in our setup): ==== capacity constraint and ==== convex cost. Attar et al. (2011) consider a seller with a linear cost and a capacity constraint, whereas Attar et al. (2014) consider a seller who has convex preferences but does not have any capacity constraints.==== Our model differs from Attar et al. (2011) in that the freelancer has a convex cost in the aggregate quantity traded, and differs from Attar et al. (2014) in that the freelancer is subject to a capacity constraint. Therefore, by bringing the capacity constraint and convex cost together, not only do we consider a natural and relevant setup for labor markets but also we provide a bridge between the results of Attar et al., 2011, Attar et al., 2014. Table 1 summarizes the differences between our paper and these two papers.====Our results confirm that the Akerlof-like equilibrium outcomes presented in the earlier works extend to our setting. For instance, if the freelancer with the high-quality service is not willing to work at a price equal to the expected quality, then an equilibrium can be supported where only the low-quality freelancer has a chance to trade. In this case, the buyers protect themselves against the information asymmetry by offering a contract that is only acceptable to the low-quality freelancer. On the other hand, when the high-quality freelancer is willing to work for the price equal to the expected quality, the buyers find it profitable to offer a pooling contract. Then, no equilibrium can be supported unless the capacity constraint is low enough in the sense that at any feasible trade point, the marginal cost of the high type is less than the expected quality. In this case, a pooling equilibrium exists, and both types trade at the capacity. In any equilibrium, competition pushes the price up so that the buyers end up having zero expected profit.====Attar et al. (2014) note that “==== ==== ==== ==== ==== ====” In a subsequent paper, Attar et al. (2019), they extend their model to arbitrary discrete distributions, concentrating on pure-strategy equilibria under the assumption that the buyers offer concave quantity-transfer schedules. By following a similar methodology, we also show that our results extend to more general discrete type distributions of the freelancer when the buyers are restricted to offer concave tariffs.====Inderst and Wambach, 2001, Inderst and Wambach, 2002 study the role of capacity constraints in competitive screening models, which features an environment à la Rothschild and Stiglitz (1976). Somewhat parallel to our conclusion, they show that the presence of a capacity constraint alleviates the problem of non-existence of equilibrium in pure strategies. Their work departs from ours in two main aspects: They do not allow nonexclusive competition and assume that each contract issuer (buyer in our setting) faces a capacity constraint. Such a constraint on the buyers’ side limits their ability to unilaterally deviate and make a profit. In our paper, the capacity constraint is on the freelancer’s side, which, together with the non-negative trades assumption, limits the set of feasible deviations available to the buyers.====The organization of the rest of the paper is as follows. In Section 2, we introduce our model. In Section 3, we provide a characterization of the equilibrium. Section 4 presents our main results for the case where the freelancer has two types. Section 5 extends our model and the results to the case where the freelancer has multiple types when the buyers are restricted to offering concave tariffs. Finally, Section 6 concludes the paper.",Nonexclusive competition for a freelancer under adverse selection,https://www.sciencedirect.com/science/article/pii/S0304406822000994,30 September 2022,2022,Research Article,7.0
"Monet Benjamin,Vergopoulos Vassili","Centre d’Economie de la Sorbonne, Université Panthéon-Sorbonne, France,Laboratoire d’Economie Mathématique et de Microéconomie Appliquée, Université Panthéon-Assas, France","Received 9 November 2021, Revised 12 August 2022, Accepted 19 September 2022, Available online 29 September 2022, Version of Record 20 October 2022.",https://doi.org/10.1016/j.jmateco.2022.102770,Cited by (0),This paper studies decision-making under uncertainty and introduces a new preference axiom called ====. The latter requires some consistency between two forms of stochastic independence that can be inferred from choice behavior. Yet it can also be understood as a purely subjective version of the classical Independence axiom commonly used under risk. The main result presented in this paper uncovers the role that Subjective Independence plays in the axiomatic characterization of Subjective Expected Utility preferences.,"The axiom of Independence commonly used in decision theory under risk captures the idea that there is no meaningful complementarity between alternatives that are chosen on incompatible events. It is due to Marschak (1950) and Samuelson (1952) and plays a fundamental role in what has come to be called the von Neumann and Morgenstern (1947) theory of expected utility under risk by essentially delivering linearity in probability. For instance, see Fishburn and Wakker (1995) or Moscati (2016). The formal expression of Independence relies on exogenously given probabilities and requires the invariance of preference toward the familiar ==== operation on lotteries.====It is not clear if this axiom can be adapted to situations of uncertainty à la Savage (1954) where probabilities are rather endogenously determined by preferences. Indeed, the straightforward and naive version of Independence that results proves to be excessively restrictive. This naive version requires the invariance of preference toward the splicing operation on acts.==== As we will see in greater detail, under standard interpretations, it would force an agent to never update his preferences on the information he acquires, a peculiar motive suggesting that he already knows the true state ====.====To further illustrate, suppose that an event (a subset of the state space) ==== is believed by the decision-maker to be just as likely as its complement ====. Let ==== and ==== be acts in the nature of symmetric binary bets. That is, ==== is a bet on the event ==== occurring and ==== is the same bet, but on the event ==== occurring. Then, because ==== is believed to be as likely as ====, it is intuitive to have an indifference between ==== and ====. Meanwhile, because ==== represents a sure win while ==== only pays on ====, it seems reasonable to expect a strict preference for ==== over ====, in contradiction with the naive version of Independence and the invariance toward splicing it requires.====This example clearly illustrates what goes wrong in the naive version of Independence. Indeed, conditional upon observing ====, it seems reasonable to update the initial indifference between ==== and ==== into a strict preference for ==== over ==== because observing ==== completely reveals the outcomes induced by the two acts and hence the fact that ==== pays more. This suggests that the naive axiom must be modified by restricting it to cases where, contrary to the example, the acts under consideration are “stochastically independent” from the event. Such a restriction by “stochastic independence” would indeed explain why the agent does not update his ==== preferences: observing that some event holds conveys no useful information for ranking two acts that are “stochastically independent” from that event and leaves the ==== ranking of the two unchanged. However, it is still not clear at this stage what “stochastic independence” might mean in the Savage framework where probabilities are not exogenously available but only endogenously determined.====In this context, the first objective of this paper is to identify the precise behavioral notion of stochastic independence that is needed for the formulation of a meaningful version of Independence in the Savage framework. We will dub the resulting axiom that we propose ==== to highlight the fact that it is a purely subjective version of Independence; that is, one that does not rely on exogenous probabilities and is rather adapted to subjective probabilities and uncertainty. The second objective is to explore the implications of Subjective Independence for the axiomatic characterization of Subjective Expected Utility representations à la Savage.====Our approach to stochastic independence invokes first the standard definition of a decision-maker’s beliefs as revealed from his primitive preferences over acts. Indeed, a family ==== of events is said to be ordinally independent from another such family ==== if observing an event in ==== does not affect the comparative likelihood ranking of any two events in ====. Assuming an SEU representation of preferences, our first result, Proposition 1, shows that ordinal independence as just defined is equivalent to stochastic independence as ordinarily defined in probability theory, at least provided that the two families are “sufficiently rich and structured”. Finally, an act ==== is said to be ordinally independent from an event ==== if there exist such families ==== and ==== satisfying the following conditions: the outcome of ==== depends on events in ====, ==== lies in ====, and ==== is ordinally independent from ====. Assuming again an SEU representation, our second result, Corollary 2, shows how this notion of ordinal independence of an act from an event allows us to formulate a purely subjective version of lottery mixing and holds the key to our approach to Independence.====The first main result of this paper, Theorem 3, consists of an axiomatic derivation of the Subjective Expected Utility (SEU) representation of Savage (1954) based on Subjective Independence. It shows that the same SEU representation as that of Savage can be obtained by dispensing with Savage’s Sure-Thing Principle (P2) and Eventwise Monotonicity (P3), strengthening his Comparative Probability (P4) into Machina and Schmeidler’s (1992) Strong Comparative Probability (P4====) and additionally invoking our Subjective Independence. Equivalently, SEU can be obtained by substituting P3 for Subjective Independence in the axiomatic system that Machina and Schmeidler (1992) use to characterize probabilistic sophistication. The second main result, Theorem 4, shows that Machina and Schmeidler’s (1992) Strong Comparative Probability (P4====) is not a necessary ingredient of our first theorem. Indeed, the Savage SEU representation can also be obtained by dispensing with both P2 and P3, invoking Subjective Independence and additionally a condition of richness of the state space requiring, roughly, that sufficiently many sources of uncertainty are perceived as stochastically independent of each other. We will dub this condition ====.====Our approach and results provide a novel way to understand SEU. As already explained, our purely subjective version of Independence is restricted by stochastic independence. Indeed, under risk, any lottery can be considered to be stochastically independent from any event in a sense that we later explain in greater detail. But the state space used under uncertainty explicitly records the possible stochastic dependence between acts and events, and the restriction by stochastic independence becomes crucial for the logic of Independence. The intuition for Independence must then be amended accordingly in the following way: there is no meaningful complementarity between alternatives that are chosen on incompatible events if the alternatives in question are stochastically independent from the events. Furthermore, this restriction by stochastic independence ends up conferring a novel interpretation to the whole idea of Independence. As we will see, Subjective Independence can also be understood as a form of consistency between the agent’s perception of stochastic independence as revealed from his beliefs and his perception of stochastic independence as revealed this time from his preferences. It can be summarized by the following maxim: if one believes in the stochastic independence between two sources of uncertainty, they must be willing to act in consequence. In our view, that a purely subjective version of Independence turns out to admit such an interpretation is a rather unexpected feature.====Our study is not the first one to use notions of stochastic independence in the axiomatic derivation of some form of SEU. Mongin and Pivato (2015) and Mongin (2020) represent notable antecedents. These authors assume a two-tiered state space in the form of a Cartesian product of two finite factors and a topological outcome space and use axioms expressing the agent’s belief in the stochastic independence of the two factors to derive a form of SEU. However, this approach is unlikely to yield the sort of insight described in the previous paragraph because it makes it very difficult to establish a bridge to Independence and non-expected utility models of choice under risk. Furthermore, Grabisch, Monet, and Vergopoulos (2020) is a paper companion to the present one. Still within a two-tiered state space, they employ a condition similar to Subjective Independence and assert the consistency between stochastic independence as revealed ==== (i.e. after the resolution of the uncertainty attached to the one factor) and stochastic independence as revealed ==== (i.e. prior to the resolution of uncertainty attached to that same factor). This approach turns out to be more specifically adapted to the study of stochastic independence under uncertainty and ambiguity, as opposed to risk.====This paper is structured as follows: Section 2 introduces our framework, notation and key definitions of stochastic independence. Section 3 presents the axioms needed for our main result which is then stated in Section 4. All proofs are gathered in the appendices.",Subjective probability and stochastic independence,https://www.sciencedirect.com/science/article/pii/S0304406822000969,29 September 2022,2022,Research Article,8.0
"Mao Tiantian,Wang Ruodu","Department of Statistics and Finance, School of Management, University of Science and Technology of China, China,Department of Statistics and Actuarial Science, University of Waterloo, Canada","Received 12 March 2022, Revised 29 August 2022, Accepted 12 September 2022, Available online 24 September 2022, Version of Record 3 December 2022.",https://doi.org/10.1016/j.jmateco.2022.102766,Cited by (0),"Two notions of fractional ==== (SD) were recently proposed by Müller et al. (2017) and Huang et al. (2020) based on mean-reducing spreads and the coefficient of absolute ====, respectively. We formulate a general class of fractional SD generated by a convex transform, which includes those built from absolute or relative ==== as special cases, and this serves as a convenient technical tool for construction of new notions of fractional SD. We obtain equivalent conditions for a preference modeled by rank-dependent utility or cumulative prospect theory to be consistent with each notion of fractional SD. Furthermore, we provide an empirical estimator for the parameters in fractional SD relationships, and we illustrate this with a financial data analysis.","Two notions of fractional stochastic dominance (SD) were recently proposed by Müller et al. (2017) and Huang et al. (2020), respectively. Fractional SD was introduced because the classic notions of first-order SD (FSD) and second-order SD (SSD) are too often coarse and they could not capture, e.g., local convexity of a utility function in the expected utility (EU) model. Studies of stochastic dominance help to analyze decisions for a class of heterogeneous decision makers sharing some similarity in their risk attitude, without specifying the preference of a particular decision maker. The first notion of Müller et al. (2017) is based on ====-spread for ====,==== and will henceforth be referred to as ====-SD; see also Müller et al. (2021). The second notion of Huang et al. (2020) is based on the Arrow–Pratt coefficient of absolute risk aversion, and will be referred to as ====-SD for ====. Precise definitions are put in Section 2. For comprehensive discussions on the relevance of these notions, we refer to Müller et al. (2017) and Huang et al. (2020).====Risk aversion has been a critical concept in decision making since (Pratt, 1964, Arrow, 1974). Various notions of risk aversion were proposed, observed and tested from empirical studies by Harrison, 1986, Tversky and Kahneman, 1992, Kimball, 1993, Rabin, 2000, Rabin and Thaler, 2001 and Schmidt and Traub (2002), amongst others. As the most classic notion, a preference relation is strongly risk averse (Hadar and Russell, 1969, Hanoch and Levy, 1969, Rothschild and Stiglitz, 1970) if it is monotone in SSD. Since fractional SD bridges SSD and FSD, monotonicity in fractional SD can be seen as a property of fractional risk aversion.====Our main contribution is a characterization of monotonicity in fractional SD for the behavioral decision models of rank-dependent utility (RDU) of Quiggin (1982),==== and cumulative prospect theory (CPT) of Tversky and Kahneman (1992). The considered notions of fractional SD include ====-SD, ====-SD and the latter’s analogue based on relative risk aversion. Both RDU and CPT are generalizations of the EU model and the dual utility theory (DT) of Yaari (1987). Although RDU can be seen as a special case of CPT, conditions for fractional SD in RDU are more mathematically concise and economically interpretable, and hence we will present RDU and CPT results separately in Section 3.====To explain our motivation for studying fractional SD in behavioral models, we look again at FSD and SSD, which are limits of fractional SD. FSD and SSD were traditionally formulated based on EU, although these properties are model free. For instance, SSD can be equivalently formulated via mean-preserving spread (Rothschild and Stiglitz, 1970), conditional expectations (Strassen, 1965), dual utility (Yaari, 1987), and aversion to positive dependence (Wang and Wu, 2020); the case of FSD is similar. Likewise, the notions of fractional SD are suitable for study beyond EU, and in particular, we are interested in their implication for the popular descriptive decision models of RDU and CPT.====Equivalent characterization of strong risk aversion in different decision models has been studied by Pratt (1964) and Arrow (1974) for EU, Yaari (1987) for DT, Chew et al. (1987) for RDU, and Schmidt and Zank (2008) for CPT. Our results generalize the above results to several formulations of fractional SD, including ====-SD and ====-SD. Moreover, characterization results are obtained for a class of fractional SD connected to SSD via a transform ====, called ====-SD, which includes ====-SD as special cases.====As we will see in Section 2, ====-SD is closely related to an exponential transformation. More precisely, ==== is dominated by ==== in ====-SD if and only if ==== is dominated by ==== in SSD, where ====. Therefore, many results and convenient properties of SSD can be translated to those of ====-SD. A negative result (Proposition 2) implies that there does not exist a risk transform or probability distortion such that ====-SD can be associated with SSD. To be more precise, ====-SD between ==== and ==== cannot be equivalently described by SSD between ==== and ==== for any transform ====, and this remains so if we further allows for probability distortions as in RDU. This nonexistence illustrates that the mathematical basis of ====-SD is fundamentally different from SSD and ====-SD, which also explains why technical results such as a characterization in RDU and CPT are much more complicated for ====-SD than that for ====-SD.====To better understand applications of fractional SD in behavioral decision models outside EU, we proceed to study further technical properties of fractional SD in Section 4, empirical estimators of parameters of fractional SD between two distributions in Section 5, and a real-data analysis in Section 6. These additional results illustrate what we can analyze when fractional SD is brought outside EU.====All proofs are relegated to Appendix A. Some simulation results for estimating the parameters ==== and ====, complementing Section 6, are put in Appendix B.",Fractional stochastic dominance in rank-dependent utility and cumulative prospect theory,https://www.sciencedirect.com/science/article/pii/S0304406822000921,24 September 2022,2022,Research Article,9.0
"Bich Philippe,Fixary Julien","Paris School of Economics, Université Paris 1 Panthéon-Sorbonne UMR 8074 Centre d’Economie de la Sorbonne, France,Université Paris 1 Panthéon-Sorbonne UMR 8074 Centre d’Economie de la Sorbonne, France","Received 15 December 2021, Revised 7 July 2022, Accepted 7 September 2022, Available online 21 September 2022, Version of Record 3 December 2022.",https://doi.org/10.1016/j.jmateco.2022.102767,Cited by (0),", which we characterize. This improves recent results in Bich and Morhaim (2020) or in Herings and Zhan (2022), and can be applied to many existing models, as for example to the public good provision model of Bramoullé and Kranton (2007), the information transmission model of Calvó-Armengol and İlkılıç (2009) or the two-way flow model of Bala and Goyal (2000).","Pairwise stability concept, proposed by Jackson and Wolinsky (1996), plays a central role in strategic network formation models. Roughly, a network is pairwise stable if “====” (Jackson and Wolinsky, 1996). First introduced for unweighted networks (i.e. networks for which relationships are measured by ==== or ====), it was extended for weighted networks (i.e. networks for which relationships are measured by real numbers between ==== and ====), and recently, Bich and Morhaim proved that a pairwise stable weighted network always exists, for large classes of payoff functions (Bich and Morhaim, 2017, Bich and Morhaim, 2020).====The aim of this paper is to provide new important properties of the set of pairwise stable networks. In game theory, similar properties have already been proved for the set of Nash equilibria: in 1971, Wilson (1971) has established his famous oddness theorem (====, each finite strategic-form game has an odd number of mixed Nash equilibria); then, several authors have proposed alternative proofs (e.g. Blume and Zame, 1994, Govindan and McLennan, 2001, Govindan and Wilson, 2001, Harsanyi, 1973, Herings and Peeters, 2001, Mas-Colell, 2008, Pimienta, 2009), some of them being based on Kohlberg–Mertens’ structure theorem (Kohlberg and Mertens, 1986). In this paper, we prove similar results for the set of pairwise stable networks, by allowing large classes of payoff functions:====We would like to emphasize that our theorems are not applications of existing results in game theory. First, by nature, a pairwise stable network is not a Nash equilibrium: two agents who want to create a link together have to decide it simultaneously (i.e. deviations have to be bilateral in some cases), whereas deviations are always unilateral in Nash equilibrium concept. More precisely, assessing that there exist generically an odd number of Nash equilibria is equivalent to say that there are generically an odd number of fixed-points of the best-reply correspondence, but there is no natural and analogue formulation for pairwise stability concept. Second, a notion of “mixed pairwise stability”, comparable to the notion of mixed Nash equilibrium in game theory, seems less meaningful in network formation theory. This explains also – apart from its mathematical interest – why we consider general classes of polynomial payoff functions, going beyond the case of multilinear payoff functions. However, this also creates new technical difficulties: to prove our oddness theorem, we have to enter more deeply in the theory of semi-algebraic sets (in particular, we will provide some new decomposition result for semi-algebraic sets). Third, we have to “extend” Kohlberg–Mertens’ theorem to the framework of network formation theory and pairwise stability. Our extension (i.e., our structure theorem) is not only a rewriting of existing proofs in game theory, since, in essence, we do not deal with standard non-cooperative strategic-form games.====To conclude this paper, we describe several standard models in network formation theoretical literature to which our oddness theorem can be applied. A first example is the public good provision model of Bramoullé and Kranton (2007): ==== agents are characterized by some level of effort ====, ====, (e.g. it could be the amount of time a consumer spends researching a new product) and interact in some network ==== (the idea being that agents could benefit from the other agents’ efforts, thanks to network externalities). Oddness theorem implies that, in this standard model, there exist an odd number of pairwise stable networks, generically with respect to some parameters of the model (i.e., in short, there exist an odd number of pairwise stable networks for “most” parameters of the model). We prove similar results in the information transmission model of Calvó-Armengol and İlkılıç (2009) and the two-way flow model of Bala and Goyal (2000).====The paper is organized as follows: in Section 2, we first recall some basic definitions and notations about strategic network formation theory and pairwise stability, and we define the graph of pairwise stable networks (Section 2.1), then we state our structure theorem (Theorem 2.1, Section 2.2) and a corollary of this theorem (Corollary 2.1) which will be important for the next section; in Section 3, we first state our oddness theorem (Theorem 3.1, Section 3.1), then we provide several examples of applications, including in particular the ones mentioned before (Section 3.2); in (Appendix), we provide first the necessary reminders about real algebraic geometry (Appendix A.1) and about specific notions of general topology (Appendix A.2), we provide the proofs of Theorem 2.1 (Appendix A.3), of Corollary 2.1 (Appendix A.4) and of Theorem 3.1 (Appendix A.5). We also provide sketches of proofs at the beginning of Appendix A.3 and of Appendix A.5 to help the reader. Finally, we provide a table of the main notations which are used in this paper (Appendix A.6).",Network formation and pairwise stability: A new oddness theorem,https://www.sciencedirect.com/science/article/pii/S0304406822000933,21 September 2022,2022,Research Article,10.0
Besner Manfred,"Department of Geomatics, Computer Science and Mathematics, HFT Stuttgart, University of Applied Sciences, Schellingstr. 24, D-70174 Stuttgart, Germany","Received 16 April 2022, Revised 25 July 2022, Accepted 29 August 2022, Available online 5 September 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102764,Cited by (0),"We propose a value for games with transferable utility, called the grand ====. This new value is an alternative to the ====, in particular in games where, in the process of coalition formation, two overlapping coalitions can, at least hypothetically, guarantee their members their full worth simultaneously. Central is the concept of the grand surplus, which is the surplus that results when all coalitions, each lacking one player of the player set, no longer act individually but only cooperate as the grand coalition. All the axiomatizations presented have an analogous equivalent for the ","The concept of a coalition function, also called characteristic function, goes back to von Neumann and Morgenstern (1944). In Shapley (1953b), a TU-game is given by a finite subset ==== of the universe of all possible players and a superadditive set function (the coalition function) from the subsets of ==== into the real numbers with the only condition that the worth of the empty set is zero. We will follow Shapley’s approach but dispense with superadditivity. The coalition function can be used, for example, to model and analyze economic, political, or other social phenomena. In general, the worth of a coalition is the reward that this coalition can guarantee its players, regardless of what the other players outside the coalition do.====In the model of Harsanyi, 1959, Harsanyi, 1963, the fundamental assumption is that each player is simultaneously a member of all possible different coalitions (Harsanyi uses the term ‘syndicate’) which contain it. Introducing the important concept of his (Harsanyi) dividends, he assumes that each coalition guarantees a certain payment, the Harsanyi dividend, which should be divided among the members of this coalition. Moreover, these dividends should be assumed in addition to any dividends that each member of the coalition may receive from other coalitions. Under these assumptions, Harsanyi can show that his solution for TU-games provides each player with an equal share of all Harsanyi dividends from coalitions containing this player and coincides with the Shapley value Shapley (1953b). Thus, according to Harsanyi, the coalition function inherently justifies the Shapley value, but only under the above assumptions; in particular, no externalities must have to be taken into account.====For many scenarios, these assumptions are quite reasonable. But other situations are also conceivable. Harsanyi (1959) himself points out that von Neumann and Morgenstern (1944) assume that each player is a member of only one coalition of players from a player set. For the equal division value (see, e.g., Zou et al. (2021)), we can assume that the grand coalition (the coalition containing all players) is the only coalition that forms. If we assume that if the grand coalition does not form, no other coalition is formed, then, in the case of cooperation, only the grand coalition receives a dividend equal to its worth, which is then distributed equally among all players. Considering the equal surplus division value, introduced in Driessen and Funaki (1991) as the center-of-gravity of the imputation-set value, if the grand coalition would not form, the singletons can be assumed to be the only coalitions formed. As a payoff for the equal surplus division value, each player receives an equal share of the surplus of the worth of the grand coalition over the worths of the singletons as a dividend of the grand coalition and its stand-alone worth as a dividend, paid in full, for the surplus of the singleton over the empty set.====While the last two values consider only a (small) part of the worths of all possible coalitions, this is not the case for the Shapley value and the following new value. Just as Shapley (1953b) did for the introduction of his value, we focus on games without externalities.====Unlike in the model of Harsanyi, 1959, Harsanyi, 1963, in our model, we assume that, in the process of forming coalitions, each coalition formed prevents the simultaneous formation of any proper subcoalition. As in Hart and Kurz (1983), we posit that “...interactions among players will be conducted on two levels: first, among the coalitions, and second, within each coalition”. But we are not restricted to considering just partitions of the player set, called coalition structures (Aumann and Dréze, 1974, Owen, 1977).==== In this process, we will allow overlapping coalition formations in which each such formed coalition will then be guaranteed, at least hypothetically, the full coalitional worth at the same time. Therefore, for the coalition function, we assume, at first, it represents the worths of coalitions of players who bargain less with physical goods and more with goods that are more or less arbitrarily multipliable or applicable, particularly in the area of intellectual property. These would be, for example, patents, data, software, process engineering and production methods, film and music industry products, multi-agent systems in artificial intelligence, and the like.====In Shapley (1953b), marginal contributions are the central point for the distribution of cooperative gains. The marginal contribution of a player ==== to a coalition ==== containing ==== can be viewed as the benefit of ==== over ==== that results if ==== joins ====. If we still subtract the stand-alone worth from a marginal contribution, we get the marginal surplus, which can be viewed as the benefit that results when the individual player and the coalition ==== join forces.====As in Hart and Kurz (1983), “... we assume as a postulate that society as a whole acts efficiently;..”., which, in our model, means that in the end, only the grand coalition should emerge. The central concept in this study is the benefit that arises when all coalitions, each lacking one player from the player set, join to form the grand coalition, referred to as the grand surplus.====Then, we can take a closer look at subgames on the player sets where one player of the original player set is removed and, accordingly, get the grand surplus for the grand coalition in each subgame. Proceeding in this way, we obtain grand surpluses for all coalitions, until finally, each player receives its stand-alone worth as the grand surplus for its singleton on the one-player game. Of course, non-positive grand surpluses may also occur, just as happens for Harsanyi dividends. For player sets with only two players, the grand surplus coincides with the marginal surpluses of both players.====With the concept of the grand surplus, we can introduce a new TU-value, called grand surplus value. As a payoff, for the grand surplus value, each player receives an equal share of the grand surplus of all subgames in which the player is a member of the player set. Note, however, that, depending on the size of the player set and the number of members in a coalition, we may have to take into account the same dividend several times, just as our assumption above would dictate.====The grand surplus value satisfies many axioms that are also satisfied by the Shapley value, and it also satisfies a set of new axioms that are analogous to ones also satisfied by the Shapley value. Therefore, we can give axiomatizations of the grand surplus value which are analogous to axiomatizations of the Shapley value in Shapley (1953b), Myerson (1980), and Besner (2020). Especially, the grand surplus monotonicity, which states that for a player, the payoff does not decrease if the grand surpluses do not decrease, has interesting economic significance, similar to strong monotonicity Young (1985). It offers, along with efficiency and symmetry, an analogous characterization of the grand surplus value to the axiomatization of the Shapley value in Young (1985).====Within noncooperative game theory, the study of repeated games occupies a broad space (see, e.g., Benoit and Krishna (1985), Aumann and Shapley (1992), DeMarzo (1992)). In contrast, the number of studies within cooperative game theory is quite modest. In this context, we refer to Oviedo (2000), Kranich et al. (2005), or, as a recent study, to Berden et al. (2022). Here, a repeated cooperative game is considered as a model, where the players of the same player set play more than one cooperative TU-game over time or sometimes also simultaneously. The setting is usually dynamic so that, for example, the utility function or coalition function changes over time, the payoff is discounted, payoff shares are carried over from previous games, or sometimes, the set of players changes.====In this study, we use a different cooperative approach. We introduce repeated cooperative cross-games with coalitional collaboration (====-games for short) and show that for this class of games, the grand surplus value is preferable to the Shapley value. An ====-game combines multiple times the same base games, but there can be cooperation between a set of players from a preceding game and a disjoint set of players from a following game.====For the payoff calculation of the grand surplus value, the same grand surplus of a subgame is used several times, depending on the size of the initial player set and the subgame player set. In the last content section, we combine these multiple grand surpluses of the same coalition into multiple dividends. If these multiple dividends are interpreted as Harsanyi dividends of a new coalition function, we can show that the Shapley value for the resulting multiple dividends game is equal to the grand surplus value for the original game, which has far-reaching consequences.====The article is organized as follows. In Section 2, we give some preliminaries. Section 3 introduces the grand surplus and the grand surplus value. Section 4 proposes coveradditive games, the cover-core, and an example shows that in a special case, which corresponds to our assumptions above, the grand surplus value is preferable to the Shapley value. In Section 5, we give two axiomatizations which are analogous to axiomatizations in Besner (2020) and Myerson (1980). In Section 6, we provide an axiomatization that is similar to the classical axiomatization of the Shapley value in Shapley (1953b). Section 7 introduces the ====-games and shows that the grand surplus value is preferable to the Shapley value for these games. In Section 8, we give an axiomatization that corresponds to the axiomatization of the Shapley value in Young (1985) and give an example that also supports the preferability of the grand surplus value over the Shapley value for ====-games. Next, in Section 9, we recall some results of the potential, the reduced game, and the consistency property in Hart and Mas-Colell (1989). Afterward, we introduce multiple dividends and an associated multiple dividends game, which is then used to establish a strong connection of the grand surplus value to the Shapley value. Finally, Section 10 contains some concluding remarks and points out possible extensions of the grand surplus value. The Appendix (Appendix) shows the logical independence of the axioms in our characterizations.",The grand surplus value and repeated cooperative cross-games with coalitional collaboration,https://www.sciencedirect.com/science/article/pii/S0304406822000908,5 September 2022,2022,Research Article,11.0
"Araujo A.,Gama J.,Suarez C.E.","IMPA, Rio de Janeiro, Brazil,FGV EPGE, Rio de Janeiro, Brazil,Universidade Federal de Minas Gerais, Belo Horizonte, Brazil","Received 24 September 2021, Revised 15 August 2022, Accepted 24 August 2022, Available online 5 September 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102763,Cited by (0)," will induce the Prospect Theory agents to abandon the reference point and instead choose a boundary allocation. This behavior contrasts with the predictions of the endowment effect. Assuming the reference point as initial endowment, we prove the prevalence of either a non-trading equilibrium or a frontier equilibrium.","Despite the empirical and experimental evidence on various economic behaviors, such as ambiguity loving and Prospect Theory (PT), such behaviors, to the best of our knowledge, have not been analyzed deeply in general equilibrium literature. This study considers economies with PT and risk-averse agents. Specifically, we prove that there are only two types of equilibria in exchange economies with these two agents: a frontier-located or a no-trade equilibrium.====The first type of equilibrium corresponds to an allocation on the Pareto frontier. To generate it requires a significant aggregate uncertainty in the economy in all states of nature. This uncertainty is mainly in the hands of risk-averse agents instead of PT agents. Suppose the aggregate risk in the hands of the risk-averse agents is sufficiently large. In that case, the equilibrium price will cause the PT agent to abandon the reference point by choosing a boundary allocation thereby overcoming the endowment effect. In other words, the risk-loving attitude for losses of the PT agent then exceeds and dominates their loss aversion.====The second type of equilibrium occurs when there are low levels of risk in the hands of the risk-averse agents. In this case, an interior equilibrium exists since the loss aversion of the PT agents overcomes their risk-loving for losses. Therefore, under this condition, PT decision-makers consume their endowment allocation, thereby generating an autarky equilibrium.====In both types of equilibria, two properties of PT agents are essential for the outcomes: risk-loving for losses and loss aversion. The former is a consequence of the S-shaped utility that this type of agent has, making the utility concave above the reference point and convex below it. The latter is a consequence of how PT agents perceive more losses than gains.====In the case of the frontier equilibrium, PT agents will buy part of the aggregate risk owned by the risk-averse agent at equilibrium, leading to a mutually beneficial exchange of risk between the agents across all states (as in Chateauneuf et al., 2000) in the case of ambiguity aversion with Choquet expected utility). The type of aggregate uncertainty needed in this case is quite different from that mentioned in Araujo et al. (2018) where the variation of the aggregate endowment occurs only in a single state, and in our model corresponds to a variation among all states of nature.====The aggregate uncertainty conditions needed for the existence of this type of equilibrium can be associated with large-scale changes that can occur in the economy. For example, nuclear accidents, pandemics, mighty earthquakes, tsunamis, severe droughts, floods, hurricanes, variations in solar radiation, war, financial crises, and global warming. Depending on its magnitude, each of these events has a global impact affecting a large proportion of the population. In these situations, ambiguity lovers will absorb most of the uncertainty in the economy. Similarly, here, we illustrate that PT decision-makers behave similarly to uncertainty lovers under the aggregate uncertainty conditions mentioned above. This implies more equal prices among states and less uncertainty being absorbed by the risk-averse decision-makers in a two-state economy. However, PT agents do not behave as ambiguity lovers in economies with more than two states. They only neglect the least essential state, implying that they have a positive consumption in most states of nature.====The endowment effect determines the PT optimal consumption as the initial endowment in the interior equilibrium. This case corresponds to a non-trading equilibrium. Therefore, all risk in the hands of PT agents transfers to risk-averse agents. However, for this equilibrium to exist, it is not sufficient that the risk in the hands of the risk-averse agents is as low as in the exogenous reference point case. The aggregate risk of the economy must be small. Appendix A.3 has an analogous study of PT agents with endogenous reference points similar to Bell, 1985, Gul, 1991, and Loomes and Sugden (1986). Although no theoretical works, to the best of our knowledge, analyze PT and risk-averse agents in a general equilibrium model, some authors have found experimental evidence for the coexistence of both kind of agents (see Abdellaoui, 2000, Abdellaoui et al., 2007, Abdellaoui et al., 2008, Etchart-Vincent, 2004, Laury and Holt, 2008, Booij et al., 2010, Fehr-Duda et al., 2006). Other works have explored how PT decision-makers behave in simple market frameworks. Kahneman et al., 1990, Kahneman et al., 1991 show the prevalence of the endowment effect in most participants in their experiments.====List (2004) not only corroborated the existence of status quo bias for inexperienced agents in market environments but also showed that more experienced agents behave as risk-averse. Myagkov and Plott (1997) showed in experiments that individuals tend to be risk-loving for losses, undertaking extreme consumption once they enter the market. Although, this effect tends to decrease as individuals gain experience. The two previous behaviors correspond to two fundamental characteristics of PT: the endowment effect and the reflection effect. The former is strongly related to loss aversion since a high-level of it causes aversion to consumption allocations outside of the reference point. The latter is related to being risk-averse for gains and risk-loving for losses.====From a theoretical point of view, several authors have explored the risk aversion properties of Cumulative Prospect Theory, including Schmidt and Zank (2008) noting that risk/ambiguity lovers and PT decision-makers have different properties in the presence of uncertainty. However, due to the reflection effect, the equilibrium analysis of economies with PT agents has remained related to economies with uncertainty-loving agents. Araujo et al. (2018) proved that under mild assumptions, it is possible to ensure equilibrium in economies with uncertainty-loving preferences and the risk lovers absorb the risk that exists in the economy.====The remainder of this study is organized as follows. In Section 1, we analyze the Edgeworth box and the two types of equilibrium in the economy as mentioned above. In Section 2.1, we recall the Arrow–Debreu (AD) model with convex utilities, as in Araujo et al. (2018). In Sections 2.2 Model, 2.3 Existence of equilibrium with large aggregate uncertainty, we present the model with PT decision-makers and offer conditions for the existence of a frontier equilibrium. Finally, in Section 3, we present the conclusions of this study.",Lack of prevalence of the endowment effect: An equilibrium analysis,https://www.sciencedirect.com/science/article/pii/S0304406822000891,5 September 2022,2022,Research Article,12.0
"Ceron Federica,Vergopoulos Vassili","Univ Lyon, UJM Saint-Étienne, GATE UMR 5824, F-42023 Saint-Étienne, France,Université Paris 2 Panthéon-Assas, LEMMA, 4 rue Blaise Desgoffe, 75006 Paris, France","Received 22 November 2021, Revised 9 August 2022, Accepted 22 August 2022, Available online 31 August 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102761,Cited by (0),"We provide an axiomatic characterization of recursive MaxMin preferences that stem from a (possibly) incomplete relation representing preference judgements that are justified by objective probabilistic information. To ensure that her choice behavior be informed by objective information, dynamically consistent, and ambiguity averse, the decision-maker evaluates alternatives by their minimal expected value over the rectangular hull of the objective information set. The characterization builds upon two axioms that naturally combine the three requirements in a behavioral way. Our main result suggests a principled justification for the use of recursive MaxMin preferences in applications to dynamic choice problems.","Since most decision processes take place over time in an uncertain environment with accumulating evidence, it is important that decision models provide sound explanations concerning how preferences evolve in response to the arrival of new information. In this regard, a key property of preferences is dynamic consistency, which roughly states that if an alternative is preferred to another one conditionally upon the realization of every event in some collection of exhaustive mutually exclusive events (a partition of the state space), it shall be preferred ====. Dynamic consistency is largely viewed as a rationality tenet, as it entails a number of desirable properties such as immunity to Dutch book operations and the non-negative value of information. Furthermore, it guarantees that value functions have a recursive structure, which ensures equality between ==== optimal plans and backward induction solutions. In this paper, we analyze preferences conforming to one of the most prominent models of decision theory under uncertainty, namely the MaxMin Expected Utility model of Gilboa and Schmeidler (1989). While MaxMin preferences (as most non-Bayesian decision theories) generally violate dynamic consistency, Sarin and Wakker (1998) and Epstein and Schneider (2003) show that they can satisfy dynamic consistency with respect to a given information structure,==== rather than with respect to all of them (see also Epstein and LeBreton, 1993 and Riedel, 2004). In particular, this requires the agent’s set of priors to have a particular shape, i.e. to be rectangular. The fact that, for MaxMin preferences, dynamic consistency translates into a condition on the decision-maker’s set of priors raises the question of how restrictive such an assumption is. Indeed, in most economic applications, where the information structure is fixed from the start and the decision-makers’ priors are interpreted to represent evidential probabilistic information available to them, focusing on rectangular sets of priors may give the impression of overly constraining the economic behavior that one can model. As Al Najjar and Weinstein (2009) put it: “A sensible theory of updating should not selectively limit its scope to those situations where its desired implications seem to hold, while remain silent about what happens if slight perturbations to the information structure are introduced” (Al Najjar and Weinstein, 2009 p. 271).====The objective of this paper is to counter this negative conclusion by showing that, as long as the decision-maker expects to observe some partial information, there are sound reasons to replace the objective probabilistic information available in a decision problem with its rectangular hull, a strategy already suggested by Epstein and Schneider (2003).==== More precisely, we provide a novel rationale for the emergence of dynamically consistent MaxMin preferences that is based on the evidential probabilistic information available to the decision-maker. We accomplish this task by endowing the decision-maker with two preference relations, “objective” and “subjective” rationality, that are meant to capture different aspects of the decision-maker’s preferences.==== Objective rationality captures the partial ordering of alternatives that is justified by hard evidence, while subjective rationality describes the decision-maker’s choice behavior. They represent (respectively) a mental and a behavioral notion of preferences: an alternative ==== is objectively preferred to another alternative ==== if the decision-maker believes that ==== is superior to ==== by virtue of the available evidential information; it is subjectively preferred to it if the decision-maker picks ==== whenever she has to choose among ==== and ====. We suppose that the choice domain is the set of (Anscombe–Aumann) consumption processes proposed by Epstein and Schneider (2003) and that uncertainty resolves sequentially over time. Objective rationality is then modeled as an incomplete relation ==== reflecting probabilistic information prior to any resolution of uncertainty, and subjective rationality as a collection of binary relations {====, ====}, representing choices at each time-state pair. We suppose further that objective rationality admits a discounted unanimity representation, according to which a consumption process ==== is objectively preferred to a consumption process ==== if and only if ==== where ==== is the discount factor for future consumption, ==== is the decision-maker’s utility, and ==== is the set of probability measures representing the objective probabilistic information. (An axiomatic characterization of representation (1) is provided in Appendix A). Then, we address the question of how the decision-maker can use objective information to construct the subjective ordering of alternatives that guides her behavior. A natural way to do so is to require that objective preference statements be preserved by subjective preferences, as proposed by Gilboa et al. (2010). However, as we illustrate in the next section by means of an example, this form of coherence with respect to objective information comes at the price of forsaking the dynamic consistency of behavior. Intuitively, this is because alternatives that are ==== unambiguous according to objective information may become objectively ambiguous upon observing every event in a given partition, so that the ==== estimation turns uniformly less precise no matter how uncertainty unfolds. In such situations, an ambiguity-averse decision-maker who systematically upholds the objective evaluation will exhibit intertemporal preference reversals (dynamic inconsistencies). To guarantee dynamic consistency, we essentially require the objective evaluation to be binding for subjective preferences provided that no conditional ambiguity arises, i.e. when the uncertainty attached to the alternatives that are compared is fully resolved in the next period. Concretely, this translates into two novel axiomatic conditions that naturally combine the three desiderata of having choice behavior being informed by objective information, ambiguity averse, and dynamically consistent.====Our main contribution, Theorem 1, consists in showing that, under some basic conditions, these two axioms characterize Sarin and Wakker (1998) and Epstein and Schneider (2003)’s recursive MaxMin representation for subjective rationality, with the additional property that the representing set of priors is the rectangular hull of the objective information set, i.e. the smallest rectangular set containing it. Formally, and letting {====, ====} denote the rectangular hull of ====, each ==== is represented by the functional ====Representation (2) achieves the dynamic consistency of MaxMin preferences without imposing rectangularity constraints on the evidential information that is available to the decision-maker. It does so by proposing a specific methodology for constructing the decision-maker’s subjective preferences in which non-compliance with objective information is only allowed when the latter fails to detect the conditional ambiguity attached to alternatives. As such, we view it as offering a novel possible account by which MaxMin behavior can emerge from ambiguous evidential information. An account that, moreover, can provide guidance for the use of MaxMin preferences in economic applications.====Our approach was inspired by the paper of Bastianello et al. (2022), who are the first to characterize the preferences of a MaxMin agent whose choice-relevant priors correspond to the rectangular hull of objective information within the model of Gilboa et al. (2010). The authors suppose that the sequential resolution of uncertainty is modeled by a single partition of the state space and consumption occurs at the terminal date. They further assume that the decision-maker is endowed with a second preference relation ==== that admits a unanimity representation and identify axiomatic conditions on the interplay between objective rationality and ==== that guarantee that the set of priors representing ==== is the rectangular hull of the one representing objective rationality. These axiomatic conditions essentially demand that ==== be the maximal restriction of objective rationality such that the set of priors representing ==== is contained in the rectangular hull of the objective information set. They then establish the MaxMin representation of subjective rationality with respect to this rectangular hull by applying the Gilboa et al. (2010) completion technique to ====. In contrast, the rectangularization procedure of objective information that is described in Theorem 1 solely emerges from the interplay between objective and subjective preferences, and hence does not invoke the intermediary preference relation ====. Nonetheless, following Ghirardato et al. (2004), it is possible to retrieve ==== in our model by looking at the “unambiguous restriction” of subjectively rational preferences (i.e. the maximal restriction of ==== that satisfies the axiom of independence).",Objective rationality and recursive multiple priors,https://www.sciencedirect.com/science/article/pii/S030440682200088X,31 August 2022,2022,Research Article,13.0
"De Donno Marzia,Menegatti Mario","Department of Mathematics for Economics, Finance, and Actuarial Sciences, Catholic University of Milan, Italy,Department of Economics and Management, University of Parma, Italy","Received 11 March 2022, Revised 7 June 2022, Accepted 8 August 2022, Available online 22 August 2022, Version of Record 2 September 2022.",https://doi.org/10.1016/j.jmateco.2022.102757,Cited by (0),"We show conditions which ensure that the comparisons between ==== of different orders of two ==== are related. In particular, we derive a condition ensuring that greater downside ==== implies greater risk aversion and a different condition ensuring that the opposite implication holds. We then generalize these results to higher order greater risk aversion, obtaining conditions which make it possible to infer the direction of the comparison for risk aversion of a given order from the knowledge of the direction for a different order.","In his seminal paper, Pratt (1964) formalizes the relationship between aversion to risk and the concavity of the utility function. He also introduces a comparison between preferences in terms of local risk aversion and risk premium and shows that one decision maker has ==== than another when the utility function of the first is a concave transformation of the utility function of the second (i.e. the ratio of the marginal utilities of the two decision makers is a decreasing function). Rothschild and Stiglitz (1970) deepen the analysis of risk aversion and identify the concavity of the utility function with the aversion toward mean-preserving spreads in the distribution of random variables. Since that paper, this kind of change in the distribution of wealth has usually been called an ====. Diamond and Stiglitz (1974) adapt this notion to the comparison between preferences, reinterpreting greater risk aversion of one decision maker compared to that of another as greater aversion to increases in risk.====Menezes et al. (1980) extend Rothschild and Stiglitz’s approach to define ==== as the aversion to changes in risks that shift the distribution of wealth towards the lower tail, preserving its mean and variance. This kind of change in the distribution is called an ====. Menezes et al. (1980) show that an agent exhibits downside risk aversion if and only if his/her marginal utility is convex. Similarly to Diamond and Stiglitz, Keenan and Snow (2002) extend this approach to the comparison between preferences, showing that one decision maker has ==== than another when the utility function of the first decision maker is related to the utility function of the second by a transformation function whose derivative is convex (i.e. the ratio between their marginal utilities is a convex function).====The notion of greater downside risk aversion cannot however be used to rank preferences, because it is neither transitive nor antisymmetric. In order to introduce a (partial) ranking of preferences, Keenan and Snow (2016) introduce the concept of strongly greater downside risk aversion reflecting the case where one decision maker has greater aversion than another to both increases in risk and increases in downside risk. This analysis has also been generalized by Keenan and Snow (2018) and Liu and Wong (2019) with the introduction of the notions of higher order greater risk aversion and higher order strongly greater risk aversion (mixed risk aversion in Liu and Wong (2019)).====An issue which has not as yet been thoroughly investigated is however the relationship between different degrees of greater risk aversion. While it is clear that, by its definition, strongly greater risk aversion of some order implies (both strongly and not) greater risk aversions of the lower orders, to our knowledge little has been written about the converse implication, as well as any kind of relationship between greater risk aversion of different orders. The main goal of this paper is to identify conditions under which a degree of greater risk aversion can be inferred from another one. In this regard, we first discuss a condition on preferences ensuring that greater downside risk aversion implies greater risk aversion. Secondly, we derive a different condition ensuring that greater risk aversion implies greater downside risk aversion for sufficiently large levels of wealth. It is worth noting that, under the derived conditions, greater downside risk aversion and strongly greater downside risk aversion are equivalent. We then extend our conclusions to higher orders: specifically we derive generalized conditions, ensuring both that greater risk aversions of higher orders implies greater risk aversions of lower orders and vice versa.====Following the approach of Diamond and Stiglitz (1974) and Keenan and Snow, 2002, Keenan and Snow, 2009 we interpret our results on greater risk aversion in terms of compensated increases in risk and differential risk premium. Moreover, as in Keenan and Snow, 2016, Keenan and Snow, 2018 and Liu and Wong (2019), strongly greater risk aversion of a certain order has implications on the response of a decision maker to stochastic dominance shifts of the same order. Our results allow us to show that, under the appropriate conditions, decision makers response to some kinds of shift (compensated or stochastically dominated) in risk is related to decision makers response to shifts of different kinds and orders.====The analysis in the present paper is also related to a different branch of literature, which studies the relationship between different aspects of the attitude toward risk of an agent. In this field, Menegatti (2001) shows conditions ensuring that, for a single agent, risk aversion implies prudence and temperance.====
 Menegatti (2014) examines the opposite direction of this linkage, deriving conditions ensuring that temperance implies prudence and prudence implies risk aversion. Menegatti (2015) generalizes this reasoning to higher orders of risk aversion.==== Lastly, De Donno and Menegatti (2020) study conditions for the equivalence of risk aversion of different orders. As noted above, all these results show the existence of linkages between the aspects of risk attitude of ====. The analysis in the present paper is thus also complementary to this approach, since it studies the linkages between the aspects of the ==== risk attitude of ====.====The paper proceeds as follows. Section 2 introduces all preliminary concepts and results. Section 3 derives the results involving greater risk aversion, greater downside risk aversion and strongly greater downside risk aversion. Section 4 generalizes to ====th-order greater and strongly greater risk aversion. Section 5 studies compensated increase in risk, differential risk premia and stochastic dominance shifts. Lastly Section 6 concludes.",On the relationship between comparisons of risk aversion of different orders,https://www.sciencedirect.com/science/article/pii/S0304406822000854,22 August 2022,2022,Research Article,14.0
Morimoto Shuhei,"Faculty of Economics and Business Administration, Tokyo Metropolitan University, 1-1 Minami-Osawa, Hachioji, Tokyo 192-0397, Japan","Received 18 August 2021, Revised 30 March 2022, Accepted 3 August 2022, Available online 13 August 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102755,Cited by (0),"We study probabilistic voting rules in the case where agents have single-peaked preferences over alternatives. A probabilistic rule decides a probability distribution over the set of alternatives for each profile of agents’ preferences. In this paper, we characterize the class of group strategy-proof and peak-only probabilistic rules.","This paper studies non-manipulable probabilistic voting rules. Randomization is a widely accepted method to achieve a fair outcome. Agents have ordinal preferences over a finite set of alternatives. A ==== decides a probability distribution over the set of alternatives for each preference profile. ==== is a key axiom related to incentive compatibility. It provides each individual agent a strong incentive to reveal his own preferences truthfully. ==== is an even stronger axiom than strategy-proofness. It prevents strategic misrepresentations of preferences via ==== of agents as well as individual agents.====In voting contexts, one of the most widely studied preference domains is the domain of single-peaked preferences. In this paper, on the single-peaked domain, we study the class of probabilistic rules that satisfy the following notion of group strategy-proofness: whenever a group of agents jointly misrepresents their preferences, for at least one of them, the distribution chosen under truth-telling stochastically dominates the distribution chosen under joint misrepresentation.==== This is a stronger requirement than strategy-proofness based on stochastic dominance relations, which is an extensively studied concept in probabilistic environments.====As is well known, on the single-peaked domain, any strategy-proof ==== rule is also group strategy-proof.==== In contrast to the deterministic case, the equivalence between strategy-proofness and group strategy-proofness does not extend to the probabilistic case. Thus, in probabilistic environments, some strategy-proof rules may be vulnerable to strategic misrepresentations via groups. In our probabilistic setting, the class of group strategy-proof rules is much smaller than that of strategy-proof rules.====In our main result, we characterize the class of group strategy-proof and peak-only probabilistic rules (Theorem 1).==== We also characterize its subclass that additionally satisfies unanimity.====
 Ehlers et al. (2002) characterize the class of strategy-proof and peak-only probabilistic rules, which they call “fixed-probabilistic-ballots rules”. The class of rules we characterize in this paper is a subset of fixed-probabilistic-ballots rules. Our proofs also rely on their results.==== The fixed-probabilistic-ballots rules are a probabilistic version of Moulin’s (1980) well-known generalized median voter rules. Under the generalized median voter rules, a certain collection of alternatives associated with agents groups, called ballots, is fixed in advance, and then, the median of these prefixed ballots and agents’ peaks is chosen as a final outcome. In the probabilistic version, a certain collection of probability distributions over alternatives is considered fixed ballots instead. Under the fixed-probabilistic-ballots rules, these prefixed probabilistic ballots behave in a similar manner to the generalized median voter rules.====What kind of fixed-probabilistic-ballots rules are group strategy-proof? To examine this, consider the case where there are two agents and three alternatives, say, ====, ====, and ====. Assume that agents have single-peaked preferences over these three alternatives. First, consider a unanimous fixed-probabilistic-ballots rule that assigns positive probabilities to ==== alternatives ==== and ==== when the reported peak of one agent is ==== and another agent’s peak is ====.==== Now, assume that the true peak of one agent is equal to ==== and another’s true peak is equal to ====. Then, under this rule, because both alternatives ==== and ==== are chosen with positive probability, for each agent, the probability that the first- or second-ranked alternatives are chosen is less than one under truth-telling. However, if both agents jointly misreport their peaks as ====, because this rule is unanimous, alternative ==== is chosen with probability one. Thus, both agents can increase the probability that the first- or second-ranked alternatives are chosen by jointly misreporting their preferences. This is a violation of group strategy-proofness.====Next, consider a unanimous fixed-probabilistic-ballots rule that puts positive probabilities only on at most ==== alternatives when the reported peak of one agent is ==== and another’s is ====. Then, under this rule, even if both agents jointly misreport their peaks as ====, it is impossible for at least one of them to increase the probability that the first- or second-ranked alternatives are chosen.==== Therefore, under any group strategy-proof and unanimous rule, each associated probabilistic ballot must put positive probabilities only on at most ==== alternatives. In this case, we can describe each probabilistic ballot as a convex combination of the two degenerate distributions choosing these adjacent alternatives. This observation extends to the class of group strategy-proof and peak-only rules. Our result shows that any group strategy-proof and peak-only rule also shares a similar structure, i.e., each associated probabilistic ballot coincides with a convex combination of a certain pair of probability distributions.====Many authors have analyzed strategy-proof probabilistic voting rules on the single-peaked domain. As mentioned above, Ehlers et al. (2002) characterize the classes of strategy-proof probabilistic rules together with axioms such as peak-onlyness, anonymity, and unanimity. Peters et al., 2014, Pycia and Ünver, 2015, and Roy and Sadhukhan (2019) study extreme point characterizations. Peters et al. (2021) investigate strategy-proof and unanimous probabilistic rules on the domain of single-peaked preferences over graphs. Chatterji et al. (2022) characterize a restricted class of fixed-probabilistic-ballots rules by strategy-proofness and unanimity on domains, which are hybrids of the single-peaked and the unrestricted domains. Characterizations of single-peaked domains and extensions to multi-dimensional settings have also been studied by authors such as Dutta et al., 2002, Chatterji et al., 2016, and Chatterji and Zeng, 2018, Chatterji and Zeng, 2019.====By contrast, the literature on group strategy-proof probabilistic rules is still relatively small. Barberà (1979) characterizes the class of group strategy-proof probabilistic rules on the unrestricted domain.====
 Bogomolnaia et al. (2005) examine group strategy-proof rules on the domain of dichotomous preferences and obtain some impossibility results. However, the class of group strategy-proof probabilistic rules is still less well understood on many important domains. The present paper investigates group strategy-proof probabilistic rules on the single-peaked domain, which is one of the most important domains in the social choice literature.====Several authors have examined group strategy-proof rules in the random assignment and matching models. In the random assignment problem of non-identical indivisible goods, Bade (2016) shows that there is no group strategy-proof rule that satisfies ex-post efficiency and equal treatment of equals.====
 Bogomolnaia and Moulin (2004) construct a desirable group strategy-proof rule in the random matching model with dichotomous preferences.====In the multiple assignment problem of non-disposable identical indivisible goods, Hatsumi and Serizawa (2009) investigate group strategy-proof probabilistic rules defined over single-peaked and risk-averse utility functions.====This paper is organized as follows. In Section 2, we introduce the model and axioms. Our main result is provided in Section 3. Section 4 provides the proofs.",Group strategy-proof probabilistic voting with single-peaked preferences,https://www.sciencedirect.com/science/article/pii/S0304406822000842,13 August 2022,2022,Research Article,15.0
Tido Takeng Rodrigue,"CREM (CNRS 6211), Normandie University, UNICAEN, France,TEPP (CNRS 2042), France","Received 11 October 2021, Revised 27 June 2022, Accepted 9 July 2022, Available online 22 July 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102744,Cited by (1)," Myerson value and its individual rational revision called the ==== Myerson==== value. Furthermore, we generalize the core to this environment.","Pongou and Tondji (2018) recently introduced an uncertain production environment in order to address some problems observed in industrial organization. Following Pongou and Tondji’s paper (2018), an uncertain production environment for a finite set of inputs is given by a production function that assigns to each input profile a measurable output. An input supply is uncertain. Each input takes its values from a finite set of actions, and uncertainty is modeled as a probability distribution over this set. For instance, if an input is a worker, then the set of actions could be the set of hours of labor or effort levels a worker can put in. The set of actions for each input may represent a different set of objects and does not have to be an ordered set. It can be the set of job types available in the firm, where the jobs are not necessarily ranked.====An uncertain production environment generalizes the transferable-utility environment also known as a cooperative game with transferable utility (TU game). A TU game is described as a situation, in which inputs (called players in the framework of TU games) can obtain worth through cooperation. The payoff is distributed among the inputs by using one-point solutions. The Shapley (1953) value is probably the best-known efficient one-point solution concept for TU games.====Pongou and Tondji (2018) consider the problem of valuing inputs in a production environment, in which input supply is uncertain. They suggest an extension of the Shapley value as a solution to the valuation problem and call this solution “the ==== Shapley value”. Knowing the output level, they provided a new solution called “the Bayesian Shapley value”. An axiomatic characterization of these two solutions was proposed by using the Young (1985) approach.====Another important problem observed in industrial organization is the valuation of inputs in such an uncertain production environment, considering that some of these inputs can be involved in networks. In an uncertain production environment, there are no constraints on communication possibilities among inputs. There are some situations where the communication between inputs is restricted and not all groups of inputs can cooperate. Let us consider the following valuation problem taken from Pongou and Tondji (2018) and adapted to our context.====In this paper, we mean to address the following valuation problems set by Pongou and Tondji (2018).====If the suppliers are organized as shown in Fig. 1, then our valuation problem can be solved by using the Pongou and Tondji (2018) approach. However, if our suppliers are organized as shown in Fig. 2, then our valuation problem cannot be solved by using the Pongou and Tondji (2018) approach since there is a communication restriction among the suppliers.====Myerson (1977) introduced the communication structure, which can be described by an undirected graph with vertices as inputs and links as communication relationships. Myerson defined a communication game as a combination of a TU game and communication structure. The Myerson value for a communication situation is, therefore, the Shapley value of its communication game. Aside from Myerson (1977), more axiomatizations can be found in the literature (Myerson, 1980, Borm et al., 1992, Béal et al., 2015, etc.).====The Myerson value is generally not individually rational, i.e., there are cases in which the Myerson value of an input (payoff) is lower than its individual worth. Amer and Carreras (1995) revised the way to find the worth of a coalition in a communication game and then introduced an individual rational Myerson value for a communication game called the Myerson==== value. The Myerson==== value is an extension of the Myerson value. The two values coincide when the TU game is superadditive.====In TU games, we can observe that there are two participation levels: 0 and 1 (the set of actions for each input is 0 and 1). If we suppose that inputs have the possibility to cooperate with different participation levels, Aubin (1981) proposed the concept of a fuzzy coalition considering the uncertainty of inputs participation levels and introduced fuzzy cooperative games. All the inputs have the same set of participation levels and each participation level belongs to the bounded set ==== (input participation levels are a continuous variable). To calculate the worth of a fuzzy coalition (input profile of participation levels) in a game, it is necessary to consider a specific partition by levels of this fuzzy set. Two specific partitions by levels of fuzzy coalitions studied in the literature are the proportional model (Butnariu, 1980) and the Choquet model (Tsurumi et al., 2001). Genjiu et al. (2017) consider cooperative games on communication structure with fuzzy coalition. The Myerson value and its individual rational revision are defined, axiomatically characterized and discussed based on the Choquet integral form and proportional form, respectively. The core of fuzzy cooperative games has been studied by Aubin (1981), Tijs et al. (2004), Yu and Zhang (2009), and Genjiu et al. (2017).====In this paper, we study the problem of valuing inputs in an uncertain production environment with a communication structure. By using the Myerson (1977) and Genjiu et al. (2017) approaches, we generalize the solution proposed by Pongou and Tondji (2018) (the ==== Shapley value). We uniquely characterize this solution and call it the ==== Myerson value. Its individual rational revision, called the ==== Myerson==== value, is also uniquely characterized. Two intuitive axioms are used for the a priori Myerson value (and are adapted to the ==== Myerson==== value): probabilistic component efficiency and probabilistic fairness (respectively, optimal-probabilistic component efficiency and probabilistic fairness). Knowing the output level, we generalize the Bayesian Shapley value proposed by Pongou and Tondji (2018) and obtain a new solution called the Bayesian Myerson value. We uniquely characterize this solution by using two intuitive axioms: component efficiency ==== and fairness ====. Its individual revision, called the Bayesian Myerson====, is also characterized by using two axioms: optimal-component efficiency ==== and fairness ====. The notions of imputation and core are introduced in this new model.====Many authors have studied the generalization of the Shapley value to games in which players have multiple options. Among them we have Freixas (2005), Courtin et al. (2016), and Hsiao and Raghavan (1993). The Uncertain production environment introduced by Pongou and Tondji (2018) generalizes multiple options. We depart from Pongou and Tondji’s paper [2018] by incorporating the communication structure. By using the Myerson (1977) approach, we extend the Shapley value to that environment and propose an axiomatic characterization with two intuitive axioms. We also introduce the notion of core in this paper.====The rest of the paper is organized as follows. In Section 2, we present some preliminaries on uncertain production environments and communication games. We then introduce a new model called the uncertain production environment with communication structure. Section 3 studies the valuation problem ==== in a production environment with communication structure, and Section 4 deals with the ==== version. We close this section with the study of the non-emptiness of the core. Section 5 is devoted to discussions where we show how to adapt our results to the field of fuzzy cooperative game theory and obtain a notable result. Section 6 concludes, and all the proofs are presented in Appendix.",Uncertain production environment and communication structure,https://www.sciencedirect.com/science/article/pii/S0304406822000751,22 July 2022,2022,Research Article,16.0
Yang Jian,"Department of Management Science and Information Systems, Business School, Rutgers University, Newark, NJ 07102, United States of America","Received 24 February 2022, Revised 17 May 2022, Accepted 9 July 2022, Available online 21 July 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102746,Cited by (0),"We investigate a nonatomic game (NG) involving a random state of the world. Every player receives only a signal of the state’s realization. Not knowing the true state, the player would strive for a higher average payoff conditioned on her received signal. We demonstrate that equilibria exist for the game under reasonable conditions. Not only are they in existence, but these equilibrium points are also useful. When the state space is finite, such an equilibrium would help induce asymptotically equilibrium behaviors as ==== tends to ==== in ====-equilibria would likely emerge even though the NG equilibrium might well be mixed to start with. Pure versions of the latter would exist when the NG is anonymous.","We study a game with both nonatomic and incomplete- information elements. In the semi-anonymous convention of nonatomic games (NGs), a player’s characteristic ====, her own action ====, and the external environment in the manner of the joint player–action distribution ==== formed by other players, could all decide her payoff. On top of this, we suppose the latter is also influenced by a random state of the world ====. Thus, such a payoff function ==== and a prior distribution ==== on the player space are both standard features of the game. Its connection to traditional Bayesian games is through the fact that a player ==== does not actually observe the realized state ==== but rather just a signal ==== say ====.====In the game, players ==== might be commuters in a metropolitan area who have diverse traits in their origin–destination demands, departure- and arrival-related time windows, chronic health conditions, long-formed personal preferences, and so on; meanwhile, their actions ==== might represent transportation modes which could include hybrid ones such as “subway first and bus next”. Note action choices themselves do not have to enumerate all the myriads of possible ways that a commuter might impact the regional transportation system. Rather, it is the combination of one ==== which could contain very rich information and one fairly simple ==== that can map out a complex picture such as ==== taking the Q train from 72nd Street in Manhattan at around 8am to 42nd Street and then transferring to the M42 bus at around 8:30am to head for her work on the far west side. Certainly, all other commuters ==== along with their action choices ==== would impact the transportation system profoundly say propelling the Q train to a given congestion level during the 8-to-9am rush hour. This, when summarized into our ====, would in turn affect the current player ====’s comfort level as well as other personally-felt matters at any of her action choices ====.====The state ==== could be all-inclusive, covering global factors such as the S&P 500 index and the Brent crude oil price, local variables such as regional weather patterns and overall transit conditions, and all players’ recent pre-trip experiences such as sleep qualities and breakfast quantities. When comfort levels, delays, and expenses are all taken into account and monetized, we can model a commuter ====’s payoff under a state ====, her own transportation mode choice ====, and an external other-commuter-induced environment ==== by some ====. The formula ====, on the other hand, delineates how a complex state ==== would project onto an individual commuter ====’s mind some potentially simple and yet action-differentiating signal ====. For a carefree commuter, her signal variety might just barely distinguish between “a good day” and “a bad day”. For a more sophisticated one, there might be more than a few shades between “a perfect day” and “a terrible day”.====For a regional planner who strives for an aggregate understanding of commuter behaviors and traffic patterns, there should be hardly any need to reach the granularity level capable of telling apart individual commuters. It would thus be natural to model overall commuter characteristics by a distribution ====. A mixed strategy profile ==== for the resulting NG would comprise those ==== numbers where each is the chance for a commuter ==== who has received a signal ==== to choose a transportation mode ====. For ==== representing “subway first and bus next” and ==== “Uber all the way”, ==== and ==== would indicate that a commuter ==== when receiving her signal ==== would most likely choose public transport.====Under any realized state ====, which very likely is understood by most any single commuter only crudely, a commonly adopted mixed strategy profile ==== would result in a joint player–action distribution ====. It would impact all commuters’ well beings. For instance, if most people feel like a stroll when they are upbeat, then a commuter should better anticipate the sidewalks of midtown Manhattan to be packed on a sunny Spring morning when the futures markets indicate a 1% single-day surge in the S&P 500 index. That ==== is inserted as a whole into her payoff computation ====, thus presuming her own contribution to the environment as negligible, is one feature that distinguishes NG from its finite-player counterparts. As already stressed, the commuter does not know the actual state ==== rather just the signal ==== coming her way. When rational, she would favor a transportation mode ==== that promotes the average payoff ==== premised on a true state ==== whose random distribution accords with Bayesian learning conditioned on ====.====A mixed strategy profile ==== should clearly be considered an equilibrium when the average payoff under it is greater than that reachable by any single action ==== for any player ==== and signal ====. After going through Propositions 1 to 4 that cover properties including nonemptiness, convexity, compactness, measurability, and continuity, we can show in Theorem 1 the existence of such equilibria. Our NG setup ought to be justified by its helpfulness to more conventional and realistic finite-player Bayesian games. Indeed, Theorem 2 illustrates that as long as ==== is large enough, any NG equilibrium ==== would have a greater-than-==== chance to serve as an ====-equilibrium for an ====-player Bayesian game whose player characteristics are randomly sampled from the NG’s signature distribution ====. An important intermediate step is Proposition 5’s convergence of a random player–action empirical distribution in the finite game to its NG counterpart ==== in a certain probabilistic sense.====Furthermore, Theorem 3 conveys a stronger mixed-to-pure message. To the randomly generated finite Bayesian game, we could apply the pure-strategy profile where player ====’s signal-based action plan is sampled from ==== even while her own characteristic ==== has been sampled from ====. Again with a greater-than-==== chance, this profile would be an ====-equilibrium when ==== is large enough. Our derivation relies on Proposition 6, which links different weak convergences among probabilities. It also owes critically to the fact that players, even while receiving correlated signals, are autonomous entities capable of meting out independent decisions. When the NG is anonymous so that a player’s payoff is influenced by other players only through the marginal action distribution they form, we can indeed derive in Theorem 4 the existence of its own pure equilibria.====We have assumed a common signal space ==== for all players. Similarly to the widely accepted case for a common action space ====, this would not consign different players to the same set of messages. For a player ====, a certain ==== might signal the appearance of a rose while for a different player ====, the same might entail the tolling of a bell. Since our state-to-signal mapping ==== is allowed to vary with the player ====, the same signal ==== could portend differently for two players ==== and ==== in not only forms but also substances. Also, the cardinality ==== caters to only the maximum need among all players.====Although having much to improve in terms of model generalities, we have nevertheless succeeded in developing a Bayesian NG model with natural large finite counterparts in which players possess ==== messages: an ==== may among other things narrate a downpour in midtown Manhattan; consequently, the signals received by two neighbors ==== and ==== in the area may both discourage their respective receivers from walking. What we call signals were termed as types by a majority of the literature. While the characteristics ==== serve as players’ more innate features such as their long-formed personal preferences and the states ==== potentially reflect some particular and temporary situations such as stock market conditions and weather patterns, we believe “signal” is a more apt name in our context. When the distribution of signals happens to be independent and thus lending them a more private-type connotation, we shall occasionally revert to the name “type” when recounting the literature.====In the remainder, we survey past research in Section 2 and formulate the NG involving incomplete information in Section 3. The NG equilibria are defined in Section 4 and their existence is established in Section 5. For finite Bayesian games introduced in Section 6, we show the usefulness of NG equilibria to them in Section 7. This message is strengthened in Section 8 to one concerning a mixed-to-pure guarantee. When it is anonymous, the NG’s own pure equilibria are shown to exist in Section 9. We conclude the paper in Section 10.",A Bayesian nonatomic game and its applicability to finite-player situations,https://www.sciencedirect.com/science/article/pii/S0304406822000763,21 July 2022,2022,Research Article,17.0
"Lowing David,Techer Kevin","Univ Lyon, UJM Saint-Etienne, GATE UMR 5824, F-42023 Saint-Etienne, France,Gaz Réseau Distribution France, Direction Économie et Régulation, France","Received 3 November 2021, Revised 28 April 2022, Accepted 2 July 2022, Available online 14 July 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102740,Cited by (2),"This paper analyzes cooperation situations between heterogeneous players. It considers two types of heterogeneity. First, the players are differentiated with respect to a priority structure. This structure captures asymmetries between players, which may reflect exogenous rights, different needs, merit, or hierarchical constraints. Second, each player may have different cooperation possibilities represented by a set of activity levels. To analyze these situations, we enrich the model of multi-choice games, which is a natural extension of transferable utility games, with a priority structure. A new value on the class of multi-choice games with a priority structure is introduced. To accommodate the different activity levels and the asymmetries between players, this value follows an allocation process based on a lexicographic procedure. New axioms for multi-choice games with a priority structure are introduced. These axioms endogenously determine the lexicographic procedure used to define the value. Two axiomatic characterizations of this value are provided.","A situation in which players can obtain payoffs by cooperating can be described by a cooperative game with transferable utility (TU-game henceforth). To consider more realistic situations, Hsiao and Raghavan (1992) and van den Nouweland et al. (1995) introduce multi-choice cooperative games, which are a natural extension of TU-games. In a multi-choice game, each player has several activity levels at which it can cooperate. In this class of games, a coalition is a list describing each player’s activity level within this coalition. A characteristic function for multi-choice games measures the worth of each coalition. A (multi-choice) payoff vector assigns a payoff to each activity level of each player. Finally, a single-valued solution (a value for short) on a class of multi-choice games assigns a unique payoff vector to each game in this class.====In many situations, there exist asymmetries between players that are not captured by a standard multi-choice game. These asymmetries may reflect exogenous rights, different needs, merit, or hierarchical constraints that discriminate between players. In such settings, it would be desirable for a value to reflect these exogenous asymmetries between players. To this end, this paper enriches the model of multi-choice games with a priority structure. This structure, modeled by a partial order on the player set, captures the exogenous asymmetries among the players. In the TU-games framework, several studies have considered a partial order on the player set under different interpretations (see for instance Béal et al., 2022, Faigle and Kern, 1992 and Gilles et al. (1992)).====In the framework of multi-choice games, the use of a partial order on the player set brings additional possibilities that would not have been possible in the framework of TU-games. First, the set of activity levels is linearly ordered and provides an “intra-player” information. Then, the priority structure provides an “inter-player” information. Depending on the cooperative situation, these two pieces of information can be useful to define a relevant allocation process. These pieces of information are aggregated into a relationship on the set of pairs formed by a player and one of its activity levels. This relationship is then used to define a new value for multi-choice games with a priority structure. Below, an example is provided to illustrate our approach.====Consider a hospital that cares for several patients suffering from a disease. Suppose that the patients are partially ordered according to their comorbidities (respiratory failure, cardiac problems, etc.) through a priority structure. Moreover, suppose that not all patients are equally affected by the disease. To model this idea, each patient is endowed with a maximal severity degree. Finally, assume that the hospital is able to estimate the medical resources needed to treat any group of patients. The problem of the hospital is then to allocate the medical resources among the patients, taking into account both their comorbidities and their possible severity degree. A possible solution would be to consider an extension of the Priority value introduced by Béal et al. (2022) to multi-choice games. In the framework of TU-games, this value allocates the net surplus of a coalition, i.e., the Harsanyi dividend of a coalition, among the priority players in the coalition. A straightforward generalization of the Priority value to the above example amounts to favor patients with the higher comorbidities in the allocation process, regardless of their severity degree. As a result, medical resources may be consumed by a patient with a low severity degree. This may have a negative impact on seriously affected patients. Thus, it seems more reasonable to favor the patient(s) with the highest severity degree, regardless of its comorbidities. In case two patients have the same severity degree, one can decide to favor the patient with the most serious comorbidities.====To fit with this idea, the ordered set of the activity levels is combined with the priority structure in a lexicographic manner. This lexicographic order enables to compare pairs formed by a player and one of its activity levels in the following way. Consider any two players with a certain activity level, i.e., any two pairs. Similarly to the hospital example, the activity levels of the players are assumed to be the most significant criterion. If one of the two pairs has a higher position than the second in the lexicographic order, then it features a higher activity level than the second pair. If the two activity levels are equal, then the pair featuring the player that has priority over the second player has a higher position than the other pair in the lexicographic order. In line with this lexicographic order, a generalization of the Priority value from TU-games to multi-choice games is introduced: the ====. This value equally divides the net surplus of each coalition among the player(s) in the coalition with the highest position in the lexicographic order. Such allocation process especially makes sense when resources are scarce and trade-offs have to be made among different criteria. Interpreted within the above illustration, the patient with the most serious condition receives the net surplus of needed resources. If two patients have the same severity, then the one with the highest comorbidities receives the net surplus of needed resources.====Two axiomatic characterizations of the multi-choice Priority value are provided. Both characterizations rely on standard axioms for multi-choice games and specific axioms related to the priority structure.====Regarding the first characterization, the axioms invoked carry a relevant meaning in hospital resources allocation problem. The standard axioms are Efficiency, Independence of the maximal activity level and Independence of level reductions. The two axioms of independence relate to the variation in the payoffs when a player gets available a higher activity level. Independence of the maximal activity level requires that the resources allocated to a patient at a certain severity of illness do not depend on its highest severity degree. Thus, the excess needs created by an increase in severity should be allocated to this new severity degree. Independence of level reductions considers two patients with different severity conditions. If the condition of the patient with the more severe condition worsens, the amount of resources allocated to the other patient should not vary. This ensures that the less affected patient is still treated appropriately. Two other axioms that take into account the priority structure are introduced. The axiom of Priority relation for the same maximal activity level considers two players with the same maximal activity level and comparable in terms of priority. Regarding the hospital resources allocation problem, consider two patients with the same maximal severity degree, and such that one of the two patients has more significant comorbidities than the other. The axiom requires that the amount of resources allocated to the patient with less significant comorbidities should not vary whenever the condition of the other patient, that has more significant comorbidities, becomes slightly less severe. Then, the axiom of Balanced contributions for the same prevailing group is introduced.==== This axiom relies on a fairness requirement for indistinguishable players regarding both the priority structure and their maximal activity level. It indicates that such players affect each other payoff in the same manner. Regarding the hospital resources allocation problem, consider two indistinguishable patients in terms of severity degree and comorbidities. If the condition of one of the two patient gets worse, then the amount of resources allocated to the other patient may vary. This variation is the same no matter which patient’s condition worsens. As our first main result, we show that the multi-choice Priority value is characterized by the above list of axioms (Proposition 2).====The second characterization of the multi-choice Priority value is close to the one of the multi-choice Shapley value studied by Lowing and Techer (2022).==== It invokes Efficiency, Additivity, Independence of the maximal activity level, Independence of level reductions as standard axioms. In addition, it invokes the axiom of Independence of dummy level: removing a player’s maximal activity level does not alter the payoffs of the remaining player’s activity level if this player is unproductive in each coalition in which it plays its maximal activity level. Yet, the multi-choice Priority value departs from the multi-choice Shapley value in one important aspect. Contrary to the latter, it does not satisfy the axiom of Equal treatment for equal pairs: two players that contribute the same amount to each coalition in which they play the same activity level should receive the same payoff for this activity level. In the presence of exogenous asymmetry, this axiom becomes very strong. Therefore, it is replaced by two new axioms that take into account the priority structure. Both axioms deal with decisive players. A player is decisive if each coalition in which it does not play its maximal activity level generates zero worth. This notion generalizes the notion of a necessary player from TU-games to multi-choice games. Consider two decisive players with the same maximal activity level. First, the axiom of Priority relation for decisive players requires that the payoff of the first decisive player is zero if the second decisive player has priority over it. Second, the axiom of Equal treatment for decisive players with the same prevailing group deals with decisive players that are incomparable in terms of both maximal activity level and priority. It requires that such players should be treated equally for their maximal activity level. Therefore, this axiom is a relaxation of the axiom of Equal treatment for equal pairs. As our second main result, we show that the multi-choice Priority value is the unique value that satisfies this second set of axioms (Proposition 5).====Finally, we consider priority structures in which the set of players can be partitioned into several priority classes. Precisely, each class contains incomparable players that have priority over each player in the next class. In such priority relations structured by classes, we show that the multi-choice Priority value can be interpreted as a sequential procedure involving specific TU-games. Consider a given activity level that is played by at least one player, and a priority class ====. Consider all players in this priority class which are able to play the required activity level. These players obtain their contribution to the coalition formed by all players over which this class has priority (and that are able to play the required activity level). The final payoff of players in this priority class for the required activity level is the Shapley value applied to a TU-game on the subset of players in the priority class ====, which are able to play the required activity level (Proposition 6).",Priority relations and cooperation with multiple activity levels,https://www.sciencedirect.com/science/article/pii/S0304406822000738,14 July 2022,2022,Research Article,18.0
Wong Kit Pong,"University of Hong Kong, Hong Kong","Received 18 October 2021, Revised 21 May 2022, Accepted 29 May 2022, Available online 10 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102736,Cited by (0),"In this paper, we examine risk attitudes toward two risks within the set of bivariate utility functions that are risk averse to one risk in isolation and correlation averse (correlation loving). Specifically, correlation averters (correlation lovers) exhibit risk loving or risk aversion to one risk in the presence of another, depending on whether the riskiness of the former risk is below or above a positive threshold. The prevalence of such risk attitudes toward two risks uniquely describes the dependence structure of the two risks being characterized by the notion of negative (positive) expectation dependence. We then propose to use the positive threshold as an intensity measure of risk aversion with two risks, which is shown to be equivalent to the concept of generalized Ross risk aversion. We offer two applications regarding primary prevention for health risks and the motive for precautionary saving in the presence of wealth and interest rate risks, both of which involve random variables whose dependence structure is governed by the notion of expectation dependence.","Economists have long been fascinated by the pivotal role of risk aversion played in the decision-making process under uncertainty (Bernoulli, 1954). In the seminal work of Arrow (1965) and Pratt (1964), risk aversion is defined as the preference for the expected outcome of a risk with certainty over the risk itself. It then follows immediately from Jensen’s inequality that risk aversion is equivalent to the concavity of a univariate utility function within the framework of expected utility theory.====In reality, people often have to make decisions that involve multiple risks and multi-attribute preferences. Jensen’s inequality as such is no longer applicable to characterizing risk aversion with more than one risk, particularly when these risks are correlated (Pratt, 1988). Using a bivariate utility function, ====, defined over two risky attributes, ==== and ====, Finkelshtain et al. (1999) refer to ==== as exhibiting risk aversion to ==== in the presence of ==== if ====, where ==== is the expectation operator with respect to the joint distribution of ==== and ====.====
 Finkelshtain et al. (1999) show that ==== has this property for any ==== and ==== such that ==== is positively regression dependent on ==== in the sense of Lehmann (1966) if, and only if, ==== is risk averse to ==== in isolation and correlation averse (Richard, 1975, Eeckhoudt et al., 2007). For a differentiable utility function, this is the case if ==== and ==== for all relevant values of ====, where ====.==== Risk aversion to ==== in isolation gives rise to a risk effect that ensures the preference for ==== over ==== as a stand alone risk. Correlation aversion captures the common preference for combining good with bad so as to disaggregate losses on different attributes across states of nature (Eeckhoudt et al., 2007; Jokung, 2011). Correlation aversion as such gives rise to an apportionment effect that ensures the preference for negative correlation over positive correlation. The two effects reinforce each other when ==== and ==== are positively regression dependent in the sense of Lehmann (1966), thereby invoking risk aversion to ==== in the presence of ====. Li et al. (2016) extend the results of Finkelshtain et al. (1999) and show that the dependence structure of the two risks can be further weakened by using the notion of expectation dependence (Wright, 1987).====In a recent article of this Journal, Wong (2021) constructs an example wherein ==== and ==== are jointly normally distributed with a negative correlation coefficient and sufficiently small variances. Wong (2021) replaces ==== with ====, where ==== is a parameter that solely measures the riskiness of ==== since ==== for all ====. In this example, individuals who are correlation averse prefer ==== to ==== in the presence of ==== when ==== is below a positive threshold; otherwise, risk aversion to ==== in the presence of ==== prevails. The intuition for these findings is as follows. On the one hand, the risk effect that ensures the preference for ==== over ==== as a stand alone risk depends on the variance of ====, which is proportional to ====. On the other hand, the apportionment effect that ensures the preference for ==== over ==== in the presence of ==== depends on the negative covariance between ==== and ====, which is proportional to ====. Hence, there is a tradeoff between the two effects, where the apportionment effect dominates (is dominated by) the risk effect when ==== is sufficiently small (large) so that risk loving (risk aversion) behavior with two risks arises.====Wong (2021) proposes a stronger definition of risk aversion with two risks, which obeys the betweenness property of expected utility theory in the bivariate context. Specifically, Wong (2021) refers to ==== as exhibiting risk aversion to ==== in the presence of ==== for all ==== if ==== for all ==== so that the bivariate betweenness property of expected utility theory holds. This definition is more stringent than the one proposed by Finkelshtain et al. (1999) and Li et al. (2016) in that the former requires the inequality to hold for all ==== while the latter only at ====. Wong (2021) shows that all bivariate utility functions that are risk averse to ==== in isolation and correlation averse exhibit risk aversion to ==== in the presence of ==== for all ==== if, and only if, ==== is positively expectation dependent on ====.==== When ==== is infinitesimally small, the risk effect is negligible so that the preference for ==== over ==== in the presence of ==== is driven entirely by the apportionment effect. Hence, under the more stringent definition of risk aversion with two risks, no tradeoff between the risk effect and the apportionment effect is allowed, making the positive expectation dependence between the two risks not only a sufficient condition (Li et al., 2016) but also a necessary condition.====The purpose of this paper is to contribute to the extant literature on risk aversion with two risks by examining two missing ingredients that deserve a close scrutiny. First, the two risks involved may be negatively correlated as in the example constructed by Wong (2021). A real-life example is a portfolio that consists of an airline stock and an oil stock. The returns on these two stocks are by nature negatively correlated, making the return on the portfolio far less volatile. Another example is health savings accounts (HSAs). Savings via an HSA spent on medical expenditures are tax-exempt, whereas savings withdrawn for non-medical consumption are subject to a repayment of the tax advantage and an additional penalty, resulting in the return on saving via an HSA negatively correlated with the account holder’s health status (Peter et al., 2016, Courbage et al., 2022). Second, correlation loving, as characterized by ====, is by no means unreasonable. For example, a widely used assumption in health economics is that the marginal utility of wealth increases as health status improves (Bleichrodt et al., 2003, Liu, 2004, Fujii and Osaki, 2019), which is supported by ample empirical evidence (Viscusi and Evans, 1990, Carthy et al., 1998, Sloan et al., 1998, Finkelstein et al., 2013, Attema et al., 2019). Furthermore, correlation loving may naturally evolve as a consequence of the decision-making process. For example, consider the case that ==== is uncertain wealth and ==== is a random gross return (Dionne and Li, 2014) so that ==== and ==== are multiplicative financial risks and ====. As such, we have ==== for all relevant values of ==== if, and only if, the Arrow–Pratt measure of relative risk aversion, ====, is bounded from below (above) by unity for all ====. Hence, both correlation aversion and correlation loving are theoretically plausible in this example.====Unlike Wong (2021), we consider an arbitrarily chosen ==== and allow for either risk aversion or risk loving with two risks to prevail, resuming the potential tradeoff between the risk effect and the apportionment effect. We refer to those individuals who have bivariate utility functions that are risk averse to ==== in isolation and correlation averse (correlation loving) as correlation averters (correlation lovers). All correlation averters (correlation lovers) exhibit risk loving or risk aversion to ==== in the presence of ====, depending on whether ==== is below or above a positive threshold. We show that such risk attitudes toward two risks exist if, and only if, ==== is negatively (positively) expectation dependent on ====. When ==== and ==== are negatively (positively) correlated, the apportionment effect driven by correlation aversion (correlation loving) ensures the preference for ==== over ==== in the presence of ====, which counteracts the risk effect that ensures the preference for ==== over ==== as a stand alone risk. Since the apportionment effect increases linearly while the risk effect increases at an increasing rate when ==== increases, the former dominates (is dominated by) the latter should ==== be sufficiently small (large). Hence, we have risk loving (risk aversion) to ==== in the presence of ==== for all ==== below (above) a positive threshold. The prevalence of such risk attitudes toward ==== and ==== is equivalent to the dependence structure of ==== and ==== being uniquely described by the notion of expectation dependence, rendering expectation dependence to be the weakest dependence concept that is suitable for the characterization of risk attitudes toward two risks.====The results of this paper complement those of Wong (2021) in two ways.==== First, given correlation aversion as in Wong (2021), we derive the risk attitudes toward two risks whose dependence structure is described by the notion of negative expectation dependence. Second, given positive expectation dependence between the two risks as in Wong (2021), we derive the risk attitudes toward these two risks when preferences are characterized by correlation loving. In either case, the apportionment effect counteracts the risk effect so that the tradeoff between these two effects resumes, in contrast to the scenario analyzed by Wong (2021). As such, there is risk loving or risk aversion behavior with two risks, depending on whether the apportionment effect dominates or is dominated by the risk effect.====The rest of the paper is organized as follows. Section 2 defines risk attitudes toward two risks, and introduces the notion of expectation dependence. We then derive our main results that relate the risk loving or risk aversion behavior with two risks to the dependence structure of the two risks. Section 3 proposes an intensity measure of risk aversion with two risks that can be used for interpersonal comparisons. Section 4 offers two applications regarding primary prevention for health risks and the motive for precautionary saving in the presence of wealth and interest rate risks. The final section concludes.",Diversification and risk attitudes toward two risks,https://www.sciencedirect.com/science/article/pii/S0304406822000714,10 June 2022,2022,Research Article,19.0
"Béal Sylvain,Rémila Eric,Solal Philippe","CRESE EA3190, Univ. Bourgogne-Franche-Comté, F-25000 Besançon, France,Université de Saint-Etienne, CNRS UMR 5824 GATE Lyon Saint-Etienne, France","Received 3 September 2021, Revised 27 April 2022, Accepted 24 May 2022, Available online 8 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102738,Cited by (3),"A coalitional ranking describes a situation where a finite set of agents can form coalitions that are ranked according to a weak order. A social ranking solution on a domain of coalitional rankings assigns a social ranking, that is a weak order over the agent set, to each coalitional ranking of this domain. We introduce two lexicographic solutions for a variable population domain of coalitional rankings. These solutions are computed from the individual performance of the agents, then, when this performance criterion does not allow to decide between two agents, a collective performance criterion is applied to the coalitions of higher size. We provide parallel axiomatic characterizations of these two solutions.","In a large variety of social environments, a population of agents have the possibility to form coalitions in order to cooperate. For many real world applications however, it is not possible to evaluate precisely the worth of the coalitions (e.g. due to the lack of data, the complexity of the problem at hand, etc.). In this case, we can be satisfied with qualitative information on the power of these coalitions, expressed by a ====, which provides binary comparisons between coalitions. This binary relation is supposed to be a weak order, that is, it is a complete and transitive relation over the set of nonempty coalitions. Given this qualitative information on the power of the coalitions, the main objective is to design a social ranking/weak order over the agent set. A ==== on a class of coalitional rankings is defined as a mapping assigning to each coalitional ranking a unique social ranking over the agent set.====Social ranking solutions have been recently investigated by Khani et al. (2019), Bernardi et al. (2019) and Algaba et al. (2021). Khani et al. (2019) introduce and axiomatically characterize a social ranking solution that is inspired from the Banzhaf index for cooperative voting games (Banzhaf III, 1964). Bernardi et al. (2019) and Algaba et al. (2021) study lexicographic solutions based on the idea that the most influential agents are those who belong to (small) coalitions ranked in the highest position in the coalitional ranking, and provide several axiomatic characterizations of these solutions.====In this paper, we introduce two new lexicographic solutions for coalition rankings based on the individual and the collective performance of the agents in coalitions. First, the social ranking solution ==== ranks the agent according to the following procedure. If the individual performance of an agent is strictly better than the individual performance of another agent, then the rank of the former agent is strictly better than the one of the second agent. The ==== of an agent is evaluated by the rank of its singleton coalition in the coalitional ranking. In case the individual performance of two agents is identical, that is, if their respective singleton coalitions belong to the same equivalence class of the coalitional ranking, the procedure examines the performance of these agents in coalitions of size two to which they belong. This ==== is measured by the number of such coalitions of size two that are strictly better ranked than their singleton coalition. In this way, we measure the ability of an agent to cooperate more efficiently with another agent than it would do alone. Given two agents with the same individual performance, if the collective performance of one of these two agents is strictly better than the collective performance of the other agent, then our social ranking solution ranks the first agent ahead of the second agent. If these two agents have the same collective performance for coalitions of size two, then the procedure applies the same criterion to coalitions of size three, and so on for each coalition of higher size. Thus, the social ranking solution ==== proceeds lexicographically over the size of the coalitions to break the tie between two agents with identical individual performance.====A drawback of ==== is that it is insensitive to the quality of the performance of coalitions to which an agent may belong, provided this performance is strictly better than the individual performance of this agent. This point leads us to a second social ranking solution, denoted by ====, designed to correct this bias: if two agents have identical individual performance, then the coalitions of size two whose performance is strictly better than this individual performance are explored, starting with the best equivalence class. If the number of coalitions of size two containing the first agent is strictly greater than the number of coalitions of size two containing the second agent in the best equivalence class, then the first agent is ranked ahead of the second agent. Otherwise, that is, if these two numbers coincide, we move to the second best equivalence class and proceed in the same way. We continue to explore each equivalence class from the best one to the equivalence class preceding the one measuring the individual performance of these two agents. If the procedure does not allow to break the tie between these two agents, then the procedure continues by exploring the collective performance of coalitions of size three and so on. Thus, the social ranking solution ==== proceeds with a double lexicographical criterion: the criterion on the size of the coalitions is applied first (to the equivalence class preceding the one containing the singleton coalition), then the criterion on the index of the equivalence classes (from the best equivalence to the equivalence class of the singleton coalition) is used.====We provide comparable axiomatic characterizations of ==== and ====. To that end, several value judgments about social rankings solutions, expressed by the following list of principles, are introduced: a principle of neutrality, which indicates that the agents are impartially treated; a very weak principle of anonymity for coalitions, which indicates that only the size of the coalitions to which two agents may belong matters to rank these two agents; a principle which indicates that the positions of some coalitions are irrelevant to rank two agents; a principle of monotonicity which indicates how the ranking between two agents behave when the collective performance of one of the two agents change between two coalitional rankings; a principle of standardness for the two-agent case, which says that the first agent is better rank than the second agent if and only if the individual performance of the first agent is strictly better than the one of the second agent; and a principle of converse consistency, which allows to deduce the ranking of two agents in a group from the knowledge of the ranking of these agents for the associated reduced problems that some subgroups face.====Our work and the literature on coalitional ranking problems have numerous connections with other literatures and in particular with cooperative games with transferable utility (TU-games). Firstly, the social rankings that we are interested in are similar to the rankings that can be deduced from payoff vectors in TU-games, from richest to poorest. Thus, a social ranking solution can be viewed either as the ordinal counterpart of a value for TU-games or as the inverse problem of the well-known problem of ranking groups of objects from a ranking over the individual objects (see, e.g., Barberà et al., 2004).====Secondly, the two social ranking solutions that we introduce in this article have the same flavor as well-known and recent values for TU-games which mostly rely on the individual performance of the agents. For instance, the Center-of-gravity of the imputation-set (CIS) value (Driessen and Funaki, 1991) first assigns to each agent its stand-alone worth and distributes the remainder of the worth of the grand coalition equally among all agents. Other examples are the Proportional value (Moriarity, 1975, Zou et al., 2021a) and the family of Proportional surplus division values introduced recently by Zou et al. (2021c). The Proportional value distributes the worth of the grand coalition in proportion to the stand-alone worths of its members (Zou et al., 2021a). Each Proportional surplus division value assigns to each agent a compromise between its stand-alone worth and the average stand-alone worths over all agents, and then allocates the remaining worth among the agents in proportion to their stand-alone worth. For all these values, the ranking of agents in the population according to the payoffs they receive depends only on their individual performance, i.e., their stand-alone worth. Contrary to these aforementioned values, Zou et al. (2021b) design a new family of values, called the ====-mollified values, that not only adopt the proportional and equal division principles, but also take the worths of all coalitions into account. This is close in spirit to the principles behind our solutions ==== and ====, even if we use lexicographic criteria whereas the ====-mollified values are built from linear combinations.====Thirdly, our axioms are inspired by principles used in the axiomatic literature. Standardness is a well-accepted principle in TU-games which indicates that in two-agent TU-games, each one receives its stand-alone worth plus an equal share of the collective surplus (see, e.g., Hart and Mas Colell, 1989). Converse consistency is used to characterize solutions in several class of problems such as matching problems, fair division problems, cooperative and non-cooperative games. Our axiom of neutrality is the counterpart of the axiom of anonymity used in numerous contexts and especially in TU-games. Our axioms of monotonicity are in line with other axioms of monotonicity which describe how a solution is influenced by modifications of the worth of coalitions or of the marginal contributions of the agents (for TU-games, see Megiddo, 1974, Young, 1985, and van den Brink et al., 2013, among others).====The rest of the article is organized as follows. Section 2 introduces the main notation and definitions. Section 3 presents the axioms and some preliminary results. Section 4 is devoted to the axiomatic characterizations of ==== and ====. Section 5 introduces variants of our monotonicity axioms which enable to provide two alternative characterizations of both ==== and ==== respectively. These characterizations are based on two new versions of the principle of monotonicity. In particular, one of them allows to obtain a characterization of our solutions without the axioms of Converse Consistency and Standardness. Section 6 demonstrates the logical independence of the axioms used in the main characterization results.",Lexicographic solutions for coalitional rankings based on individual and collective performances,https://www.sciencedirect.com/science/article/pii/S0304406822000726,8 June 2022,2022,Research Article,20.0
"Afacan Mustafa Oğuz,Bó Inácio","Sabancı University, Faculty of Art and Social Sciences, Orhanli, 34956, Istanbul, Turkey,China Center for Behavioral Economics and Finance, Southwestern University of Finance and Economics, Chengdu, China","Received 16 September 2021, Revised 24 February 2022, Accepted 30 May 2022, Available online 7 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102734,Cited by (0),"We consider the allocation of indivisible objects when agents have preferences over their own allocations, but share the ownership of the resources to be distributed. Examples might include seats in public schools, faculty offices, and time slots in public tennis courts. Given an allocation, groups of agents who would prefer an alternative allocation might challenge it. An assignment is popular if it is not challenged by another one. By assuming that agents’ ability to challenge allocations can be represented by weighted votes, we characterize the conditions under which popular allocations might exist and when these can be implemented via strategy-proof mechanisms. Serial dictatorships that use orderings consistent with the agents’ weights are not only strategy-proof and Pareto efficient, but also popular, whenever these assignments exist. We also provide a new characterization for serial dictatorships as the only mechanisms that are popular, strategy-proof, non-wasteful, and satisfy a consistency condition.","Consider the problem of assigning a finite set of multi-unit indivisible objects to a finite set of agents who share a loosely defined “common ownership” of the objects, but only care about the object that is assigned to themselves. Examples of problems like these include the assignment of offices to faculty in a department, time slots in public tennis courts, or seats in public schools.==== In these settings, standard properties such as Pareto efficiency or the core might not correctly capture the nature of the conflict created by these preferences combined with the shared ownership of the objects.====Take Pareto efficiency. An allocation of objects to agents is Pareto efficient if there is no alternative allocation in which no agent is worse off and at least one is strictly better off. When this is the objective of the designer, there are many mechanisms with good outcome and incentive properties available (Shapley and Scarf, 1974, Abdulkadiroğlu and Sönmez, 2003, Pycia and Ünver, 2017). Many allocations, however, are Pareto efficient because to improve the allocation of many agents it would be necessary to make one or a few agents worse off.==== When all the agents share the ownership of the objects, however, such allocations could be challenged by those who would be better-off under an alternative allocation. The concept of ==== considers these issues. A matching ==== is ==== than matching ==== if the number of agents who strictly prefer their assignments under ==== to those under ==== is larger than the number of agents who strictly prefer their assignments under ==== to those under ====. Here we would say that ==== is challenged by ====. A matching is ==== if there is no matching that is more popular than it. Notice that, similarly to the test of whether an allocation is in the core, we test whether coalitions of agents could reallocate the objects that they own in a way that makes them better off. Differently from the core, however, agents are not endowed with part of the objects to be allocated. Moreover, we consider not only whether the agents in the coalition would be better off, but also how many outside of that coalition would be worse off. In our setup, therefore, no subset of agents has an endowment of some set of objects to reallocate between themselves, and every agent has, to some extent, a say on the allocation of all objects.====When considering the standard notion of popularity, we hit strong negative results. Popular matchings might not exist (Corollary 1)—a previously known fact—and there is no mechanism that is strategy-proof and produces a popular matching when one exists, regardless of the allocations that it produces when those do not exist (Corollary 2). In other words, even if we ignore the fact that popular matchings might not exist, there is no “good” method of implementing them when they do exist.====We then use a more general notion of popularity, that allows agents to have different weights. A matching ==== is ==== ==== than matching ==== if the sum of the weights of the agents who strictly prefer their assignments under ==== to those under ==== is larger than that of the agents who strictly prefer their assignments under ==== to those under ====. Group decision-making involving weighted voting similar to the one implied by this notion of popularity is common. Examples include stockholder voting in corporations, and elections for university deans (Lucas, 1983). Moreover, in many real-life environments, the idea that participants do not have the same ability to influence the allocation is natural: senior faculty have a bigger influence than juniors, newcomers have less influence than veterans, etc. Even with this more general model, however, the existence of a ====-popular matching is hard to guarantee: unless the weights are ====, a very restrictive condition, ====-popular matchings might still not exist (Proposition 1).====Our approach, therefore, will be to require maximum compliance with our objective. We say that a mechanism is ====-popular if it produces ====-popular allocations ====. The space of ====-popular mechanisms includes every possible combination of allocations for problems in which a ====-popular matching does not exist. For example, every mechanism that produces a ====-popular matching when one exists and satisfies any “second-best” property when one does not exist, is ====-popular.====Although the space of ====-popular mechanisms is extensive, we obtain a series of well-defined results. We characterize the profiles of weights of the agents that allow for ====-popular and strategy-proof mechanisms to exist and show that when they do, serial dictatorships (SDs) in which the order of the agents is consistent with their weights are ====-popular and strategy-proof. In addition, the set of outcomes of SDs consistent with the weights fully characterizes the set of ====-popular allocations (Theorem 1). Since SDs are Pareto efficient, these mechanisms are as well. We show that these profiles of weights also characterize the scenarios in which ====-popular matchings can be implemented in Nash equilibrium (Theorem 3).====When the profile of weights allows for ====-popular and strategy-proof mechanisms to exist, we obtain a characterization for SD: a mechanism is strategy-proof, non-wasteful, ====-popular and preserves dispute resolutions (a weak consistency requirement) if and only if it is an SD consistent with the weights of the participants (Theorem 2).====These results provide, therefore, a new case for the use of SDs in problems of allocation of objects with common ownership. Whenever the problem involves weights for the participants, either by its nature or by design, and if these weights allow for a strategy-proof and ====-popular mechanism to exist, then SDs are not only strategy-proof and Pareto efficient but also ====-popular. Any other mechanism that is also strategy-proof and ====-popular would have undesirable inconsistency or wasteful properties.",Strategy-proof popular mechanisms,https://www.sciencedirect.com/science/article/pii/S0304406822000702,7 June 2022,2022,Research Article,21.0
"Huremović Kenan,Ozkes Ali I.","IMT School for Advanced Studies Lucca, Piazza S. Francesco, 19, 55100 Lucca, Italy,Léonard de Vinci Pôle Universitaire, Research Center, 92916, Paris La Défense, France","Received 16 November 2021, Revised 10 April 2022, Accepted 17 May 2022, Available online 6 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102732,Cited by (0),"We introduce a model of polarization in networks as a unifying setting for the measurement of polarization that covers a wide range of applications. We consider a substantially general setup for this purpose: node- and edge-weighted, undirected, and connected networks. We generalize the axiomatic characterization of Esteban and Ray (1994) and show that only a particular instance within this class can be used justifiably to measure polarization in networks.","Polarization in a population denotes an intensified disconnect among its groups. The analysis of the sources and the consequences of polarization depends highly on what is measured and how, which, in turn, is strictly contingent on the particular context. For instance, while in the context of American politics polarization is perceived as the division of masses into the cultural camps of liberals and conservatives, in the context of European multi-party parliaments, it is seen as the existence of ideologically cohesive and distinct party blocks.==== So even the term “political polarization” is not indicative of what is being measured and how. Existing literature reflects this complexity, and there is an abundance of measures without a unified formalism that applies to comparable contexts.====Although there are substantial differences among existing measures across fields, one ubiquitous feature can be identified. Namely, most of the current measures are proposed in settings with a uni-dimensional scalar attribute on which the polarization is assumed to occur. However, conflicts in societies are in general related to an irreducibly complex set of attributes and most of the empirical work rely on categorical data on various characteristics.==== Dimensionality reduction approaches are called for in many instances, because the existing polarization measures allow for only a uni-dimensional, or at most a bi-dimensional domain (Hill and Tausanovitch, 2015). However, reduced dimensions can be questionable for their capacity to represent the actual phenomenon of interest (Kam et al., 2017).====In this paper, we propose the formalism of network theory to study the measurement of polarization as it delivers the desired generality and spans a large variety of contexts. We fully characterize a polarization measure following the axiomatic setting introduced by Esteban and Ray (1994) (henceforth ER) for distributions on the real line. Same as ER, we restrict ourselves to distributions with finite support.====Our setup is built on undirected networks in which both nodes and links are weighted. A node in the network represents a certain attribute or grouping of individuals in the population. The weight of a node corresponds to the number of individuals in the population that are characterized by the attribute or members of the group (==== a political party). The weighted links describe (direct) bilateral relationships between nodes. This setup is quite general and can represent a wide range of settings in which measuring polarization is an issue of first-order importance. We describe a number of important examples in the next section, with a particular focus on the political domain, not only because it is a central point of discussion, but also because it comprises of a variety of aspects that can be captured distinctly within network formalism. For instance, we show how elite polarization can be modeled within our framework through networks of politicians, parties, or policy space. Mass polarization, on the other hand, can be modeled through a network of opinions or preferences. Going beyond the political domain, we furthermore discuss how our approach can be used to study polarization in any setting with multidimensional distributions with finite support.====The axiomatic approach developed by ER for distributions with finite support on the real line led to the development of measures in several other domains, such as measures for continuous distributions as in Duclos et al. (2004) and measures for binary classifications as in Montalvo and Reynal-Querol (2008) (henceforth MRQ). Most of the applications employing measures within this line of work lie in the fields of income inequality and social conflicts. In their seminal contribution in this context, ER conceptualize polarization as the aggregate antagonism in a population. The effective antagonism an individual feels against another depends on how alienated she feels from the other’s group and how identified she feels with her own group. According to ER, a population in which individuals are identified within groups is polarized if there is a high level of intra-group homogeneity, a high level of inter-group heterogeneity, and a small number of large-enough groups. They deliver a characterization of a class of polarization measures, based on an axiomatization built around distributional properties and not confined to incomes or wealth, although the main motivations of ER were about income and wealth distributions.====Following ER, we provide an axiomatic characterization for measures of network polarization. We argue that networks represent a powerful tool to capture any distribution with a finite support and a notion of distance. Thus, the strength of our contribution lies in the fact that we deliver an axiomatic foundation for a family of measures that are applicable in a significantly larger set of domains. Furthermore, as any distribution considered in ER or MRQ can be represented as a network, our work can be seen as a unifying generalization, with ER and MRQ as special cases.====The class of measures characterized by ER is identified by the range of values that parameter ====, which captures the importance of identification in the effective antagonism, can take. Our first result shows, quite surprisingly, that this class is thinned down by a unique value, ==== ====
 (Theorem 1).==== Note that adaptations of these axioms are neither trivial nor straightforward, as networks allow for a much larger generality in representing discrete distributions than the real line. Recent literature on the measurement of polarization carried along the restricting assumption that the attributes can be captured by the values of a scalar variable. We take off where ER leave, and deliver an analysis that does not “sweep a serious dimensionality issue under the rug” (ER, p. 823). Our approach accommodates a significantly larger variety of settings that are not confined to scalar attributes, and naturally include the case of the Euclidean distance on the real line as a special case. This entails a solution to an unresolved issue in this line of research as a by-product, in that our results point to the choice of an exact value within the interval ====.====It is desirable that polarization measures attain their maximum at the symmetric bipolar distribution. Contrary to the real intervals, in networks there can be any finite number of nodes with maximal distance between them. Still, we show that any measure within the family we characterize is maximized at the symmetric bipolar distribution — when the population is symmetrically distributed among the ==== most distant nodes in the network (Proposition 1).====Finally, we show that if we restrict our attention to particular classes of networks emerging in certain domains such as income distributions (class of line networks), one of the axioms, Axiom 3, can be weakened in a systematic way to allow for a wider class of measures that can be used consistently (Theorem 2). For instance, in the special case of line networks that can be used to represent income distributions, our set of axioms and the class of measures reduce to the ones in ER.====It presents a challenge to pay a fair tribute to the ever-growing literature on the measurement of polarization. Here, we refer to a set of papers in different domains and discuss a few closely related ones. We mention several other works in Section 5.====Polarization is studied in social sciences (particularly in economics and political science) in relation to economic inequality (Esteban et al., 2007, Esteban and Ray, 2012, Zhang and Kanbur, 2001), social conflict (Desmet et al., 2017, Montalvo and Reynal-Querol, 2008, Østby, 2008), political economy (Aghion et al., 2004, Desmet et al., 2012, Lindqvist and Östling, 2010), international relations (Maoz, 2006b), political ideologies (Abramowitz and Saunders, 2008, Fiorina and Abrams, 2008, Lelkes, 2016, Martin and Yurukoglu, 2017), political sentiments (Boxell et al., 2017, Garcia et al., 2015), and social attitudes (DiMaggio et al., 1996, Lee et al., 2014, McCright and Dunlap, 2011), among others. There are also several related indices discussed in the literature other than polarization such as diversity (Nehring and Puppe, 2002), ethnic stratification (Hodler et al., 2020), fractionalization (Bossert et al., 2011), and group dependent deprivation (Bossert et al., 2022 based on (Runciman, 1966) and that also includes ER as a special case), among others.====We want to emphasize that we are not the first to consider an ER-type approach to the measurement of polarization in networks. For instance, both Esteban and Ray (1999) and Esteban and Ray (2011) explore this issue. However, to the best of our knowledge, this is the first paper to provide an axiomatic characterization for measures of polarization in networks.====
 Fowler, 2006a, Fowler, 2006b and Maoz (2006b) are among the leading examples where network formalism is proposed for the measurement of polarization, without an axiomatic treatment.==== Finally, Permanyer and D’Ambrosio (2015) characterize a distinct family of measures for categorical attributes by using identification–alienation framework and a number of additional axioms.====The ER polarization index is often used in applied work, in some cases (for instance Aghion et al., 2004, Alesina et al., 2003, Collier and Hoeffler, 2004, Desmet et al., 2009 and Dower et al., 2017) with data that cannot be represented as a distribution on the real line, but can be represented as a network. Our paper provides a justification to use the ER measure with ==== in such cases, as done in Desmet et al. (2009) and Dower et al. (2017), but also suggests that using values of ==== that are different than 1 (as in Alesina et al., 2003, Aghion et al., 2004, and Collier and Hoeffler, 2004) may not be appropriate.====The rest of the paper is organized as follows. In Section 2 we describe the environment we study and illustrate the wide applicability of our approach. In Section 3 we define polarization, state the axioms, and deliver our major results. In Section 4 we discuss the importance of network structure in terms of polarization and formally illustrate the connection between our work and previous literature. We conclude in Section 5.",Polarization in networks: Identification–alienation framework,https://www.sciencedirect.com/science/article/pii/S0304406822000696,6 June 2022,2022,Research Article,22.0
Han Seungjin,"Department of Economics, McMaster University, 1280 Main Street West, Hamilton, ON, Canada","Received 18 August 2021, Revised 22 April 2022, Accepted 17 May 2022, Available online 2 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102728,Cited by (1),". The equilibrium ratio of the number of buyers to the number of sellers is lowest at the monopoly price, and increases as the price moves farther away from it in either direction.","Buyers are informed about terms of trade or prices offered by sellers in the market as they search for a better deal. Sellers often gather market information from buyers to determine his terms of trade in addition to eliciting their willingness to pay (i.e., payoff type). This is a prominent feature of a seller’s web design in on-line markets. On-line sellers can keep track of buyers’ search history based on html cookies, most of which are based on simple binary messages: Whether or not a buyer revisits a seller’s web site, whether or not a buyer clicks a certain part of a seller’s web site, etc. This type of information can reveal what buyers know about competing sellers’ terms of trade (Board and Lu, 2018, Peters, 2015). Despite the prevalent use of buyers’ search behavior or their market information in practice, it is not easy to develop a tractable competition theory that reflects it, due to the potential complexity of market information (Epstein and Peters, 1999, Yamashita, 2010).====This paper introduces a ==== of incomplete information. In this game, buyers’ payoff types are their private information and each buyer has to choose only one seller for trading as in the literature on competing auctions (e.g., McAfee, 1993, Peters and Severinov, 1997, Virág, 2010, etc.). Sellers choose only reserve prices for their auctions in competing auctions and participating buyers are restricted to submit their bids. However, our game is more general in terms of applications, and importantly it can incorporate buyers’ on-line search behavior, allowing buyers’ communication to take place over two rounds.====Given a profile of contracts offered by sellers, each buyer privately sends a first-round message to each seller.==== Each seller’s contract specifies a menu of mechanisms conditional on all buyers’ first-round messages and then the seller chooses a mechanism from the menu. The messages in the first round are similar to information that buyers leave (through html cookies) on sellers’ web sites they visit. In this round, buyers do not need to select a seller. Sellers’ web sites are then responsive in the sense that they can change a selling mechanism conditional on what are left in html cookies (e.g., the reserve price in an auction can be responsive to information in html cookies).====In the second round, after observing sellers’ mechanisms, each buyer privately sends a message only to a seller whom she selects (e.g., buyers submit their bids only to a seller’s auction site they select after search). A seller’s mechanism then specifies a profile of probabilities of action alternatives and monetary transfers to participating buyers conditional on their second-round messages. The nature of a seller’s action can be general, and different depending on applications such as selling a private good with or without a capacity constraint, selling an (excludable) public good, etc.====Buyers adopt symmetric selection behavior in the sense that a buyer chooses one of the sellers with the same contracts with equal probability.==== Therefore, sellers with the same contracts may end up with a different number of buyers, which can be thought of as endogenous frictions in the market. This paper does not attempt to characterize the set of all possible equilibrium allocations. Instead, it characterizes the set of equilibrium allocations that can be sustained when non-deviating sellers always choose dominant-strategy incentive compatible direct mechanisms when buyers’ first-round messages reveal a deviation by a competing seller.====Dominant-strategy incentive compatibility of a direct mechanism is appealing particularly in a model where a buyer selects one seller for trading. It is very hard to work with Bayesian incentive compatible (BIC) direct mechanisms because Bayesian incentive compatibility is based on buyers’ interim payoffs, which depends on buyers’ selection strategies in a continuation equilibrium given a profile of mechanisms chosen by sellers. This implies that Bayesian incentive compatibility of a seller’s direct mechanism depends on the mechanisms that all sellers choose. On the other hand, the dominant strategy incentive compatibility of a direct mechanism can be defined without reference to buyers’ selection strategies or other sellers’ mechanisms. For that reason, we focus on the set of equilibria where non-deviating sellers punish a deviating seller with dominant strategy incentive compatible (DIC) direct mechanisms. We call it a ====.====We can show that in such equilibria, the lower bound of each seller ====’s equilibrium payoff is his minmax value where the min is taken over the other sellers’ DIC direct mechanisms and the max is taken over ====’s BIC direct mechanisms conditional on the others’ DIC direct mechanisms. However, one may wonder if and when we can restrict a deviating seller’s deviation to DIC direct mechanisms without loss of generality. If buyers’ types are one-dimensional, private and independent and payoff functions are linear, we can extend the BIC-DIC equivalence in Gershkov et al. (2013) to show that any BIC direct mechanism induced by a pair of (i) a deviating seller’s mechanism and (ii) buyers’ communication strategies can be replaced by a payoff-preserving DIC direct mechanism in a given continuation equilibrium. Using this BIC-DIC equivalence, we establish that every seller can restrict himself to offer a contract that specifies a menu of DIC direct mechanisms conditional on buyers’ messages on and off the equilibrium path without loss of generality, even if a seller can offer a contract that assigns a set of any arbitrary mechanisms, not just DIC direct mechanisms. Therefore, the lower bound of each principal’s equilibrium utility is his minmax value only over DIC direct mechanisms. We also show that the first-round messages in the non-deviators’ equilibrium contracts can be as simple as binary (i.e., whether or not there is a deviating seller). As in Yamashita (2010), agents can truthfully reveal whether or not there is a deviating seller when there are three or more agents because a non-deviating seller can take the same report from a strict majority of agents as the true information about deviating sellers.====This paper considers two applications where sellers can each produce a unit of a homogeneous private good at a constant marginal cost and each buyer has a unit demand with private valuation: one without a seller’s capacity constraint and the other with. We first consider the case where sellers have no capacity constraint. Non-deviating sellers can lower their price down to their marginal cost upon a competing seller’s deviation reported by buyers in the first around of communication. This implies that a deviating seller must incur loss (i.e., lower his price below the marginal cost) in order to attract buyers upon deviation to any arbitrary mechanism. Therefore, the lower bound of a seller’s equilibrium profit can be as low as a seller’s reservation profit. If the hazard rate of the buyer’s valuation is non-decreasing, the upper bound of a seller’s equilibrium profit is reached at the monopoly price, i.e., the buyer’s valuation that makes her virtual valuation equal to zero. This analysis is based on a fixed number of buyers and sellers.====However, sellers may freely enter the market by incurring a fixed entry cost ====. A seller has a belief on the trading price ==== in the market. Given the number of buyers ====, sellers will keep entering the market as long as they can earn non-negative ex-ante expected profit net of the fixed entry cost. Given ==== and ====, the equilibrium number of sellers ==== is determined at the point where one additional seller in the market yields a negative ex-ante expected profit net of the fixed entry cost. Let a buyer’s valuation follow a probability distribution ====. Each seller’s equilibrium ex-ante expected net profit is an equal split of the total profit that a single seller could earn if he had sold the product to buyers at price ==== alone - ====. Further, there is a range of equilibrium prices.====In a finite market, each seller’s equilibrium ex-ante expected net profit ==== can be positive. However, as ====, the equilibrium ratio of the number of buyers to the number of sellers makes each seller’s equilibrium ex-ante expected net profit converges to zero. Despite this, the multiplicity of equilibrium prices does not go away even in a large market. One interesting equilibrium feature is that in both finite and large markets, the largest number of sellers in the market occurs when the trading price is equal to the monopoly price, and the number of sellers in the market decreases as the trading price moves farther away from the monopoly price in either direction.====Finally, we show that this large market result in the case with no capacity constraint holds even when each seller faces a capacity constraint. As in a canonical environment for competing auctions (Burguet and Sákovics, 1999, Han, 2015, McAfee, 1993, Peters and Severinov, 1997, Peters, 1997, Virág, 2010), we consider a large market where the number of buyers is taken to infinity with a fixed ratio of buyers to sellers. There is a range of equilibrium reserve prices. Given an equilibrium reserve price, the equilibrium ratio of the number of buyers to the number of sellers is determined at the point where a seller’s ex-ante expected net profit is equal to zero. The equilibrium ratio of the number of buyers to sellers is lowest when the reserve price is equal to the monopoly reserve price and increases as the reserve price moves farther away from the monopoly price.",General competing mechanism games with strategy-proof punishment,https://www.sciencedirect.com/science/article/pii/S0304406822000672,2 June 2022,2022,Research Article,23.0
Yu Zhixian,"University of Nottingham Ningbo China, Business School, 199 Taikang East Road, Ningbo, 315100, China","Received 9 May 2021, Revised 27 April 2022, Accepted 19 May 2022, Available online 2 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102724,Cited by (0),"We generalise Admati and Perry (1991)’s two-player, alternating contributions model, allowing participants to be asymmetric from two dimensions: 1. one of the players is the deadline player and 2. players receive different rewards on completing the project. In equilibrium, the project either ends in the first few stages or at the deadline. A slight change in the environment can influence the ending time dramatically. Based on that, we discuss the appropriate rewarding distribution.","This paper studies the following problem. Suppose there is a joint project developed between two agents, and one of the agents is responsible for the ultimate submission. How should the rewards from the project be shared? Which agent should receive a higher share?==== For that purpose, we extend the contribution game model of Admati and Perry (1991), where two agents take turns to make non-refundable, non-contractible contributions to a joint project. Compared to the original model, ours has two different features: (i) unequal valuations, i.e., players value the project differently because they can be rewarded differently on the completion, and (ii) finite horizon, i.e., there is a pre-determined deadline for the completion of the project. Research has been conducted to study (i) and (ii) separately, but not jointly (Table 1).====The two key features of our model introduce two novel forces:====1. Value effect: the participant rewarded better on the completion (in other words, has a higher valuation) suffers from a greater loss with time passes and is therefore more motivated to contribute.====2. Deadline effect: with no follow-up, the ultimate responsible person (the deadline contributor) has full incentives to contribute to the project at the deadline. Foreseeing this, the non-deadline contributor has the incentive to delay the contribution to the pen-ultimate stage, makes the smallest contribution with which the deadline contributor has an incentive to complete the project in the last stage. Anticipating this, the deadline contributor is more motivated to contribute.====The following paragraphs introduce the research in Table 1 and interpret their main results under none or one of the two effects.====Admati and Perry (1991) set up the basic structure of the contribution game: two participants complete the project when their total effort reaches a certain level, then they each earn a payment from the project. In their model, the project has no deadline and the payments are equal, making the two participants symmetric. In equilibrium, no one makes any effort unless they are able to complete the project alone. As a result, a project worth completing (total revenue exceeds total cost) may end up undone. The intuition is that because the symmetric players have the same free-riding power, whenever one plans to make a contribution, he/she expects the other to contribute instead. As a result, no one takes the first step.====Compte and Jehiel (2003) assume unequal payments in their model, showing that a project worth completing ends in an early stage and the high valuation player earns a zero payoff in equilibrium. Marx and Matthews (2000) study a simultaneous contribution game, discussing the finite horizon version of Admati and Perry (1991) in their Appendix C. They generate a similar result to Compte and Jehiel (2003) that the deadline contributor behaves like the high valuation player when the discount factor approaches 1. This paper considers aforementioned two dimensions of asymmetry simultaneously. The connection between our research with the existing ones will be discussed in Sections 4.1 Relationship with, 4.2 Relationship with.====Contribution games have been explored from various perspectives. Observability of efforts is explored in continuous effort models by Fershtman and Nitzan (1991) and Kessing (2007), to learn whether the players’ efforts are strategic complements or substitutes. Strategic experimentation models have been introduced to explain the procrastination in joint projects in studies from Bonatti and Hörner, 2011, Campbell et al., 2014 and Georgiadis (2017). The model in this paper stands apart from above mentioned literature in that it does not have incomplete information or uncertainty.====Although there are some similarities between this study and the works from Nishikawa (2015), Tajika (2020), and Itaya et al. (2018), the difference is more significant. Nishikawa (2015) adopts an N-player, randomising protocol and Tajika (2020) allows only a one-shot contribution. Itaya et al. (2018) use the Hamilton and Slutsky (1990) type model in which players decide their moving sequence in period one and make their contributions in period two.====This paper contributes from three aspects: (i) It is a generalisation of existing literature. This paper thoroughly explores the free-riding behaviour in joint projects, the model can easily converge to Compte and Jehiel (2003), Marx and Matthews (2000) Appendix C (thereafter MMC) or Admati and Perry (1991). (ii) In this model, players either make their contributions immediately or reserve them until around the deadline. Furthermore, the ending time of the project leaps when the environment changes slightly. This threshold property has not been studied before. (iii) This model studies the ratio of players’ valuations in a range, instead of at a certain point. Based on it, the reward distribution is discussed.====This paper is organised as follows. Section 2 introduces the generalised model; Section 3 derives and analyses the equilibrium; Section 4 provides extensions with other models and Section 5 makes conclusion. All the proofs are in the Appendix A Proof of, Appendix B Proof of, Appendix C Selection of, Appendix D Proof of, Appendix D Proof of.",Contribution games with asymmetric agents,https://www.sciencedirect.com/science/article/pii/S0304406822000659,2 June 2022,2022,Research Article,24.0
"Maslov Alexander,Schwartz Jesse A.","Owen Graduate School of Management, Vanderbilt University, United States of America,Department of Economics, Finance and QA, Kennesaw State University, United States of America","Received 1 February 2022, Revised 19 April 2022, Accepted 17 May 2022, Available online 2 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102730,Cited by (2),"We study online markets where two sellers sequentially choose reserve prices and then hold ascending auctions. Buyers can bid in both auctions and can switch between them as frequently as they like. We adapt the revenue equivalence approach of Myerson (1981) to obtain total revenue generated by the buyers and the split of this revenue between the sellers. We find a unique (====-perfect) equilibrium outcome, in which the first seller to choose a reserve price enjoys a first-mover advantage, maintained by selecting a price low enough to discourage the second seller from undercutting this price. Both prices are above the sellers’ marginal costs, but below what a monopolist would set. Our results contrast with Burguet and Sákovics (1999), in which the sellers simultaneously choose reserve prices for separate, traditional sealed-bid auctions in which each buyer can bid in a single auction only. They do not find a unique equilibrium outcome (due to mixed strategies) and they only partially characterize equilibrium payoffs.","E-commerce has substantially transformed ordinary retail markets. It has also influenced the evolution of selling mechanisms and their contextual applications. Sealed-bid auctions were prevalent before the advent of the Internet, but have lost their popularity due to a drastic improvement in the communication technologies and reduction of search costs for buyers.====The sparse theoretical and empirical literature on competing online auctions investigates the behavior of buyers or final purchase prices. We, on the other hand, are interested in how competing sellers choose their reserve prices. To analyze this problem, we show how the revenue equivalence approach of Myerson (1981) can not only be used to give the total revenue generated by the buyers, but can also be used to capture how this revenue is split between the competing sellers.====Using this new approach, we show that the first-arriving seller chooses a reserve price just low enough to safeguard against being undercut by the second seller, and in the equilibrium the first-arriving seller makes more profit than the second-arriving seller. Both reserve prices are set above the marginal costs of the sellers resulting in inefficiency.====Two factors drive these results. The first one is the ability of buyers to switch costlessly between auctions, which is a unique feature of online markets. Some buyers may even acquire computerized bots scanning for desired goods across different digital auction platforms and bidding on their behalf. This behavior leads to well-structured profit functions, which further allow to trace the division of the revenue from the highest-valued buyers. The second factor is that sellers have identical costs. Many sellers in online consumer-to-consumer markets are reselling previously bought items, so one would not expect significant variability in their costs.====Peters and Severinov (2006) prove that when there are many sellers and buyers in online-auction markets, the reserve prices set by the sellers are equal to their marginal costs. In contrast to sealed-bid auctions with simultaneous choice of reserve prices, it is unlikely that in online markets sellers choose reserve prices simultaneously. Rather, a seller who comes to the market first, chooses a reserve price expecting a subsequent arrival of another seller. Such a strategic environment may be framed as a Stackelberg-like model where sellers choose reserve prices, and our results are consistent with the standard symmetric Stackelberg model, in which the first-moving seller earns a higher profit.====Our model has an important difference from the literature on competing auctions which assumes commitment of buyers after they select an auction conditional on the observed reserve prices (McAfee, 1993, Peters and Severinov, 1997, Burguet and Sákovics, 1999, Hernando-Veciana, 2005, Virág, 2010). In our model (similar to Peters and Severinov 2006) buyers are free to move among concurrently running auctions. The latter property of the buyers’ behavior allows us to apply marginal revenue approach (Myerson, 1981) and dramatically simplify our calculations. By considering sequential choice of reserve prices (which reflects the observed regularities of online markets) we show that a pure-strategy equilibrium exists even in a two-seller setting, which is not the case for a simultaneous-move game (Burguet and Sákovics, 1999). A somewhat similar emergence of a pure-strategy equilibrium after switching from simultaneous to sequential moves may be observed in Bertrand models with capacity constraints (e.g., Hviid, 1990, Deneckere and Kovenock, 1992).====We argue that just like in the environment considered by Burguet and Sákovics (1999), competition between two sellers competing in online auctions is not enough to drive reserve prices to marginal costs. In addition, we show that each seller has an incentive for some strategic move in which he commits to a reserve price prior to the other seller. To our knowledge, Bapna et al. (2009) and Han et al. (2018) are the only studies that investigate the structure of reserve prices in online markets. Our theory confirms their findings and explains why variation in reserve prices exists even with two sellers. This contrasts with a monopolist who sells items by auctions at the same optimal reserve price and a competitive market in which reserve prices are equal to marginal costs. The monopolist outcome may also arise if competing duopolists were to collude.====In the next section we describe the model. In Section 3 we describe the sellers’ profits directly and then adapt the revenue equivalence theorem to rewrite the sellers’ profits. In Section 4 we describe the equilibrium. Section 5 provides a numerical example with three buyers whose values are distributed uniformly. Section 6 provides a comparison to other strands of the literature: we consider an alternative formulation of our model in which the sellers choose prices simultaneously and buyers may bid in both auctions. We also consider the implications of using ====-equilibrium – the equilibrium that applies most naturally in our paper – to other models in the literature. Section 7 concludes.",Imperfect competition in online auctions,https://www.sciencedirect.com/science/article/pii/S0304406822000684,2 June 2022,2022,Research Article,25.0
"Maniquet François,Nosratabadi Hassan","Université catholique de Louvain, Center for Operation Research and Econometrics (CORE), Voie du Roman Pays 34, 1348 Louvain-la-Neuve, Belgium","Received 24 August 2021, Revised 10 February 2022, Accepted 13 May 2022, Available online 1 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102718,Cited by (0),We provide a revealed preference analysis in models where choice may be status-quo biased. We study the relevant case in which data is limited and make reasonable assumptions on how status-quo may structure behavior. We show how to elicit ==== of revealed preference characterizes status-quo biased behavior.,"There is ample evidence in support of relevance of ==== alternatives in choice. The term describes the tendency of a decision-maker (henceforth DM) to favor a previously chosen alternative more than she would have, had it not been chosen in the past. This bias creates a challenge for a policy-maker who is interested in inferring DM’s welfare, since choice may not anymore indicate what maximizes welfare. This challenge does not appear as a central focus in the existing literature. The benchmark axiomatic approach in Masatlioglu and Ok (2005) precedes the main strand of the literature which makes use of the assumption of complete dataset.==== In practice, however, a policy-maker has only access to a ==== dataset. It turns out that one may not ==== the analysis of complete dataset to limited ones, and thus may not make welfare inference using approaches that rely on the former. To our knowledge, the only paper in the literature that addresses the case of limited datasets is Dean et al. (2017). However, it does so in a rather general model that aims to explain aspects of status-quo bias that go beyond the aforementioned classical interpretation. While their level of generality strengthens the explanatory power of their model, it limits what can be learned about DM’s welfare.====In this paper, we study status-quo bias with a focus on the question of welfare. In order to achieve this practical goal, we (1) assume that data is indeed limited and (2) do not refrain from making reasonable assumptions on the manner status-quo may structure behavior. Applying revealed preference techniques, we essentially explain how to learn ==== from a given limited dataset. Using this information, we provide the preferences with which welfare analysis can be performed and the axioms with which the status-quo bias model may be tested.====Our approach follows in the footsteps of classical theory of revealed preference initiated by Richter (1966).==== We first apply revealed preference techniques to the benchmark model of status-quo biased behavior introduced in Masatlioglu and Ok (2005) which is the most structured model in the literature. This model may be captured in what follows: DM behaves rationally in a choice situation where she does not possess a status-quo alternative. Otherwise, she sticks to her status-quo unless there is an alternative that is ==== better than the status-quo. If this latter is indeed the case, then DM chooses the best among those that are obviously better than the status-quo. Hence, the model uses two binary relations: one for preferences and another for obvious preferences.====In contrast to the existing literature which has gone in the direction of generalizing the benchmark model (for example, Masatlioglu and Ok, 2014 and Dean et al., 2017), in a second model we ==== structure the model by requiring the obvious preference relation to be ==== the preference relation. This notion requires an alternative that is preferred to a second one which, in turn, is obviously preferred to a third, to be obviously preferred to the third. We (1) argue that this more structured model is compatible with a number of status-quo formation cases such as ==== considerations and the so-called ==== and (2) show that the relative transitivity provides significantly stronger welfare analysis compared to the benchmark model and, thus, even more compared to its generalizations.====Targeting a revealed preference style characterization theorem, we provide a procedure to elicit ==== preferences and obvious preferences of DM using choice observations. In this regard, we make two interesting observations. First, we show that this procedure does ==== boil down to removing the observations in which DM sticks to her status-quo—that is, those that are seemingly welfare-irrelevant.==== Second, we show that ==== revelations do play a critical role in identifying DM’s preferences. To our knowledge, there is no behavioral model of choice where this latter observation has been made. Finally, we show that the axioms that characterize the models are nothing but variants of the well-known ==== applied to both revealed preferences and obvious preferences. In other words, our axioms simply require the procedure of eliciting preferences and obvious preferences not to induce any cycles.====The structure of this paper is as follows. In Section 2, we present the main model. We show how to use the data to build revealed preference and obvious preference relations. We also introduce two variants of the well-known strong-axiom—one applied to preferences and another to obvious preferences—and show that they together characterize the main model. In Section 3, we replicate the definitions and the theorem for our second model. We discuss generalizations of our models that incorporate indifference in Section 4. In Section 5, we discuss the relation between this paper and the literature. In Section 6, we provide some concluding comments.",Welfare analysis when choice is status-quo biased,https://www.sciencedirect.com/science/article/pii/S0304406822000623,1 June 2022,2022,Research Article,26.0
Kops Christopher,"Maastricht University, The Netherlands","Received 21 December 2021, Revised 14 April 2022, Accepted 18 May 2022, Available online 1 June 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102726,Cited by (0),Clustering is an essential part of data analysis and a natural simplifying operation to perform when making choices in complex situations. It is the act of grouping similar objects together. This paper develops an individual choice theory based on the idea of clustering. It shows that two simple conditions behaviorally characterize this theory and lays out to what degree its parameters can be identified from choice data. It furthermore presents an attempt at a taxonomy of some of the most prominent models in the literature on two-step conceptualizations of the act of choice.,"Clustering is the process of grouping a set of objects in such a way that similar objects are grouped together. It is an integral aspect of data analysis and a natural simplifying operation to perform when making choices in complex situations. Imagine a decision maker (DM) wandering the aisles of a bookstore. Every two books on display may vary or coincide with respect to some of their attributes: genre, gender of protagonist, plot type, bestseller, classic, etc. Each such attribute induces a natural partition of books into clusters, where books in the same cluster agree on the corresponding attribute (e.g., fall into the same genre). Our DM focusses on books that stand out when clustered with other available books, and then chooses the best such book.====We study a general choice procedure that uses clustering in this way as a core component and conceptualizes the act of choice in two stages. More specifically, the first stage involves a consideration set mapping (Masatlioglu et al., 2012, Manzini and Mariotti, 2014, Brady and Rehbeck, 2016, Lleras et al., 2017, Caplin et al., 2018, Dardanoni et al., 2020) that filters out alternatives which never stand out when clustered with some specific other available alternative. Then, in the second stage, the DM picks an alternative that is preferred to all alternatives that survive the first stage.====Earlier studies have focussed on how different cognitive factors may affect decision-making. A related strand of the literature studies choice behavior when only a subset of the available alternatives is considered for choice, because of factors such as norms (Baigent and Gaertner, 1996), framing effects (Salant and Rubinstein, 2008), categorization (Manzini and Mariotti, 2012a), cognitive limitations (Eliaz et al., 2011, Masatlioglu et al., 2012, Lleras et al., 2017) or social influence (Cuhadaroglu, 2017, Borah and Kops, 2020). Another strand of the literature models choice as the result of (sequentially) maximizing more than one preference relation (Manzini and Mariotti, 2007, Manzini and Mariotti, 2012b, Cherepanov et al., 2013, Tyson, 2015, Dutta and Horan, 2015, Horan, 2016, Kops, 2018, Armouti-Hansen and Kops, 2018). A third strand provides applications of these models to competitive markets (Eliaz and Spiegler, 2011), finance (Stango and Zinman, 2014) and medical decision-making (Katz and Ted George, 2019).====What separates our approach from these earlier two-stage conceptualizations of the act of choice is twofold. First, no other choice procedure builds on the idea of cluster analysis and addresses the inherent multiplicity in clustering that the same object may end up in several groups. Second, our approach over consideration sets (Wright and Barbour, 1977, Hauser and Wernerfelt, 1990, Roberts and Lattin, 1991) provides a limited taxonomy of two-stage conceptualizations of the act of choice that brings some structure to the literature. In particular, this approach allows to lay out what connects the rational shortlist method (Manzini and Mariotti, 2007), reference-dependent preferences (Kőszegi and Rabin, 2006), collective choice (Horan and Sprumont, 2022), categorization (Manzini and Mariotti, 2012a), rationalization (Cherepanov et al., 2013) and overwhelming choice (Lleras et al., 2017)====This taxonomy links the axiomatic characterizations of these theories to contraction and expansion consistency (Sen, 1993) on the level of consideration set mappings. To illustrate this structure and suggest possible applications of our model, a few examples may be instructive at this point. Their formal discussions are delayed to the body of the paper.====—In recent years, many large retailers have opened smaller-sized supermarkets in highly populated, urbanized areas. When shopping at such convenience stores, it is conceivable that consumers consider buying all products that they would likewise consider buying at larger, inventory richer supermarkets as well.====—Consumers in smaller-sized supermarkets may consider buying all products that they would likewise consider buying at larger supermarkets—and then some. This is so, because limiting the product offering may counteract cognitive impairments like choice overload. Our model accounts for such phenomena and illustrates why grocery shopping at a smaller store may be welfare-enhancing for a consumer even when the smaller store charges moderately higher prices than larger stores do.====—Returning to our bookstore example from the beginning of the Introduction, imagine a book that the DM considers buying for some of its attributes. This book stands out when, for each such attribute, it is only compared to books sharing its expression of this attribute. Then, the DM also considers buying this book when she compares it with respect to all such attributes at once.====The paper is organized as follows. Section 2 lays out the primitives. Section 3 presents the model and Section 4 the behavioral foundation. Section 5 discusses questions of identification. Section 6 extends the model and its characterization to choice correspondences. Section 7 presents the attempt at a taxonomy of the related literature. Section 8 concludes.",Cluster-shortlisted choice,https://www.sciencedirect.com/science/article/pii/S0304406822000660,1 June 2022,2022,Research Article,27.0
"Fan Cuihong,Jun Byoung Heon,Wolfstetter Elmar G.","Shanghai University of Finance and Economics, China,Korea University, Korea,Humboldt-University at Berlin, Germany","Received 15 March 2021, Revised 6 May 2022, Accepted 21 May 2022, Available online 30 May 2022, Version of Record 1 November 2022.",https://doi.org/10.1016/j.jmateco.2022.102722,Cited by (1), and its ==== and find conditions for profitable espionage. We also show that the spied-at firm cannot benefit from bypassing the spy if its cost is low by delaying its pricing decision (or firing the spy) because this would trigger a devastating cascade of belief changes.,"The present paper analyzes a general duopoly model with differentiated products where firms engage in price competition. Prices are strategic complements, payoff functions are strictly supermodular, and firms are subject to incomplete information concerning each other’s cost. The peculiar feature of our analysis is that one firm is served by a spy who has infiltrated the rival firm. That spy informs the firm it serves about the pricing decision of the spied-at firm before it chooses its own price. It thus induces a Stackelberg game in which the firm that is served by the spy is Stackelberg follower and the spied-at firm Stackelberg leader.====Spying is a natural narrative for a Stackelberg game where someone must inform the follower of the leader’s choice of action because absent an informer the game would fall back to the simultaneous moves Bertrand game, yet poses its own intricate issues, especially if firms are subject to incomplete information.====Absent incomplete information, the impact of the induced sequential game is fairly obvious and well-known from the literature: in the (subgame-perfect) Stackelberg equilibrium both first- and second-movers benefit, albeit the second-mover benefits more (see, at different levels of generality, Gal-Or, 1985, Amir and Stepanova, 2006). In that case spying is highly profitable, not only for the culprit, the firm that is served by the spy, but also for the spied-at firm.====However, as Bagwell (1995) pointed out, that equilibrium is not robust because if the second-mover’s observation is subject to noise, the unique equilibrium is that of the simultaneous moves Bertrand game, even if that noise is arbitrarily small.==== A robust equilibrium in which the follower responds to the leader’s action can only emerge if equilibrium strategies prescribe a random move or, as in the present paper, players are subject to incomplete information and strategies are type dependent.====Incomplete information not only remedies this robustness problem, it also drastically affects the impact of spying and gives rise to an enhanced role of espionage. It enhances the role of spying, because if one knows the rival’s price, incomplete information concerning the rival’s cost is inconsequential. It drastically affects the impact of spying because, unlike in the case of complete information, the spied-at firm no longer benefits from spying if its cost is low, and it is not even assured that the spying firm benefits from spying.====In order to assess the overall impact of spying and find sufficient conditions for profitable spying, we decompose it into two effects: the ==== and the ====. This is achieved by introducing a hypothetical Bertrand game with one-sided private information, in which the spying firm knows the other firm’s unit cost, while the spied-at firm only knows its own unit cost. Comparing the game with spying and the Bertrand game with this hypothetical game allows us to isolate and assess these two effects.====More information is of course beneficial for the spying firm if the spied-at firm’s behavior is unchanged. However, as the spied-at firm knows that the rival observes its price it responds by adjusting its own price downward if its cost is low and upward if that cost is high. Therefore, it is not clear, ====, whether the firm that is served by the spy altogether benefits from espionage. Although the sequential moves effect unambiguously benefits the firm that is served by the spy, the sign of the information effect may be negative; therefore, the overall impact of spying on the spying firm’s payoff is not a foregone conclusion.====In turn, the spied-at firm benefits from having its private information disclosed if its cost is high, yet tends to be worse-off if its cost is low. This is due to the fact that a low cost is associated with a low price which, when revealed by the spy, induces the spying firm to respond with a low price, and ====.====The fact that the spied-at firm is made worse-off by spying if its cost is low suggests that it should perhaps bypass the spy if its cost is low by delaying its pricing decision until the spying firm has set its price or, for that matter, by firing the spy if its identity has been disclosed. Again, it is not clear, ====, whether this reaction is profitable, and indeed, we find that bypassing the spy if the spied-at firm’s cost is low induces a cascade of adverse belief changes to such an extent that bypassing is never profitable.====The present paper is related to the recent literature on the second-mover advantage in sequential games in which prices are strategic complements (see, for example, Amir and Stepanova, 2006). Like that literature we assume general demand functions that are super-modular in the price vector rather than the simple linear examples assumed in much of the literature. However, while this literature considers games of complete information, incomplete information is essential in our analysis. In fact, abstracting from the particular aspects of spying, our analysis can also be interpreted as an attempt to provide sufficient conditions for extending the second-mover advantage to Bertrand–Stackelberg games under incomplete information.====The present analysis of spying is also related to the literature on information sharing in oligopoly (see, for example, Gal-Or, 1986, Vives, 1984). The common denominator is that revealing private information reduces uncertainty concerning rival firms’ play and allows firms to better correlate their pricing or output decisions which contributes to increase welfare, but not necessarily profits. However, there are fundamental differences.====Whereas the information sharing literature assumes that firms may commit to honestly exchange information before they observe private signals, regardless of whether they constitute either “good” or “bad” news, in our present analysis one firm uses the services of a spy to unilaterally steal private information about the rival’s pricing decision. Moreover, whereas in Bertrand competition with substitutes firms generally do not benefit from exchanging information about their cost, spying out the rival’s price makes information about the rival firms’ costs irrelevant and in many cases benefits both the spying and the spied at firms.====There is also a small theoretical literature on espionage in market games. While most contributions consider spying out rivals’ type, the present paper assumes spying out rivals’ actions. The two kinds of espionage are fundamentally different. Whereas spying out rivals’ ==== allows players to sharpen their prediction of rivals’ play, spying out rivals’ ==== also changes the order of moves and transforms a simultaneous into a sequential moves game. As a byproduct of observing a rival’s action a player may also learn about its rival’s type. Indeed, in the present model, after observing the rival’s price, its cost can also be inferred. However, that inference is irrelevant because knowing the rival’s cost in addition to its price adds no value.====Spying out rivals’ type is considered in several contributions. Wang (2020) considers a linear Cournot duopoly where one firm is served by a spy who provides noisy information about the other firm’s unit cost. Similarly, Kozlovskaya (2018) assumes that firms can learn about demand by conducting their own market research and by spying out the results of rivals’ market research, and Zhang (2015) considers spying in two-player contests, assuming a spy provides noisy information about the other contestant’s valuation for the given prize.====Solan and Yariv (2004) analyze the impact of spying on entry deterrence, assuming the spy serves a potential entrant and gathers noisy information about the incumbent’s willingness to fight entry. Adhering to a similar framework, in Barrachina et al. (2014) the spy provides the potential entrant noisy information about the incumbent’s capacity which affects the incumbent’s response to entry, while Barrachina et al. (2021) assume that the spy provides noisy information about the incumbent’s unit cost.====There is also a literature on spying in auctions. Several contributions explore how bidding is affected if one bidder acquired information about a rival bidder’s valuation or private signal (an early example is Vickrey, 1961 pp. 17–20 and Appendix III) and Kim (2008), Jun et al. (2021) show that such information acquisition may have negative value. Other contributions consider spying on early bids. Fischer et al. (2021) compare the effect of spying in first- and second-price auctions, focusing on behavioral assumptions other than profit maximizing and experimental testing,==== and Fan et al. (2021a) consider first-price auctions where the spy may report distorted information.====Spying in auctions has also been analyzed in the context of corruption, where a dishonest agent auctioneer either allows a bidder to adjust his bid after reporting rivals’ bids to him (as, for example,==== ====in Burguet and Perry, 2007, Arozamena and Weinschelbaum, 2009, Fan et al., 2021b) or flexibly seeks a deal with the bidder who gains the most by either lowering or increasing his bid (as in Lengwiler and Wolfstetter, 2010).====The plan of the paper is as follows: Section 2 states the model. In Section 3 we disentangle the ==== and ==== of spying and use this decomposition to examine who benefits from spying and to find sufficient conditions for profitable spying. In Section 4 we consider a detailed example that allows us to explicitly solve all equilibrium strategies, assess the impact of spying on firms’ expected profit, expected consumer surplus, and welfare. In Section 5 we allow the spied-at firm to bypass the spy if its cost is low by delaying its pricing decision or by firing the spy if his identity has been revealed. We show that such thus bypassing the spy would trigger a devastating cascade of belief changes. In Section 6 we close with a discussion. Some proofs are relegated to the Appendix.",Spying in Bertrand markets under incomplete information: Who benefits and is it stable?,https://www.sciencedirect.com/science/article/pii/S0304406822000647,30 May 2022,2022,Research Article,28.0
Yazıcı Ayşe,"Durham University Business School, Department of Economics and Finance, Mill Hill Lane, Durham, DH1 3LB, UK","Received 30 November 2020, Revised 20 March 2022, Accepted 9 May 2022, Available online 26 May 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102720,Cited by (0),"We consider senior-level labor markets and study a decentralized game where firms can fire a worker whenever they wish to make an offer to another worker. The game starts with initial matching of firms and workers and proceeds with a random sequence of job offers. The outcome of the game depends on the random sequence according to which firms make offers and therefore is a probability distribution over the set of matchings. We provide theoretical support for the successful functioning of decentralized matching markets in a setup with myopic workers. We then identify a lower bound on outcomes that are achievable through strategic behavior. We find that in equilibrium either any sequence of offers leads to the same matching or workers (firms) do not agree on what matching is the worst (best) among all possible realizations of the outcome. This implies that workers can always act to avoid a possible realization that they unanimously find undesirable. Hence, a well-known result for centralized matching at the entry-level carries over to matching at the senior-level albeit without the intervention of a mediator.","Centralized job matching has received much attention in theory and practice since centralized procedures were introduced by market organizers to address market failures such as uncontrolled unraveling of appointment dates, recontracting and welfare losses; notable examples are medical residency matching and school choice. ‘Stability’ of outcomes is considered to be the main property that accounts for the success of centralized matching procedures. A matching is ‘stable’ if no agent and no firm-worker pair have an interest to deviate, i.e., no agent prefers being unmatched to being matched to her current partner and no firm-worker pair prefer each other to their current partners.====It is generally thought that decentralized markets do not function well and will therefore benefit from improved coordination or centralization. However, it is not well understood why they remain decentralized.==== One possible reason offered by theoretical studies is that these markets reach stable outcomes by means of decentralized decision-making. However, this result does not suggest much about the incentives of market participants to improve their prospects, in particular, the limits on successful (welfare enhancing) strategic behavior and the welfare implications of strategic interaction for agents. The theory of matching has clearly shown that there are systematic common/opposing interests among certain groups of agents over the set of stable matchings. Unlike in decentralized systems, the clearinghouse in centrally organized institutions decides which procedure to use and therefore can control the welfare consequences of strategic interaction via restricting achievable outcomes. To the best of our knowledge, this paper is the first study to understand the limits that agents face in strategic behavior in decentralized senior-level labor markets.====Due to the functional differences between institutions at the entry-level and the senior-level, studies of the former, albeit having advanced, do not apply to or address how equilibrium is reached at the senior-level professional markets. Entry-level job positions, e.g., the labor market for medical interns and residents, are initially vacant. Senior-level positions may not be initially vacant and become available when an incumbent vacates a position due to retirement or termination/expiration of contract.====Among examples of matching at the senior-level are the markets for CEO’s and sports coaches. Each December, at the end of the football season, colleges make coach replacements prior to National Signing Day in February which is the last day for a high school senior to sign with the football team of an American college. Firing and hiring of head coaches mostly occur in December and the vacancy is filled in advance of the National Signing Day (Thomas and Van Horn, 2016). Nevertheless, such a short period of opportunity window for firing and hiring helps coordination in the market. The internal governance structure of some colleges may lead them to act faster than others; therefore some offers may reach candidates earlier than others. In the firing and hiring season, when several senior positions become vacant, which vacancies are filled with which coaches depend on the order in which colleges make offers. Another source of uncertainty is the practice that colleges fire a coach whenever they wish to make an offer to another coach, without knowing whether the offer will be accepted or not.====To model decentralized senior-level matching where uncertainty over outcomes is accounted for, we study a sequential game that starts with an initial matching and proceeds with a random sequence of job offers.==== The game begins with a lottery over sequences of firms. Each firm in the sequence is given the opportunity to offer its unique position to a worker. A worker who receives an offer compares it with the offer that she may be holding, and rejects one and keeps the other. We assume that no firm proposes to the same worker more than once and that no worker rejects the offer she is holding unless she receives a new one. Uncertainty over sequences of offers is translated to uncertainty over outcomes, therefore the outcome of the game is a lottery over matchings. Our decentralized procedure reduces to the firm-proposing Deferred Acceptance (DA) algorithm (Gale and Shapley, 1962) when each firm’s position is initially vacant, in other words, the market in question is at the entry-level.==== In such an instance, uncertainty over sequences of firms does not have any influence on outcomes, i.e., any sequence of firms leads to the same matching.====We show that the equilibrium outcome is a lottery over matchings such that, unless degenerate, workers (firms) do not agree on what is the worst (best) element in the support (Theorem 3). Thus, either each sequence of offers leads to the same matching in equilibrium or workers act to avoid a matching that they unanimously find undesirable. Based on the lessons learnt from two-sided matching at the entry-level, at a first glance firms seem to have a favored position in the game as proposers. Indeed, if each agent acts according to her/its true preference ordering and if each firm’s position is initially vacant, then any execution of the procedure leads to the firm-optimal stable matching which firms (workers) unanimously find the best (worst) among all stable matchings.====The advantage of being proposers is not straightforward in our context. First, when strategic considerations are taken into account, our result shows that a matching that favors firms arises only if it is the unique realization of the equilibrium outcome or else workers can always act to eliminate such a possibility. A similar result is also present in centralized entry-level professional markets. Consider a central agent who, upon receiving rank-order lists of preferences from market participants, applies the firm-optimal stable mechanism==== to produce an outcome. When confronted with such a clearinghouse, workers can always eliminate their worst achievable partner by misreporting their preferences (Theorems 4.6 and 4.7, Roth and Sotomayor, 1990). Second, due to the existence of an initial matching situation, welfare of firms and of workers does not monotonically decrease and increase respectively during the execution of the decentralized procedure, preventing a stable outcome occurring even when both sides act based on their true preference orderings.====There are two studies that are closely related to our paper. In a sightly different formulation of the game by Pais (2008), each firm keeps its initial partner until it makes a successful offer or until its initial partner receives and accepts an offer and vacates the position. In our formulation a firm fires its initial partner when it chooses to make an offer regardless of the outcome of the offer. The main result in Pais (2008) establishes that in an equilibrium where each firm acts according to its true preference ordering, any realized matching is ==== at the ==== preferences. The same result holds for a similar decentralized game formulated by Blum et al. (1997) to study the vacancy chain problem. Starting from a ==== matching (i.e., no blocking pair involves a matched firm), offers can only be made by a random sequence of firms with vacant positions. In decentralized matching, actions may be history dependent and therefore an agent’s actions may not comply with a fixed preference ordering. Our first result is the counterpart of theirs in a more restricted but natural setting where workers are myopic and offers are not history dependent, that is, the strategy set of each agent is the class of preference orderings (Theorem 2). A myopic worker bases her decisions on a predetermined preference list and always accepts the offer that is ranked higher on a preference list.====Our second result is established for the general setup where the strategy space is not restricted to preference profiles. Different from the two papers, we study the structure of possible realizations of any equilibrium outcome and show that in equilibrium either any sequence of offers leads to the same matching or workers can always act to avoid the one that they unanimously find undesirable; unlike in centralized markets, it is achieved without the intervention of a mediator.====The organization of the paper is as follows. In Section 2, we introduce the model. In Section 3, we describe the decentralized game and define the equilibrium notions. In Section 4, we address the strategic questions and present our results. In Section 5, we conclude. We defer all proofs to the Appendix A, Appendix B.",Decentralized matching at senior-level: Stability and incentives,https://www.sciencedirect.com/science/article/pii/S0304406822000635,26 May 2022,2022,Research Article,29.0
"Kunimoto Takashi,Zhang Cuiling","School of Economics, Singapore Management University, 90 Stamford Rd, Singapore, Singapore,The Institute for Advanced Economic Research, Dongbei University of Finance and Economics, Dalian 116025, Liaoning, China","Received 4 August 2021, Revised 5 April 2022, Accepted 27 April 2022, Available online 11 May 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102714,Cited by (0),"This paper first considers a bilateral-trade model with one-sided asymmetric information in which one agent (seller) initially owns an indivisible object and is fully informed of its value, while the other agent (buyer) intends to obtain the object whose value is unknown to himself. As no mechanisms can generally result in efficient, voluntary bilateral trades (Jehiel and Pauzner, 2006), we aim to overturn this impossibility result by employing two-stage mechanisms (Mezzetti, 2004) in which first, the outcome (e.g., allocation of the goods) is determined, then the agents observe their own outcome-decision payoffs, and finally, transfers are made. We show that the generalized two-stage Groves mechanism and the shoot-the-liar mechanism both induce efficient, voluntary bilateral trades. We next consider a two-sided asymmetric information setup in which both parties have ====. We show by means of a stylized example that the shoot-the-liar mechanism “sometimes” induces an efficient, voluntary trade, while the generalized two-stage Grove mechanism never induces it.","This paper investigates efficient, voluntary bilateral trades in an interdependent values environment in which one agent (seller) initially owns an indivisible object which the other agent (buyer) intends to obtain. Efficiency adopted in this paper is an ex post notion, which requires that (i) there be a trade of the good if and only if the buyer’s valuation for the good is at least as high as the seller’s valuation (decision efficiency) and (ii) whatever the buyer pays be always exactly what the seller receives (budget balance). Voluntary trades mean that each agent of every type has a weak incentive to participate in the mechanism (interim individual rationality). By the well-known revelation principle, efficient, voluntary trades are implementable through standard (one-stage) mechanisms if there exists a ==== mechanism satisfying decision efficiency (EFF), interim individual rationality (IIR), and ex post budget balance (BB) in which each agent is asked to announce his own type and telling the true type profile constitutes a Bayesian Nash equilibrium (i.e., Bayesian incentive compatibility (BIC)).====This paper considers two different classes of asymmetric information environments. The first class is a ==== asymmetric information environment in which the seller is fully informed of the value of the object, while the buyer is uninformed of its value. The second class is a ==== asymmetric information environment in which both parties have private information and each party’s valuation depends on the other’s information in the same way. Unfortunately, general negative results have been established in both environments. In one-sided asymmetric information environments, Proposition 1 of Jehiel and Pauzner (2006) shows that when the single crossing condition fails, no mechanisms satisfy BIC, EFF, BB, and IIR.==== Moreover, in a general two-sided asymmetric information environment, Fieseler et al. (2003) show that no mechanisms satisfy BIC, EFF, BB, and IIR.====To overcome this negative message in the bilateral trade model with interdependent values, we seek more positive results by looking at ==== mechanisms (Mezzetti, 2004) in which first, the outcome (e.g., allocation of the goods) is determined, then the agents observe their own outcome-decision payoffs, and finally, transfers are made. In his Proposition 1, Mezzetti (2003) establishes the generalized revelation principle, which says that it entails no loss of generality to focus on the following two-stage generalized revelation mechanisms: in the first stage, agents are asked to report their type and the allocation of the good is determined on the type reports; after agents observe their allocation payoff, they are asked to report their realized allocation payoff in the second stage; and finally, the monetary transfers are finalized on the reports of both stages. By this generalized revelation principle, we call a two-stage generalized revelation mechanism simply a two-stage mechanism. Appealing to the generalized revelation principle, we need to modify the notion of Bayesian incentive compatibility: a two-stage mechanism satisfies BIC if there exists a ==== of that two-stage mechanism in which all agents tell the truth in both stages.====The assumption behind the use of two-stage mechanisms can be justified. Imagine that two parties in a bilateral trade setup invite a trusted mediator (a third party) to their contractual relationship: the mediator asks both agents to put a large sum of money as a deposit in the mediator’s account and the mediator pays back the remaining deposit to each agent after the two-stage mechanism is played out. We can replace the mediator by a ==== based on the blockchain technology as a commitment device that prevents agents from reneging the contract terms (see, for example, Matsushima and Noda (2020)). Even without a trusted mediator or smart contracts, we can sometimes implement a two-stage mechanism by a long-term relationship. For example, in the context of a labor market, employers learn the quality of the workers after employing them and after both the employer and the worker find out the extent to which the worker is qualified for the job, the worker’s contract is revised. Of course, it goes without saying that the power of two-stage mechanisms may well be compromised in some other scenarios.====Section 3 investigates the performance of the ==== mechanism and the ==== mechanism in the bilateral trade model under one-sided asymmetric information. These two-stage mechanisms are proposed by Mezzetti, 2004, Mezzetti, 2007. We argue that the assumption of one-sided asymmetric information, while restrictive, fits well with a number of applications: Samuelson (1984, p.995) says “In many commonly encountered bargaining settings, one individual possesses better or worse information about the potential value of the transaction than the other. Examples range from the sale of a used automobile to a corporate acquisition via tender offer, where in each case the buyer is likely to be less well-informed than the seller”. Yang et al. (2009) and Gümüş et al. (2012) study the supply-risk management in which a manufacturer faces suppliers who possess private information about supply disruptions. Lewis (2011) studies the online auctions of used goods where sellers voluntarily disclose their private information to buyers through photos, text, and graphics on the auction website. The main results of this section are: (i) the generalized two-stage Groves mechanism with lump-sum transfers satisfies BIC, EFF, BB, and IIR (Theorem 1) and (ii) the shoot-the-liar mechanism satisfies BIC, EFF, BB, and IIR (Theorem 2). This exhibits a contrast with Jehiel and Pauzner (2006) who establish a general impossibility result within the class of “one-stage” mechanisms.====In Section 4, we ask how the generalized two-stage Groves mechanism and the shoot-the-liar mechanism perform in a ==== asymmetric information environment. We scrutinize this very question by focusing on a stylized model in which each agent’s type is chosen from the uniform distribution over ==== and each agent ====’s valuation for the object is represented by a linear function, i.e., ====, where agent 1 is the seller, agent 2 is the buyer, and ==== is considered the degree of interdependence of preferences for agent ====. We impose the single crossing condition on this example, which means that ==== for each agent ====. It follows from Fieseler et al. (2003) that in our example, no “one-stage” mechanisms satisfy BIC, EFF, BB, and IIR. We show that the generalized two-stage Groves mechanism never satisfies IIR (Proposition 1). We also show that the shoot-the-liar mechanism satisfies BIC, EFF, BB, and IIR when ====, whereas it does not when ====
 (Proposition 2).====In Section 5, we argue that the generalized two-stage Groves mechanism and the shoot-the-liar mechanism both perform similarly in one-sided asymmetric information environments (Claim 1).====In Section 6, we discuss the connection to a closely related paper by Galavotti, Muto, and Oyama (henceforth, GMO, 2011). GMO consider the problem of partnership dissolution with interdependent values, which includes our bilateral trade problem. GMO provide what they call Assumption 5.1 under which the shoot-the-liar mechanism satisfies BIC, EFF, BB, and IIR for any ownership structure. Specializing in our bilateral trade model, we obtain the following results: (i) GMO’s Assumption 5.1 is violated in a large class of environments under one-sided asymmetric information and (ii) GMO’s Assumption 5.1 holds if and only if ==== in a two-sided asymmetric information environment based on the example of Section 4. By (i), we conclude that our Theorem 1 is not implied by GMO, which makes our Theorem 1 of an independent interest. By (ii), we conclude that there are many more cases than GMO’s Assumption 5.1 permits where the shoot-the-liar mechanism satisfies BIC, EFF, BB, and IIR.====The rest of the paper is organized as follows. In Section 2, we introduce the general notation and basic concepts for the paper and go over some key important results in the literature. In Section 3, we show in a one-sided asymmetric information environment that the generalized two-stage Groves mechanism and the shoot-the-liar mechanism both satisfy BIC, EFF, BB, and IIR (Theorems 1, 2). Section 4 introduces a stylized interdependent values model of bilateral trade under two-sided asymmetric information. In this stylized model, we show that the generalized two-stage Groves mechanism always violates IIR (Proposition 1), while the shoot-the-liar mechanism “sometimes” satisfies all the four properties (Proposition 2). In Section 5, we argue that the generalized two-stage Groves mechanism and the shoot-the-liar mechanism both perform similarly in one-sided asymmetric information environments (Claim 1). In Section 6, we discuss the relation with GMO. Section 7 concludes the paper with final remarks. In the Appendix, we provide all the proofs of the results omitted from the main text of the paper.",Efficient bilateral trade via two-stage mechanisms: Comparison between one-sided and two-sided asymmetric information environments,https://www.sciencedirect.com/science/article/pii/S0304406822000593,11 May 2022,2022,Research Article,30.0
"Karni Edi,Safra Zvi","Johns Hopkins University, Department of Economics, United States of America,Warwick Business School, University of Warwick, United Kingdom","Received 9 September 2021, Revised 21 March 2022, Accepted 9 April 2022, Available online 7 May 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102700,Cited by (1),"Departing from the reduction of compound lotteries axiom on multi-stage lotteries, this paper proposes a new hybrid model to analyze decision trees. Applied to multi-stage decision trees induced by experiments, Blackwell’s (1953) definition of the relation “more informative” on the set of information structures is equivalent to experiments being more valuable to a class of non-expected utility preferences. This result extends Blackwell’s theorem and provides new insights regarding the evaluation of information produced by experiments.","From a decision making point of view, experimentation is valuable because it provides information that helps decision makers choose courses of actions whose payoffs are higher in the states that are more likely to obtain. Blackwell (1953) formalized this perception as a binary relation: “more informative than” on the set of experiments. According to Blackwell, one experiment is more informative than another if, ==== (i.e., expected-loss vectors) each of which corresponds to an action taken contingent on the experimental observations. Blackwell characterized this relation by proving that one experiment is more informative than another if and only if the information content of the latter is obtained by garbling the information content of the former. Equivalently, an experiment is more informative if it allows choices that have higher expected utility. We refer to this equivalence as Blackwell’s theorem.====Seen in this way, being better informed seems unambiguously beneficial. Thus, the equivalence between ranking experiments by their information content and their ranking by the expected utility criterion seems oddly restrictive. This equivalence is particularly disconcerting in view of experimental evidence suggesting that subjects systematically violate the tenets of expected utility theory – the sure thing principle and the independence axiom – and the proliferation, over the last 40 years, of non-expected utility models of decision making under risk and under uncertainty.====To grasp the issue, consider a decision maker facing a choice among feasible actions whose consequences depend on the realization of some underlying states. Suppose that the likelihood of the various states materializing is quantified by a (prior) probability distribution function. Before choosing an action, the decision maker receives a signal (i.e., an observation), produced by an experiment, that informs him about the likely realization of the states. Upon receiving such signal, the decision maker invokes Bayes’ rule to update the prior state probabilities and then proceeds to choose an action from the feasible set. This process may be thought of as two-stage compound lottery. In the first stage, the experiment produces a signal, according to some probability distribution on the set of signals, following which the decision maker chooses an action. In the second stage, a state is selected (according to the posterior distribution) and the decision maker is awarded the prize that corresponds to the image of the selected state under the chosen action.==== The question is how decision makers perceive this two-stage lottery.====We argue that the critical aspect of the expected utility model that underlies Blackwell’s theorem is the way these compound lotteries are handled. The standard way of handling compound lotteries is to apply the reduction of compound lotteries axiom (henceforth, RCLA). This axiom asserts that a multi-stage lottery is reduced to a single-stage lottery by attributing to each ultimate payoff a probability equal to the product of the probabilities on the events that lead to it. In the context of the choice of experiments, the RCLA assigns the ultimate outcome the probability of the signal multiplied by the posterior probabilities of the states to which the chosen course of action assigns that outcome.====Analysis that treats the two-stage process as equivalent to its one-stage reduction runs the risk of ignoring subtleties that beset the extensive form decision process. Wakker (1988) and Safra and Sulganik (1995) showed that departing from the independence axiom and maintaining the RCLA implies a widespread and robust aversion towards information.====An alternative procedure of handling compound lotteries is to use the certainty-equivalent reduction. In this procedure, multi-stage lotteries are reduced to single-stage lotteries by folding back the lottery tree, replacing the lotteries along the branches by their certainty equivalents. Schlee (1990) and Safra and Sulganik (1995) showed that in non-expected utility theories, this procedure implies that information is not always valuable.====We propose a hybrid decision model that invokes the RCLA in the second stage of the decision-making process and a procedure analogous to certainty-equivalent reduction in the first stage. Application of the RCLA, which seems compelling when the transition between the stages is automatic, seems less so when the two stages are separated by an intermediate decision.====To formalize this idea, we propose a new model, dubbed the ====, and show that it identifies a class of preferences that unambiguously value one experiment over another if and only if the information content of the latter is obtained by garbling that of the former. The expected utility model is a special case of this class. In fact, it is the only hybrid model that is consistent with the RCLA. Application of the hybrid model provides new insight into the manner in which information is evaluated. While admitting the possibility that some decision makers may not consider information to be unambiguously beneficial, it maintains that, in Blackwell’s analytical framework, such behavior is unreasonable.====The surprising (difficult) aspect of Blackwell’s theorem is that a more informative experiment (that is, one that affords better decisions by the expected utility criterion) implies clearer signals. In this paper, informativeness corresponds to an experiment being more valuable in the sense of affording better decisions for a broader set of preferences, including expected utility preferences. Consequently, this direction of the proof relies on Blackwell’s theorem. The novelty of this paper is the observation that the full power of expected utility – in particular, the RCLA – is not needed for Blackwell’s result. To the best of our knowledge, ours is the only nontrivial model in which a large class of non-expected utility preferences can satisfy this direction of Blackwell’s theorem.====The rest of the paper is organized as follows. The next section provides a brief review of Blackwell’s (1953) theorem. Section 3 reviews the reduction procedures. Section 4 introduces and characterizes the hybrid decision model. Section 5 extends Blackwell’s theorem. Section 6 discusses the value of information and reviews the related literature.",Hybrid decision model and the ranking of experiments,https://www.sciencedirect.com/science/article/pii/S0304406822000489,7 May 2022,2022,Research Article,31.0
Uyanik Metin,"School of Economics, University of Queensland, Brisbane, QLD 4072, Australia,Department of Economics, Johns Hopkins University, Baltimore, MD 21218, United States","Received 26 August 2021, Revised 16 January 2022, Accepted 18 April 2022, Available online 30 April 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102704,Cited by (1), and ,"Tjalling Koopmans mentions the continuity postulate early on in the first of his 1957 magisterial essays “on the state of economic science”. After a footnote reference to a ==== function, an adjective he does not define but freely use, and then to a ==== preference ordering that he does define, he masks the continuity postulate under the assumption of ==== referring to the latter as “a rather weak continuity property of preferences”. Subsequently, and in the interest of precision, he sees the completeness postulate on preferences as “suggested by considerations of continuity and nonsaturation”.==== It is only in the second half of the essay, well after he has developed the basic theorems that the essay is to present, that he uses the adjective ==== for a correspondence, but here again does not define the “appropriate generalization of the concept of continuity.==== The point is that the notion of continuity is one of the signature notions in the ==== and his reliance on it continues on with more or less increasing force and emphasis throughout his work subsequent to them.====We shall have more to say about Koopmans’ fraught relationship to the continuity postulate: its exploitation of the interplay between its bearing on a function as opposed to that on a binary relation and/or a correspondence is an interplay that runs throughout his later ==== However, this is not a paper on Koopmans’ place in the history of economic thought, and we begin with him only because of the impact that the 1957 ==== and his subsequent papers, have had on the direction of the literature: it is this direction and its consequence for ongoing research that is of primary concern of this paper’s attempted examination of the postulate. In this connection, it is worth pointing out that in his 1960 and 1964 work, Koopmans assumes preferences to be parametrized by a continuous function, and investigates additional axioms under which such a function takes on a specific form. Later in 1972, in his two papers in honor of Jacob Marschak, he considers the representation of a binary (preference) relation by a (utility) function, the so-called representation problem, and of necessity, introduces continuous relations. Since the work reported here draws on the insights of the antecedent mathematical literature on the continuity of functions, specifically on the application of Rosenthal’s 1955 theorem to continuous relations (see below), both aspects of his work are relevant for us.====Remaining with the ==== but now shifting from the first to the second, Koopmans returns in his masterful discussion of  Herstein and Milnor (1953) and Savage (1954) to the continuity postulate in the context of a “single decision maker faced with uncertainty”. In this discussion, he refers to the “surprisingly far-reaching implications of simple postulates of consistency and sharpness of preferences concerning objects capable of a particular type of continuous variation”, but again masks the continuity property of preferences, herein ==== in a postulate that exerts the existence of a probability mixture equivalent to the one intermediate between three ranked objects of choice, referred to as ==== In his discussion of the representability problem, he writes: ====However, in this entire discussion there is no reference to Eilenberg (1941), a pioneering paper only acknowledged as a precursor of Debreu fifteen years later in Koopmans (1972).==== More generally, it is this omission of Eilenberg’s paper, as well as the masking of the continuity postulate, both in the theory of competitive equilibrium and in uncertainty theory as presented by von Neumann–Morgenstern, that serves as the point of departure for, and motivational center of, the results to follow. There is no bridging between the two subjects, and they are confined to separate essays, a separation and a consequent obscuring that is seen here as a consequence of not beginning with the primitive raw notion of continuity. In short, this essay is an angled contribution to the theory, its two arms are represented by the twin ==== and ==== each hitched to Koopmans’ ====After a section on mathematical underpinnings, we discuss the omission in Section 3 below and turn to what we mean by the term “masking”. This is really brought out in Koopmans’ presentation of continuity as a non-satiation or an existence property. The point is that there is no single and unambiguous rendering of the continuity postulate: different notions of continuity have been used in economic and decision theory depending on the different needs and requirements of a particular result, as indeed they ought. There is no standard continuity assumption even when differences in topologies are disregarded. In terms of a further unpackaging, one can discern three types of continuity pertaining to a binary relation and labeled as ==== continuity, ==== continuity, and ==== continuity. All impose topological assumptions on the relation itself – graph continuity on it as a subset of the product space and section continuity on its sections – but while the first two use the topological structure of the space on which the choice function or relation may be defined, the third, in its reference to the ==== and ==== properties, works with an algebraic structure on the choice set, and limits itself only to the topological structure of the unit interval.==== All properties are widely used in economic theory, but perhaps the last is more pervasive in decision theory, while the first two constitute more of a staple of Walrasian general equilibrium and game-theoretic analyses. The point that constantly guides this essay is that one has to go back to the most raw, primitive and undefined, meaning of the term if the analytical and substantive connections between the subsequent conceptual proliferations are not to be missed.====With this preamble to the antecedent literature on “decision theory”, we can turn to another dissonance and exclusion, that in the now-standard expositions of neoclassical theory of the consumer. In seminal work on the theory of individual choice, Wold (1943–44) and Wold and Jureen (1953), also touch on the problem of the representation of a preference relation by a real-valued (utility) function in the context of a finite-dimensional Euclidean space. However, unlike Eilenberg (1941), they work with binary relations that are not necessarily anti-symmetric:, and whose indifference curves are therefore not singletons. In their discussion of Wold’s theorem,  Arrow and Hahn (1971, p. 106) write: ====In summary, adjectives change their meaning depending on the noun they modify, and the adjective in question here in this work is ‘continuous,’ and is being used to modify different classes of functions and binary relations. In addition to the usual variations involving topologies in use, ‘continuous’ may also refer to (i) continuity defined on a choice space in terms of the topological structure of a possibly ==== parameter set, e.g. as in mixture continuity, or (ii) continuity restricted to pertinent subsets of the choice set, e.g. as in continuous on lines in a vector space or on paths in a path-connected space. Of particular import are the complementarities between different forms of continuity and the geometry of the individual functions or relations.====With this introductory framing as the backdrop, we are now in a position to spell out succinctly the substantive contribution of the paper. In a nutshell, we introduce new continuity concepts for relations by using the concepts developed for functions, and use them not only to unify and simplify a range of results, but also to bring to light novel equivalences hitherto unseen in the economic literature. And perhaps most importantly, to see the neoclassical consumer theory as including decision theory now associated with the names of von Neumann, Morgenstern, Savage, Anscombe and Aumann. To be sure, as brought out above, some of the continuity concepts for functions are part of the historical record, linear continuity and Rosenthal’s restricted continuity readily come to mind, but the translation from functions to relations has also yielded a new result for functions themselves; see, for example, the very first theorem below.==== In Section 2, we recall the theorems of Rosenthal and Eilenberg along with the Genocchi–Peano example. In Section 3, we present seven theorems, two propositions and a corollary: rather than repeat and summarize them – they are already clean and clear enough in their statements==== – we can invite the reader to quickly see how they articulate of the basic motivations delineated above, and how they relate to previous work in economic theory. In Section 4 we present seven propositions and three corollaries, and further discuss the implications of the results to the antecedent literature: these applications of the theory constitute the deliverables of the theory, applied theory so to speak. Section 5 concludes the paper with a summary and some observations regarding future and open direction. We reserve Appendix A.1 to the proofs of the results and the technical lemmas they require and Appendix A.2 to a battery of nine examples that close lacunae that may suggest themselves to an interested reader.",The continuity postulate in economic theory: A deconstruction and an integration,https://www.sciencedirect.com/science/article/pii/S0304406822000507,30 April 2022,2022,Research Article,32.0
"Goswami Anindya,Rana Nimit,Siu Tak Kuen","Department of Mathematics, IISER, Pune, India,Fakultät für Mathematik, Universität Bielefeld, Germany,Department of Actuarial Studies and Business Analytics, Macquarie Business School, Macquarie University, Sydney, NSW, Australia","Received 24 November 2021, Revised 8 March 2022, Accepted 15 April 2022, Available online 28 April 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102702,Cited by (0),"We consider a risk-sensitive optimization of consumption-utility on an infinite time horizon where the one-period investment gain depends on an underlying economic state whose evolution over time is assumed to be described by a discrete-time, finite-state, ","Stochastic optimal growth model pioneered by Brock and Mirman (1972) has received a considerable attention in the economics or related literature. We consider, in this paper, a regime switching optimal growth model for a single sector in the discrete time settings where the agent has risk sensitive preferences. Markov regime-switching models have important applications in economics and econometrics (see, for example, Hamilton, 1989, Hamilton, 2016). It is assumed that at the beginning of a trading day an economic agent can access to information about the income from the previous investment and the current economic state. Given this information, the agent invests a portion of the previous income in the production technology and consumes the rest. Depending on the investment and the economic state, the next random income is realized at the beginning of the next day. The next market state is also observed simultaneously. This cycle goes on. The agent’s objective is to maximize the discounted risk sensitive non-expected utility of consumption in the infinite time horizon.====A similar problem has been studied in Bäuerle and Jaśkiewicz (2018) assuming absence of transitions in the economic state. In other words the productivity shocks have been assumed to be i.i.d., instead of following Markovian dynamics. We extend the results of Bäuerle and Jaśkiewicz (2018) to include the regime switching scenarios, since many state of the art macroeconomic models consider Markov shocks in productivity rate. For example, Mendoza (1991) considered a real business cycles (RBC) model, where technological disturbances including interest-rate and productivity shocks were assumed to be governed by two-state Markov chain, (see Page 802 therein). Wessels (1994) discussed a RBC theory in which productivity shocks were supposed to follow a stationary Markov process, (see Page 1754 therein). Van Nieuwerburgh and Veldkamp (2006) assumed that the technology shock follows a two-state Markov switching process when discussing asymmetries in a RBC model. Heutel (2012) adopted a Markov process for modelling persistency in productivity shocks when investigating optimal environmental policies responding to business cycles. Azzimonti and Talbert (2014) supposed that an aggregate productivity shock was governed by a first-order Markov process when studying polarized business cycles. The Markov regime-switching model for optimal growth considered here can capture an important aspect of economic growth, namely the impacts of transitions in different phases of business cycles such as expansion and recession on the growth in an unified setting. Specifically, the model may provide theoretical insights into making investment–consumption decisions with the objective of achieving sustainability in the economic growth in the presence of changing economic regimes. We describe probabilistic transitions in economic regimes using a discrete-time, finite-state, Markov chain whose probability laws are given. To the best of our knowledge, this problem has not been studied in the literature.====For the sake of generality, in this paper, the production function and the utility function are allowed to be unbounded from above. We refer to Bäuerle and Jaśkiewicz (2018), Jaśkiewicz and Nowak (2011), Durán (2003), Kamihigashi (2007) and Wessels (1977) and references therein for similar considerations. We follow the weighted supremum norm approach for studying the optimization problem. As in Hansen and Sargent (1995) and Bäuerle and Jaśkiewicz (2018) the finite horizon objective function has been defined recursively. Its limiting value is then shown to exist which gives rise to the non-expected discounted utility in the infinite time horizon. We then study the corresponding optimality equation using the Banach fixed point theorem. To show that the dynamic programming operator maps a space of certain functions into itself, we consider the class of concave, non-decreasing and non-negative functions with a weighted supremum norm as in Bäuerle and Jaśkiewicz (2018). In Bäuerle and Jaśkiewicz (2018) the contraction property of the operator has been established by applying Chebyshev’s association inequality of random variables. We have first extended that association inequality to the regime switching case (see Lemma 4.4) and then applied the inequality for establishing the contraction mapping. Furthermore, it is shown that the optimal investment policy exists in the class of stationary policies.====The Euler equation for optimal consumption is also considered. An Euler equation generally states that the average gain in utility for saving now and then consuming in future, instead of immediate consumption should, after discounting, be equal to the utility gain of consuming now. The readers can find the study of Euler equation in Brock and Mirman (1972) and Kamihigashi (2007). It is interesting to note that under the present settings for a nonzero risk aversion parameter, the Euler equation involves an expectation with respect to a probability measure different from the standard equilibrium market measure. Specifically, under an optimal consumption policy, the measure depends on the value function. Consequently, higher (lower) probabilities are assigned to the scenarios corresponding to smaller (larger) values of the value function. In view of this, such measures are also referred to as the worst-case measure (see Hansen and Sargent (2007)).====In addition to the above results, we have also established that, under additional assumptions, the joint process consisting of the optimal income and the underlying economic state has a non-trivial joint stationary distribution. This is accomplished as an application of a result from Meyn and Tweedie (2009). To facilitate the application of this result, we prove that the stochastic kernel of the joint process is Feller and bounded in probability.====Numerical results based on hypothetical parameter values in a parametric model with three regimes of the production rate are provided to illustrate the impacts of changes in the production rate regime on the optimal investment–income ratio and the value function. To this end, a regime switching extension of the Cobb–Douglas production function and the power utility is considered. The numerical results are presented graphically along with some insightful economic interpretations. In this connection a fair comparison is added between the regime-switching model and its fixed-regime counterpart examined in Bäuerle and Jaśkiewicz (2018, Example 1). Prior to choosing specific numerical values for computation, we identify the range of parameter values for which all the theoretical assumptions in this paper are fulfilled.====The rest of this paper is organized as below. In Section 2 we present the regime switching investment–consumption model for a single sector. The optimization problem is formulated in Section 3. In Section 4 the optimality equation of the value function is obtained. Moreover, the existence of the optimal stationary policy is also established. The Euler equation for optimal consumption is derived in Section 5. The establishment of the existence of the joint stationary distribution for the optimal income process and the underlying economic state is presented in Section 6. The numerical results and their interpretations for a natural regime switching extension of a model with Cobb–Douglas production function, coupled with a power utility are discussed in Section 7. We conclude the paper in Section 8 which contains some directions of future investigations. For the sake of self-containedness some results with proofs have been added at the end in the Appendix section.",Regime switching optimal growth model with risk sensitive preferences,https://www.sciencedirect.com/science/article/pii/S0304406822000490,28 April 2022,2022,Research Article,33.0
"Ma Xiaohan,Samaniego Roberto","Texas Tech University, Department of Economics, P.O. Box 41014, Lubbock, TX 79409, United States of America,Department of Economics, The George Washington University, 2115 G Street, NW Monroe Hall 340, Washington, DC 20052, United States of America","Received 1 July 2021, Revised 1 April 2022, Accepted 2 April 2022, Available online 17 April 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102694,Cited by (0),"We develop a ==== business cycle model with imperfectly observed neutral and investment-specific technology shocks, which agents learn from noisy signals. Estimated using US data, the model implies that neutral technology shocks generate more volatile responses than in an environment with perfect information, whereas investment-specific shocks have more persistent impact. Noise accounts for substantial variation in investment, and alters agents’ behavior persistently through its influence on beliefs — even when the underlying fundamentals are unchanged and the noise itself is not persistent. Further implications for business cycle analysis are explored.","In addition to neutral productivity shocks, ==== shocks are thought to be key drivers of macroeconomic dynamics.==== It is generally assumed that the value of investment-specific productivity at any given date is known. However, there are reasons to believe that the presence of investment specific technological change (ISTC) introduces ==== into the economic environment. If so, then neither the value of neutral nor investment-specific productivity would be exactly known in real time. This begs a question: can uncertainty about current economic fundamentals (and ISTC in particular) affect the response of the economy to ==== in those fundamentals? The ISTC-uncertainty link has not been explicitly addressed in the literature, yet the link is evident in the disagreement about how to even measure ISTC among key references such as Greenwood et al. (1997), Cummins and Violante (2002) and Whelan (2003).====The inability to observe fundamentals such as ISTC in real time could address several key questions in the business cycle literature. First, it could lessen the reliance of business cycle models on highly persistent shocks for the persistence of macroeconomic variables. This is because, when technology is unobserved, agent behavior would depend not only on the dynamics of the technology process, but also on the dynamics of agent ==== about technology, which would lag. Second, it is well known that ISTC shocks introduce counterfactual negative comovement between consumption and investment into business cycle models====: however, if it takes time to learn the value of ISTC shocks, they could remain important for the business cycle even without introducing such comovement. Third, a model of this kind necessarily has noise, and as a result it can be used to quantify the impact of noise on macroeconomic dynamics — specifically noise about ISTC, which is novel in the literature.====We develop a model economy where technological fundamentals are imperfectly observed, and study the implications of the resulting uncertainty for macroeconomic dynamics in an otherwise standard business cycle framework. In the model, economic agents never perfectly observe the values of the fundamentals of the economy, which are investment-specific and neutral productivity shocks. We use a specification with neutral and ISTC shocks for several reasons. First, these shocks are known to be important fundamentals that drive the business cycle and, as discussed, ISTC could be related to the unobservability of fundamentals. Second, there must be at least ==== unobservable fundamentals for agents to be unable to infer their values based on observed output.====Since agents do not observe economic fundamentals, their behavior is guided by their ====, and these beliefs change over time based on the signals agents receive. One signal is the observed value of GDP. The other is a noisy signal of the value of investment specific technological change (ISTC), which is revealed when agents operate the production technology to produce GDP. This signal is a function of the actual value of ISTC and a stochastic ==== term which itself may lead to fluctuations in investment and output even if fundamentals remain unchanged. Since this noise represents a signal that fundamentals have changed when they may not have, we call it an “ISTC noise shock.”====The model is difficult to solve. Beliefs about fundamentals are state variables of the economy, and are unbounded continuous functions. Moreover, beliefs will not all have analytically tractable expressions. However, we show that agent beliefs approximately follow a multivariate Gauss–Markov process after linearization. In addition, the gap between agent expectations of the unobservables and their actual values follows a VAR system. Based on this VAR system we demonstrate that the extent of uncertainty (the variance of agent beliefs about fundamentals) settles down to a constant. Also, in the absence of noise shocks, beliefs converge to the correct values of the uncertain economic fundamentals asymptotically.====Using this approximation strategy, we deliver several analytical results. First, a shock to any of the fundamentals, or an ISTC noise shock, generates uncertainty about ==== fundamentals in subsequent periods. This is because no observed change in signals or outcomes can be attributed with certainty to a change in any particular fundamental. Second, for the same reason, if there is uncertainty about the value of any particular fundamental at a given point in time, in subsequent periods uncertainty will spread to ==== fundamentals. Third, an ISTC noise shock has a persistent impact on beliefs, and hence on behavior, even if the noise itself is not persistent. All these results suggest that the presence of noise in the economic environment likely increases the ==== of macroeconomic variables.====Next, we quantify the macroeconomic impact of technological uncertainty due to ISTC noise and the learning process it engenders by estimating the critical parameters of the model economy using US data and Bayesian techniques, comparing it to a framework without technological uncertainty. First, we find that the presence of ISTC noise shocks leads output to respond more strongly to changes in neutral productivity (TFP) and less strongly to changes in ISTC. This is because, in either case, agents cannot be sure which shock they are observing. In the first case the possibility of a high return to investment encourages more production than otherwise, and in the second case the possibility that it might be a neutral shock has the opposite effect. Second, we find that ISTC noise shocks are a significant source of variation, and that their impact on beliefs is persistent even though these shocks turn out to decay rapidly. Based on the estimated persistence of signals, the half-life of noise is less than ==== months. However, because of its persistent impact on ====, the half-life of the change in GDP ==== by a noise shock is over ==== years.====The model also provides a solution to the Barro and King (1984) “co-movement” puzzle, the result that consumption and investment tend to have low or negative comovement in RBC models when shocks other than TFP shocks (such as ISTC) are important driving forces of fluctuations. This is inconsistent with the highly ==== correlation observed in US data. Our model shows that the introduction of ==== about the form of technological shocks can solve this issue, without introducing additional complexity, as we discuss later. When economic agents observe a signal that ISTC has changed, they always put some weight on the possibility that this could be due to changes in neutral productivity instead, or even just noise, so they respond with caution. Our model explains almost half of the empirical correlation between consumption and investment whereas, when we remove technological uncertainty from the model, the correlation becomes negative.====The idea that beliefs about the state of the economy could drive economic fluctuations dates back at least to Pigou’s (1927) theory of “errors of undue optimism or undue pessimism.” Our paper contains a new notion of unwarranted optimism or pessimism regarding the current value of ====. Our paper is also related to the extensive literature on the importance of TFP and ISTC for business cycle dynamics, such as Greenwood et al. (2000), Fisher (2006), Jaimovich and Rebelo (2007), Justiniano et al. (2010) and Görtz and Tsoukalas (2013). Our focus is on how ==== about the current values of these fundamentals affects economic dynamics. We show this implies that the value of the ==== is not known with precision when technology is uncertain in this way,==== which is essential for why observed output cannot be a sufficient statistic for productivity.====The fact that there is a signal about future ISTC – albeit a noisy one – is related to the literature on information about “news shocks”, such as Beaudry and Portier (2006), Schmitt-Grohé and Uribe (2012), Barsky and Sims (2012) or Ben Zeev and Khan (2015). In those papers, however, the current state of the world is always known, even if its components are not.==== In our environment, the ==== values of fundamentals are not known. To underline how our model is different, note that Chahrour and Jurado (2018) show that there is an equivalent representation of any data on both fundamentals and beliefs in terms of “news” shocks and in terms of “noise” shocks, under certain assumptions.==== Our model, however, does not neatly fit either their view of “noise” (there is a fundamental which will be observed at ====, and we receive a noisy signal of it at time ====) nor “news” (the fundamental at date ==== is the sum of two shocks, only one of which is known in advance). In both cases, the fundamental is observed when it is realized, and the history of signals and shocks is known. In our case, while the history of signals is observed, the fundamentals are never observed, nor are their histories.====Section 2 describes the model and environment. Section 3 derives theoretical results regarding the properties of beliefs in the model economy. Section 4 presents the data and estimation methodology and the quantitative results. Section 5 concludes with a discussion of potential directions for future research.",Business cycle dynamics when neutral and investment-specific technology shocks are imperfectly observable,https://www.sciencedirect.com/science/article/pii/S0304406822000453,17 April 2022,2022,Research Article,34.0
He Wei,"Department of Economics, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong Special Administrative Region,Departments of Economics and Mathematics, National University of Singapore, 10 Lower Kent Ridge Road, Singapore 119076, Singapore","Received 3 August 2021, Revised 3 February 2022, Accepted 23 March 2022, Available online 15 April 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102698,Cited by (0)," compactness and preservation of weak/weak==== upper hemicontinuity) for Bochner/Gel====fand integral of Banach valued correspondences are obtained as corollaries. We prove the necessity of the nowhere equivalence condition for any of these properties to hold. Economic applications, including a general class of large games and abstract economies with asymmetric information, are discussed.","Since optimal choices of economic agents under various contexts are non-unique in general, correspondences (also called set-valued functions) arise naturally in economic theory. For a measurable correspondence on an atomless measure space, one can consider the set of integrals, or more generally, conditional expectations of its integrable selections. The regularity properties of convexity, compactness and preservation of upper hemicontinuity for such mathematical constructions are very relevant for economic applications. When the range space is finite dimensional, the theory of integration and conditional expectation for correspondences has been extensively studied in the literature.====Infinite-dimensional spaces are widely used in economics. To model uncertainty with an infinite number of states or intertemporal interactions with an infinite number of periods, one needs to work with infinite-dimensional spaces.==== This paper aims to characterize the underlying properties of convexity, compactness and preservation of upper hemicontinuity for Bochner/Gel====fand conditional expectation of Banach valued correspondences, and to discuss some illustrative economic applications.====It is well known that even if one considers the integral of correspondences, all the regularity properties (convexity, compactness, and preservation of upper hemicontinuity) may fail when the range space is an infinite-dimensional Banach space.==== Thus, these regularity properties would also fail in general for conditional expectation of Banach valued correspondences. To resolve this issue, we work with the condition of “nowhere equivalence”, which captures in a certain sense relative richness between a ====-algebra and its sub-====-algebra. Based on the nowhere equivalence condition, we are able to characterize those desirable properties for both Bochner and Gel====fand conditional expectation of Banach valued correspondences.====Formally, let ==== be an atomless probability space and ==== a countably generated sub-====-algebra of ====. Then ==== is said to be nowhere equivalent to ==== if these two ====-algebras do not coincide (modulo null sets) on any non-negligible subset of ====. In Theorem 1, Theorem 2, we show that if a Banach valued correspondence ==== is ====-measurable and its selections are allowed to be ====-measurable, then all of these regularity properties hold for the Bochner and Gel====fand conditional expectation of the correspondence ====, respectively.====Theorem 1, Theorem 2 indicate that the nowhere equivalence condition is sufficient for the relevant regularity properties to hold. We show in Theorem 3 that the nowhere equivalence condition is also necessary for the validity of any of those regularity properties. Thus, the nowhere equivalence condition is “minimal” if one needs to work with any of those properties for conditional expectation of Banach valued correspondences in general.====It is clear that the integral can be viewed as the conditional expectation with respect to the trivial ====-algebra, and any atomless ====-algebra is obviously nowhere equivalent to the trivial ====-algebra. Thus, the results on the Bochner/Gel====fand integral of Banach valued correspondences can be delivered as direct corollaries. Our Proposition 3, Proposition 4 generalize the results in a few earlier papers on integral of Banach valued correspondences by using the more general condition of nowhere equivalence.====To illustrate the usefulness of the characterization results, we discuss some economic applications in Section 4. We apply Theorem 1, Theorem 2 to study a new class of large games and obtain new equilibrium existence results. In a typical large game, each player’s payoff depends on her action and the societal aggregate, which is often modeled as the average actions from all the players. Such modeling of “aggregate actions” completely removes the differences among individual players. As argued in Khan et al. (2013), economic agents are often associated with biological or social traits, and hence it is possible that an individual player’s payoff depends on the aggregate actions in a rich manner. We consider a large game where the action space is a subset of some infinite-dimensional Banach space, and the externality part is the conditional expectation of players’ action profiles, given their characteristics. That is, a player may care about the aggregate actions from those players who share the same characteristics.==== We show in Theorem 4 that the nowhere equivalence condition characterizes the pure-strategy equilibrium existence in such large games in the sense that a pure-strategy Nash equilibrium exists if and only if the nowhere equivalence condition is satisfied on the player space.====In the second application, we study a model of abstract economies with many agents and asymmetric information. Yannelis (2009) presented a model of abstract economies in which the action space is a Banach space and the externality is the action profile. To guarantee the existence of social equilibria, he imposed the condition of “many more agents than strategies”, implying that the underlying agent space is a saturated probability space when the Banach space is infinite dimensional. He and Sun (2018b) studied the setting with finite-dimensional action spaces and the externality being the average actions along all the groups with the same private information. We generalize these results to the setting in which the action space is an infinite-dimensional Banach space and the agent space can be a non-saturated probability space (====, the widely adopted Polish space).==== It is shown that the nowhere equivalence condition guarantees the existence of a social equilibrium.====The paper is organized as follows. Section 2 collects some basic definitions of correspondences and also the nowhere equivalence condition. In Section 3, we prove the regularity properties for Bochner and Gel====fand conditional expectation of correspondences, and present the necessity result. Section 4 discusses economic applications. All the proofs are collected in Appendix.",Conditional expectation of Banach valued correspondences and economic applications,https://www.sciencedirect.com/science/article/pii/S0304406822000477,15 April 2022,2022,Research Article,35.0
Geng Sen,"MOE Key Laboratory of Econometrics, Wang Yanan Institute for Studies in Economics, Xiamen University, China","Received 13 July 2021, Revised 26 March 2022, Accepted 3 April 2022, Available online 14 April 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102692,Cited by (1),"When making a choice from a menu, individuals sometimes consider only a subset of the menu and choose one alternative according to their preferences from the subset. We provide a unified model that formalizes two aspects of such consider-then-choose procedures: the circumstances under which individuals have limited consideration and the structures that are imposed on consideration sets in the presence of limited consideration. Full consideration is guaranteed when the size of a menu does not exceed a cutoff number. Limited consideration may occur or must occur when the size of a menu exceeds the cutoff number, which is correspondingly interpreted as the trigger of initiating limited consideration or the capacity of full consideration. We also assume that the consideration function from a menu to its consideration set satisfies an intuitive property that captures a range of real-world heuristics to form consideration sets. We provide a characterization of the model.","In light of the evidence documented in marketing, empirical industrial organization, and experimental studies, individuals sometimes unintentionally or intentionally consider only some alternatives in a menu and choose the most preferred option among those considered.==== This behavior is likely to violate Samuelson’s weak axiom of revealed preference (WARP).==== Numerous decision theory models formalize such consider-then-choose procedures and provide behavioral postulates that characterize these models.====An important and well-explored aspect of these choice procedures is the structures that are imposed on consideration sets. For example, while with different motivations, categorization (Manzini and Mariotti, 2012) and rationalization (Cherepanov et al., 2013) impose the same structure on consideration sets as the property of the competition filter (Lleras et al., 2017) does, which assumes that an alternative considered in a menu is still considered after some options are removed from the menu. The sequential procedure in Apesteguia and Ballester (2013) applies a list of acyclic rationales to eliminate alternatives. These different models address the question of how to form a consideration set of a menu in the presence of limited consideration.====Another important yet less-explored aspect is the circumstances under which limited consideration arises. When the consideration set always coincides with the original menu, the consider-then-choose procedures are equivalent to full rational choice. A key question is under what circumstances a consideration set of a menu is a proper subset of the menu. One natural approach to addressing this question is to associate the answer with the number of alternatives in a menu. It seems plausible that in some decision contexts, there is a cutoff of menu size for a decision-maker (DM): a menu with a size not exceeding a certain number is fully considered but full consideration of a menu with a size exceeding the number is not guaranteed (Hauser, 2014).==== When full consideration is not guaranteed, limited consideration ==== occur or ==== occur. In the former case, the size of the consideration set can exceed the cutoff number, and we interpret the cutoff as the ==== of initiating limited consideration. In the latter case, the size of the consideration set is bounded above by the cutoff number, and we interpret the cutoff as the ==== of a DM’s full consideration. For example, the shortlisting procedure with capacity-==== in Geng and Ozbay (2021) assumes that the DM eliminates alternatives according to a rationale only when the size of a menu exceeds her capacity and after elimination, the number of uneliminated alternatives does not exceed the capacity.====In this paper, we aim to provide a unified model that incorporates important factors about the consider-then-choose procedures in two aspects and characterize the model. Similar to the modeling approach used in Masatlioglu et al. (2012) and Lleras et al. (2017), we assume general properties on consideration sets without committing to a particular formation of consideration sets.====Our first property, ====, requires that if an alternative is considered in a menu, then it will be considered in at least one subset of the menu in which another alternative is removed. It describes contexts in which a product grabs the consideration of consumers/investors due to the product’s own characteristics or due to other product characteristics, such as the presence of a decoy product. In the former case, the product will still be considered when any other product is removed. In the latter case, the product will be considered or not depending on whether the decoy product is removed. This property generalizes the property of the competition filter and captures a range of heuristics to form consideration sets including categorization, rationalization, the shortlisting procedure and the sequential procedures with two rationales in the consideration stage.====Our second property, ====, requires that the size of the consideration set does not exceed a cutoff number ====. It describes contexts in which consumers/investors can only fully consider a certain maximum number of products. In these contexts, limited consideration must occur after the cutoff number is exceeded. For example, as one of the most highly cited papers in psychology, Miller (1956) shows that ordinary people can hold at most seven plus or minus two objects in short-term memory.====Our limited consideration model with a trigger postulates that the DM has a complete and asymmetric preference over alternatives. The DM chooses one alternative according to her preferences when the size of a menu does not exceed a trigger number, and when the trigger number is exceeded, she first forms a consideration set of the menu according to a weak competition filter and then maximizes her preference within her consideration set.====Our characterization result shows that the limited consideration model with a trigger number exceeding one is captured by the property of ====. The property requires that if alternative ==== is chosen in a menu, then there exists a chain of subsets of the menu including ==== such that the chosen alternative of each subset on this chain is also chosen in the binary choice between it and ==== if it differs from ====. It is clear that the property weakens Manzini and Mariotti (2007)’s WWARP, which requires that if alternative ==== is chosen in a menu, then for any subset of the menu including ====, the chosen alternative of the subset is also chosen in the binary choice between it and ==== if it differs from ====. We also show that the limited consideration model with a trigger number of one is falsifiable.====Our limited consideration model with a capacity augments the limited consideration model with a trigger by additionally assuming the property of capacity-====. We find that the limited consideration model with a capacity is fully characterized by the property of weak WWARP and a testable condition about consideration sets that are recovered from the choice function.====We also investigate a special case of the two models by replacing the property of the weak competition filter with the stronger property of the competition filter. We call them overwhelming choice with a trigger or a capacity. It turns out that WWARP is a necessary and sufficient condition for a choice function to be rationalized by an overwhelming choice with a trigger. We find that the upper choice set, which consists of all alternatives that are chosen in some supersets of a menu, is particularly relevant to the empirical content of overwhelming choice with a capacity. Specifically, an overwhelming choice with a capacity is characterized by WWARP and a testable condition that the size of the upper choice set of any menu falls below the size of the smallest menu that violates WARP. In addition, the upper choice set largely reveals information about the DM’s consideration sets and preferences when her choice is rationalized by an overwhelming choice with a trigger or a capacity.====The remainder of this paper proceeds as follows. We formally introduce the limited consideration model with a trigger or a capacity in Section 2. Section 3 presents the empirical content of the models. Section 4 characterizes overwhelming choice with a trigger or a capacity and delivers information about consideration sets and preferences. We discuss the inclusion relation between our models and other closely related models in Section 5. Section 6 concludes. All the proofs are provided in the appendix.",Limited consideration model with a trigger or a capacity,https://www.sciencedirect.com/science/article/pii/S0304406822000441,14 April 2022,2022,Research Article,36.0
"Bottazzi Giulio,Dindo Pietro","Istituto di Economia & Department EMbeDS, Scuola Superiore Sant’Anna, Piazza Martiri della Libertà 33, 56127 Pisa, Italy,Dipartimento di Economia, Università Ca’ Foscari Venezia, San Giobbe, Cannaregio 873, 30121 Venezia, Italy","Received 13 April 2021, Revised 20 December 2021, Accepted 2 April 2022, Available online 14 April 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102696,Cited by (0),"We provide sufficient conditions for the persistence or transience of stochastic processes on the real line based on the behavior of the first and second moment of their conditional increments at the boundaries. Our findings extend previous results in the literature (Lamperti, 1960) to the large class of discrete-time processes with bounded increments. We present some examples of application by studying survival and dominance of agents trading in complete financial markets.","In this paper, we provide sufficient conditions for a process ==== on the real line to be persistent or transient. A real process ==== is persistent if there exists a finite interval to which it belongs recurrently, it is transient otherwise. Two sets of sufficient conditions are derived. The first set of conditions is based on the sign of the asymptotic conditional drift of the process, i.e. the drift for very large (positive or negative) values of the process. The second set of conditions considers also the behavior of the conditional second moment of the process increment and, in general, relaxes the prescriptions on the asymptotic sign of the drift.====Our work has been inspired by the large class of economic models in which a distribution of resources in a population evolves as driven both by chance (shocks) and necessity (interaction, e.g. competition). For instance, one might be interested in how wealth is reallocated across heterogeneous households distinguished by different income processes in presence of both market interactions and government policies (see e.g. Benhabib et al., 2011); or in the dynamics of population shares of randomly meeting players adopting different strategies, possibly depending on the level of experimentation and/or the probability that a specific game is played (see e.g. Cabrales, 2000); or in the wealth dynamics of investors having heterogeneous preferences and beliefs, depending on the types of contracts that can be traded in a financial economy (as in Guerdjikova and Quiggin, 2019, Bottazzi and Dindo, 2014, Sandroni, 2005) and/or their learning models (as in Blume and Easley, 2006, Bottazzi and Giachini, 2019, Dindo and Massari, 2020, Giachini, 2021).====In all these examples, a central question is the characterization of long-run outcomes. Most of the models retain the Markovian property and are investigated using Markov chain methods for general state spaces (see e.g. Meyn and Tweedie, 1993). Within the mathematical literature, some works concentrate on linear processes in which the distribution of the shock depends on the present state of the system, also named (spatially) non-homogeneous random walk (see e.g. Menshikov et al., 2016). In particular, Kersting (1986) investigates the case of possibly unbounded drift. Other works exploit the powerful, but demanding, splitting condition and show convergence to a stationary distribution (see e.g. Bhattacharya and Majumdar, 2007 Chapter 3.5). The criteria presented in this paper, broadly inspired by Lamperti (1960), dispose of both the Markovian and linearity property. This extends their application to a large class of different situations by an indirect approach based on the dimensional reduction of the process via a properly chosen real function.==== The main difference with respect to Lamperti (1960) is that we consider real processes rather than only positive processes. Moreover, we replace the requirement of the process to have an infinite limsup almost surely with a boundedness condition on the increments of the process, often easier to check. Finally, our second set of conditions, those based on the first and second moments of the process conditional increment, are different to those advanced in Lamperti (1960) and cover situations not addressed in that paper.====After having introduced notations, definitions, and assumptions (Section 2), in the first part of the paper (Sections 3 Persistent processes, 4 Transient processes), we propose the two sets of sufficient conditions. The first set of conditions is based on the sign of the asymptotic drift, i.e. the drift for very large (positive or negative) values of the process. The conditions are rather intuitive: if the drift conditional on a large enough positive state is negative and the drift conditional on a large enough negative state is positive, then the process is persistent. Conversely, if the drift conditional on a large enough positive state is positive and the drift conditional on a large enough negative state is negative, then the process is transient. The second set of sufficient conditions relaxes the prescriptions on the asymptotic sign of the drift. For example, having a negative drift for large enough positive states can be replaced by having a drift that, for large enough positive states, approaches zero at a rate which is fast enough if compared to the asymptotic behavior of the second moment of the conditional increment. Similar conditions can be derived for other limit values of the drift. The conditions are presented in Section 3 for persistence and in Section 4 for transiency.====The advantages in applying our conditions over existing ones is illustrated with a series of examples in the second part of the paper (Section 5). To keep the presentation of the economic set-up concise, all applications study a financial economy with heterogeneous agents and complete markets, similar to that studied by Sandroni (2000) or Blume and Easley (2006).==== In Section 5.1, we study an economy populated by two agents with endogenous beliefs. In this case, the drift of the process becomes state dependent. We show that by studying the process on the real line, rather than only a positive process as in Lamperti (1960) or Kersting (1986), we can properly identify more economic relevant outcomes, in particular the convergence to the representative agent case. In Section 5.2, we extend the previous analysis to ====-agent economies and apply our methods to investigate the dynamics of the relative wealth of group of agents by studying the non-Markovian process of group wealth shares. Sufficient conditions that rely on the Markov property, such as those discussed in Meyn and Tweedie (1993), could not be used for this purpose. Finally, in Section 5.3, we extend the analysis to possible heterogeneous preferences. We illustrate how, despite the consumption shares dynamics becomes only implicitly defined, by working with the asymptotic drift of a suitably defined real process and by using the representative agent limit, our transiency/persistency conditions can still be applied.",Drift criteria for persistence of discrete stochastic processes on the line,https://www.sciencedirect.com/science/article/pii/S0304406822000465,14 April 2022,2022,Research Article,37.0
Qu Xiangyu,"CNRS, Centre d’Economie de la Sorbonne, France,Wuhan University of Technology, the School of Management, China","Received 25 June 2021, Revised 18 February 2022, Accepted 9 March 2022, Available online 18 March 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102690,Cited by (0),The actual level of income ,"It is increasingly understood that inequality has impacted nearly every aspect of economics. Now, as then, most economists agree that inequality studies lie at the heart of economic analysis. The many inequality measurement studies, Dalton, 1920, Kolm, 1969, Atkinson, 1983 and Sen and Foster (1997) among many others, carried out over the past several decades have provided a precise snapshot of inequality, especially in situations of perfect certainty.====To alleviate inequality, effective policies are needed, which, accordingly, require a trustworthy inequality estimate to signal and predict the effectiveness of any given policy. Except for a few studies, most inequality analyses assume perfect knowledge of the economic present and foresight of the economic future. Since the effects of uncertainty on policies determining inequality are widely acknowledged, assuming away uncertainty largely limits our ability to understand the elements that determine inequality, as well as the reliability of selected policies. Therefore, little progress can be made in the analysis of economic policy toward inequality without a satisfactory measurement under uncertainty.====There are a variety of stylized answers to how to incorporate uncertainty into inequality measurements. Two very natural approaches are, as Ben-Porath et al. (1997) pointed out, (1) first to calculate each individual allocation by its expected value and then to measure the inequality index based on individual expected values; and (2) first to measure the inequality index at each state and then to calculate the expected inequality index. For reasons of social principles or statistical tractability, it is reasonable to apply the different approaches to different types of problems. To avoid the inability to address some type of injustice, see, for example, Ben-Porath et al. (1997) and Chew and Sagi (2012), two approaches can be generalized in some way to reconcile different principles in distributional problems.====While the above generalized approach seems to be a very natural extension of inequality measurements under conditions of pure certainty to uncertainty, it nevertheless still possesses some troubling aspects. The present inequality measures are grounded on the allocation comparison among the ==== set of allocations, which includes both ==== and ==== allocations. A serious measurement of inequality should only be based on the feasible allocations to which the allocation judgement is being applied: what are the attainable allocations given the implementable policies? In other words, the ranking of feasible allocations should not solely depend on the income or wealth distributions and social justice principles we choose. Alesina and Angeletos (2005) observed that the United States and the Europe, though are quite similar in tastes, technologies and political systems, have considerably various social policies, such as redistribution policy. One possible explanation is that two societies have different views about the feasible allocations. As Bénabou (2000) claimed, ====. The feasible allocations in the United States are relatively restricted compared to the Europe, which might generate the different perception of inequality. Therefore, incorporating the concern of feasible allocations into inequality measurement would help explain the redistribution puzzle observed by Alesina and Angeletos (2005).====We should also consider the possible allocations by introducing all practical modifications to the economy. Adopting an inequality measure that ignores the national ‘cakes’ makes it hard to understand why a less equal society, in terms of a lower index of equality, sometimes feels more equal than a society with a higher index of equality. However, the most ubiquitous inequality indices are not characterized based on the opportunity set of allocations, which may lead to a trap that an unequal society is preferred.====The principal aim of this paper is to tackle the inequality measurement problem within a context-dependent framework that takes both uncertainty and feasibility of allocations into account. The inequality index we suggest is in a specific quantitative sense, so it would be clear what can sensibly be used to measure inequality. Specifically, let ==== denote the opportunity set of contingent allocations. Then, the inequality degree of ==== given ==== is measured by ====Function ==== is a classical Gini index measurement of allocation whenever state ==== is realized. The set of probabilities, ====, represents society’s probability estimation over states. The expression ==== shows the most egalitarian allocation we can obtain from ==== at state ====. Therefore, ==== measures exactly the ==== at state ==== if allocation ==== is selected. Hence, the index we propose reflects the minimum expected equality loss. Moreover, this index can be regarded as deriving from the psychological notion of regret. That is, the reaction to receiving allocation ==== when there is a best alternative would have led to allocation ====. It is worth emphasizing the distinction between uncertainty and risk, which is why our index takes a set of probabilities to represent that the events are not perfectly foreknown. In fact, our index coincides with the second approach discussed above whenever the set of probabilities is a singleton.====The main goal of this paper is to axiomatize the above inequality index. We shall provide principles on which comparisons of allocations can be justified. We shall also discuss the normative arguments for and against those principles. As a result, we shall show how our principles can be equivalently translated into our proposed index, which can be used in practice.====It is worth mentioning that our index is a natural extension of both Ben Porath and Gilboa (1994) and Ben-Porath et al. (1997). On the one hand, it is a generalization of the principles used in Ben Porath and Gilboa (1994) for evaluating inequality under certainty. On the other hand, Ben-Porath et al. (1997) is a special case of our index under uncertainty whenever a feasible set ==== is universal or the opportunity cost is ====. It is well-known that the inequality measured by Ben Porath and Gilboa (1994) can have either Choquet integral form or ==== expected utility form. However, in the face of uncertainty, both ex-ante and ex post inequality matter. Therefore, to maintain the consistency, it would be plausible that the form is invariant if we apply the same form over states first and, then over individuals, or vice versa. As Ben-Porath et al. (1997) demonstrated, only MEU form has invariant property, which, alternatively, justify the rationale of our index to adopting the MEU form.====However, there are many situations, for example, where a society considers how climate changes affect allocations, in which this attaching of probabilities to various alternative possible events in advance cannot reasonably be performed because the outcome is strongly influenced by some elements about which we have little or no prior information. While the classical indices can readily be extended to cover cases in the face of uncertainty, they cannot easily be extended to do so in the face of ignorance. Therefore, we revise the above index using the ==== approach to achieve a measurement under ignorance. ====This is actually a special case of the ==== in which the set of probabilities is the universal set of probabilities. We also axiomatize this inequality index and discuss the related principles.====Section 2 first presents the framework, followed by a discussion of the axioms in subsections. They include a set of axioms, which extends the assumptions from certainty to an uncertainty framework. The representation result is presented in Section 2.3. In Section 2.4, we also discuss the index measurement under ignorance. Section 3 concludes with a discussion. All proofs are provided in Appendix.",On the measurement of opportunity-dependent inequality under uncertainty,https://www.sciencedirect.com/science/article/pii/S030440682200043X,18 March 2022,2022,Research Article,38.0
"Pelgrin Florian,Venditti Alain","EDHEC Business School, France,Aix-Marseille Univ., CNRS, AMSE & EDHEC Business School, France","Received 9 July 2021, Revised 10 January 2022, Accepted 23 February 2022, Available online 15 March 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102670,Cited by (1)," of intertemporal substitution or equivalently the sign of the cross-derivative of the utility function whereas the second rests on sectoral technologies through the sign of the capital intensity difference across two sectors. Furthermore, building on the quasi-palindromic nature of the degree-4 characteristic equation, we derive some meaningful sufficient conditions associated to the occurrence of complex roots and a Hopf bifurcation in a two-sector OLG model.","In a very influential contribution, Piketty (2011) shows that inherited wealth has again a prominent role for life-cycle income, especially with respect to human capital and labor income. One explanation is based on the so-called ==== theory of Piketty (2011). Especially, the gap between the (steady-state) growth rate ==== and the rate of return on private wealth ==== might explain the empirical dynamics of bequests.==== Based on the extensive data work of Piketty (2011), particularly with long-dated administrative tax records, two key empirical features are the very pronounced U-shaped pattern of the inheritance (relative to income or wealth) variable in France (Piketty, 2011), Germany (Shinke, 2012), UK (Atkinson, 2018), Sweden (Ohlsson et al., 2020), and Switzerland (Brülhart et al., 2018) and the fact that the annual bequest flow is slowly recovering its level after the First World War.====In this paper, we show that the dynamics of the inheritance flow can be consistent with the predictions of long-run (stochastic) limit cycles and that a Barro-type bequests model can generate such endogenous fluctuations.==== Notably the economy can converge to a stable (long-run) cyclical path where some macroeconomic variables oscillate indefinitely around the steady-state, and thus bequest flows can go back and forth to a low, steady or high level. Moreover, when examined over long periods, historical series of inheritance flows (Piketty, 2011) look part of a long cycle through its U-shaped pattern, especially when extracting its medium to long-run component.==== In a broader perspective, this descriptive fact is to be reconciled with recent papers (e.g., Beaudry et al., 2015, Beaudry et al., 2017, Beaudry et al., 2020 and Growiec et al., 2015, Growiec et al., 2018) that challenge the seminal contributions of Granger (1966) and Sargent (1987): macroeconomic variables do not display (very) pronounced peaks at business cycles frequencies and thus data are not supportive of strong internal boom-bust cycles.==== For instance, Beaudry et al., 2015, Beaudry et al., 2020 show the existence of a recurrent peak in several spectral densities of US trendless macroeconomic data suggesting the presence of periodicities at medium term irrespective of the exogenous cyclical forces. At least their results run counter the empirical irrelevance of endogenous fluctuations.==== We build on these results to reconcile the predictions of our model with empirical evidence.====Looking at long-date inheritance flows data for Sweden (Ohlsson et al., 2020) as well as France, UK and Germany (Piketty, 2011), we first provide a quantitative assessment of the long-run cyclical behavior of bequests as a share of national income. In so doing, we proceed in two steps. First, using the low-frequency methodology of Müller and Watson, 2016, Müller and Watson, 2017 and band pass filters at low frequencies, we extract the long-run component of the inheritance variable of interest and then characterize the business, medium- and long-run fluctuations. Notably, empirical evidence suggest that movements of the inheritance variable are dominated by medium and low frequencies and are characterized by a slow cyclical mean reversion. Second, following Beaudry et al., 2015, Beaudry et al., 2020, we make use of the spectral density to identify peak ranges over some frequency intervals, and thus to provide some support of recurrent (medium and long-run) cyclical fluctuations at that frequency (interval). Moreover, we test the presence of a shape restriction on the spectral density, i.e. the statistical significance of the “peak range” of the spectral density at a given frequency interval with respect to a flat prior. Our results strongly support the view of medium-term business fluctuations. While the presence of a peak range does not necessarily imply strong endogenous cyclical forces, it suggests that data cannot, at least, contradict the existence of endogenous (stochastic) limit cycles.==== We also show that the inheritance flow and the national income experience the same periodicity as that of the inheritance income ratio and this suggests that the cyclical component of both the inheritance ratio and the two corresponding level variables are dominated by medium-term fluctuations of a comparable periodicity.====Based on this empirical assessment, we view the existence of such limit cycles for the inheritance flow (in level or as a share of national income) as a complementary interpretation of Piketty (2011) and thus model and rationalize it by formally characterizing the corresponding complex dynamics.====Capitalizing on Michel and Venditti (1997), we do so through the lens of a two-sector overlapping generation (henceforth, OLG) model with a pure consumption good and one capital good, and a constant population of finitely-lived agents. We consider the decentralized problem where altruistic agents determine their life-cycle consumptions, savings and inheritance to their children. As long as bequests are strictly positive across generations, the optimal solution is described by a dimension-four dynamical system. The degree-4 characteristic polynomial associated to the linearized dynamical system around the steady state appears to have a quasi-palindromic structure. It is then possible to explicitly solve it and provide a complete assessment of its characteristic roots.==== To the best of our knowledge, it is the first available proposition that exploits the quasi-palindromic property of the characteristic equation in the literature of macroeconomic dynamic models.==== This result is the cornerstone of our paper. First, we show that the steady state with strictly positive bequests can be either saddle-point stable or (totally) unstable. In the latter case, endogenous cycles can occur. Second, it is well-known since Benhabib and Nishimura (1979) that the existence of a Hopf bifurcation in models featuring forward-looking agents and a competitive equilibrium structure requires the consideration of at least three sectors and thus of dimension-four dynamical systems.==== However, as shown in several contributions, e.g. Magill, 1977, Magill, 1979a, Magill, 1979b and Magill and Scheinkman (1979), the curse of dimensionality prevents the derivation of meaningful sufficient conditions for the existence of complex characteristic roots.====Our paper shows that only two sectors are sufficient in an OLG economy and it makes one step further to a better understanding of the occurrence of complex roots.==== We provide indeed clear-cut sufficient conditions for the existence of complex roots leading to a Hopf bifurcation, and as far as we know, this is the first time such conditions are exhibited in the literature. Third, we can identify two key mechanisms that lead to quasi-periodic cycles through a Hopf bifurcation. The first one is based on the properties of preferences, and especially the sign of the cross-derivative of the utility function or equivalently the elasticity of intertemporal substitution. The second rests on sectoral technologies through the sign of the capital intensity difference across sectors.====Furthermore, these preference and technology-based mechanisms can either generate endogenous fluctuations independently or self-sustain themselves and thus amplify or mitigate (long-run) limit cycles. In the case of non-strictly concave preferences, mild perturbations of either the capital intensity difference across sectors or of the elasticity of intertemporal substitution lead to a flip bifurcation and thus to persistent period-2 cycles.==== The global dynamics can then be described as the product of two cycles implying complex properties of the optimal path.==== On the one hand, the elasticity of intertemporal substitution must be large enough to allow a substitution effect between the first and second period consumptions while satisfying the standard transversality conditions and the convergence towards the period-two cycle. On the other hand, the consumption good sector needs to be capital intensive to generate fluctuations of the capital stock.====In contrast, these two mechanisms can no longer be separated in the case of strictly concave preferences, and a Hopf bifurcation can occur with quasi-periodic cycles under some intermediate and plausible values for the elasticity of current consumption, the elasticity of intertemporal substitution and an upper threshold condition for the sectoral elasticities of capital-labor substitution.==== After providing conditions for the existence of strictly stationary bequests, we argue that both bequests and bequests as a share of GDP can be characterized by optimal periodic and quasi-periodic cycles. This sharply contrasts with the predictions of the standard Barro model in which the optimal path monotonically converges toward the steady state if the life-cycle utility function of a representative generation living over two periods is additively separable.====These results provide a theoretical support for our empirical findings of Section 2 suggesting the existence of medium-term business cycles of inheritance. They also have strong policy implications. Indeed, in contrast to Piketty (2011) where the condition ==== leading to an inherently unstable behavior of the distribution of wealth requires a tax on bequest to avoid such a pattern, the recent rise in property income can be interpreted as an optimal phenomenon that may reverse automatically later on. The rise of inequality can then be only temporary and may not call for any redistribution policy and bequest taxation.====The paper is organized as follows. Section 2 discusses the empirical relevance of interpreting the dynamics of inheritance flows (in level or as a share of national income) of long-dated Sweden and France annual data as a long endogenous cycle. Section 3 presents the two-sector model with non-additively separable preferences, defines the decentralized solution with altruistic agents, proves the existence of a steady state with positive bequests, and finally derives the characteristic polynomial from which the stability analysis is conducted. The existence of endogenous cycles is discussed in Section 4 considering successively period-two cycles under the assumption of a non-strictly concave utility function and quasi-periodic cycles under a Hopf bifurcation with a strictly concave utility function. In Section 5 we consider a general class of homogeneous preferences and sectoral Cobb–Douglas technologies to illustrate all our main results. Concluding comments are provided in Section 6 and all the proofs are contained into a final Appendix.",On the long-run fluctuations of inheritance in two-sector OLG models,https://www.sciencedirect.com/science/article/pii/S030440682200026X,15 March 2022,2022,Research Article,39.0
Sun Chaoran,"School of Finance, Shanghai University of International Business and Economics, Shanghai, 201620, China","Received 13 June 2021, Revised 23 January 2022, Accepted 2 March 2022, Available online 15 March 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102686,Cited by (0),We introduce two mechanisms that implement the ,"Game theory is traditionally divided into two branches, cooperative game theory and non-cooperative game theory. Cooperative game theory focuses on the possible payoffs that players may obtain in a game, taking into account the worth of the coalitions of players and abstracting from the actions or decisions that may lead to these payoffs. It often adopts an axiomatic or normative approach to characterize solution concepts: it sets up a number of normative goals and derives their logical implications. By contrast, a non-cooperative game makes each player’s actions and strategies explicit. A player in a non-cooperative game is obliged to choose an action or strategy knowing that the final outcome hinges not only on his own choice, but also on his opponents’ choices. Non-cooperative game theory adopts a strategic approach.====The previous description manifests a gap between the two branches of game theory. To bridge this gap, Nash (1953) initiated the so-called Nash program: assigning each cooperative game with an extensive-form game such that a given cooperative solution of the former coincides with some non-cooperative solution of the later. Since then, the Nash program has been, and still is, pursued by many authors, and has grown into a large body of literature.==== One of the most important themes in this literature is finding non-cooperative games that lead to the Shapley value of coalitional games as their subgame perfect Nash equilibrium outcome. Notable examples of this theme include Gul (1989), Hart and Mas-Colell (1996), and Pérez-Castrillo and Wettstein (2001). Moreover, some mechanisms proposed in these papers are compatible with the theory of implementation because their rules do not depend on the characteristic function, which only the players are supposed to know.====In this study, we design “natural” non-cooperative games, where players have equal possibilities to propose and to reject offers. At the beginning of the mechanisms, all players are “insiders” (we call them “proposers”), but as the games proceed, some of them may be bought out. The remaining proposers will try to reach a consensus, and if a consensus is not reached, then one of them will be bought out so that the others can try to reach a consensus in the next stage. The inspiration for our mechanisms comes from the phenomenon in corporation management practice of some shareholders buying out those with misaligned interests in order to achieve a consensus among shareholders.==== To facilitate a buyout, we use Pérez-Castrillo and Wettstein’s (2001) (====) multibidding procedure (the PW procedure for short) in which each player submits a bid against every other player and the player with the highest total net bid is selected. However, in contrast to the PW setting, we endow every player with the right to propose allocation plans simultaneously at the beginning, and whenever there is a disagreement, the PW procedure selects the proposer with the lowest total net bid.====Thus, at each stage of our mechanisms, players have the possibility of reaching an agreement, which requires that all the proposals are identical. In case of a disagreement among proposers, one proposer, who is bought out by the PW procedure, will receive the submitted bids from the other players, and his role will change to a respondent. This buyout process will continue until the proposers reach a consensus. The players who are bought out may rejoin the coalition of proposers by accepting the consensual allocation plan. It is worth mentioning that renegotiation, as a way of incentivizing the players to form the grand coalition, is realistic, and has appeared in Ju and Wettstein (2009), Ju (2012), and Borm et al. (2015).====We note that the previous description leaves the procedure of how respondents address a consensual allocation plan unspecified. We consider two different specifications. In the first specification, a representative will be selected to accept or reject the plan on behalf of all respondents. In this case, the respondents’ bargaining power is vested in one of them. In the second specification, each respondent will decide to accept or reject the plan for himself. In this case, the respondents’ bargaining power is dispersed among them. Our main results are that the mechanism with the first specification (====) implements the Shapley value, whereas the mechanism with the second specification (====) implements the equal surplus value.====The Shapley value and the equal surplus value have been compared using the aforementioned axiomatic and strategic approaches. Using the axiomatic approach, Casajus and Huettner (2014) pinned down the difference between the Shapley value and the equal surplus value by one out of four axioms. Roughly speaking, the Shapley value of a player who never collaborates with others is equal to his individual rational payoff, while the equal surplus value of a player who prevents collaboration within his coalition is equal to his individual rational payoff. Using the strategic approach, Ju and Wettstein (2009) pinned down the difference between them by constructing two mechanisms differing in the choice of who makes an offer between the rejected proposer and the proposer. Our paper provides a new comparison of the two values based on the strategic approach.====A remarkable by-product of Pérez-Castrillo and Wettstein’s (2001) result is revealing a close connection between the Maschler and Owen’s (1989) recursive formula of the Shapley value and the PW bidding mechanism. Similarly, we show that our two mechanisms are associated with two new recursive formulae, respectively. It should be noted that the underlying recursive formula of the Mechanism A can be derived from the Maschler and Owen’s formula by the self-duality of the Shapley value in response to Ju’s (2012) suggestion that the self-duality of the Shapley value be explored in the future research of the Nash program.====Relatedly, this paper can also be seen as an exploration of “dualizing” the original PW mechanism and the like, including mechanisms in Ju and Wettstein (2009) and Ju (2012) (we call them PW-alike mechanisms). The “duality” between PW-alike mechanisms and ours is explicit in four aspects: First, no player has the right to propose at the start of PW-alike mechanisms, while every player has the right to propose at the start of our mechanisms. Second, a player with the highest total net bid is selected in PW-alike mechanisms, while a player with the lowest total net bid is selected in our mechanisms. Third, the selected player pays his submitted bids to the rest of players in PW-alike mechanisms, while the selected player receives the submitted bids from the rest of players in our mechanisms. Fourth, the selected player will be granted the right to propose in PW-alike mechanisms, while the selected player’s right to propose will be forfeited in our mechanisms. In fact, we can make this “duality” between them precise via a concept of value-free reduction introduced by Pérez-Castrillo and Sun (2021). In brief, they introduced an operator, which is called “value-free reduction”, that maps a TU game to a “similar” TU game played by a subset of players. They also provided a duality theory for value-free reductions, which is built upon the familiar concept of dual TU game. In our context, associating a mechanism with a value-free reduction needs three steps: first, apply this mechanism to a TU game, which yields an extensive-form game; second, take a subgame of this extensive-form game, which is essentially played by a subset of players; third, reverse this subgame to a TU game played by this subset of players. As shown by Pérez-Castrillo and Sun (2021), the value-free reductions of the original PW mechanism and our Mechanism A are dual to each other.====Broadly speaking, our mechanisms belong to a class of mechanisms using the PW procedure. In their original PW mechanism, one proposer is selected using the PW procedure. This proposer has the right to put forth an allocation plan which will be implemented only if the rest of players unanimously accept. Otherwise, the proposer drops out of the game and remains alone. Several papers have considered variants of the PW mechanism where rejected proposers are granted a second chance to rejoin the coalition of active players. Ju and Wettstein (2009) considered granting only the latest rejected proposer this option. Depending on who makes the offer in the renegotiation, they constructed three mechanisms that implement the Shapley value, the equal surplus value, and the consensus value (Ju et al., 2007), respectively. Ju (2012) allowed all the rejected proposers’ return and constructed three mechanisms that all implement the Shapley value. Another line of research generalizes the PW mechanism to different environments. Pérez-Castrillo and Wettstein (2005) constructed a mechanism for pure exchange economies to implement the Pérez-Castrillo and Wettstein’s (2006) ordinal Shapley value for three or less players. Macho-Stadler et al. (2006) proposed two mechanisms for TU games with positive externalities and negative externalities which implement two distinct generalizations of the Shapley value, respectively. Slikker (2007) presented three mechanisms for the Jackson and Wolinsky’s (1996) network allocation problem which implement the Myerson value, the position value and the componentwise egalitarian rule, respectively. Different from our mechanisms, all the mechanisms mentioned above use the PW procedure to select a single proposer each round.====The rest of this paper is organized as follows. Section 2 provides definitions and proves useful results for the Shapley value and the equal surplus value. Section 3 constructs a mechanism and shows that it implements the Shapley value in every subgame perfect Nash equilibrium (====). Section 4 constructs a comparable mechanism and shows that it implements the equal surplus value in SPNE. Section 5 concludes the paper with a discussion on possible variations and extensions.",Bidding against a Buyout: Implementing the Shapley value and the equal surplus value,https://www.sciencedirect.com/science/article/pii/S0304406822000416,15 March 2022,2022,Research Article,40.0
"Chatterji Shurojit,Roy Souvik,Sadhukhan Soumyarup,Sen Arunava,Zeng Huaxia","School of Economics, Singapore Management University, Singapore,Economic Research Unit, Indian Statistical Institute, Kolkata, India,Department of Computer Science and Automation, Indian Institute of Science, India,Economics and Planning Unit, Indian Statistical Institute, Delhi Center, India,School of Economics, Shanghai University of Finance and Economics, and the Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai 200433, China","Received 25 May 2021, Revised 1 January 2022, Accepted 25 January 2022, Available online 12 February 2022, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2022.102656,Cited by (4),"We study a class of preference domains that satisfies the familiar properties of minimal richness, diversity and no-restoration. We show that a specific preference restriction, ====).","Our goal in this paper is two-fold. The first is to investigate a class of preference domains by means of some simple and well-known axioms, without specifying any preference restriction. We call such domains ==== since, loosely speaking, the preferences of these domains are revealed to be single-peaked at the “extremes” and unrestricted in the “middle”. Our second objective is to comprehensively analyze the class of unanimous and strategy-proof random social choice functions (RSCFs) defined on these domains. We refer to these RSCFs as ====.====Two familiar preference domains in the literature on mechanism design in voting environments are the complete domain and the domain of single-peaked preferences. The complete domain arises when there are no a priori restrictions on preferences (see Gibbard (1973), Satterthwaite (1975) and Gibbard (1977)). Single-peaked preferences on the other hand, require more structure on the set of alternatives. They arise naturally in a variety of situations such as preference aggregation (Black, 1948), strategic voting (Moulin, 1980), public facility allocation (Bochet and Gordon, 2012), fair division (Sprumont, 1991, Barberà et al., 1997a) and object assignment (Bade, 2019).====In order to discuss hybrid domains, we briefly recall the definition of single-peaked preferences. The set of alternatives is a finite set ==== which is endowed with the prior order ====. A preference ordering over ==== is ==== if there exists a top-ranked alternative, say ====, such that preferences decline when alternatives move “farther away” from ====. For instance, if “==== or ====” , then ==== is strictly preferred to ====. A preference is ==== if there exist ==== alternatives ==== and ==== with ==== such that preferences over the alternatives in the interval between ==== and ==== are “unrestricted” relative to each other, while preferences over other alternatives retain features of single-peakedness. Thus, the set ==== can be decomposed into three parts: left interval ====, right interval ==== and middle interval ====. Formally, a preference is ====-==== if the following holds: (i) for a voter whose best alternative lies in ==== (respectively in ====), preferences over alternatives in the set ==== are conventionally single-peaked, while preferences over alternatives in ==== are arbitrary subject to the restriction that the best alternative in ==== is the left threshold ==== (respectively, the right threshold ====), and (ii) for a voter whose peak lies in ====, preferences restricted to ==== are single-peaked but arbitrary over ====. Observe that if ==== and ====, then preferences are unrestricted, while the case where ==== coincides with the case of single-peaked preferences.====Hybrid preferences can plausibly arise when an inherently multi-dimensional model is embedded in a one-dimensional model. For instance, to address the structure of preferences over political parties according to different aspects of a party’s platform, Reffgen (2015) adopts the value-restricted preferences of Sen (1966) to introduce a domain of ====, and shows that the domain degenerates to a union of single-peaked domains w.r.t. multiple distinct prior orders. Indeed, Reffgen’s multidimensional (locally) single-peaked preference is a special case of our hybrid preferences. As a second instance, in Section 3, we provide an example to show that a domain of ==== introduced by Barberà et al. (1993) reduces to a domain of our ====-hybrid preferences in the framework of voting under constraints studied in Barberà et al. (1997b).====Hybrid preferences also arise in the public facility allocation environment. Imagine a transportation system in a city/region which consists of two suburban zones on “opposite” sides of a central urban zone. The central urban zone has a well-connected transportation system. It has two hubs on its boundaries, each of which is connected to a proximate suburban zone via a unique railway line. Consider the location of a good public facility in the city. Each citizen naturally prefers the public facility being located closer to his/her own location. Thus, a citizen living in a suburban zone has a single-peaked preference on locations along the railway line towards its hub, prefers the location of the hub to any other location in the central urban zone, and has a single-peaked preference on locations along the railway line towards the other suburban zone. A citizen living in the central urban zone can have arbitrary preferences over all locations in the central zone due to the denseness of the transportation system. She also prefers a location along a suburban railway line that is closer to its hub. All these preferences can be expressed in terms of ====-hybridness according to the prior order ====, where ==== and ==== are the “remotest” suburban locations and the thresholds ==== and ==== are the two hubs.====Observe that the domain of ==== ====-hybrid preferences is the unrestricted domain. It follows that ==== domain is a subset of ==== ==== (i.e., the domain of ==== ====-hybrid preferences) for some ====.==== One of our main results (Theorem 1) is that every domain that satisfies three familiar axioms must be ==== ==== for some ====, i.e., a domain of ====-hybrid preferences that ====, satisfies two special properties in terms of the ==== introduced in Chatterji et al. (2013). The details of the two properties are described in Section 3. We note here that the domain of all ====-hybrid preferences is of course a ====-hybrid domain, and the domain of single-peaked preferences is a ====-hybrid domain for all ====. The multiple single-peaked domain introduced by Reffgen (2015) is also included in the class of our hybrid domains, but the semi-single-peaked domain of Chatterji et al. (2013), is not.====The three familiar axioms that we impose are ====, ==== and ====. The first axiom requires every alternative be first-ranked by some preference in the domain. The second requires the existence of two completely reversed preferences in the domain. The third axiom requires that for each given pair of preferences and each given pair of alternatives, there exists a path from one preference of the given pair to the other in the domain, by a sequence of specific preference switches, such that the relative ranking of the given pair of alternatives is switched at most once along the path. The first two axioms focus on the richness of a domain in question, while the third one not only is concerned with the richness of the domain (the existence of a path in the domain that reconciles the difference between two preferences), but also ensures that all preferences of the domain be well organized; for instance, if two given alternatives are identically ranked in two given preferences, they must be identically ranked in every preference along one path connecting the two given preferences.==== No-restoration has been shown to be a key property for the equivalence between local and global strategy-proofness — see Sato (2013), Kumar et al., 2021b, Kumar et al., 2021a and Hong and Kim (2020). Moreover, no-restoration is also shown to be a sufficient condition that endogenizes the tops-only property in all unanimous and strategy-proof RSCFs — see Chatterji and Zeng (2018) and Kumar et al. (2021a). Related conditions on domains such as ==== have been used extensively in the literature on Condorcet domains (e.g., Monjardet, 2009, Puppe, 2018, Puppe and Slinko, 2019). Unlike the restricted domains studied in the literature, the axiom of no-restoration does not exogenously specify any explicit preference restriction on the domain; for instance, a no-restoration domain can be as permissive as the unrestricted domain, or be as restrictive as a single-peaked domain or a single-crossing domain of Saporiti (2009) or Carroll (2012). Therefore, no-restoration creates a unified framework to cover some important domains in the literature, and our Theorem 1 reveals the key common feature, hybridness, satisfied by all these domains. An important feature of Theorem 1 is that the condition on the domain does not specify an underlying structure of single-peakedness or threshold alternatives. These are derived endogenously from our hypotheses.====The paper also investigates the structure of unanimous and strategy-proof RSCFs on regular domains, i.e., domains satisfying the three aforementioned familiar axioms. A RSCF associates a lottery over alternatives to each profile of preferences. Randomization is a way to resolve conflicts of interest by ensuring a measure of ex-ante fairness in the collective decision process. More importantly, it has recently been shown that randomization significantly enlarges the scope of designing well-behaved mechanisms, e.g., the compromise RSCF of Chatterji et al. (2014) and the maximal-lottery mechanism of Brandl et al. (2016).====In order to define the notion of strategy-proofness, we follow the standard approach of Gibbard (1977). For every voter, truthfully revealing her preference ordering must yield a lottery that stochastically dominates the lottery arising from any unilateral misrepresentation of preferences according to the sincere preference. Unanimity is a weak efficiency requirement which says that the alternative that is unanimously best at a preference profile is selected with probability one.====According to Theorem 2, a RSCF defined on a regular domain, which is further revealed to be a ====-hybrid domain, is unanimous and strategy-proof if and only if it is a ==== (or PFBR). A PFBR is specified by a collection of probability distributions ====, where ==== is a coalition of voters, over the set of alternatives. We formally call ==== a ====. If ====, then a PFBR reduces to a ==== that is originally introduced by Ehlers et al. (2002). However, if ====, then a PFBR requires an additional restriction on the probabilistic ballots: each voter ==== has a fixed probability weight ==== such that the probability of the right interval ==== according to ==== is the total weight ==== of the voters in ==== and that of the left interval ==== is the total weight ==== of the voters outside ====. Recall the equivalence between local and global strategy-proofness and the tops-only property related to the no-restoration domains. It is worth mentioning that Theorem 2 decodes that these two important properties emerge from the class of PFBRs on a no-restoration domain that in addition satisfies minimal richness and diversity, and more importantly provides a clear, operable and exhaustive guideline for a social planner’s task of designing strategy-proof mechanisms.====The paper is organized as follows. Section 1.1 reviews the literature, while Section 2 sets out the model and no-restoration domains. Sections 3 Hybrid domains and their salience, 4 Probabilistic fixed ballot rules and a characterization introduce hybrid preferences, PFBRs and the characterization results, while Section 5 concludes.",Probabilistic fixed ballot rules and hybrid domains,https://www.sciencedirect.com/science/article/pii/S0304406822000167,12 February 2022,2022,Research Article,41.0
"Bosi Stefano,Le Van Cuong,Pham Ngoc-Sang","EPEE, Université Paris-Saclay, France,PSE, France,CNRS, France,TIMAS, Vietnam,CASED, Vietnam,EM Normandie Business School, Métis Lab, France","Received 5 February 2021, Revised 18 December 2021, Accepted 2 February 2022, Available online 10 February 2022, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2022.102651,Cited by (1),"We show that both real indeterminacy and asset price bubble may appear in an infinite-horizon ==== with infinitely lived agents and an imperfect financial market. We explain how the asset structure and heterogeneity (in terms of preferences and endowments) affect the existence and the dynamics of asset price bubbles as well as the equilibrium indeterminacy. Moreover, this paper bridges the literature on bubbles in models with infinitely lived agents and that in overlapping generations models.","The existence and dynamics of asset price bubbles are one of the fundamental questions in economics and finance. According to the classical paper by Santos and Woodford (1997), conditions under which bubbles exist are relatively fragile. After the global financial crisis of 2007–2009, this topic has regained momentum and different new mechanisms of bubbles have been proposed.==== To date, the literature on rational asset price bubbles has focused on two frameworks: (1) overlapping generations models (OLG) and (2) infinite-horizon general equilibrium models with infinitely lived agents. Note that since the influential paper of Tirole (1985), numerous studies have privileged OLG models to study the existence of bubbles and their macroeconomic implications. Although it is also important to study infinite-horizon models of bubbles,==== this type of framework has received relatively less attention.==== As recognized by Kocherlakota, 2008, Miao, 2014 and Martin and Ventura (2018), our understanding of bubbles in infinite-horizon models is far from complete.====The present paper aims to address basic and open questions about rational asset price bubbles in intertemporal competitive equilibrium: Why do asset price bubbles exist in equilibrium? What is the connection between the existence of bubbles on the one hand and the economic agents’ consumption and trade on the other? How do the existence and dynamics of the asset price bubble depend on asset structure and economic fundamentals such as endowments?====To answer these questions, we consider an infinite-horizon general equilibrium model with a finite number of agents, where there are one consumption good and one financial asset as Lucas’ tree (Lucas, 1978). Our model has two key ingredients: first, agents are heterogeneous in terms of endowments and preferences; and second, there exist financial frictions in the form of short-sale constraints, i.e., the asset quantity that each agent can buy does not exceed an exogenous limit. As in Tirole, 1982, Kocherlakota, 1992 and Santos and Woodford (1997), given an equilibrium, we say that there exists a bubble in this equilibrium if the equilibrium asset price exceeds the fundamental value of the asset, defined as the present value of dividend streams. An equilibrium with (resp., without) bubble is said to be bubbly (resp., bubbleless).====Our contribution is three-fold. First, we provide new necessary conditions for the existence of bubbles in equilibrium. The literature on bubbles in infinite-horizon models shows several conditions ruling out asset price bubbles. Kocherlakota (1992) questioned the relationship between the existence of bubbles and borrowing constraints. He pointed out that, in the presence of bubbles, the limit infimum of the differences between asset holding and borrowing limit equals zero. We go further by proving that in any equilibrium with bubbles, there exist at least two agents whose borrowing constraints bind (i.e., asset holding equals borrowing limit) at infinitely many dates and whose assets holdings fluctuate over time. Moreover, if borrowing constraints of an agent never bind, the existence of bubble requires that the asset holding of this agent must converge to the borrowing limit.====Another famous no-bubble condition, given by Theorem 3 in Santos and Woodford (1997), states that, under mild conditions, bubbles are ruled out if the present value of aggregate endowments is finite. This condition still holds in a model with debt constraints (Werner, 2014) and in a model with land and collateral constraints (Bosi et al., 2018b). In our model with short-sale constraints, we also obtain this no-bubble condition (see Corollary 3).====Motivated by the fact that most of the no-bubble conditions are based on endogenous variables, we contribute to the literature by providing conditions based on fundamentals. The first condition (see Corollary 2) shows the role of the borrowing limits: there is no equilibrium with bubbles if borrowing limits are high enough. The second condition (see Proposition 3) shows the role of impatience: under the assumption of uniform impatience, there is no bubble if agents strongly prefer the present. The intuition is simple: if agents strongly prefer the present, they do not buy the asset in the long run, ruling out bubbles. In particular, this is the situation in finite-horizon models in which no one buys the asset in the last period eliminating the possibility of bubbles.====Our second contribution concerns the construction of models with bubbles where we can explicitly characterize the existence and the dynamics of bubbles by using fundamentals such as agents’ endowments, borrowing limits, asset dividends, and asset supply.====The above no-bubble conditions suggest us to focus on a two-agent model and characterize the equilibrium in which borrowing constraints of both agents bind infinitely many dates (more precisely, the first agent’s borrowing constraint will bind, for instance, at even dates and that of the second agent at odd dates). Focusing on such equilibrium, we find that bubbles are ruled out if (1) borrowing limits of agents are high enough and (2) the value of endowments (discounted by using the interest rates of ====) of the agent who buys asset vanishes in the infinity (see Proposition 4). By consequence, there cannot exist a bubble if the benchmark interest rates are high. The basic idea is that asset buyers’ income must be high enough so that they are willing to buy the asset, even when the asset price exceeds the fundamental value.====Notice that this condition concerning the benchmark interest rates is based on fundamentals and cannot be obtained from the famous condition in Santos and Woodford (1997), which is based on endogenous variables. Our finding can be viewed as an extension of the no-bubble condition of Tirole (1985) in an OLG model==== to our general equilibrium model with infinitely lived agents. In this sense, our paper is the first to create the connection between the no-bubble conditions in Tirole (1985) and those in infinite-horizon general equilibrium models. Recall that Tirole, 1985, Farhi and Tirole, 2012 need the convergence of interest rates of the economy without asset while we do not require such convergence.====In the existing literature, there are some examples of bubbles in general equilibrium models with infinitely lived agents.==== Concerning the asset having zero dividends and positive supply (i.e., fiat money), Bewley (1980) (Section 13) , Townsend, 1980, Kocherlakota, 1992 (Example 1) and Scheinkman and Weiss (1986) show in infinite-horizon general equilibrium models, that, when borrowing is not allowed, there exists an equilibrium in which fiat money’s price is strictly positive. Santos and Woodford (1997) present several examples of this kind of bubbles: their examples 4.1, 4.2 study fiat money in deterministic models while their example 4.4 investigates fiat money in a stochastic model. Hirano and Yanagawa (2017) also give sufficient conditions for the existence of stochastic bubbles of an asset without dividend. There are a few examples of bubbles of assets with positive dividends. In a deterministic set-up, Example 4.3 in Santos and Woodford (1997) studies bubbles of an asset with positive dividends but with zero net supply. Like us, Example 4.5 in Santos and Woodford (1997) also investigates bubbles of the Lucas’ tree, although they use a stochastic model with a single representative household.==== Recently, Le Van and Pham (2016) and Bosi et al., 2017, Bosi et al., 2018b show that bubbles of assets with positive dividends and positive net supply may appear even in deterministic models. Bloise and Citanna (2019) provide a sufficient condition based on trade and punishment for default for the existence of the bubble of an asset with vanishing dividends of an equilibrium whose sequence of allocations converges.====To date, no example shows how the existence and the dynamics of asset price bubbles depend on fundamentals such as endowments and the asset structure (dividends, asset supply, and borrowing limits). Our paper contributes to fill this gap. More precisely, Section 4 of the present paper provides several conditions (based on fundamentals) for the existence of bubbles. Notice that this task is not easy because studying equilibrium requires us to work with a dynamical system that has infinitely many parameters (which are our model’s fundamentals and dividends). We point out that: when the benchmark economy has low interest rates and verifies the seesaw property, the following factors promote bubble in equilibrium: (1) asset supply is low, (2) borrowing limits of agents are low, (3) the level of heterogeneity (proxied by the differences between agents’ fundamentals such as endowments, initial asset holdings, rates of time preferences) is high, and (4) asset dividends are low with respect to agents’ endowments. Consequently, our results suggest that bubbles may appear if (i) the agents’ endowments grow asymmetrically, and (ii) there is a shortage of financial assets (i.e., there is a low supply and assets provide low dividends).==== We also prove that bubbles may not exist if one of these four conditions is not satisfied.====Let us explain the basic mechanism of asset price bubbles in our model. In each period, there is at least one agent who really needs to save by buying the asset. When the asset supply and borrowing limits are low, the asset price would be high (even higher than its fundamental value) because it is the only way allowing this agent to smooth consumption.====To the best of our knowledge, we are the first to show that there is a continuum of bubbly equilibria (with real indeterminacy) in models with infinitely lived agents and with assets having positive supply and possibly positive dividends. With additional specifications, we can further provide a complete characterization of the set of equilibria with bubbles, and compute the bubble component as a function of fundamentals (see Proposition 6). In our models of bubbles, the asset price may converge to any value in ==== or may fluctuate over time, depending on the fundamentals’ properties. Furthermore, here the existence of bubbles does not violate individual transversality conditions (TVC, henceforth). Notice that individual TVC ensures the optimality of agents’ choices and always holds in equilibrium while bubbles may exist or be ruled out.====Our third contribution is to clarify the relationship between the existence of bubble and real indeterminacy. The equilibrium indeterminacy in our model is global, and no local approximation is invoked to prove this indeterminacy. Our proof relies on the fact that asset prices, in some cases, can be recursively computed. Hence the sequence of prices can be computed as a function of the initial price. Therefore, at the initial date, any value can be an equilibrium price if it is low enough so that the price and the bubble component will be not too high in the future, ensuring that agents can buy them. As a result, there may be a continuum of asset prices and a continuum of equilibrium trajectories. Notice that we require neither the convergence of these trajectories nor the existence of a steady-state. So, the indeterminacy in our model is quite different from the concept of dynamic indeterminacy in macroeconomics (see Benhabib and Farmer, 1999, Farmer, 2019 for surveys on this issue). Our result on indeterminacy complements the findings in Kehoe and Levine (1985) and Kehoe et al. (1990) who show that in a general equilibrium model with a finite number of infinitely lived consumers and with complete financial markets, equilibria are generically determinate.==== Unlike them, we introduce financial frictions and prove that equilibria may be generically indeterminate. Moreover, the real indeterminacy in our model is associated with the existence of bubbles.====The rest of the paper is organized as follows. Section 2 presents the framework and provides the fundamental properties of equilibrium. Section 3 provides no-bubble conditions. A number of models with bubbles and real indeterminacy are presented in Section 4. Finally, Section 5 concludes and mentions future works. Technical proofs are gathered in the appendices.",Real indeterminacy and dynamics of asset price bubbles in general equilibrium,https://www.sciencedirect.com/science/article/pii/S0304406822000131,10 February 2022,2022,Research Article,42.0
"Ma Qingyin,Stachurski John,Toda Alexis Akira","International School of Economics and Management, Capital University of Economics and Business, China,Research School of Economics, Australian National University, Australia,Department of Economics, University of California San Diego, United States of America","Received 13 October 2021, Revised 21 January 2022, Accepted 2 February 2022, Available online 10 February 2022, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2022.102652,Cited by (1),"We propose a new approach to solving dynamic decision problems with unbounded rewards based on the transformations used in Q-learning. In our case, however, the objective of the transform is not learning. Rather, it is to convert an unbounded dynamic program into a bounded one. The approach is general enough to handle problems for which existing methods struggle, and yet simple relative to other techniques and accessible for applied work. We show by example that a variety of common decision problems satisfy our conditions.","Dynamic programming forms the backbone of modern economics. Every year, thousands of students in graduate programs around the world learn the standard methodology for infinite horizon problems with discounting. Constructed primarily by Blackwell, 1962, Blackwell, 1965, this theory uses contraction mappings over spaces of bounded functions metrized by the supremum norm. The approach is elegant, powerful in terms of deriving theoretical results and, when applicable, generates globally convergent algorithms. Standard textbook treatments can be found in Stokey et al. (1989) and Bertsekas (2017).====Unfortunately, the approach is rarely applicable in most concrete economic problems. This is due to the fact that the majority of reward functions used in applications are unbounded. For example, in quantitative work, the most commonly used flow utility function is the constant relative risk aversion (CRRA) specification ====where ==== is the risk aversion coefficient. The function ==== is unbounded above if ====, is unbounded below if ====, and is unbounded both from above and below if ====. Unbounded reward functions violate Blackwell’s conditions.====The need to deal with unbounded reward functions has led researchers to build various extensions of Blackwell’s theory. These extensions typically involve either (a) recovering contractivity by modifying the metric that measures distance between candidate value functions, or (b) introducing a weaker form of contractivity that preserves at least some of Blackwell’s optimality results. The former approach is exemplified by the weighted supremum norm method, introduced by Wessels (1977) and applied to economic problems by Boyd III (1990), Alvarez and Stokey (1998) and Bäuerle and Jaśkiewicz (2018) and several other authors.==== The second approach can be seen in the work of Rincón-Zapatero and Rodríguez-Palmero (2003) and Martins-da-Rocha and Vailakis (2010) for the deterministic case and Matkowski and Nowak (2011) for the stochastic case, who apply local contractions on successively larger subsets of the state space.====While many of these techniques are ingenious, their direct impact on quantitative applications in economics has been limited. Weighted supremum norms work well with certain problems but struggle with others, such as when rewards are unbounded below.==== Local contraction methods are broadly applicable under reasonable assumptions but require verifying technical conditions involving increasing sequences of compact sets that exhaust the state space.==== Proofs of convergence properties are more complex than the bounded case, and the statements of the theorems are more challenging to interpret.====In this paper we take an alternative route. Rather than transforming the standard contraction mapping theory of Blackwell to handle unbounded dynamic programs, we transform the unbounded dynamic programs into bounded ones so that standard contraction mapping theory can be applied. The transformation that we use maps value functions into the “action-value” functions used in Q-learning, a popular reinforcement learning algorithm that allows online updating by a controller in an incremental fashion (see, e.g., Watkins and Dayan (1992) or Szepesvári (2010)). It has been shown that the algorithm has strong global convergence properties and our results are in this spirit.====The core idea is as follows: In standard dynamic programs, the Bellman operator is defined by composing the following two operations: given a candidate value function ====, (a) compute the discounted expectation ==== over current states and actions, and (b) add current reward ====, maximize over current feasible actions, and then update the candidate value function as ====. Instead of aiming at updating ====, one can transform the Bellman operator by applying operations (b) ==== (a), which is the Q-transform, and focus on updating the candidate “action-value” function ====. We show that, under relatively weak and easily testable conditions, the transformed Bellman operator is a contraction with unique fixed point ==== that is bounded, and the true value function ==== can be recovered as ====.====One advantage of the transformation-based approach to unbounded programs adopted in this paper is that the methodology fits well with the case where the weighted supremum norm approach struggles: maximization problems where rewards are unbounded below (see Footnote 2). Such optimization problems are commonplace in quantitative applications, such as those involving CRRA flow utility with ====, which is the empirically relevant case. We show that, for many canonical applications from this class of problems, the Q-transform converts unbounded value functions into bounded action-value functions. Standard contraction mapping theory can then be applied.====A second advantage of the transform method is that it has no difficulty handling stochastic dynamic programs. In fact, the action-value functions associated with the transform tend to be well-behaved and regular in the stochastic case, due to the fact that conditional expectations operators have a smoothing effect on functions. Therefore, it requires less restriction on the primitive setup compared with many existing methods. In contrast, many existing papers on unbounded dynamic programming either focus on the deterministic case or restrict the support of shocks.====A third advantage is that, despite its generality, the transform approach is accessible to a general audience. In particular, it relies mainly on the standard contraction mapping theorem, and can be conveniently generalized to handle dynamic programs where rewards are both unbounded above and unbounded below. We provide many canonical examples such as optimal savings, optimal default, job search, and optimal portfolio, all with unbounded rewards and in stochastic environments.====On a technical level, the contribution of our paper is twofold. First, we identify general sufficient conditions under which unbounded dynamic programs can be transformed into bounded ones. Second, we prove that, when such a transformation is available, the solution to the transformed problem is equal to that to the original problem.====There are connections between our work and the study of unbounded dynamic programming in Kamihigashi (2014). Assuming the Bellman operator maps an order interval of functions into itself and some transversality-like conditions hold, Kamihigashi (2014) shows the existence and uniqueness of the fixed point of the Bellman equation and obtains optimality properties. The relative advantages of the approach presented here include treating stochastic decision problems (Kamihigashi (2014) restricts attention to the deterministic case) and obtaining uniform geometric rates of convergence in value function iteration, rather than pointwise convergence.====Our work is also related to results in Van Der Wal (1980) and Jaśkiewicz and Nowak (2011), which explicitly admit problems with rewards that are unbounded below. In this setting, Jaśkiewicz and Nowak (2011) show that the value function of a Markov decision process is a solution to the Bellman equation. The methodology developed here strengthens their results by adding uniqueness and proving that value function iteration leads to an optimal policy. In an extension section, we combine our methodology with the weighted supremum norm approach, allowing us to handle problems that are both unbounded above and unbounded below.====Moreover, Feinberg et al. (2012) study a cost-minimization Markov decision problem with bounded below but potentially unbounded above one-step cost functions. They impose relatively mild assumptions on the feasible correspondence and establish existence of optimal policies and convergence of value iterations. A parallel result for Markov decision problems with setwise continuous transition probabilities is developed by Feinberg and Kasyanov (2021).==== The relative advantages of our Q-transform method over their methods are that it establishes contraction mapping and geometric convergence properties, and serves a somewhat broader class of dynamic programs, including those with risk-sensitive preference and more general reward functions.====Some studies have approached dynamic programming with unbounded rewards via an Euler equation method, as seen for example in Kuhn (2013) and Ma et al. (2020). This methodology can be powerful but is limited in scope. For example, in Section 4, we show how the Q-transform method can be applied to the kinds of optimal savings problem with endogenous labor choice that are common in both theoretic and applied works (see, e.g., Castañeda et al. (2003) and Zhu (2020)). The Euler equation method of Kuhn (2013) and Ma et al. (2020) is not applicable in this setting because the choice variable (consumption and labor) is multi-dimensional. Similarly, the method of Euler equation is not applicable in the optimal savings problem in Section 4.5, due to nontrivial portfolio choice. Optimal consumption–portfolio problems have been mostly studied in the literature under special homogeneity assumptions (Samuelson, 1969, Toda, 2014) or finite horizon (He and Pearson, 1991). Our framework shows that the problem can be studied in an infinite-horizon environment when the utility function is unbounded below.====The transform method is not limited to dynamic programs that are additively separable. In a recent paper, Bäuerle and Jaśkiewicz (2018) study an optimal growth model in the presence of risk-sensitive preference, in which the agent is risk averse in future utility (in addition to being risk averse in future consumption).==== They provide valuable optimality results, although these results cannot treat many common period utility functions, such as CRRA with relative risk aversion at least one or logarithm utility, because they exclude all utility functions that are unbounded below. Furthermore, an optimal growth model is a rather special dynamic program. In an extension section, we present a general theory of dynamic programming with risk-sensitive preferences via the Q-transform.====The rest of our paper is structured as follows. Section 2 starts the exposition with typical examples. Section 3 presents the general theory when rewards are bounded above (though potentially unbounded below). Section 4 provides additional applications. Section 5 extends the general theory to the case when rewards are unbounded both from above and below, and also considers the case with recursive (risk-sensitive) preferences. Section 6 concludes. Main proofs are deferred to the Appendix.",Unbounded dynamic programming via the Q-transform,https://www.sciencedirect.com/science/article/pii/S0304406822000143,10 February 2022,2022,Research Article,43.0
Kukushkin Nikolai S.,"Dorodnicyn Computing Centre of the Russian Academy of Sciences, 40, Vavilova, Moscow 119333, Russia,Moscow Institute of Physics and Technology, 9, Institutskiy per., Dolgoprudny, Moscow region 141701, Russia","Received 11 April 2021, Revised 24 September 2021, Accepted 22 January 2022, Available online 3 February 2022, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2022.102647,Cited by (0)," determined by comparisons with the choices of others. In contrast to the original model, as well as its modifications considered so far, we allow for some players not caring about comparisons with some others. Assuming that the status of each player may only be “high” or “low,” the existence of a strong ==== is shown; for a particular subclass of such games, the convergence of ","The objective of this paper is rather technical and narrow. We study what happens if the assumption that “everybody watches everybody” implicit in the ordinal status games of Haagsma and von Mouche (2010), as well as their modifications in Kukushkin and von Mouche (2018) and Kukushkin (2019), is abandoned so that some players may be indifferent to the very existence of some others.====The place of all those models in the literature (Frank, 1985, Easterlin, 1995, Akerlof, 1997, Clark and Oswald, 1998, Becker et al., 2005, Bilancini and Boncinelli, 2008, Arrow and Dasgupta, 2009, Ghiglino and Goyal, 2010, Immorlica et al., 2017) can be delineated by these characteristics: the number of players is finite and the notion of status is ordinal. Roughly speaking, the status of a player is determined by comparisons, rather than differences, between her choice and the choices of others.====An attractive feature of models with purely ordinal status is that the best response of a player to a profile of strategies of the others, typically, exhibits the effect of “keeping up with the Joneses” in the literal sense: the player makes her choice above what is intrinsically the best for her, thus catching up with somebody else’s choice so that a loss in her internal satisfaction is (over)compensated for with a gain in status. In contrast to that, in the model of Immorlica et al. (2017), e.g., where the definition of social status is “mostly cardinal”, the best response also tends to be above the intrinsically optimal level, but how much above depends on the ==== of players with greater choices, rather than on those choices themselves. (Thus, ironically, ordinal notion of status leads to the best responses based on cardinal factors, while cardinal notion of status, to ordinal-based responses.)====A reference to “the Joneses” is practically indispensable in any textbook discussion of the role of status considerations; economic analysis of decision-making situations usually centers on a notion of equilibrium. Therefore, the question of whether (and under what assumptions) strategic games models in the style of Haagsma and von Mouche (2010) possess equilibria certainly deserves attention. There is no easy answer to the question since the best responses here are neither continuous (in which case the Brouwer fixed point theorem could be invoked), nor monotone (in which case the Tarski fixed point theorem could be invoked, as in Immorlica et al. (2017)): If keeping up with the Joneses becomes too expensive, the player just drops it and “downshifts”, either keeping up with somebody less pretentious or ceasing to pay attention to the others at all. As a result, even recently obtained general theorems on the existence of Nash equilibrium in discontinuous games (Reny, 1999, Reny, 2016, McLennan et al., 2011, Prokopovych, 2013, Prokopovych and Yannelis, 2017, Kukushkin, 2018) are inapplicable.====Although Haagsma and von Mouche (2010) obtained some ==== conditions for equilibrium existence, no ==== condition was found, with the exception of the two-person case. Kukushkin and von Mouche (2018) showed the existence of Nash equilibrium and convergence of (consecutive) Cournot tâtonnement in somewhat modified models, with only two possible status levels and the top tier consisting of the players whose choice is maximal of all.====Kukushkin (2019) established the existence of a ==== Nash equilibrium, which weakly Pareto dominates all other Nash equilibria, in a wider class of similar games where the line between the “top” and the “bottom” may be drawn anywhere (e.g., at the median choice). Besides, Cournot tâtonnement in such games always finds a Nash equilibrium in a finite number of steps. If there are three possible status levels, the existence of a Nash equilibrium is ensured under an additional assumption (which is automatically satisfied if the effects of consumption and of status are separated in the utility functions); with a greater number of possible status levels, there seems to be no reasonably general sufficient condition for Nash equilibrium existence.====In this paper, we consider two ways to define a network structure on the set of players. In a “simple” model, there is an undirected graph with players as vertices and edges between “neighbors”; each player only observes the choices of neighbors and her status is determined by the order rank of her choice among theirs. Defining a network with an undirected graph looks standard; however, the status of a player in such a model may not be visible to anybody else, which is hardly consistent with the usual connotations of “social status”.====This inconsistency is (largely) avoided in a more general model: Instead of drawing edges between players, a set of ==== is added such that each player, typically, belongs to several communities, her status in each community is determined separately and is visible to all members of the community. (To borrow an example from Immorlica et al. (2017), an individual is seen in the same Toyota car by her neighbors at home, by her colleagues in the parking lot near her job, near her gym, etc.) The “global” status of each player is an aggregate of her status levels in all the communities she belongs to; inevitable subjectivity of that aggregation diminishes the relevance of the question of whether anybody else observes all those local statuses.====A natural further step would be to allow the players to choose (within certain limits) where to belong, cf. Le Breton et al. (2021) and references therein. Unfortunately, it is unclear at the moment what assumptions would be needed to ensure, say, the existence of equilibrium in such a model, and what techniques would be needed to prove it.====Not surprisingly, extending the class of models under consideration, we lose some “positive” results. First of all, even in a simple model with a linear graph and quite nice utility functions, there may be no Nash equilibrium if more than two status levels are allowed. Consequently, we restrict attention in this paper to “dichotomic” games: a player’s status may be either “top” or “bottom” throughout. In such games, each player’s utility (and hence behavior) is only affected by other players’ choices through a scalar aggregate: the minimal choice that would place her in the top tier. The question of where and how often such “two-tiered” social hierarchies are met in the real world goes beyond this paper; our only point is that for a strategic game model based on an ordinal notion of status to reliably possess an equilibrium, the agents had better exhibit this attitude to social status.====The main findings are as follows. Exactly as when “everybody is everybody’s neighbor”, there exists a strong Nash equilibrium; however, the set of equilibrium utility profiles, even in a simple model, may have a non-trivial Pareto border. Cournot tâtonnement always finds a Nash equilibrium after a finite number of steps in every simple model. In a general model, Cournot tâtonnement may cycle; the question of exactly what assumptions would ensure acyclicity remains open.====In Section 2, some standard definitions are recalled; Section 3 provides the formal description of our class of games. Our main results are in Section 4: Theorem 4.3 asserts the existence of a strong equilibrium; Theorem 4.4, the convergence of Cournot tâtonnement. The proofs of those theorems are deferred to Sections 6 Proof of, 7 Proof of, respectively. “Counterexemples” in Section 5 show the impossibility of easy generalizations.",Ordinal status games on networks,https://www.sciencedirect.com/science/article/pii/S0304406822000118,3 February 2022,2022,Research Article,44.0
"Kumar Rajnish,Manocha Kriti,Ortega Josué","Queen’s Management School, Queen’s University Belfast, UK,Indian Statistical Institute, Delhi, India","Received 22 July 2021, Revised 26 October 2021, Accepted 4 January 2022, Available online 13 January 2022, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2022.102637,Cited by (1),"We study the welfare consequences of merging Shapley–Scarf markets. Market integration can lead to large welfare losses and make the vast majority of agents worse-off, but is on average welfare-enhancing and makes all agents better off ex-ante. The number of agents harmed by integration is a minority when all markets are small or agents’ preferences are highly correlated.","Shapley–Scarf markets, in which agents own one house each which they can exchange among themselves without using monetary transfers, have been helpful to analyse several real-life allocation problems, such as the assignment of campus housing to students (Chen and Sönmez, 2002), house allocation with existing tenants (Abdulkadiroğlu and Sönmez, 1999) and kidney exchanges involving incompatible donor–patient pairs (Roth et al., 2004). A common complication in these allocation problems is that a big market is fragmented into several small and disjoint ones, causing inefficiencies. For example, house swaps in Australia are restricted to tenants within the same constituencies and community housing provider, blocking potentially beneficial exchanges (Powell et al., 2019 p. 31). Similarly, most kidney exchanges in the US are conducted locally, despite the existence of centralized clearinghouses, which if used could increase the number of transplants by up to 63 percent (Agarwal et al., 2019).====Motivated by these observations, we investigate theoretically the welfare effects of integrating disjoint Shapley–Scarf markets. In our model, there are ==== Shapley–Scarf markets with ==== agents each (==== is potentially different for each market) and ==== agents in total. The segregated allocation is obtained by treating each community separately and calculating the core allocation for each of them. The integrated allocation is the core allocation for the entire economy.====Our first result (Proposition 1) states that up to, but not more than, ==== agents may be harmed by integration, i.e., they receive a house they prefer more when trade is only allowed within their own disjoint markets. This upper bound holds for any choice of ==== and ====. It shows that Shapley–Scarf markets may fail to integrate because doing so could generate significantly more losers than winners.====Our second result (Proposition 2) concerns the size of the gains from integration in terms of house rank. For example, if an agent receives her 3rd best house before integration, but her 1st best after integration, the size of her gains from integration is ====. Even if most agents are harmed by the merge of disjoint markets, integration may still be justified if the size of the gains from integration experienced by a few is substantially larger than the size of the losses from many. We show that, in the worst-case scenario, the size of the average gains from integration may be down to, but not less than, ====. This lower bound can be achieved for any choice of ==== and ====, and shows that, asymptotically, integration may increase the average house rank by 50% of the size of agents’ preference lists.====Taken together, our first two results show that there are real obstacles to the integration of Shapley–Scarf markets. For example, if we have three small markets that merge into one with 60, 30 and 10 agents respectively, up to 97 agents may obtain a worse house after integration occurs, and on average (across all agents) each agent may receive a house 50 positions down on her preference list, equivalent to going from her top choice to her 51st choice.====However, these results are obtained in worst-case scenarios, which occur only when preferences are very specific. Consequently, studying the expected gains from integration across all possible preference profiles may be more informative. Therefore, our third result studies the size of the expected gains from integration in random Shapley–Scarf markets, in which agents’ preferences over houses are drawn uniformly and independently.====In Proposition 3, we compute the exact expected gains from integration, which equal ==== (where ==== is the ====th harmonic number). This result shows that the expected welfare gains from integration are positive for all agents, and larger for agents belonging to smaller markets. Going back to our example of three markets integrating with 60, 30 and 10 agents, the agents of the market with size ten go up 16 positions in their expected house rank, whereas those in the market of size sixty also increase their expected allocated house rank, but only by 2 rank positions. Our third result gives some context to our first two propositions, and shows that on average we should expect an overall positive effect from integration in Shapley–Scarf markets for agents from all disjoint markets.====Our fourth result (Proposition 4) establishes a connection between the number of trading cycles that occur in the top trading cycles algorithm and the expected number of agents harmed by integration. We use this connection to show that the expected number of agents harmed by integration in each economy is less than ====, and consequently the expected number of agents harmed by integration in the entire economy is smaller than ====. In our example regarding the integration of markets with size 60, 30 and 10, our result implies that the expected number of agents harmed by integration is less than 44, 19, and 4 for each respective market. A consequence of our result is that, when all markets are of the same size, the expected fraction of agents harmed by integration is less than 50% whenever each market has less than ==== agents.====A different approach to ensure that integration does not harm a majority of agents is to focus on specific preference domains. We find a preference domain that achieves this purpose, called sequential dual dictatorship, which enforces a particular correlation among agents’ preferences. When preferences satisfy this property, we can guarantee that no more than 50% of agents in any individual market are harmed by integration (Proposition 5). The sequential dual dictator property is equivalent to assigning the title of dictator to at most two agents at each step of the top trading cycle algorithm, therefore bounding the length of cycles that can occur.",On the integration of Shapley–Scarf markets,https://www.sciencedirect.com/science/article/pii/S0304406822000027,13 January 2022,2022,Research Article,45.0
"Altuntaş Açelya,Phan William","Deakin University, Australia,North Carolina State University, United States of America","Received 21 April 2021, Revised 13 December 2021, Accepted 27 December 2021, Available online 7 January 2022, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2021.102631,Cited by (1)," of objects along a cycle. We ask if the attractive properties, namely ====, ====, and ====. We characterize separately the subclass of TPAC rules satisfying the ==== and ====. Regarding fairness, we follow in spirit to the ==== condition of Schmeidler and Vind (1972), where the set of allocations satisfying the property essentially coincides with the set of competitive equilibria, and augment the notion appropriately for our environment. We further generalize the TPAC family while extending results on ==== and the ====, and provide sufficient conditions on parameters for the rules to arbitrarily closely satisfy the ====.","A city public housing authority wishes to reshuffle tenants due to demographic changes. For example, family sizes fluctuate and there could be cases of one person living in a large apartment while a family of five resides in a small space. In the reshuffling process, the authority may have various goals including: As much as possible, place tenants where they want to live while enacting some type of fairness if multiple people apply for the same apartment. Certain tenants should have sufficient chance to be in certain arrangements e.g. senior tenants should have sufficient chance to be close to particular resources, or persons with disabilities have first priority at buildings with accommodations.====New York City provides various forms of public housing for over half a million residents and recently faced this complicated task.==== It is imaginable that many other cities face similar problems. How should they design such processes?====We model this problem as one of re-allocating objects (apartments) to agents wherein agents have preferences over objects, and probabilistic assignment – defined by a lottery over objects – is possible. To express various guarantees that the authority may wish to respect, we consider the environment where agents also have rights to certain objects. These rights are indicated by fractional ownership of the objects and represent a lower bound on their welfare. We study a large family of probabilistic rules that contain as a special case Gale’s Top Trading Cycles (TTC), and view them as a natural generalization (Shapley and Scarf, 1974). Our results shed light on tradeoffs regarding efficiency, manipulability, and fairness when we move to the probabilistic domain.====In deterministic environments, whether objects are initially owned or have attached priorities, TTC is ubiquitous. When each agent owns one and consumes one object, it is the only ====, ====, and ==== rule (Ma, 1994, Sönmez, 1999, Anno, 2015). Dropping ====, the expanded set of rules may be defined as an outcome of an algorithm where agents trade along cycles given a particular ownership structure (Pápai, 2000, Pycia and Ünver, 2017). In school choice where students have affixed ==== as opposed to ownership, TTC can still be used to define an ==== and ==== mechanism that satisfies further fairness criteria (Abdulkadiroğlu and Sönmez, 2003, Dur, 2013, Dur and Morrill, 2017, Morrill, 2013, Morrill, 2015a, Morrill, 2015b).====We ask the following question: Taking the intuition of trading objects along cycles as a starting point, does the procedure maintain its attractiveness in the more general environment of probabilistic assignment? That is, instead of simply trading entire objects, does trading ==== result in satisfactory rules? If ==== owns 0.2 of ==== and ==== owns 0.4 of ====, then it seems entirely reasonable for them to trade their shares if they wish.====To answer, we define a large family of probabilistic rules built upon such a premise. We start with the Trading-Probabilities-Along-Cycles (TPAC) family, expand to the Generalized TPAC family, and along the way study their axiomatic properties. Each rule in the TPAC family is defined by a TTC-like algorithm wherein agents have rights to trade or consume fractions of objects. Since several agents may have the right to trade fractions of the same object, each rule also specifies a particular order in which the trading rights are respected. Different trading rights and priority orders define different rules. Thus, we can define a family of TPAC rules, each associated with two exogenously given parameters encoding trading rights and priority orders.====Since agents may be assigned lotteries over objects, we extend their preferences over objects to preferences over lotteries by means of stochastic dominance, and consider variants of ====, ====, and ==== appropriate to the probabilistic environment (Bogomolnaia and Moulin, 2001).==== We indicate these variants with the “====” preface.====Our first contribution is to show to what extent this probabilistic extension of TTC satisfies the probabilistic analog of the original three properties characterizing it. The first two results are encouraging and straightforward: Each rule in TPAC family satisfies ==== (Proposition 1). For each fractional endowment profile, we characterize the subfamily of TPAC satisfying the ==== (Proposition 2). Our next result illustrates a known trade-off between the three properties in rich environments: If there is even a single agent with positive fractional endowment of more than one object, then there is no rule in the TPAC family that satisfies the ==== and ==== (Theorem 1). The intuition and success of trading along cycles runs into serious difficulties when we allow for the least bit of fractional endowment. A direct corollary of this result is that, within the TPAC family, TTC is the only ==== rule (Corollary 1).====Our second contribution is to show that interesting notions of ==== are achievable. For the classical exchange problem, Schmeidler and Vind (1972) proposes a ==== concept that accounts for agents’ possibly unequal endowments. At a proposed allocation, each agent moves from their endowment to their assignment—resulting in a net positive trade of some objects and negative of others; their ==== property requires that each agent prefers their own net trade to any other’s. They show that the concept is tightly linked with the set of competitive equilibrium allocations.==== We adopt this fairness notion for probabilistic assignment with endowments. Then, we introduce the Generalized TPAC family of rules wherein each member operates by allowing agents to appear ==== within the order of trading rights; this allows agents to alternate in the order of trading rights. We show that we can achieve with arbitrary closeness ====, and provide sufficient conditions on the parameters to do so (Theorem 2). The ==== and ==== properties of TPAC family are also preserved by the Generalized TPAC family (Proposition 3, Proposition 4).====Our result hints at known connection between TTC algorithms and competitive equilibria. In the original housing market, the allocation obtained by TTC can be supported as a competitive equilibrium (Shapley and Scarf, 1974). In school choice where agents have priorities instead of endowments, a type of competitive equilibrium is unique and coincides with the (school choice) TTC allocation (Dur and Morrill, 2017). Although we do not define a competitive notion here, our TTC-inspired mechanisms can satisfy one of its fundamental ==== properties, namely ====.====We discuss our results in relation to the probabilistic assignment literature. Hylland and Zeckhauser (1979) introduces the problem and imagines a pseudo-market mechanism wherein each agent is endowed with an income and purchases probabilities of objects according to prices. Note that in contrast to our model, agents do not own fractions of objects as a primitive. Their mechanism is ====, but is not ====. While they employ the intuition of the competitive equilibrium, we conduct a parallel exercise except with TTC. Unfortunately, there is no rule satisfying ====, ====, and ==== when considering an equal division of the endowments (Athanassoglou and Sethuraman, 2011). Weaker notions of ==== (requiring only that any outcome from a lie does not stochastically dominate the truth) does not recover compatibility (Aziz, 2018). Both of the previous papers prove their results at particular endowment profiles e.g. equal division in the former. Within the TPAC family, we confirm that some special characteristic of the previous endowment profiles, e.g. full support or a type of cycle in the support, is not driving the incompatibility; rather, it is pervasive, holding for ==== non-extreme points (Theorem 1). In light of these impossibilities, the focus turned to fairness and several papers proposed rules satisfying notions based on equal treatment or elimination of justified envy (Athanassoglou and Sethuraman, 2011, Echenique et al., 2021, Kesten, 2009, Yılmaz, 2010).====Closest to ours are Aziz (2015) and Yu and Zhang (2021). Additionally allowing indifference in preferences, Aziz (2015) defines a class of rules satisfying ==== and the ====. Yu and Zhang (2021) define a large class of rules relying on an algorithm that is procedurally significantly different from ours. The key difference is that in each step of their algorithm, each object points to each agent that owns positive amount of it. Subsequently, when each agent points to their most preferred object, the resulting graph can be complex with multiple overlapping cycles. They encode this information into system of linear equations and constraints representing feasibility. Within the constraints, there are additional exogenous parameters that specify for each object and each owner of the object the “share” that they ==== use of the object to trade. The assignment at this step is the maximum solution to the system with constraints. Contrast this with our algorithm, which is a rather straightforward extension of Gale’s Top Trading Cycles with trading rights and orders. When restricted to strict preferences, Yu and Zhang (2021) rules are a strict superset of our TPAC family which in turn is a strict superset of those in Aziz (2015).====We are also related to papers that employ ==== of TTC rules to recapture fairness. Interestingly, Random Serial Priority is equivalent to the Core from Random Endowments (Abdulkadiroğlu and Sönmez, 1998, Knuth, 1996).==== Although equal treatment and no-envy were the focus of these papers, such mixtures can satisfy the ==== for some fractional endowment profile by varying the weights placed on each component rule. For example, if an agent owns 10% of object ==== and 20% of object ====, then the mixture can place 10% weight on a rule where the agent initially owns ==== and 20% weight on a rule where the agent owns ====. Harless and Phan (2019) perform this type of analysis, but for the environment where each agent partially owns only one object.==== These mixtures are ==== but not generally ====; in contrast, our TPAC rules are ==== but not generally ====.====The paper is organized as follows. In Section 2, we define the probabilistic assignment problem. In Section 3, we define several desirable properties of an allocation rule. In Section 4, we define the TPAC family of rules, and we state properties of this family in Section 5. Pivoting to fairness, we discuss ==== and the Generalized TPAC family in Section 6. We conclude in the final section.",Trading probabilities along cycles,https://www.sciencedirect.com/science/article/pii/S0304406821001749,7 January 2022,2022,Research Article,46.0
"Lahmandi-Ayed Rim,Laussel Didier","University of Carthage, ESSAI, LR21ES21, L.R. MASE, 6, rue des métiers, La Charguia 2, 1080 Tunis, Tunisia,Aix-Marseille University (Aix-Marseille School of Economics), CNRS & EHESS, France","Received 28 June 2021, Revised 24 November 2021, Accepted 20 December 2021, Available online 7 January 2022, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2021.102633,Cited by (2),"We consider a ==== model with vertical preferences, where workers and consumers are differentiated respectively by their sensitivity to effort and their intensity of preference for quality. We consider a public monopoly, i.e. which is owned equally by all individuals. The question is under which conditions the firm will be privatized and at which rate/price. The decisions are taken through majority vote in a plurality system. When the firm is controlled by the State, the price is determined through a vote among all the population. Otherwise, the price is the one which maximizes the profit. We prove that, when the maximum disutility of working in the firm is higher than the maximum utility of consuming its output, privatization may emerge as a possible choice of the majority, even if no hypothesis is made on the efficiency of a private management relative to a public one.","The main object of the present paper is to study the relation between democracy and privatization, i.e. whether majority voting may or not lead to the privatization of a public monopoly, when the consumption of the output produced by the monopoly and the employment by the monopoly are important in the economy.====A great wave of privatization began in Great Britain and the USA under the Thatcher and Reagan administrations,==== lasting until the recent years. Mass privatizations affected many British firms in the eighties and nineties (British Rail, British Aerospace, Rover Group, British Telecom, Sealink ferries, British Petroleum, Rolls-Royce, British Steel Corporation, British Gas among others). Many others took place in Latin America at the same period (water management, transportation, telecommunication enterprises being sold off to the private sector) as part of liberal economic policies. In France, privatizations began under the Chirac government (1986) and continued over all the following administrations until now, affecting a great number of important enterprises (Saint-Gobain, Paribas, Société Générale, Havas, Renault, Total to name only a few). In Russia, Eastern and Central Europe, the transition from socialist to market economies was also accompanied in the nineties by massive privatizations of State-Owned Enterprises.====A greater efficiency of private firms was generally invoked to motivate privatization. But efficiency may be intended in different ways, as noticed by Willner (2001) “Attitudes to ownership are however often based on [..] confusion about the meaning of efficiency” (page 723).====A first unambiguous meaning is cost-efficiency. Because the managers of public firms are less easily monitored than the managers of private firms, they are supposed to have less incentives to exert efforts to reduce costs. For instance Laffont and Tirole (1991) write that “the cost of public ownership is a sub-optimal investment by the firm’s managers in those assets that can be redeployed to serve social goals pursued by the public” (page 84).====But there is also a second, more disputable, sense of the word, namely State-Owned Enterprises are often considered inefficient because they pursue other objectives than profit-maximization, leading to levels of output and/or employment which are considered as too high compared to their profit-maximizing levels.====
 The problem with this use of the word is that profit-maximizing output and employment levels are not necessarily welfare-maximizing. In particular, regarding SOEs which are generally large firms having substantial market power, profit-maximization is well-known to generally lead to too small output and employment levels, relative to welfare maximizing ones. This is particularly the case for some services such as water provision or health. Regarding the management of SOEs and their possible privatization, labor and/or access to the output are fundamental issues and may legitimately be considered to make the “right” decision. Room has also to be made for democracy and the opinion of majority. The existing theoretical literature heeds attention to labor at the aggregate level, thus from a centralized point of view, while we deal with employment by the monopoly at the individual level, thus from a decentralized point of view. Moreover the literature is scarce on output. The access to such outputs may nonetheless be fundamental, even vital in some cases, for instance water, electricity, transportation etc. A study of the World Bank (Jones et al., 2008) assessed the impact of privatization in 4 sub-saharian countries on workers as well as on consumers and concluded that, if privatization is done properly “it can lead to substantial welfare gains that are reasonably and equitably distributed across stakeholders -consumers, workers, governments, and owners or operators”. However, taking as an example water provision, according to Lobina et al. (2014), “Cities, regions and countries worldwide are increasingly choosing to close the book on water privatization and to “remunicipalize” services by taking back public control over water” because of the failures of the private sector in terms of output provision in quantity and quality. “Where near-universal access to water has been achieved, it has virtually always been through a public commitment” (The Guardian====).====These considerations may explain why, even when decided by elected governments, privatizations are often unpopular. In France for instance, according to a YouGov poll,==== 75% of respondents were ready to sign a petition to stop the privatization of ADP, the Paris airports society, which was decided by the Macron administration. This fear from privatization brings us back to the debate between the nineteenth century economists on the effect of democracy, in particular of universal suffrage, on property rights. The fear of some economists then was that the universal extension of voting would lead to “expropriation of capital” for redistribution purposes (what may take place nowadays in the form of nationalization).==== Even Ricardo who supported the expansion of suffrage in fact did not support “extending the elective franchise [ ] universally to all the people, but to that part of them which cannot be supposed to have any interest in overturning the rights of property”.==== In the same spirit, Hayek (1982) proposed to limit drastically the powers of future political majorities in order notably to avoid any infringement of property rights.====Though privatizations are not decided by majority voting, they are easier to implement and more likely to take place if they have a majority support. In the present paper, assuming for heuristic purposes no superior cost efficiency of private relative to public management and considering explicitly employment and output provision, we determine under which circumstances a majority support is more likely to occur for the privatization of a public firm and under which ones it is likely to be opposed by a majority of voters.====To this end, we adapt the small general equilibrium model developed in Kahloul et al. (2017) with an initially public monopoly and an exogenous quality of the product, supposing that the firm’s output price is determined through majority voting whenever the State retains a majority of shares and is fixed so as to maximize profits when the private investors take over the control.==== Any sale of shares of the firm is supposed to take place at a price which reflects the post-privatization firm’s value==== and may lead either to a situation when the firm is privately managed (“radical” privatization) or to a situation where the enterprise remains publicly managed. As already noticed, though it is often argued that private firms are more cost efficient because of greater incentives of private owners than politicians to keep costs low by more effectively monitoring and/or motivating appointed managers (see for instance Bishop et al., 1994), we suppose here for heuristic reasons that there is no intrinsic superiority or inferiority of private management over public management: the only difference between the two is that a public firm may choose a price which differs from the profit-maximizing one. The economy encompasses a single firm producing a vertically differentiated product using labour as the unique input and a population of workers/consumers/shareholders characterized by two parameters: preference for product quality and sensitivity to effort. Each individual decides whether to work or not and decides whether to purchase one unit of the product or not, in order to maximize his/her utility given his/her income. When the price is set at some given level, the salary adjusts in order to equalize demand and supply on the labor market.====We study a three-stage game. In the first step, the individuals vote in order to decide the fraction of the shares to be sold to private investors (thus whether to privatize the firm and at which rate).==== In the second step, the price per share is determined through a vote. Finally, in the third step, the output price is selected.==== If the private investors hold a majority of shares, the equilibrium price is the profit-maximizing one. When the enterprise is (or remains) controlled by the State, the price is the one which is preferred by a majority of voters. In this latter case, we show that this is the profit-maximizing price iff the public share in the capital of the firm is large enough and/or working in the firm and consuming its output is not very attractive for most agents; but it is otherwise a low price (involving financial losses which have to be covered by the State budget) which is intended to please the, then more important, part of the population which consumes the good and is employed by the enterprise.====Our main results are as follows. When the maximum disutility of working in the firm is greater than the maximum utility of consuming its output, any ownership structure such that the firm maximizes its profit is an equilibrium structure. That of course includes the cases where the State retains less than one half of the shares (private control) but also the cases where it retains an important fraction of the shares but with a majority of voters preferring the profit-maximizing output price. An immediate consequence of this first result is that any cost-efficiency advantage, even infinitesimal, of private over public control is then enough to entail a majority support for a radical privatization. When, on the contrary, the maximum disutility of working in the firm is smaller than the maximum utility of consuming its output, then an absolute majority of voters oppose any sale of shares to private investors. Moreover they strictly prefer that the firm remains totally public to a radical privatization, so that a possible small cost-efficiency superiority of private over public management would not be enough to reverse the voting result.====Now, when the maximum disutility of working in the firm is greater than the maximum utility of consuming its output, ==== or, equivalently, at the competitive equilibrium, the individuals who choose not to consume the good nor work at the firm are more numerous than individuals who consume the good and work at the firm.==== In the reverse case, ====, an absolute majority of the population is better off consuming the good or working in the firm.====Though the main part of the paper is about investigating the conditions under which there may or not exist a majority support for the privatization of a SOE, we also analyze here the welfare effects of privatization. Given the quasi-linear individual utility function in our model, welfare is expressed in units of numeraire and is the sum of the utilities of the voters.==== The conclusions of this analysis are surprisingly similar to the results obtained about majority voting, hinging as well on the comparison between the maximum utility of consuming one unit of the firm’s output and the maximum disutility of producing it. When the latter is greater than the former, any ownership structure such that the firm maximizes its profit, including the initial one (all shares belonging to the State), gives the same welfare level, which is greater than an ownership structure leading to a smaller price. When the former is greater than the latter, any privatization reduces welfare.====It is also interesting to understand the factors which are driving the results. Basically the agents have preferences over the ownership structure, namely over the fraction of shares which is held by the State, only in so far as it determines the level of the output price, which, in turn, determines the levels of wage and profits, and the agents’ decisions whether or not to consume the product and/or to work at the firm.====One would think that the agents who have no stake in the firm as consumers or workers or both want it to maximize profits and do not care whether this is as a public or a private entity and that, on the contrary, the agents who have a stake in the firm favor an output price lower than the profit-maximizing one, which occurs necessarily at the expense of the agents who have no interest in the enterprise. This retains part of the truth but is too simplistic and accordingly a bit misleading since the agents’ decisions to work and consume depend themselves endogenously on the output price which is implied by the ownership structure (and in the case of a public firm by the agents’ votes). It may happen for instance that an agent would be better off with a low price, in which case he/she would work and consume, but nevertheless choose not to work nor to consume when the higher profit-maximizing price prevails. In this paper we escape this ambiguity by referring to the social optimum/competitive equilibrium benchmark situation: when a relative majority of agents would in this benchmark case neither consume nor work, any ownership structure yielding profit-maximization is preferred by a majority of workers.==== There is an abundant literature on privatization. Two main streams exist (Cavaliere, 2006). The first one applies the principal–agent theory to the question of privatization. Comparing State Owned Enterprises (SOE) and regulated private firms, Sappington and Stiglitz (1987) shed light on the role of information asymmetry, risky production and the attitude toward risk of private producers. Shapiro and Willig (1990) assumed that a public framer has to make a choice between operating production through a SOE by delegating administrative power to a “malevolent agent” who pursues a private agenda, or with a regulated private firm facing an asymmetry of information on production. Another group of articles assume benevolent governments. Laffont and Tirole (1991) extended their previous model with incomplete information to compare public and private firms in the framework of incomplete contracts. Schmidt (1996), concerned with the soft budget constraint, compared SOEs with managers weakly motivated by reducing costs and regulated private firms with asymmetry of information.====The second stream of literature deals with privatization from a political economy point of view. Vickers and Yarrow (1988) pointed out the importance of labour in the decision of privatization which will avoid excess employment. Politicians may nevertheless refrain from privatization even if it is Pareto efficient because it will not be supported politically. The strong resistance of workers who will lose their employment will not be outweighed by the support of winners because privatization benefits are widespread. Labor is also at the core of the analysis of Shleifer and Vishny (1994) and Boycko et al. (1996). Because politicians will try to maintain excess employment even in privatized firms, privatization will not necessarily lead to reduced labor costs. Robinson and Torvik (2005) examined the soft budget constraint from another perspective. Politicians want “bad projects” to be supported by voters who will benefit from the redistribution of subsequent resources. Bortolotti and Pinotti (2003) compared the motivation for privatization of “majoritarian” political systems and “consensual-corporatist” democracies and conclude that the former are more likely to privatize. Biais and Perotti (2002) showed the difference in motivation for privatization by right wing and left wing politicians. Right wing politicians are motivated by future support from the constituency of shareholders of newly privatized firms, and left wing by redistribution of revenues accruing from privatization. Borner (2004) distinguished between privatization and restructuring of SOEs on the one hand, and between different types of governments, welfare or voter oriented, on the other hand. While a welfare maximizer will trade-off efficiency gains following privatization with the choice of the socially optimal employment level, a voter-oriented government will weigh the possibility to transfer to citizens the revenues accruing from privatization against the possibility to increase employment level in SOEs, with the objective of being re-elected.====The present paper belongs to the second stream, as we consider a voting model of privatization with no asymmetry of information. The employment level is central in our analysis, but also the level of consumption of the good produced by the monopoly. Willner (2001) is probably the paper which is the closest to ours. He also considered the output and employment levels. In his paper, the public firm’s output level is indeed fixed through a Nash bargaining process in which output and profits have complementary weights. This situation is then compared to a private Cournot oligopoly characterized by a given value of the Herfindahl index. Welfare is greater in the public monopoly than under privatization if the output weight in the bargaining process and/or the Herfindahl index are important. The main differences with the present paper is that (i) we use a general equilibrium model and (ii) we consider that the public firm’s price decisions as well as the privatization one are taken through majority voting.====Finally, loosely related to the present paper, there is an abundant literature on privatization in a mixed oligopoly framework, when competition exists initially between public and private firms, generally in partial equilibrium settings (to name only a few, Lin and Matsumura, 2018, Capuano and De Feo, 2010, Matsumura, 1998, Matsumura and Okumura, 2013).====The paper is organized as follows. Section 2 describes the model and a preliminary result on the consumption and work decisions at the competitive equilibrium. Section 3 provides the outcome of a democratic choice in terms of privatization. Section 4 analyzes the welfare effects of privatization. Section 5 concludes. All proofs are given in Appendix.",When do privatizations have popular support? A voting model,https://www.sciencedirect.com/science/article/pii/S0304406821001750,7 January 2022,2022,Research Article,47.0
"Hong Miho,Park Jaeok","Department of Economics, Yale University, United States of America,School of Economics, Yonsei University, Republic of Korea","Received 15 October 2020, Revised 12 June 2021, Accepted 15 December 2021, Available online 25 December 2021, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2021.102627,Cited by (2),"In this paper, we incorporate externalities into Shapley–Scarf housing markets, expressing agents’ preferences as defined over allocations rather than houses. We introduce a class of preferences called hedonic preferences, extend the top trading cycles (TTC) algorithm to housing markets with hedonic preferences, and investigate the existence and core properties of TTC allocations. In order to further expand the applicability of the TTC algorithm, we consider a class of preferences called trading-cycle-lexicographic preferences, and we also construct worst-case hedonic preferences from arbitrary preferences. Lastly, we study the properties of the TTC algorithm as a mechanism on the domain of preferences called egocentric preferences. Our results show that many ==== of the TTC algorithm for housing markets without externalities can be extended to housing markets with certain kinds of externalities.","In this paper, we study a market with indivisibilities and externalities. In their pioneering work, Shapley and Scarf (1974) introduce an exchange economy with a finite number of agents in which each agent owns an indivisible object (say, a house), has use for exactly one object, and looks for trading with other agents. Such an economy is referred to as a housing market in the literature. It is one of the simplest kinds of exchange economies one can imagine, and together with a house allocation problem (Hylland and Zeckhauser, 1979) it has served as a basic model for indivisible goods allocation problems and one-sided matching problems (see, for example, Moulin, 1995, Abdulkadiroğlu and Sönmez, 2013). Externalities are abundant in the real world, and a market with indivisible goods is not an exception. For example, in residential areas, a resident of a house cares about his neighbors’ demographics, and in universities, a faculty member takes into consideration his colleagues using nearby offices when choosing the location of his office.====Despite the ubiquity of externalities, most existing works on housing markets have not taken externalities into account, assuming that each agent cares only about the house he receives. The main results on housing markets without externalities include the following. Shapley and Scarf (1974) show that the core (defined by strong domination) of a housing market is nonempty, and they introduce an algorithm called the top trading cycles (TTC) algorithm that produces an allocation in the core. Roth and Postlewaite (1977) prove that, if agents have strict preferences over houses, the core (defined by weak domination) is a singleton consisting of the allocation obtained by the TTC algorithm. It is also shown that the TTC algorithm as a mechanism is strategy-proof (Roth, 1982) and coalitionally strategy-proof (Bird, 1984, Moulin, 1995, Lemma 3.3). Ma (1994) shows that a mechanism is individually rational, Pareto efficient, and strategy-proof if and only if it is the TTC mechanism. Our goal in this paper is to investigate whether these desirable properties of the TTC algorithm are preserved in the presence of externalities.====The core is the set of allocations that are immune to coalitional deviations, and thus it depends on the outcome a coalition can attain by deviating from a proposed allocation. When there are externalities, it matters to the members of a coalition how the residual agents react to their deviation. Thus, a deviating agent needs to form a belief about the residual agents’ reaction, and different beliefs lead to various concepts of the core. In order to define the core of a housing market with externalities, we take a trading cycle, which describes trading partners and their allotments, as the deviating unit and consider two extreme kinds of expectation deviating agents can have about the other agents’ behavior. The most optimistic belief leads to the smallest core called the ====-core (Kóczy, 2007), while the most pessimistic belief results in the largest core called the ====-core (Aumann, 1961). When these two extreme cores coincide, we refer to them as the core. We also consider a house allocation problem in which agents own houses collectively and define the concepts of stable and Pareto efficient allocations. Following the terminology of Roth and Postlewaite (1977), we say that an allocation is stable if no reallocation from it can benefit the reallocating agents. When there are no externalities, stability is equivalent to Pareto efficiency, and any allocation in the core is stable. However, when there are externalities, stability is a stronger property than Pareto efficiency, and it is logically independent of the core. Our results suggest that stability is a more relevant concept than Pareto efficiency in the presence of externalities.====In order to apply the TTC algorithm to housing markets with externalities, we consider a particular class of preferences called ==== preferences, with which an agent cares only about the trading cycle he belongs to. Banerjee et al. (2001) and Bogomolnaia and Jackson (2002) study coalition formation games in which each player’s payoff depends only on the members of his coalition and refer to this feature as the hedonic aspect. An allocation in a coalition formation game partitions the set of players into coalitions, whereas an allocation in a housing market divides the set of agents into trading cycles. Hence, we adopt the terminology “hedonic” to describe preferences defined over trading cycles one can belong to. While standard preferences represent ====, hedonic preferences can be regarded as expressing ====. When traders in a market are total strangers and do not care about each other at all, standard preferences would be relevant. On the other hand, when participating in trade requires intense interaction among acquaintances, hedonic preferences would be reasonable. For example, consider a scenario where a professor randomly assigns presentation slots to students enrolled in a class and allows them to exchange their slots. Then a student who wants to change his slot needs to contact other students in order to find out their assigned slots and their preferences, and it will be difficult for him to trade with students he barely knows or has a bad relationship with. A similar situation arises to general managers (GMs) in a professional sports league, who need to maintain good working relationships with other GMs in order to facilitate trades of players with other teams.==== In these situations, trade will occur easily if the parties involved have amicable relationships and are satisfied with the outcome, and thus an agent is concerned not just with the object he receives but also with his trading partners and the objects they obtain. A special case of hedonic preferences occurs when an agent cares about the object he receives and the agent who obtains his endowment, which is called ==== by Klaus and Meo (2021). Real-world scenarios where limited externalities arise naturally include kidney exchange and vacation home exchange.====We extend the TTC algorithm to housing markets with hedonic preferences as follows. At each step of the algorithm, each remaining agent points to the agent who comes next in his most preferred trading cycle in the submarket consisting of the remaining agents. In order to guarantee that the algorithm proceeds in a unique way, we assume that each agent is indifferent between two allocations only if he is assigned the same house at the two allocations. Then, even if an agent has multiple most preferred trading cycles, he has a unique agent to point to. At each step, at least one cycle is formed by agents pointing to each other. If the formed cycle is a top trading cycle in the sense that it is most preferred in the submarket by every agent in the cycle, trade is executed according to the cycle. If there is no top trading cycle formed at a step, the algorithm fails. When there are no externalities, every cycle formed in the TTC algorithm is a top trading cycle at the step where it is formed. However, when agents have hedonic preferences, a formed cycle does not need to be a top trading cycle, and thus the TTC algorithm may fail to produce an allocation. We propose a condition for the existence of a TTC allocation, called the (iterative) top trading cycle property, which is analogous to the (weak) top coalition property of Banerjee et al. (2001). We also show that, if a housing market with hedonic preferences has a TTC allocation, it is stable and is the unique allocation in the core. This result generalizes the existing result on the TTC allocation of a housing market without externalities (Roth and Postlewaite, 1977).====In order to further expand the applicability of the TTC algorithm, we propose two approaches of deriving hedonic preferences from given preferences so that we can apply the TTC algorithm using the derived hedonic preferences. In the first approach, we focus on a class of preferences called ==== preferences. An agent with trading-cycle-lexicographic preferences is primarily interested in the trading cycle he belongs to and secondarily in the others. In other words, an agent with trading-cycle-lexicographic preferences has a well-defined preference ordering over trading cycles he can belong to and compares two allocations according to this preference ordering whenever they yield different trading cycles to him. Hence, we can regard trading-cycle-lexicographic preferences as ==== added to hedonic preferences. Since perturbing hedonic preferences by adding weak externalities does not affect the core and stable allocations, we obtain that, if agents have trading-cycle-lexicographic preferences and the housing market with the associated hedonic preferences has a TTC allocation, it is stable and is the unique allocation in the core for the original preferences. In the second approach, we construct hedonic preferences from arbitrary preferences by comparing the worst allocations consistent with given trading cycles, and we call the constructed preferences ==== preferences. We show that the ====-core of a housing market contains the core of the housing market with the associated worst-case hedonic preferences. Using this result, we can obtain a sufficient condition for the ====-core to be nonempty.====Lastly, we study the properties of the TTC algorithm as a mechanism. In order to guarantee that the TTC mechanism yields an allocation for any reported preference profile, we consider a preference domain consisting of ==== preference profiles. An agent with egocentric preferences cares about his own allotment prior to others’. Thus, egocentric preferences can be obtained by adding weak externalities to standard preferences. Though restrictive, the class of egocentric preferences is reasonable in that it captures the idea that an agent’s own allotment is of higher significance than the other ones in his utility, and the same kind of preference classes has been widely studied in the literature (see, for example, Sasaki and Toda, 1996, Li, 1993, Graziano et al., 2020). In real-life situations, heterogeneous objects (for example, houses with distinct features) are more likely to induce egocentric preferences than homogeneous ones (for example, apartments with similar characteristics). For any reported egocentric preference profile, the TTC mechanism selects the TTC allocation of the housing market with the standard preferences associated with the reported preferences. As weak externalities are added to standard preferences, the core can expand while the set of stable allocations remains the same. This implies that the TTC mechanism yields a stable allocation in the ====-core for the reported egocentric preferences. We also show that the TTC mechanism is coalitionally strategy-proof and that a mechanism is individually rational, stable, and strategy-proof if and only if it is the TTC mechanism. These results extend the existing results of Roth and Postlewaite (1977), Roth (1982), Moulin (1995), and Ma (1994), who focus on the domain of standard preferences.====The contribution of this paper can be summarized as follows. We present preference domains for housing markets with externalities in which the TTC algorithm can be used. Our main results reveal that the desirable properties of the TTC algorithm are maintained even in the presence of local or weak global externalities, while some modifications are needed depending on the preference domain. Therefore, the TTC algorithm offers the best solution for housing markets in much wider preference domains than that of standard preferences.",Core and top trading cycles in a market with indivisible goods and externalities,https://www.sciencedirect.com/science/article/pii/S0304406821001725,25 December 2021,2021,Research Article,48.0
"Wang Dazhong,Xu Xinyi","School of Economics, Nanjing University, Jiangsu, China,Department of Economics, Lingnan (University) College, Sun Yat-Sen University, Guangzhou, China","Received 12 March 2021, Revised 26 November 2021, Accepted 12 December 2021, Available online 24 December 2021, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2021.102623,Cited by (0),"This study considers a seller using an equity auction to sell an indivisible asset for which bidders have interdependent valuations. First, we introduce bidders’ conditional virtual valuations for this setting, and identify sufficient conditions for conditional virtual valuations’ monotonicity and single-crossing property. Then, we characterize the optimal equity mechanism, which is ex post incentive compatible and individually rational. In general symmetric case, English equity auction or two-round sealed bid equity auction with an endogenous take-it-or-leave-it contract can implement the optimal ex post equity auction; in asymmetric case, English auction with an endogenous discriminatory equity contract could be constructed for the implementation of the optimal ex post equity auction under certain conditions.","Equity auctions are commonly used in actual practice, ranging from corporate acquisitions through equity and government procurement with incentive contracts to venture capital financing and oil lease auctions with profit sharing.==== In an equity auction, each bidder bids with his own equity shares (or profit shares of target project), and the winning bidder’s eventual payment to the seller is determined by the equity transfer and his operation profit. In a seminal work, Liu (2016) has analyzed the optimal equity mechanisms with private valuations. However, many auctions contain both private value and common value characteristics. For instance, in an auction of a company, the company’s “core” cash flow is a common value for all bidders, but synergies likely differ across bidders and therefore contribute an element of independent private values (Dasgupta and Hanse, 2007), or the asset for sale may be resold, and buyers have different information about resale markers; thus, bidders’ valuations could be affected by others’ information. The importance of interdependent valuations in auctions has been theoretically and empirically examined (see Milgrom and Weber, 1982, Hendricks et al., 2003). However, equity auctions with interdependent valuations are little-studied.====This study follows the mechanism design approach to analyze the optimal equity auction when bidders have interdependent valuations and correlated signals. To provide robust incentives and no regret for participation to the bidders, we focus on equity auctions that admit ex post equilibria with non-negative payoff for each bidder. By the revelation principle, we restrict attention to direct equity mechanisms that are ex post incentive compatible and ex post individually rational (referred to as ex post feasible equity mechanisms). In an ex post incentive compatible equity mechanism, each bidder’s optimization problem can be transformed into an equivalent one via dividing the objective function by his valuation, then applying the general envelope theorem yields the envelope formulae for each bidder’s ex post equilibrium payoff that only involves the allocation rule and the lowest signal equilibrium payoff. We then show that all ex post feasible mechanisms can be characterized by envelope-form equilibrium payoffs of the bidders, monotonicity of winning probability, and non-negative payoffs of the lowest signals.====Furthermore, we introduce the conditional virtual valuation of each bidder for ex post feasible equity mechanisms, and provide conditions on bidders’ valuation functions and distributions, ensuring the monotonicity and single-crossing property of the conditional virtual valuations. Then, the optimal equity mechanism allocates the asset to the bidder with the highest conditional virtual valuation, and the winner’s equity retention is equal to the ratio of his stand-alone value over the lowest valuation guaranteeing his win. Bidders with lowest possible signals can only obtain zero payoff in the optimal equity mechanism. The winner’s equity retention is equal to zero as his standalone value approach zero, thus, the seller can extract full surplus. In the general symmetric case, which is originally considered by Milgrom and Weber (1982), the optimal equity mechanism allocates the asset to the bidder with the highest signal.====On implementation, we first show that in the general symmetric cases, the English equity auction with a committed contract contingent on losing bidders’ drop-out prices, or with post-auction negotiation between the seller and the last active bidder, can implement the optimal ex post equity auctions. Besides, a two-round sealed bid equity auction with a commitment of an endogenous contract can be constructed for the implementation. Moreover, we find that in the asymmetric case, English auction with an endogenous discriminatory equity contract can implement the optimal ex post equity auction, if bidders’ conditional virtual valuations satisfy the generalized single-crossing condition, besides monotonicity and regularity. Lastly, equity auctions with loser payments and efficiency objective of seller are also discussed.",Optimal equity auction with interdependent valuations,https://www.sciencedirect.com/science/article/pii/S0304406821001701,24 December 2021,2021,Research Article,49.0
Beggs Alan,"Department of Economics and Wadham College, Oxford University, Oxford, OX1 3PN, UK","Received 4 May 2021, Revised 30 September 2021, Accepted 11 December 2021, Available online 24 December 2021, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2021.102621,Cited by (0),"This paper studies learning when agents evaluate outcomes in comparison to reference points, which may be adjusted in light of experience. It shows that certain models of reinforcement learning, motivated by those popular in machine learning and neuroscience, lead to classes of recursive preferences.","The idea that agents evaluate outcomes relative to reference points has been influential in the economics literature since the pioneering work of Kahneman and Tversky (1979). It also has support in the literature in neuroscience, as outlined in the stimulating book by Glimcher (2011). How reference points are determined remains, however, an open question. Köszegi and Rabin (2006) suggest that reference points are determined by expectations and Glimcher (2011) argues that this theory is consistent with models of learning in neuroscience. This paper explores the link with these models from a theoretical perspective.====In Köszegi and Rabin (2006)’s approach expectations, and so reference points, are determined by rational expectations. In many environments the assumption of rational expectations seems too strong and it seems more reasonable to assume that expectations and reference points are determined adaptively. In the literature in neuroscience much interest has focused on modeling learning by reinforcement learning. An agent is assumed to learn in a dynamic but stationary environment of the kind studied in dynamic programming. Simple procedures can lead the agent to learn values of policies, and indeed optimal actions, even if she is unaware of the true stochastic process governing the environment and the relationship between actions and payoffs. In these papers agents form expectations and adjust them linearly according as outcomes are above or below their expectations or reference points. This paper studies generalizations of these models where the relationship between gains and losses need not be symmetric or even linear, as suggested by Kahneman and Tversky (1979) in the context of prospect theory.====Agents are assumed here to be learning about a dynamic programming problem in a stationary environment. In each state they form expectations or reference points about future payoffs and adjust these in the light of experience. Equivalently they learn about their value functions. They adjust their expectations in the manner suggested by reinforcement learning but it is assumed that they evaluate gains and losses using loss–gain functions which need not be linear. It is shown that, under mild assumptions, agents will learn to have recursive preferences. More precisely, if preferences are intertemporally separable, then if ====, ==== and ==== denote action, current and future states respectively and ==== and ==== denote payoffs and the discount factor respectively, then instead of agents’ long-run value functions, ====, satisfying the usual recursive equation ====they will instead satisfy an equation with the conditional expectation operator replaced by a (conditional) generalized certainty equivalent, ====: ====
 ==== is not in general the usual conditional expectation operator but rather a (conditional) generalized certainty equivalent of the kind found when agents have non-expected utility preferences over static risky lotteries.====Such recursive preferences have become popular in macroeconomics following the work of Epstein and Zin (1989) but are sometimes regarded as rather exotic. The current paper shows they may arise as the result of boundedly rational agents learning about their environment and so provides a behavioral justification for their use. This complements the usual justification for Epstein–Zin preferences, which is purely axiomatic.====The paper also shows that under such preferences action choice can be represented as agents seeking to maximize gains or minimize losses relative to reference points as in the behavioral literature. The reference points are, however, the result of long-run learning and so agents respond rationally to shocks given their induced preferences. It also studies convergence when preferences are not intertemporally separable.====The fact that agent use reference points in their learning is taken as given. The contribution of the paper is to study how they evolve and to show how their evolution results in agents coming to have recursive preferences.====The procedures considered in the paper are simple ones. A sophisticated agent could learn about his environment more efficiently but such agents are not the focus of this paper. The purpose of the paper is instead to show that simple procedures can enable relatively unsophisticated agents to learn about their environment and in doing so come to have recursive preferences. The models in this paper are of course idealized and real agents may behave in complex ways but if they are a reasonable approximation they lend support to the idea that agents can learn to have recursive preferences.====Kuhnen (2015) provides some experimental evidence that consumers learn differently from losses and gains. In her paper she elicits beliefs from subjects and shows that agents form overly pessimistic beliefs as a result of losses. In the approach here, agents do not directly form beliefs about outcomes, rather they form expectations about future payoffs. The nonlinearity of loss–gain functions means, however, that they may revise their expectations differently in response to losses and gains. The two approaches are therefore complementary.====The models of reinforcement learning studied here are somewhat different to those familiar in the economics literature from the work of Erev and Roth (1998). In that literature the focus is on simple rules for a single player attempting to learn in a static environment or in a static game interacting with other players. Formal analyses of the convergence properties of these models can be found in Beggs (2005) and Hopkins and Posch (2005). The models here instead analyze a single player learning in a dynamic, but stationary, environment and draw inspiration from the models of reinforcement learning in the tradition of Sutton and Barto (2020) in machine learning, which in turn have heavily influenced neuroscience.====In the neuroscience literature (Niv et al., 2012) find that a model with a piecewise-linear gain–loss function may fit neural data better than conventional models. They use the model of Mihatsch and Neuneier (2002) from the machine-learning literature on risk-sensitive reinforcement learning. The current paper extends this work to general non-linear loss–gain functions and gives it an economic interpretation. In recent independent work in the machine learning literature Shen et al. (2014) have also considered non-linear loss–gain functions but they allow for a less general class than those considered here. In addition, they give a different interpretation and do not make the link with Epstein and Zin (1989) preferences.====In the economics literature Sarver (2012) considers an intertemporal model where consumers may gain utility from anticipation if they choose a high reference point but must balance this against losses from realized outcomes below the reference point. He gives an axiomatic characterization of the resulting preferences. The model here is one of learning rather optimal anticipation and the focus is on the convergence of learning schemes rather than axiomatic characterizations. Related literature is discussed in more detail in the body of the paper.====The models of reinforcement of the kind studied in Sutton and Barto (2020) have had little application in the economics literature to date. Cho and Rubinchik (2017) study a model of contemplation and intuition in a reinforcement-learning framework. There has been some application in game theory, see for example Leslie and Collins (2005), but only with standard preferences. Charpentier et al. (2021) provide a survey of reinforcement learning and possible applications in Economics and Finance but also consider only standard preferences. Hill et al. (2021) explore using reinforcement learning to solve general equilibrium macroeconomic models where agents have standard preferences.====The paper proceeds as follows. Section 2 outlines the background on learning from neuroscience to motivate the models studied. Section 3 outlines the basic environment, which is one of stationary dynamic programming. Agents adjust their value functions evaluating a gain loss-function. Section 4 shows that these gain–loss functions correspond to estimating a certainty-equivalent. In the long-run reference points convergence to those whose expected gain–loss is zero but this is equivalent to being a generalized certainty equivalent. These generalized certainty equivalents usually do not coincide with the standard certainty equivalent but belong to the class introduced by Chew (1989) and are those which arise in Epstein and Zin (1989) preferences. Examples of the resulting preferences are given. These include disappointment aversion (Gul, 1991) but also less familiar ones.====Section 5 studies reinforcement learning in a dynamic environment. It shows that if agents use reinforcement learning with non-linear loss–gain functions then their preferences over policies converge under mild conditions to recursive preferences of the kind introduced by Epstein and Zin (1989).====Conceptually learning to play an optimal policy has two components (a) a method to find the value of any given policy, (b) a procedure for searching for the best policy given one can determine the value of any policy. There are a number of methods one could use to implement (b), and there is some disagreement as to which is more plausible neurologically. For a fixed policy, however, most of these reduce to the same algorithm and so result in the same preferences over policies. As a result the same optimal policy will be chosen even if a different search procedure is used. For this reason although one could leap directly to learning the optimal policy, the early sections focus on learning about a fixed policy as the resulting preferences over policies will be robust to the procedure used to find the optimal policy.====Section 6 discusses learning optimal policies by using Q-learning. Conditions for convergence are given. Section 7 shows that optimal actions can be interpreted as maximizing gains relative to reference points.====Section 8 considers extensions. In Section 5 it is assumed that preferences are intertemporally separable. Section 8 shows that this assumption can be relaxed and local convergence results obtained for general Epstein–Zin preferences. It also discusses robustness of the results to other assumptions, including the form of the reference point and the timing of shocks. Section 9 concludes.",Reference points and learning,https://www.sciencedirect.com/science/article/pii/S0304406821001695,24 December 2021,2021,Research Article,50.0
"Yu Jingsheng,Zhang Jun","Economics and Management School, Wuhan University, China,Institute for Social and Economic Research, Nanjing Audit University, China","Received 17 September 2020, Revised 14 December 2021, Accepted 14 December 2021, Available online 23 December 2021, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2021.102625,Cited by (0),"We consider two variants of Shapley and Scarf’s (1974) housing market model in which agents’ rights to consume own endowments are restricted but their rights to exchange endowments are unrestricted. In each of the two models, we propose a core notion to characterize allocations that are immune to coalition blocking under such restricted rights. In each model we prove that the corresponding core can be found by a strategy-proof variant of the Top Trading Cycle mechanism. Our results generalize the classical result in the housing market model that the core can be found by the strategy-proof Top Trading Cycle mechanism.","In a pioneer work in the market design theory, Shapley and Scarf (1974) propose the housing market model in which a group of agents own distinct objects and wish to reallocate their objects without using monetary transfers. In the absence of any allocation mechanism, the core notion in cooperative game theory is a powerful tool to predict reasonable allocation outcomes. An allocation belongs to the core if no coalition of agents can do better by reallocating their endowments among themselves. It turns out that in the housing market model, the core is a singleton and the unique allocation in the core can be found by the famous Top Trading Cycle mechanism (TTC; Roth and Postlewaite, 1977). Roth (1982) proves that TTC is strategy-proof, meaning that agents are willing to reveal true preferences. These results become the foundation of many follow-up studies in the market design literature.====This paper considers two discrete exchange models deviating from the housing market model in that agents’ rights over their endowments are restricted. In the housing market model, every agent’s right over his endowment includes two components: the right to consume his endowment and the right to exchange his endowment with the others’. In the two models we study, agents’ rights to consume own endowments are restricted, yet their rights to exchange endowments are unrestricted. In the first model that we call ====, an agent can consume his endowment only if the others are not interested in his endowment. This restriction can be formalized as a stability requirement in a priority structure in which every agent has the lowest priority for his endowment. By comparison, the housing market model can be understood as a priority structure in which every agent has the highest priority for his endowment. Yu and Zhang (2020) use the restricted housing market model to describe real-life rotation schemes. For example, many firms rotate their employees’ positions from period to period (Osterman, 1994). Some countries allocate irrigation systems and fishing spots in a rotation way (Ostrom, 1990, Berkes, 1992). In both situations, agents’ rights to keep what they are assigned are restricted when resources are reallocated. In the second model that we call ====, agents are fully banned from consuming own endowments. So agents have to exchange endowments. This models rotation schemes where agents have to rotate their assignments no matter they wish or not.====We seek appropriate core notions in the two models to demonstrate agents’ restricted rights over endowments. Note that in our models, for any coalition, not every reallocation of their endowments is permitted. In the restricted housing market model, every coalition can freely exchange their endowments among themselves. But if any coalition member wants to consume his own endowment, he needs to get agreements from all of the other agents to make sure that the others are not interested in his endowment. So the agent has to invite all of the others to join the coalition, and the new allocation implemented by the coalition has to be stable. We call the set of stable allocations that are unblocked by any coalition the ====. In the mandatory rotation model, because agents’ rights to consume own endowments are banned, every coalition has to exchange their endowments. We define the ==== as the set of allocations in which no agent consumes own endowment and no coalition can do better by exchanging their endowments among themselves.====It turns out that the generalized core in the restricted housing market model is nonempty and a singleton. The unique allocation in the generalized core coincides with the outcome of ==== (BTTC), which is a generalization of TTC proposed by Yu and Zhang (2020). In particular, we present a hybrid of the restricted housing market model and the housing market model in Section 2. In the hybrid model some agents’ rights to consume own endowments are restricted while the others’ rights are unrestricted. We show that the results in the two models can be unified. In the mandatory rotation model the rotation core can be empty, but when it is nonempty, it is a singleton and coincides with the outcome of another generalization of TTC that we call ==== (RTTC). The mandatory rotation model and the rotation core are presented in Section 3. It is remarkable that both BTTC and RTTC are strategy-proof. Therefore, the classical result in the housing market model mentioned at the beginning of the introduction is generalized in the two models we study.",Cores and mechanisms in restricted housing markets,https://www.sciencedirect.com/science/article/pii/S0304406821001713,23 December 2021,2021,Research Article,51.0
"Bossert Walter,D’Ambrosio Conchita,Weber Shlomo","CIREQ, University of Montreal, Canada,University of Luxembourg, Luxembourg,New Economic School, Moscow, Russia","Received 16 June 2021, Revised 22 October 2021, Accepted 18 November 2021, Available online 16 December 2021, Version of Record 10 May 2022.",https://doi.org/10.1016/j.jmateco.2021.102617,Cited by (1),"We present a unified approach to the design of social index numbers. Our starting point is a model that employs an exogenously given partition of the population into subgroups. Three classes of group-dependent measures of deprivation are characterized. The three groups are nested and, beginning with the largest of these, we narrow them down by successively adding two additional axioms. This leads to a parameterized class the members of which are based on the differences between the income (or wealth) levels of an individual and those who are better off. We then proceed to show that our measures are sufficiently general to accommodate a plethora of indices, including measures of inequality and polarization as well as distance-based measures of phenomena such as diversity and fractionalization.","A wide range of social index numbers are based on comparisons among individuals along various socially relevant characteristics such as income and wealth, ethnicity, language and religion. One reason for this phenomenon is the realization that individuals do not live in isolation and compare themselves with others to understand their standing in society or to evaluate the degree of societal diversity; see Clark and D’Ambrosio (2015) for a comprehensive survey. These indices have appeared in many contributions and relate to different characteristics of societies, including deprivation, inequality, polarization, fractionalization and diversity. Our objective is to show that these substantially different concepts can be unified within the same basic structure.====The notion of a sense of deprivation that may arise from these comparisons appears first in Runciman (1966, p. 10), who writes “[w]e can roughly say that [a person] is relatively deprived of X when (i) he does not have X; (ii) he sees some other person or persons, which may include himself at some previous or expected time, as having X; (iii) he wants X; and (iv) he sees it as feasible that he should have X”. On the same page, Runciman (1966) adds that “[t]he magnitude of relative deprivation is the extent of the difference between the desired situation and that of the person desiring it”. Deprivation has been introduced in the Economics literature by Yitzhaki (1979) who proposed an index that continues to be one of the most (if not the most) widely used measure in the requisite literature. Hey and Lambert (1980) phrase the Yitzhaki index in terms of the income differences between an individual and those who are better off; see also Yitzhaki (1980). The importance of designing a suitable measure of deprivation is underlined by its role as one of the main driving forces behind major social phenomena, including crime (Kawachi et al., 1999), political violence (Gurr, 1968) and migration decisions (Stark and Taylor, 1989).====Indices of individual deprivation naturally constitute a platform for aggregate societal indices. The first, and still the most remarkable, contribution in this vein is to be credited to Gini (1912). In his book ==== published in Italian, Gini (1912) provides a thorough examination of societal diversity and introduces two main groups of measures, one for each of the types of phenomena that his work focuses on. The first part of his book is devoted to indices of variability where he analyzes distributions of economic variables such as income or wealth; see, for instance, Mehran (1976), Weymark (1981) and Yitzhaki (1983) for generalizations and extensions of the Gini index of income inequality. In the second part of his monograph, he moves on to what he refers to as indices of mutability with a focus on social variables that address notions such as fractionalization and ethno-linguistic diversity, among others.====The two types of indices introduced by Gini continue to have a profound impact on theoretical and empirical research to examine the consequences of diversity on various economic and societal outcomes; a thorough review is provided by Ginsburgh and Weber (2020). The first of his measures consists of the well-known relative Gini coefficient designed to measure the degree of inequality in an income or wealth distribution and its absolute counterpart. Yitzhaki (1979) provides the interesting and thoughtful observation that the value of the absolute Gini coefficient is equal to his aggregate measure of deprivation. Gini’s second index is what is now called the Gini–Simpson fractionalization index; see, in addition to Gini (1912), Simpson (1949). This measure extends the notion of deprivation beyond income differences to include attributes such as ethnicity, language or religion. It is worth noting that the Gini–Simpson index can be expressed as one minus the Herfindahl–Hirschman index of industrial concentration; see Hirschman (1945) and Herfindahl (1950).====While both of Gini’s indices rely on the concept of differences (or, more generally speaking, distances), their conceptual underpinnings are quite different. In the case of incomes, it is universally agreed that, from the viewpoint of an individual, a higher income is better than a lower income. This is not necessarily the case in the framework of fractionalization, an area in which such a simple uni-dimensionality is absent—and in which there is no agreement that there is a well-defined notion of betterness; in fact, it is impossible to even begin to think of better and worse groups in this context. It is therefore not too surprising that the theoretical and empirical examination of such diverging indices has largely proceeded through separate channels. It is only in some recent contributions that there appear first attempts to link the Gini coefficient to ethno-linguistic diversity; examples of such approaches are Esteban and Ray (2011) and Alesina et al. (2016).====Esteban and Ray (1994) develop a theory for the measurement of income polarization based on the absolute Gini coefficient. In a multi-group setting, they introduce an alienation-identification framework according to which the members of different groups are alienated from each other while, within each group, a sentiment of identification prevails. Esteban and Ray (1994) modify the absolute Gini index by introducing an interaction term that is intended to capture the notion of clustering around the mean values of the various income groups. Duclos et al. (2004) extend the applicability of this measure and illustrate how the original measures of Esteban and Ray (1994) can be generalized to notions of distance that may be very different from income gaps. Prominent examples include distances between ethnic groups or the phenomenon of social polarization that may be mediated by income differences.====The primary objective of our contribution is to provide a unified approach that encompasses the measurement of social phenomena such as income inequality, deprivation, polarization, fractionalization and diversity. Our basic model assumes that there is an exogenously given partition of the population into subgroups; the criterion used to define group membership is left open so that the approach is universally applicable to any arbitrary group structure. In our baseline results, we assume that the variable the distribution of which is to be assessed is individual income—or, indeed, any one-dimensional attribute that can be unambiguously ranked. In our approach, having more of the variable is better than having less but it is, of course, straightforward to accommodate phenomena such as pollution where having less is preferable. We already pointed out the important role played by the sentiment of individual deprivation in the development of the theory of social index numbers so it is only natural that deprivation measures form the cornerstone of our analysis. Because an individual experiences a sense of deprivation when comparing his or her position with those who are better off, an examination of the phenomenon seems suitable in the context of an unambiguously ranked variable. What is more surprising is that the general classes of deprivation measures that we propose and characterize encompass, in addition, conceptually very different phenomena such as fractionalization and diversity, where a unique measuring rod cannot but be absent. We stress that, although deprivation is a one-sided phenomenon in the sense that only those with higher incomes matter to an individual, our basic classes are sufficiently flexible to accommodate measurement issues that involve a symmetric treatment, such as income inequality. Therefore, the scope of this paper is by no means limited to measures with this characteristic.====We provide characterizations of three nested classes of individual measures of deprivation in Section 2. Beginning with the largest of these, we narrow them down by successively adding two further axioms. This leads to a parameterized class the members of which are based on the differences between the income (or wealth) levels of an individual and those who are better off. Almost all of the properties that we employ in our characterizations are well-established and accepted in the literature. The only axiom that is new refers to the group structure that is novel to our approach, and it is very mild and intuitive indeed—it merely requires that the index treats all individuals within each group symmetrically, paying no attention to their identities. Thus, the class of measures that we axiomatize rests on a solid theoretical and conceptual foundation.====The first result illustrates the impact of adding our new within-group anonymity axiom to three basic axioms. We characterize a general additive structure that allows the income of an individual ==== and the income of a person ==== who is better off than ==== to be compared by means of an arbitrary group-dependent function of the two corresponding income levels. This is a very general structure that is capable of capturing a large class of indices. In fact, all the deprivation measures that we are aware of are more specialized in that they employ income shortfalls to measure the extent to which an individual is deprived in comparison to one who is richer—that is, the foundation of all pairwise comparisons is given by the difference in incomes.====Our second result is an axiomatization of a more specific class—namely, that of all measures that satisfy translation invariance in addition to the four basic axioms. In the presence of this axiom, the group-dependent functions obtained in the first result can be written as group-dependent functions of the income differences. A special case consists of a class of inequality measures that appears in Ebert (2010).====An interesting observation that applies to Ebert’s (2010) indices is that, unlike the majority of measures in the literature, they are not necessarily linearly homogeneous. By adding the axiom of linear homogeneity, we arrive at our final characterization, which shows that the group-dependent functions of the income differences must be linear. This yields a parameterized class which forms the basis of all remaining special cases.====In the remainder of Section 2, we move from individual deprivation measures to aggregate indices. As is common in the literature, we arrive at a societal measure of deprivation by calculating a weighted mean of the individual deprivation levels. This is a widely accepted method because the sentiment of deprivation is one that is experienced by individuals and, therefore, averaging is a perfectly legitimate operation. We stress that the weights we employ respect the essential requirement that members of the same group be treated symmetrically. Our indices generalize Yitzhaki’s (1979) well-established measure by allowing for the existence of a group structure and, naturally, the Yitzhaki index itself emerges as a special case if there is but a single group. As is typically the case in this literature, the deprivation experienced by an individual is determined by the differences in incomes (or wealth) between the person under consideration and everyone in society who is richer (in the sense of having a higher level of income or wealth). These differences are weighted where the parameters (the weights) may differ across groups—that is, the income difference between the person in question and another member of society may depend on the groups these two individuals belong to. We do not restrict the relationship between different parameter values in any way; this is in line with the observation that there is no natural ‘ranking’ of the groups—all we know is that they differ and they may share different degrees of similarity. In this sense, our measures are very general and allow for a large variety of index numbers as special cases.====Section 3 is devoted to a discussion of special cases of our parameterized measures. The rich variety of examples that we exhibit demonstrate that the measures we propose are remarkably accommodating indeed. We discuss Ebert’s (2010) indices separately in the preceding section because they involve a general function rather than a linear function and, as a consequence, elude the parameterization that is present in the linearly homogeneous examples. The common theme that runs through all applications is that the requisite measures are based on the notion of distance—differences in the case of monetary variables, potentially more elaborate notions of distance (between groups, for instance) in more complex environments without a common unambiguous ranking. This suggests that our measures are absolute (in the sense of satisfying the translation-invariance property), which is indeed the case. However, relative notions can easily be accommodated—taking logarithms is a commonly-applied method of doing so.====We begin with the most naturally emerging special cases. In particular, we show that the indices of income polarization introduced by Esteban and Ray (1994) are obtained by a suitable choice of the parameter values. Although Esteban and Ray do not include it among their indices because they focus on the measurement of polarization rather than inequality, the absolute Gini (1912) coefficient is a member of our class as well. It is not too surprising that these measures appear as special cases because they are, as are measures of deprivation, based on income differences.====We then move on to more complex environments in which a single unambiguously ranked variable is not present. The fundamental difference is that these measures are based on general distance functions as compared to the natural distance that emerges as the difference of income or wealth levels in the earlier examples. We show that a large class of fractionalization indices can be expressed by means of the structure developed for our deprivation measures, although it is clear that they are not special cases in the strict sense of the term because they do not operate on income distributions; conceptually, however, they follow the pattern described by the members of our class. This class contains itself numerous special cases of interest, as we show subsequently in that section. In particular, the Gini–Simpson index (also referred to as Greenberg’s, 1956, A-index in the literature) and the measure introduced by Davydov and Weber (2016) which generalizes the index of Reynal-Querol (2002) are included. In addition, we discuss Greenberg’s (1956) B-index and some of its generalizations and variations developed in contributions by Davydov and Weber (2016) and Desmet et al. (2017). We then proceed with indices that combine income distributions with information on ethnic, linguistic or religious groups, leading to the diversity-inequality indices of Desmet et al. (2011), Chakravarty (2015) and Hodler et al. (2020).====The section is concluded with an extension of the polarization measures of Esteban and Ray (1994) that combines polarization and a notion of diversity. This index has, to the best of our knowledge, not appeared in the previous literature. Thus, in addition to providing a unifying framework for a plethora of social index numbers, our contribution illustrates how our class can be employed to generate previously unidentified measures that may turn out to be very useful, especially in view of the current movement towards the integration of various social phenomena. We stress, again, that all of these measures can be accommodated within the conceptual framework provided by our fundamental class of deprivation indices.====In the concluding Section 4, we summarize the parameter structures that lead to the various special cases and, moreover, we explain why some other measures cannot be accommodated in our setting. In particular, indices that appear in the literature on measuring communication benefits in a multi-linguistic setting (see, for instance, the contributions by Selten and Pool (1991), Church and King (1993), Lazear (1999), Gabszewicz et al. (2011)) fail to be members of our class because the group structure that they are based on does not provide a partition of the population. In addition, our class does not include indices of ordinal inequality (Allison and Foster, 2004) and polarization (Apouey, 2007, Permanyer and D’Ambrosio, 2015) since there is no natural ranking of the groups in our setting.",Distance-based social index numbers: A unifying approach,https://www.sciencedirect.com/science/article/pii/S0304406821001671,16 December 2021,2021,Research Article,52.0
"Duddy Conal,Piggins Ashley","Department of Economics, Cork University Business School, University College Cork, Cork, Ireland,School of Business and Economics, and Whitaker Institute, National University of Ireland Galway, Ireland","Received 2 December 2020, Revised 27 October 2021, Accepted 17 November 2021, Available online 9 December 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102615,Cited by (0)," where all elements of ==== are available; society will not be required to make a choice from a strict subset of ====. Arrow assumes that the social preference relation is an ordering of the alternatives in ====. However, under the assumption that the collective choice rule satisfies unrestricted domain, independence of irrelevant alternatives and the weak Pareto principle, this can be weakened to the requirement that (i) the social preference relation generates a non-empty set of maximal alternatives, and (ii) every maximal alternative is strictly socially preferred to every non-maximal alternative. Requirement (ii) can be weakened further when there are four or more alternatives. When only property (i) is satisfied, we characterize all collective choice rules that output complete social preferences and satisfy unrestricted domain, Pareto, neutrality and anonymity. These collective choice rules are related to the ","In many social choice problems there is a fixed, finite set of alternatives. A social choice must be made from this set alone; society is not required to make a choice from any strict subset. Arrow’s (1951) impossibility theorem still applies in this setting. Arrow proves that there exists no collective choice rule (CCR) that satisfies unrestricted domain, independence of irrelevant alternatives, the weak Pareto principle and non-dictatorship, when the output of the CCR is required to be an ordering of the alternatives. An ordering is a complete and transitive binary relation. In this paper, we do not require that the output be an ordering. Rather, we require that (i) the social preference relation generates a non-empty set of maximal alternatives of ====, and (ii) every maximal alternative is strictly socially preferred to every non-maximal alternative. An alternative is maximal if no other alternative is strictly preferred to it. Although Arrow’s ordering assumption implies (i) and (ii), these two properties do not imply the existence of an ordering. We prove, firstly, that Arrow’s impossibility theorem still holds when all that is assumed about the social preference relation is (i) and (ii). A binary relation satisfying (i) and (ii) will be called a ====-maximal relation. The paper then explores various weakenings of property (ii) and therefore studies, in a systematic fashion, CCRs under various assumptions about social maximality. We conclude this exercise by considering CCRs with property (i) alone. These assumptions about social preferences allow, in principle, for the possibility of strict preference cycles among the non-maximal alternatives.====Weakenings of condition (ii) may be motivated by a margin-of-error model of social preferences. More specifically, let us consider the influential concept of ==== introduced by Luce (1956). Consider Fig. 1(a) which represents an ordering of four alternatives, ====, ====, ==== and ====, with alternatives to the right strictly preferred to those to the left (and so ==== is uniquely maximal). However, now suppose that we represent alternatives by intervals of equal length rather than by points. One alternative is strictly preferred to another if and only if its interval is entirely to the right of the interval of the other. An example is Fig. 1(b).====This might be natural if we think that there is some indeterminacy in social preferences, i.e., it is not certain as to how the alternatives compare. In Fig. 1(b), ==== and ==== are the maximal alternatives (their intervals overlap and so we cannot say which is preferred). Alternatives ==== and ==== are not maximal because ==== is strictly preferred to each of them. However, ==== is not strictly preferred to ==== even though ==== is maximal and ==== is not. The social preference relation represented in Fig. 1(b) does not satisfy ====-maximality. Perhaps, then, the requirement that every maximal alternative be strictly preferred to every non-maximal alternative is too strong.====We can see immediately, however, that the rightmost interval must always be entirely to the right of every non-maximal interval. This simple observation motivates a weakening of clause (ii) in the definition of ====-maximality to the following, which we call (ii*): there exists at least one maximal alternative that is strictly socially preferred to all non-maximal alternatives. A binary relation satisfying (i) and (ii*) will be called a ====-maximal relation. The social preference relation represented in Fig. 1(b) satisfies ====-maximality.====A symmetric requirement would be that at least one non-maximal alternative is strictly inferior to every maximal alternative. However, the semiorder model does not support this. It is perfectly possible that, in a semiorder, even the leftmost interval overlaps with the interval of a maximal alternative. However, we can see immediately that if a non-maximal interval ==== is entirely to the left of another non-maximal interval, then that ==== interval cannot overlap any maximal interval. This motivates a second, different weakening of ====-maximality which we call ====-maximality. Loosely speaking, this condition says that if a non-maximal alternative is inferior to some other non-maximal alternative, then it is inferior to all maximal alternatives. ====-maximality is logically independent of ====-maximality when there are four or more alternatives, but equivalent when there are three alternatives.====Unfortunately, as with ====-maximality, our second and third impossibility theorems show that Arrow’s theorem still holds under ==== and ====-maximality provided that there are at least four alternatives. However, with exactly three alternatives, the Pareto rule satisfies the requirements of these theorems (and, in addition, is an anonymous rule). The Pareto rule is a CCR under which the Pareto optimal (efficient) alternatives at each profile are socially maximal and no other alternatives are maximal.====One more condition we consider, ====-maximality, is weaker than both ==== and ====-maximality. Again, loosely speaking, it says that for each non-maximal alternative there exists some maximal alternative (not necessarily the same for all non-maximal alternatives) that society strictly prefers to it. Unlike our other conditions, we prove that ====-maximality gives rise to oligarchies, not dictatorships.==== The Pareto rule is a CCR that satisfies all of the assumptions of this theorem. Under the Pareto rule, each non-maximal alternative is Pareto dominated by some maximal alternative. After giving formal definitions of these various maximality conditions in Section 2, statements of these results are contained in Section 3. The maximality conditions are then revisited in Section 5.====After completing this treatment of various binary relations with maximal alternatives in Section 3, we consider the structure of CCRs when we weaken our maximality requirement to property (i) alone, i.e., we only assume that the social preference relation generates a non-empty set of maximal alternatives at each profile of individual preferences. This is the weakest condition that we consider in the paper. Under the standard assumptions of unrestricted domain, anonymity, neutrality and the Pareto principle, we characterize, in Section 4, all of the CCRs that satisfy these assumptions and output social relations that are complete and satisfy only property (i). We call these rules ==== rules, following Bossert and Suzumura (2008), who characterize a family of CCRs called ==== rules using almost the same normative axioms as we do. More importantly, they assume that social preferences are reflexive and Suzumura consistent (but not complete). We weaken Suzumura consistency to maximality but add completeness. The rules that result, the ==== rules, are like ==== rules but they replace social non-comparability with social indifference. However, ==== rules and ==== rules generate identical strict social preferences.====To give an example of an ==== rule, assume that there are three alternatives and seven individuals. The profile of preferences is given in Table 1.====Individual 1 strictly prefers ==== to ==== and ==== to ====, individual 6 is indifferent between ==== and ==== and strictly prefers ==== to ==== and ==== to ====, and so on. We construct a set ==== which contains ordered pairs of natural numbers (including zero). The specification of ==== determines the particular ==== rule that will be used for the purpose of aggregating preferences. For example, assume that ====. If ==== denotes the weak social preference relation which comes from applying this particular ==== rule to this profile of preferences, then we define, for all ====, ==== if and only if there exists ==== and exactly ==== individuals strictly prefer ==== to ==== and exactly ==== individuals strictly prefer ==== to ====.==== Applying this to the profile in Table 1 leads to ==== and ==== where ==== denotes the asymmetric component of ==== and ==== denotes the symmetric component. ==== is intransitive in this example, but the social preference relation does possess a unique maximal alternative, ====. Note that under the original ==== rules, ==== and ==== would be socially non-comparable. However, as we define them, ==== rules always generate a complete relation over the alternatives in ==== since (as we prove in Section 4) if ==== then ====.====The Pareto principle implies that the ordered pairs ==== are in ====. The example above shows that an ==== rule may not be monotonic. If ==== then, intuitively, ==== should also be in ==== but this is not the case. However, an ==== rule can be monotonic.====There is a parallel here with two well-known characterizations, Theorem 5*3 in Sen (1970) and Theorem 3 in Weymark (1984). Sen studied CCRs mapping profiles of individual preference orderings into social preference relations that are reflexive, complete and quasi-transitive. Assuming strong Pareto, independence of irrelevant alternatives, and anonymity, Sen characterized the Pareto ==== rule. Weymark studied CCRs mapping profiles of individual preference orderings into social preference relations that are reflexive, transitive, but not necessarily complete (i.e., the social preference relation is a quasi-ordering). Using the same normative axioms as Sen, Weymark characterized the ==== Pareto rule. Both rules generate identical strict social preferences, but when two individuals have opposing strict preferences over a pair of alternatives the alternatives are socially non-comparable by the strong Pareto rule, whereas they are socially indifferent by the Pareto extension rule.====Compared to Sen, Weymark weakened completeness but strengthened transitivity. Compared to Weymark, Bossert and Suzumura strengthened independence to neutrality, but weakened transitivity to Suzumura consistency. We retain neutrality, weaken Suzumura consistency to maximality and assume completeness.====If the number of alternatives is at least as large as the number of individuals, then the Pareto extension rule is the only possible ==== rule. However, as we see from the example above, when the number of individuals exceeds the number of alternatives, additional ==== rules are possible. In most social choice problems the number of individuals does exceed the number of alternatives.====A characterization of ==== rules, obtained under our weakest assumption about maximality, is contained in Section 4. The final section, Section 5, considers the relationship between the various binary relations presented in the paper and existing coherence conditions on social preferences that are known in the literature.====In Section 5 we prove a number of logical equivalence results which shed light on the theorems in Section 3. For example, we show that if a CCR outputs ====-maximal social preferences at every profile of individual preferences, and if this CCR satisfies unrestricted domain, weak Pareto and independence of irrelevant alternatives, then the strict social preference relation is a negatively transitive and asymmetric relation of ====. This is an interesting result since it is known that Arrow’s impossibility theorem holds under this assumption about strict social preferences.==== However, neither Arrow’s assumption that the weak social preference relation is an ordering (or Kreps’ assumption that the strict social preference relation is asymmetric and negatively transitive) are logically implied by ====-maximality alone. Therefore, there is a sense in which Arrow’s model of social choice (in the setting of this paper) is over-specified: once the normative assumptions about CCRs are in place, then the requirement on the social preference relation can be weakened. However, as we see in Section 5, these normative assumptions then ==== the social preference relation to be, in fact, stronger than merely maximal. In Section 5 we prove corresponding equivalence results for ====-maximality, ====-maximality, ====-maximality and property (i), which corresponds to maximality alone.====In summary, the contribution of the paper is as follows. First, we study in a systematic fashion, preference aggregation under various assumptions about social maximality. This enables us to derive various impossibility theorems. Second, we provide a characterization of ==== rules under our weakest assumption about social preferences. ==== rules are similar to ==== rules but generate complete rankings of the alternatives and can yield smaller maximal sets than the Pareto rule. Additionally, from a technical point of view, the proof of this characterization theorem is different from the proof in Bossert and Suzumura (2008) and this may be of independent interest. Finally, we prove several logical equivalence results showing the relationship between the different binary relations presented here and various coherence conditions on social preferences, under the assumption that CCRs satisfy the usual Arrow requirements. The final section of the paper contains concluding remarks.",Collective choice rules with social maximality,https://www.sciencedirect.com/science/article/pii/S030440682100166X,9 December 2021,2021,Research Article,53.0
"Jain Ritesh,Lombardi Michele","Institute of Economics, Academia Sinica, Taiwan,University of Liverpool Management School, Liverpool, UK,Department of Economics and Statistics, University of Napoli Federico II, Italy","Received 14 July 2021, Revised 11 November 2021, Accepted 21 November 2021, Available online 2 December 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102605,Cited by (0),", as well as in interim correlated rationalizable strategies, by a finite mechanism.","A traditional assumption of implementation theory is that of complete information, meaning that the state (of nature) is common knowledge among players (but unknown of course to the designer). However, a fundamental contribution of game theory shows that predictions of strategic situations are sensitive to common knowledge assumptions (see, for example, Rubinstein, 1989, Weinstein and Yildiz, 2007). This paper considers the question of when an SCR is virtually implementable not only when the state is common knowledge among players but also when it is ==== to common knowledge.==== Thus, the paper studies virtual continuous implementation, and it provides a characterization of continuously virtually implementable SCRs in the preference environment introduced by Abreu and Matsushima (1992a).====Instead of exact implementation, in which every outcome recommended by the SCR is achieved with probability one, our designer wishes to achieve outcomes specified by an SCR with probability arbitrarily close to one: he requires ==== implementation.====By following Oury and Tercieux (2012), we use the model of incomplete information introduced in Harsanyi (1967) and developed in Mertens and Zamir (1985). Our notion of continuous virtual implementation requires that in any “nearby” model in the universal type space that embeds our baseline complete information model, all Bayesian Nash equilibrium outcomes of the devised mechanism are ====-close to the recommendations provided by a given SCR, at all types close to the initial common knowledge types, and for every ====. If such a mechanism exists, we say that the SCR is continuously virtually implementable.====We consider continuity with respect to the uniform-weak topology (see, for example, Monderer and Samet, 1989, Chen et al., 2010). Roughly speaking, this topology preserves common knowledge. That is, types close to the common knowledge types in this topology have approximately common knowledge of the state in the following sense: for some number ==== close to one, they assign probability of at least ==== to the state, assign probability of at least ==== to the event that the state occurs in and the other players assign probability of at least ==== to the state, and so forth, ad infinitum.====Under a well-known domain restriction due to Abreu and Matsushima (1992a), we show that any SCR is continuously virtually implementable in Bayesian Nash equilibria when there are at least three players. Moreover, we also achieve continuous virtual implementation in interim correlated rationalizable (ICR) strategies, which assures that the outcomes generated by a given SCR are achieved despite the presence of strategic uncertainty. These results are obtained by devising a ==== mechanism. As a consequence, every player’s best response correspondence is always well-defined, and it does not rely on any tail-chasing procedure to eliminate unwanted best replies, such as integer games. Although we use a module game in our construction, our characterization result accounts for mixed-strategy equilibria, unlike existing results that rely on modulo games. We also show that for the devised mechanism, virtual implementation in strict Nash equilibria, and in rationalizable strategies, suffices for continuous virtual implementation when the state is common knowledge among players. Similar to Chung and Ely (2003), strict incentives are used in order to attain continuous virtual implementation in uniform-weak topology for any “nearby” model.====In a seminal paper, Abreu and Sen (1991) characterize the class of correspondences that are virtually implementable in Nash equilibria. This result relies on a tail-chasing construction, which is typical in the classical literature on implementation theory. Following Abreu and Matsushima (1991), we characterize the class of correspondences that are virtually implementable by a finite mechanism.====There are several reasons why focussing only on social choice functions can be considered unsatisfactory (see, for instance, Thomson, 1996). Firstly, multi-valued SCRs typically represent many social decisions. Prominent examples include the Pareto, the Walrasian, the Condorcet, and the no-envy correspondences. Secondly and foremost, since ==== represents the social objectives that the society or its representative want to achieve, its full implementation is the correct objective of the society. It would be unacceptable to partially implement ==== by implementing a social choice function ==== which systematically picks, for each ====, a socially optimal outcome ====.==== The reason is that the implementation of this subselection ==== may violate some of the normative properties that led the society or its representatives to choose ====. As Thomson (1986, p. 135) aptly noted “it most certainly will be unacceptable when the correspondence embodies some minimal concerns about fairness distribution.”====An argument made in favor of a partial implementation of ==== is based on the interpretation that the mechanism designer views the outcomes in ==== as equally good (Abreu and Sen, 1991, Mezzetti and Renou, 2012). A shortcoming of this interpretation is that in some situations we do not know whether the mechanism designer is indifferent between socially optimal outcomes or not. Moreover, the classical interpretation of implementation of ==== requires that each outcome in ==== must be supported by a distinct equilibrium (Maskin, 1999, Abreu and Sen, 1991) and it does not assume planner’s indifference. This paper shows that this classical interpretation is not restrictive when the objective is to implement ==== virtually. This is in sharp contrast to the case of “exact” implementation where implementation in the classical sense is more restrictive than implementation under the assumption of planner’s indifference (Mezzetti and Renou, 2012).====The remainder of the paper is organized as follows. Section 2 defines the implementation model. Section 3 presents the characterization result and provides an informal discussion of the implementing mechanism, with the proof offered in Section 4. This result builds on the existing literature on implementation theory, which is discussed in Section 5. Section 6 concludes by highlighting possible extensions.",Continuous virtual implementation: Complete information,https://www.sciencedirect.com/science/article/pii/S0304406821001610,2 December 2021,2021,Research Article,54.0
Hiller Timo,"Department of Economics, PUC-Rio, Rua Marquês de São Vicente, 225, Rio de Janeiro, RJ 22451-900, Brazil","Received 1 June 2021, Revised 22 September 2021, Accepted 24 November 2021, Available online 1 December 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102611,Cited by (0),"This paper provides a game-theoretic model of network formation with a continuous effort choice. Efforts are strategic complements for direct neighbors in the network and display global substitution/competition effects. We show that if the parameter governing local strategic complements is larger than the one governing global strategic substitutes, then all pairwise ","Many social and economic environments are characterized by local bilateral influences, inducing local strategic complementarities, and global competition effects, which generate global strategic substitutabilities. Frequent examples include the choice of criminal effort in a network of criminals and production choices of firms engaging in bilateral R&D agreements. Further applications are interbank lending and trade. We focus on the first two settings and briefly explain the mechanism leading to the type of strategic interaction considered.==== In the case of crime networks, local strategic complementarities are due to a direct know-how transfer on how to commit a crime. That is, the criminal activities of a criminal’s direct neighbors in the network translate into a lower probability of being caught and therefore increased incentives to commit crimes. Global strategic substitution effects stem from global competition effects for crime opportunities. For R&D collaborations, consider firms that compete in quantities in a common market. Production leads to learning-by-doing, while bilateral R&D agreements allow for a direct, cost reducing know-how transfer, which gives rise to local strategic complementarities. Global strategic substitution effects arise directly when the goods produced are substitutes and sold in a common market. More generally, introducing competition effect is relevant for many applications, as disregarding them often requires adopting strong assumptions. For example, in the case of crime networks, one would need to assume that there is no competition for crime opportunities, and for R&D networks that all firms operate in completely separate markets. Similarly, disregarding global substitution effects implies for interbank lending that consumers cannot substitute across loans, while in trade networks goods produced by different countries cannot be substitutable.====Network structures are crucial in determining individual behavior and aggregate outcomes. It is therefore important to understand why certain network structures arise.==== In this paper we endogenize the network in a simple game-theoretic setup, allowing for local strategic complements, as well as global strategic substitutes. In accordance with empirically observed networks, we obtain nested split graphs, which are a subset of core–periphery networks. The defining feature of nested split graphs is “nestedness” of neighborhoods in the following sense: agents with a higher number of links are connected to all agents to which an agent with fewer links is connected. Recently, these types of networks have drawn increased attention in the economics literature on networks.==== The particular structure of crime networks depends on the type of criminal activity. Canter (2004), for example, finds that networks of hooligans are less structured than property crime and drug networks. However, the presence of a core group is described as the most recognized structural feature.==== Nestedness and core–periphery structures have been frequently observed in R&D networks.====In the following we provide a brief description of the model, together with a more detailed account of the main results. We propose a simple simultaneous move game, in which agents choose a non-negative, continuous effort level and announce with whom they wish to be linked. A bilateral link is created when the announcement is mutual. Gross payoffs are based on the linear quadratic payoff function first presented in Ballester et al. (2006). We assume that the parameter governing strategic complementarities is (weakly) larger than the one for global strategic substitutes. Note that this is assumption has empirical support for the main applications considered.==== Links are assumed to be unweighted, undirected and to incur a linear cost. The equilibrium concept used is pairwise Nash equilibrium. Pairwise Nash equilibrium refines Nash equilibrium and allows for deviations in which agents simultaneously create a link (and best respond to each other’s effort level). This rules out configurations in which pairs of agents are not connected, but both agents find it profitable to create a link among themselves. We find that all pairwise Nash equilibrium networks are nested split graphs and that a pairwise Nash equilibrium always exists. Finally, we analyze the problem of a planner, who can place links according to a network cost function. It is shown that all optimal networks are again nested split graphs. However, the optimal network may be different from equilibrium networks and efficient effort levels do not coincide with equilibrium effort levels.====Ballester et al. (2006) was the starting point for a rich body of theoretical and empirical research (see, for example, Calvó-Armengol et al., 2009, Ballester et al., 2010, Helsley and Zenou, 2014). However, endogenizing the network proved to be challenging. Recent efforts have focused on models of network formation with stochastic elements and action choices in a dynamic setting with myopic agents (see, for example, König et al., 2014, König and Liu, 2019). In these papers noise is introduced into the decision process and typically agents cannot revise their whole linking strategy (e.g., only create at most one one-sided link, often at zero cost, or only delete a single link) and/or the deletion of links is not strategic and occurs due to decay over time. It is then shown that the stochastically stable networks of the dynamic process are nested split graphs. One of the advantages of the aforementioned models is that they can be brought to the data. However, the stochastically stable states obtained need not be Nash equilibria and are therefore also not pairwise Nash equilibria (as illustrated in Example 4). In contrast we show in a simple game-theoretic model that if the parameter governing local complements is larger than the one governing global substitutes, then all pairwise Nash equilibria are nested split graphs.====Joshi and Mahmud (2016) assume the payoff function provided in Ballester et al. (2006) and present a two-stage game, in which agents play a simultaneous link announcement game in the first stage and (gross) payoffs are determined in the second stage by the corresponding Nash equilibrium effort levels (given the network obtained in the first stage). The stability concept with respect to the first stage is pairwise stable equilibrium, which ensures that there is no pair of agents that can profitably deviate by adding a link in the link announcement stage. The authors show that with local complementarities and global substitutes all pairwise stable equilibria are nested split graphs. However, only limiting cases of parameters are considered and no existence results are provided for the network formation game. Obtaining results for the sequential model is difficult, since for a deviation in the link announcement game, agents take into account the whole vector of Nash equilibrium effort levels for the resulting network. That is, all agents may update their effort levels, not only deviating agents (as is the case in pairwise Nash equilibrium).====Hiller (2017) studies pairwise Nash equilibrium networks assuming a general class of payoff functions for which the linear–quadratic specification is a special case, but disregards the global substitution term.==== Pairwise Nash equilibrium and socially optimal networks are shown to be nested split graphs. However, in the presence of substitution effects, socially optimal networks may display different type of structures within the class of nested split graphs. For linear network cost functions the bang–bang type solution of an empty or complete network, as presented in Hiller (2017), may not be socially optimal with global strategic substitutes (as illustrated in Example 6). Note that introducing competition effects significantly complicates the analysis. For the equilibrium characterization, we can follow Hiller (2017) for the outline of the argument, but each step requires a different proof. For the efficiency result we need to resort to a different proof strategy.====Belhaj et al. (2016) consider the linear–quadratic payoff specification in Ballester et al. (2006), disregarding the global substitution term and solve the following two planner problems. In the first case the planner can place links according to a network cost function and agents play Nash equilibrium effort levels, given the network. In the second case, the planner can not only choose the network, but also agents’ effort levels. The authors show that in both cases the optimal networks are nested split graphs. Note that in the absence of global substitution effects, the two problems are closely related and one can show that characterizing one also characterizes the respective other problem. This relationship breaks down when introducing global substitution effects. We show here that in the latter case, i.e. when the planner can choose both the network and effort levels, then again nested split graphs are socially optimal.==== The former case, however, remains an open problem.====A strand of the literature studying the private provision of public goods also jointly analyzes effort choices and linking decisions.==== In these papers, however, effort levels are assumed to be strategic substitutes for connected agents, while direct global effects are absent. Galeotti and Goyal (2010) present a one-sided network formation model with two-way flow and a continuous effort level choice. All (strict) Nash equilibria of the simultaneous move game are shown to display a core–periphery structure. Moreover, it is shown that the fraction of agents that provide effort (and that are in the core) goes to zero as the number of agents goes to infinity. This is known as the law of the few. Note that the law of the few does not hold in our model. For given parameter values, if the number of agents is sufficiently large, then there exists a pairwise Nash equilibrium such that the network is complete, i.e. such that the core is maximal. (Effort levels are always strictly positive for all agents in any equilibrium). Kinateder and Merlino (2017) extend the model of Galeotti and Goyal (2010) to allow for heterogeneous agents. It is shown that, depending on the type of heterogeneity introduced, Nash equilibrium networks are either nested split graphs or complete multipartite graphs.====The paper is organized as follows. Section 2 provides the model description, while Section 3 shows that all pairwise Nash equilibria are nested split graphs and that a pairwise Nash equilibrium always exists. Section 4 solves the planner’s problem. A formal derivation of the payoff function for crime and R&D networks is provided in the appendix and we also relate our payoff function in detail to Ballester et al. (2006). All proofs are relegated to the appendix.",A simple model of network formation with competition effects,https://www.sciencedirect.com/science/article/pii/S0304406821001646,1 December 2021,2021,Research Article,55.0
"Demuynck Thomas,De Rock Bram,Freer Mikhail","Department of Economics, University of Leuven, E. Sabbelaan 53, B-8500 Kortrijk, Belgium,ECARES, Université Libre de Bruxelles, Avenue F. D. Roosevelt 50, CP 114, B-1050 Brussels, Belgium,Department of Economics, University of Essex, Wivenhoe Park, Colchester CO4 3SQ, United Kingdom","Received 7 September 2021, Revised 4 November 2021, Accepted 21 November 2021, Available online 1 December 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102607,Cited by (0),We provide a revealed preference characterization of expected utility maximization in binary lotteries with prize-probability trade-offs. We start by characterizing optimizing behavior when the empirical analyst exactly knows the utility function or the ,"We analyze models of expected utility maximization in which the decision maker (DM) faces a binary lottery that is characterized by a prize-probability trade-off. In particular, we take a framework where a lottery yields a reward ==== with probability ==== and a payoff of zero with probability ====. Here, the value of ==== is exogenously given and ==== is a cumulative distribution function. The DM’s problem is to choose the optimal value of ====. In other words, she faces a trade-off between the value of the reward and the probability of winning.====This type of decision problem occurs frequently in economics. A notable example is the (independent private values, sealed-bid) first price auction where the DM is one of the participants. In this case the prize of the lottery is given by the value ==== of the object for the DM minus the DM’s bid ==== to win the auction. The DM can choose to increase the probability of winning the auction (in a monotone equilibrium) by increasing her bid ====, but this implies that the final value of winning the auction, i.e. ====, decreases. In what follows, we do not explicitly consider the strategic aspect of this game and concentrate mainly on the single-agent decision problem. Under the assumption that players play a Bayesian Nash equilibrium, the probability of winning, given the bid ====, captures all the relevant information for the DM to choose her optimal bid. The first price auction is just one instance fitting in our general set-up. In Section 2 we will briefly discuss additional examples of often studied decision problems that are also characterized by prize-probability trade-offs in – admittedly – settings that are mostly more complex in reality.====Our main contribution is that we develop a revealed preference approach to characterize behavior that is expected utility maximizing under price-probability trade-offs. A distinguishing and attractive feature of our revealed preference characterizations is that they do not require a (non-verifiable) functional specification of the optimization problem. They define testable conditions for optimizing behavior that are intrinsically nonparametric and, therefore, robust to specification bias. To define these testable conditions, we will assume that the empirical analyst can use, for a given DM, a sequence of observations on rewards ==== (received when winning the lottery) and on money amounts ==== (called “bids” in what follows) that the DM is willing to forego in order to increase her probability of winning. Our set-up is clearly data restrictive since it assumes multiple observations of the same agent. This makes our approach more readily applicable to an experimental set-up that wants to investigate theoretical properties or identifications strategies for a given setting (see Capra et al., 2020 for a motivating review of different games and experiments). Coming back to our first price auction example, it is in particular interesting to note that there is a sizeable experimental literature that focuses on this specific decision situation (see, for example, Kagel and Levin (2016) for an overview).====As a preliminary remark, the nonparametric revealed preference approach that we present in this paper follows the tradition of Afriat (1967), Diewert (1973) and Varian (1982). A sizeable literature has emerged on testing decision theories under risk using this revealed preference approach. However, this literature has mainly focused on choices involving Arrow–Debreu securities from linear budgets (see, for example, Varian, 1983, Green and Osband, 1991, Kubler et al., 2014, Echenique and Saito, 2015, Chambers et al., 2016, Polisson et al., 2020), with a few papers focusing on the full mixture space (see, for example, Kim, 1996). We complement these earlier studies by considering expected utility maximization in a distinctively different decision setting.",Revealed preference analysis of expected utility maximization under prize-probability trade-offs,https://www.sciencedirect.com/science/article/pii/S0304406821001622,1 December 2021,2021,Research Article,56.0
"Fonseca-Mairena María Haydée,Triossi Matteo","Department of Economics and Management, Universidad Católica del Maule, Observatorio Laboral del Maule, San Miguel 3605, Talca, Chile,Department of Management, Ca’ Foscari University of Venice, Fondamenta San Giobbe, Cannaregio 873, 30121 Venice, Italy","Received 31 August 2020, Revised 21 November 2021, Accepted 24 November 2021, Available online 1 December 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102613,Cited by (1),We study the implementation of ==== rules in environments with externalities. We prove the impossibility of implementing efficient and ====-individually rational rules in ====. We prove that the ==== of our results. We extend our analysis to weakly efficient rules.,"We study the non-cooperative implementation of cooperative solutions to allocation problems with externalities. That is, we are interested in environments in which the agents care not only about what they receive, but also about what other agents get. An example is the mask allocation in the midst of a viral pandemic where the use of the masks reduces contagion (see, among others Prather et al., 2020). Other relevant examples include housing markets and roommate problems in which the agents are concerned also about their potential neighbors, labor markets in which workers care about their colleagues, and school choice problems in which families care about their children’s classmates.====We are interested in implementing social choice rules that are efficient and guarantee individual participation. Among those, the core is of focal importance in cooperative game theory. An allocation is in the core if it is resistant to group deviations. Sönmez (1999) proves that a strategy-proof and individually rational social choice function exists if and only if the core is essentially single valued, whenever the core is nonempty. Unfortunately, the core is often empty in markets with externalities. The reason is that the standard definition of blocking assumes that, when a coalition deviates, the agents outside the coalition will not react. The assumption is overly optimistic from the point of view of the deviating agents and multiplies the number of blocking coalitions.==== In our model, the members of a coalition do not make any assumption about the goods assigned to the agents outside the coalition and they act prudently. More precisely, we assume that a coalition blocks an allocation if each member of the coalition prefers the worst allocation consistent with the deviation to the proposed allocation.==== We call this kind of blocking ====-blocking and the ====-core is the set of ====-unblocked allocations.==== The ====-core has been introduced in Aumann and Peleg (1960) and Aumann (1961).==== Several works have assumed prudent behavior for contexts with externalities. Among them, Sasaki and Toda (1996) and Contreras and Torres-Martínez (2021) study pairwise stability in marriage markets with externalities and in roommate problems with externalities, respectively.====First, we study implementation in dominant strategies. Hong and Park (2018) consider a model with externalities and prove that if the agents care about their allocation prior to the other agents’, the Top Trading Cycle (==== from now on) generates an efficient, individually rational, and strategy-proof, social choice function. We prove that their result do not generalize to arbitrary externalities: no ====-individually rational and efficient mechanism is implementable in dominant strategies in allocation problems with externalities.====This negative result leads us to consider the implementation of the ====-core correspondence in Nash equilibrium. This research question relates to Sönmez (1996) who shows that the core correspondence is implementable in Nash equilibrium in a class of assignment problems in which the core is nonempty and agents have strict preferences over the objects, thus without externalities. Kara and Sönmez, 1996, Kara and Sönmez, 1997 prove the Nash implementability of stable (and thus core) solutions in one-to-one and many-to-one matching markets, respectively, under strict preferences over partners. Fonseca-Mairena and Triossi (2019) prove the Nash implementability of stable solutions in one-to-one matching markets with externalities, in which pairs act prudently when deviating. In the last case, stable allocations are not necessarily efficient and thus do not belong to the core.====We prove that the ====-core is not implementable in Nash equilibrium under general preferences. Restricting our attention to the domain ==== in which the ====-core equals the weak ====-core and both are nonempty, we prove that the ====-core correspondence is implementable in Nash equilibrium. When preferences are strict the ====-core equals the weak ====-core. This condition is satisfied, for example, in the marriage market with externalities studied in Sasaki and Toda (1996), the roommate problem with externalities studied in Contreras and Torres-Martínez (2021), and the housing market with externalities studied in Mumcu and Saglam (2007), among others. The equality between the ====-core and the weak ====-core is also satisfied in marriage markets and roommate problems without externalities and strict preferences over agents (see Sönmez, 1996). Our result thus applies to all such markets when the ====-core is nonempty. Also, relaxing the efficiency requirement to weak efficiency, we prove the Nash implementability of the weak ====-core correspondence on the domain of preference in which it is nonempty.====We then study the limits of implementation. In the subdomain of ==== in which the preferences are strict the ====-individually rational and efficient social choice rule is implementable in Nash equilibrium. We prove that this is no longer true in ====. In this case, the ====-core is the maximal implementable, ====-individually rational, and efficient social choice rule with domain ====.====Finally, we prove that the ====-core is the minimal monotonic (and thus implementable), ====-individually rational, and efficient solution, under an additional assumption which limits the externalities that can occur when agents keep their endowments. Thus, under this assumption, the ====-core is the unique implementable, ====-individually rational, and efficient social choice rule with domain ====. We also consider the limits of implementation of ====-individually rational and weakly efficient social choice rules.====The paper is organized as follows. In Section 2 we introduce the model. In Section 3 we consider implementation in dominant strategies. In Section 4 we study implementation in Nash equilibrium. In Section 5 we conclude. The Appendix includes complementary results and examples about the environments we consider.",Incentives and implementation in allocation problems with externalities,https://www.sciencedirect.com/science/article/pii/S0304406821001658,1 December 2021,2021,Research Article,57.0
Quartieri Federico,"Department of Economics and Management, University of Florence, Florence, Italy","Received 24 April 2021, Revised 11 September 2021, Accepted 21 November 2021, Available online 29 November 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102609,Cited by (2),The paper examines the conditions for the existence of maximals of a relation on every nonempty ,"The maximal elements of a preference relation on a set of feasible alternatives are often interpreted as the optimal choices of a rational agent. Given this interpretation, any set of conditions that guarantees the existence of a maximal of a relation supplies us with information about the circumstances that allow for an optimal choice. Some authors, however, are inclined to evaluate the importance of these sets of conditions according to their capability to be applied to parametric optimization problems of economic interest. In this regard, Walker (1977) observed: ====Walker’s observation was motivated by the structure of fundamental problems of economic theory like, for instance, the nonemptiness of the consumer’s demand. In the basic version of that problem, the consumer is endowed with a level of wealth ==== and with a preference relation ==== defined on a commodity space ==== identified with the nonnegative orthant of some ====-dimensional Euclidean space. As long as all components of the market price vector ==== are positive and the consumption set ==== is a closed subset of the commodity space ====, the image of the demand correspondence is the set of maximals of ==== on a compact budget set ====. Without additional assumptions, the budget set ==== could be any compact subset of the ground set of ==== and the issue of the non-vacuity of consumer’s optimal choice for every possible configuration of the triple ==== admitting a nonempty budget set boils down, simply, to the abstract problem of the existence of maximals of ==== on every nonempty compact subset of the ground set of ====. The present work deals, precisely, with such abstract problem.====Alexander Doniphan Wallace proved, in a 1945 article on fixed points (see Wallace, 1945), that any transitive and reflexive relation with closed upper sections possesses a maximal on every nonempty compact subset of its ground set. That result – which, in fact, is asserted under additional assumptions of topological nature – is presented as one of many auxiliary lemmas of Wallace’s article. Only some years later Lewis Edes Ward, Jr. – a student of Wallace – reformulated it as a formal theorem on the existence of a maximal element in his 1954 article on partially ordered topological spaces (see Ward, 1954). In the literature, the result obtained in the 1945 article by Wallace is sometimes reckoned “a folk theorem in optimization theory” (see, e.g., Evren and Ok, 2011) and, indeed, the very Wallace (see Wallace, 1962) seemed to share that opinion when claiming that his result “was certainly to be obtained by any mathematician who was interested in these matters”. It is a fact, however, that it was first asserted and proved in the 1945 article by Wallace.====About a quarter of a century later, a series of works – among which, the mentioned article by Walker (1977) – contributed to the formulation of a result according to which any acyclic relation with open lower sections has a maximal element on every nonempty compact subset of its ground set. Subsequently, at the end of the nineties, that result has been generalized in Subiza and Peris (1997) by relaxing the openness of lower sections. The natural question is: What connection is there between the old existence result that assumed the closedness of the upper sections of a preorder relation and the relatively new results assuming the openness of the lower sections of an acyclic relation (or its weakening introduced in Subiza and Peris, 1997)? This work answers to the previous question by proving the following fundamental result: ====As right traces are transitive and reflexive by nature, the closedness of their upper sections implies the existence of a maximal element of the right trace by virtue of Wallace’s result: observing that a relation ==== that satisfies the assumptions of any existence result mentioned so far is Suzumura-consistent and has a transitive closure whose right trace possesses closed upper sections, the desired connection is readily established. In the light of this fact, the Suzumura-consistent relations whose transitive closure possesses a right trace with closed upper sections will be called relations with the W-property. A variant of the previous observation concerning the closedness of the upper sections of the right trace of a relation is proved in Banks et al. (2006): the application of such a variant in Duggan (2011) further testifies to the value of the use of right-traces in the analysis of the existence of maximals. It must be observed, also, that the idea of using one-sided traces to prove the existence of undominated maximals – a selection of unconstrained maximals due to Peris and Subiza (2002) – is pursued in Alcantud et al. (2010); however, the use made in that article and the purposes thereof differ substantially from those of the present paper. This work considers also a weakening of the W-property that is satisfied by some relations of interest to economics (used, e.g., in Welfare Economics and Multi-Utility Theory) and proves the sufficiency of that condition for a relation to possess a maximal on every nonempty compact subset of its ground set. Interestingly, a relativized version of the W-property – that further weakens the definition of the W-property – turns out to be necessary and sufficient for a relation to possess a maximal on every nonempty compact subset of its ground set.====The paper is structured as follows. Section 2 introduces some definitions and notation. Section 3 enunciates the fundamental result mentioned above and discusses two important conditions employed in our analysis. By making use of the W-property, Section 4 shows a first unifying existence theorem and discusses some examples of economic interest. By making use of a weakening of the W-property, Section 5 proves a second unifying existence theorem and discusses other examples of economic interest. Section 6 shows a necessary and sufficient condition for the existence of a maximal on every nonempty compact subset of the ground set of a relation. Section 7 concludes. An Appendix contains some preliminary results as well as the proofs of all Propositions enunciated in the main body of the paper.",A unified view of the existence of maximals,https://www.sciencedirect.com/science/article/pii/S0304406821001634,29 November 2021,2021,Research Article,58.0
"Núñez Matías,Pimienta Carlos,Xefteris Dimitrios","CNRS, CREST, Ecole Polytechnique, 91128 Palaiseau, France,School of Economics, UNSW Business, The University of New South Wales, Sydney, Australia,University of Cyprus, Department of Economics, P.O. Box 20537, 1678 Nicosia, Cyprus","Received 8 February 2021, Revised 19 October 2021, Accepted 31 October 2021, Available online 17 November 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102595,Cited by (1),"In the single-peaked domain, the median rule is strategy-proof but not implementable in (Bayes–)Nash equilibrium by its associated direct mechanism. We define the value-based median mechanism that implements the median rule in (Bayes–)Nash equilibrium in the single-peaked domain under complete and incomplete information. Such a mechanism selects the median of the profile of different values announced by the agents (i.e., ignoring redundant announcements). The value-based median does not depend on agents’ beliefs (in line with robust mechanism design). In the case of incomplete information, it induces truthful revelation of preferences (in line with strategy-proofness) for almost all peaks. We present extensions of our results to generalized median rules and finite policy spaces and their limitations.","The median rule is well known to be strategy-proof in the single-peaked domain. Therefore, revealing one’s actual peak is always a best response under the associated direct mechanism (henceforth, ====). Yet, as observed by Repullo (1985) and later generalized by Saijo et al. (2007), the direct mechanism associated with a rule need not Nash implement such a rule. In many environments of interest, if the direct mechanism associated with a rule implements the rule in dominant strategies, it fails to implement it in (Bayes–)Nash equilibria. There are reasons to investigate implementation in equilibrium even if the rule in question is implementable in dominant strategies. Indeed, there is a literature dealing with the agents’ difficulties to recognize and behave according to the fact that truth-telling is always a best response in strategy-proof mechanisms. This literature contains both experimental evidence (see Kagel et al., 1987, Kagel and Levin, 1993, Attiyeh et al., 2000, Kawagoe and Mori, 2001, Cason et al., 2006) and recent theoretical developments on ==== (see Li (2017) and Arribillaga et al. (2020)).====The classical domain of single-peaked preferences is one environment where it is impossible to reconcile strategy-proofness with implementation in (Bayes–)Nash equilibrium. Saijo et al. (2007) show that any social choice rule that is implementable in Nash equilibrium and dominant strategies must be either dictatorial or Pareto inefficient. In particular, the median mechanism admits a multiplicity of equilibrium outcomes so that it does not implement the median rule in Bayes–Nash equilibria.==== This observation leads to the following questions: Is the median mechanism implementable in equilibrium by some other direct mechanism? Does it matter for equilibrium implementation whether agents’ preferences are public or private information?====We present a unified treatment of both questions. We study the implementation of the median rule in (Bayes–)Nash equilibrium in both complete and incomplete information settings and show that, with a continuum set of alternatives, the median rule is indeed implementable using a direct mechanism: the value-based median (see a detailed description at the end of the introduction). Therefore, we show that one can design mechanisms whose unique equilibrium outcome coincides with the alternative indicated by the median rule. These positive results provide a novel intuition since, to the extent of our knowledge, little is known about (Bayes–)Nash equilibrium implementation of the median: it satisfies Maskin monotonicity (and hence is Nash implementable), and there are indirect rules that Nash implement it (see Sprumont (1995), Barberà and Jackson (1994), Berga and Moreno (2009) and Núñez and Xefteris (2017)). Notably, not much is known about Bayes–Nash equilibrium implementation of social choice rules in the single-peaked domain when the agents’ preferences are private information. We build a direct mechanism whose unique Bayes–Nash equilibrium is truthful and whose outcome coincides with the one prescribed by the median rule with probability one.====Apart from establishing that the median rule is implementable in (Bayes–)Nash equilibrium in its usual setting (finite number of voters and a continuum of alternatives) we also show that we can achieve this in a robust and truthful manner. The former property relates to ==== (see Bergemann and Morris (2005) and Saijo et al. (2007)), and it allows the designer to be ignorant about the agents’ beliefs. The latter is a feature typically found in strategy-proof mechanisms. These results seem to be of independent interest as the trade-off between strategy-proofness and (Bayes–)Nash equilibrium implementation has attracted recent attention within the literature. Our findings imply that in the single-peaked domain with a continuum of alternatives, it is possible to combine appealing features of all approaches: i.e., it is possible to design a mechanism that (a) does not depend on the exact beliefs that agents hold about the preferences of the other agents, (b) involves truthful behavior by every agent, and (c) whose equilibrium outcome coincides with the alternative indicated by the median rule.====We also extend our results and explore to which extent they can be used to implement GMRs or when the set of alternatives (and preference types) is finite. In terms of GMRs,==== we show that any GMR with distinct phantoms is Nash implementable through a direct mechanism in environments of complete information, and we establish the analogous result under incomplete information. When the set of alternatives is finite (as detailed in the appendix), the implementation of the median rule depends on the information that agents hold about other agents’ preferences. With complete information, we can obtain Nash implementation of the median rule using direct and minimally probabilistic mechanisms (i.e., off-equilibrium, the mechanisms induce a lottery over alternatives). Regarding incomplete information, we present an example (based on a result by Triossi (2005)) that shows that the median rule fails Bayesian Monotonicity (a necessary condition for Bayes–Nash implementation, see Jackson (1991) among others).====We now turn to describe the value-based median mechanism and its differences with respect to the median mechanism.",On the implementation of the median,https://www.sciencedirect.com/science/article/pii/S030440682100152X,17 November 2021,2021,Research Article,59.0
"Roy Souvik,Sadhukhan Soumyarup","Economic Research Unit, Indian Statistical Institute, Kolkata, India,Department of Computer Science and Automation, Indian Institute of Science Bangalore, India","Received 19 June 2021, Revised 5 October 2021, Accepted 31 October 2021, Available online 15 November 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102593,Cited by (0),"We consider a weaker notion of strategy-proofness called upper contour strategy-proofness (UCSP) and investigate its relation with strategy-proofness (SP) for random ==== (RSCFs). Apart from providing a simpler way to check whether a given RSCF is SP or not, UCSP is useful in modeling the incentive structures for certain behavioral agents. We show that SP is equivalent to UCSP and elementary monotonicity on any domain satisfying the upper contour no restoration (UCNR) property. To analyze UCSP on multi-dimensional domains, we consider some block structure over the preferences. We show that SP is equivalent to UCSP and block monotonicity on domains satisfying the block restricted upper contour preservation property. Next, we analyze the relation between SP and UCSP under unanimity and show that SP becomes equivalent to UCSP and multi-swap monotonicity on any domain satisfying the multi-swap UCNR property. Finally, we show that if there are two agents, then under unanimity, UCSP alone becomes equivalent to SP on any domain satisfying the swap UCNR property. We provide applications of our results on the unrestricted, single-peaked, single-crossing, single-dipped, hybrid, and multi-dimensional domains such as lexicographically separable domains with one component ordering and domains under committee formation.","We consider a society with ==== agents and ==== alternatives. Each agent has a preference over the alternatives and a ==== (RSCF) selects a probability distribution over the alternatives at every collection of preferences of the agents. An RSCF is ==== (SP) if no agent can increase the probability of any upper contour set of his preference by misreporting his preference. It is ==== (UCSP) if no agent can increase the probability of an upper contour set by misreporting his preference ==== the same upper contour set. Clearly, UCSP is much weaker than SP.====The main objective of this paper is to find conditions on a domain so that UCSP and SP become equivalent (with or without unanimity). The question arises: why is it important to explore such an equivalence? The most important reason is to provide a simpler way to check if a given RSCF is strategy-proof. The fact that UCSP is a significant weakening of SP is established in the literature; in fact, the proportion of the number of constraints under UCSP to that under SP goes to zero as the number of agents ==== goes to infinity (see Chun and Yun, 2019 for a detailed account of this). Another reason we study UCSP is that it can be used to model behavioral agents. Even though agents have complete preferences, they might have alternatives classified as acceptable, unacceptable, etc. Due to behavioral reasons such as ethics, stigma, or self-guilt, they might be uncomfortable to vouch for some candidate who they very much dislike. Consequently, they maintain some upper contour sets of their sincere preference while manipulating and look for an increase of the probability of these sets only.==== (LSP) is another well-studied weakening of SP. LSP requires SP only for profiles that are “close” in some sense (see Sato, 2013, Carroll, 2012, Kumar et al., 2021; Kumar et al., 2021 for details).==== The reason we work with UCSP is that there is not much progress with the LSP approach for RSCFs, particularly for multi-dimensional (separable) domains (in comparison with that for deterministic social choice functions where necessary and sufficient conditions are known). Cho (2016) shows that LSP is equivalent to SP (without unanimity) on any domain satisfying the “====” property. Multi-dimensional separable domains are not “====” and hence this result does not apply to these domains (among others). To our understanding, it is “hard” to keep track of probabilities by LSP on a domain that is not swap-connected, which is why the literature has not progressed much in this direction. This motivates us to take the new approach of UCSP, which has its own importance as well.====We provide a condition on a domain so that SP becomes equivalent to UCSP and a property called ==== (EM) (see Majumdar and Sen, 2004, Mishra, 2016). EM is an immediate consequence of SP and cannot be avoided in characterizing SP. Our result applies to a large class of one-dimensional domains of practical importance such as the unrestricted domain, single-peaked domain, single-crossing domain, single-dipped domain, hybrid domain, etc. Next, we explore the equivalence of SP and UCSP when there is a “block structure” over the preferences. We introduce a generalization of EM called ==== (BM) and show that if a domain satisfies the ==== (BRUCP) property then SP becomes equivalent with UCSP and BM. This result applies to many well-known multi-dimensional domains like Lexicographically separable domains with one component ordering and domains under committee formation. Finally, we investigate the equivalence of SP and UCSP under unanimity. We introduce a restricted version of EM called ==== and show that if a domain satisfies the multi-swap upper contour no-restoration (multi-swap UCNR) property, then a unanimous RSCF on it is SP if and only if it is UCSP and satisfies multi-swap monotonicity. We further show that if there are two agents and the domain satisfies the swap upper contour no-restoration (swap UCNR property), then a unanimous RSCF on it is SP if and only if it is UCSP. In other words, when there are two agents, UCSP ==== becomes equivalent to SP on the mentioned class of domains. These results apply to a large class of well-known domains such as the unrestricted domain, single-peaked domain, single-crossing domain, single-dipped domain, hybrid domain.====It is worth mentioning that our results (for both with and without unanimity) apply to many more domains relative to the domains on which the equivalence of LSP and SP is known to hold for RSCFs. Moreover, we provide our analysis for both unanimous and non-unanimous RSCFs on a common platform; to the best of our knowledge, these two cases are treated separately in the context of LSP.",On the equivalence of strategy-proofness and upper contour strategy-proofness for randomized social choice functions,https://www.sciencedirect.com/science/article/pii/S0304406821001518,15 November 2021,2021,Research Article,60.0
Dosis Anastasios,"Department of Economics – ESSEC Business School and THEMA Research Center, 3 Av. Bernard Hirsch, B.P. – 50105, Cergy, 95021, France","Received 22 October 2020, Revised 17 September 2021, Accepted 20 October 2021, Available online 8 November 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102591,Cited by (1),"This article studies general economies with adverse selection in which symmetric companies supply (potentially multiple) plans to privately informed consumers and compete in terms of price schedules. I show that a basic price cap regulation, in which the price caps are endogenously determined by companies, discourages risk selection over efficient allocations, and therefore, equilibrium exists in every economy. Moreover, I demonstrate that in generalisations of Rothschild and Stiglitz (1976) and Wilson (1977) economies, companies earn zero profits in equilibrium, and every equilibrium allocation is efficient."," ====■ It is widely accepted that competition in markets is desirable in the sense that it forces companies to price products according to their marginal cost of production. Nonetheless, in markets plagued by adverse selection, such as insurance markets, there are concerns that fierce competition does not necessarily promote efficiency due to risk selection.==== Risk selection is considered to be an impediment to effective and fair competition, and different policies are thus implemented in practice to mitigate it.====Frequently implemented policies include, among others, plan restrictions and/or risk adjustment. Plan restrictions, such as minimum insurance requirements, implicitly limit insurers’ ability to effectively target specific risk profiles but encourage low-risk individuals (i.e., young and healthy individuals) to remain uninsured. Therefore, to avoid potential death spirals, policy makers usually impose subsidies or penalties to expand the share of insured.==== In contrast, risk adjustment rewards (punishes) insurers for accommodating high (low) risks, thereby mitigating the incentives to target specific risk profiles. Nonetheless, risk adjustment is never perfect because it relies on signals attempting to predict a risk profile. Provided that most commonly implemented policies have drawbacks, an open question is whether alternative policies might exist that correct risk selection without undermining competition.====In light of the rising costs of health insurance premiums, it has been suggested that price regulation be imposed on health insurance rates. For instance, in 2011, the California Assembly passed a bill (i.e., AB 52) that subjects health insurance companies to vigorous rate regulation. Similar rate regulations are in place in several other insurance markets. According to the US National Association of Insurance Commissioners, ====Some economists and policy makers have gone even further by pointing to a specific form of price regulation, that of price caps, as a way to alleviate inefficiencies and tame rising health insurance rates. For example, Chernew et al. (2019) write, ====This article evaluates the effectiveness of price cap regulation, in which price caps are endogenously determined by companies, in general insurance economies and shows that price caps could eliminate risk selection without undermining competition.====Section 2 presents a general model in which there is a finite number of symmetric insurance companies and price-taking consumers. As in the seminal studies of Rothschild and Stiglitz (1976) and Wilson (1977), insurance companies supply insurance plans. The set of feasible insurance plans is allowed to be finite or a continuum. Each consumer is characterised by a type that includes non-observable characteristics, which, by definition, cannot be used for direct discrimination, or observable characteristics, which, for institutional reasons, cannot be used for direct discrimination (e.g., pre-existing conditions). As is common in markets with adverse selection, the cost of a plan depends on the type of consumer who purchases this plan. For instance, the same health insurance plan has a lower cost if purchased by a healthy rather than a nonhealthy individual. Utility and cost functions do not have a specific structure.====An allocation is a set of plans, one for each type. An allocation is incentive compatible (IC) if every type has an incentive to truthfully reveal her type. An IC allocation is feasible if the average resource constraint is satisfied. An allocation is efficient if it is IC and feasible, and there exists no other allocation that is IC and feasible and improves the payoff of at least one type without hurting the remaining types.==== An IC allocation is individually rational (IR) if every type earns a higher payoff than it would if it remained uninsured.====The early seminal contributions of Rothschild and Stiglitz (1976) and Wilson (1977) emphasised the difficulties in sustaining efficient allocations as equilibrium allocations in markets with adverse selection when cross subsidisation is required. One of the main findings of these articles is that efficient allocations are susceptible to cream skimming by competitive insurers. As mentioned above, the main contribution of the present article is to study the effectiveness of price caps in eliminating risk selection without restricting competition in general insurance economies. To that extent, the present article studies a game in which companies each propose two price schedules: a schedule of proposed price caps and a competitive price schedule. Any company’s final price is the minimum price between the price schedules it proposes and the remaining companies’ price caps. For instance, if a company offers a price for a plan that is above the proposed price caps, that plan’s price is adjusted downwards to meet the proposed price cap.====Two main results emerge from the analysis. Theorem 1 establishes that price caps limit the set of profitable deviations of firms, and consequently, every efficient and IR allocation can be sustained as an equilibrium allocation. The universal existence theorem established in this article starkly contrasts with Rothschild and Stiglitz (1976), who find robust regions of nonexistence. The discrepancy between the existence result established in this article and the nonexistence result established in Rothschild and Stiglitz (1976) lies in the nature of competition. In particular, Rothschild and Stiglitz (1976) envision a market in which a company can freely enter and offer any price that it wishes by disregarding the prices offered by rivals. Therefore, entrants can attract low-risk consumers without attracting high-risk consumers. This contrasts with the model studied in this article, in which companies are able to set price caps. When companies are allowed to decrease but not increase the prices of plans over the proposed price caps, any attempt to select the most profitable types will be accompanied by the attraction of the less profitable types that will render risk selection unprofitable.====The second main result of the article concerns generalisations of Rothschild and Stiglitz (1976) and Wilson (1977) economies. Theorem 2 establishes that in these types of economies, every equilibrium allocation is efficient. Such markets feature a sufficient structure that allows familiar Bertrand-style competition among companies to steal profits away. In particular, for every inefficient allocation, a company, by decreasing its prices for some plans, can attract a nonzero measure of types and increase its profits. Combined with the existence result described above, this implies that in these economies, the set of equilibrium allocations consists of every efficient and IR allocation.====It is worth noting that if a regulator had precise information on the type space and its distribution, it would be able to set the price caps without relying on the companies. It appears to be more realistic to assume that the regulator lacks such information and needs to rely on the truthful revelation of the companies. Therefore, one of the innovations of the present article is that price caps are determined endogenously by companies and not by any central authority. Therefore, this article contributes to a recent growing body of literature that studies regulation under the assumption that regulators have little or no information.",Price caps and efficiency in markets with adverse selection,https://www.sciencedirect.com/science/article/pii/S0304406821001506,8 November 2021,2021,Research Article,62.0
Cheng Xiaoyu,"Department of Managerial Economics and Decision Sciences, Kellogg School of Management, Northwestern University, Evanston, IL, USA","Received 18 June 2021, Revised 7 September 2021, Accepted 23 October 2021, Available online 6 November 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102587,Cited by (1),This paper proposes and axiomatizes a new updating rule: Relative Maximum Likelihood (RML) updating for ambiguous beliefs represented by a set of priors (====). This rule takes the form of applying Bayes’ rule to a subset of ====. This subset is a linear contraction of ,"For decisions under uncertainty, when information is not sufficient to pin down a unique distribution over states, the decision-maker (DM)’s revealed preference sometimes is not consistent with any single probabilistic belief. Yet it could be consistent with a set of priors (Ellsberg, 1961, Machina and Schmeidler, 1992). When the DM learns additional information, updating of the set of priors may involve two steps. First, she could use this information to make inference about the plausibility of each prior and discard those deemed to be implausible. Second, she applies Bayes’ rule to update every prior in this refined set conditional on the information received.====The two most popular updating rules for multiple priors, ==== and ====, are both examples of such an updating procedure. FB takes the form of applying Bayes’ rule to the entire set of priors. In other words, the DM does not discard any prior under FB updating. In contrast, under ML updating, the DM discards priors that do not ascribe the maximal probability (among all priors in the set) to the observed event and updates the remaining priors according to Bayes’ rule. Therefore, FB and ML can be regarded as polar extremes in terms of discarding priors based on likelihood of the observed event in updating.====However, sometimes a DM may not be willing to be as extreme as either case. For example, she may find the priors assigning a very small probability to the observed event to be implausible, while at the same time would like to keep the priors ascribing an almost maximal probability. Such an intermediate behavior cannot be captured by either of the two extremes. Moreover, the preference behaviors corresponding to such non-extreme updating rules have not been extensively studied and identified in the literature.====In this paper, I propose and characterize a new updating rule, ==== updating, which takes the form of applying Bayes’ rule to a subset of the set of priors. The specific subset is determined by a convex combination of the set of priors and its subset which ascribes a maximal probability to the observed event.==== It can also be understood as a linear contraction of the set of priors towards its maximum-likelihood subset. Consequently, RML is able to provide a means of intermediate updating between FB and ML. Moreover, its specific form suggests that under RML not only likelihood but also ==== of the initial set of priors are crucial for determining the set of posteriors.====Formally, let a closed and convex set ==== denote the set of priors over a state space ====. For some conditioning event ====, let ==== denote the subset of ==== attaining maximum likelihood of the event ====, i.e. ====.====RML captures the intermediate updating behaviors in the sense that for all ==== one has ====. Furthermore, FB and ML are included as extreme special cases.====The updating rule, RML, is defined under a single event ====. For conditional preferences, a DM may apply RML updating with different parameters ==== for different events. Such a phenomenon can be seen in an experiment by Liang (2021).==== To allow for such a possibility, I consider two different representations of conditional preferences that differ in the extent to which the parameter ==== is allowed to vary across events. On the one hand, the conditional preferences are represented by ==== if the DM applies RML updating with parameter ==== that potentially depends on the event ====. On the other hand, the conditional preferences generated by applying RML with a constant parameter ==== are said to be represented by ====.====I provide foundations for conditional preferences represented by Contingent RML and RML when the preferences admit Maxmin Expected Utility (MEU) representations (Gilboa and Schmeidler, 1989). Under MEU, the DM evaluates prospects according to the worst expected utility generated by the set of priors.====For MEU preferences, Pires (2002) characterizes FB by a simple behavioral axiom. Gilboa and Schmeidler (1993) axiomatize ML when preferences admit both MEU and Choquet Expected Utility (CEU) representations, which is a strict special case of the MEU preferences. Their axiomatization, however, does not extend to the more general case. In fact, there has been no axiomatization of ML under general MEU preferences in the literature. The first main result of this paper (Theorem 2.7 and more generally Theorem B.1) addresses this long-standing open question by providing a characterization of ML for MEU preferences. Moreover, this result also proves to be instrumental for the characterization of Contingent RML and RML.====Both representations are characterized by weakening the axioms leading to FB and ML. In turn, those axioms are relaxations of the well-known ==== principle.==== More specifically, Theorem 3.3 characterizes the Contingent RML by two behavioral axioms. In addition, Theorem 3.4 shows that adding one axiom to the characterization of contingent RML is necessary and sufficient to pin down a constant ==== across events.====These characterization results not only identify the behavioral foundations for the type of non-extreme updating specified by RML updating, but also suggest the common behavioral patterns under FB and ML, two seemingly orthogonal updating rules.====Motivated by these characterizations, in this paper I further identify a key issue in the applications of ambiguity and showcase how RML can be applied to address it.====Many recent applications of the MEU model assume the players update according to FB.==== In settings such as mechanism design or information design, these applications find that the introduction of ambiguity is strictly beneficial for the principal. However, there is clearly a caveat that this finding may hinge on the specific assumption of FB updating. RML, as a larger family of updating rules including FB, provides a useful tool for examining this issue.====As an illustration, Section 5 analyzes an example in the context of ambiguous persuasion studied by Beauchêne et al. (2019). In this example, they construct an ambiguous persuasion scheme granting the sender strictly more payoff than the optimal Bayesian persuasion under the assumption that the receiver uses FB updating. I first show that this scheme becomes strictly worse than the optimal Bayesian persuasion whenever the receiver slightly deviates from FB in the direction of ML. The level of such a deviation can be conveniently captured by the parameter ==== under RML. This finding shows that the particular construction used by Beauchêne et al. (2019) can be non-robust with respect to updating rules.====Nonetheless, I construct an alternative ambiguous persuasion scheme. It further requires that all the probabilistic devices in an ambiguous device have the same overall probability of sending the same signal. As a result, the receiver would behave exactly the same no matter what attitude he has towards discarding priors based on likelihood. I call such a scheme ====. I further show that the sender can do strictly better using uniform-likelihood ambiguous persuasion compared to Bayesian persuasion in this example. In other words, the strict gain from using ambiguous strategies does not rely on the specific assumption of FB updating in this case.====The remainder of this paper is organized as follows. Section 2 sets up the environment and provides a characterization of ML updating. Section 3, the main section, gives the formal definition of preferences represented by Contingent RML and RML and also provides the foundations for them. Section 4 looks at a special environment with ambiguous signals and illustrates the predictions and interpretations that Contingent RML and RML are able to offer. Section 5 applies RML to the example of ambiguous persuasion and addresses the issue of robustness to updating. Section 6 talks about the related literature. Section 7 concludes. All the proofs are collected in Appendix A.",Relative Maximum Likelihood updating of ambiguous beliefs,https://www.sciencedirect.com/science/article/pii/S0304406821001488,6 November 2021,2021,Research Article,63.0
"Carmona Guilherme,Podczeck Konrad","University of Surrey, School of Economics, Guildford, GU2 7XH, UK,Institut für Volkswirtschaftslehre, Universität Wien, Oskar-Morgenstern-Platz 1, A-1090 Wien, Austria","Received 23 November 2020, Revised 15 September 2021, Accepted 5 October 2021, Available online 29 October 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102580,Cited by (1),"We present results on the relationship between non-atomic games (in distributional form) and approximating games with a large but finite number of players. Specifically, in a setting with differentiable ====, we show that: (1) The set of all non-atomic games has an open ==== such that any finite-player game that is sufficiently close (in terms of distributions of players’ characteristics) to a game in this subset and has sufficiently many players has a strict pure strategy ","In a seminal paper, Schmeidler (1973, Theorem 2) proved the existence of pure strategy Nash equilibria in games with a continuum of players and finite action spaces. This was done by Schmeidler in the context of anonymous games (i.e., games where the payoff of a player is, apart from his/her own action, determined by the distribution of the actions taken by the other players). After all, of course, the continuum specification is an idealization of situations with a large but finite number of players, and the question arose whether Nash equilibrium existence results for games with a continuum of players are reflected in large finite-player games. In terms of ====-equilibria, this question was answered positively first by Rashid (1983). Since then, an extensive literature on this topic emerged.====In two recent papers (Carmona and Podczeck, 2020a, Carmona and Podczeck, 2020b) we have addressed the question to which extent pure strategy Nash equilibrium existence results for anonymous games with a continuum of players (for short, non-atomic games in the sequel) carry over into pure strategy Nash equilibrium existence results for large finite-player games. In Carmona and Podczeck (2020a) we have shown that, with finite action sets, there is a generic set of non-atomic games such that sufficiently large finite-player games which are close (in terms of distributions of players’ characteristics) to a game in this set have indeed Nash equilibria in pure strategies. Based on this result we have shown in addition that, in fact, any equilibrium distribution of any non-atomic game can be approximated by equilibrium distributions defined from pure strategy equilibria of large finite-player games (provided that action sets are finite).====In Carmona and Podczeck (2020b) we have dropped the requirement that action sets be finite and have proved analogous results in a setting with differentiable payoff functions. Actually, the results in this latter paper were formulated in a framework where the action set of players has non-empty interior in some Euclidean space and payoff functions satisfy a boundary assumption guaranteeing that equilibrium strategies are always in the interior of the action set. These assumptions were made to be in a convenient position to employ differentiability assumptions on payoff functions.====The purpose of the present note is to obtain results analogous to those in Carmona and Podczeck (2020b) for the case where the boundary assumption on payoff functions made there is not met, or the action set of players does not have non-empty interior in any Euclidean space. This makes it necessary to impose some smooth structure on the boundary of action sets. To achieve this, we find it convenient to work with local coordinates, assuming that every point in the action set has a neighborhood that look like a certain piece of the non-positive orthant of some Euclidean space. Technically speaking, we assume that the action set of players is a compact differentiable manifold with corners; see the next section for a definition. We emphasize that even though the general structure of the proofs of the results of the present note is similar to that in Carmona and Podczeck (2020b), numerous adjustments in the details become necessary to accommodate the proofs to the setting of action sets studied now (cf. Section 5.1 and Remark 2 in Section 3).====To illustrate the usefulness of dispensing with the assumption that players’ action sets have non-empty interior in some Euclidean space, and thus the usefulness of the present results to economic applications, consider a large population of individuals who live, or are considering living, in a given city. Modeling the city as the unit-circumference of a circle (as in, e.g., Salop (1979)) yields players’ action sets that have an empty interior. If, in addition, each player’s payoff function is ==== and depends both on his choice on where in the city to live and, due to the influence on how popular each neighborhood is, on summary statistics of the distribution of the choice of all other players (e.g. its first ==== (non-central) moments), then our results apply. They yield a strict equilibrium in all sufficiently large finite-player games that are close to a generic non-atomic game; thus such a potentially complex model of a city could be analyzed more conveniently in pure strategies or with a continuum of players or both.====To illustrate the usefulness of removing the assumption that equilibrium actions are in the interior of the action set, we present as a special case of our results an example on existence of Cournot equilibria for generic distributions of cost functions if the number of firms is large. In this application, any inverse demand function and any cost function are allowed provided that they are ====. This is in contrast with what we considered in the working paper version of Carmona and Podczeck (2020b) where some constraints on inverse demand and cost functions were imposed to guarantee that firms’ optimal actions are strictly positive and strictly below their capacities.====Technically, a circle is an example of a manifold without boundary, which is a special case of manifolds with corners. An example of a genuine manifold with corners is a compact interval in ====. Many other examples of compact differentiable manifolds with corners are possible. These include finite sets, disks, simplices, and stripes in any Euclidean space, or the set of solutions to some systems of equations. In fact, any subset of a Euclidean space which is homeomorphic to a differentiable manifold with corners can be given a structure such that it becomes a differentiable manifold with corners, too; e.g., a square in ====, being homeomorphic to a circle in ====, can be given a smooth structure.====It is true that our specification of action sets is not a must. An alternative would be to define action sets globally as the sets of those points ==== in ==== satisfying conditions of the form ==== and ==== where the ==== and ==== are finitely many differentiable functions from ==== to ==== such that constraint qualification holds. In practically all applications in economics, action sets which are covered as manifolds in our paper are defined in this way. The examples from above can be written in this form. One could then use the famous Kuhn–Tucker theorem to characterize maxima of payoff functions. However, we find it more convenient to state first and second order conditions for maxima of payoff functions in local coordinates. For this reason, and for sake of generality,==== we prefer to use the notion of differentiable manifold to specify action sets.====The rest of this note is organized as follows. In Section 2 some general notation is settled. In Section 3 the model and the main results are presented. This is followed by a section with the example on Cournot oligopoly. Proofs can be found in Section 5. An appendix contains some auxiliary material.",Strict pure strategy Nash equilibrium in large finite-player games when the action set is a manifold,https://www.sciencedirect.com/science/article/pii/S0304406821001439,29 October 2021,2021,Research Article,64.0
Kawamori Tomohiko,"Faculty of Economics, Meijo University, 1-501 Shiogamaguchi, Tempaku-ku, Nagoya 468-8502, Japan","Received 3 November 2020, Revised 28 September 2021, Accepted 9 October 2021, Available online 29 October 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102582,Cited by (0),"We investigate legislative bargaining where players first bargain over coalitions and after one coalition is formed, players in this coalition bargain over allocations. We show that if discount factors in coalition bargaining are smaller than those in allocation bargaining and sufficiently large, in any stationary subgame perfect equilibrium (SSPE), relatively impatient proposers immediately form a minimal winning coalition of relatively impatient players, but relatively patient proposers fail to form a coalition and bargaining delays occur. We also show that if the discount factors in coalition bargaining are smaller than those in allocation bargaining and sufficiently small, delays do not occur. Furthermore, we show that if the discount factors in allocation bargaining are smaller than those in coalition bargaining and sufficiently similar across players, there exist multiple SSPEs exhibiting delays and having different payoff tuples. We also introduce leader-dependent hedonic games, where each player has a preference relation over pairs of a coalition and its leader. We view a truncated game with replacing subgames of allocation bargaining by their SSPE playoff tuples to be based on a leader-dependent hedonic game.","In parliamentary democracy, a (coalition) government is formed, and policies are subsequently decided and implemented. After a general election, a prime minister is elected in a parliament, and a government is formed. If no party has a majority, a coalition government is formed. After the government formation, policies are decided and implemented. Usually, supporting parties agree on policies in the government formation phase. However, the agreement on policies is not justiciable (enforceable) because it is a political question. Also, non-fulfillment of the agreement may not lead to dissolvement of the government owing to the cost of formation of a new government.==== Thus, policies cannot be committed to in the government formation phase and are decided and implemented after the government formation.====We investigate legislative bargaining where players first bargain over coalitions and then bargain over allocations. More specifically, players bargain over coalitions, and after one coalition is formed, players in this coalition bargain over allocations within the coalition. This contrasts with settings in the existing literature on legislative bargaining, where players bargain over coalitions and allocations jointly.====We derive stationary subgame perfect equilibria (SSPEs) and show that bargaining delays may occur. If discount factors in coalition bargaining are smaller than those in allocation bargaining and sufficiently large, relatively impatient proposers immediately successfully form a minimal winning coalition of relatively impatient players, but relatively patient proposers fail to form a coalition, which implies that delays occur. In the legislative bargaining literature such as Baron and Ferejohn (1989), Eraslan (2002) and Kawamori (2013), where coalitions and allocations are jointly bargained over, delays do not occur. In these papers, each patient player gives impatient players such large shares of the surplus that they accept the proposal; in our paper, each patient player cannot do so, and thus, delays may occur. Comparing such existing papers with our paper, we can present a testable hypothesis: in countries where coalition agreements are justiciable (thus, coalition and allocation are jointly bargained over), delays do not occur in government formation; in the other countries, delays occur. We also show that if the discount factors in coalition bargaining are smaller than those in allocation bargaining and sufficiently small, delays do not occur. Furthermore, we show that if the discount factors in allocation bargaining are smaller than those in coalition bargaining and sufficiently similar across players, there are multiple SSPEs exhibiting delays and having different payoff tuples.====Our model is related to the leader-dependent hedonic games, which are defined in our paper. In a hedonic game, each player has a preference relation over coalitions. Meanwhile, in the leader-dependent hedonic game, each player has a preference relation over coalitions with leaders.==== In reality, coalitions often have a leader (e.g., a prime minister in government formation), and each coalition member’s well-being depends on the identity of the leader because the leader substantially influences decisions within the coalition. In our extensive form game, SSPE payoff tuples of subgames of allocation bargaining depend on coalitions and first proposers (leaders) in allocation bargaining. Thus, the truncated game obtained by replacing subgames of allocation bargaining by their SSPE payoff tuples is viewed as a game based on a leader-dependent hedonic game.====The leader-dependent hedonic games and bargaining delays are related to Bloch and Diamantoudi (2011) and Eraslan and Merlo (2017). Bloch and Diamantoudi (2011) investigate a non-cooperative bargaining game based on a hedonic game. However, their hedonic game is leader-==== one. They show that if the hedonic game is totally stable (i.e., the restriction of the hedonic game to any subset of players has a nonempty core), there exists an equilibrium exhibiting no delays. Eraslan and Merlo (2017) consider an environment where the surplus depends on the proposer, which is similar to our leader-dependent setting. However, in their non-cooperative game, players bargain over coalitions and allocations jointly,==== and thus, their non-cooperative game is not based on hedonic games. They show that there may exist an equilibrium exhibiting delays. In their paper, the ==== weakest players (i.e., the players who bring the ==== smallest surpluses) fail to pass a proposal, whereas in our paper, the ==== strongest players (the ==== most patient players) fail to pass a proposal, where ==== is the number of players and ==== is the voting quota.====Our model is related to proto-coalition bargaining. In proto-coalition bargaining, a tentative coalition (proto-coalition) is first formed, and the players in this proto-coalition then bargain over allocations. The most related paper in the proto-coalition bargaining literature is Montero (2015). In Montero (2015), when a proposal is rejected in allocation bargaining, the proto-coalition is dissolved with a certain positive probability. Montero (2015) shows that in any equilibrium, a proto-coalition is immediately formed. In our model, the coalition is not dissolved even when a proposal is rejected in allocation bargaining, and thus, it is not tentative but final. This is because if the coalition is dissolved, the players in the coalition suffer from the cost of forming a new coalition and/or risk of his/her failing to belong to a new coalition. The difference in delays between Montero (2015) and our paper may be caused by whether the coalition is dissolved or not.====Our model employs the rejecter-propose protocol. Under the rejecter-propose protocol, the rejecter of a proposal proposes a counter-offer in the next round. In coalitional bargaining or legislative bargaining, this protocol is often used (e.g., Chatterjee et al., 1993 and Kawamori, 2013). In reality, the rejecter of a proposal is often asked to propose a counter-offer. Another often-used protocol is the random-proposer protocol, under which the proposer is randomly selected in each round (e.g., Baron and Ferejohn, 1989 and Okada, 1996). Each player has a weaker incentive to reject a proposal under the random-proposer protocol than under the rejecter-propose one, because the rejecter becomes the proposer in the next round with a lower probability. Thus, our results on delays may not hold under the random-proposer protocol.====Our results are indirectly related to the literature regarding bargaining delays. Chatterjee et al. (1993) showed that delays occur in coalitional bargaining. In their example, where only two-player coalitions have a positive worth and the worth of two-player coalitions containing a certain player (weak player) is a half of that of the other two-player coalitions, the weak player offers an unacceptable proposal, two of the other players form a coalition and exit from the game, and he/she forms a coalition with the remaining player. In our paper, strong (i.e., patient) players’ proposals are rejected. Ray and Vohra (1999) showed that delays occur in coalitional bargaining with externalities. In their example, a player offers an unacceptable proposal, some of the other players form a coalition, and he/she then enjoys a positive externality from this coalition. In our model, there are no externalities of coalition formation.====Our results resemble those of Gamson (1961) in that players with some advantage are more likely to be excluded from a coalition. In Gamson (1961), players with a larger voting weight, who could obtain a larger payoff if they joined a coalition, are more likely to be excluded. In our paper, more patient players are more likely to be excluded. Gamson (1961) investigated a static model and did not consider bargaining procedures. Our paper explicitly considers bargaining procedures.====The remainder of this paper is organized as follows. Section 2 describes the model. Section 3 derives SSPEs. Section 4 presents a hedonic-game view. Section 5 discusses the assumptions in this paper.",Coalition-then-allocation legislative bargaining,https://www.sciencedirect.com/science/article/pii/S0304406821001452,29 October 2021,2021,Research Article,65.0
"Anderson Robert M.,Duanmu Haosui,Uyanik Metin","Department of Economics, University of California, Berkeley, CA 94720, USA,Institute for Advanced Study in Mathematics, Harbin Institute of Technology, Harbin 150001, China,Department of Economics, Johns Hopkins University, Baltimore, MD 21218, USA,School of Economics, The University of Queensland, Brisbane, QLD 4072, Australia","Received 20 November 2020, Revised 14 September 2021, Accepted 5 October 2021, Available online 28 October 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102581,Cited by (3),"In this paper we show the existence of equilibrium in an abstract economy with an arbitrary set of players, each of whose compact convex action sets lie in different ====, and whose preferences obey weaker continuity postulates that have been used in the original treatments of Arrow and Debreu in the early fifties, and of Shafer and Sonnenschein in the mid-seventies. Our theorem synthesizes results around the cardinality of the set of agents, and incorporates in particular recent results of Carmona–Podczeck and He–Yannelis. The proof of the theorem is based on nonstandard analysis and its novelty has independent methodological interest.","An abstract economy as originally formulated by Arrow and Debreu (1954) and Debreu (1952), and their followers==== sits astride an economy and a normal-form game, including both but not subsumed by either.==== As such it is a mathematical object with an autonomy and integrity on its own. In this paper we contribute to the theory of such abstract economies on both substantive and technical grounds: substantive with respect to our reading of the antecedent literature, and technical in that we present a methodology for the application of the non-standard analysis through which results taken from a finite set of agents are seamlessly extended to those for an arbitrary set of agents.====This is a one-theorem paper illustrated with two examples, on the existence of equilibrium in an abstract economy based on a set of agents of arbitrary cardinality, each of whose action sets belonging to possibly different locally convex topological vector spaces and whose preferences are continuous in a sense fully in keeping with the recent advances in the subject. We show that the theorem already testifies to its worth by generalizing results that are considered to be the state of the art in the subject. We see the contribution of this paper to mathematical economics hinging as much on technique and methodology as on the novelty and generality of the result: the methodological innovation stems from proceeding from an economy with a finite number of agents to an arbitrary set, rather than a measure-theoretic continuum, and on its drawing on the Shafer–Sonnenschein theorem as an ingredient of the proof of the result. This results in a satisfying seamlessness of the theory, and also testifies to the strong synthetic aspect of the work. Myerson (1984, p. 70) has commented on this, albeit in another context.====
 ====There is an added consideration not raised by Myerson. In so far as economies with a continuum of agents are concerned, an atomless measure-theoretic continuum has occupied center stage, but one can ask what is it that necessitates such a structure, and thereby to emphasize or to underscore and downplay its importance, especially when the underlying motivation is a synthetic one.==== Remaining within the synthetic motivation, games and economies in the mathematical economics literature, with a countable agent space, be it finite or infinite, have been analyzed without an explicit measure-theoretic structure.==== The first study of games with an arbitrary set of agents and the attendant use of the product topology is, to the best of our knowledge, due to Ma (1969) and Peleg (1969), and extended by Hart and Schmeidler (1989) and Salonen (2010). The theorem that we present generalizes the results of Carmona and Podczeck (2016) for games with a finite number of agents and those of He and Yannelis (2016) for an arbitrary set of agents.====Next, the assertion that there is some methodological novelty in the application presented here needs justification, and we turn to it. In a nutshell, the methodology derives from the work of Duanmu et al. (2021) and Duanmu and Roy (2021) in the context of mathematical statistics: more specifically, recent work on Markov Processes ascribed to them and their followers.==== In the context of the application of nonstandard analysis to mathematical economics, this then is the first paper==== to take a theorem true for a finite economic model and use it to prove properties for the infinite model by embedding it in a hyperfinite model. The use of nonstandard analysis allows an implementation of such a synthesis, and has so far not been posed in this way in the register of mathematical economics. We also know of no paper that applies nonstandard analysis to abstract economies. Two final points: first, it is our considered view that the line of investigation that it opens, and the method it investigates and executes, could not have proceeded in 1972 for the simple reason that the earlier work took as its point of departure results that were ==== for finite economies.==== Second, the methodology harkens back to Loeb’s ==== of a standard measure space,==== and the construction of a hyperfinite partition. It is worth noting that even though we do not appeal to Loeb, 1971, Loeb, 1972, Duanmu and coauthors were directly inspired by the results and their proofs in these two papers.====We now turn to the plan of the paper. Section 2 introduces basic notions of nonstandard analysis, but does it in a way that the reader can keep in mind elementary topological results relating to continuity and compactness. Section 3 presents our main result, Theorem 3.4, illustrated by two examples. Section 4 shows how Theorem 3.3 generalizes the results in the antecedent literature. Section 5 presents the proofs. We end the paper in Section 6 by discussing what we see to be exciting possibilities opened by the methodology presented and pursued in this work.",On abstract economies with an arbitrary set of players and action sets in locally-convex topological vector spaces,https://www.sciencedirect.com/science/article/pii/S0304406821001440,28 October 2021,2021,Research Article,66.0
"Li Yuan,Yang Jinqiang,Zhao Siqi","School of Finance, Shanghai University of Finance and Economics, Shanghai Key Laboratory of Financial Information Technology, China","Received 2 March 2021, Revised 14 August 2021, Accepted 9 October 2021, Available online 23 October 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102579,Cited by (0),"This paper studies the sovereign debt dynamics when the government exhibits present-biased preferences and can freely adjust sovereign debt. First, present bias plays a dual role. If the country is moderately indebted, the desire for instantaneous gratification drives the government to issue debt. However, with excessive indebtedness, the top concern is to procrastinate costly default, and present bias serves as an implicit commitment device that induces the government to repurchase existing debt. Second, the severe present-biased government suffers from debt intolerance: it has small debt capacity and pays high sovereign credit spreads, despite potentially maintaining a low target debt level. Finally, we find that long-term debt and budgetary restriction can mitigate the negative impact of present bias by disciplining the government’s over-borrowing incentives.","It has long been recognized that repurchasing the existing debt is suboptimal for sovereign (Bulow et al., 1988, Bulow and Rogoff, 1991, Aguiar et al., 2019). As pointed out, buying back sovereign debt implicitly transfers wealth from the debtor country to international lenders since the market price of debt is inflated in the secondary market. In particular, the government without ex-ante commitment is tempted to dilute the existing creditors by ratcheting the leverage up. Nevertheless, the voluntary debt buyback programme during the Greek debt crisis indicates that renewed attention should be paid (Aguiar and Amador, 2014); (Stiglitz and Rashid, 2020).====In this paper, we show that the combination of commitment friction and present-biased preferences generates the prominent features of sovereign debt dynamics mentioned above. A central issue in analyzing sovereign debt management is the credibility of the debtor country (Eaton and Gersovitz, 1981, Bulow and Rogoff, 1989). We model the commitment friction in the spirit of DeMarzo et al. (2021): the government cannot commit to future debt policy and default.==== In this circumstance, the government can freely adjust sovereign debt period by period. Default on sovereign debt triggers costly debt restructuring.====Moreover, prior studies mainly assume that the government is an exponential discounter, which implies that the government can commit to discount consumption flows with a constant interest rate (i.e., time-consistent preferences).==== Exponential discounting pioneered by Samuelson (1937) is widely employed to simplify the analysis, partly because preferences are likely to be time-consistent. However, as shown in Persson and Svensson, 1989, Alesina and Tabellini, 1990, and Aguiar and Amador (2011), political turnover leads to present-biased preferences.==== Due to rotation of power, political incumbents tend to make policies using discount factors that increase in relative time.==== Time-inconsistency arises because decision-makers act relatively patiently when two payoffs are both far away in time, but more impatiently when both of them are brought forward in time.====For tractability reasons, we model present-biased preferences in the spirit of Grenadier and Wang (2007) and Harris and Laibson (2013). They characterize the short-term impatience with hyperbolic discount functions, in which the discount rate is declining in the time horizons.==== As in Amador (2004) and Aguiar and Amador (2011), the government can be modeled as a sequence of hyperbolic discounting incumbents. Each incumbent controls leverage and default decisions during its term in office but derives continuation value from the entire stream of debt policies chosen by future incumbents. In particular, the present-biased government holds a naive belief,==== in which the current incumbent knows its present bias but is unaware of the present bias of all future incumbents (Akerlof, 1991). Absent present bias, the government and the international creditors are both exponential discounters with the same discount rate. In this situation, debt issuance merely results in high default risks. Consequently, the government remains passive in the debt market, never actively issuing or buying back outstanding sovereign debt.====Our main result states that present bias plays a ==== role in that it generates borrowing incentives for the moderately indebted country, but incurs debt repurchase motives for the country with excessive indebtedness. Because present bias makes the government prefer to realize immediate rewards sooner and delay immediate costs until later, our result emerges from the interplay of two effects. First, debt issuance immediately rewards the government by advancing consumption. Thus, the desire for instantaneous gratification drives the government to issue debt. We refer to it as the ====. Second, debt issuance increases future default risks, which might trigger default before the arrival of future incumbents. As a result, sovereign default incurs not only restructuring costs but also loss of continuation value. This indicates that by raising the default costs, present bias serves as an implicit commitment device against over-borrowing and even induces debt buyback incentives. We term this channel as the ====. With moderate indebtedness, the splurging effect dominates so that the government takes on debt. However, when the country is excessively indebted, the inter-temporal effect plays a dominant role, thus the government actively repurchases the outstanding debt to procrastinate the immediate-cost default.====Taken together, three factors generate the debt buyback incentive: (i) the government lacks commitment ability, (ii) the government has present-biased preferences, and (iii) there are high default risks. This mechanism is consistent with the case of Greek debt buyback, which contradicts the traditional prediction that repurchasing sovereign bonds is inefficient for the debtor country (Bulow et al., 1988, Bulow and Rogoff, 1991).====Second, stronger present bias induces ==== debt issuance incentives and ==== debt repurchase motives. The intuition hinges upon the following trade-off. On the one hand, stronger present bias strengthens the desire for advancing consumption, thereby amplifying debt issuance incentives and attenuating debt repurchase motives. On the other hand, a higher magnitude of present bias lowers the option value of waiting and thus accelerates default, which weakens debt issuance and strengthens debt buyback. It turns out that in the region with mild present bias, the former force dominates when the country is moderately indebted and the latter force plays a dominant role near default. This pattern reverses as present bias becomes severe.====Interestingly, although stronger present bias might generate a lower target debt level due to the hump-shaped borrowing incentives mentioned above, it always leads to smaller debt capacity and higher sovereign credit spreads. This result provides a potential resolution for the “debt intolerance” phenomenon.==== The intuition is the following. Absent commitment, the severe present-biased government borrows aggressively and hastens default. Creditors anticipate the strong dilution==== in the future and require high yields that eventually eliminate all debt financing benefits. In particular, large credit spreads arise even for countries with low indebtedness. This negative impact can be ameliorated by long-term debt and budgetary restriction, as they discipline the present-biased government’s incentive to borrow for instantaneous gratification.====Finally, we explore a financing trade-off between sovereign debt issuance and taxation. We find that when the country is moderately indebted, the government prefers to raise debt. Near the default threshold, the government would rather raise the tax rate than issue debt. Intuitively, low taxation induces domestic firms to invest more and thus boosts the government’s long-term income. Nevertheless, this benefit is negatively correlated with the level of indebtedness because heavier indebtedness can easily trigger debt restructuring, incurring the wealth transfer from the country to the remaining creditors. As a result, excessive indebtedness strengthens the government’s incentive to choose a high tax rate.==== Two groups of papers are related to this work. First, our study is connected to a fast-growing strand of sovereign debt literature pioneered by Hatchondo et al., 2016, Debortoli et al., 2017, and Aguiar et al. (2019). They highlight the role of leverage commitment in sovereign debt management.====
 Rebelo et al. (2021) develop a model of sovereign debt with limited commitment and limited spanning. The above market incompleteness is interpreted as the level of financial development. They find that a low level of financial development addresses the “debt intolerance” phenomenon. We show that present-biased preferences can also be a potential channel that generates debt intolerance. Among others, our paper is most related to DeMarzo et al. (2021). They also study the sovereign debt management problem without leverage commitment and emphasize the potential welfare gains from commitment devices. We contribute to this strand of literature by modeling a critical determinant behind the sovereign debt policy: present-biased preferences. A novel insight is that the government’s present bias generates not only debt issuance motives but also debt buyback motives.====The second group of papers investigates the role of present-biased preferences in corporate finance with leverage commitment. Tian (2016) studies present-biased preferences in the context of capital structure with real option investment (Grenadier and Wang, 2007). She states that entrepreneurial firms may choose different leverages, depending on the magnitude of present bias. Li et al. (2016) introduce managerial present bias into contract theory and find that a higher magnitude of present bias decreases the firm’s debt capacity. Gan et al. (2018) further extend this framework and find that managerial present bias might ameliorate the agency cost of equity. In their models, the leverage policy is time-consistent due to the existence of a commitment mechanism. We improve this line of research by revealing that present-biased preferences take full significance when combined with commitment friction.====The rest of the paper is organized as follows. In Section 2, we introduce a dynamic sovereign debt model with present-biased preferences. Section 3 derives the model solutions for optimal debt policy. In Section 4, we provide model implications on sovereign debt decisions. Section 5 extends the model by incorporating an endogenous tax policy. Finally, we conclude in Section 6.",Present-biased government and sovereign debt dynamics,https://www.sciencedirect.com/science/article/pii/S0304406821001427,23 October 2021,2021,Research Article,67.0
"Boucekkine Raouf,Fabbri Giorgio,Federico Salvatore,Gozzi Fausto","Rennes School of Business, 2 Rue Robert d’Arbrissel, 35065 Rennes, France,Univ. Grenoble Alpes, CNRS, INRA, Grenoble INP, GAEL - CS 40700 - 38058 Grenoble CEDEX 9, France,Dipartimento Economia, Università di Genova, Via Francesco Vivaldi, 5, 16126 Genova GE, Italy,Dipartimento di Economia e Finanza, Libera Università degli Studi Sociali ,, Roma, Italy","Received 27 April 2021, Revised 3 September 2021, Accepted 4 October 2021, Available online 20 October 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102577,Cited by (1), is proposed for illustration. We focus on the determination of the optimal short-term spatiotemporal dynamics induced by the resulting non-autonomous problems.,"Transboundary pollution is readily recognized as a major topic in economics, notably in economic geography and environmental economics. A key issue posed by transboundary pollution is the treatment of the related externality problem: pollution generated in a certain place may flow integrally or partially into another distant place. Such a problem has been addressed by Candel-Sanchez (2006) in a quite simple theoretical framework, the main objective being the identification of compensation mechanisms to the externality problem posed. Yet, just like in the latter paper, the spatial dimension has not been often accounted for in the related theories developed, and has mostly remained metaphorical, most of the time using the two-country model narrative (see Unteroberdoerster, 2001). Indeed, a large majority of papers in this topic is empirical (see for example, Gibson and Carnovale, 2015, or Henderson, 1996).====This said, an increasing number of theoretical works in the topic of transboundary pollution are introducing an explicit spatial dimension, that is they are based on a well defined spatial setting. An overwhelming part of it is static and it usually entails an ==== assumption on the level of pollution at given location. This is clear for example in Hutchinson and Kennedy (2008) and in Arnott et al. (2008). This is of course acceptable in certain contexts, and it is even more acceptable if the alternative (more) accurate specifications are intractable. This paper is a methodological contribution to this important area of economics at the intersection of geographic and environmental economics. Since spatial heterogeneity is admittedly key in shaping (local) pollution and the induced spatial distribution of pollution, we propose a methodology which closely incorporates a large set of spatial heterogeneities while the diffusion process of pollution over time and space is accurately modeled through a diffusion partial differential equation as in the pioneering works of Camacho and Perez Barahona (2015) and de Frutos and Martin-Herran (2019a).====In short, we propose a new spatiotemporal framework allowing to incorporate geographic heterogeneity to a large extent while modeling pollution diffusion as a physical diffusion problem over time and space. Moreover, we are not only able to put together all these admittedly complex ingredients, we find a way for our setting to produce closed-form solutions for any of these spatial heterogeneity traits, and analytical characterizations of the induced long-term spatial distributions as well. This is done at the cost of a simplification in the shape of the production function (linearity assumption) but the economic insight from the analytical exploration of spatial heterogeneity impact on economic decision making and pollution outcomes well outweigh the latter cost in our view.====As one can infer from above, we are firstly interested in transboundary (diffusive) pollution like air or water pollution. The problem of pollution control is deeply intricate in this case. As our main objective is methodological in this paper, we shall not address the strategic aspects originating in the externality problem outlined in the beginning of the introduction.==== We rather concentrate on a second set of issues, those inherent in the fact that the impact of air pollution is first of all local, and its magnitude and persistence depend pretty much on the local conditions. If a central planner at a country or international level has to set a pollution control policy for the benefit of all the individuals concerned, then she should take as much as possible into account the heterogeneity across locations, in addition to the fact that air pollution, being transboundary, requires the internalization of spatial externalities.====There are several spatial heterogeneity features to account for. Obvious ones are technological heterogeneity and heterogeneity in preferences (which covers cultural discrepancies with respect to the environment among others). But there are also more geographic and ecological differences, which matter a lot both in the diffusion of pollution across locations and in its local impact. The self-cleaning capacity of Nature may vary from a region to a close one, the local topography, land use and infrastructures may speed up pollution diffusion or slow it down... etc. These issues are quite known across disciplines (see Colls and Tiwary, 2009, for an excellent book on air pollution), and, as indicated in the beginning of this introduction, they are also very much recognized in the literature for decades. Several recent papers have been devoted to transboundary pollution.==== None of these papers however poses the latter problem in a full-fledged analytical spatiotemporal frame incorporating the above mentioned technological, preference, geographic and ecological spatial discrepancies.====Clearly, taking up the challenge would involve plenty of technical problems, with a very likely lack of tractability and the forced use of numerical solutions. Contrary to other disciplines (like quantitative geography, climate science or ecology) in which the use of black-box disaggregated models is routine, the economists, being more interested in identifying mechanisms, are more keen to develop parsimonious models. In this paper, we build up a spatiotemporal optimal control framework allowing to encompass the heterogeneity traits outlined above in the presence of transboundary pollution, and still producing a comprehensive analytical characterization. In particular, with respect to the aforementioned recent literature on transboundary pollution, we considerably enrich the geography of the model. On the technological ground, we allow productivity both time and space-dependent: any pattern of technology diffusion across time and space can be accommodated. As for preferences, we make them time and space-dependent as well. Finally, to account for the local geographic and ecological conditions discrepancy, we allow for a space dependent self-cleaning capacity and pollution diffusion speed.====This generality has a methodological implication, since the heterogeneities in space and time make very difficult to use the dynamic programming method (see e.g. Boucekkine et al., 2019a, Boucekkine et al., 2019b) or the maximum principle (e.g. Brito, 2004, Camacho et al., 2008 or Brock et al., 2014) in order to produce analytical solutions. Instead, we apply a functional transformation technique observing that the objective functional can be rewritten in a way that allows for a direct maximization method, ultimately finding the explicit form of the optimal control.==== An extremely appealing feature of this method is that it builds on a pivotal spatial function (denoted ==== in Section 3), which admits a neat economic interpretation: it corresponds at any location ==== to the discounted sum of future disutility stream of a unit of pollutant initially located at ====. Indeed, in our spatiotemporal framework with transboundary pollution, a unit of pollutant has a different effect on social welfare depending on where it is initially located and how it is going to spread over space in the future. Our alternative method has the virtue to put forward this deep economic concept, which makes it definitely transparent and useful for economic analysis.====In a companion paper Boucekkine et al. (2021), we apply this methodology to the problem of optimal pollution control in a particular spatiotemporal case, the main objective being the derivation and analysis of the long-run distributions of some relevant economic and environmental variables depending on technological, ecological and culture spatial heterogeneities. Here we develop our methodology in a much more general spatiotemporal framework, and provide with an additional application, enhancing the applicability of our method to more complex spatiotemporal contexts with a wider set of nontrivial heterogeneities and non-autonomous problems. In particular, this paper generalizes and complements the companion paper in two essential dimension. First, we allow for full spatial heterogeneity in structural preference, technological and physical parameters. This notably concerns the pollution diffusion parameter ====, the unitary emission of investment ====, and the intertemporal elasticity of substitution is consumption, ====. In the companion paper, these parameters are constant, thus for example preventing to consider emissions depending on production and time-increasing environmental efficiency of the technology in use, which admittedly limits the realism of the exercises. Second, instead of analyzing the implications for the long-run distributions depending on the particular spatial heterogeneity considered, as in the companion paper, we shall in a complementary way run our methodology on non-autonomous problems and study spatiotemporal transition dynamics. As a useful illustration, we tackle a problem of technological dynamics over space and time adapting an initial non-spatial frame due to Nocco (2005).====More precisely, we use our methodology to study, following the terminology of Krugman (1993), a first nature cause of spatial externalities, that is ==== geographic differences in terms of efficiency either at production or at pollution abatement. Also as written just above, we adapt a non-spatial technology evolution model specified in Nocco (2005). A key feature of our adaptation is that technological spillovers tend to decrease with distance to a given technological center. This is totally consistent with empirical evidence. For instance, Deltas and Karkalakos (2013) found that in increase in distance between the center and the recipient by 500 km reduces spillovers by 55%–70%. It is important to note here that our method can also in principle support the study of second nature causes. As a matter of fact, it can be readily shown that the inclusion of an additional policy control, like pollution abatement, does not at all break down the analytical solution scheme despite the nonlinearities potentially conveyed by this addition.==== That speaks a lot about the flexibility of our approach. In this paper, we prefer to emphasize more the ability of our method to address a large set of geographic heterogeneities, in particular in structural parameters, and to deliver properly optimal transitional spatiotemporal dynamics.====The paper is organized as follows. Section 2 presents the basic economic problem and defends its economic relevance. Section 3 develops the method used to solve the generic spatiotemporal optimal growth model under transboundary pollution. In particular, closed-form solutions of the optimal strategies are identified. We show also further analytical results on transition dynamics and asymptotics (that is the computation of asymptotic spatial distributions). Section 4 gives an illustration of the method in the presence of technological discrepancy across space using the spatiotemporal spillover mechanism explained just above. Of course, our aim in this ultimate section is not to exploit entirely the richness of our setting but only to indicate how far the analytical study can go. Section 5 concludes. Appendix contains the formal proofs.",Managing spatial linkages and geographic heterogeneity in dynamic models with transboundary pollution,https://www.sciencedirect.com/science/article/pii/S0304406821001403,20 October 2021,2021,Research Article,68.0
"Feng Tangren,Ke Shaowei,McMillan Andrew","Department of Decision Sciences and IGIER, Bocconi University, Italy,Department of Economics, University of Michigan, United States of America,Department of Mathematics, University of Michigan, United States of America","Received 9 March 2021, Revised 21 July 2021, Accepted 2 October 2021, Available online 19 October 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102576,Cited by (1), is associated with a more unequal assignment of utilitarian weights across generations.,"To evaluate economic policies or projects that have long-term impacts, such as environmental policies, a ==== often needs to be chosen in order to compare, for example, future benefit and current cost from a society’s point of view. The choice of the social discount function, however, is often controversial. For example, Stern (2007) uses a social discount function with a near-zero social discount rate, but Nordhaus (2007) argues that Stern’s conclusion will change significantly if market rates are used instead.====Many researchers have adopted a social-choice approach to understand the determination of the social discount function: Taking an arbitrary set of individual preferences as given, the social preference must satisfy certain reasonable assumptions, after which the implications of those assumptions on the social discount function are analyzed.==== The most widely used assumption is the ==== condition, which states that whenever “all” individuals prefer one alternative to the other, the social preference should agree. Exactly which individuals should be considered under the Pareto condition varies with the setting. For example, in a dynamic setting with multiple generations of individuals, Feng and Ke (2018) introduce a Pareto condition that involves all current- and future-generation individuals; that is, whenever all current- and future-generation individuals prefer one alternative to the other, the social preference should agree. This is called ====.====A seminal paper by Harsanyi (1955) characterizes the implications of a Pareto condition on the social preference, which provides an axiomatic foundation for utilitarianism. In a setting with finitely many individuals, it is shown that if individuals’ and the planner’s preferences have expected utility representations, the planner’s preference satisfies the Pareto condition if and only if she aggregates individuals’ preferences through utilitarianism.====Whether Harsanyi’s (1955) utilitarianism theorem can be extended to a dynamic setting, however, is not clear. In most dynamic models, the time horizon is infinite. Then, for example, the intergenerational Pareto condition will become a Pareto condition that involves at least countably infinitely many individuals, since there are infinitely many future generations.====We consider a setting that is natural for the study of social discounting. The time horizon is discrete and infinite, and in each period there are finitely many individuals who live for only one period. The population and the set of consumption goods may differ across generations. We show that under some additional assumptions, Harsanyi’s (1955) utilitarianism theorem can be extended to this setting.====Harsanyi (1955) requires that individual preferences and the social preference have expected utility representations. In our setting, a natural analogue will be that individual preferences and the social preference can be represented by discounted (expected) utility functions.==== We do not require that the discounted utility functions have exponential discount functions, and we do not require that future-generation individuals’ discounted utility functions be related to past generations’.====Our first result establishes that Harsanyi’s (1955) utilitarianism theorem can be extended to our setting. When we have countably infinitely many individuals, the Pareto condition still implies that the planner’s utility function is a linear functional of individuals’ utility functions. This linear functional is represented by the integration of individuals’ utility with respect to a finitely additive measure. To have utilitarian aggregation, the measure must be countably additive. This is achieved mainly due to two assumptions. First, the planner and individuals have discounted utility functions. This is a rather general assumption that almost all utility functions in dynamic economic models would satisfy. Second, for each consumption sequence, individuals’ discounted utility functions are uniformly bounded. This is almost a necessary condition: If there is some consumption sequence that leads to infinite utility for infinitely distant future generations, the utilitarian aggregation may not be well defined.====We provide a new condition that allows the utilitarian weights to be uniquely determined. This is done by taking a classic affine independence assumption from static settings (that ensures that the utilitarian weights are uniquely determined in those settings) and extending it to ours.====Once we know that the normatively appealing Pareto condition again does not allow for anything other than utilitarianism in this general setting, and that the utilitarian weights are uniquely determined, given individual preferences and the social preference, a natural and useful next step is to analyze the properties of the utilitarian weights. Such analyses might be particularly useful in understanding, for example, inequality or fairness based on the utilitarian weights implied by a social discounted utility function.====We show that asymptotically, future-generation individuals’ utilitarian weights diminish exponentially at a rate equal to the social discount rate. Therefore, roughly speaking, a higher social discount rate is associated with a more unequal (across generations) assignment of utilitarian weights.====In addition, we illustrate some counterintuitive properties of the utilitarian weights in a simple setting. We show that when the social discount factor converges to the discount factor of one family of individuals (who share the same discount function and same instantaneous utility function), but the social risk attitude converges to the risk attitude of a different family of individuals, only the utilitarian weights of the former family converge to zero, regardless of the relative speed of convergence.====Several papers have extended Harsanyi (1955) to a setting with infinitely many individuals. Zhou (1997) allows for an arbitrary number of individuals and shows that the Pareto condition holds if and only if the planner’s expected utility function is a linear functional of individuals’ expected utility functions. The linear functional, however, does not always take the form of utilitarian aggregation, in which case we may not be able to analyze the properties of utilitarian weights. It does take the form of utilitarian aggregation—for example, when the set of individuals is a compact metric space. Similar types of results are established by McCarthy et al. (2019), although they further allow individuals’ and the planner’s preferences to violate completeness and continuity. We will explain in Section 3.2 how our approach avoids this issue. In Al-Najjar and Pomatto (2020), the set of individuals is a probability space with the number of individuals being countably infinite but the measure being nonatomic and finitely additive (defined on the power set of the set of individuals). Therefore, this is also different from the utilitarian aggregation in our case.====The static version of our assumption that ensures the uniqueness of utilitarian weights would assume that individuals’ expected utility functions are affinely independent. This affine independence assumption is used in Mongin (1998) and shown in Coulhon and Mongin (1989) to be equivalent to the independent prospects condition introduced by Fishburn (1984) and later used by Weymark (1994) and Börgers and Choo (2017).====The paper proceeds as follows. Section 2 describes individuals’ and the planner’s preferences and utility functions. In Section 3, after introducing the Pareto condition, we characterize utilitarian aggregation for both the finite- and infinite-horizon settings and introduce a simple condition that ensures the uniqueness of utilitarian weights. In Section 4, we study the comparative statics of the utilitarian weights. Section 5 concludes.",Utilitarianism and social discounting with countably many generations,https://www.sciencedirect.com/science/article/pii/S0304406821001397,19 October 2021,2021,Research Article,69.0
Seshimo Hiroyuki,"Senshu University, 3-8 Kanda-Jimbocho, Chiyoda-Ku, Tokyo 1018425, Japan","Received 9 April 2021, Revised 30 July 2021, Accepted 27 September 2021, Available online 12 October 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102572,Cited by (0),"This paper provides a perspective that unifies two controversial arguments about the extended liability rule in a competitive financial market, i.e., the negative arguments about the exaggeration of the judgment proof problem and the positive arguments about the entry deterrence of inefficient firms. This paper also gives the theoretical solution for the ==== of the extended liability rule. In this paper, heterogeneous firms are assumed for the cost parameter of the precautionary effort, and distortions are dependent on the heterogeneities. Then, even considering extended liability with any share to the financier, the extended liability rule never achieves a socially first-best situation. Alternatively, the conditions under which the second-best optimal extended share to the financier should suffice are provided. Furthermore, I discover that the information structure is one of the crucial factors in determining the socially second-best solution for the extended share.","Modern firms are so large that they can cause considerable environmental damage or health hazards to a purchaser or a third party. Therefore, firms are legally required to compensate for tort liability based on damage in the same way as natural persons. Such tort liability forces the firm to internalize the future risks of causing damage and to engage in its best precautionary efforts, even when a third party suffers damage—that is, even when the event generates a negative externality.==== However, internalization through tort liability is partially prevented when the firm does not have sufficient assets to compensate for the liability; under the limited liability (LL) rule in bankruptcy, such a firm can be exempted from any tort liability that cannot be covered by its assets. Consequently, such a firm will not take into account the exempted liability, which induces insufficient precautionary efforts. This problem is the so-called judgment proof problem.====An important related argument is the extended liability (EL) rule, which extends the obligation for the firm’s unpayable tort liability to a deep-pocket financier who has sufficient financial assets to fulfill it.==== Therefore, the risk of unpayable liability may be incorporated into the financier’s decision making. Indeed, Section 3 of Boyer and Laffont (1997) shows that the first-best allocation is achievable under complete information by extending full liability to the financier. However, Balkenborg (2001) suggests that this is true only when the financier can control the firm, that is, when the financier has sufficiently high bargaining power to extract the firm’s profits.====Generally, the financial market is considered to be highly competitive such that financiers may not gain sufficient bargaining power. In this context, two contrasting perspectives pertain to the EL rule. The first is from Pitchford (1995), who insists that the EL rule does not necessarily achieve a first-best solution but, instead, deteriorates efficiency: the interest rate required by the financier from the judgment-proof firm becomes higher under EL than under the usual LL. This higher interest rate diminishes the incentive for the firm to exert precautionary effort against accidental or environmental damage.==== I refer to this effect as the exaggeration effect of the judgment proof problem in the following.====The other perspective is proposed by Balkenborg (2006) and Hutchinson and Van’t Veld (2005). They consider the problem of excess entry caused by the exemption of tort liability under the LL rule because such an exemption of tort liability fails to internalize the social cost of the firm’s operation; thus, the LL rule induces firms’ excess entry from the viewpoint of social welfare.==== In this situation, Balkenborg (2006) obtains the results for the socially first-best optimality of the full or punitive EL when the lender cannot observe a firm’s type, and Hutchinson and Van’t Veld (2005) report the results for the second-best optimality of the full EL under complete information.==== These results occur because the higher interest rate that reflects the internalization of risk under EL is a barrier to market entry for inefficient judgment-proof firms given that the higher interest rate decreases firms’ profitability. I refer to this effect as the entry deterrence effect on inefficient firms.====Thus, the efficiency of the EL rule under a competitive financial market has been controversial, and a consensus for this rule has been required in practice. In this paper, I create a theoretical model that encompasses these analyses to provide a unified perspective for this rule. Notably, the arguments for the exaggeration effect of the judgment proof problem have never considered the heterogeneity of firm types, whereas the arguments for the entry deterrence effect on inefficient firms have never considered the possibility of a difference in the effect on firm incentives caused by the higher interest rates entailed with the EL rule. I introduce a firm’s heterogeneity into the model to analyze the judgment proof problem as the difference in the cost parameters for precautionary measures: a firm with a low cost parameter can generate effort more effectively than a firm with a high cost parameter. Therefore, firms with different cost parameters exert different precautionary efforts.==== Then, the unified perspective describes the efficiency of the EL rule as the problem that includes these two effects.====I examine this model with heterogeneous judgment-proof firms under two informational environments. The first environment is the case in which a firm’s cost parameter for precaution is observable to financiers. Then, under the full EL rule, the criterion for the firm’s entry is consistent with the criterion for whether it can yield positive social welfare. Thus, any firm that yields strictly negative social welfare can be deterred from entering the market. However, a higher face value on senior credit leads to the exaggeration effect of the judgment proof problem. Therefore, neither the socially first-best nor the second-best situation is ever achievable under the full EL rule. Rather, at most, only a partial share can achieve the second-best situation.====In contrast, when a firm’s cost parameter is unobservable, internalization is insufficient for socially inefficient firms, even under full EL. That is, even firms with negative social welfare can fundraise and operate in competitive markets because a cross-subsidization effect exists among firms. Therefore, ceteris paribus, the extended share in the socially second-best EL is larger than that in the observable case, and the second-best optimal EL rule does not exclude full liability. Notably, in this case, it is possible that the higher extended share may reduce the face value of the senior credit. In such situations, a higher extended share is desirable for social welfare because it can enhance the precautionary efforts of firms. However, even in this case, the first-best is never achievable for any extended share from 0 to 1.====The contributions of this paper are as follows.====This paper is organized as follows. Section 2 explains the basic model and the benchmark of the first-best situation. Section 3 provides the optimal EL rule under complete information. Section 4 analyzes the EL rule under incomplete information. Section 5 concludes the paper.",Optimal extended liability rule in a competitive financial market with heterogeneous borrower firms,https://www.sciencedirect.com/science/article/pii/S030440682100135X,12 October 2021,2021,Research Article,70.0
Combe Julien,"CREST, Ecole polytechnique, France,IP Paris, France","Received 8 January 2021, Revised 28 June 2021, Accepted 4 September 2021, Available online 30 September 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102563,Cited by (0),"We consider a hybrid model at the intersection of the standard two-sided matching market as proposed by Gale and Shapley (1962) and a housing market as proposed by Shapley and Scarf (1974). Two sets of agents have to be matched in pairs to a common set of objects. Agents of one type have preferences that depend on not only the object they are matched to but also the agent of the other type matched to this object. The crucial difference lies in the fact that the common side is interpreted as an object and has no intrinsic preferences over the agents matched to it. We introduce a natural definition of the ownership of the objects that determines which agent owns the object he is matched to. Ownership restricts the objections of agents who are not owners and defines a notion of stability. We consider two natural ownership structures and show that stable matchings exist in both structures. The first ownership structure, i.e., one-side ownership, always gives ownership to agents of the same side. Even if this structure shares similarities with the classical two-sided matching framework, we show the following important difference: stable matchings and Pareto-efficient matchings can be disjoints, implying that the core can be empty. We also propose two subdomains of preferences, i.e., lexicographic and couple preferences, where core matchings exist in one-sided ownership structures. The second notion is joint ownership, where any reallocation of objects must be jointly agreed upon by the two agents initially assigned to them. As discussed in Morrill (2010), this notion is equivalent to Pareto-efficient matchings, and we discuss possible algorithms that can be used to check ====. Finally, we propose a general definition of ownership structures and show that one-sided ownerships are not the only ones that can guarantee the existence of stable matchings. To further investigate the link with the housing market literature, we also introduce an initial allocation to objects and define a core notion with respect to this initial allocation. We also show that in contrast to the standard setting, this housing market core can be empty. However, we show that in this housing market framework, there always exists a Pareto-efficient matching that is not blocked by any coalition of size two. In both settings, pairwise stability is the only minimal requirement that one can ensure.","Matching problems have received great attention over the past decade and have been used in several practical applications, such as school choice (see Abdulkadiroglu and Sonmez, 2003, for instance), kidney exchange (Roth et al., 2004) or public housing (Shapley and Scarf, 1974). In these problems, there are two main frameworks. The first framework, which was introduced by Gale and Shapley (1962), is the two-sided market structure, i.e., two sets, men and women, have preferences over each other and have to be matched together. In this framework, an important question is whether stable matching exists, i.e., matching such that there is no man and woman who would prefer to be matched together rather than with their current partner. One may also study other concepts, such as Pareto-efficient matchings or core matchings, i.e., where no group of agents prefers to be rematched with each other. The second setting, which was introduced by Shapley and Scarf (1974), is the housing market structure, where agents want to exchange their house. This setting also has two sides (agents and houses), but only one has preferences. Moreover, each agent has ownership rights over his initial house. In this context, one may also define core matchings, except that the deviating group of agents can only trade their initial houses and not those they were assigned under a given matching.====However, one may think of other types of practical problems that cannot be described by a standard two-sided structure. For instance, in many public administrations, the salary scale is fixed by law, but one often has to match workers and managers to possible projects. Each project has to be assigned to a manager and a certain number of workers. Even if managers have intrinsic preferences over projects, the quality of and interest in working on a project also depend on the set of workers assigned to the project. Workers also have similar preferences as follows: an interesting project may not be valuable if a bad manager is assigned to it. This introduces complementarities into the preferences of the agents. Generally, once a manager is assigned to a project, he could be free to choose his team of workers and ask some workers to leave the project if needed. Hence, in this application, there is a natural ==== over the projects as follows: managers always own the project to which they are assigned. In some other contexts, ownership does not have to always be given to agents of the same type. In a school choice setting, heads of schools can have an important impact on the quality of their schools and thus affect the desirability for parents. However, the head of a school also cares about the set of students who are assigned to the school. In some schools, the head is responsible for decisions regarding the acceptance or rejection of students. However, in some other schools, such as charter schools in the U.S., parents are greatly involved in managerial decisions such that recruitment or firing decisions concerning a badly managed school could be greatly influenced by parental satisfaction. We can capture this feature with an ownership structure over the schools.====Once again, such applications cannot be studied using standard models in the literature. In this work, we introduce a new general framework, i.e., a hybrid model between a two-sided matching market and a housing market. There are two sides of individuals, for instance, workers and managers, who have to be matched together and be assigned to objects, such as projects. For each possible match, we introduce the notion of an ====, which defines who ==== the object and who can thus credibly block a given matching. In some contexts, this ownership has a natural structure, i.e., managers are responsible for their teams and can decide whether to recruit or replace a worker with another worker. This structure corresponds to a ==== structure, where agents of one type always own the object (e.g., managers always own projects). In some other problems, this situation may not be the case, as in the aforementioned “heads of schools and students example”. Indeed, as we highlighted, some schools give more decision power to the head of the school, and in other schools, parents have a greater influence on managerial decisions. However, one can also consider the example of men and women who have to match in pairs and choose a house. It is not a priori clear that the house is owned by the same side (i.e., always women or always men). This situation motivates another natural structure, i.e., ====, where both agents must agree to any reallocation of their object. We show that stable matching exists in the two natural structures, i.e., one-sided and joint ownership.====One-side ownership stable matchings share similarities with stable matchings in the standard two-sided problem. In this problem, it is well known that pairwise stable matchings are equivalent to core matchings. In contrast, we show that this is not the case in our one-sided ownership stable matchings. We provide an example where the set of stable matchings and the set of Pareto-efficient matchings are disjoint. In addition, we propose two natural preference domains, i.e., lexicographic and couple preferences, and show that core matchings exist in such domains when one considers one-sided ownership.====Finally, we consider a setting closer to the housing market problem, where all houses are initially owned by some agents. Shapley and Scarf (1974) and Roth and Postlewaite (1977) defined a different core notion where a group of agents could only block a given matching in exchanging their initially owned houses and not those they are assigned to ex post. We show that in our environment, this notion of core could also be empty. However, we show that there always exists a Pareto-efficient matching that is not blocked by any coalition of size two.====The three-sided matching structure is not new in the literature. This structure was first introduced by Knuth (1976) with the matching of men, women and dogs. In such problems, all sides have preferences over the pairs of the two other sides, and matchings are defined by a collection of triplets. Subsequently, Alkan (1988) provided an example to prove the nonexistence of stable matchings in such an environment. Biró and McDermid (2010) proved the nonexistence in a restricted set of cyclic preferences and proved the NP completeness of the problem of deciding whether a stable matching exists in such an environment.==== In the above impossibility results, all sides need to have preferences over the pairs of the other sides since all counterexamples need all sides to be able to break their match to form a blocking triplet. In our model, however, projects cannot “unilaterally deviate”from their managers or workers, and only agents can decide to leave.==== Recently, Nicolò et al. (2019) considered a model where a group of agents has to be matched in pairs to a task. Each agent has preferences over both the partner and the project to which they are assigned. The authors considered the subdomain of separable and dichotomous preferences, where each agent partitions the set of partners into friends and not friends and the set of projects into good and bad projects. Friendships define a partition of the agents, i.e., mutual and transitive. In addition, preferences over projects among friends are correlated. In this subdomain, the authors proposed a strategy-proof mechanism called the minimum demand priority algorithm (MDPA), which generates an assignment in the weak core. The main difference from our model is that similar to a roommate problem, agents have a one-sided structure and have to be matched in pairs. Our model considers a two-sided structure of workers and managers and does not restrict the set of preferences. This difference is important since in the standard framework without projects, the one-sided roommate problem is known to have no stable matching, in contrast to the two-sided matching problem. The existence of stable matches in the latter setting is important for the existence of our result in the full preference domain. Recently, Rodríguez Álvarez and Romero Medina (2020) proposed a school choice model in which students have transferable characteristics that influence their priorities for schools. These authors propose a mechanism that is stable and constrained efficient. The conceptual approach is close to ours as follows: a characteristic is similar to an object in our model. However, there are two important differences. The first difference is that a student has multiple characteristics, i.e., one for each school, as opposed to our model where a house is a unique object. The second difference is that the preferences of the students are not influenced by their characteristics, i.e., trading a characteristic does not influence their preferences over schools and only influences the ranking of the schools over students. In our model, exchanging a house can change the preferences of both sides of the market over each other. This important difference enables the existence of stable matchings in their setting and the existence of a polynomial algorithm to find a stable and constrained efficient matching. In our setting, we have to restrict the definition of blocking by introducing the idea of ownership and the existence of a polynomial algorithm to find a stable and constrained efficient matching when ownership is one-sided and still open. Our notion of joint ownership is directly related to Morrill (2010), who studied a roommate problem and defined a stability notion by considering that roommates are assigned to rooms and have some ownership over their rooms. By noting that joint ownership is equivalent to Pareto efficiency, he proposed a polynomial time algorithm to check the Pareto efficiency of a matching. As previously mentioned, our framework is two-sided, but his notion can be directly applied to our model. Regarding the complexity question of checking Pareto efficiency, Aziz et al. (2013) proposed an algorithm, i.e., the Preference Refinement Algorithm, which can check Pareto efficiency in polynomial time in a large class of coalition formation models.====Another related study addresses coalition formation. Our model shares some features with the hedonic coalition literature initiated by Dreze and Greenberg (1980) and subsequently developed by Bogomolnaia and Jackson (2002). Works concerning partitioning games performed by Kaneko and Wooders (1982) (and subsequently by Kaneko and Wooders, 1986 or Hammond et al., 1989) are also related to this literature. These authors ex ante restrict the possible admissible coalitions of an assignment game and ask whether core assignments exist. Pápai (2004) also studied a model in which agents have hedonic preferences over coalitions, but the set of admissible coalitions is restricted. Blocking coalitions could only be admissible ones, and she found a necessary and sufficient condition for the set of admissible coalitions to ensure the existence and uniqueness of core coalitions. Once again, objects cannot unilaterally deviate and do not have ownership, which is an important feature of our model and cannot be captured in her model. However, our ownership structure approach shares a similar idea of restricting the possible blocking coalitions.",Matching with ownership,https://www.sciencedirect.com/science/article/pii/S0304406821001269,30 September 2021,2021,Research Article,71.0
Levy Yehuda John,"Adam Smith Business School, University of Glasgow, Glasgow, G12 8QQ, UK","Received 23 March 2021, Revised 12 September 2021, Accepted 16 September 2021, Available online 29 September 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102571,Cited by (0),"This paper considers uniformly bounded classes of non-zero-sum strategic-form games with large finite or compact action spaces. The central class of games considered is assumed to be defined via a semi-algebraic condition. We show that for each ====, the support size required for ","An important strand of literature in game theory is dedicated to computing how many strategies players must randomize over if they wish to be playing, approximately, an equilibrium. The primary technique was laid out in the seminal paper (Althofer, 1994) which studied, among another class of linear problems, small-support ====-optimal strategies in two-player zero-sum games. That paper demonstrates that in an ==== matrix game (payoffs in ====), the players possess ====-optimal strategies with support size ====, and in fact it is demonstrated that this required growth rate is tight. For each player, an approximately optimal strategy can be found, with high probability, by random sampling w.r.t. the distribution induced by an optimal strategy. Indeed, standard law of large number arguments show that such a random sampling of an optimal strategy would converge; the specific bounds are then obtained by bounds on the error, specifically Chernoff’s inequality.====This result was extended in Lipton et al. (2003) to ====-equilibria of any game with arbitrary (but fixed) number of players, and then again extended by Babichenko et al. (2014) to games with variable number of players; specifically, to ====-player games, ==== actions each, by showing that ====-equilibrium exists in which each player uses strategies with support ====; see also Barman (2015), which improves the bound for bimatrix games which are sparse (i.e., relatively few profiles give non-zero payoffs). Both papers discuss the implications for computation of approximate Nash equilibria in quasi-polynomial time. Again, in both cases, the existence is proven by showing that random sampling w.r.t. an equilibrium distribution will again give, with high probability, an ====-equilibrium strategy profile. Although not the topic of this paper, it is interesting to note that is unknown if the dependence on the number of players in the later result is tight, or for that matter, if dependence on the number of players is needed at all. This open question is discussed further in Arieli and Babichenko (2017).====One can naturally ask on the support size required if we allow the players to possess a continuum of strategies, say, the unit interval. In general one could not impose any bounds; indeed, any game with finitely many actions could be embedded into one in which players have a continuum of actions, even in such a way that gives continuous utility functions, while preserving (in an appropriate sense) the (exact and approximate) equilibria; this point is elaborated on in Section 6. The question then is, for a class of games and given ====, can the size of support required for the existence of ====-equilibrium be bounded by some function of ==== and of the parameters or properties of the class?====A natural class of payoff functions to consider, a class which displays significant regularity properties, are the ====. Semi-algebraic geometry has an intimate relationship with game theory, see, e.g., Schanuel et al. (1991) or Neyman and Sorin (2003, Ch. 6). Semi-algebraic sets are those subsets of Euclidean spaces defined using logical formulas whose atoms are polynomial equalities and inequalities; semi-algebraic functions are those with semi-algebraic graphs. The set of Nash equilibria of a game with finitely many actions is naturally a semi-algebraic set, as is the manifold of Nash equilibria (see Kohlberg and Mertens, 1986) and many other closely related sets, mappings, and correspondences. The regularity and decomposition properties of semi-algebraic sets can be used, as in the above papers, to derive various useful results concerning equilibria and how equilibria change as the game does, and in turn, Nash equilibria and the Nash equilibrium correspondence have been shown to be rich, universal is certain senses, in the appropriate classes of semi-algebraic objects; see, e.g., Levy (2016), Vigeral and Viossat (2016) and Balkenborg and Vermeulen (2019).====In this paper, one of our central goals is to study classes of games with semi-algebraic payoffs on semi-algebraic action spaces ====. The complexity (====, in this paper, following Ben-David and Lindenbaum, 1998) of a semi-algebraic payoff function on ==== can be defined in terms of the number of polynomials ==== of degree at most ==== needed to define its graph. A particular case of such uniformly bounded class over a collection of games arises when the payoffs are defined by a function which depends, in addition to the actions, on a parameter from a semi-algebraic set ====. Our paper’s main theorems, which will be stated in greater precision later, can be summarized as follows:====We note, for the latter part of the theorem, that if ==== were compact as well, the result would follow easily; however, this is not assumed. In particular, we deduce as a corollary that if ==== and the games are zero-sum, and ==== is an interval, the limits of the value at the end-points of the interval exist. This result is shown despite the fact that the value, it turns out, need not be a semi-algebraic function of the parameter; see Bolte et al. (2015). We remark also that our proof relies crucially on the boundedness of ====, i.e., on the payoffs being uniformly bounded. If the corollary on zero-sum games were to be established without the boundedness assumption, where the limit would be required to exist in the extended real line ====, it would imply the existence of the asymptotic value (that is, convergence of the discounted values) of stochastic games with finitely many states, compact action spaces, and semi-algebraic transitions, due to a result of Attia and Oliu-Barton (2019). Indeed, that paper defines, from a stochastic game, a family of auxiliary strategic-form games ==== (one for each state ====) which depend on a parameter ==== and on a real parameter ====, and show that not only can the ====-discounted value of state ==== be characterized the unique solution ==== of ====, but that if for each ====, ==== converges quickly enough as ====, then the limit of the discounted values exists. We elaborate in Section 7. Hopefully the techniques presented here can be extended to address this long-standing open question.====The primary tool we use is that of Vapnik–Chervonenkis (VC) dimension and its applications to uniform convergence of random sampling, as introduced in Vapnik and Chervonenkis (1971). These results show that if a class of sets has a certain bounded complexity, formalized in terms of having a bounded VC dimension, then computing the measure of sets via random sampling converges in probability at a uniform rate over all sets in the class and over all probability measures. The results on uniform convergence for the VC classes (those classes of finite VC dimension) can replace, with a considerable amount of care, the use of laws on large deviations (in particular, Hoeffding’s inequality) in the proofs of Althofer (1994) and Babichenko et al. (2014). Indeed, we will show the assumption of the class of games being semi-algebraic of a specific class can be replaced with the weaker assumption of the collection of payoff functions considered, as a function of other players actions, having finite VC dimension.====The paper is outlined as followed. Section 2 presents background notions and results on games, semi-algebraic geometry, and VC dimension. Section 3 develops these concepts to auxiliary results that we will use, many of which may be of independent interest. Section 4 presents the results. Section 5 extends the discussion to classes of games defined in an o-minimal structure, a generalization of semi-algebraic structures. Section 6 has some discussion on the bounds derived, and presents a technique of going from games with finite actions to continuous actions. Section 7 discusses potential connections to stochastic games. Appendix presents briefly a slightly different approach.",Uniformly supported approximate equilibria in families of games,https://www.sciencedirect.com/science/article/pii/S0304406821001348,29 September 2021,2021,Research Article,72.0
"Ma Qingyin,Toda Alexis Akira","International School of Economics and Management, Capital University of Economics and Business, China,Department of Economics, University of California San Diego, United States of America","Received 4 March 2021, Revised 30 June 2021, Accepted 7 September 2021, Available online 20 September 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102562,Cited by (2),"We prove that the consumption functions in optimal savings problems are asymptotically linear if the marginal utility is regularly varying, or loosely speaking, behaves like a power function for wealthy agents. We also analytically characterize the asymptotic marginal propensities to consume (MPCs) out of ====. Our results are useful for obtaining good initial guesses when numerically computing consumption functions, and provide a theoretical justification for linearly extrapolating consumption functions outside the grid.","The optimal savings problem – a dynamic optimization problem in which an agent chooses the optimal level of consumption and savings – is a fundamental building block of modern macroeconomics and contributes to a wide range of research fields ranging from asset pricing, life-cycle choice, fiscal policy, social security, to income and wealth inequality, among others.==== The last several decades have witnessed substantial development in the theory of optimal savings. At the same time, existing studies find supporting evidence that the optimal consumption function – solution to the optimal savings problem – is asymptotically linear in wealth in various specialized settings.====In simple analytically solvable models that feature homothetic preferences and no income risk as in Samuelson (1969), it is well known that the marginal propensity to consume (MPC) out of wealth is independent of the wealth level. In more complicated models, the asymptotic linearity of consumption functions has been numerically observed as in Zeldes (1989, Figure IV), Huggett (1993, Figure 1), Krusell and Smith (1998, Figure 2), and Toda (2019, Figure 4). With hyperbolic absolute risk aversion (HARA) preferences and general income shocks, Carroll and Kimball (1996) show that the consumption functions are concave, which implies that the MPCs converge, although they do not characterize the limit. More recently, Ma and Toda (2021) establish the asymptotic linearity of consumption functions and analytically characterize the asymptotic MPCs when the utility function exhibits constant relative risk aversion (CRRA).====In spite of these interesting findings, the asymptotic properties of the optimal consumption function have hitherto received no general investigation. One cost of this status quo is that in various applications, the asymptotic behavior of agents’ consumption as asset tends to infinity has substantial impact on economic activities. For example, when studying wealth inequality, the saving performance of the rich, which is closely related to the asymptotic MPCs, is a driving force of the fat-tailed wealth distribution and its evolution (Fagereng et al., 2019). Without a systematic understanding of the asymptotic properties of consumption, researchers will have to provide their own analysis piecemeal in individual applications.====A second cost is concerned with numerical computation. When solving for the optimal consumption function numerically, it is common to evaluate the functions on a finite grid and interpolate or extrapolate off the grid points. Some extrapolation is usually necessary because even if the agent’s asset is currently inside the grid, when the return on wealth is sufficiently high, the next period’s asset may fall outside the grid with positive probability. Having a theory of optimal consumption at infinity is useful because it tells us how to properly set up the grid points and extrapolate functions outside the grid.====In this paper, we systematically study the asymptotic behavior of the optimal consumption function in a highly general framework that contains a wide range of important settings as special cases, including the settings of some recent advancements in optimal savings (Ma et al., 2020, Ma and Toda, 2021). Our main result is that, under the weak assumption that the marginal utility asymptotically performs like a power function as consumption increases (plus some other regularity assumptions),==== the consumption functions are asymptotically linear, or equivalently, the asymptotic MPCs converge to some constants.==== Furthermore, we analytically characterize the asymptotic MPCs.====Different from the existing literature, which typically focuses on special utility functions such as CRRA or HARA in relatively stylized settings, we only require that the marginal utility function asymptotically behaves like a power function, which is mathematically defined as ====. Our results are significantly more general than the existing literature because regular variation is a parametric assumption only at infinity, and we do not impose any assumptions on the utility function on compact sets beyond the usual monotonicity and concavity.====The theory we develop provides a theoretical justification for linearly extrapolating policy functions outside the grid when solving optimal savings problems numerically as in Gouin-Bonenfant and Toda (2018). Although the applied literature often uses the CRRA utility, for which asymptotic linearity is already established (Ma and Toda, 2021), our results suggest that asymptotic linearity holds in a large class of problems including HARA and thus applied researchers may have confidence in linearly extrapolating consumption functions outside the grid. A closely related contribution is that our theory explains the “approximate aggregation” property in heterogeneous-agent general equilibrium models as in Krusell and Smith (1998). Approximate aggregation refers to the observation that, when solving heterogeneous-agent general equilibrium models, keeping track of just the first moment of the wealth distribution is nearly sufficient, despite the fact that the entire wealth distribution is a state variable. Because the market clearing condition involves aggregate savings, aggregation would be possible if saving is linear in wealth. Our results show that consumption (hence saving) is approximately linear in wealth, which explains the approximate aggregation property.====Furthermore, based on the theory we develop, we systematically study computation methods. We focus on both computation speed and solution accuracy. As to the former, we apply our theory to construct proper initial guesses that facilitate efficient computation. The initial guess we propose relies on the asymptotic MPCs we derive and can be solved conveniently in applications. Numerical experiments show that policy iteration via the initial guess we propose is about 1.3 to 1.8 times faster than via the routine initial guess of consuming all current assets. As to the latter, we study in depth how to properly set up the grid points and extrapolate policy functions outside the grid when solving models numerically. This is realized by comparing the distances of MPCs at different asset levels from their theoretical asymptotes, as well as by exploring how truncating the grid space affects solution accuracy (measured by the error of the calculated consumption function relative to the true consumption function).====The rest of this paper is structured as follows. The remaining of this section discusses related literature. Section 2 formulates the optimal savings problem and establishes our main theoretical results. Asymptotic properties of the optimal consumption function are studied in general settings. Sufficient conditions for asymptotic linearity of the consumption function are discussed. Section 3 inspects the computation method in detail. By applying our theory, we propose useful initial guesses for efficient computation and discuss various details concerning solution accuracy. Section 4 concludes. Main proofs are deferred to the appendices.",Asymptotic linearity of consumption functions and computational efficiency,https://www.sciencedirect.com/science/article/pii/S0304406821001257,20 September 2021,2021,Research Article,73.0
"Kang Jingoo,Kang Minwook","Nanyang Technological University, Singapore,Korea University, Seoul, Republic of Korea","Received 22 January 2021, Revised 1 September 2021, Accepted 5 September 2021, Available online 17 September 2021, Version of Record 8 March 2022.",https://doi.org/10.1016/j.jmateco.2021.102561,Cited by (2),Why do some people prefer owning ,"Purchasing durable goods (e.g., cars, homes, and large appliances) is a major decision for consumers because durable goods are expensive and long-lasting (Levinthal and Purohit, 1989, Purohit, 1992). Fortunately, consumers who need the service of durable goods have options other than purchasing, such as renting and sharing. Renting and sharing of durable goods provide flexibility and other benefits to consumers and thus have gained more popularity (Belk, 2007, Belk, 2010, Price and Belk, 2016, Rudmin, 2016). From the perspective of conventional manufacturers and marketers of durable goods, increasing competition from renting and sharing options of durable goods poses a critical challenge to their business models and profitability (Eckhardt and Bardhi, 2015). For example, if more consumers prefer renting or sharing cars to owning them, it means lower car sales and profit for auto manufacturers. Since owning versus renting durable goods is an important question for both consumers and firms, marketing scholars have strived to understand consumer choice between owning and renting (Ozanne and Ballantine, 2010, Lamberton and Rose, 2012, Bardhi and Eckhardt, 2012, Bardhi and Eckhardt, 2017, Belk, 2014, Schaefers et al., 2016, Price and Belk, 2016, Rudmin, 2016, Lamberton and Goldsmith, 2020). The extant literature provides cultural, psychological, emotional, and even philosophical explanations including having a sense of control (Fuchs et al., 2010, Furby, 1980, Peck and Shu, 2009, Pierce et al., 2003), personal identification (Belk, 1988, Dittmar, 1992, Pierce et al., 2003), risk reduction (Locander and Hermann, 1979, Lawson et al., 2016), and Anglo-Saxonian values (Belk, 2010). In this paper, we approach this question from the perspective of economic and psychological theory of time inconsistent preferences.====Economics and psychology literature has shown that individuals with time inconsistent preferences fail to maximize their utility because they make suboptimal consumption decisions in the future.==== However, understanding this time inconsistent preferences problem, wiser individuals attempt to voluntarily restrict their future choices using some commitment devices.==== For example, people willingly commit substantial funds to illiquid financial assets (e.g., commitment savings account), from which they cannot withdraw money freely and thus help them constrain future consumption (Strotz, 1956, Phelps and Pollak, 1968, Laibson, 1997, Amador et al., 2006). Unfortunately, illiquid financial assets do not effectively serve as commitment devices in a complete market where consumers can borrow against future liabilities (Laibson, 1997).==== This suggests that, in most contemporary economies with sufficiently developed financial markets, individuals with time inconsistent preferences cannot restrain their consumption behavior using illiquid financial assets.====In this paper, we investigate how owning durable goods may operate as an effective commitment device for individuals with time inconsistent preferences. Durable goods are distinct from illiquid financial assets to the extent that they have nontrivial effects on utility. The nontrivial utility effects mean that the cross derivative of utility functions with respect to durable and nondurable goods is not zero (i.e., ==== where ==== is the nondurable good, ==== is the durable good, and ==== is the period-utility function of the two goods). Many real-life examples support this idea. For example, if one invests in an expensive home theater, it would decrease the marginal utility of watching movies in a public theater. Also, purchasing a coffee maker would decrease the marginal utility of buying coffee from café. Similarly, buying a car would reduce the use of public transportation or taxis. In line with these examples, empirical studies with aggregate consumption levels show that durable and nondurable goods are not separable but submodular in utility, meaning that an increase in durable good consumption decreases the marginal utility of consumption of nondurable goods (see Eichenbaum and Hansen, 1990, Ogaki and Reinhart, 1998).====We show that when durable goods have non-trivial effects on preferences, consumers are willing to pay a commitment premium for the durable goods, as opposed to the case of illiquid financial assets (Laibson, 1997). The necessary and sufficient condition for the existence of a positive commitment premium in the price of durable goods is that the period-utility is substitutable (i.e., ====). To the best of our knowledge, ours is the first study to examine the commitment incentive of durable goods under a non-separable utility function.====The intuition behind the positive commitment incentive of durable goods is as follows. A consumer with self-control problems knows that her future self will consume too much. To restrict the future self’s overconsumption, the current consumer may want to decrease the future consumer’s marginal utility of consumption. One way to influence future marginal utility of consumption is to adjust the current purchase of durable goods that will be used in the future as well as in the present. When durable and nondurable goods are substitutes, purchasing more durable goods in the present decreases the marginal utility of future consumption of nondurable goods and hence induces the future self to consume less. In this case, the demand price of the durable good is higher than the marginal utility of durable goods because there is a positive commitment premium in the demand price. Using the same logic, when durable and nondurable goods are complements, decreasing the purchase of durable goods would induce the future consumer to consume less of nondurable goods. In this case, the commitment premium is negative, which demonstrates that the commitment incentive from durable goods does not arise from the same mechanism that previous literature has suggested (Laibson, 1997, Kocherlakota et al., 2001).====Our finding speaks to the marketing and consumer behavior literature on consumer choices between owning and renting as well as to the economics and psychology literature on intertemporal decision-making and time inconsistent preferences. For firms and their marketers, it has been one of the most important questions to understand why consumers choose to purchase rather than rent durable goods. (Price and Belk, 2016). Our finding provides a unique economic explanation (i.e., commitment incentive) of why owning may be better than renting for consumers.====The remainder of the paper is organized as follows. Section 2 develops a simple three-period nondurables–durables-savings decisions model with quasi-hyperbolic discounting. Section 3 presents a demand function for durable goods and shows that the commitment premium has a positive value if the period-utility is substitutable. Section 4 investigates how a commitment incentive makes consumers prefer owning a durable good over renting it. Section 5 is the conclusion. The proofs of the lemmas and propositions are in the Appendices. In Appendix E, we apply the main result to a housing-consumption-savings model and present a quantitative analysis.",Durable goods as commitment devices under quasi-hyperbolic discounting,https://www.sciencedirect.com/science/article/pii/S0304406821001245,17 September 2021,2021,Research Article,74.0
Athanasoglou Stergios,"Department of Economics, University of Milan - Bicocca, Building U6, Piazza dell’Ateneo Nuovo 1, Milano 20126, Italy","Received 25 January 2021, Revised 8 July 2021, Accepted 24 August 2021, Available online 5 September 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102560,Cited by (1)," emissions that enhances every region’s welfare, while at the same time achieving ==== and respecting norms of fairness.","The difficulty of sustaining cooperation in the management of common resources is well-established, both theoretically and empirically. International environmental agreements (IEAs) provide a salient example of this general fact (Barrett, 2003). With few exceptions, it has proven very hard to negotiate effective treaties to curb global emissions of pollutants. The main reason behind the elusive nature of these agreements are the strong incentives for free-riding that countries are faced with. Free-riding occurs when it is possible to reap the benefits of cooperation without contributing to it. A large body of economic literature, too extensive to cite here, has advanced our understanding of the drivers and impacts of free-riding behavior.====In the climate context, various ways have been proposed to move the negotiations forward. Climate clubs (Nordhaus, 2015), voluntary pledge-and-review frameworks such as the Paris Agreement (Sheriff, 2019), agreements that focus on the supply side of fossil fuels (Harstad, 2012) are just a few examples of innovative mechanisms that have been considered. Of greater relevance to the current paper are appeals to ethical norms and principles as a negotiating device. Empirical and experimental evidence suggests that equity considerations can play an important role in environmental policymaking (Lange et al., 2007, Gampfer, 2014). In the context of climate-change policy, equity is taken very seriously by the United Nations’ International Panel on Climate Change (IPCC). A number of recent papers study the consistency of various mitigation trajectories with prominent ethical principles (Raupach et al., 2014, du Pont et al., 2017).====As one might expect, there is a lively debate on the ethical principles that should underpin climate policy. This debate is far from settled (Höhne et al., 2014, du Pont et al., 2017, Sheriff, 2019). However, even if the interested parties could agree on an appropriate standard of equity, two major problems would persist. First, the chosen ethical principle might lead to an outcome that clashes with mild requirements of efficiency such as Pareto efficiency. Second, its implementation might imply a reallocation of emissions that some agents find worse than the status-quo, and thus violate individual rationality. Both issues present major challenges to an agreement’s acceptability. As regards efficiency, it would be difficult to justify choosing an allocation over another that would Pareto-improve upon it. Analogously, it would be hard to convince the relevant parties to participate in a collective effort if doing so would leave some of them worse off.====It is therefore important to know whether allocations satisfying a given standard of fairness are consistent with Pareto efficiency and individual rationality. If they are not, then this would suggest that it may be necessary to rethink the fairness requirement. Conversely, if suitable allocations do exist, it would be useful to have a practical way of computing them.","On the existence of efficient, individually rational, and fair environmental agreements",https://www.sciencedirect.com/science/article/pii/S0304406821001233,5 September 2021,2021,Research Article,75.0
"Houba Harold,Li Duozhe,Wen Quan","School of Business and Economics, Vrije Universiteit Amsterdam and Tinbergen Institute, De Boelelaan 1105, 1081 HV Amsterdam, Netherlands,Department of Economics, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong,Department of Economics, University of Washington, Box 353330, Seattle, WA 98195-3330, USA","Received 1 March 2021, Revised 12 July 2021, Accepted 6 August 2021, Available online 26 August 2021, Version of Record 4 February 2022.",https://doi.org/10.1016/j.jmateco.2021.102558,Cited by (1),"We study a bargaining model in which players compete for the right to propose in every period, hence the bargaining protocol is determined endogenously. This model admits multiple equilibria, including many with delayed agreements and costly competition. We develop a general self-generating technique to characterize the extreme equilibria, and hence, the entire set of equilibrium payoffs. Our technique resembles an optimal contract design problem with all the sequential rationality conditions being treated as the incentive constraints. By means of an example, we show that as the discount factor goes to one, a player could obtain almost the entire surplus in his best equilibrium, while players’ competition efforts diminish to zero. We also find that some asymmetric equilibria are more efficient than the stationary equilibrium.","Since the seminal work of Rubinstein (1982), the non-cooperative bargaining literature has grown tremendously to study all kinds of bargaining problems and issues in economics and other social sciences.==== It has been widely recognized that bargaining outcomes can be quite sensitive to specific details of the bargaining protocol. However, one robust and prominent result is that a bargainer usually benefits from serving as the proposer, a feature that is commonly known as the “proposer advantage” or “agenda-setting power”. With a few exceptions, most of the existing work in the literature focuses on bargaining models with exogenously fixed bargaining protocols, by which who proposes in each period is modeled either deterministically (e.g., Rubinstein, 1982) or stochastically (e.g., Miller et al., 2018). However, in many real-life situations, such as political campaigns by rivaling parties, competition for proposing new policies and regulations by special interest groups, the bargaining protocol is endogenous and can be influenced by the parties involved. Given the strategic advantages of serving as a proposer or agenda-setter, it is important to understand how bargainers would engage in costly competition for the right to propose. Along this line, some work has been done and most of it has focused on symmetric and stationary equilibria.==== However, it is not immediately clear whether endogenous competition leads to multiple of equilibria, and if so, what the range of possible equilibrium outcomes is.====In a non-cooperative bargaining model with costly competition for the right to propose, our objective in this paper is to provide a complete equilibrium analysis, in particular, a full characterization of the extreme equilibria that are non-stationary, hence all equilibrium outcomes. In each period of the bargaining game, players first compete for the right to propose by exerting costly efforts. Each player will be recognized as the proposer with some probability that is determined by both players’ contest efforts. Then, the recognized proposer will propose a feasible allocation, and the other player responds. Bargaining ends if the proposal is accepted; otherwise, the game proceeds to the next period.====In order to characterize the set of all equilibrium outcomes, we utilize the recursive structure of this canonical bargaining model. We first establish the existence of a stationary subgame perfect equilibrium. To pin down the set of all equilibrium outcomes, we derive the extreme equilibria by adapting the backward induction technique on the supremum and infimum of a player’s equilibrium payoffs as initially developed by Shaked and Sutton (1984). An equilibrium is extreme if one player receives his lowest or highest equilibrium payoff. All other equilibrium outcomes can be supported by the extreme equilibria by using a player’s worst equilibrium as punishment if this player fails to comply with the prescribed outcome.==== More specifically, any outcome that gives each player a payoff higher than his worst equilibrium payoff can be supported as an equilibrium outcome when players are sufficiently patient, i.e., as the discount factor is sufficiently close to one. Hence, once we identify the extreme equilibria, particularly every player’s worst equilibrium payoff, we will be able to characterize all the other equilibrium outcomes. The key element here is that any deviation, including exerting competition effort, will be punished by an immediate switch to the deviator’s worst equilibrium in the continuation game. This allows the construction of efficient non-stationary equilibria in which no wasteful competition takes place. In contrast, as reported in the literature, the stationary equilibrium often entails wasteful competition and is thus inefficient.====Our technique of equilibrium construction is similar to that commonly used in establishing folk theorems for repeated games. When the discount factor is sufficiently close to one, a player’s short-term gain from deviation will be offset by his long-term loss from future punishment, i.e., his worst equilibrium in the continuation game. While a bargaining game ends whenever an agreement is reached, its incentive mechanism is in the same vein as that in repeated games. However, the adaptation of this general technique to the current model turns out to be challenging. Although our extreme equilibria are valid for any value of the discount factor, players may have short-term incentives to deviate, and thus, a sufficiently high discount factor is required to validate other (non-extreme) equilibrium outcomes.====Using a backward induction method, Shaked and Sutton (1984) establish the uniqueness of equilibrium for Rubinstein’s (1982) bargaining game. This method has also been adapted to characterize the extreme equilibria in bargaining games with multiple equilibria. More specifically, sequential rationality conditions can be specified as a set of inequalities for the supremum and the infimum of each player’s equilibrium payoff. When applying this method in our model, we also need to consider players’ sequential rationality in choosing their competition efforts. We show that these bounds of equilibrium payoffs must satisfy a system of equations that involve constrained optimization problems, that is, either maximizing or minimizing one player’s payoff subject to players’ incentive constraints in choosing their contest efforts. These incentive constraints are determined by the extreme equilibrium payoffs in the continuation game. Each of these optimization problems resembles an optimal contract design problem with a player’s payoff as the objective function and players’ sequential rationality conditions as the incentive constraints. Any fixed point, in terms of optimal payoffs of the resulting equation system, can be self-sustained as equilibrium payoffs. In fact, this equation system admits a fixed point corresponding to a stationary subgame perfect equilibrium. We show that the extreme fixed point of this equation system provides all four extreme equilibrium payoffs at once. An extreme equilibrium outcome is sustained by different extreme equilibria in different continuation games. Note that our equation system cannot be further reduced to a smaller system with two infimum payoffs only. This is another complication that does not appear in other bargaining models.====This approach of finding the extreme equilibrium payoffs resembles the “self-generating” technique of identifying the set of equilibrium payoffs in repeated games with imperfect monitoring, see, for example, Abreu et al. (1990), Mailath and Samuelson (2006). In a repeated game, imperfect monitoring generally restricts the set of equilibrium payoffs in all directions. In our model, once we have derived all the extreme equilibria, we only need to focus on every player’s worst equilibrium payoffs, as other equilibria can be constructed from these worst equilibrium outcomes. Unlike in a repeated game, players’ interaction within a period in our model is dynamic, players first compete for the right to propose, then one player proposes, and the other responds. In our adaptation of this “self-generating” method, we have to incorporate all players’ dynamic incentives in the optimization problems.====By identifying the extreme fixed point of the aforementioned equation system, we can not only pin down the extreme equilibrium payoffs but also derive the extreme equilibrium strategy profiles. In player 1’s worst equilibrium, for example, his payoff will be minmaxed with respect to both players’ contest effort levels, while player 2’s incentive to comply with the minmax scheme can be enforced by player 2’s worst continuation equilibrium. In player 1’s best equilibrium, on the other hand, his payoff will be maximized with respect to the contest effort levels; however, the continuation play after the contest is generally not player 1’s best equilibrium. This is because, to effectively induce player 2’s cooperation in the competition stage, the continuation play has to allocate him a greater share than that prescribed in player 1’s best equilibrium. We will illustrate these subtle features of the extreme equilibria with an example, in which one player can obtain almost the entire surplus in his best equilibrium when the discount factor goes to one. Similar techniques have been applied to derive the extreme equilibria in some other bargaining models, such as Houba and Wen (2011), but their characterizations are valid only when the discount factor is sufficiently large. In our treatment, by incorporating all the equilibrium conditions, the system of equalities pins down the extreme SPE payoffs for all values of the discount factor.==== There are a few papers in the non-cooperative bargaining literature that explicitly model how the right to propose is allocated through auctions or contests. Yildirim (2007) studies a multilateral legislative bargaining model with players competing for the right to propose in every period. The recognition process is continuous in players’ effort levels. Focusing on the stationary subgame perfect equilibrium, Yildirim (2007) investigates how the bargaining outcome is affected by the discount factors, the cost functions of effort, and the voting rule. Branco (2008) studies a modified Rubinstein model without a pre-specified first mover; instead, the two players first play a coordination game to determine, once and for all, who proposes first.====Under a dynamics legislative bargaining setting, Fong and Deng (2011), Levy and Razin (2013) model multilateral competition for the right to propose in the form of an all-pay auction. They consider majority voting over a one-dimensional policy space, which differs substantially from our bargaining model with unanimity rule. Focusing on the stationary equilibrium, they aim to provide insights into the understanding of political institutions. In particular, they establish a generalized median voter theorem.====Board and Zwiebel (2012) analyze a finite-horizon bilateral bargaining model with competition for the right to propose. The competition takes the form of a first-price auction, for which each player faces a budget constraint. The equilibrium outcome critically depends on players’ initial endowments, and in some cases, the familiar alternating-offer bargaining protocol emerges endogenously.====Ali (2015) introduces a multilateral bargaining model with either a finite or an infinite horizon, in which players compete to propose via an all-pay auction in every period. He focuses on the subgame perfect equilibrium in the finite horizon model and the stationary subgame perfect equilibrium in the infinite horizon model. Ali (2015) shows that under any voting rule without veto power, the proposer captures the entire surplus. Under the unanimous voting rule, the proposer will share the surplus with at most one other player. As the length of a bargaining period vanishes, one player may obtain virtually all the surplus, which would be this player’s best equilibrium outcome.====Cuellar (2020) studies a bilateral bargaining model where the two players make two types of efforts in every period: productive efforts to increase the total surplus and unproductive efforts to compete for the right to propose. He focuses on how these two types of efforts interact in the Markov perfect equilibrium. More recently, Karagözoğlu and Rachmilevitch (2021) study a bilateral bargaining model with a random proposer. In every period, in order to stay in the game, each player must pay a cost; otherwise, he concedes all the surplus to the other player who chooses to stay. Their model admits one symmetric “war of attrition” equilibrium, in which players randomize between stay and quit, and, the proposer always demands the entire surplus, which is always rejected. This equilibrium outcome is stochastic and inefficient, with each player’s payoff converging to zero as bargaining frictions vanish. Nevertheless, asymptotic efficiency can be achieved in asymmetric equilibria.====Non-cooperative bargaining models with endogenous bargaining protocols have been used to implement cooperative-game theoretic solutions. Evans (1997) studies a coalitional bargaining game in which patient players compete to make an offer in every period via an all-pay auction. It is shown that the set of stationary subgame perfect equilibrium outcomes coincides with the core of the underlying coalitional game. Pérez-Castrillo and Wettstein (2001) introduce a finite-horizon coalitional bargaining game, in which players bid for the right to propose in each round and the winner pays his net bids to other players before he can make a proposal. If the proposal is accepted, a grand coalition is formed and the value is divided accordingly; otherwise, the proposer leaves with his stand-alone value while other players continue in a similar fashion. The subgame perfect equilibrium outcome of their game implements the Shapley value of the underlying coalitional game. Borm et al. (2015) implement the Rational Belief Shapley value in a bargaining mechanism in which players also compete to propose.====It is worthwhile to emphasize that in all these related studies, only symmetric and stationary equilibria have been studied. We know very little about multiplicity and asymmetric equilibria, and yet, some studies, such as Karagözoğlu and Rachmilevitch (2021), indicate the importance of asymmetric equilibria. Our study not only characterizes the extreme equilibria, and hence all possible equilibrium outcomes, but also sheds light on how to find relatively simple asymmetric equilibria, including efficient equilibria.====The remainder of this paper is organized as follows. We present the model in Section 2. In Section 3, we establish the existence of a stationary subgame perfect equilibrium. Section 4 contains our main analysis of the extreme subgame perfect equilibria. In Section 5, we provide an example to illustrate our findings. Section 6 concludes with a few remarks. All the proofs are relegated to the Appendix.",Bargaining with costly competition for the right to propose,https://www.sciencedirect.com/science/article/pii/S030440682100121X,26 August 2021,2021,Research Article,76.0
"Baril-Tremblay Dominique,Marlats Chantal,Ménager Lucie","Paris School of Economics, Université Paris 1, France,LEMMA, Université Paris 2 Panthéon-Assas, France","Available online 20 May 2022, Version of Record 11 August 2022.",https://doi.org/10.1016/j.jmateco.2022.102705,Cited by (0),None,None,Corrigendum to “Self-isolation” [J. Math. Econom. 93 (2021) 102483],https://www.sciencedirect.com/science/article/pii/S0304406822000519,20 May 2022,2022,Research Article,79.0
"Loi Andrea,Matta Stefano","University of Cagliari, Italy","Received 11 March 2021, Revised 5 July 2021, Accepted 4 August 2021, Available online 13 August 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102555,Cited by (2),". The following conjecture is proposed under the assumption of a uniform probability distribution: entropy is minimal if and only if the price is unique for every economy. We show the validity of this conjecture for an arbitrary number of goods and two consumers and, under certain conditions, for an arbitrary number of consumers and two goods.","In a pure exchange economy let us denote by ==== an initial regular allocation ==== and its supporting equilibrium price vector ====. Suppose that ==== is slightly perturbed in the same smooth selection by exogenous, i.e. shocks, or endogenous factors, e.g. the uncertainty related to the effects of Safra’s competitive manipulation (Safra, 1983). The result of this perturbation is a new allocation and equilibrium price vector, ====, belonging to a neighborhood ==== of ====. We can represent this process as a probability model, where the random variable ranges in the set ====. Observe that ==== is not a Euclidean space. It belongs to a space of endowments and prices and consists of points such that aggregate excess demand function is equal to zero. Under standard smooth assumptions and in a fixed total resource setting, ==== becomes a submanifold with boundary of a manifold called the ====, denoted by ==== which in turn is a smooth submanifold of ====, where ==== is the space of prices and ==== the space of economies (see the seminal work by Balasko (1988) and also Section 2).====Thus ==== can be equipped with a natural measure, namely the Riemannian volume form ==== associated to the Riemannian metric ==== induced by the flat metric of its ambient space ==== (see e.g. Loi and Matta (2018)).====We recall that a Riemannian metric ==== on a smooth manifold ==== is a family of inner products ==== on the tangent space ====, ====, varying smoothly with ====. A smooth manifold with a Riemannian metric is called a Riemannian manifold. In this paper ==== is ==== and ====, where ==== denotes the standard (flat) inner product on ====.====The probability that ==== belongs to ==== is ====where ==== is a given probability density on ==== (the reader is referred to Pennec (2004) for a geometric approach to probability theory on Riemannian manifolds). Roughly speaking, the volume form ==== is locally (i.e. in a chart) given by the product of a smooth function with the standard Lebesgue measure. This definition of probability density takes into account the change of coordinates coming from the differentiable structure of ====.====Since ==== is globally diffeomorphic to ==== (Balasko, 1988 Corollary 5.2.5), a probability density on ==== naturally induces a probability density on ====. The advantage of considering ==== relies on the fact that this probability density depends on the shape of ====, which provides more information since it reflects the interplay between equilibrium prices and endowments.====Moreover, following Shannon and Weaver (1949) (see also Cover and Thomas (2006)), in this framework we define the ==== ==== as====
 ====It should be observed that several issues may arise when entropy is defined for continuous probability distributions: e.g., the density function, or the integral, could not exist or the entropy could be negative (Cover and Thomas, 2006 Chapter 8), Hence, generally speaking, differential entropy does not enjoy all the properties of its discrete counterpart==== (see also Reza (1961)). But, as Shannon observes: “The scale of measurements sets an arbitrary zero corresponding to a uniform distribution over a unit volume. A distribution which is more confined than this has less entropy and will be negative” (Shannon and Weaver, 1949). Hence, as we will see (cf. (1) below), we can consider differential entropy as a misure of missing information and then it becomes natural to investigate under which conditions it is minimized.====Therefore we provide the following:====It would be interesting (and challenging) to study how the choice of different probability distributions affects the economic properties implied by (MEP). This issue, which deserves further analysis, is beyond the scope of this paper.====On the other hand, it is natural to restrict to the case of uniform distribution, namely when the probability density function is given by ====where ==== is the volume of ==== and ==== is the characteristic function supported on ====.====The choice of the uniform distribution is motivated by the fact that it reflects the state of uncertainty before the perturbation takes place, where every position of ==== in ==== is equiprobable: i.e., the amount of prior information is minimal by default. The uniform distribution achieves this minimization since, among all continuous distributions supported in a given volume ====, the uniform distribution is the maximum entropy distribution (see Shannon and Weaver (1949)).====Under this assumption, ====and so, by the increasing property of the logarithm, we deduce that in the case of uniform distribution the (MEP) is equivalent to the following====Now the (MVP) for ==== can be translated into the language of differential geometry: the equilibrium manifold ==== is a ==== of ====, i.e. a local minimum of the volume functional. In particular, ==== is a critical point of the volume functional, namely ==== is a ==== of ====.====Observe now that according to Theorem 2.1, if for every economy there is uniqueness of equilibrium, the equilibrium manifold is “flat” (and hence minimal): i.e., (global) uniqueness implies (MVP). Here we explore the reverse of this implication: if there is price multiplicity, can (MVP) hold true? In other words, does (MVP) imply uniqueness? This is not a trivial issue: in fact the equilibrium manifold can almost arbitrarily be twisted for an appropriate preference profile.==== Hence one could expect to find a utility profile which gives rise to multiplicity and minimality. Actually, we believe that exactly the opposite is true. Indeed we address the following:====In other words, we believe that an utility profile which minimizes entropy (and hence volume) with uniform distribution is incompatible with price multiplicity.====In this paper we show the validity of this conjecture in the case of an arbitrary number of goods and two consumers (Theorem 3.1) and in the case of an arbitrary number of consumers and two goods (Theorem 4.1) under the additional assumption that the normal vector field of ==== is constant outside a compact subset of the ambient space. The proof of Theorem 3.1 strongly relies on geometric and economic properties: the classification of ruled minimal submanifolds of the Euclidean space, the bundle structure of the equilibrium manifold and the positiveness of prices. On the other hand, the proof of Theorem 4.1 combines deep geometric results relating the topology of a minimal submanifold of the Euclidean space with the fact that ==== is globally diffeomorphic to an Euclidean space.====In Loi and Matta (2018) we study the link between the curvature of the Riemannian metric ==== induced by the flat metric and uniqueness of equilibria. While the curvature is an intrinsic property, i.e. it does not depend on the ambient space, the minimality assumption deals with the shape of ==== in its ambient space ====.====We believe that this information-theoretic and geometric approach can be further extended in different directions. Following the seminal contribution by Theil (1967) (see also Cowell, 1980, Cowell, 2011 and Maasoumi (1999)), an entropy-based metric could be developed in order to compute geodesics representing redistributive policies. Another direction of research (see Safra (1983) and Goenka and Matta (2008)) is to analyze the extrinsic uncertainty in ==== caused by coalitional manipulation of endowments. This approach could provide new insights into the understanding of coalition formation and sunspot equilibria. Finally, due to the economic relevance of the shape of the equilibrium manifold, it can be worth investigating the connection between its shape and the primitives of the model, an issue still largely unexplored. This local–global view can hopefully lead to new perspectives on issues like uniqueness and stability (see Kehoe (1998) and Mas-Colell (1991) for a survey).====This paper is organized as follows. Section 2 recalls notations, definitions and the existing results which will be used to prove our main results. Sections 3 The case, 4 The case prove our main results, Theorem 3.1, Theorem 4.1. Finally, Section 5 draws the conclusions summarizing the main results.",Minimal entropy and uniqueness of price equilibria in a pure exchange economy,https://www.sciencedirect.com/science/article/pii/S030440682100118X,13 August 2021,2021,Research Article,84.0
"Bhattacharya Mihir,Mukherjee Saptarshi,Sonal Ruhi","Department of Economics, Ashoka University, Rajiv Gandhi Education City, Rai, Sonipat, NCR, Haryana, 131029, India,Department of Humanities and Social Sciences, IIT Delhi; Centre for Mathematical and Computational Economics, School of AI and Data Science, IIT Jodhpur, India,Department of Humanities and Social Sciences, IIT Jodhpur; Centre for Mathematical and Computational Economics, School of AI and Data Science, IIT Jodhpur, India","Received 25 July 2020, Revised 18 July 2021, Accepted 23 July 2021, Available online 13 August 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102553,Cited by (2),"We consider an individual decision-making problem where the ==== is subject to inattention. The source of this inattention is the frames with which the alternatives appear; leading to randomness in choice. We characterize a rule called ====, according to which the choice ==== of an alternative (say, ====) is the ==== with which attention is drawn by its frame and not by the frames which are associated with the alternatives that beat ==== according to a complete and antisymmetric ====. The choice probability is zero if a better alternative appears with the same frame. The rule and the treatment are largely influenced by Manzini and Mariotti (2014). Our framework is applicable to settings where frames are advertisements or types of packaging."," We consider a decision maker whose choice is random due to inattention. There are many papers which model random choice.==== Some papers have modeled inattention as a source of the randomness (e.g. Manzini and Mariotti, 2014, Cattaneo et al., 2020, Kovach, 2018).==== In our paper, inattention is attributed to the ==== with which the alternatives appear. As we will elaborate later, we use frames in a rather specific sense: a frame is the manner in which an alternative is presented to the decision maker. It may take various forms such as endorsement of a product by a celebrity, packaging an item in a particular manner, etc. Broadly, the concept of frames is used in this paper to present the alternatives to a decision maker. This interpretation follows Salant and Rubinstein (2008) — we note that frames do not provide any additional information that helps the decision maker in the rational evaluation of the alternatives. Consider the following examples which exhibit how such frames may affect attention paid to the products and thereby the choice probabilities:==== Framing effects such as the above cause randomness in the decision maker’s attention and therefore lead to random choice. In this paper, we model randomness in attention paid to an alternative as the ==== source of randomness in choice. Generally the decision maker’s attention to different products may vary. She may pay more attention to some products as compared to others because of the frames in which they appear. We aim to provide a testable theory for analyzing the same. As mentioned earlier we use frame in a particular sense, i.e. the effect of the frame is limited only to the extent it influences the decision maker’s attention. Specifically, we note the following:====We provide some examples which fit our setting:==== There are many papers, especially in marketing literature that acknowledge the impact of frames on the decision maker’s attention: Bloch (1995), Bloch et al. (2003) discuss the importance of packaging in drawing the decision maker’s attention to a product; Pieters and Wedel (2007) analyze effects of pictorial and text advertisements on attention; the effects of product placement on attention are discussed in Chandon et al. (2009), Nedungadi et al. (2001) etc. For a seller it is useful to identify these effects of various frames. For instance, if a supermarket would like to improve the chances that a product is sold, it may be useful to place it in a location that draws more attention, e.g. near the checkout counter. Rook and Fisher (1995) discuss such “impulsive” purchase decisions. Most of these papers carry out analysis based on data collected from experiments or other samples. To the best of our knowledge there is no existing theoretical model that provides identification or isolates such framing effects. In our model, the attention drawn by a frame is identified.==== A “product” in our model is a pair ==== where ==== is an alternative and ==== is the frame associated with ====. A random choice rule determines the probability with which a product is chosen from any set of products.==== We characterize a class of stochastic choice rules which we call ====. This rule can be described as follows: there exists a complete, antisymmetric binary relation (====) over the set of alternatives and an attention function that assigns a probability to every frame such that for any set of products ====, the choice probability of ==== in ==== is the probability that attention is drawn by frame ==== and not by frames attached to those alternatives in ==== that beat ==== according to the binary relation, if ==== is the ====-maximal alternative amongst those appearing with frame ==== in ====. If an alternative that beats ==== also appears with the frame ====, then ==== is chosen with zero probability. The binary relation and the attention function in the rule are identifiable from the choice probabilities. We interpret the attention parameter as the probability with which a particular frame draws the decision maker’s attention. We characterize frame-based stochastic choice rule using three axioms: ====, ==== and ====.====Invariance of singletons requires that two products with the same frame are chosen with the same probability from singleton sets. This implies that the probability of choosing a product from a singleton set is independent of the alternative and depends solely on the frame with which the alternative appears.====Dominance has two parts. Part (i) of dominance requires that for a given set of products ==== if ==== ====
 ==== when they appear in frames ==== and ==== respectively, then ==== dominates ==== when they appear in any other frames ==== and ==== respectively. In other words, if ==== dominates ====, controlling for the frames ==== and ====, then ==== dominates ==== for any two frames. Dominance (ii) requires that given a set of products ====, if an alternative ==== dominates all other alternatives ==== in ====, then its choice probability is the same as its choice probability when it appears alone. This is an independence requirement similar to the classical ==== axiom.====The third axiom is similar to ==== in Yildiz (2016) and ====    in Manzini and Mariotti (2014). It can also be seen as a stochastic version of Plott’s ==== condition applied to the setting with frames.==== (Manzini and Mariotti, 2014)==== There are many papers which deal with stochastic choice in different settings==== such as Aguiar (2017), Echenique et al. (2018), Manzini and Mariotti (2014), Tserenjigmid (2019), Fudenberg et al. (2015), Li and Tang (2017), Ahn et al. (2017), Caplin and Martin (2018), etc. The paper that motivates our work the most is Manzini and Mariotti (2014). Manzini and Mariotti (2014) characterize a stochastic choice rule with limited attention ====, called the random consideration set (RCS) rule. Therefore, a crucial difference between our model and theirs is that in their model the decision maker’s attention biases depend on the alternatives, while we model limited attention as a result of frames. They model stochastic choice as the following two-stage decision making process. In the first stage, a ‘consideration set’ is formed from which the decision maker chooses an alternative in the second stage. The formation of consideration set is random since every alternative is considered with some probability. We do not model the two-stage procedure explicitly, but the formation of consideration sets can be interpreted in our framework as follows: every product is considered with the probability with which its frame attracts the decision maker’s attention. The final choice therefore depends on the attention probabilities of the frames.====Our rule intensely uses the structure of the RCS rule in Manzini and Mariotti (2014). RCS rule brings out, in a very powerful way, how choice probability of an alternative is determined through an interaction between the decision maker’s preference over the alternatives and their attention probabilities. The frame-based stochastic choice rule uses the interaction between the decision maker’s preference over the alternatives and the attention probabilities over ==== with which the alternatives appear. Further, the axioms and the execution are different in this paper.====Caplin and Martin (2018) model the welfare aspects of frame-based individual decision making. In their paper, a planner uses framing as an information structure to influence decision making. They extend this idea to stochastic choice as well; however they do not explicitly model the randomness in attention attributed to framing.====Kovach (2018) models the effect of status quo in the setting of stochastic choice with limited consideration. In this paper, a “status quo random choice rule” is characterized and three different channels through which status quo affects choice are discussed. Status quo can be perceived as a type of frame and it would be interesting to model the same in future work on stochastic choice and framing.====Suleymanov (2018) considers a reference dependent random choice model to obtain several representation theorems for different consideration set models. The reference point can be treated as a frame and hence the results in this paper could be used to explain the effect of frames on consideration set formation.==== In this paper, we develop a general framework for the analysis of framing effects in a stochastic choice setting. The attention probability in our model is a function of the frame, but is not required to follow a specific functional form. Further, it is independent of the decision maker’s preferences and also of the characteristics of the alternatives. Frame-based stochastic choice rule demonstrates the effects of frames in drawing the consumer’s attention to a product and thus helps isolating the effect of frames. More specifically:====Our framework can be modified to apply to settings where the frames are positions in a list (Rubinstein and Salant, 2006) or ordered trees (Mukherjee, 2014).====The paper is organized as follows. Section 2 describes the model and discusses the axioms. Section 3 provides the main result and some examples. The proof of the theorem is presented in the Appendix.",Frame-based stochastic choice rule,https://www.sciencedirect.com/science/article/pii/S0304406821001166,13 August 2021,2021,Research Article,85.0
"Camera Gabriele,Gioffré Alessandro","Economic Science Institute, Chapman University, One University dr., Orange, CA 92866, United States of America,DSE, University of Bologna, Italy,DISEI, University of Florence, Italy","Received 8 February 2021, Revised 1 June 2021, Accepted 8 July 2021, Available online 18 July 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102552,Cited by (2),"The sudden appearance of the SARS-CoV-2 virus and the onset of the COVID-19 pandemic triggered extreme and open-ended “lockdowns” to manage the disease. Should these drastic interventions be the blueprint for future epidemics? We construct an analytical framework, based on the theory of random matching, which makes explicit how epidemics spread through economic activity. Imposing lockdowns by assumption not only prevents contagion and reduces healthcare costs, but also disrupts income-generation processes. We characterize how lockdowns impact the contagion process and social welfare. ==== suggests that protracted, open-ended lockdowns are generally suboptimal, bringing into question the policy responses seen in many countries.","The emergence of the SARS-CoV-2 virus and the onset of the COVID-19 pandemic motivated many governments to bring to a stand-still all human activity, social and economic, for many months. The stated objective is to slow down contagion and prevent healthcare systems from being overwhelmed. Many countries have gone to the extreme of imposing long-lasting and drastic “lockdowns” (e.g., China, Italy, Spain, UK), i.e., mandatory stay-at-home orders, business closures, and sweeping limitations to the freedom of movement. These lockdowns (also known as NPI’s for nonpharmaceutical interventions) have been implemented on an open-ended basis, with a severity and duration primarily tied to the growth rate in infections. Apart from a few notable exceptions (e.g., Sweden), most countries have sought to minimize a single risk, that of contagion from the SARS-CoV-2 virus, without fully accounting for the economic and social consequences of doing so. In the aftermath of these interventions, a public debate emerged questioning the ==== of these policies. Are these drastic interventions optimal from a social welfare perspective? Should we keep them in place to address future epidemics? Should their implementation be open-ended?====The answer to these questions partly depends on how one models the relevant economic tradeoffs. This paper develops an analytical framework that makes explicit the process of contagion, and ties it to the frequency of economic activity. The model economy has a constant population composed of individuals who can earn income only in periods in which they meet a trade partner. Meetings occur on a market where a matching process pairs individuals at random—all pairs generate a deterministic flow of income, and dissolve at the end of the period. The model assumes transmissibility via asymptomatic individuals, a central feature of the COVID-19 epidemic, as well as no cost from trading while asymptomatic. This implies that individuals who are unaware of being infected, have no incentive to stay out of the market and, hence, can spread the disease by meeting healthy trade partners. Repeating this random matching process period after period is how the epidemic spreads over time. It is assumed that precluding business activity by closing the market, i.e., imposing a lockdown, stops further contagion. Apart from the possibility of reaching ====, no other intervention is assumed to exist to manage the progression of the disease. Therefore, lockdowns are the go-to policy to reduce healthcare costs, which are assumed proportional to the spread of the infection across the population. The severity of the policy intervention corresponds to the lockdown duration.====Our analysis is divided into two parts. First, we lay out the mathematical machinery needed to characterize the contagion process for general interventions, ranging from minimal as in Sweden to extreme as in China or Italy. The initial step is to construct ==== that determine the path of the infection, for any initial state of the infection. These are then used to calculate the dynamic evolution of the epidemic when we impose lockdowns of various degree of severity. The analysis considers two scenarios, depending on whether the infection can or cannot die out by achieving herd immunity by medical means or naturally.====Second, we construct a measure of social welfare that combines individual payoffs from trade with expected healthcare costs associated with the spread of the disease. Lockdowns now delineate a tradeoff: more drastic interventions prevent overwhelming the healthcare system but destroy income flows. ==== suggests that welfare nonlinearly responds to the severity of the intervention, which leads to two main results. Imposing a lockdown is generally welfare-enhancing if the infection spreads easily. However, the welfare benefit rapidly dissipates as the lockdown length increases, and turns into a welfare loss eventually. If the infection is detected early and has reached only a small subset of the population, then imposing an extreme lockdown is counterproductive in terms of social welfare. Open-ended lockdowns are not necessarily optimal either, especially if the epidemic can be brought under control via herd immunity.====Intuitively, in our model the social gains from not overburdening the healthcare system are eventually overtaken by the economic losses stemming from further reductions in income flows. This is why extreme lockdowns are largely suboptimal. In fact, the analysis also reveals that naïvely matching the severity of the intervention to the spread of the infection is not the most logical policy because welfare gains are non-linear. Overall, this exercise suggests that policymakers should tread carefully. To the extent that healthcare conditions and income-generating processes are country-specific, the model indicates that the tradeoffs associated with lockdown policies are also country-specific. In other words, there is no “one-size-fits-all” kind of policy, which seems opposite to the adoption patterns seen so far, where many governments simply followed similar policies.====There is a voluminous literature on infectious diseases, an extensive review of which is beyond the scope of this paper. Due to space limitations, we refer the reader to the recent survey in ==== and here we explain what our study adds to the existing literature. The novelty of our contribution lies in the approach to studying the diffusion of epidemics, which relies on the theory of random matching. This technique allows us to offer a framework that makes explicit the transmission of a disease in the population—in contrast to the standard epidemiological model, which uses a reduced-form approach. This framework is then used to assess the economic optimality of policy interventions based on the lockdowns imposed in the recent past.====To elaborate on this, start by noting that the typical model used in the epidemiology literature – known as the SIR model – is based on three possible states for an individual: S for susceptible (to infection), I for Infected, and R for Recovered. The evolution of these three mutually exclusive states is governed by laws of motion that underlie a reduced-form process of contagion. We retain the three-state representation typical in the literature, and innovate by constructing an ==== model of contagion, which is based on a pairwise random meeting process and the analysis technique developed in ====. In this manner, the model allows us to track the evolution of the disease when the markets are open or close – thanks to the explicit matching process – and thus study the optimality of lockdown policies under alternative scenarios for the initial state of the infection and the probabilistic nature of reaching immunity.==== By contrast, we work with a model where everyone in the population is generally susceptible to the disease until a point where the disease can be fully eradicated. It is assumed that in each period there is a probability that contagion stops, and until that happens everyone remains susceptible to infection, even recovered individuals. In this manner, a state of immunity is reached probabilistically and simultaneously by everyone in the population. This is a mathematically convenient way to capture a prominent aspect of current thinking behind lockdown policies: the COVID-19 disease is so dangerous and hard to contain that the epidemic ==== be stopped with a mass-vaccination campaign. This set-up allows us to trace a most favorable scenario for imposing lockdowns, which is when they delay the progression of the disease while medical and pharmacological interventions are being developed to address the problem.====The paper proceeds as follows. Section ==== presents the model economy. Section ==== characterizes the dynamic process of contagion. Section ==== studies the impact of lockdowns on the spread of the epidemic in a baseline, worst-scenario model when there is no herd immunity threshold. In Section ====, we extend the analysis to a richer model where herd immunity can be reached by naturally acquired immunity and medical discovery. In Section ====, we apply this machinery to determine how interventions of varying severity impact social welfare; this is done by studying income losses and healthcare cost savings associated with lockdowns via numerical experiments. Broader policy implications of our analysis are discussed in Section ====, which concludes the study.",The economic impact of lockdowns: A theoretical assessment,https://www.sciencedirect.com/science/article/pii/S0304406821001154,18 July 2021,2021,Research Article,86.0
"Brandt Nikolai M.,Eckwert Bernhard,Várdy Felix","Bielefeld University, Germany,International Monetary Fund, United States","Received 7 August 2020, Revised 27 June 2021, Accepted 30 June 2021, Available online 15 July 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102544,Cited by (1),"How much can be learned from a noisy signal about the state of the world not only depends on the accuracy of the signal, but also on the distribution of the prior. Therefore, we define a ","Traditionally, information concepts for Bayesian learning have started out from a ==== of the state variable, and have asked what can be learned from different experts (signal technologies) about the unknown state.==== This is true for MIO-ND (Athey and Levin, 2018), Accuracy (Lehmann, 1988, Persico, 2000), but also for Blackwell (1953), who allows the fixed prior to be arbitrary, however.====Certain questions in economics and social science call for a learning concept that compares tuples consisting of different experts ==== priors. For instance, patients with pre-existing medical conditions may differ with regard to their prior probabilities of actually carrying a certain disease, while medical experts may also differ with regard to the diagnostic services (signal technologies) they provide. Similarly, investors may hold different portfolios and thus differ with regard to the prior distributions of their portfolio payoffs, while financial experts may differ in regard to the expertise they provide. Finally, suppose that pharmaceutical firms participate in an R&D race to develop a (Covid) vaccine. The health authorities may give consideration to various requirement profiles (efficacy, tolerability etc.) for the approval of the vaccine. The requirement profiles involve varying challenges and difficulties in the development process leading to different prior distributions of the firms’ random vaccine arrival times. Upon inspecting a firm, experts may provide different assessments of the firm’s strategic position and expected performance in the race.====These examples suggest that, for the purpose of broader applicability, a learning order should allow for variable priors and different experts, thus ranking ==== consisting of a signal technology and a prior. In this paper we propose a framework which meets this requirement. In our definition, learning is measured by the ‘distance’ between prior and posterior beliefs about state quantiles. We then characterize the implied learning order in two ways. The first characterization is in terms of the dispersion of posterior quantile expectations. Broadly speaking, we find that higher ranking in the learning order corresponds to higher mean-preserving spread (MPS) dispersion of posterior quantile expectations. When restricted to information systems with the ==== expert, the learning order turns out to be increasing in a notion of dispersion of the prior state distribution. For a linear expert, i.e., an information structure that is linear in the state variable, this notion of dispersion boils down to a MPS of the prior. When experts ==== but are restricted to be of the Farlie–Gumbel–Morgenstern type, higher ranking in the learning order can be compatible with a less dispersed prior.====The second characterization is in terms of the value of learning for two particular classes of decision makers. The first class consists of all agents with quasi-linear quantile preferences. The second class contains all agents with supermodular quantile preferences. Quantile preferences may describe the behavior of a gambler who is betting on the outcome of a contest. Alternatively, quantile preferences can be used as a vehicle to model the behavior of agents who care about their rank in a population, in regards to, e.g., wealth, income, etc. We find that the learning order corresponds to (and is fully characterized by) larger ex-ante welfare gains for all agents in each class. This implies, of course, that the larger class of all decision makers with either quasi-linear or supermodular quantile preferences also characterizes the learning order.==== Our paper is related to a large body of literature aimed at extending the Blackwell (1951) information order. A signal ==== is more informative than another signal ==== in the Blackwell sense if, for arbitrary but fixed prior, ==== Bayesian decision maker prefers ==== to ====. Various papers have proposed weaker criteria that can be applied to a broader set of information systems. For instance, Lehmann (1988) and Persico (2000) introduce an information criterion according to which all decision makers with payoffs satisfying the single-crossing property prefer one information system over another.====Ganuza and Penalva (2010) and Eckwert and Zilcha (2008) follow a different approach. They propose that informativeness can simply be expressed as MPS dispersion of posterior expectations. However, Brandt et al. (2014) have shown that this dispersion order does not satisfy the desirable property of ordinality of states (OS), i.e., it is not invariant to increasing monotone state transformations. Keeping the prior fixed, Brandt et al. (2014) go on to characterize the weakest information criterion that does respect OS and implies the MPS dispersion order. In this paper, we build on that information order and show that the order can be extended to information systems with ==== priors.",Bayesian learning with variable prior,https://www.sciencedirect.com/science/article/pii/S0304406821001075,15 July 2021,2021,Research Article,87.0
"Dubey Ram Sewak,Laguzzi Giorgio","Department of Economics, Feliciano School of Business, Montclair State University, Montclair, NJ 07043, United States of America,University of Freiburg in the Mathematical Logic Group at Eckerstr. 1, 79104 Freiburg im Breisgau, Germany","Received 11 November 2020, Revised 3 February 2021, Accepted 22 June 2021, Available online 7 July 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102542,Cited by (0),We propose generalized versions of strong equity and Pigou–Dalton ====. We study the existence and the real-valued representation of social welfare relations satisfying these two generalized equity principles. Our results characterize the restrictions on one period utility domains for the equitable social welfare relations (i) to exist; and (ii) to admit real-valued representations.,"The subject matter of ==== equity has received considerable attention in economics and philosophy literature in recent times. It deals with the important question of how to treat the welfare of infinite future generations compared to those living at the present time. The equity principles employed in this literature can be classified in two broad categories, namely, procedural and consequentialist (or redistributive).====Equity concepts which show indifference between any pair of infinite streams whenever the changes do not alter the distribution of utilities are called procedural equity principles. Ramsey (1928) while devising the idea of inter-generational equity, observed that discounting one generation’s utility relative to another’s is “ethically indefensible”, and something that “arises merely from the weakness of the imagination”. Diamond (1965) formalized the concept of “equal treatment” of all generations (present and future) in the form of an ==== axiom (AN) on social preferences. Anonymity requires that the society should be indifferent between two infinite streams of well-being, if one is obtained from the other by interchanging the levels of well-being of any two generations.====Equity concepts which require an alteration in the distribution of utilities are called consequentialist equity principles. Several formulations of this equity notion has been discussed in the social choice literature. Important redistributive equity axioms are ==== (PD), ==== (SE) and ==== (HE). Originally, these equity notions were conceptualized in social choice models with finitely many agents or in finite time horizon economies, which consider finitely many generations. However, these ideas have now been extended to the standard infinite horizon economy with infinitely many generations or in social choice models with infinitely many agents. The standard framework in the literature treats infinite utility streams as elements of the set ====, where ==== is a non-empty subset of real numbers and ==== is the set of natural numbers. The set ==== describes all possible levels of utility that any generation can attain. The object of study in our paper is ==== (a reflexive and transitive binary relation) which satisfies some equity axioms. In case the social welfare relation is complete, it is known as ==== and a real-valued function representing the social welfare order is termed as a ====. The PD and SE axioms generate strict preference (ranking) for an infinite utility stream obtained from another after carrying out desirable redistribution. In contrast, HE generates weak preference (ranking) in similar situations. In what follows, we present a brief description of these axioms in the general setting of comparing infinite utility streams, ====.====Strong Equity compares two infinite utility streams (==== and ====) in which all generations except two have the same utility levels in both utility streams. Regarding the two remaining generations (say, ==== and ====), one of the generations (say ====) is better off in utility stream ====, and the other generation (====) is better off in utility stream ====, thereby leading to a situation of a conflict. Strong equity requires that if for both utility streams, it is generation ==== which is worse off than generation ====, then generation ==== should choose (on behalf of the society) between ==== and ====. Thus, utility stream ==== is socially preferred to ====, since generation ==== is better off in ==== than in ====. In essence, strong equity axiom yields strict preference for inequality reducing transfers which do not alter the ranking of the rich and the poor generations involved in the transfer. Strong equity was introduced by d’Aspremont and Gevers (1977), who referred to it as an “extremist equity axiom”. The term “strong equity” has been used relative to the axiom introduced by Hammond (1976), which he called the “equity axiom”. Hammond explains his axiom to be in the spirit of the “weak equity axiom” of Sen (1973). His axiom is now referred to as the Hammond equity principle. In the setting of strong equity ranking, Hammond equity yields weak preference of ==== compared to ====.====Pigou–Dalton transfer principle insists on exact transfer of the utility (welfare) from the rich generation to the worse off generation (====) for the desired social preference in the redistribution scheme described above for strong equity. This inequality reducing property was initially hinted at by Pigou (1912, p. 24) as “The Principle of Transfers”. Dalton (1920, p. 351) described it as “If there are only two income-receivers and a transfer of income takes place from the richer to the poorer, inequality is diminished.”. In words, Pigou–Dalton transfer principle shows preference for non-leaky and non-rank-switching transfers. This equity concept has been widely studied in literature on economic inequality and philosophy (see Adler, 2013 for a discussion on philosophical foundation).====The literature on inter-generational equity deals with improving the welfare of current and future generations extending over an infinite time horizon. For this, we need to evaluate infinite utility streams consistently with social preferences which respect the desirable class of equity axiom(s). In what follows, we will refer to the social preferences satisfying equity axiom(s) as ethical or equitable preferences. In the case of procedural equity notion of anonymity, it is easy to construct an ethical social preference. A social preference relation which evaluates all infinite utility streams as indifferent satisfies the anonymity axiom (in addition to Hammond equity) trivially. However such a preference relation is of limited practical significance as it does not distinguish between any pair of utility sequences. Since anonymity and Hammond equity axioms fail to generate any strict preference, additional conditions in the nature of sensitivity (Pareto)==== and/or continuity axioms are imposed to obtain actionable social preferences. In contrast, social preference relations satisfying strong equity or Pigou–Dalton transfer principle enable strict ranking of pairs of utility sequences. Therefore, properties of such preferences without any additional sensitivity (Pareto) or continuity axioms are worthy of investigation. Our goal is to propose a general framework for this study and report results of our inquiry. We begin with some more historical background and a brief overview of the recent literature on the inter-generational equity and discuss in detail the scope and position of our contribution within the established literature.",Equitable preference relations on infinite utility streams,https://www.sciencedirect.com/science/article/pii/S0304406821001051,7 July 2021,2021,Research Article,88.0
"Aouani Zaier,Chateauneuf Alain,Ventura Caroline","Independent Researcher,IPAG Business School and Paris School of Economics, Université de Paris 1, France,Université de Paris 1, France","Received 23 October 2020, Revised 25 June 2021, Accepted 27 June 2021, Available online 6 July 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102543,Cited by (0),"We study ambiguity aversion by introducing some new notions of propensity for hedging that are less general than convexity of preferences. We therefore characterize the corresponding properties of the capacity and Choquet functional, and link them with actual observed behaviors under uncertainty (Fox et al., 1996; Tversky and Wakker, 1995).","The classical interpretation of convexity of preferences in terms of negative attitude towards the presence of ambiguity dates back to Debreu (1959). It turns out that in the Choquet Expected Utility (CEU) model, introduced by Schmeidler (1989), convexity of preferences is nicely characterized by a convex capacity or, equivalently, a superadditive Choquet functional. This straightforwardly remains true in the simplified version of Schmeidler’s model envisioned in this paper, also referred to as CEU, where the outcome space is the set of real numbers, ====, considered as the space of monetary payments, and where we assume that the decision maker (DM) displays a constant marginal utility of wealth, i.e., ==== for all ====. The results of this paper can readily be extended to the popular Anscombe–Aumann model, where mixtures are considered instead of sums. As we are interested in behavioral properties that are reflected solely through the properties of the capacity, we adopt the simplified model.====Ambiguity aversion stipulates, roughly speaking, that mixing any two indifferent ambiguous acts ==== and ==== reduces ambiguity and therefore allows to obtain a new act that is, from the perspective of the ambiguity averse DM, at least as desirable as ==== or ====. This means that any act may be a hedge against any other act, implying that ambiguity aversion cannot distinguish between separate attitudes towards hedging.====In fact, ambiguity aversion encompasses a wide spectrum of attitudes towards hedging, and this paper disentangles a variety of them (====, ====, and ====) by focusing on the apparently pertinent attraction to hedging effects that result from the addition of ==== acts, i.e., acts which vary in opposite directions. The choice of anticomonotonic acts as hedging instruments is only natural for two reasons. First, in the extreme case of ==== where mixing two acts leads to a constant act, those two acts are in fact anticomonotonic, and second, adding ==== acts, i.e., acts which vary in the same direction, does not reduce variability hence, as emphasized by Schmeidler, two comonotonic acts cannot be considered hedges against each other.====This paper shows that mere inclination to hedging by anticomonotonic acts, dubbed propensity for hedging, is characterized by a ==== capacity, or, equivalently, an anticomonotonic superadditive Choquet functional. In addition to superadditivity, pseudo-convex capacities maintain convexity at the sure event (Tversky and Wakker, 1995, Fox et al., 1996).====Propensity for minimum hedging, reflecting tendency to prefer hedges against the worst (minimum) outcome, is defined in a similar way to propensity for hedging, except that hedging instruments are further restricted to be ====, that is, only improvement to only the lowest outcome of the act to be hedged. Of course, a minimum-hedge for an act is anticomonotonic with the act. We show that propensity for minimum hedging is characterized by a superadditive capacity, or, equivalently, a minimum-hedge superadditive Choquet functional.====Finally, ====, a notion introduced under risk by Cheung et al. (2014), are used as hedging instruments to define propensity for proper hedging in a similar way to propensity for hedging. Proper hedges of an act are anticomonotonic with the act. We show that propensity for proper hedging is characterized by a capacity that is ====, or, equivalently, a proper-hedge superadditive Choquet functional. We, furthermore, show that propensity for proper hedging coincides with preference for ====; a relaxed version of preference for sure diversification as defined in Chateauneuf and Tallon (2002) where now only two equivalent acts are used.====The paper is organized as follows. Section 2 presents the decision theoretic setup. Section 3 is concerned with the different notions of propensity for hedging. We first (Section 3.1) define pseudo-convex capacities and show (Theorem 1) that they characterize CEU preferences exhibiting propensity for hedging. An application to the elicitation of a lower bound for the reservation price of a European put option (see e.g. Hull, 2018) is provided. Clearly the payoff of a European put option is anticomonotonic with the payoff of the stock position. Hedging a stock position by purchasing put options on the stock is a very common hedging strategy.====In Section 3.2, we propose a new notion of ambiguity aversion, ==== (aversion to the lowest outcome of a distribution), and compare it to the notion of ====, which has been proved by Chateauneuf (1991) to characterize (under standard axioms) CEU preferences with a convex capacity, i.e., to characterize convexity of preferences. Proposition 3 provides a direct proof of this characterization. The new notion of propensity for minimum hedging is introduced and, for CEU preferences, is shown to be characterized by superadditive capacities.====In Section 3.3, we emphasize that pseudo-convex capacities, being convex at the sure event, exhibit attraction for certainty; a property that is supported by empirical evidence for options traders (see Fox et al., 1996) and, along the way, confirmed by practitioners in the field (see Tversky and Wakker, 1995, Gonzalez and Wu, 1996, and Wakker, 2001).====Section 3.4 shows that proper hedges are particularly relevant under ambiguity (they are, for instance, ambiguity reducers; see Proposition 12). We use proper hedges as hedging instruments to define propensity for proper hedging. For CEU preferences, propensity for proper hedging is shown to be characterized by a capacity that is superadditive at the sure event; a property usually seen as a no-arbitrage condition and therefore considered a natural requirement. Concluding remarks are in Section 4 and proofs are gathered in Section 5.",Propensity for hedging and ambiguity aversion,https://www.sciencedirect.com/science/article/pii/S0304406821001063,6 July 2021,2021,Research Article,89.0
"Gürtler Marc,Koch Florian","Braunschweig Institute of Technology, Department of Finance, Abt-Jerusalem-Str. 7, 38106 Braunschweig, Germany","Received 17 September 2020, Revised 27 May 2021, Accepted 18 June 2021, Available online 1 July 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102541,Cited by (0),We analyze the alignment of incentives between an originator and investors in a ,"Securitization is an important mechanism to facilitate the funding of consumers and businesses. It entails that originators==== of illiquid financial assets, like auto or mortgage loans, issue “asset-backed” securities (ABS) repayable from collections of cash on the assets. Further, the claims on the securitized cash flow are partitioned into so-called ABS “tranches” that are rank-ordered by seniority (Boot and Thakor, 1993), resulting in AAA-rated debt securities as well as subordinate equity-like tranches. According to the basic idea of securitization, the ABS are bought by third-party investors while the originator keeps performing tasks to maintain the assets’ quality, like the monitoring of borrowers. Market participants and regulators consider securitization as an important element of well-functioning financial markets. In 2019, the issuance volume of ABS (including mortgage securitization) in the U.S. was $2.4 trillion, just below the issuance of U.S. treasuries ($2.9 trillion).==== However, securitization induces a moral hazard problem, because the investors typically cannot verify the originator’s level of effort after the issuance of ABS.====The importance of the moral hazard problem in securitization markets is empirically confirmed, as loans are shown to perform systematically worse after being securitized (Kamstra et al., 2014, Kara et al., 2019). Addressing monitoring tasks more specifically, Piskorski et al. (2010) and Agarwal et al. (2011) provide evidence of biased mortgage foreclosure decisions after securitization. Regarding securitized corporate loans, Wang and Xia (2014) provide similar evidence of biased (i.e., omitted) responses to covenant violations. To mitigate the moral hazard problem, the originator can retain “skin in the game” by retaining a part of the securitized cash flow. In practice, there is a broad range of retention instruments, which can be divided into horizontal and vertical instruments. In vertical retention, the originator receives a fixed proportion of every repayment (e.g., by retaining a fixed proportion of all ABS tranches); in horizontal (or “equity”) retention, the originator does not participate in the repayments up to a certain threshold but receives all repayments beyond that threshold (e.g., by retaining only subordinate ABS tranches).====A retention’s capacity to incentivize monitoring effort is determined by how sensitive the realization (probability) of retained claims reacts to the originator’s level of effort.==== It is commonly argued that subordinate “equity” tranches are more sensitive to effort than are AAA-rated “senior” tranches because the latter receive full repayment almost surely even if low effort is exerted. Consistent with this argument, existing studies dealing with the alignment of incentives in securitization markets (implicitly) assume a ==== relationship between subordination level and effort sensitivity (Chemla and Hennessy, 2014, Vanasco, 2017). In contrast, we consider that some middle tranches are even more responsive to effort than equity tranches. Maturana (2017) suggests that costly loan modifications are especially useful when applied to larger loans of better borrowers, who default primarily during difficult economic times. Simply put, in stable economic times, effort has a small effect on equity tranches while middle tranches succeed regardless, whereas in harsh economic times, effort has a major impact on middle tranches while equity tranches fail regardless. As will be shown in greater detail, these mechanisms result in a ==== concave relationship between subordination level and effort sensitivity. A model allowing for securitized cash flows with this property is needed to determine the optimal retention and to evaluate whether retention should be regulated. Such an evaluation has to include three central steps: (i) understanding equilibrium without retention regulation, (ii) analyzing the welfare losses in the different types of equilibria, and (iii) deriving regulatory measures that tackle the welfare losses efficiently.====To address these steps, we develop a specific principal–agent model in which an asset manager (i.e., the originator) sells the future cash flow of an asset to outside investors but maintains unobservable tasks that affect cash flow realization. For the alignment of incentives, we focus in the main body of the paper on the two practicable dimensions of cash flow retention in securitization markets (i.e., horizontal and vertical retention), and arbitrary combinations thereof.==== To allow for non-trivial relationships between subordination level and effort sensitivity, we extend the basic cash flow model considered, for example, in Vanasco (2017) and Chemla and Hennessy (2014)—which consists of one non-risky (and hence effort-insensitive) layer and one effort-sensitive layer—with a second effort-sensitive layer. We particularly allow the middle layer to be the most sensitive layer, resulting in a non-monotonic “effort sensitivity (layer) structure”.====The agency problem in our model is primarily based on the fact that the investors cannot verify the true level of effort. What the investors can do is translating the observed multidimensional retention into a one-dimensional measure of incentives, based on the concept of effort sensitivity structure discussed in this article. However, measuring the originator’s incentives is not necessarily sufficient for anticipating his or her true level of effort. Rather, different originators might exert heterogeneous effort under the same retention, provided they differ in terms of their cost of effort. We analyze both cases, hidden action after contracting accompanied by either observable or hidden cost of effort.====The model disentangles the commitment and signaling functions of cash flow retention, predicting different types of equilibrium. Further, a standard equilibrium selection procedure brings out a unique equilibrium. Provided the net social value of monitoring is sufficiently high or low for all originators, a pooling equilibrium is predicted under which all originators jointly have incentives either so strong or weak that their commitment to effort is clear without knowing their cost of effort. Otherwise, a separating equilibrium similar to common signaling games like in Spence (1973) is predicted.====While securitization induces the moral-hazard problem, it offers two major welfare benefits. First, outside investors can insure the originator against economic risk, which constitutes a benefit of risk transfer (Guo and Wu, 2014, Malekan and Dionne, 2014, Dionne and Malekan, 2017), resulting particularly in capital charge reduction (Kiff and Kisser, 2014). Second, originators can liquidate assets to use private investment opportunities, which constitutes a benefit of refunding (Pagès, 2013, Chemla and Hennessy, 2014, Kawai, 2014, Vanasco, 2017). These two benefits are not equivalent because selling ABS tranches, with different seniority, at the same price typically entails not the same capital charge reduction. However, no study relevant to this issue distinguishes the two benefits, a further innovation of our study. We measure risk transfer based on a spectral risk measure (Acerbi, 2002), while refunding is determined by the price competitive investors pay for an ABS tranche.====The choice between horizontal and vertical retention is basically driven by a trade-off between refunding and risk transfer. Our model is the first to capture this trade-off combined with the trade-off between selling claims and aligning incentives. Consistent with Kiff and Kisser (2014), we find that horizontal retention is disadvantageous in the presence of tight capital requirements. In addition, we show that a strong need for refunding, compared to risk transfer (i.e., capital charge reduction), is an argument that supports horizontal retention. A further novel result is that originators who focus on one of the two benefits do not apply the same retention dimension to all cash flows. Regarding originators who focus on refunding, we extend the results of Chemla and Hennessy (2014) and Vanasco (2017). Considering only monotonic effort sensitivity structures, the authors found that horizontal retention is always optimal, whereas we show that rather vertical retention can be optimal if the effort sensitivity structure is non-monotonic.====Some important results relate to the originator’s incentive level, which measures the sensitivity of the originator’s preference value to increases in effort. On one hand, given the originator’s incentive level, we show that vertical retention is rather applied to cash flows of lower quality (e.g., pools of non-performing loans) than to high-quality cash flows. On the other hand, the model predicts that horizontal retention prevails in equilibria with high incentive levels, whereas vertical retention arises rather in equilibria with lower incentive levels. Similar to Vanasco (2017), we find that equilibrium incentive (and effort) levels are increased by factors that improve the cost-incentive ratio of retention, such as low profitability of reinvestment (or “gains from trade”) and high cash flow sensitivity to effort. In addition, we show that equilibrium incentive and effort levels decrease with cash flow quality. Some of the novel insights indicate new ways to regulate retention efficiently.====While post-crisis regulation usually mandates the (minimum) amount of retention, originators may choose its dimension. In the U.S., originators may also combine the two dimensions (in what is called “L-shape” retention). However, we show that L-shape retention is dominated for all cash flows with monotonic or concave effort sensitivity structure. Moreover, our results indicate ways how regulators can promote different types of equilibrium. Aiming at high originator effort, regulators can mandate skin in the game. We propose a form of skin-in-the-game requirements that addresses the incentives provided by multidimensional retention schemes adequately, by incorporating the cash flow’s effort sensitivity structure. We argue that the existing regulation design can lead to a costly distortion in the choice of retention dimensions, without affecting the agency problem as intended. Furthermore, we show that regulators who aim at higher transfer of economic risk can rule out certain dimensions of retention, for example, to promote separation of originator types, under which considerably more economic risk is transferred to investors than in an equilibrium under which all originators pool at high incentive levels.====This study contributes to the analysis of the optimal design of cash flow retention in securitization markets (e.g., Guo and Wu, 2014, Kiff and Kisser, 2014, Malekan and Dionne, 2014, Dionne and Malekan, 2017, Pagès, 2013, Chemla and Hennessy, 2014, Vanasco, 2017). In this regard, our main contribution is to reveal two important determinants of the optimal dimension of retention: the effort sensitivity structure of the cash flow and the incentive level of the originator. In addition, we contribute to the discussion on how to regulate securitization markets (e.g., Guo and Wu, 2014, Kiff and Kisser, 2014, Dionne and Malekan, 2017, Vanasco, 2017), particularly by highlighting new approaches to the design of skin-in-the-game requirements. Moreover, the model can be applied to other agency problems, including employment and financing relationships (Sung, 2005, Kumar and Sivaramakrishnan, 2008) as well as (re-)insurance relationships (Stewart, 1994, Fuller, 2014).====The remainder of this article is organized as follows. Section 2 introduces the study’s model. Section 3 analyzes equilibrium retention design and effort. Section 4 discusses the model’s application to securitization markets and its implications for regulation. Finally, Section 5 concludes the article. All proofs are presented in the Appendix.",Multidimensional skin in the game,https://www.sciencedirect.com/science/article/pii/S030440682100104X,1 July 2021,2021,Research Article,90.0
Wong Kit Pong,"University of Hong Kong, Hong Kong","Received 11 January 2021, Revised 19 April 2021, Accepted 27 May 2021, Available online 11 June 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102536,Cited by (2),"This paper characterizes aversion to one risk in the presence of another, which is invulnerable to the size of exposure to the former risk and consistent with the common ==== of the underlying random variables is governed by the notion of expectation dependence.","Decisions are often multidimensional, as decision makers usually face multiple risks and their preferences depend on multiple attributes. Within the framework of expected utility theory in the univariate context, Arrow (1965) and Pratt (1964) define risk aversion as the preferences for receiving the expected outcome of a risk with certainty rather than the risk itself, which leads to the concavity of the utility function.==== However, such an attitude towards a risk may change if another risk is present in general and the two risks are dependent in particular (Pratt, 1988). This makes the characterization of risk aversion with two risks deserve a close scrutiny.====Using a bivariate utility function, Finkelshtain et al. (1999) show that aversion to one risk in the presence of another prevails when the two risks are positively regression dependent in the sense of Lehmann (1966) if, and only if, the bivariate utility function is risk averse to the former risk in isolation and correlation averse (Richard, 1975, Eeckhoudt et al., 2007).====
 Li et al. (2016) extend the results of Finkelshtain et al. (1999) and show that the dependence structure of the two risks can be further weakened by using the notion of expectation dependence (Wright, 1987).==== When higher-degree (i.e., weaker) expectation dependence (Li, 2011) is used, higher-order bivariate risk apportionment (Eeckhoudt et al., 2007, Jokung, 2011) that goes beyond correlation aversion is called for to preserve the aversion to one risk in the presence of another.====The purpose of this paper is twofold. First, we show that the definition of aversion to one risk in the presence of another as proposed by Finkelshtain et al. (1999) is likely to be incompatible with the betweenness property of expected utility theory, particularly when the two risks are negatively correlated (Denuit and Mesfioui, 2017). Such a violation of the betweenness property has an immediate implication that a full-hedge against a risk by means of an actuarially fair insurance scheme is not necessarily optimal when a correlated background risk prevails. To ensure the validity of the betweenness property in the bivariate context, we propose an alternative, yet stronger, definition of aversion to one risk in the presence of another, which is invulnerable to the size of exposure to the former risk. We show that all bivariate utility functions that satisfy bivariate risk apportionment as considered by Li et al. (2016) exhibit aversion to one risk in the presence of another for all sizes of exposure to the former risk if, and only if, the dependence structure of the two risks is characterized by the notion of expectation dependence (Wright, 1987, Li, 2011). These results are stronger than those of Li et al. (2016) at the expense of a more stringent definition of aversion to one risk in the presence of another that requires the betweenness property to hold in the bivariate context.====Second, based on the characterization of aversion to one risk in the presence of another irrespective of the size of exposure to the former risk, we propose an intensity measure of risk aversion with two risks so that interpersonal comparisons can be made. We follow Crainich et al. (2020) to extend the univariate intensity measure introduced by Liu and Meyer (2013) to the bivariate context. Specifically, we use the utility premium, which is the reduction in expected utility caused by an introduction of one risk in the presence of another, normalized by the marginal utility evaluated at an arbitrarily chosen pair as our intensity measure.==== The normalization ensures that the intensity measure is invariant to any positive affine transformations of the bivariate utility function and has the same unit as the risk in question. We show that the intensity measure of one bivariate utility function is uniformly larger than that of another bivariate utility function if, and only if, the former utility function is more risk averse than the latter utility function in the generalized sense of Ross (1981). These results provide a theoretical justification of using the concept of generalized Ross risk aversion when conducting comparative statics with respect to interpersonal comparisons of risk attitudes towards any two risks whose dependence structure is governed by the notion of expectation dependence.====In a recent article, Gollier (2021) develops a general theory of risk apportionment. He shows that increases in the statistical concordance between the higher degree riskiness of one random variable and that of another raise the higher degree riskiness of their joint distribution, thereby making decision makers who display bivariate risk apportionment dislike such changes. In this paper, we follow the tradition of Arrow (1965) and Pratt (1964) to consider the preferences for receiving the expected outcome of a risk with certainty rather than the risk itself, albeit in the presence of another risk. In this sense, our results complement those of Gollier (2021).====The rest of the paper is organized as follows. Section 2 introduces the notion of expectation dependence (Wright, 1987, Li, 2011) and the concept of bivariate risk apportionment (Eeckhoudt et al., 2007, Jokung, 2011). Section 3 proposes an alternative, yet stronger, definition of aversion to one risk in the presence of another that requires the betweenness property to hold in the bivariate context. Section 4 proposes an intensity measure of risk aversion with two risks that allows for interpersonal comparisons. Section 5 offers an application for optimal prevention in a two-period model. Section 6 concludes.",Comparative risk aversion with two risks,https://www.sciencedirect.com/science/article/pii/S0304406821000999,11 June 2021,2021,Research Article,91.0
"McCarthy David,Mikkola Kalle,Thomas Teruji","Department of Philosophy, University of Hong Kong, Hong Kong,Department of Mathematics and Systems Analysis, Aalto University, Finland,Global Priorities Institute, University of Oxford, United Kingdom","Received 20 June 2017, Revised 28 May 2021, Accepted 1 June 2021, Available online 9 June 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102538,Cited by (1),A mixture preorder is a preorder on a mixture space (such as a convex set) that is compatible with the mixing operation. In decision ,"The importance of allowing for incomplete preferences is by now beyond dispute. In the context of expected utility, von Neumann and Morgenstern (1953, p. 630) themselves remarked, of the completeness axiom, that it is “very dubious, whether the idealization of reality which treats this postulate as a valid one, is appropriate or even convenient”. In the first systematic treatment of expected utility without the completeness axiom, Aumann (1962, p. 446) wrote that while all the expected utility axioms are descriptively implausible, the completeness axiom alone is “hard to accept even from the normative viewpoint”. With normative questions especially in mind, we address the problem of representing incomplete preferences by sets of utility functions.====Following Aumann (1962) and Shapley and Baucells (1998), we suppose that preferences are given by a preorder on a mixture space, in the sense of Hausner (1954). A mixture space is a set ==== together with a mixing operation, so that for any elements ==== and ==== in ==== and ====, the element ==== of ==== is understood to be a mixture of ==== and ==== in which ==== is given weight ==== and ==== weight ====. We give the standard axiomatization of mixture spaces in Section 2. For now, the best known example involving uncertainty is when ==== is the set of probability measures on some outcome space, and ==== is taken to be the probability measure ====. More generally, any convex set, and thus any vector space, is a mixture space, with the mixing operation defined by the same formula.====Given a possibly incomplete preorder ==== on mixture space ====, a ==== is a nonempty set ==== of functions ==== such that ==== if and only if, for all ====, ====.====It is natural to require that the functions ==== respect the mixing operation. A function ==== between mixture spaces is ==== when ====. In a multi-representation, as we have defined it, ==== is the vector space of real numbers. So the question we consider is under what conditions a preorder ==== on ==== has a ====; that is, under what conditions does it satisfy====It is well known that ==== mixture space is isomorphic to a convex set. Using this fact, our question is mathematically equivalent to the question of when a preorder on a convex set has a multi-representation consisting of affine (or even linear) functionals on the ambient vector space, restricted to the convex set. We will exploit this equivalence in proofs (see Section 4.1), but we follow Mongin (2001) in thinking that mixture spaces are conceptually more fundamental for decision theory. For example, it is often easier to verify that an algebraic structure of interest to decision theorists is a mixture space than to show directly that it is isomorphic to a convex set.====Much of the literature on mixture-preserving multi-representations has focused on specific types of mixture spaces. Besides sets of probability measures (with different possible assumptions about the underlying measurable space), examples include sets of Savage-acts, at least given mild structural assumptions (Ghirardato et al., 2003); Anscombe–Aumann acts; charges (i.e. finitely additive measures); and vector-valued measures representing imprecise probabilities. Mixture-preserving multi-representations themselves come in a variety of forms. In the popular Anscombe–Aumann setting, for example, incomplete preferences may be a matter of incomplete beliefs, incomplete tastes, or both, and multi-representations can reflect these distinctions.====While one could consider these different frameworks one at a time, taking into account their special features, we think it is interesting to consider the unifying question of when one may obtain a mixture-preserving multi-representation of a preorder on an abstract mixture space. This fits with the appealing methodology of assuming as little mathematical structure as possible, and addressing general questions with general tools.====To introduce our main results, let us mention two axioms that must clearly be satisfied for MR to hold, i.e. for the existence of a mixture-preserving multi-representation.==== First, the preorder must be what we call a ‘mixture preorder’: it must satisfy what is arguably the central axiom of expected utility theory, strong independence. Strong independence is not in general a natural assumption for preferences on mixture spaces; few people’s preferences satisfy it on the simplex whose points denote different proportions of coffee, milk, and sugar. But it is a plausible normative requirement in the examples of mixture spaces introduced above, which all involve uncertainty. Second, it is not hard to show that if a mixture preorder has a mixture-preserving multi-representation, it must satisfy the mixture continuity axiom of Herstein and Milnor (1953).====The result which sets the stage for our discussion, Theorem 2.1, shows, we think rather surprisingly, that mixture continuity is not sufficient for a mixture preorder to have a mixture-preserving multi-representation. However, mixture preorders that satisfy mixture continuity without having a mixture-preserving multi-representation must be rather complicated; for example, Theorem 2.3 shows that they must have uncountably infinite dimension. This raises the question of whether there are normatively natural ways of strengthening or supplementing mixture continuity that do guarantee MR.====Our strongest positive result, Theorem 2.4, shows that, in combination with mixture continuity, an axiom we call ‘countable domination’ is sufficient for a mixture preorder to satisfy MR. We provide two interpretations of this axiom. First, it is a member of a natural but apparently novel family of decision-theoretic axioms that constrain what we call the ‘Archimedean structure’ of the preorder. Another axiom in this family is the standard Archimedean axiom, which is much stronger than countable domination. Second, countable domination may be seen as a dimensional restriction on mixture preorders that is much less demanding than the requirement of countable dimension.====Our strongest negative result, Theorem 2.5, considers what happens if we impose a topology on mixture spaces and upgrade mixture continuity to a stronger continuity condition. It notes that any mixture preorder that satisfies MR must be both continuous and closed in the weak topology, understood as the coarsest topology on the mixture space in which the real-valued mixture-preserving functions are continuous. However, more surprisingly, it also shows that being continuous is not sufficient for MR. We leave it as an open question whether being closed is sufficient.====Section 2 states our axioms more formally and presents our main results. Section 2.1 relates them to the most immediately relevant literature, showing how they extend results of Shapley and Baucells (1998) and answer a question posed by Dubra et al. (2004). Section 3 discusses the interpretation of countable domination. Section 4 provides proofs of our main results; it emphasizes the central ideas, appealing to a series of auxiliary results whose proofs we defer to Appendix C. Section 5 refines our results by considering two topics. Section 5.1 presents results concerning the existence of mixture-preserving multi-representations that consist entirely of strictly increasing functions, and relates them (in Section 5.1.1) to results by Aumann, 1962, Kannai, 1963, Dubra et al., 2004, Evren, 2014 and Gorno (2017). Section 5.2 presents a uniqueness result for mixture-preserving multi-representations that is an abstract version of the uniqueness result of Dubra et al. (2004). Appendix A explains the connection between our axioms and other independence and continuity axioms common in the literature. Appendix B provides a geometrical interpretation of our discussion of Archimedean structures. And, as we mentioned, Appendix C contains proofs of the auxiliary results.====It might seem that the problem we are addressing was settled by Shapley and Baucells (1998, Theorem 1.8). In a result that is still quoted, they claimed that a condition they called ‘properness’ is necessary and sufficient for a mixture-continuous mixture preorder to have a mixture-preserving multi-representation. We discuss properness further in Section 2.1, but, notwithstanding the importance of their contribution, the necessity part of this claim is mistaken when the mixture space has infinite dimension.====Finally, we acknowledge the centrality to our results of the work of Klee (1953).",Expected utility theory on mixture spaces without the completeness axiom,https://www.sciencedirect.com/science/article/pii/S0304406821001014,9 June 2021,2021,Research Article,92.0
Yukalov V.I.,"Bogolubov Laboratory of Theoretical Physics, JINR, Dubna 141980, Russia","Received 22 November 2020, Revised 24 May 2021, Accepted 26 May 2021, Available online 8 June 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102537,Cited by (5),"The St. Petersburg paradox is the oldest paradox in decision theory and has played a pivotal role in the introduction of increasing concave utility functions embodying ==== and decreasing marginal utility of gains. All attempts to resolve it have considered some variants of the original set-up, but the original paradox has remained unresolved, while the proposed variants have introduced new complications and problems. Here a rigorous mathematical resolution of the St. Petersburg paradox is suggested based on a probabilistic approach to decision theory.","The famous St. Petersburg paradox is, probably, the oldest paradox in decision theory, and can be said to have promoted the birth of modern decision theory itself. It was invented by Nicolas Bernoulli in 1713 and discussed in his private letters. The paradox was formally stated by his cousin Bernoulli (1738), who worked in St. Petersburg. The paradox and the suggested solution were published in the Proceedings of the Imperial Academy of Sciences of St. Petersburg (Bernoulli, 1738).====In economics, the St. Petersburg paradox has played a particularly important role in pointing out situations in which supposedly rational decisions based on expected gains or even expected increasing utilities are not endorsed by real rational human decision makers. The St. Petersburg paradox has opened a flood of attempts to solve it, which turn out all to modify it in one way or another. The most important change involves the introduction of a concave utility function which, in the words of Cramer (1728), captures the concept that “men of good sense estimate money in proportion to the usage that they may make of it” (and not necessarily in proportion to its quantity). Motivated by the St. Petersburg paradox, the introduction of concave utility functions, which embody risk aversion and decreasing marginal utility of gains, remains the central pillar of modern economic theory.====However, it turns out that the solution in terms of utility functions of Cramer (1728) and Bernoulli (1738) is not completely satisfactory since, as is stressed below, a slight change in the formulation makes the paradox reappear. This problem and the many proposed solutions have been revisited recently by Clark (2002) and by Martin (2008). Thus, Clark states “This seems to be one of those paradoxes we have to swallow”. Or, as Martin concludes “The St. Petersburg result is strange... The appropriate reaction might just be to try to accept the strange result”. As the suggested solutions all modify the initial problem, it is important to emphasize that the original paradox has remained unresolved.====Here, we propose a resolution of the St. Petersburg paradox based on the introduction of stochasticity of the decision making problem to address the paradox. Stochasticity is argued to be the natural intrinsic feature of human behavior.====To be precise, first of all, in Section 2, we identify the origin of the St. Petersburg paradox. In Section 3, we briefly summarize the most known attempts to avoid it by changing the original formulation and explain why such attempts do not resolve the paradox in its general form. In Section 4, we briefly mention the models used in stochastic decision making and discuss the justification of stochastic approach. Then, in Section 5, we formulate the probabilistic approach to decision theory to be used and in Section 6 show how the paradox becomes resolved in the frame of this approach. In the probabilistic approach there exists a belief (disbelief) parameter whose estimation for the case of the St. Petersburg paradox is considered in Section 7. The role of the trial prior probability is discussed in Section 8. The existence of repeated Bernoulli games is mentioned in Section 9, and in Section 10, the so-called inverse St. Petersburg paradox is investigated. Section 11 concludes the paper.",A resolution of St. Petersburg paradox,https://www.sciencedirect.com/science/article/pii/S0304406821001002,8 June 2021,2021,Research Article,93.0
Liao Xiaoye,"NYU Shanghai, 1555 Century Ave, Pudong District Room 1131G Shanghai 200122, China","Received 23 December 2019, Revised 22 April 2021, Accepted 5 May 2021, Available online 19 May 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102534,Cited by (0)," is to immediately reveal the truth, which is true for a large set of primitives. We construct the designer’s maximum payoff and find a discontinuous drop in it as compared with the standard model. Unlike in many standard persuasion models, the designer is not able to appropriate all the rents of information disclosure while the receiver often achieves the highest possible benefit from being able to repeatedly sample from the strategically offered information structure.","The theory of Bayesian persuasion provides a formal framework to explore how information is strategically transmitted when senders can commit to communication rules ====, which has found applications in various scenarios such as voting, competition between firms, and information design in organizations. Most existing works in this field have assumed that information disclosure is static, that is, each receiver passively observes a signal realized from the information structure specified by the sender.==== Quite natural in many cases, there are examples that are not sufficiently captured by this paradigm. In particular, receivers may not be passive audiences but have a say through the use of the information sources offered by senders. This paper studies a specific setup of this type and derives new insights into similar examples.====To fix idea, consider, among others, the following concrete example. A group of researchers are in charge of designing clinical trials for a new drug, which they want to get approved by the FDA. Typically, the clinical study is designed at the beginning and is adhered to throughout to guarantee the consistency of data that will be generated. As pointed out in Carpenter (2002), the FDA is faced with an ====. Information about the quality of a drug is accumulated over time rather than acquired in a lump-sum way, but the FDA cannot always wait to collect more information since its time preference renders the delay costly.==== The FDA thus decides how much evidence will be enough for an approval to strike a balance. Given the FDA’s tradeoff, the developers, whose preferences are conceivably misaligned with the FDA, may play strategically in the designing stage to win a better chance of getting the drug marketed, which is beyond the FDA’s direct control.==== How will the developers maximize their private objective? To what extent will this lead to an inefficiency in information transmission? Will this cause a serious hazard to the public safety?====To answer questions alike in scenarios with similar economics, we capture the communication problem using a Bayesian persuasion game with one substantive difference from its paradigm. Specifically, the receiver (he) faces a binary decision problem with incomplete information about a constant state. Uninformed about the state ====, the designer (she) cares about the receiver’s action and can, like in standard Bayesian persuasion models, influence it by communicating with him via a committed statistical experiment (i.e., an information structure). Importantly, the receiver can acquire, instead of a single observation, (conditionally) i.i.d. signals from the information structure. Information acquisition is time-consuming and necessarily leads to a delay of the receiver’s (final) decision, which is costly since he discounts future payoffs at a constant rate. Weighing the information gains and learning costs, the receiver chooses an optimal time to conclude his learning and make his final decision (e.g., approve or not in the leading example). The designer, who is fully patient, selects the format of her persuasion so as to maximize the probability that the receiver eventually chooses her preferred action (like “approval” in the leading example).====While the most natural setup for information acquisition is a discrete-time model with the receiver observing at each integer time an independent signal from the same information structure, its tractability poses a big challenge. Following a standard treatment in the literature,==== we instead consider a continuous-time model, wherein information about the constant state is transmitted via a diffusion process, called a ====, unless the designer chooses to disclose the true state immediately or to offer no information. It bears the interpretation that the observation at each moment with a manipulated information structure is an i.i.d. signal generated from a normal distribution with an unknown mean and a known variance, the mean featuring the true state. The receiver thus sees a sample path of the random process, which can be thought of as the set of signals observed up to each point in time. Under this setup, it is without loss of generality to consider the problem of information design as the choice of ====, which is the variance of the diffusion process, or alternatively, the “variance” of the normal information structure at each moment of time.====Following the belief-based approach, we then represent communication by its induced distribution of ====—those at which the receiver will terminate learning and make his final decision. The continuous-time setup admits a closed-form optimal learning policy, which maps each information structure (as characterized by its accuracy) to a pair of stopping posteriors, called an ====. The range of the admissible pairs as accuracy varies constitutes the feasible set and thus represents information in our model; the stopping posteriors in an admissible pair are, moreover, monotonically related to each other, and so the information design problem is essentially reduced to the choice of a single stopping posterior. Although the distribution over each admissible pair necessarily averages back to the prior, not all such distributions satisfy the receiver’s sequential rationality in information acquisition. A proper refinement to Bayes plausibility is thus introduced by the receiver’s optimal learning, for which the information representation is termed ====.====The structure of constrained Bayes plausibility delivers a convenient geometric characterization of the optimal information policy. Say that a prior is ==== if the designer does not favor the receiver’s optimal (terminal) action at the prior. Provided a disadvantageous prior, Proposition 1 pins down a closed-form cutoff prior====; below, the receiver features ==== and the designer can do no better than being immediately revealing, while above, it is optimal to provide a manipulated information structure that motivates the receiver to sample sequentially and induces delay. The result disproves that the designer can, as intuition might suggest, counteract “too much information (via multiple signals)” by lowering information accuracy. In fact, since learning is costly, the receiver, facing a less accurate information structure, will choose to explore ==== in general. However, owing to the receiver’s incentive compatibility, the designer cannot control how the “==== exploration” is allocated on various events, leaving the profitability of lowering the accuracy unclear. On the contrary, the immediately revealing policy is optimal more often than one might have expected, in which the receiver is immediately informed about the underlying state. Thus, what makes the designer more informed in these cases is the option of, rather than actually carrying out, sequential tests. The geometric characterization also allows us to fully identify the optimal communication rule when a ceiling of information accuracy is present. Again, it turns out that the designer will usually find it optimal to be as accurate as possible. On the application side, the result, in a sense, seems to suggest that it is usually not a big problem for the FDA to allow for “wide latitude in clinical trial design” since the strategic play of designers will not cause twists of design. Thus, in the leading example, strict monitoring over the process of clinical trials might be more important to the FDA.====Some comparative statics are facilitated by the complete characterization. For the receiver, we investigate how the quality of information responds to changes of model parameters. An initially more skeptic receiver will enjoy higher information accuracy, while higher alignment of the agents’ preferences==== will generally hurt the receiver. We also find that the optimal information accuracy is increasing in the receiver’s cost of learning (impatience), but the two can always “cancel out” each other so that the ratio between them, in a simple but properly defined form, is constant. In a sense, the ratio captures the “net information” when signals can only be acquired costly, and so there is a “fixed” volume of net information that should be optimally delivered to the receiver at all levels of patience. For the designer, we focus on the maximum probability of a persuasion with a disadvantageous prior and profitable information manipulation. In the ====,==== the probability of a persuasion tends to ==== as the prior approaches the no-disclosure region, but such probability is bounded from above by ==== in our model, whereby a discontinuous drop in the designer’s payoff is identified. As a consequence, a slight change in the receiver’s initial belief, if triggering information disclosure, may lead to a disproportionate drop in the odds of a persuasion.====From a more general perspective, we may view the change in protocol as a reallocation of bargaining power in information transmission; the designer preserves the control over ====, yet that over ==== is ceded to the receiver. The welfare implication of this is that the designer is not able to appropriate all of the rents of information provision in our model, while the opposite holds in many standard Bayesian persuasion models. More precisely, we show that the designer’s payoff (i.e., the probability of a persuasion), plotted as a function about the prior, is the upper envelope of a class of affine functions generated by admissible pairs. Thus, instead of being achieved via concavification, the designer’s payoff is a convex function at the presence of constrained Bayes plausibility. Such a difference, as compared with the static benchmark, reveals a wedge that can be viewed as a proxy of the value of controlling information quantity to the designer.==== We further show that the control over information quantity generally becomes less valuable to the designer as the initial level of skepticism increases.====The paper is structured as follows. We review briefly in the rest of this section some related works. Section 2 specifies the model and describes the posterior dynamics. We then analyze in Section 3 the optimal information acquisition problem, by which we obtain a representation of information in our setup. Section 4 is devoted to examining the optimal information policy and how it hinges on primitives. Section 5 illustrates how the probability of a persuasion in this model compares with the static benchmark. Section 6 concludes. All proofs are relegated to the Appendix.",Bayesian persuasion with optimal learning,https://www.sciencedirect.com/science/article/pii/S0304406821000975,19 May 2021,2021,Research Article,94.0
"Jang Inkee,Kang Kee-Youn","Department of Economics, The Catholic University of Korea, South Korea,School of Business, Yonsei University, South Korea","Received 22 September 2020, Revised 10 February 2021, Accepted 22 April 2021, Available online 15 May 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102533,Cited by (1),"We develop an asset exchange model with adverse selection and costly information acquisition incentives. A seller of an asset knows the true value of the asset, while a buyer can obtain information about the asset’s quality at a cost. An equilibrium offer is pooling, but a buyer can purchase only good assets after producing the costly information about the asset’s quality. When the ","Markets in which assets are traded as a medium of exchange are often subject to asymmetric information problems. In particular, an asset seller tends to have more information about the asset’s value than a potential buyer. Adverse selection problems in asset markets have been extensively covered in the literature, where adverse selection problems are perceived as friction that impedes the efficient functioning of the market. For example, several studies, such as Akerlof (1970), Wilson (1979), Klein and Leffler (1981), Shapiro (1983), and Allen (1984), show that the adverse selection problem can lead to market failure. More recently, Rocheteau (2011) and  Wang (2021) show that the adverse selection problem, although it may not lead to the market collapse, reduces the efficiency of assets as a medium of exchange.====In reality, however, a less informed buyer can obtain information about an asset’s quality at some cost to overcome informational disadvantages. For example, investors may purchase analytic reports about a firm’s profitability before buying a company’s stock, and a household can gather information about a neighborhood before buying a house there. While information acquisitions of this kind can change market economic outcomes, the studies cited above pay little attention to information acquisition incentives of less informed agents, and, hence, there is a tension between theory and practice in reality.====The objective of this paper is to construct a unified framework that provides a systematic analysis of the adverse selection problem and information acquisition incentives in an exchange economy. The model consists of two risk-neutral agents and two periods. The seller holds a perfectly divisible financial asset in the first period that yields dividends at the end of the second period if the asset is good and yields nothing if it is bad. The seller has liquidity needs in the first period and wants to sell his/her asset to the buyer in exchange for the buyer’s consumption goods. The key friction in the economy is that the type of asset is the seller’s private information, and the buyer can verify the quality of the seller’s asset at a fixed cost.====We first study the bargaining problem between the seller and the buyer in a decentralized market. In an equilibrium with trading, a bad type (a seller with a bad asset) always has incentives to mimic a good type (a seller with a good asset); hence, the terms of trade do not signal anything about the type of asset. However, when the information acquisition cost is sufficiently low, the buyer can separate good types from bad types with the obtained information and purchase only good assets. In this case, the trade volume and the seller’s surplus decrease with the information acquisition cost. On the other hand, when the buyer does not acquire information due to a sufficiently high information acquisition cost, a trade occurs only if the probability of facing a good type is high enough, and the trade volume increases with the probability of facing a good type. Finally, when the adverse selection problem is severe, and the information acquisition cost is sufficiently high, profitable offers are impossible for a good type; hence, a trade does not occur, i.e., the market collapses.====We then study the constrained optimal allocation by solving the problem of a planner who proposes a trading mechanism to agents. Specifically, the trading mechanism is designed to maximize expected social welfare, taking as given the frictions in the environment (e.g., a lack of commitment, asymmetric information, and information acquisition incentives with an exogenously given information acquisition cost). The structure of allocations under the optimal mechanism is similar to that of allocations in the bargaining problem: When the information acquisition cost is sufficiently high, a trade occurs only if the probability that the seller holds good assets is sufficiently high, and information is produced only if the acquisition cost is sufficiently low.====The analysis of the constrained optimum shows that costly information is produced more often than the socially desirable level in the market. The intuition for this finding is as follows. Without information production, the bad seller enjoys the trade surplus, while the bad seller cannot trade if the information is produced. When the good seller offers a contract that triggers information production to the buyer, he/she does not take into account the lost trade surplus of the bad seller. The good seller only compares the cost of not being separated from the bad type and the cost of information acquisition and decides whether to induce the buyer to acquire information or not. On the other hand, the planner values the well-being of good and bad sellers symmetrically, and optimally chooses allocations considering the utility loss of the bad seller from not having trades if the information is produced. Consequently, the costly information is produced less frequently in the constrained optimal allocations than in the market allocations.====In the model economy, welfare is maximized if the information acquisition is costless, thereby eliminating asymmetric information problems, consistent with results in the standard information economics literature. In reality, however, it is almost infeasible to make information acquisition costless. Given this practical constraint, the model suggests that any efforts to make the information acquisition process easier should be made cautiously. Specifically, when the asset is traded without information acquisition, any attempt to make the information acquisition process easier could deteriorate welfare.==== However, it does not imply that we can always improve welfare by increasing the information acquisition cost. If the probability of facing bad assets is sufficiently high, an increase in the information acquisition cost can lead to the market collapse.",Adverse selection and costly information acquisition in asset markets,https://www.sciencedirect.com/science/article/pii/S0304406821000963,15 May 2021,2021,Research Article,95.0
Nishimura Hiroki,"Department of Economics, University of California Riverside, United States of America","Received 18 May 2020, Revised 15 January 2021, Accepted 24 April 2021, Available online 6 May 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102522,Cited by (0),"This paper studies rational choice behavior of a player in sequential games of perfect and complete information without an assumption that the other players who join the same games are rational. The model of individually rational choice is defined through a decomposition of the behavioral norm assumed in the subgame perfect equilibria, and we propose a set of axioms on collective choice behavior that characterize the individual rationality obtained as such. As the choice of subgame perfect equilibrium paths is a special case where all players involved in the choice environment are each individually rational, the paper offers testable characterizations of both individual rationality and collective rationality in sequential games.","A game is a description of strategic interactions among players. The players involved in a game simultaneously or dynamically choose their actions, and their payoffs are determined by a profile of chosen actions. A number of solution concepts for games, such as the Nash equilibrium or the subgame perfect equilibrium, have been developed in the literature in order to study collective choice of actions made by the players. In turn, these solution concepts are widely applied in economic analysis to provide credible predictions for the choice of economic agents who reside in situations approximated by games.====However, most of solution concepts for games are defined by preference relations (usually represented by payoff functions) of the players, while in practice only the choices of actions are observed. So, even when a certain solution concept appears reasonable to make predictions of the outcome of a game, we may not be able to apply such a concept unless preference relations of the players are known to the outside observer. Hence, it is important for the empirical contents of the game theory that there is a method that allows us to test the rationality of players and to reveal their preference relations on the basis of observed data set.====In this paper, by assuming observability of the choice of actions made by players involved in games, but not of their preference relations, we study a method to test whether they choose their actions rationally according to their preference relations. This question in principle follows the idea of the revealed preference theory pioneered by Arrow (1959), Houthakker (1950), Samuelson (1938), Sen (1971), and others. Yet, we depart from the classical revealed preference theory for an individual decision maker by assuming observability of ==== choice behavior from games and by studying the rational choice behavior of multiple decision makers (or players) involved in such a choice environment. In this regard, Sprumont (2000) takes the games of simultaneous moves and investigates axiomatizations of Pareto optimality and Nash equilibrium. Ray and Zhou (2001) assume observability of paths chosen by the players in extensive games and characterize the choice of subgame perfect equilibria. In other contexts, Bossert and Sprumont (2013) show that every choice function is rationalizable as a result of the backwards induction when the observed data is limited, Carvajal et al. (2013) develop the revealed preference theory for Cournot competition games, and Carvajal and González (2014) and Chiappori et al. (2012) examine the testability of Nash solution in two-player bargaining games. Recently, Schenone (2020) studies the relation between the subgame perfect equilibria and the backwards induction in a choice theoretic framework with, and without, the weak axiom of revealed preferences (WARP).====It is however worth noting that the existing literature has focused on behavioral characterizations of the ==== rationality (by which we mean the game theoretic equilibria) in the collective choice environments, and a question of characterizing the ==== rationality in this framework remains unanswered. Indeed, the historical and recent developments in experimental economics and in the theory of bounded rationality (Binmore et al. (2002), Leng et al. (2018), and others) suggest that decision makers tend to violate predictions of the rational choice theory in practice. Therefore, even when observed choice data does not support the collective rationality, it is still of interest to identify a set of players, if not all, who make individually rational choice. A question thus leads to finding a testable characterization of the individual rationality in the collective choice environment. This is what this paper is after. (See Table 1.)====To this end, the paper also relates to the work of Gul and Pesendorfer (2005), in which they study intertemporal decision problems of a single decision maker with changing tastes. When the tastes of a decision maker change over time, the same decision maker may be tempted to deviate from a path of actions that she previously found optimal. In this situation, the model of consistent planning, originally proposed by Strotz (1955), views the decision maker at different points in time as different players and solves for an action path that she can actually follow through. Gul and Pesendorfer axiomatize a representation termed the ====, in which only the decision maker at the initial period is assumed to be fully rational while she may, at any point in the future, choose her actions to maximize her payoffs given incorrect predictions of the behavior of the subsequent selves. The present paper marks a contrast with the weakly Strotz model by characterizing the collective choice behavior where we postulate ==== behavioral assumption on players at any decision nodes in the future. To highlight a difference, for any game with 2 periods, the weakly Strotz model coincides with the evaluation of the game by the optimal subgame perfect equilibrium path, as player 2, the player at the last period, does not have to form any predictions for the subsequent players and is hence rational. In contrast, the concept of the individual rationality studied in this paper allows behavioral choice of the players at period 2 (see Example 4).====This paper aims to characterize the individual rationality in the collective choice environment, without relying on any behavioral assumption on the other players. In particular, following Gul and Pesendorfer (2005) and Ray and Zhou (2001), we adopt a plain setup of sequential games where linearly ordered players make their actions one after another, and observability is assumed for paths of actions chosen by the players in such games. We focus on a person, called player 1, who chooses an action first among all the players (i.e. the one who stands at the initial node) and seek a testable axiomatization of the individual rationality for this player. A notion of individual rationality in our simple framework is derived from a decomposition of the behavioral norm assumed in the subgame perfect equilibria into the level of the individual player. Specifically, if player 1 is rational, then she should correctly form a history-dependent prediction of actions chosen by the subsequent players and choose her own action in order to achieve an optimal path among those that are actually followed by the players. Importantly, the other players may choose ==== actions, but even then, player 1 takes such choices of actions into consideration and chooses her action to achieve the best outcome among those that she can achieve. The main theorem of the paper provides an axiomatization on the observed choice data that is equivalent to the rationality of player 1 in this sense.====The collective rationality is reconstructed from the individual rationality. We show that any path chosen by the players in sequential games must be a subgame perfect equilibrium path, provided that ==== players in the environment are individually rational in the same sense above. Notably, since the individual rationality of each player is testable through a plain adjustment of the same axiomatization for player 1, this paper thus provides an axiomatic characterization for the choice of subgame perfect equilibrium paths in sequential games, parallel to the result by Ray and Zhou (2001).==== We will also demonstrate a subtle yet important difference between the collectively rational choice and the choice of subgame perfect equilibria.====The paper is structured as follows. In Section 2, we introduce a model of sequential games used throughout the paper and then place the main concept of this paper, the individually rational choice correspondences. We will also use some examples to motivate the model. In Section 3, we discuss certain testable axioms on observed choice behavior in sequential games and show that the proposed axioms characterize the individual rationality in the sequential games. The collective rationality is studied in Section 4. We show that we can use the same set of axioms to test whether the observed choice behavior is collectively rational, and, moreover, that a collectively rational choice correspondence closely relates to the choice of subgame perfect equilibria. All proofs are given in the Appendix.",Revealed preferences of individual players in sequential games,https://www.sciencedirect.com/science/article/pii/S0304406821000859,6 May 2021,2021,Research Article,96.0
Demuynck Thomas,"ECARES, Université Libre de Bruxelles. Avenue F.D. Roosevelt 50, CP 114, B-1050 Brussels, Belgium","Received 26 August 2020, Revised 19 April 2021, Accepted 24 April 2021, Available online 6 May 2021, Version of Record 24 November 2021.",https://doi.org/10.1016/j.jmateco.2021.102523,Cited by (1),.,None,A Markov Chain Monte Carlo procedure to generate revealed preference consistent datasets,https://www.sciencedirect.com/science/article/pii/S0304406821000860,6 May 2021,2021,Research Article,97.0
"Kokonas Nikolaos,Monteiro Paulo Santos","University of Bath, United Kingdom,University of York, United Kingdom","Received 1 September 2020, Revised 9 March 2021, Accepted 19 April 2021, Available online 30 April 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102521,Cited by (0),"We derive an aggregation result in economies with indivisible labor supply choices and frictional labor markets, obtaining a tractable model of gross worker flows in aggregate labor markets with search frictions. Our result explores the fact that economies with non-convex choice sets and idiosyncratic shocks allow for sunspot equilibria à la ","Since the seminal work of Merz (1995) and Andolfatto (1996), dynamic stochastic general equilibrium (DSGE) models with labor market search frictions have been widely used to study unemployment fluctuations. However, these two approaches place different restrictions on individual choices. In turn, Merz (1995) assumes the existence of a representative “large family”, constrained by budget sets and an employment law of motion, while Andolfatto (1996) assumes a game of “musical chairs” (exogenous shocks), that randomly allocate individuals to labor market states, with perfect insurance against idiosyncratic risk.==== We take our cue from the latter approach and make the following contribution: we generalize the musical chairs’ approach to a model with gross worker flows and individual participation choices, using results from the literature on sunspots and lotteries, along the lines of Kehoe et al. (2002). To the best of our knowledge, ours is the first paper to offer an aggregation result in economies with three state labor markets, indivisible labor and search frictions, based on individual choices (without having to impose the assumption of the “large family”), that yields a constrained efficient competitive equilibrium.====Our approach delivers a tractable characterization of equilibrium in economies combining indivisible labor supply choices (participation margin), and labor market frictions. The literature has often restricted attention to two-state labor markets, ignoring participation and focusing on the margin between employment and unemployment.==== However, recent empirical work attributes an important role to the participation margin for labor market transitions. Elsby et al. (2015) showed that the participation margin accounts for one-third of the cyclical variation in the unemployment rate. Moreover, unlike the Merz (1995) large family set-up, which only identifies net worker flows, our model yields a characterization of equilibrium gross worker flows. This is important, since Krusell et al., 2010, Krusell et al., 2011 and Krusell et al. (2017) stressed the importance of gross worker flows and developed models with missing insurance markets and indivisibilities in labor supply choice to account for these transitions.====We develop a general equilibrium model of gross worker flows with complete markets, where individuals face heterogeneous employment histories and idiosyncratic risk. Following Andolfatto (1996), musical chairs allocate individuals to different labor market states each period; conditional on this, individuals face an indivisible participation choice, in labor market with search frictions. To overcome indivisibilities, individuals play lotteries over participation as in Rogerson (1988) and Hansen (1985). The decision of each individual is based on the joint outcome of public (“musical chairs”) and contrived randomness (lotteries) and the realization of idiosyncratic shocks. This hybrid decision process may seem unusual, but we argue it can be microfounded as follows. We demonstrate that one can mimic the joint effects of musical chairs and lotteries by indexing on the basis of two naturally occurring random variables (sunspots) prior and after the realization of the idiosyncratic shocks. Such an arrangement is consistent with the existence of the usual Arrow–Debreu contingent commodities.====Subsequently, similarly to Christiano et al. (2020) we use comparative steady state analysis as a short-cut for analyzing model dynamics. Two main insights emerge from this analysis. First, our model reconciles the neoclassical growth model with search frictions with a mildly procyclical participation rate. This result is particularly important given the tendency for models featuring intertemporal substitution in frictional labor markets to deliver excessively procyclical participation and, thus, procyclical unemployment (a problem stressed by Ravn, 2008, Veracierto, 2008, Shimer, 2013 for example). Second, we show using a calibrated example that the model accounts well for the observed flows. In particular, it is able to match the high transition rate from unemployment to inactivity, which early papers by Garibaldi and Wasmer (2005) and Krusell et al., 2010, Krusell et al., 2011 have shown to be challenging for equilibrium models of gross worker flows, under either complete or incomplete markets.====The literature on sunspots and lotteries in economies with non-convexities and complete markets includes, among others,  Prescott and Townsend (1984), Shell and Wright (1993), Garratt (1995), Garratt et al. (2002), Kehoe et al. (2002), and Garratt et al. (2004). Our results generalize models with indivisibilities to include idiosyncratic risk arising from frictional labor markets. To achieve that, we introduce the distinction between public randomizations prior and after the realization of idiosyncratic shocks in each period—although this distinction is already discussed by Kehoe et al. (2002), it is not important for their analysis.====Our paper contributes to a recent literature that combines indivisible labor supply choices in models with intertemporal substitution (what Krusell et al. (2008) call “non-trivial labor supply choices”), together with search frictions in the labor market. Krusell et al. (2008) show that in a set-up with indivisibility and incomplete markets (similar to Chang and Kim, 2006, Chang and Kim, 2007), search frictions avoid indeterminacy in labor supply choices.==== Building on this framework, Krusell et al., 2010, Krusell et al., 2011 study individual transitions across employment, unemployment and non-participation, in a three-states labor market model, with incomplete markets, search frictions and non-trivial labor supply choices.==== They show that whilst the benchmark model is unable to match the persistence of the employment and non-participation states found in the data, a version of the same model with persistent idiosyncratic productivity shocks affecting the individual value of work is able to match the transition flows well.====Further, Krusell et al. (2011) study a version of their model with complete markets (with insurable idiosyncratic shocks), but do not discuss decentralization and, instead, consider the solution to the social planner’s problem, in which each individual receives equal weight. The resulting equilibrium allocations imply labor market gross flows that are comparable to those obtained in the incomplete market economy. Thus, they conclude that uninsurable risk is not a necessary ingredient to obtain a satisfactory representation of labor market transitions.==== In our paper, we show how the decentralized competitive equilibrium with complete markets can be obtained using either lotteries or sunspots, to produce constrained efficient allocations. At the same time, the resulting model is as successful at matching empirical gross worker flows as the incomplete markets model. In particular, using sunspots as the randomization mechanism generates heterogeneous employment histories across individuals that are conforming with realistic transitions across labor market states (comparable to what is achieved by Krusell et al., 2010, Krusell et al., 2011 using idiosyncratic productivity shocks).====Finally, Krusell et al. (2017) augment the set-up developed in Krusell et al., 2010, Krusell et al., 2011 with job-to-job transitions and aggregate shocks to labor market frictions, in order to study gross worker flows over the business cycle. We consider shocks to the job finding rate, and show how the model with indivisible labor supply, complete markets, and extrinsic randomization, can deliver either a countercyclical or a procyclical participation rate. Thus, the neoclassical growth model with search frictions can be reconciled with a mildly procyclical participation rate, that is supported by empirically realistic gross worker flows.====The remainder of the paper is organized as follows: Section 2 explains the environment; Section 3 establishes that an equilibrium with musical chairs and lotteries corresponds to a sunspot equilibrium; Section 4 presents the comparative steady state analysis; Section 5 concludes the paper.",Aggregation in economies with search frictions,https://www.sciencedirect.com/science/article/pii/S0304406821000847,30 April 2021,2021,Research Article,98.0
"Light Bar,Perlroth Andres","Graduate School of Business, Stanford University, Stanford, CA 94305, USA,Google Research, Mountain View, CA 94043, USA","Received 25 March 2020, Revised 24 March 2021, Accepted 15 April 2021, Available online 24 April 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102520,Cited by (1),"In this paper we provide a novel family of ==== where ==== parameterizes the degree of ====. The ====-concave stochastic orders allow us to derive novel comparative statics results for important applications in economics that cannot be derived using previous stochastic orders. In particular, our comparative statics results are useful when an increase in a lottery’s riskiness changes the agent’s optimal action in the opposite direction to an increase in the lottery’s expected value. For this kind of situation, we provide a tool to determine which of these two forces dominates — riskiness or expected value. We apply our results in consumption–savings problems, self-protection problems, and in a ==== game.","Stochastic orders are fundamental in the study of decision making under uncertainty and in the study of complex stochastic systems. They have been used in various fields, including economics, finance, operations research, and statistics (for a textbook treatment of stochastic orders and their applications, see Müller and Stoyan (2002), Shaked and Shanthikumar (2007), or Levy (2015)). In this paper we provide a family of stochastic orders that is based on a novel family of utility functions, which allows us to compare two random variables, where one random variable has a higher expected value and is also riskier than the other random variable.====For instance, consider the following two simple random variables (also called lotteries) ==== and ==== described in Fig. 1.====Lottery ==== yields ==== dollars with probability ==== and ==== dollars with probability ==== where ====, ====, and ====. Lottery ==== yields ==== dollars with probability ====. If ==== is not very high, it is reasonable to assume that most risk-averse decision makers would prefer lottery ==== over lottery ====. For example, if ====, ====, ====, and ====
 ==== 1,000,000, then lottery ==== yields 500,000 dollars with probability ==== while lottery ==== yields 1,000,000 dollars with probability 0.55 and ==== dollars with probability 0.45. Lottery ==== has a higher expected value (550,000 dollars) than lottery ==== but a high probability (a probability of 0.45) of receiving ==== dollars. Thus, in this case, it seems reasonable that most risk-averse decision makers would prefer lottery ==== over lottery ====. Note that for every ====, lottery ==== has a higher expected value and is riskier than lottery ====. Thus, standard stochastic orders cannot compare the two lotteries. In particular, since the expected value of ==== is higher than the expected value of ====, ==== does not dominate ==== in most popular stochastic orders because these stochastic orders impose a ranking over expectations to determine whether ==== dominates ====. In particular, ==== does not dominate ==== in the second order stochastic dominance (Hadar and Russell, 1969, Rothschild and Stiglitz, 1970), third order stochastic dominance (Whitmore, 1970), higher order stochastic dominance (Ekern, 1980), decreasing absolute risk aversion stochastic dominance (Vickson, 1977), or in the almost second order stochastic dominance (Leshno and Levy, 2002). In Section 2, however, we show that the stochastic orders provided in this paper that are based on a novel set of risk-averse decision makers can compare ==== and ====.====In this paper we provide a family of stochastic orders indexed by ==== where ==== and ==== is a subset of ====, which we call the ====-concave stochastic orders. The family of ====-concave stochastic orders generalizes second order stochastic dominance (SOSD),==== which corresponds to the ====-concave stochastic order. The main idea of the ====-concave stochastic orders is that the inequality ==== is required to hold only for a subset of the concave and increasing functions (and not for all of them) in order to determine that a random variable ==== dominates a random variable ==== in the ====-concave stochastic order. In particular, the inequality ==== is not required to hold for a function ==== that is affine or for a function ==== that is nearly affine in the sense that the elasticity of ==== with respect to ==== is bounded below by a number that depends on ====. This elasticity measures the function’s concavity degree in a natural way and relates to the coefficients of prudence and risk aversion (see Section 2 for more details).====An important feature of the ====-concave stochastic orders is that for ====, ==== dominating ==== in these orders neither implies that ==== has to be lower than ====, nor does it imply the opposite. In Section 2 we provide examples of random variables ==== and ==== where ==== has a higher expected value and is riskier than ====, and ==== dominates ==== in the ====-concave stochastic order. For instance, we show that ==== dominates ==== in the ====-concave stochastic order for the example presented in Fig. 1. Another feature of the ====-concave stochastic orders is their dependence on the support of the distribution. We show that this dependence is helpful for applications where agents’ behavior depends on their wealth level. We illustrate this in a consumption–savings example (see Section 1.1).====For general random variables it is not trivial to check whether a random variable dominates another random variable in the ====-concave stochastic orders. Finding a simple integral condition to characterize stochastic orders that generalize SOSD is impossible or not trivial (see Gollier and Kimball (2018)). However, we provide a sufficient condition for domination in the ====-concave stochastic order that is based on a simple integral inequality (see Section 2). Similar integral conditions are used to determine whether a random variable dominates another random variable in other popular stochastic orders. The sufficient condition generates a stochastic order that is of independent interest and can be easily used in applications. We partially characterize the maximal generator of this new stochastic order (see Appendix A) for ====.====To illustrate the usefulness of the family of ====-concave stochastic orders, we derive novel comparative statics results in three applications from the economics literature. The first application is a consumption–savings problem with labor income uncertainty. It is established in previous literature that a prudent agent (i.e., an agent whose utility function has a positive third derivative) saves more if the labor income risk increases in the sense of SOSD (see Leland (1968)). It is also easy to establish that the agent’s current savings increase if the labor income’s expected present value increases. We do not know of any comparative statics results for the case when both the present value and the risk of future labor income increase. We show that under certain conditions on the agent’s marginal utility (the marginal utility must be “very convex”), an increase in the risk of future labor income together with an increase in the expected present value of future labor income increase savings. That is, the precautionary saving motive is stronger than the permanent income motive.====The second application deals with self-protection problems. We consider a standard self-protection problem (e.g., Ehrlich and Becker (1972)) where choosing a higher action is more costly but reduces the probability of a loss. Stochastic orders can be used as a tool to decide whether the level of self-protection should be higher or lower. For a decision maker that makes decisions according to the decision rule implied by the ====-concave stochastic order, we provide conditions that imply a change in the level of self-protection.====In our third application, we show that the ====-concave stochastic order can be used in a non-cooperative framework as well. We study a Bayesian game which is a variant of the search model studied in Diamond (1982) and in Milgrom and Roberts (1990). In this game, there are two players that exert a costly effort to achieve a match, and the probability of a match occurring depends on the effort exerted by both. We analyze how different beliefs affect the equilibrium probability of matching.====Our ====-concave stochastic orders are also useful in proving inequalities that involve convex functions. To show the usefulness of these stochastic orders in proving inequalities, we prove a novel Hermite–Hadamard type inequality for decreasing functions ==== such that the square root of ==== is convex (see Section 3.4).====There is extensive literature on stochastic orders and their applications (for a survey see Müller and Stoyan (2002) and Shaked and Shanthikumar (2007)). The stochastic orders we study in this paper are integral stochastic orders (Müller, 1997). Integral stochastic orders ==== are binary relations over the set of random variables that are defined by a set of functions ==== in the following way: for two random variables ==== and ==== we have ==== if and only if ==== for every ==== and the expectations exist. Many important stochastic orders are integral stochastic orders. For example, SOSD corresponds to the stochastic order ==== where ==== is the set of all concave and increasing functions.====The integral stochastic orders we present in this paper are related to stochastic orders that weaken SOSD by restricting the set of utility functions under consideration. Third order stochastic dominance (Whitmore, 1970) requires that the functions have a positive third derivative. Higher stochastic orders (see Ekern (1980), Denuit et al. (1998), and Eeckhoudt and Schlesinger (2006)) restrict the sign of the functions’ higher derivatives. Leshno and Levy (2002), Tsetlin et al. (2015), and Müller et al. (2016) restrict the values of the functions’ derivatives. Vickson (1977) and Post et al. (2014) add the assumption that the functions are in the decreasing absolute risk aversion class. Post (2016) requires additional curvature conditions on the functions’ higher derivatives.====The above stochastic orders are significantly different from the stochastic orders we introduce in this paper. All these stochastic orders impose a ranking over expectations, while the stochastic orders presented in this paper do not impose a ranking over expectations. Other known stochastic orders that do not impose a ranking over expectations are introduced in Fishburn (1976) and in Meyer (1977a). Meyer (1977a) imposes a lower and an upper bound on the Arrow–Pratt absolute risk-aversion measure (see more details on this stochastic order in Appendix A). Fishburn, 1976, Fishburn, 1980 studies a stochastic order that is based on lower partial moments. While these stochastic orders are based on an integral condition, the main disadvantage of these stochastic orders is that their maximal generator is not known (see more details in Appendix A).====The paper is organized as follows. In Section 1.1 we study a consumption–savings problem that illustrates the usefulness of our stochastic orders. In Section 2 we define the ====-concave stochastic orders and study their properties. In Section 3 we study the applications discussed above. Section 4 contains concluding remarks. The Appendix contains the proofs not presented in the main text and a discussion on the maximal generator of stochastic orders.","The Family of Alpha,[a,b] Stochastic Orders: Risk vs. Expected Value",https://www.sciencedirect.com/science/article/pii/S0304406821000835,24 April 2021,2021,Research Article,99.0
"Chiu Yen-Lin,Karni Edi","Johns Hopkins University, United States of America","Received 22 July 2020, Revised 8 March 2021, Accepted 6 April 2021, Available online 21 April 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102519,Cited by (1),. Customers do not possess the expertise necessary to assess the service they need either ex ante or ex post. We show that there exists no fraud-free equilibrium in the markets for credence-quality goods and that fraud is a prevalent and persistent equilibrium phenomenon.,"Customers seeking to purchase services that require specialized knowledge are susceptible to fraud by suppliers who prescribe unnecessary services. Examples include, medical tests and treatments, auto repairs, equipment maintenance, and taxi cab service. In these markets the service suppliers make diagnostic determinations of the service required and offer to provide it, and the customers must decide whether to purchase the prescribed service or to seek, at a cost, a second service prescription. Typically in these situations, the customer can judge, ex post, whether or not the service provided was ==== to solve the problem, but is unable to assess whether the prescribed service was also ====.====Darby and Karni (1973) were the first to identify the fundamental ingredients of the problem underlying the provision of what they dubbed ====. First, information asymmetry between the customer who lacks the expertise necessary to assess the service needed and service provider who possess the required expertise and, second, the cost saving of the joint provision of diagnosis and services.==== They proceeded to discuss and analyze the economic implications of transactions involving this type of asymmetric information. Specifically, Darby and Karni argued that in competitive market equilibrium for credence-quality goods there is persistent tendency of suppliers to over-prescribe services (that is, to prescribe services that are sufficient but are unnecessary to solve the problem at hand).====The nature and extent of fraudulent practices depend on the specific characteristics of the credence-good market. For example, the demand for auto repair at a given service station depends on the waiting time (that is, the length of the queue of customers waiting to be served) which is not an issue when it comes to taxi cab service. It also depends on the information the customer may acquire before choosing the service provider and the cost of seeking a second opinion. For instance, in medical diagnosis that requires an invasive procedure the cost of obtaining a second opinion is prohibitively high. It is obvious, therefore, that modeling of credence-goods markets, while incorporating the fundamental ingredients of the problem – information asymmetry and the bundling of diagnosis and service – must be based on the specifics of the market under consideration. In this paper we focus on markets for the provision of services, such as auto-repair services, in which the capacity limitations may result in waiting for service. We underscore this point to avoid the impression that this is a general model of credence-good markets. We believe, however, that the game-theoretic approach invoked here is not specific to the analysis of the model we study in this paper, rather it is a natural framework for the analysis of credence-good markets in general.====Since the publication of Darby and Karni (1973), numerous studies confirm the prevalence of fraudulent behavior in the markets for credence-quality goods.==== For medical services, especially physicians’ services, over treatment, a phenomenon known in medical literature as supplier induced demand, is widely documented (see Mcguire, 2000, Currie et al., 2011, Dranove, 1988). Gianfranco et al. (1993) found that in Swiss canton of Ticino on average the population has one third more operations than medical doctors and their relatives, suggesting that greater information symmetry tends to reduce overprescription of surgical procedures. The same type of conclusion was reached by Balafoutas et al. (2013). They report the results of a natural field experiment on taxi rides in Athens, Greece, designed to measure different types of fraud and to examine the influence of passengers’ presumed information on the extent of fraud. Their findings indicate that passengers with inferior information about optimal routes are taken on significantly longer detours. Iizuka (2007) finds physicians drugs prescriptions are influenced by markup. Schneider (2012) reports the results of a field experiment designed to assess the accuracy of service provision in the auto repair market. He finds evidence for over prescription of services as well as under prescription. Beck et al. (2014) report that in experimental setting, car mechanics are significantly more prone to supplying unnecessary services than student subjects.====The work of Darby and Karni, while calling attention to a neglected aspect of economic interactions that results in market failure, lacks the formal structure necessary to derive more subtle implications of the concept they introduced. In this work we take a step towards a more formal analysis of markets for credence-quality services with some specific characteristics. Specifically, taking a game-theoretic approach we analyze the equilibrium behavior in a market in which two suppliers operating service stations are engaged in Bertrand competition. The suppliers are assumed to be ex ante identical in every respect. The sole asymmetry between the suppliers, which arises endogenously, is the lengths of their queues (i.e., the waiting time for service). The critical aspect of the model is the information asymmetry regarding the service that is required to address the problem at hand. The suppliers are supposed to possess the expertise necessary to assess the required service while the customers do not.====Customers heterogeneity is the consequence of idiosyncratic costs of seeking a second prescription and of waiting for service. We assume that these costs are the customers’ private information. The customers are assumed to discover the lengths of the suppliers queues (that is, the waiting time) only when they visit the supplier’s service outlet.====We study the market in a stationary symmetric equilibrium in which normal profits discourage entry or exit.==== In other words, the idle time at the service stations is short enough so that no supplier loses money but is sufficiently long so as to discourage new entries or installing additional service capacity. In addition to proving its existence, we show that there exists no fraud-free equilibrium in this market, that the level of fraud committed by the two suppliers depends on the lengths of their queues, and that the short-queue supplier is more likely to overprescribe service than the long-queue supplier. These conclusions highlight the message of this work, namely, that the ==== suggesting that the study of these markets, while maintaining the unifying characteristics, information asymmetry and the bundling of diagnosis and service, should proceed on a case by case basis.====In the next section we describe the credence good market. The equilibrium analysis appears in Section 3. Some economic implications of our analysis are discussed in Section 4. Section 5 includes a discussion of related literature and some concluding remarks. To allow for uninterrupted reading we collected the proofs in Section 6.",Competitive equilibrium fraud in markets for credence-goods,https://www.sciencedirect.com/science/article/pii/S0304406821000823,21 April 2021,2021,Research Article,100.0
"Faggian Silvia,Gozzi Fausto,Kort Peter M.","Department of Economics, Universitá “Ca’ Foscari” Venezia, Italy,Dipartimento di Economia e Finanza, Universitá LUISS - “Guido Carli”, I-00162, Roma, Italy,CentER, Department of Econometrics & Operations Research, Tilburg University, P.O. Box 90153, 5000 LE Tilburg, The Netherlands,Department of Economics, University of Antwerp, Prinsstraat 13, 2000 Antwerp 1, Belgium","Received 26 September 2020, Revised 15 February 2021, Accepted 15 March 2021, Available online 20 April 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102516,Cited by (1),The paper concerns the study ,"Computing equilibrium points, or steady states, and describing their properties is one of the main goals in the mathematics of economic models. This task, when presuming an underlying optimal control problem with infinite horizon, is already nontrivial with one state variable, but it becomes harsh when the dynamics of the system are infinite dimensional, like in cases when heterogeneity/path dependency is taken into account. This is the case, for instance, of optimal investment with vintage capital (capital stock is heterogeneous in age, see ==== Feichtinger et al., 2006, Fabbri and Gozzi, 2008), of spatial growth models (capital stock is heterogeneous in space, see ==== Boucekkine et al., 2013, Fabbri, 2016, Boucekkine et al., 2018), of growth models with time-to-build (capital stock is path-dependent, see ==== Asea and Zak, 1999, Bambi, 2008, Bambi et al., 2012), or of models with heterogeneous agents (see ==== Moll and Nuno, 2018). In all these examples, equilibrium points are indeed functions (of vintage, or space, or age) and may be more properly referred to as “equilibrium distributions”. Up to now, such equilibrium distributions have been studied ==== – a requirement which is very seldom met – so that many interesting cases are left out of the picture.====On the contrary, this work addresses the study of equilibrium distributions in cases where no explicit formula for the value function is available, moreover it does so under the general assumptions of an infinite-horizon infinite-dimensional control problem with linear state equation and general convex (concave, in the application) payoff, providing a theoretical tool that can be used in a variety of applied examples. In fact, the theory is put immediately into practice for the optimal investment model with vintage capital, obtaining analytic formulas for the equilibrium distributions, and a complete sensitivity analysis for some instances of the problem. Hence the paper contains a theoretical and an applied part, both of equal weight and dignity, whose main achievements are listed below.====For the general theory (Sections 2 The optimal investment model with vintage capital, 3 The theoretical framework, 4 Equilibrium points), we reprise and complete the study of the control problems analyzed in Faggian and Gozzi (2010).==== There, Dynamic Programming (DP) was employed to prove the existence and uniqueness of a regular solution ==== of the Hamilton–Jacobi–Bellman (HJB) equation, as well as a verification theorem implying existence and uniqueness of optimal feedback controls, and the fact that ==== coincides with the value function. Overall and differently from most contributions to the subject, this work presents an integrated approach between the DP and MP methods of optimal control theory. In particular:====It is important noting that the theory cannot be used straightforwardly to treat applied problems in a satisfactory way. This happens on the one hand because the results in infinite dimension need to be translated into terms of the application under analysis, and on the other hand as it may be necessary to exploit the particular structure of the applied problem to specify formulas for practical use. One example is worked out through Theorem 5.5, in the case of the model of optimal investment with vintage capital.====In the applied part of this work (Sections 5 Application to optimal investment with vintage capital, 6 Sensitivity analysis in two special cases), the theoretical results are used on the optimal investment model with vintage capital deriving:====In particular, the sensitivity analysis enables the development of new economic results while analyzing the vintage capital stock model in which revenue is a strictly concave and linear–quadratic function of output, where the strict concavity is caused by market power on the output market. As is standard in this literature (Feichtinger et al., 2006), output linearly depends on the capital goods, whereas investment costs are convex and linear–quadratic. We show that the equilibrium distribution capital stock is first increasing and then decreasing in the age of the capital good. The increasing part is the result of investment costs being relatively large when capital goods are relatively new. On the other hand, such investments are attractive due to the long lifetime of new capital goods. Capital goods of older age have a shorter lifetime. This gives an incentive to reduce investments in older capital goods, resulting in the fact that the equilibrium distribution capital stock for old machines decreases with respect to age. We further establish another non-monotonicity dependence of the equilibrium distribution capital goods level, but now with respect to the productivity of the capital goods. If productivity is relatively low, the number of capital goods increases if productivity goes up. This is because a given capital good produces more so that the firm is more eager to invest in it. On the other hand, if productivity is relatively large the firm decreases investments, because otherwise the firm overproduces resulting in a too low marginal revenue. In other words, some optimal output level exists and less capital goods are needed to produce this level when productivity is high.====In conclusion, this work shows how successfully and effectively the theoretical machinery of optimal control in infinite dimension is in computing ==== formulas and studying properties for equilibrium distributions, also ==== We believe that the theoretical tools developed in the first part of this work can be successfully employed in examples yielding the same abstract structure (like those mentioned at the beginning of this introduction) and possibly extended to more complex cases with the use of suitable numerical approximations. One example is to consider harvesting models with age-structured populations (see, e.g., Anita̧, 2000). This seems to be a promising topic for future research.====The paper is organized as follows. Section 2 presents a family of optimal investment models with vintage capital. Section 3 presents the abstract optimal control problem and shows that the problem contained in Section 2 falls into that wider class. Section 4 is the theoretical core of the paper, where we recall the results obtained with the DP approach in Faggian and Gozzi (2010) (Section 4.1), we state and prove first order optimality conditions in terms of a Maximum Principle (Section 4.2), and we present and discuss the general results on equilibrium points (Section 4.3). In Section 5, the general results of the previous sections are applied to the model of optimal investment with vintage capital, providing a technique to derive analytic formulas for the equilibrium distributions. Finally, in Section 6, a sensitivity analysis is conducted on some instances of the problem of Section 5, i.e. where both revenues and costs are chosen linear–quadratic. This section also contains numerical results as illustration. An appendix with proofs of the theorems of Sections 4 Equilibrium points, 5 Application to optimal investment with vintage capital, as well as some additional results, completes the work.====We remark that the paper is organized as to allow the reader less interested in mathematical details to approach Sections 5 Application to optimal investment with vintage capital, 6 Sensitivity analysis in two special cases without necessarily going through the theoretical Sections 3 The theoretical framework, 4 Equilibrium points.",Optimal investment with vintage capital: Equilibrium distributions,https://www.sciencedirect.com/science/article/pii/S0304406821000793,20 April 2021,2021,Research Article,101.0
Chen Zhuoqiong,"Harbin Institute of Technology, Shenzhen, China","Received 20 February 2020, Revised 6 October 2020, Accepted 2 April 2021, Available online 17 April 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102518,Cited by (5)," sufficiently heterogeneous, the optimal signals work through an information-rent channel by inducing allocative efficient contests. When the players are ==== sufficiently homogeneous, the optimal signals work through an unlevel-playing-field channel by inducing asymmetric contests. In order to guarantee efficient allocation, a regulator can punish any exchange of information when the players are sufficiently homogeneous and impose no restrictions when they are sufficiently heterogeneous.","Firms in competition benefit from exchanging information as it allows them to compete more efficiently.==== To avoid the accusations that the exchange of private information with competitors is anti-competitive, they often rely on a third party to collect and disseminate information publicly.==== This paper studies optimal information exchange, which maximizes each player’s expected payoff in a competition, through public information disclosure by a third party.====Competitions in a wide array of business, economics, and politics environments resemble a contest: all players’ resources invested in the competitions are sunk cost, regardless of the outcome.==== Such an “all-pay” feature of the contest is present in many critical strategic interactions, for example, research and development (Dasgupta, 1986, Lichtenberg, 1988, Che and Gale, 2003, Kaplan et al., 2003), public procurement (Kaplan, 2012), and rent-seeking and lobbying (Ellingsen, 1991, Baye et al., 1993, Che and Gale, 1998).====We model contests as a first-price all-pay auction with two players who have independent private binary values. For tractability, we assume that each player has either a ==== or a ==== value from winning the contest, following previous literature (Lu et al., 2018, Serena, 2017, Liu and Chen, 2016). The players exchange information by reaching ==== a binding “industry-wide agreement” (Kovenock et al., 2015) such that either all firms disclose private information to the competitors or no firm does.==== The agreement specifies a rule of information disclosure that aims to maximize each player’s expected payoff and is implemented by a benevolent information center that discloses a public signal conditional on their private values.====We show that the monotonicity of a likelihood ratio between posteriors regarding the opponent’s value ==== upon observing a public signal ==== given that a player has the high value ==== and the low value ====, where ====, ====is sufficient and necessary for non-monotonicity of the equilibrium in the all-pay auction. The likelihood ratio (1) being monotonically increasing in ==== (==== condition) features a highly competitive environment in which each player believes that her opponent is likely to be equally competitive as she does. In the resulting ==== (SCE), each player earns zero expected payoff. Alternatively, a monotonically decreasing likelihood ratio (1) (==== condition) features an uncompetitive environment in which each player believes that she is likely to compete against an opponent with a different value. In the resulting ==== (WCE), players earn a strictly positive ==== expected payoff. Finally, if (1) is non-monotonic in ====, the bidding strategy in the resulting ==== (MSE) is monotonically increasing in types.====The equilibria are also characterized by the studies on the all-pay auctions with affiliated values (Liu and Chen, 2016, Chi et al., 2019). The uniqueness of the equilibria corresponding to the MSE and the SCE is established by Chi et al. (2019). We contribute to this literature by proving the uniqueness of the WCE and establishing a novel connection between the likelihood ratio (1) and the monotonicity of equilibria.====We characterize the upper bound of each player’s expected payoff, given that the public signal can take any finite number of values. We show that two-value public signals are sufficient to achieve the maximum, which is strictly greater than the expected payoff in the independent private value (IPV) setting and is equal to the maximum given by Lu et al. (2018). The design of optimal signals utilizes the features of the equilibria given different degrees of heterogeneity of the players. On the one hand, when players are ==== sufficiently heterogeneous such that the low value ==== is sufficiently small, the optimal signals maximize the players’ expected payoffs by inducing efficient allocation by guaranteeing that the MSE is played. Hence, the optimal signals maximize players’ expected payoff through an information-rent channel. On the other hand, when players are sufficiently homogeneous such that the low value is sufficiently large, the optimal signals maximize the expected payoffs through inducing perceived asymmetry in the contest, that is, by inducing the WCE. In this case, however, the optimal signals work through an unlevel-playing-field channel, which suggests a loss of efficiency from the information exchange. The optimal signals that maximize each players’ expected payoff induce efficient allocation if and only if they are sufficiently heterogeneous.====In this paper, we focus on payoff maximization instead of effort maximization in contests (Kuang et al., 2019, Lu et al., 2018, Serena, 2017) for the following reasons. First, effort maximization cannot be tackled by minimizing the expected payoff of players. Since there exist non-monotonic equilibria in the all-pay auctions, the minimization of the expected payoff may be associated with efficiency loss, which has adverse effects on the expected effort. Second, Kuang et al. (2019) study optimal information design to maximize the total expected effort in the same setting using the Bayesian persuasion approach. To bridge a gap in the literature and differentiate from the existing studies, we focus on information exchange between players, and as a result, the objective of information design is naturally payoff maximization. In fact, the current paper’s approach is consistent with models in the literature (Lu et al., 2018, Azacis and Vida, 2015, Fang and Morris, 2006), and the optimal signals characterized are easy to interpret.==== This paper contributes to the literature on information disclosure in contests by introducing an enlarged space of disclosure policies for analysis. Most of the existing studies on disclosure policies in static contests focus on the comparison between full disclosure and no disclosure (Aoyagi, 2010, Fu et al., 2011, Fu et al., 2014, Fu et al., 2016, Chen et al., 2017, Chen, 2019, Chen et al., 2019). In Tullock contests (Tullock, 1967), Serena (2017) takes one step further by introducing partial disclosure policies. Each partial disclosure policy is a mapping from a set of anonymous type profiles,====
 ====, to a binary decision between ==== (====) or ==== (====) each profile to both players.====
 Lu et al. (2018) examine the same disclosure policies in an all-pay auction and show that ====, i.e., disclosing the type profile only when both players are low type, maximizes players’ expected payoff. The key innovation of the current paper is enlarging the space of disclosure policies to include type-profile dependent stochastic disclosure policies (i.e., any realized type profile ==== is revealed with a probability in between 0 and 1), which include the partial disclosure policies in the previous studies as special cases.==== In the enlarged space of disclosure policies, ==== remains to be optimal.====Azacis and Vida (2015) study collusive communication through a benevolent information center between two bidders in the first-price auction with binary IPV. The fact that a low-value player in the first-price auction bids his value in all equilibrium enables the authors to consider a more general signal structure, which allows an arbitrary number of signal realizations and partial correlation of signals. Hence, the current paper is similar to Azacis and Vida (2015) except that the former focuses on public signal disclosure in the all-pay auction due to our specific research questions and mathematical difficulties.====The current paper is also related to the literature that considers bidders’ incentives to share/disclosure their own exclusive information to their competitors in auctions (Ford et al., 2020, Denter et al., 2018, Ewerhart and Grünseis, 2018, Wu and Zheng, 2017, Kovenock et al., 2015) and the literature on information sharing between competing firms (Vives, 1984, Gal-Or, 1985, Gal-Or, 1986, Raith, 1996).====Kuang et al. (2019) study optimal information design to maximize the total expected effort in an all-pay auction contest with two-sided incomplete information using the Bayesian persuasion approach (Kamenica and Gentzkow, 2011). The space of disclosure policies in such an environment is, in general, very large, and Kuang et al. (2019) conduct their analysis on public and private disclosure separately.==== On the Bayesian persuasion in Tullock contests, Zhang and Zhou (2016) focus on public disclosure in a one-sided private information setting.",Optimal information exchange in contests,https://www.sciencedirect.com/science/article/pii/S0304406821000811,17 April 2021,2021,Research Article,102.0
"Jia Hao,Sun Ching-jen","Department of Economics, Deakin University, 1 Gheringhap Street, Geelong, VIC 3220, Australia,Department of Economics, Deakin University, 70 Elgar Road, Burwood, VIC 3125, Australia","Received 14 August 2020, Revised 9 February 2021, Accepted 15 March 2021, Available online 15 April 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102515,Cited by (0),"We analyze a three-stage game where an organizer sets an entry fee for a Tullock contest event, and a finite population of homogeneous agents simultaneously decide whether to participate or not. We show that in the unique symmetric subgame perfect ","Many contests involve an uncertain number of contestants. Real-life examples from qualifying school tournaments (chess, golf, snooker, etc.) to the World Series of Poker (WSOP) suggest that the number of participants in a contest depends on the value of the prize as well as the entry fee. Typically, a higher grand prize attracts more players, while entry fees help the contest organizer to screen out less capable or insincere applicants and ensure the competition is of high quality. Public procurements often share the same structure.====In the contest literature, two types of models have been used in studying contests with an uncertain number of contenders. The first type, known as ==== models, take the approach of the population uncertainty notion of Myerson (1998) and analyze contests with a stochastic number of players, which is determined by an exogenous probabilistic distribution. For instance, Myerson and Wärneryd (2006) analyze a Tullock contest model with infinitely many potential players. Based upon a rather strong assumption that the expected number of participants being a publicly known constant, they show that if it is known for certain that there will be at least one participant, then total equilibrium effort is strictly lower in a contest with population uncertainty than in a certain contest with the same expected number of players. Münster (2006) considers a model where contestants are drawn from a candidate pool of a certain size. The chance of a candidate being selected is fixed, hence the number of contestants is a binomial distributed random variable. His study focuses on the players’ risk attitudes and shows that equilibrium efforts are lower under risk aversion if and only if the expected fraction of participants is low. Lim and Matros (2009) further derive the result for a Tullock contest with a binomial distribution of contestants. They demonstrate that ex-post over-dissipation is a feature of the pure-strategy equilibrium in their model and the contest designer can benefit in terms of higher total effort from revealing the number of contestants. A more recent paper by Kahana and Klunover (2015) examines that the findings of Myerson and Wärneryd (2006) are robust in contests where the number of contestants is a Poisson random variable with at least two expected contestants and a nonincreasing return to effort.====The second type of models emphasize that the entry decision should be endogenously made by potential contestants, i.e., the players are allowed to choose whether to participate in a contest. This type of models, known as ==== models, are postulated in Corcoran, 1984, Higgins et al., 1985, and Corcoran and Karels (1988) within a rent seeking framework. In those papers, the analyses are mainly based on imposing a long-run equilibrium zero-profit condition, which requires that the agents should always participate in the game until all rents are dissipated. Specifically, zero profit occurs when the total effort from all the contestants is equal to the prize value. The long-run equilibrium zero-profit condition has been challenged and criticized by several scholars such as Michaels (1988), Baye et al. (1993), and Amegashie (1999). In an all-pay auction setting,  Thomas and Wang (2013) examine the optimal punishment with endogenous entry, where the participant with the lowest performance may be punished. In particular, they derive a sufficient condition for the optimal punishment to be zero if the objective of the organizer is to maximize the expected total effort. From a mechanism design perspective, Fu et al. (2015) abandon the zero-profit condition and consider the contests where the number of players is unavailable to the contestants throughout the games. They show that a uniform upper bound for expected total effort can be achieved through a Tullock contest with a single contingent prize, which adopts compatible bundles of success function and entry fees/subsidies.====The current paper examines a principal–agent model with externality and focuses on the endogenous entry in a contest framework. It is worth noting that, among all the studies mentioned above, the models in Higgins et al. (1985) and Fu et al. (2015) are the closest to the one examined in this paper. However, our work differs from them in various ways. First of all, while the current paper shares a similar framework with Higgins et al. (1985), the results are obtained without imposing the long-run equilibrium zero-profit condition. So our work can be viewed as a generalization of Higgins et al. (1985). Secondly, the assumption that the number of contestants becomes public information before the tournament stage distinguishes the current paper from Fu et al. (2015), which assumes that this piece of information remains unavailable to the contestants throughout the game. As shown in the next section, this assumption drastically changes the nature of the payoffs in the tournament stage, hence affects the subgame perfect equilibrium of the dynamic game discussed in the current paper. We argue that this assumption is also natural as many contests with endogenous entry (such as Marathon, dancing auditions, etc.) do feature this property.====More specifically, this paper studies the optimal entry fee-prize ratio by postulating a three-stage game, where homogeneous, risk-neutral agents from a given population endogenously decide whether to participate in a Tullock contest. We show that a unique symmetric subgame perfect Nash equilibrium exists in this game. When the population size is greater than one, despite the absence of information asymmetry and risk aversion, the contest organizer always obtains a positive expected payoff by setting the entry fee-prize ratio at an optimal level between zero and one.====Furthermore, we show that the number of contestants converges to a Poisson distributed random variable as the population size approaches infinity. Our results establish links between the random participation models and the endogenous entry models. In other words, the current paper shows that the exogenously determined probabilistic distributions (either binomial or Poisson) can be unified in an endogenous entry framework. In particular, the binomial and Poisson distributions in stochastic participation models correspond to the two special cases in a contest with endogenous entry when the population sizes are finite and infinite, respectively.====The rest of the paper proceeds as follows. In Section 2, we setup a generic three-stage endogenous entry game, and characterize the corresponding subgame perfect Nash equilibrium. Section 3 concludes the paper. All the lengthy proofs are contained in the Appendix to keep the narrative flowing smoothly.",The optimal entry fee-prize ratio in Tullock contests,https://www.sciencedirect.com/science/article/pii/S0304406821000781,15 April 2021,2021,Research Article,103.0
"Chambers Christopher P.,Moreno-Ternero Juan D.","Georgetown University, United States of America,Universidad Pablo de Olavide, Spain","Received 28 July 2020, Revised 25 January 2021, Accepted 10 March 2021, Available online 15 April 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102517,Cited by (1),"We explore the implications of three basic and intuitive axioms for income redistribution problems: ====, ==== and ====. The combination of the three axioms characterizes in the two-agent case a large family of rules, which we call ==== rules. For each level of total income in society, a threshold is considered for each agent. It is impossible for both agents to be below their respective thresholds. If an agent’s income is below the threshold, the difference is redistributed from the other agent; otherwise, the rule imposes laissez-faire.","Economics is essentially about scarcity. The problem of dividing scarce resources is faced by many on a daily basis, and the media is plagued with references to it, especially now in the aftermath of the COVID-19 pandemic. Albeit to a lower extent, attention to this problem is also being given within the scientific literature (e.g., Chambers and Moreno-Ternero, 2019). Income taxation and redistribution are basic tools to alleviate the negative consequences of scarcity. In this paper, we focus on the specific problem of income redistribution, which we approach with the axiomatic method. We consider a stylized model in which an income profile reflects the taxable and observable income of two agents. The issue is to construct rules that transform the given income profile into another income profile, with the requirement that nothing is wasted in the redistribution process, and that no agent ends up with a negative post-tax income. It is thus reminiscent of the seminal model introduced by O’Neill (1982) to analyze claims problems, surveyed by Thomson (2019). Young (1988) reinterpreted the same model to analyze taxation problems.==== Here, we consider taxation problems with an important proviso: a zero tax revenue to be raised, and thus, negative taxes are possible. As such, our problems could be considered a case of the generalized claims problems as studied by Ju et al. (2007).====We consider three basic and intuitive axioms for this model: ==== (small changes in the data of a problem should not lead to large changes in the solution), ==== (transferring some income before redistribution takes place does not increase income after redistribution) and ==== (no further redistribution takes place once a solution is obtained). We show that the combination of the three axioms characterizes a large family, which we call ==== rules, that guarantee partial redistribution for unequal incomes. Our rules therefore convey a long-standing concern in the political philosophy literature that can be traced back to Rawls (1971) or, more recently, to Van Parijs and Vanderborght (2017). Lower bounds also have a long tradition within the literature on fair allocation (e.g., Thomson, 2011) and we have also explored them recently for taxation problems (e.g., Chambers and Moreno-Ternero, 2017).====Each rule within the family we characterize is described by two functions (over aggregate income) which determine the region where partial redistribution is imposed. The family is wide enough to encompass (while considering specific functions) rules ranging from ==== to its polar ==== rule.",Bilateral redistribution,https://www.sciencedirect.com/science/article/pii/S030440682100080X,15 April 2021,2021,Research Article,104.0
Varvarigos Dimitrios,"School of Business, Department of Economics, Finance and Accounting, University of Leicester, Mallard House (Brookfield campus), 266 London Road, Leicester LE2 1RQ, United Kingdom","Received 19 June 2020, Revised 29 January 2021, Accepted 6 March 2021, Available online 19 March 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102514,Cited by (1),"I construct a model where upstream income transfers, from adult children to their old parents, are driven by a culture of strong family ties. This evolves endogenously, through a process of intergenerational cultural transmission. The two-way causal link between economic and cultural change can be a strong enough force to offset cultural substitution, thus generating path-dependent outcomes. These outcomes show that economic development is negatively related with upstream intergenerational transfers, and with the strength of family ties. On the one hand, the economy may follow a convergence path towards a low level ","The subject of private intergenerational transfers has gained momentum in the analysis of economic growth and development. However, the vast majority of existing studies have restricted attention to downstream wealth transfers within the family, i.e., from parents to their progeny, with the purpose of establishing a link between the dynamics of lineage wealth, inequality, and economic development (e.g., Banerjee and Newman, 1993, Galor and Zeira, 1993, Zilcha, 2003, Galor and Moav, 2004). By comparison, the number of studies considering upstream transfers, from adult children to their old parents, is rather limited. This neglect is not warranted.====Several authors have argued that the financial support that adults provide their old parents is a salient feature of developing economies (e.g., Caldwell, 1976; Clay and Vander Haar, 1993, Sloan et al., 2002, Payne et al., 2019), while others have shown that the anticipation of such transfers can alter the recipients’ decision making on aspects that are pertinent to economic growth and development (e.g., Laitner, 1988, O’Connell and Zeldes, 1993, Blackburn and Cipriani, 2005).====The objective of this study is to contribute towards filling this void in the literature. It presents a growth model where the provision of income from adult children to their old parents, is driven by attitudes, norms and customs on the strength of family ties.==== These are treated as cultural traits that evolve intergenerationally, through a process of cultural transmission. The model shows that economic development is negatively related with upstream intergenerational transfers, and with the strength of family ties. The key factors of the model’s mechanisms are the impact of income transfers on parents’ saving behaviour, the declining significance of these transfers for parents’ overall income in the process of economic development, and the influence of these characteristics on the intensity of cultural instruction by parents with different attitudes on family ties. Furthermore, the study’s results offer more general implications about the role of economic factors for the evolution and establishment of cultural traits among the population.====The aspect of cultural transmission is a significant point of departure of my framework, in comparison to existing theories of economic growth that incorporate intergenerational income transfers among family members. This approach is justified though, given the focus on upstream intergenerational transfers and their underlying characteristics. Indeed, there is a plethora of arguments, offering credence to the view that a strong sense of family ties is a fillip to moral considerations of filial piety, which provide individuals with an incentive to offer financial support to their parents. Chang (2013) presents empirical evidence on a positive relation between family ties and intergenerational transfers from adult children to their old parents, whereas Costa-Font (2010) presents evidence that strong family ties generate expectations to individuals, regarding the potential support from their progeny, prompting them not to insure against the costs of their future long-term care.==== The study by Jellal and Wolff (2002) offers empirical support to the idea that the altruistic motives behind upstream intergenerational transfers are culturally transmitted from the older to the younger generations.====To some extent, these observations echo Becker’s (1992) argument that “====[====] ====”==== These ideas become even more pertinent, once we consider evidence that the culture of strong family ties is more prevalent in developing countries (e.g., Alesina and Giuliano, 2010, Alesina and Giuliano, 2014) i.e., those countries in which we observe significant transfers from adult children to their old parents.====My model is consistent with the previously discussed ideas. There are two cultural traits determining agents’ attitudes on family ties, and only those who are inculcated with a strong sense of family ties provide income transfers to their old parents. These transfers are given as ‘gifts’, i.e., they are not ====. Following the pioneering work of Bisin and Verdier (2001), I assume that the main underlying motive why parents actively undertake the cultural instruction of their children is because they use their own subjective viewpoint as the benchmark for evaluating their children’s personalities — ultimately, they want their children to uphold the same attitudes and values as they do.==== Nevertheless, the link between the strength of family ties and the potential receipt of financial support from adult children, triggers additional parental considerations of a more opportunistic nature: Given the prospect of receiving income from their children, parents who uphold a strong sense of family ties have an additional motive – and, therefore, intensify their efforts – to inculcate their offspring with the same attitudes, whereas parents whose sense of family ties is weak have a reduced incentive – and, therefore, abate their efforts – to instil their own attitudes in their offspring. As the economy grows, however, the importance of intergenerational income transfers for parents’ overall income declines. Consequently, economic development is associated with less intense instruction towards a culture of strong family ties and, therefore, a gradual reduction in the population share of individuals who uphold such attitudes and values. These outcomes cause an overall reduction of upstream intergenerational transfers in the process of economic development.====The impact of economic development on the population’s adherence to strong family ties is not the only mechanism that links economic dynamics and cultural change. On the contrary, the causal effect runs in the opposite direction as well: A shift in the distribution of traits among the population, favouring a limited adherence to strong family ties, and the corresponding decline in the overall flow of income transfers from adult children to their old parents, are factors that promote capital accumulation and economic growth. The reason is that parents who anticipate the receipt of income from their children, hence increased consumption in maturity, reduce their saving when young in an effort to smooth their intertemporal consumption profile. This mechanism is intuitive and consistent with the existing results on the relation between upstream income transfers and saving behaviour — see, for example, Laitner (1988) and O’Connell and Zeldes (1993). It is also consistent with the results presented in Costa-Font (2010).====These arguments reveal that a ==== underlies the mechanisms of the model: Limited adherence to strong family ties reduces upstream transfers, thus promoting saving and capital accumulation, while the processes of capital accumulation and economic growth bring forth an endogenous cultural change towards limited adherence to strong family ties, hence to an overall reduction of upstream transfers. This two-way causal link between economic development and the cultural evolution of attitudes on family ties, can generate path-dependent equilibria, i.e., outcomes that are not only sensitive to structural parameters, but also to the economy’s history. On the one hand, the economy may follow a convergence path towards a low level of economic development, where adherence to strong family ties is the dominant characteristic of a culturally homogeneous population, and where the overall flow of transfers from adult children to their old parents is substantial. On the other hand, the economy may follow a different path of convergence towards a higher level economic development, where the population is more diverse in terms of their attitudes on family ties, and where the overall flow of transfers from adult children to their old parents is lower by comparison. Thus, the different long-term equilibria are consistent with the negative correlation between (i) economic development and upstream income transfers within families, and (ii) economic development and the strength of family ties.====An alternative way to understand the relevance of family ties and upstream intergenerational transfers for comparative economic development is through a historical comparison of European and East Asian regions. Several researchers (e.g., Todd, 1990, Duranton et al., 2009, Greif and Tabellini, 2010) argue that the family culture that historically prevailed in Northwestern Europe is a nuclear/libertarian one, where children develop a greater sense of independence and leave their parental home early in their adulthood. In contrast, the stem/communitarian family, which dominated East Asian culture, entailed stronger norms of parental authority and filial obligation. Zhang (2019) provides evidence that the communitarian family culture is positively related with financial transfers from adult children to their parents, therefore indicating that differences in family culture could explain why upstream transfers are more significant in some East Asian countries – according to Lin and Yi (2013), the dominant form of intergenerational support in these regions – compared to the more developed European countries, in which upstream transfers are less prevalent (Attias-Donfut et al., 2005).====There are additional, indirect mechanisms that link the process of development, the strength of family ties, and the level of upstream intergenerational transfers. For example, economic conditions and the prevailing family culture in Northwestern Europe facilitated earlier urbanisation (in comparison to East Asian regions) which, in turn, weakened family ties (Greif, 2006). At the same time, however, this enabled the earlier and more widespread establishment of – as well as the increased trust to – institutions such as social security (Korpi, 2000, Caucutt et al., 2013), which, according to empirical studies, could be a substitute for private upstream transfers within the family (Sloan et al., 2002, Zhang, 2019).====Duranton et al. (2009) explicitly argue that differences in family culture could explain persistent differences in economic conditions, claiming that “==== [====] ====” (Duranton et al., 2009; p. 32). It is worth mentioning, however, that the emergence of path-dependent outcomes in my model has more general implications for the dynamics of cultural transmission – implications that go beyond the specific example of the nexus between economic development, family ties, and upstream intergenerational transfers.==== To see this, note that in archetypal economic models of cultural transmission, like the one pioneered by Bisin and Verdier (2001), the population share of individuals who possess a specific cultural trait (i.e., in terms of preferences, attitudes, values, norms etc.) is a substitute to parents’ own efforts to instruct their children towards the adoption of this trait. Under such circumstances, which Bisin and Verdier (2001) summarised through the term ====, the dynamics of cultural transmission do not display path-dependence. Instead, the long-run outcome is a unique equilibrium of cultural diversity: This equilibrium depends on the model’s structural parameters, but it is not sensitive to history (Bisin and Verdier, 2001, Bisin and Verdier, 2008). In my model, path-dependence and a long-run outcome of cultural homogeneity are possible, despite the presence of cultural substitution. There are two key features responsible for this outcome: First, the distribution of cultural traits is not the only state variable in the model. On the contrary, there is another, economic-related state variable (in this case, the stock of capital). Second, the evolution of these two state variables is subject to the complementarity that I described previously: Capital formation and economic development induce a cultural change towards reduced adherence to strong family ties, while the lower population share of those who conform to a culture of strong family ties promotes the formation of capital and, therefore, the process of economic development. If this underlying cultural-economic complementarity is strong enough, it can dominate the opposing forces of cultural substitution, thus triggering and sustaining the sequence of reinforcing effects between cultural and economic dynamics, and ultimately laying the foundations for path-dependence.====In this respect, my study evokes one of the underlying messages in Francois and Zabojnik (2010): Their model also raised awareness to the possibility that the complementary nature between economic development and cultural change can result in path-dependent outcomes.==== However, the model of Francois and Zabojnik (2010) rules out cultural substitution, as it does not account for the cost borne by parents in their effort to instil their own trait in their offspring. My model shows that the complementarity between economic development and cultural change is strong enough to offset the presence of cultural substitution in generating divergence through history-dependent outcomes.====Another strand of literature that merits mentioning, given the characteristics of my study, is the one that investigates two-sided altruism in overlapping generations economies. The focus of this literature is different from my study: It is not explicit on the cultural aspects that underlie the extent of altruistic motives and the level of intergenerational transfers. Furthermore, this literature investigates and emphasises the equilibrium and welfare implications of intergenerational transfers (including upstream ones) rather than their changing patterns in the process of economic development. For example, Kimball (1987) studies the implications for dynamic inefficiency, while Raut (2006) examines issues of optimality, given the consumption externalities that two-sided altruism entails. One exception is the study of Blackburn and Cipriani (2005) who also identify the reduced significance of upstream transfers in more developed economies. Nevertheless, their focus is on aspects of demographic change — they do not consider endogenous cultural change, saving and capital formation.====The remainder of this study is structured as follows: Section 2 describes the characteristics of the economy. In Section 3 I present the detailed analysis of cultural transmission, while in Section 4, I analyse the process of capital formation. Section 5 analyses the model’s joint dynamics, and presents the main implications for economic development, the evolution of family ties, and upstream intergenerational transfers. Section 6 discusses and concludes the paper.",Upstream intergenerational transfers in economic development: The role of family ties and their cultural transmission,https://www.sciencedirect.com/science/article/pii/S030440682100077X,19 March 2021,2021,Research Article,105.0
"Accinelli E.,Muñiz Humberto","Facultad de Economía de la UASLP, Mexico,Insituto Potosino de Investigación Científica y Tecnológica, Mexico","Received 27 March 2020, Revised 16 February 2021, Accepted 6 March 2021, Available online 18 March 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102513,Cited by (2)," for this dynamic system, which will coincide with the evolution of the economy, that is, the evolution of prices and equilibrium allocations.====The investment decisions of the administrators of the companies will change the distribution over the set of existing productive branches, which in turn will produce changes in the ==== of consumers who are also shareholders of the companies and then as a consequence, their demand will change, and therefore the equilibrium allocations and prices will too.====In most cases, these decisions lead to an improvement in the efficiency of the productive side of the economy and an increase in the welfare of the economy as a whole, but, as we will show, under some particular circumstances, even when it comes to rational decisions from the point of view of administrators, this can lead to undesirable repercussions on the welfare of consumers. Besides, in a neighborhood of a critical economy, even when these decisions may involve small changes in the distribution of companies, they can cause abrupt and unexpected changes in the behavior of the economy, or in other words, they can cause an economic crisis. These are characterized by large changes in the prices, in the demand, and in the supply of goods. In contrast, in a sufficiently small neighborhood of a regular economy, small changes in the distribution of firms produced by the investment decisions of managers do not lead to large changes in the subsequent behavior of the economy. We will exemplify these statements with several numerical examples.","General equilibrium theory is a centerpiece of modern economic theory, but it has not provided a good dynamic for modeling economic evolution. This difficulty lies in the generality of its initial assumptions; in particular in the modest assumptions about the preferences of economic agents, or what is the same, in the structural lack of information on individual preferences. The generality of the excess demand function (see Mantel (1974)), originated in the aforementioned structural lack of information on preferences, prevents this function from becoming a sufficient functional field to characterize the evolution of the economy towards or through equilibrium prices, even supposing that the economies verify the uniqueness of the equilibrium. That is until now there is no explanation of how the changes in prices and in the equilibrium allocations of each economy take place over time.====In this paper, we will build a dynamic taking into account that over time, poor companies must adapt and imitate the most successful, changing as a result of this movement, the underlying short-term equilibrium. It is this long-term adaptation process that we address in this document. More specifically, we consider economies in equilibrium, in other words, economies where prices, allocations, and production plans correspond to Walrasian equilibria. But this cannot be the end of the story; if there are heterogeneous profits, the least profitable companies will seek to replicate the most profitable ones. As in Zame (2007), we consider that economic agents can change their strategies, consumption decisions, and production plans from one period to another. Based on this statement, we introduce a dynamic system that reflects the repercussions on the economic welfare of the rational decisions made individually by company managers. This fact implies that managers become critical influencers in the emerging Walrasian equilibrium.====Following Schumpeter, our model assumes that individual decisions are sufficiently important to affect the entire economy. We introduce an endogenous model of evolution but, unlike the previous works, see for instance Aghion and Howitt (1992), we do not make reference to the creative destruction or innovation as an engine for the economic evolution. The managers of the firms, seeking to maximize profits, decide to produce according to one or another technology available at a given time.====According to this approach we will proceed following two steps.====A brief consideration of the dynamics of the replicator is necessary. The replicator dynamics is useful for expressing the evolutionary dynamics of an entity called a replicator that has the means of making more or less accurate copies of itself. The replicator can be a gene, an organism, a game strategy, a belief, a technique, a convention, or an institutional or cultural form.====In our approach, managers of the firms consider different technologies or production sectors to invest in. They are seeking to maximize the payoff of their investments, thus they will choose to invest or to produce according to the most profitable technologies or branch of production. The successful decisions will be replicated. While the replicator is a continuous dynamic, it applies to essentially discrete models. A large but finite population of replicators is assumed, in which different types meet in proportion to their share in the population. The relative fitness of a replicator determines its relative success in reproduction. The general idea is that replicators whose fitness is larger (smaller) than the average fitness of the population will increase (decrease) their share in the population. It refers basically to situations in which the changes are discrete. However, the fact of considering that the populations are sufficiently large allows considering the advent of arbitrarily small percentage variations. This assumption, together with the fact that the payment or utility functions, originally defined with domain in the set of positive integers, can be extended to continuous domains without losing their intuition, give rise to the possibility of considering evolution as a continuous time change process. See, for example, Weibull (1997).====In this work, we assume a (relatively) large but finite population of firms distributed in a finite set of branches of production. This distribution changes according to the profits available in each time in each branch of production. The branches of production or technologies that yield the highest profits will be those that will receive most of the investment. This fact modifies the distribution of the firms on the set of technologies or production branches and consequently modifies the behavior of the economy. As a result of the changes in the productive sector, the wealth of consumers, given that they are the shareholders of the firms, will be modified. In general, these changes are not big, and the structure or the behavior of the new economy is not very different from the structure of the preexistent economy. However, small modifications can lead to large deviations in the behavior of the economy in the neighborhood of a singular economy. So singular economies can be identified with the threshold of the economic crises. Two interesting results of this work are the following:====A question that arises is, to what extent the possible negative effects on the consumer’s welfare of some rational decisions can be a justification to consider the development of industrial public policies as the macroprudential policies for the financial sector of the economy, considered for example in Carrillo et al. (2017) and Bodenstein et al. (2019).====The paper is organized as follows. In Section 2 we introduce the definition of the ownership private production with production branches, and the main definitions of the model. In Section 3 we consider the main properties of singular and regular economies in the manifold given by distributions of firms over the set of branches of the industry and equilibrium prices. In Section 4 we analyze the different reactions of the singular and regular economies after a perturbation in its fundamentals. In Section 5 we consider the evolution of the economies, and we introduce the replicator dynamical system. We consider some examples to enlighten the theory previously considered in this work. In Section 7 we offer some conclusions.",Evolution in a General Equilibrium framework,https://www.sciencedirect.com/science/article/pii/S0304406821000768,18 March 2021,2021,Research Article,106.0
Fraser Clive D.,"Department of Economics, Finance and Accounting, The Business School, University of Leicester, Leicester LE1 7RH, UK","Received 30 December 2019, Revised 12 January 2021, Accepted 27 February 2021, Available online 18 March 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102510,Cited by (0),"In many contexts with endogenous physical risks – e.g., households, neighbourhood traffic calming, production quality control – risk reduction is a ==== for safety to increase with the number protected via both monotone comparative statics methodology and a “first-order” approach. I utilise a recursive decomposition of a covariance involving a ","Self-protection (SP), in the sense of Ehrlich and Becker (1972), occurs if an agent’s action reduces the probability of a loss that affects their welfare. My purpose in this paper is primarily methodological: to develop tools to analyse some situations in which an agent’s actions protect several people simultaneously. For concreteness, I study a household’s incentive for SP that reduces the endogenous risk of death or injury to its members. I mainly focus on contexts where SP is locally a pure public good in that SP reduces equally the physical risks to several household members at once and its effectiveness does not depend on the number being protected. For example, in the household, the probability of accidental death or injury to any and every child might be a scalar that depends on the parents’ protective efforts. Then, for a household with children, if the risk to each child is i.i.d. (discussed further and relaxed below), the distribution of family size or uninjured members at the end of a period is binomial. Assuming this, I exploit the simple intuitive binary structure of Bernoullian​ variables to study how a decision maker’s incentive to engage in risk reduction changes with the number protected by this activity. I analyse the household’s incentive to make such risk-reducing efforts using expected utility theory. Utility is taken to be monotonically decreasing in the number of accidents, all else equal, and I assume that the disutility of risk-reducing effort is separable.====I show that the marginal benefit of self-protection is proportional to the covariance between the number of losses in the relevant population and the associated utility level; if this covariance decreases as the population size increases, a higher protective effort level becomes optimal. It might appear intuitive that, if protection is locally a pure public good, then as the population protected increases, the marginal benefit of protection increases also, making investment in protection more worthwhile and increasing equilibrium protection. However, this is not unambiguously true without restricting preferences and the feasible risk level. To show these results, I derive a simple recursion formula for the covariance between utility and accidents that greatly simplifies subsequent analysis incorporating it.====An extensive, empirically-orientated, urban economics literature studies how population size and growth affect, among other things, per capita public spending on public safety. Regarding safety, it focuses mainly on inter-jurisdictional comparisons of protection against crime and fire. It stresses the importance of a community’s socio-economic characteristics and the nature of the congestion function for shared goods, hence their degree of publicness, for determining expenditure. See, e.g., Clark and Cosgrove (1990), Ladd (1992), McGreer and McMillan (1993) – who also consider highway maintenance – Schwab and Zampelli (1987) and Allen (2018).====An early notable exception to this empirical focus is Kolm (1976). In his theoretical analysis, the physical risk someone faces depends on two types of safety expenditures: first, one with an effect purely private to the individual; second, one that also reduces the risks for others and so has a degree of publicness. He considers the socially optimal rate of substitution between these types and the dependence of the optimal relationship between impurely public and purely public safety expenditure on whether the former complements or substitutes for purely private safety expenditures. However, Kolm does not study directly how optimal purely public safety expenditure changes as the population protected by it increases, my main concern. This neglect of the influence of the size of the protected population on optimal safety decisions is pervasive in both the theoretical literature and, as Viscusi (1995) observes, policy-making.====Several more recent papers examine public good aspects of self-protection (e.g., Kunreuther and Heal, 2003, Heal and Kunreuther, 2005, Muermann and Kunreuther, 2008, Lohse et al., 2012).==== These study scenarios where several agents with interdependent risks make safety decisions. Hence, they focus on provision of self-protection as a local public good ==== a group, and especially the free-riding that might then arise. By contrast, this paper considers when a single decision-maker, or decision-makers acting collectively, make a safety decision that influences the risk faced by several people. Thus, my focus is on protection as a local public good ==== a group and I ignore free-riding incentives.==== However, in principle, my framework can be extended to consider such incentives, with the agents analysed in this paper each being an agent in a Nash equilibrium model.====Section 2 of this paper introduces my model for endogenous physical safety in a household. I show that the covariance between utility and the number of accidents is important for how the incentive to engage in risk reduction varies with the size of the population protected. Section 3.1 gives my central result: a recursive relationship linking the covariance between utility and the number of accidents for protected populations of different sizes. I use this relationship to give sufficient conditions for this covariance to decrease (become more negative) with the size of the protected population. In the Appendix, I employ an alternative approach based on first-degree stochastic dominance (FSD) to show that my approach using this recursion relationship yields stronger sufficient conditions than the latter.====I next employ the preceding analysis to get sufficient conditions for protective effort to increase with the size of the population protected via two alternative routes: (a) Section 3.3 uses the method of monotone comparative statics, which does not rely on the optimisation problem being concave; (b) Section 3.4 employs a “first-order” approach. Here, as SP problems are not generally concave, I treat the second-order condition for optimal safety effort in detail, again via FSD. Both (a) and (b) rely on a corollary of the covariance recursion relationship derived as my main theorem in Section 3.1, highlighting its significance.====Section 4 considers some extensions of my basic model. These include: (i) correlated risks; (ii) income effects in effort on risk-reduction; (iii) a convex instead of linear cost of effort; (iv) SP as a club good — i.e., SP is subject to congestion in that the effort cost of protecting children equally at a given level increases with the number of children protected. In (i), I show that, if protective effort does not influence the correlation between risks, the results for i.i.d. risks generalise to correlated risks. Extension (ii) shows that protective effort is a normal good if household size and income are Edgeworth–Pareto complements (equivalently, if utility is super-modular in household size and income). In (iii), I show that, if the disutility of effort is convex instead of linear in effort, this leaves my qualitative results unchanged. Lastly, in (iv), I show that, all else equal, if SP is a club good rather than a local pure public good, there is ambiguity about the change in SP within the household as household size increases. Section 5 concludes the paper. The Appendix contains proofs.",Protection in numbers? Self-protection as a local public good,https://www.sciencedirect.com/science/article/pii/S0304406821000604,18 March 2021,2021,Research Article,107.0
"Torre Davide La,Liuzzi Danilo,Marsiglio Simone","SKEMA Business School - Université Cǒte d’Azur, Sophia Antipolis Campus, France,University of Milan, Department of Economics, Management and Quantitative Methods, Milan, Italy,University of Pisa, Department of Economics and Management, via Cosimo Ridolfi 10, Pisa 56124, Italy","Received 20 January 2020, Revised 4 February 2021, Accepted 28 February 2021, Available online 16 March 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102511,Cited by (9),"We analyze the implications of transboundary pollution externalities on environmental policymaking in a spatial setting, in which pollution diffuses across the global spatial economy independently of the specific location in which it is originally generated. This framework gives rise to a simple regional optimal pollution control problem allowing us to compare the global and local solutions in which, respectively, the transboundary externality is and is not taken into account in the determination of the ==== by individual local policymakers. We show that it is not obvious that transboundary externalities are a source of inefficiency per se since this is strictly related to the spatial features of the initial distribution of pollution. If the initial pollution distribution is spatially homogeneous then the local and global solutions will coincide and thus no efficiency loss will arise from transboundary externalities, but if it is spatially heterogeneous the local solution will be suboptimal and thus a global approach to environmental problems will be needed to achieve efficiency. From a normative perspective, in this latter (and most realistic) case we also quantify the amount of policy intervention needed at local level in order to achieve the globally desirable goal of pollution eradication in the long run. Our conclusions hold true in a number of different settings, including situations in which the spatial domain is either bounded or unbounded, and situations in which macroeconomic–environmental feedback effects are taken into account.","After decades of debates it has finally grown a shared consensus on the fact that anthropogenetic activities, and in particular economic activities, are an important determinant of environmental problems and climate change (Oreskes, 2004, IPCC, 2014). Policymakers need thus to critically intervene to reduce the accumulation of polluting emissions in the atmosphere in order to ensure that economic development is effectively addressed towards a sustainable path. However, understanding how to determine the optimal size of these policy interventions is not simple at all, especially because the stock of pollution in specific locations is strongly affected by the level of emissions generated in other locations as well, a phenomenon referred to as transboundary externality (Ansuategi and Perrings, 2000, Ansuategi, 2003). In order to ensure that such externalities are effectively accounted for in the design of environmental policy it has often been suggested that local policymakers should adopt a global perspective and collaborate with one another. This argument has been summarized in the popular motto ==== (or ====). Despite the fact that the benefits of such a collaborative approach to policymaking are quite clear, understanding how to effectively implement collaboration is not simple at all, since quantifying the size of such transboundary externalities and what individual policymakers should do is all but trivial. The goal of this paper is to partly contribute to this debate by formally analyzing the desirability of a think globally, act locally approach and the type of intervention that local policymakers should opt for in order to effectively implement it.====Specifically, we analyze a pollution control problem in the presence of transboundary externalities. On the one hand, several works have introduced transboundary externalities in the form of spatial spillovers in the contexts of capital accumulation (Brito, 2004, Camacho and Zou, 2004, Camacho et al., 2008, Boucekkine et al., 2009, Boucekkine et al., 2013a, Boucekkine et al., 2013b, Boucekkine et al., 2019) and environmental problems (Anita et al., 2013, Anita et al., 2015, Brock and Xepapadeas, 2008, Brock and Xepapadeas, 2010, Brock and Xepapadeas, 2017, Camacho and Pérez-Barahona, 2015, La Torre et al., 2015, La Torre et al., 2019a, La Torre et al., 2019b), but none in a pollution control framework. On the other hand, several works have discussed different types of pollution control problems under uncertainty and irreversibility (Bawa, 1975, Forster, 1972, Forster, 1975, Keeler et al., 1973, van der Ploeg and Withagen, 1991, Athanassoglou and Xepapadeas, 2012, Saltari and Travaglini, 2016, La Torre et al., 2017), but to the best of our knowledge, none has ever considered the effects of transboundary externalities as spatial spillovers. We therefore try to bridge these two branches of the economics literature by analyzing a finite time horizon framework in which the social planner tries to minimize the social costs of pollution (La Torre et al., 2017) by accounting for the fact that polluting emissions, independently of where they are originated, naturally spread in the atmosphere affecting the pollution stock of every location (La Torre et al., 2015). Such a setting allows us to compare the “global” and “local” solutions in which, respectively, the transboundary externality is and is not taken into account in the determination of the optimal environmental policy by single local policymakers. This approach gives rise to a simple “regional optimal control problem”, which is a specific type of spatially-structured optimal control problem aiming to understand whether the local solution is effectively optimal also globally (Lions, 1973, Lions, 1988). This class of problem was born as an application of the think globally, act locally motto in mathematics, thus our paper partly relates to this literature as well (see Anita and Capasso, 2018, for a recent survey of applications in epidemiology). To the best of our knowledge, ours is the first paper bringing a regional optimal control approach in economics and applying it to a transboundary pollution control problem.====Different from extant works on transboundary pollution externalities, which compare the spatial and a-spatial outcomes (La Torre et al., 2015, La Torre et al., 2019a, de Frutos and Martin-Herran, 2019b), our analysis is based on the comparison of local and global solutions, characterizing the decentralized and centralized outcomes, respectively (Ogawa and Wildasin, 2009).==== Specifically, the global solution represents a situation in which the global policymaker determines the optimal level of intervention in each single local economy by internalizing the effects of transboundary pollution externalities at local levels, while the local solution represents a situation in which single local policymakers determine their optimal level of local intervention by completing neglecting the existence of such transboundary pollution externalities. However, the evolution of the pollution stock, both in local and global solutions, depends on the level of intervention in each single location and on its transboundary effects, which are internalized in the determination of the global intervention level while they are not considered when determining the local one. Therefore, in our framework we always consider the existence of spatial spillovers (i.e., the economy is spatially structured and pollution spreads across space), thus our local solution does not represent the a-spatial equivalent of our global solution. Moreover, in the local solution single local policymakers do not interact strategically exploiting free-riding opportunities (i.e., there is no game structure), thus our local solution is not the non-cooperative counterpart of a global cooperative game equilibrium. Our setting thus significantly departs from some recent works which have analyzed the implications of transboundary pollution in differential game contexts to analyze the spatial implications of strategic interactions, comparing spatial vs a-spatial and cooperative vs non-cooperative equilibrium outcomes (de Frutos et al., 2019, de Frutos and Martin-Herran, 2019a, de Frutos and Martin-Herran, 2019b, Boucekkine et al., 2020). Our different perspective allows us to investigate whether despite the presence of transboundary pollution externalities in a spatial setting a local approach to policymaking may be enough to achieve globally desirable outcomes, and eventually how such desirable outcomes may be implemented through coordination at the local level.====The analysis of the local and global solutions in our simple regional optimal control problem, due to its peculiar quadratic–linear formulation, allows us to analytically derive two interesting sets of conclusions. First, we show that transboundary externalities are not a source of inefficiency per se since the spatial structure of the initial distribution of pollution plays a critical role in determining their efficiency implications. Indeed, if the initial pollution distribution is homogeneous the local and global solutions will perfectly coincide and thus no efficiency loss will arise from transboundary externalities. It is the existence of some heterogeneity in the initial pollution distribution that leads the presence of transboundary externalities to place a wedge between the local and the global solutions, requiring thus a global approach to environmental problems to restore efficiency, consistently with the think globally, act locally argument. To some extent these results are consistent with Ogawa and Wildasin’s (2009), who show, in a completely different and purely static setting, that despite the presence of transboundary externalities there exist situations in which local policymaking may give rise to efficient outcomes globally; different from them, we can clearly determine the situations in which this conclusion may or may not hold true, identifying the spatial features of the initial pollution distribution as the driver of this eventual lack of efficiency losses. Second, in the case of heterogeneity in the initial pollution distribution, which clearly represents the most realistic scenario from a real world perspective, we quantify how to implement coordination at local level in order to achieve globally desirable goals. Specifically, we show that if every local economy implements an environmental policy stringent enough (and we determine what stringent enough exactly means), then the global total pollution level will fall. If this is the case, then over the long run the entire global economy will be able to achieve a completely pollution-free status. These two results jointly suggest that from a normative perspective it may well be possible to determine whether and how global collaboration needs to be implemented locally in order to deal with global pollution problems. To the best of our knowledge such a neat and clear characterization of these issues from an economic point of view has never been provided before. We also show that our main conclusions do not depend on the peculiarities of our model’s formulation but they rather extend to more general and complicated frameworks.====This paper proceeds as follows. Section 2 discusses environmental policy in an a-spatial setting, where there are no transboundary externalities and there is no spatial dimension. This represents a benchmark for our following analysis, and it basically consists of a pollution control problem over finite horizon, in which the stock of pollution is entirely determined by local economic and environmental conditions. We derive the optimal policy which characterizes the environmental tax in a setting abstracting from a spatial dimension and thus from spatial spillovers. Section 3 extends our baseline model in order to allow for a spatial dimension in which transboundary externalities occur as a result of spatial pollution diffusion over a bounded spatial domain. We distinguish between the local optimal policy, in which the environmental tax is determined without considering the effects of transboundary pollution externalities, and the global optimal policy, in which the environmental tax is determined by internalizing such transboundary pollution externalities. We show that, only if there exists some spatial heterogeneity in the initial pollution distribution, the localized approach to environmental policy will give rise to suboptimal solutions, suggesting that coordination across local policymakers will be essential to deal with environmental problems only in such a setting. We also quantify the minimal level of intervention needed at local level for the global total level of pollution to decrease over time, and we show that if such a minimal level of intervention is implemented in every local economy then over the long run the entire global economy will be able to achieve a completely pollution-free status. The specific linear–quadratic formulation of our model allows us to derive the explicit local and global solutions for the spatio-temporal dynamic path of the environmental tax and the pollution level, which we illustrate through a numerical example to clearly visualize the extent to which the two solutions may differ. Section 4 presents some extensions of our baseline model showing that our main results straightforwardly apply also in more general frameworks, including traditional macroeconomic–environmental settings with macroeconomic and environmental feedback effects. Section 5 presents a further extension of our baseline model in which the spatial domain is unbounded showing that also in such a setting our main conclusions apply. Also in such a framework we derive the explicit solutions for the spatio-temporal dynamic path of the environmental tax and the pollution level and we graphically visualize through a numerical example the extent to which the local and global solutions may differ. Section 6 as usual concludes and suggests directions for future research. The proofs of most of our results are presented in the Appendix.","Transboundary pollution externalities: Think globally, act locally?",https://www.sciencedirect.com/science/article/pii/S0304406821000616,16 March 2021,2021,Research Article,108.0
"Ishii Yuhta,Kovach Matthew,Ülkü Levent","Department of Economics, Pennsylvania State University, 416 Kern Building, University Park, PA 16802, United States of America,Department of Economics, Virginia Tech, 880 West Campus Drive, Blacksburg, VA 24061, United States of America,Centro de Investigación Económica, Department of Economics, ITAM, Río Hondo 1, Ciudad de México 01080, Mexico","Received 2 July 2020, Revised 12 December 2020, Accepted 27 February 2021, Available online 16 March 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102509,Cited by (0), alternatives. In the second stage she chooses from the alternatives she has seen. Both ,"In many common choice settings the key variable that changes across choice problems is the order in which alternatives are observed, rather than the menu of available alternatives. For example, consider a shopper perusing the search results on Amazon, a voter looking over their ballot, or a researcher performing a Google search on a topic. In each of these examples, the grand menu is fixed – the set of goods available on Amazon, the candidates appearing on a ballot, and search results indexed by Google – yet the order they appear in may vary across decision makers or over time depending on past purchases, explicit randomization, or specific keywords.====In this paper we consider stochastic choice from lists. Importantly the observable variation in our model comes purely from different list orders of alternatives, while the exact set of available alternatives remains constant. In our model the decision maker chooses from any given list in two stages, using two distinct and independent primitives. First she observes a random number of alternatives in the order they are presented in the list. Next she chooses from the alternatives she has observed using a stochastic choice function over menus. The random depth of search and the stochastic choice function over menus are unobservable, and they constitute two distinct sources of randomness in the decision maker’s behavior.====In our model, the depth parameter ==== is a random variable, meaning that the menu of alternatives effectively considered by the decision maker is stochastic and dependent on the list order of alternatives. Given the realization of such a menu, the decision maker uses a standard stochastic choice function over menus to make a choice. We make no assumptions on the nature of the second stage choice behavior. We call this procedure the Random Depth Model. If ====, then the decision maker only observes and chooses the first alternative in every list. If ====, the number of alternatives, then she always chooses from the grand set of alternatives and therefore any variation in the list order has no impact on her behavior. In our model the depth parameter has full support on ====.====Random depth of search may be due to psychological constraints (e.g., cognitive limitations) which are subject to change for one decision maker over time, or alternatively across decision makers. Alternatively, random depth may correspond, loosely speaking, to a random patience parameter that governs the decision maker’s propensity to continue search faced with a list of alternatives. As both interpretations hinge on aspects that are intrinsic to the decision maker, it is reasonable to suppose that search depth is independent of the given list. In this paper, we take no stance on why the depth parameter may change in time series or a cross section, but we tease out all behavioral consequences of such randomness.====Let us denote the choice probability of alternative ==== in list ==== by ====. To see how our model operates in a simple example, consider a scenario with three alternatives, ====, ==== and ==== and a decision maker whose preferences are random, with ==== with probability ====, and ==== with probability ====. Suppose that ==== gives the probability that the decision maker searches up to the ====th alternative on any list. The decision maker randomly draws ==== and her preference. Then she chooses the best alternative on the list among those listed in the first ==== positions. Hence, if the list is ====
 (==== is presented first, then ====, then ====), the resulting choice frequencies are ====
 ====
 ====For instance, since ==== is superior to ==== with probability 1, ==== is chosen if the decision maker searches up to ==== (which happens with probability ====) or if the decision maker searches till the end of the list and prefers ==== to ==== (which happens with probability ====). Note that even though ==== is dominated by ==== and ==== in terms of preference, its salience as the alternative presented first may mean that it receives the largest choice probability on the list.====Of critical importance to our approach is the concept of====, which measure the change in an alternative’s choice probability if its position is switched with that of its successor, keeping all other positions intact. We take the marginal choice probability of the last alternative on a list to be equal to its choice probability. Denoting these quantities by ====, in the example of the previous paragraph ====
 ====
 ====In Example 1 we fully develop this example and make important observations about it which feed into our analysis. We find that studying the numbers ==== instead of ==== makes our revelation exercises more transparent. This is evident in the equations above as all primitives of the example are revealed by marginal choice probabilities. Furthermore, we also find that our model places testable and easy to interpret restrictions on the marginal choice probabilities. Hence our characterization results are also in terms of the marginals.====Here is a preview of our main results. In Proposition 1 we show that if ==== is in the ====th position in ====, then its marginal choice probability ==== is equal to the product of (i) the probability of searching precisely ==== alternatives and (ii) the probability of choosing ==== from a menu comprised of the first ==== alternatives. This is because switching an alternative ==== with its immediate successor affects the total choice probability of ==== only through the likelihood of ending search precisely at ==== (and then choosing it). From this crucial insight, we show in Proposition 2 that the sum of marginal choice probabilities of ==== alternatives in ==== judiciously chosen lists yields the probability that the depth parameter is ====. This follows from Proposition 1 as the first part of the above product remains constant among the summands, while the second part of the product sums to one.====Next we calculate the probability ==== that the underlying stochastic choice function ==== over menus chooses ==== in menu ====. Let ==== be the cardinality of ====. We take any list ==== which places the members of ==== in its first ==== positions and ==== in its ====th position. We show that ==== is precisely the ratio of the marginal choice probability of ==== in ==== to the revealed probability that the depth parameter is ====. This identification result relies on our assumption that all depth probabilities are strictly positive.====We then move on to characterize our model. We use four axioms which we state in the language of marginal choice probabilities. Our first axiom says that marginal choice probabilities are nonnegative. Hence if ==== switches the positions of ==== and its immediate successor in ====, but keeps intact the positions of the remaining alternatives, then ==== should not be chosen in ==== with a larger probability than in ====. This is essentially a position-monotonicity condition which says that switching the position of an alternative with its successor (and leaving remaining positions unchanged) cannot increase the choice frequency of the alternative.====The second axiom says that the marginal choice probability of an alternative (but not its actual choice probability) is determined by the set of its predecessors. If ==== follows the same set of alternatives in two lists ==== and ==== (even though these alternatives may be ordered differently) then its marginal choice probability is the same on ==== and ====.====Our third axiom regards the specific calculation of depth probabilities for the Random Depth Model. In this calculation we use ==== lists which “rotate” a given set ==== of size ==== in their first ==== positions as follows. Each list places a different member of ==== in position ==== and places the remaining members of ==== arbitrarily in the first ==== positions. The probability that the depth parameter is ==== is precisely the sum of marginal choice probabilities of members of ==== along the respective lists where they occupy position ====. Our third axiom says that this methodology does not rely on the choice of set ==== and the exact rotating lists. Hence it is a form of menu-independence (and rotating list-independence) condition. Any set with ==== alternatives and any ==== lists which rotate its members should yield the same probability of depth being equal to ====. The axiom yields a probability that the search stops after ==== alternatives are observed in any list ==== which is independent of the composition of ==== in its first ==== positions.====Finally our fourth axiom ensures that the depth probabilities we identify are strictly positive. Hence it says that for any menu ==== with ==== members and any ==== lists which rotate members of ==== in their ====th positions, the sum of marginal choice probabilities of members of ==== along the lists where they occupy position ==== should be strictly positive.====Our main theorem is the characterization of the class of Random Depth Models using these four axioms. We next look at (and characterize) two special cases regarding the second stage of choice. First, we consider the scenario where the second stage is deterministic and the decision maker chooses the best observed alternative according to a given preference. This special case may be pertinent if we are interested in time series behavior of a maximizing decision maker whose preferences are stable but whose search depth is variable. Our approach delineates how random choice behavior over lists reveals the decision maker’s preference. Second, we consider the maximization of random preferences in the second stage. This may be pertinent when we are interested in a cross section of maximizing decision makers making choices from lists with differing search depths and preferences. Both special cases can be characterized by appending our main characterization result with additional restrictions.====:====The paper which is closest to ours is Manzini et al. (2019). Like the current paper, they study behavior when the only observable variation comes from lists. In their model, as in ours, the decision maker peruses through the alternatives on the list and stops search randomly. However the behavior they are interested in is ====, as in Facebook likes, or Twitter shares.====Rubinstein and Salant (2006) also study choice from lists, but they allow for variation in the set of available alternatives. For them a list is an ordering of any subset of a grand set of alternatives. In their model the list is used to break ties between indifferent alternatives. Guney (2014) and Tserenjigmid (2021) also study behavior with list and menu variation. The former paper characterizes a procedure in which the decision maker iteratively compares alternatives on the list using a deterministic preference, while the latter appends the Luce model to allow for list order effects. In contrast to these papers we only observe choice behavior with list variation, holding the set of available options fixed throughout.====Lists have appeared in choice models as psychological primitives which facilitate behavior, especially in models of satisficing. Aguiar et al. (2016) and Kovach and Ülkü (2020) study random satisficing behavior. In their models the decision maker subjectively lists the available alternatives and chooses the first alternative on the list which is satisfactory. Our paper differs from these models in that we take lists as the observable domain of behavior and we do not allow for menu variation. Furthermore, our decision maker is not necessarily a satisficer.====Finally, ours is essentially a model of limited consideration. The decision maker considers a subset of available alternatives for choice. Such models have been studied in environments with menu variation by Masatlioglu et al. (2012), Lleras et al. (2017), Manzini and Mariotti (2014) and Brady and Rehbeck (2016), among others. In these models, faced with a menu, the decision maker forms a consideration set and chooses from this set instead of the whole menu. Our paper, again, differs from these papers as we do not have menu variation and our domain comprises of lists.",A model of stochastic choice from lists,https://www.sciencedirect.com/science/article/pii/S0304406821000598,16 March 2021,2021,Research Article,109.0
"Allouch Nizar,King Maia","School of Economics, University of Kent, United Kingdom of Great Britain and Northern Ireland,Department of Political Economy, King’s College London, United Kingdom of Great Britain and Northern Ireland","Received 18 November 2020, Revised 19 February 2021, Accepted 1 March 2021, Available online 16 March 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102508,Cited by (2),"This paper investigates welfare targeting for public goods in networks. First, we show that a tax/subsidy scheme (not necessarily budget-balanced) affects each consumer only insofar as it affects his neighbourhood. Second, we show that either a Pareto-improving income redistribution can be found or there exist Negishi weights, which we relate to the network structure. Third, in the case of Cobb–Douglas preferences, we show that a Law of Welfare Targeting holds and links two well-known notions of the comparative statics of policy interventions: neutrality and welfare paradoxical effects. Collectively, our findings uncover the importance of the ==== eigenvalue to economic and social policy: it is an indication of how consumers absorb the impact of income redistribution.","Many major challenges facing modern societies (essential infrastructure, information acquisition, emerging infectious disease) relate to enhancing public good provision across different consumers. While market outcomes often provide scope for policy intervention, central planners seldom have the luxury to completely change the state of the economy and implement optimal outcomes. Instead, social planners typically aim to design welfare-improving reforms — involving small changes as in the policy reform literature in the tradition of Dixit (1975) and Guesnerie (1977), and with important policy implications as in Ahmad and Stern (1984).====This paper explores welfare targeting with small tax/subsidy schemes, not necessarily budget-balanced, which are traditionally viewed as a benchmark for broader policy interventions, for the private provision of public goods in networks. In a key contribution, Bramoullé and Kranton (2007) showed that the network context, where local influences are heterogeneous among consumers, is a natural setting to examine the private provision of public goods. Bramoullé et al. (2014) investigated the whole range of strategic substitution and identified a threshold of impact related to the lowest eigenvalue of the network. Below the threshold, the uniqueness and stability of a Nash equilibrium hold. Beyond it, multiple Nash equilibria will in general exist, and stability holds only for corner equilibria. Allouch (2015) extended this model to the non-linear case, with a condition on the normality of the public good which follows the classical (Bergstrom et al., 1986) (henceforth BBV) approach, and showed that their neutrality no longer holds for income redistribution in general networks. Galeotti et al. (2020) analyse optimal policy interventions informed by the eigenvalues of the underlying network of spillovers. Other recent and relevant contributions to the network literature include those by: Galeotti et al., 2010, Ghiglino and Goyal, 2010, Acemoglu et al., 2016, Bourlès et al., 2017, Kinateder and Merlino, 2017, López-Pintado, 2017, Chen et al., 2018, Belhaj and Deroïan, 2019, Elliott and Golub, 2019, Allouch, 2017, Allouch and King, 2019, Akbarpour et al., 2020, Ushchev and Zenou, 2020, Li et al., 2021, Sun et al., 2021, Faias et al., 2015, and Herskovic and Ramos (2020).====First, we establish a property that is key to understanding the impact of welfare targeting in networks. More specifically, we show that a tax/subsidy scheme affects each consumer only insofar as it affects the consumer’s neighbourhood, formed by himself and his neighbours. That is, it is the neighbourhood scheme, rather than the tax/subsidy scheme that affects consumption. Hence the policy implications can be derived by focusing just on the neighbourhood schemes. In particular, neutral tax/subsidy schemes (budget-balanced or not) in general networks are those with null neighbourhood schemes, or equivalently those that are the eigenvectors to the ==== eigenvalue.====Secondly, to inform/guide welfare targeting, we use an approach based on a weighted utilitarian social welfare function. More specifically, the social planner maximises a weighted sum of consumers’ utility functions. We find two mutually exclusive cases — either there is a Pareto-improving income redistribution or, if not, there exist Negishi welfare weights. These Negishi weights provide the implicit welfare weights at the initial equilibrium. In particular, we find that a Pareto-improving income redistribution always exists in the class of networks that has a neutral and budget-unbalanced tax/subsidy scheme, since Negishi weights cannot exist. This finding is very much in the spirit of the BBV neutrality result as it holds for any profile of preferences of consumers. Additionally, in the case of Cobb–Douglas preferences, we show that the feasibility of Pareto-improving reform turns out to be readily interpreted and easily checked from the network structure. As a consequence, our analysis leads to a useful characterisation of welfare targeting.====Thirdly, we provide a link between key, but seemingly unrelated, notions of comparative statics: neutrality and paradoxical welfare effects. In fact, by focusing on tax/subsidy schemes that are also eigenvectors, our analysis shows that in the case of Cobb–Douglas preferences, neutrality (or, equivalently, the ==== eigenvalue) corresponds to the point of policy switch between tax/subsidy schemes where the utility levels of the donors and the recipients move in the same direction as the scheme (normal welfare impact) and tax/subsidy schemes where the utility levels of the donors and the recipients move in the opposite direction (paradoxical welfare impact). In addition, we show that a Law of Welfare Targeting holds for more general tax/subsidy schemes determined by the ==== eigenvalue.====In different settings, our results highlight the importance of the ==== eigenvalue to social and economic outcomes, since our findings identify it as a condition for neutral tax/subsidy schemes, Pareto improvement, and the policy switch. In interpretation, the ==== eigenvalue is an indication of how consumers, via their neighbourhood, absorb the impact of tax/subsidy schemes, and hence of the welfare implications. Despite the frequency with which the ====1 eigenvalue appears for many (but not all) networks, as far as we know the ==== eigenvalue is not used as a common measure of network analysis in any other fields, including sociology, computer science, and physics. Given that the ==== eigenvalue provides a key to social and economic outcomes, perhaps its relationship to the underlying network structure could usefully be studied alongside classic network statistics such as the highest, the second, and lately the lowest eigenvalues.====The paper is structured as follows. Section 2 sets out the model and Section 3 investigates welfare targeting. Section 4 looks at Pareto improvement and Negishi weights and Section 5 provides a new perspective on neutrality and paradoxical welfare effects. Section 6 concludes the paper. An Appendix provides proofs of the propositions and corollaries.",Welfare targeting in networks,https://www.sciencedirect.com/science/article/pii/S0304406821000586,16 March 2021,2021,Research Article,110.0
"Barbieri Stefano,Serena Marco","Tulane University, United States of America,Max Planck Institute for Tax Law and Public Finance, Germany","Received 5 May 2020, Revised 3 February 2021, Accepted 1 March 2021, Available online 15 March 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.jmateco.2021.102512,Cited by (1),"We investigate the temporal structure that maximizes the ==== effort in large homogeneous contests. We find that the winner’s effort ranges from a lower bound of 0 to an upper bound of one third of the value of the prize, depending on the temporal structure; the upper (lower) bound is approached with an infinite number of players playing sequentially (simultaneously) in the first periods (period). Nevertheless, when the number of players is large but finite, we show that winner’s effort is maximized when all players play sequentially except in the very last period and that, within the family of such optimal temporal structures, more players play simultaneously in the very last period than sequentially in all other periods. Furthermore, out of all players, the percentage of those playing simultaneously in the very last period goes to 100% as the number of players grows larger and larger.","In many applications of sequential contests, it is natural to ask whether the efforts of earlier players should be disclosed to later players or not. If a contest designer chooses to disclose, late movers can tailor their efforts to the choices of early movers. This dynamic effect is absent if instead the designer chooses not to disclose efforts. We address the above question in a sequential Tullock contest with ==== homogeneous players whose efforts may or may not be disclosed to later contestants. Note that the choice of disclosure policy when players move sequentially can be equivalently stated as a choice of ==== under complete information; in fact, full disclosure (concealment) in the former setup coincides with a fully sequential (simultaneous) temporal structure in the latter setup. We adopt the temporal structure framing throughout the paper, although all results can be interpreted as information disclosure policies.====The particular way the designer benefits from players’ efforts is crucial. We focus on investigating how the temporal structure affects the expected winner’s effort (henceforth, ====). Thus, we complement Hinnosaar’s (2019) analysis for total effort and keep his total-effort-maximizing temporal structures as benchmark. Our main results are derived in large contests, which are broadly applicable. Examples of contests where the number of players is large (but finite) include competitions of scientists for research grants, employees for sales in large firms, or athletes in large marathons. In these settings, in addition to being concerned with total effort, the designer may especially benefit from a large ====.==== For instance, grant-giving agencies, such as the National Science Foundation and the European Research Council, benefit if the winners of their research grants achieve groundbreaking results. A large firm running a sales competition among its employees gains visibility, prestige, and profit if its best-performing sales associate is particularly successful. In HubSpot’s large sales competition, for instance, the top-performing sales agents “bring in the best customers,” whereas for the worst-performing agents “customers are not succeeding” (Roberge, 2015; p. 73), hence making it crucial for the firm to stimulate winner’s effort. And the organizer of a large marathon gains visibility and sponsorship opportunities if the world record is beaten by the winner of the marathon. All the above examples show that, from the point of view of a contest designer, the efforts of winning and losing contestants need not be perfect substitutes as they are in total effort.====A crucial facet of the above described contests is the inherent noise, as the player exerting the highest effort is only most likely, but not certain, to win. This is important for two reasons. First, in the above real-life setups, effort is not observable or quantifiable by the designer who may, for instance, just observe a noisy outcome of effort (scientists’ publications, employees’ sales, athletes running time). The important implication is that efforts cannot be directly contracted upon by the designer. This non-contractibility of efforts is the inherent underlying assumption common in the literature that uses Tullock contests as incentive mechanisms. Second, Tullock contests are commonly-adopted reduced forms for noisy performance; in fact, the highest effort may end up losing the contest.==== Consider now an R&D contest in which the winning project receives extra funding and is the only project ever implemented. Because of noise, the winning project may not be the one with the largest effort (the “best” project). Maximization of ==== does take the noise into account and complies with the assumption that the designer benefits from the quality of the project that wins the contest and does not benefit from the quality of the projects that end up losing the contest. We believe this is reasonable in R&D contests and, to a large extent, in the other noisy contests mentioned above. Our results provide insights on how such contests should be arranged.====. We characterize the ====-maximizing temporal structure for sufficiently small or large contests, but our sharpest results are derived in the latter setup.==== We find that the limit of ==== converges to a value in the interval ====, ==== the temporal structure. We fully characterize the temporal structures whose limit of ==== converges to one of the two extreme values ==== and ====. The lower bound ==== is approached by temporal structures with an infinite number of players playing simultaneously in the first period. The upper bound ==== is approached by temporal structures with an infinite number of players playing sequentially in the initial periods. Furthermore, this upper bound is approached by a large number of temporal structures, all starting with an infinite number of sequential players. Hence, comparing these temporal structures, we characterize a number of strict rankings based on the behavior for a finite but sufficiently large number of players. In particular, we derive two main results. First, ==== is maximized when all players play sequentially except in the very last period. Second, within the family of such ====-maximizing temporal structures, more players play simultaneously in the very last period than sequentially in all other periods. Furthermore, out of all players, the percentage of those playing simultaneously in the very last period goes to 100% as the number of players grows larger and larger.====In many real-life examples of the above discussed applications, players typically move one after the other and the designer of the competition has some degree of control over the disclosure of efforts over time. For instance, scientists’ applications for grants are typically submitted at a time of the applicant’s choosing before the deadline. Our results show the optimality of sequential initial moves followed by simultaneous final ones. This suggests that information disclosure on players’ efforts is beneficial only for a certain number of initial applications, and from that point onward disclosures become detrimental to the expected winner’s effort. Similarly, information about scientists’ applications for grants could be publicly disclosed, unless they come in sufficiently close to the deadline for submission.====. Our results are closely connected to at least two strands of the literature; one on large contests and one on winner’s effort maximization in contests. First, our contribution is closely related to the literature on large contests; see, for instance, the seminal contribution by Olszewski and Siegel (2016), and also Bodoh-Creed and Hickman (2018) and Olszewski and Siegel, 2019, Olszewski and Siegel, 2020a, Olszewski and Siegel, 2020b. As mentioned above, focusing on large contests is relevant for applications and allows us to sidestep the challenge posed by the loss of tractability when moving from the analysis of total effort to that of ==== in our setup. Second, our contribution is closely related to the literature on winner’s effort maximization in contests. When considering the problem of a contest designer, objectives other than total effort have recently received special attention. In particular, the maximization of ====, typically compared to that of total effort, is discussed in several other recent papers. One of the first to study ==== maximization in Tullock contests is Serena (2017), who analyzes optimal contest design and, in particular, finds that the exclusion principle extends from total effort to ==== in Tullock contests with (sufficiently) homogeneous contestants (see Baye et al., 1993). This latter result fails to extend to a setup where contestants’ types are binary, stochastic, and significantly different from one another, as shown by Chen et al. (2020). Drugov and Ryvkin (2017) characterize the conditions under which a designer interested in ====, among other objectives, wants to bias a contest between two symmetric players in a static environment. Barbieri and Serena (2020) ask a similar question, also for ====-maximization, in a dynamic, rather than static, best-of-three setup. Fu and Wu (2018) analyze the optimal design of sequential-elimination contests when the designer is interested in ====. Deng et al. (2018) show that the introduction of draws in a contest increases ==== when the contestants’ valuations of the prize become sufficiently dispersed. Optimal information disclosure of contestants’ types in order to maximize ==== is considered in Serena (2020) in a two-sided private information environment and in Deng et al. (2020) in a one-sided private information environment with perceptional bias. None of these papers analyze the ====-maximizing temporal structure, with the exception of Barbieri and Serena (2019) whose setup is, however, sharply different. Indeed, they analyze a best-of-==== team contest, where each member of the winning team obtains a prize, rather than an ====-player individualistic contest with a single prize. Furthermore, in Barbieri and Serena (2019) all players in a component match are assumed to play simultaneously.====Clearly, the most closely related paper to ours is Hinnosaar (2019), which our analysis heavily builds on. Our paper is also closely related to the independent work of Hinnosaar (2020), who extends Hinnosaar’s (2019) analysis to a richer set of possible objectives other than total effort, including the highest effort (====), but excluding ====. Both the interpretation and results differ substantially between ==== and ====.==== As for interpretation, in contrast to ====, the maximization of ==== takes into consideration that the highest effort might end up losing in a Tullock contest; in fact, every player effort is weighted by its probability of victory. As for results, the first-mover’s effort is always the greatest one and hence it equals ====; for large contests, Hinnosaar (2020) shows that ==== is maximized by any temporal structure with a first-mover, while we show that ==== is maximized by an infinite number of players playing sequentially with a number of players playing simultaneously in the very last period. Furthermore, out of all players, the percentage of those playing simultaneously in the very last period goes to 100% as the number of players grows larger and larger.====. In Section 2, we present preliminary results and intuitions about ==== that are useful for the subsequent analysis. In Section 3, we build intuition by analyzing small contests (with low number of players) and discussing if and how the key properties of total effort carry over to ====. The main contribution of the paper is Section 4, where we provide several results in large contests (as the number of players grows larger and larger) on upper and lower bounds for ====, on which temporal structures approach these bounds, and about the convergence to the upper bound of different temporal structures as the number of players is large but finite. We also compare our results for large contests with those derived for small contests (Section 3).",Winner’s effort maximization in large contests,https://www.sciencedirect.com/science/article/pii/S0304406821000628,15 March 2021,2021,Research Article,111.0
"d’Albis Hippolyte,Kalk Andrei","Paris School of Economics, CNRS, France,University of Vienna, Austria","Received 3 June 2020, Revised 18 December 2020, Accepted 15 February 2021, Available online 5 March 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102500,Cited by (0),"This paper seeks to explain why annuity purchases are postponed to a later age. We consider an overlapping generations model with uncertain lifetime and two types of annuities. It is shown that, if the economy is dynamically inefficient, individuals demand annuities without delay. However, if it is efficient, annuity purchases are postponed. We also show that these results are robust to several extensions.","Empirical evidence for developed countries suggests that most annuity purchases are postponed to advanced ages. In the UK, for example, the modal age at purchase of annuities is between 60 and 65 (Einav et al., 2010). Moreover, less than 15% of annuitants in the UK market buy the product before the age of 60 (Taylor, 2004). In the US, nearly three-quarters of annuitants are aged 61 to 80 (Mitchell et al., 1999). Such observations stand in stark contrast to the prediction of the theoretical model proposed by Brugiavini (1993). Consumers indeed should purchase annuities only when they are young, provided that the information about longevity arrives late in life. This puzzling result has been replicated and extended in more recent studies by Sheshinski (2010) and Steinorth (2012). In particular, Steinorth (2012) has shown that early annuities can be driven out of the market if individuals have more information about their future survival type than annuity providers.====In this paper, we provide a new explanation for the phenomenon of delayed annuity purchases using a model similar to that of Brugiavini (1993) while considering general annuity pricing. We examine the timing of annuity purchases in an overlapping generations economy with production, where agents live for either two or three periods. Following Brugiavini (1993), we assume that agents can invest in annuities at an early age and receive the return in the third period of their life upon survival. In addition, they can purchase bonds in the beginning and annuities at an older age. A key feature of our model is that annuity premiums are determined in general equilibrium for perfectly competitive annuity markets. This idea can be found in the work of d’Albis and Etner (2018), who have studied illiquid annuities in an OLG economy. As in their model, annuity pricing in our framework may involve redistribution between cohorts.====We show that in the simple case where the probability of survival is known in advance to annuity providers and agents, annuities are generically purchased only once during the lifetime. In a ==== economy, i.e., one characterized by over-accumulation of capital, agents purchase annuities early in life. On the contrary, if the economy is ====, our basic model predicts that agents postpone the purchase of annuities to a later age. The intuition behind the results is based on a comparison of two saving strategies: investing a unit of consumption in annuities when young or investing it in bonds and then in annuities when old. Choosing the first strategy, agents receive the return which, in fact, represents a uniform transfer from the next generation of annuity holders. Thus, agents can benefit from population growth by purchasing annuities early in life. As long as the growth rate is higher (lower) than the long-run interest rate, this strategy is more (less) profitable than the other with delayed annuity purchases.====To check the robustness of our results, we also discuss some extensions and modifications of the basic model. The literature has argued that the irreversibility of annuity purchases (Milevsky and Young, 2007) and overly pessimistic beliefs about survival probabilities (Sheshinski, 2008) may reduce incentives to purchase annuities earlier in life. Correspondingly, we find that our results remain unchanged when annuity contracts are, in general, irreversible or agents have subjective survival expectations. This suggests that macroeconomic conditions play a major role in explaining why younger cohorts do not participate in the annuity market. The same applies to the additional assumption that the future survival probability is initially uncertain. Indeed, we show that in this case, contrary to Brugiavini (1993), all agents should postpone the purchase of annuities to an older age if the rate of interest is sufficiently higher than the rate of population growth.====The findings presented in this paper are consistent with previous empirical studies for advanced economies, which usually experience low population growth. Brown (2001), for instance, using the sample of American households, has shown that the value of annuitization is increasing with age. Similarly, the fact that annuities are less attractive earlier in life seems to be supported by Bütler et al. (2013), who have studied male workers close to retirement in Switzerland. They have found that the probability of annuitizing pension wealth is lower for men retiring at a younger age. It is noteworthy that, in contrast to these results, obtained in a natural economic context, in a survey with hypothetical choices, one observes a significant negative effect of age on the probability of choosing an annuity (Schreiber and Weber, 2016). This gives a reason to think that macroeconomic conditions matter for the decision to delay annuity purchases.====More broadly, our paper contributes to the literature that demonstrates that macroeconomic frameworks are useful for analyzing lifecycle decisions in general and the demand for annuities in particular (see Prettner and Canning, 2014, Heijdra et al., 2014, Heijdra et al., 2017).====The rest of the paper is organized as follows. In Section 2, we present the basic model and characterize the demand for annuities in a steady state. Section 3 explores the robustness of our results to a bequest motive, subjective evaluation of longevity risk, endogenous longevity, and irreversibility of annuity purchases. Section 4 deals with the case of uncertain survival as in Brugiavini (1993). Finally, we discuss some additional issues in Section 5 and conclude in Section 6.",Why do we postpone annuity purchases?,https://www.sciencedirect.com/science/article/pii/S0304406821000380,5 March 2021,2021,Research Article,112.0
Boyarchenko Svetlana,"Department of Economics, The University of Texas at Austin, 2225 Speedway Stop C3100, Austin, TX 78712, USA","Received 17 February 2020, Revised 15 January 2021, Accepted 26 January 2021, Available online 4 March 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102497,Cited by (2),"I consider two models of sponsored research — one is the model where researchers get funded until (if ever) a research project experiences the first failure (bad news model), the other one is the model where the winner (if any) of a research race is rewarded (good news model). In either case, the researchers start working on a project of unknown quality. The quality of the project is identified with its ability to generate failures or successes, in the first and second models, respectively. The rate of arrival of success conditioned on the quality of the project is an increasing function of the total time spent on research. I find equilibria in both models and show that in case of two competing researchers, one of the researchers experiments inefficiently long, in the bad news model or in the good news model when either the winner takes all, or the laggard is punished. In the good news model where the laggard is rewarded, the equilibrium outcome is efficient.","At the time when thousands of researchers in more than thirty countries are competing on hundreds of projects to develop a vaccine against the new coronavirus, it is important to understand mechanisms that can make research competition more efficient. It is not less important to understand how competition affects the process of learning about long term potentially harmful effects of certain pollutants.====At the start of a research project, nobody knows in advance which of raw ideas (if any) will generate a success and when. Therefore models that exploit random times of arrival of success are most suitable for studying sponsored research. However, a large variety of experimentation models based on exponentially distributed arrival times (so called exponential, or Poisson, bandits models) are not suitable to study long-term negative effects of experimentation such as, e.g., harmful pollutants or drugs that have a potential for accumulation and long term storage in a human body; or positive effects, such as accumulated effort of economic agents or resources.====As opposed to exponential bandits models, the hazard rate for a known quality project is an increasing function of time (alternatively, it can be viewed as an increasing function of another state variable such as the amount of capital invested in research or amount of fertilizer accumulated in soil or water) in this paper. As a result, the expected rate of arrival of breakdowns or breakthroughs is hump-shaped. It is increasing in time in early stages of an experiment until it reaches a certain maximal level; after that point, the arrival rate decreases as the experiment grows older (provided no failure or success were observed earlier). I used the same hump-shaped rate of arrival in Boyarchenko (2020), but in that paper, only the player who experienced a failure had to pay the cost in the bad news model, and only a winner-takes-it-all case was studied in the good news model. Another paper that uses a time dependent hazard rate in a winner-takes-it-all R&D race between two firms is Moscarini and Squintani (2010). There, it is assumed that the hazard rate is decreasing in time. I demonstrate that using humped bandits instead of exponential bandits changes understanding of experimentation and produces qualitatively new results. The models presented in my paper can be used as better models for the aforementioned long term effects as well as for design of grant competitions, innovation contests or even determining optimal terms of politicians.====As opposed to Boyarchenko (2020), I introduce a payoff externality into the model with failures and enrich a range of payoff externalities in the model with successes. This analysis is important because there are many real life situations with interplay of information and payoff externalities. Margaria (2020) considers such interplay in a paper which, to a certain extent, is complementary to mine. The latter paper studies when it is optimal to invest in a risky project, and the good news model in my paper examines when it is optimal to quit a risky project that fails to produce a breakthrough for a long time.====The contribution of my paper is twofold. First, I characterize equilibria in a class of stopping games with information and payoff externalities. While Boyarchenko (2020) reduces players’ strategies to threshold strategies, the class of the latter strategies is not rich enough to find equilibria in this paper. Methods of this paper are relatively simple, robust, and admit modification to other stopping games of attrition. In particular, my model can be adjusted to the case of so-called inconclusive bad news, when players have to learn about the frequency of adverse environmental events whose unknown arrival rate depends on the level of accumulated greenhouse gases GHG and decide when it is optimal to invest in abatement technologies. Exponential models will give an incorrect recommendation because they do not take into consideration that hurricanes, flooding, heat waves, etc., may happen more frequently as more GHG emissions are accumulated. Models with information externalities only would also give wrong predictions because all countries are affected by global warming to a lesser or bigger extent.====Second, I study efficiency of equilibrium outcomes in two models of sponsored research. In the first model, there is a positive probability that risky research projects can generate costly failures such as, for example, side effects of tested drugs or damages to ecosystems as a result of use of fertilizers and pesticides. The researchers get sponsored only until the first failure is observed if ever. If the project generates costly failures for certain, failures arrive at random times, and the arrival rate of failures is increasing over time. This assumption captures the fact that during the short time usage, small amounts of harmful substances, drugs or GHG may cause no trouble, but serious damages can happen as time goes by, and accumulation happens.==== At any point in time, each of the researchers can irreversibly stop their risky activity. The researcher who experienced the first failure has to pay for the direct damages caused by the failure, and in addition, all the researchers have to share an additional loss such as damage to reputation or loss in value of a good or service due to negative consumer reports, etc. For example, one of the new drug developers has to pay patients if they developed serious side effects while trying the new drug, and in addition, all other developers suffer a reputation damage because of these side effects. After Takata “Alpha” air bags started exploding when being deployed on certain 2001–2003 Honda and Acura vehicles, not only did Honda and Acura have to pay for injuries of vehicle occupants, and recall the vehicles equipped with defective bags, but 19 other car manufacturers were also affected, so that eventually about 34 million vehicles were recalled in the US alone.==== Alternatively, one can view the additional loss as a liquidation cost of a research facility if experiments conducted on site turned out to be damaging in one of the facilities.====My model predicts that for a generic set of parameters, in case when two researchers are involved, there exists an equilibrium in symmetric strategies where the researchers stop working on risky projects before the first failure occurs. As opposed to Boyarchenko (2020), the equilibrium strategies are not pure (threshold type) strategies. The players stay in the game with probability one until the increasing expected rate of arrival of failures reaches a certain barrier; after that they experiment with decreasing probabilities that converge to zero. If one of the players precommits to stop no earlier than the other one, an asymmetric equilibrium with sequential exit is possible, because there is the second mover’s advantage in the bad news model with payoff externalities. The symmetric equilibrium outcome is Pareto dominated by the asymmetric one. However, even the latter outcome is inefficient — to maximize the total value of the sponsored research, the sponsor will stop the support of one of the players even earlier than it is optimal for the leader.====The second model in the paper deals with the case when sponsored research projects can generate a success with positive probability, but the time of arrival of the success is random. If the project achieves a success for sure, the rate of arrival of success is an increasing function of the total amount of resources accumulated over time. These can be interpreted as skills, effort or gradual capital investment into the research project. The above assumption accounts for the fact that the rate of arrival of success of the good project increases in time that passed since the entrepreneur has started working on the project and more investment had been accumulated. There is still a small but growing literature on crowdfunding (see, e.g., Strausz (2017) or Deb et al. (2019) and references therein). Crowdfunding provides funds directly to the entrepreneur and enables the entrepreneur to contract directly with consumers before investment. The crowdfunding provides an important implication that success of a project depends on the total amount of money invested into the project. When one uses the standard Poisson process to model arrival time of success of a good start-up, then the rate of arrival may depend on the current investment rate (as, e.g., in Bergemann and Hege (2005)), but not on the total amount of productive investment. If in a period right after the creation of the start-up and some later time period the investment rates are the same, then the probabilities of success of the good start-up during the next small interval ==== are also the same, which is not realistic.====The stylized model presented here is suitable to describe a grant competition for long term projects, because as researchers work longer on a project and accumulate productive effort, the arrival probability of success increases if the project is good. In many contests such as career promotions, political elections, patent races, or sports, a contest designer has a limited budget for prizes, and prizes are allocated to winners according to their relative rankings. I assume that the researcher who is the first to succeed gets the major prize. Thus, there is an advantage to generating the success first (though, in this stylized model, this advantage is independent of the players’ actions). While Boyarchenko (2020) focuses on a winner-takes-it-all situation, I consider two possibilities (i) the laggard is rewarded – their prize is less than the major prize, but positive; (ii) the laggard is punished – their “prize” is negative. For example, if two research universities work on the same topic, the winner has the right to file a patent, and the laggard may experience a downgrade in ranking.====In the case when the laggard is rewarded there is a unique equilibrium in pure strategies of the threshold type. The researchers stop simultaneously when the decreasing expected rate of the arrival of success of the risky projects drops below a critical level, unless the first success happens earlier. Experimentation lasts longer than in a winner-takes-it-all case. Interestingly, in the interval between the cutoff barrier for a winner-takes-it-all case and the one for the case when the laggard is rewarded, my model predicts a herding, or a “survivor’s curse” effect similar to the one in Moscarini and Squintani (2010). The driving force behind the “survivor’s curse” in Moscarini and Squintani (2010) is private information about the arrival rates of breakthrough. In my model, the “survivor’s curse” is due to a potential positive reward to the laggard. On the one hand, observing that one’s competitor is still in the research race is a bad news about the chance of winning the major prize. On the other hand, if a player’s competitor wins the major prize, then the player knows that she will win the second prize with probability one. I prove that the equilibrium outcome is efficient if the laggard is rewarded.====If the laggard is punished, there is a unique equilibrium in symmetric strategies. The researchers stay in the grant competition for sure until the rate of the arrival of success drops below a critical level and then continue participation with a positive declining probability until another critical value of the arrival rate is reached (by then, the probability of participation drops to zero). The latter equilibrium outcome is Pareto dominated by a sequential dropping out of the grant competition which is an asymmetric equilibrium outcome in case when one of the players precommits to stop no earlier than the other one. I demonstrate that in the case when the laggard is punished, neither symmetric, nor asymmetric equilibrium outcomes are efficient. The sponsor would find it optimal to stop funding one of the researchers earlier than in equilibrium.====The reason for inefficiency of symmetric strategies equilibria in the bad and good news models is that each player gets the equilibrium payoff of the leader in the corresponding asymmetric equilibrium. In the asymmetric equilibrium the follower’s payoff is larger than the leader’s payoff. The inefficiency of the asymmetric equilibrium can be explained as follows: if the social planner makes one of the players to quit the leader’s optimal stopping time minus ==== (i.e., a little bit earlier than optimal for the leader), then reduction in the leader’s value is of the order of magnitude ====, but the gain to the follower’s value is of the order of magnitude ====. This intuition is no longer valid in case when the laggard is rewarded, because of the “survivor’s curse” effect in a left neighborhood of the equilibrium stopping time.====The good news model also demonstrates a ==== for the case when the laggard is punished and winner-takes-it-all case — there is a critical number of participants of the grant competition so that it is not profitable to participate in the competition in case the number of participants exceeds this critical number. Furthermore, in the case when participation is profitable from the viewpoint of the researchers, the sponsor may find it optimal to reduce the number of participants by offering at least one of them a safe option at the start of the competition. This can happen if the operational costs of research activities are relatively high====This paper is related to the extensive literature on experimentation based on two-armed exponential bandits. In earlier models of strategic experimentation, it was common to assume away payoff externalities and focus on information externalities, the role of information, and, in more advanced settings, on design of information. See, for example, Bergemann and Välimäki, 1996, Bergemann and Välimäki, 2000, Bergemann and Välimäki, 2006, Bergemann and Välimäki, 2008, Bolton and Harris (1999), Decamps and Mariotti (2004), Hörner et al. (2014), Keller et al. (2005), Keller and Rady, 2010, Keller and Rady, 2015. Recent developments include (but are not limited to) correlated risky arms as in Klein and Rady (2008) and Rosenberg et al. (2013), private payoffs as in Heidhues et al. (2015) and Rosenberg et al. (2013), departures from Markovian strategies as in Hörner et al. (2014), strategic costly observations as in Marlats and Ménager (2018) and payoff externalities as in Margaria (2020).====My paper focuses on the case of conclusive failures and successes, so that the first event observed on the risky arm reveals its quality completely, and the experiment is over. It is most closely related to Keller et al. (2005), Keller and Rady, 2010, Keller and Rady, 2015 and Rosenberg et al. (2013). Keller et al. (2005) and Keller and Rady (2010) study the good news model, and Keller and Rady (2015) study the bad news model; in either model, news arrives at the jump times of independent Poisson processes. Also, Keller et al. (2005), Keller and Rady, 2010, Keller and Rady, 2015 solve control problems of time allocation between the risky and riskless arms of Poisson bandits, while the present paper considers stopping time problems. Rosenberg et al. (2013) consider an irreversible exit problem in a model with breakthroughs with correlated risky arms both in the case when payoffs are public and private. In contrast, I study a problem of strategic irreversible exit both in good and bad news models, where news arrives at jump times of time inhomogeneous Poisson processes. Processes that generate news for individual experimenters are independent, but have the same characteristics. While the model of Rosenberg et al. (2013) is more general than my good news model because of correlated risky arms and private payoffs, the type of uncertainty in my model is completely different from that in Rosenberg et al. (2013).====The rest of the paper is organized as follows. The economic environment and game theoretical model are described in Section 2. Section 3 considers a model of sponsored research in a “bad” news model (risky projects can generate costly failures). A model of sponsored research in a “good” news model (risky projects can produce profitable breakthroughs) is analyzed in Section 4. Section 5 concludes the paper. Technical proofs are relegated to the appendix.",Inefficiency of sponsored research,https://www.sciencedirect.com/science/article/pii/S0304406821000355,4 March 2021,2021,Research Article,113.0
"Jaramillo Paula,Kayı Çaǧatay,Klijn Flip","Universidad de los Andes, Bogotá, Colombia,Universidad del Rosario, Bogotá, Colombia,Institute for Economic Analysis (CSIC) and Barcelona Graduate School of Economics (Barcelona GSE), Campus UAB, 08193, Bellaterra (Barcelona), Spain","Received 3 February 2020, Revised 22 October 2020, Accepted 26 January 2021, Available online 22 February 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102496,Cited by (0),"We consider school choice problems (Abdulkadiroğlu and Sönmez, 2003) where students are assigned to public schools through a centralized assignment mechanism. We study the family of so-called rank-priority mechanisms, each of which is induced by an order of rank-priority pairs. Following the corresponding order of pairs, at each step a rank-priority mechanism considers a rank-priority pair and matches an available student to an unfilled school if the student and the school rank and prioritize each other in accordance with the rank-priority pair. The Boston or immediate acceptance mechanism is a particular rank-priority mechanism. Our first main result is a characterization of the subfamily of rank-priority mechanisms that Nash implement the set of stable matchings (Theorem 1). Our second main result is a strong impossibility result: under incomplete information, no rank-priority mechanism implements the set of stable matchings (Theorem 2).","An important application of mechanism design is school choice (Abdulkadiroğlu and Sönmez, 2003). This paper analyzes so-called rank-priority mechanisms for the centralized assignment of students to public schools based on the students’ rankings (preferences) over schools and the schools’ priorities over students. Rank-priority mechanisms were first studied in Roth (1991) in the context of the assignment of medical school graduates to consultants in the UK.====A rank-priority mechanism is determined by an order of all rank-priority pairs. Here, rank refers to the position that a student assigns to a school in his ranking, while priority refers to the position that a school assigns to a student in its priority ordering. Given a profile of rankings and priority orderings, a matching is determined step-wise by going through the order of rank-priority pairs. More specifically, at each step a rank-priority pair ==== is considered. If a school is ranked ====th by some student and if the student has priority ==== for the school, then the student is assigned to the school provided that the school still has vacant seats (after which the student and the seat are removed from the market). A student remains unassigned if he is not assigned to a school at any step. A well-known rank-priority mechanism is the immediate acceptance or “Boston” mechanism which is widely used in school choice.==== The immediate acceptance mechanism is lexicographic as it first considers the student preferences and only then the school priorities.====Roth (1991, Proposition 10) shows that no rank-priority mechanism is stable. Stability is a central concept in the matching literature and puts together three desiderata: individual rationality, non-wastefulness, and no justified envy.====
 Roth (1991) also illustrates that under rank-priority mechanisms agents have incentives to misrepresent their rankings.==== Assuming complete information, Ergin and Sönmez (2006) study the strategic games induced by “monotonic” rank-priority mechanisms. Here, monotonicity refers to the requirement that for any distinct rank-priority pairs ==== and ====, if ==== and ====, then ==== is considered before ====. Ergin and Sönmez (2006, Proposition 4) show that monotonic rank-priority mechanisms Nash implement the set of stable matchings. Since the immediate acceptance mechanism is monotonic, it Nash implements the set of stable matchings (Ergin and Sönmez, 2006 Proposition 1). While monotonicity may seem natural, the economic appeal of a mechanism does not necessarily stem from its definition per se. Instead, the potential interest of a mechanism is probably mostly determined by the properties of the matchings that it induces. Therefore we investigate the following questions:====The assumption of complete information and the study of Nash equilibria is far from unusual in the school choice literature.==== However, Ergin and Sönmez (2006, Section 8) also consider an incomplete information environment where students do know the priorities and the capacities of the schools but not the realizations of the other students’ types. They show that the immediate acceptance mechanism may induce Bayesian Nash equilibria with unstable matchings in their support.==== This result prompts us to ask the following questions:====We answer all questions above. Regarding the complete information environment, we characterize the family of rank-priority mechanisms that implement the set of stable matchings (Theorem 1). Our necessary and sufficient condition is that the order of rank-priority pairs be “quasi-monotonic”. Loosely speaking, an order satisfies quasi-monotonicity if each priority appears in the sequence only after the precedent priority has appeared with sufficiently small ranks.==== For any non-quasi-monotonic mechanism we exhibit a school choice problem such that the set of equilibrium outcomes is non-empty, the set of stable matchings is a singleton, and yet neither of the two sets is a subset of the other (Proposition 2).====Regarding the incomplete information environment, our second main result (Theorem 2) is a strong impossibility result: all rank-priority mechanisms exhibit the same feature as the immediate acceptance mechanism. Therefore, one conclusion that can be drawn from our results is that in terms of stability of equilibrium outcomes there is no rank-priority mechanism that outperforms the immediate acceptance mechanism.====The remainder of the paper is organized as follows. In Section 2, we describe the school choice problem and rank-priority mechanisms. In Sections 3 Characterization, 4 Incomplete information, we present our results for complete and incomplete information settings, respectively. Section 5 concludes the paper.",School choice: Nash implementation of stable matchings through rank-priority mechanisms,https://www.sciencedirect.com/science/article/pii/S0304406821000343,22 February 2021,2021,Research Article,114.0
"Brookins Philip,Jindapon Paan","Department of Economics, University of South Carolina, Columbia, SC 29208, USA,Department of Economics, Finance and Legal Studies, University of Alabama, Tuscaloosa, AL 35487, USA","Received 3 August 2020, Revised 4 February 2021, Accepted 5 February 2021, Available online 20 February 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102499,Cited by (1),"We analyze the first model of a group contest with players that are heterogeneous in their risk preferences. In our model, individuals’ preferences are represented by a utility function exhibiting a generalized form of constant absolute ====, allowing us to consider any combination of risk-averse, risk-neutral, and risk-loving players. We begin by proving equilibrium existence and uniqueness under both linear and convex investment costs. Then, we explore how the sorting of a compatible set of players by their risk attitudes into competing groups affects aggregate investment. With linear costs, a balanced sorting (i.e., minimizing the variance in risk attitudes across groups) always produces an aggregate investment level that is at least as high as an unbalanced sorting (i.e., maximizing the variance in risk attitudes across groups). Under convex costs, however, identifying which sorting is optimal is more nuanced and depends on preference and cost parameters.","In a ====, such as research teams competing for a grant, workplace competitions, political campaigns, R&D tournaments, lobbying, and team sporting events, individuals belonging to a group make irreversible investments (e.g., effort, time, or resources) with the hopes of securing a prize (e.g., monetary reward or economic rent) for their group.==== One important feature of many contests is that the winner determination process is inherently noisy, implying that the winning group may or may not be the group with the highest investment. Instead, winning is often better expressed ====, with the intuitive properties that the likelihood of winning is increasing in own group investment level, and decreasing in the investment levels of other groups.==== Given that competing in a group contest is in essence a ====, it is immediately of interest to explore how ==== impacts contestants’ behavior.====In this paper, we present and analyze the first model of a group contest in which groups are comprised of individuals with heterogeneous attitudes towards risk. In our setting, individuals’ preferences are represented by a utility function exhibiting a generalized form of constant absolute risk aversion (CARA), allowing us to explore arbitrary configurations of risk-averse, risk-neutral, and risk-loving individuals within and between groups. Apart from the significant complication of risk preference heterogeneity, the group contest setting we consider is simple. Individual group members simultaneously and independently make costly investments, within-group investment aggregation follows a perfectly substitutable production technology, a single group-specific public-good prize is contested, and a group’s probability of winning the prize is determined by Tullock’s imperfectly discriminating contest success function with parameter ====
 (Tullock, 1980).====Our paper is organized into two parts. In the first part, we establish equilibrium existence in the group contest model discussed above under both linear and convex investment costs. To the best of our knowledge, we present the first systematic study and equilibrium analysis in ==== group contest setting beyond the risk-neutral player benchmark. Given a linear cost function, we derive in closed-form players’ participation conditions and obtain group share functions from the representative players’ share functions in each group. While aggregate investment in equilibrium is unique, each individual investment is not unique provided there are multiple representative members within a group. In such a case, not only do the non-representative members free ride, but also some of the representative members. However, with convex investment costs, even though a closed-form solution is not obtainable, we show that there exists a unique equilibrium in which all players participate in the contest with nonzero investments.====Before exploring the consequences of preference heterogeneity on contest behavior, we first characterize equilibrium behavior in two important cases involving player symmetry that yield a symmetric equilibrium at the group level. In the first case, all players’ risk preferences are identical. In equilibrium, all groups are active, all players are representative, and group investments are identical. Interestingly, we find that there exists a unique symmetric risk preference in the nonnegative domain of risk preference parameters (i.e., when players are either risk neutral or risk loving) that maximizes aggregate investment. In the second case, the least risk-averse player is identical across groups, but all other players’ risk preferences are arbitrary. In this case, when the least risk-averse group member is not risk loving, all groups are active and only the least risk-averse players are representative. This equilibrium is preserved even when the least risk-averse members of each group are risk loving, but the number of groups must be sufficiently high.====In the second part of the paper, we investigate how player heterogeneity, both within and between groups, affects investment levels. We not only characterize how individuals behave based on own risk attitude and others’ risk attitudes, but also how a group’s “risk preference” impacts group and aggregate investment. While it is natural to investigate heterogeneity in this way, comparative static predictions, such as establishing the relationship between aggregate investment and the overall degree of contestant risk heterogeneity, are often difficult or impossible to obtain in such a generalized setting. We approach this challenge by focusing most of our analysis in this part on one interesting aspect of heterogeneity in group contests. Specifically, we explore the optimal ==== of heterogeneous players, and ask the following questions: For a given set of players with heterogeneous risk preferences, how should the players be sorted—====—into competing groups so as to maximize aggregate effort? For example, if a manager wishes to hold a competition between groups of salespeople with the goal of maximizing total sales, how should she sort various risk attitudes into competing groups?==== Should she create a ==== sorting, whereby workers are “evenly” distributed across groups such that overall group risk preferences are similar across groups, or should she instead create an ==== sorting, in which similar risk attitudes are pooled together so that one group contains the least risk-averse workers, another group contains the most risk-averse workers, and everything in between? To make each sorting comparable, we assume a compatible set of players such that when the manager uses a balanced sorting she creates identical groups consisting of heterogeneous workers, and when she uses an unbalanced sorting she creates heterogeneous groups consisting of identical workers.====Sorting is especially relevant presently, as organizations have increasingly adopted team-based relative performance incentives over the last few decades.==== Furthermore, recent findings from laboratory experiments support the adoption of team-based incentives, with some studies finding that team contests stimulate higher efforts than contests between individuals (Chen and Lim, 2013, Li et al., 2019). So far, however, the theoretical literature on optimal sorting in contests is limited to two studies, each considering the case of risk-neutral players with ability heterogeneity.==== From a contest design perspective, sorting personnel into competing groups is a low-cost or costless operation for a manager, but as we show, the impact of sorting on aggregate effort is nontrivial. In practice, it may be difficult to obtain accurate cardinal measures of individual characteristics, such as ability or risk attitude. Sorting, however, only relies on ordinal rankings, which are generally easier to obtain. Ordinal rankings can be acquired directly from historical data on past performance and through repeated interactions, or inferred indirectly via data that are correlated with the characteristic being ranked. Specifically, the degree of one’s risk aversion has been found to be significantly correlated with easy-to-observe individual characteristics such as age (see, e.g., Albert and Duffy, 2012) and sex (see, e.g., Borghans et al., 2009, Croson and Gneezy, 2009, Charness and Gneezy, 2012). Moreover, many organizations are already routinely collecting risk attitude metrics, among others, by means of psychometric testing completed during job applicant screening; risk-based sorting is particularly well-suited in such organizations.====When costs are linear, we show that a balanced sorting always produces an aggregate investment level at least as high as an unbalanced sorting. However, when costs are convex, our exploratory numerical simulations show that the optimal sorting may be balanced or unbalanced, and the dominance of one sorting over the other depends on the average measure of risk aversion across players and its variation, and also the cost function. To explore this further, we restrict our attention to the case of ==== in risk preference parameters, and employ a regular perturbation problem approach in order to find an approximate, but explicit representation of aggregate investment. By weak heterogeneity, we refer the case when players’ risk attitudes are closely centered about a given average measure of risk aversion. Such weak forms of heterogeneity are arguably the most interesting cases to explore, because naturally occurring forces, such as occupation self-selection, tend to pool together individuals with similar attributes. In the quadratic approximation, we show that the optimal sorting of players critically depends on the sample variance in risk attitude across players and across groups and the cost function. Under quadratic costs, the balanced sorting is optimal when all players are risk averse, but it is possible that the unbalanced sorting is optimal when players are risk loving. The optimal sorting is even more nuanced under a general class of convex cost functions.====Our paper is organized as follows. In Section 2, we summarize the literature exploring the impact of various risk preferences on behavior in contests. In Section 3, we present our model of a group contest with risk preference heterogeneity and analyze equilibrium existence and uniqueness. In Section 4, we explore the effects of heterogeneity on investment levels and discuss the optimal sorting of players. We discuss our results and conclude in Section 5.",Risk preference heterogeneity in group contests,https://www.sciencedirect.com/science/article/pii/S0304406821000379,20 February 2021,2021,Research Article,115.0
"Aspri Andrea,Beretta Elena,Gandolfi Alberto,Wasmer Etienne","Johann Radon Institute for Computational and Applied Mathematics (RICAM), Austria,Department of Mathematics, NYU-Abu Dhabi, United Arab Emirates,Dipartimento di Matematica, Politecnico di Milano, Italy,Department of Economics, Social Science Div. NYU-Abu Dhabi, United Arab Emirates","Received 28 May 2020, Revised 15 November 2020, Accepted 23 December 2020, Available online 14 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102490,Cited by (28),"We extend the classic approach (SIR) to a SEAIRD model with policy controls. A social planner’s objective reflects the trade-off between mortality reduction and GDP, featuring its perception of the value of statistical life (PVSL). We introduce realistic and drastic limitations to the control available to it. Within this setup, we explore the results of various control policies. We notably describe the joint dynamics of infection and economy in different contexts with unique or multiple confinement episodes. Compared to other approaches, our contributions are: (i) to restrict the class of functions accessible to the social planner, and in particular to impose that they remain constant over some fixed periods; (ii) to impose implementation frictions, e.g. a lag in their implementation; (iii) to prove the existence of ","A COVID-19 outbreak has begun in China at the end of 2019 (====), later spreading to most other countries and causing a large number of infected individuals and deaths. In Italy, the first country to be hit after China, the first confirmed autochthonous case was recorded on February 21, and the first death on February 22 (====); the first death in US was recorded on February 28th (====).==== The outbreak has so far caused at least ==== million recorded cases, and ==== recorded deaths (====), with real numbers estimated at much higher values. In New York City there have been at this time at least 27,000 deaths, corresponding to 0.335% of the population.====Massive regulatory responses have been put in place by most local and central governments. Efforts to try to mitigate the outbreak have been multidimensional, from mobility restrictions, imposition of masks, sanitation, distancing, to the halting of educational, economic and social activities. The most severe restrictions have been named lockdowns.====The assessment of the effect of these measures is not simple, due to a variety of factors including non-linearities, random clustering of outbreaks, superspreading events etc. In addition, favorable situations at a certain time might hide later epidemic resurgence. In any case, there is a general consensus that the combination of all the above mentioned measures has reduced the spread of the virus and the potential mortality.====Although it might not be easy to assess the direct economic effects of containment measures, it is clear that direct restrictions, and indirect effects on demand have had a large economic costs. As of October 2020, the IMF economic projections predict a loss in real GDP in 2020 of 4.4% worldwide, as opposed to ＋3.45% in the four years before (2016–2019). Even with the IMF forecast for the rebound of 5.2% in 2021, the cumulated loss over 2020 and 2021 ==== is expected to reach 6.1% of World GDP. In advanced economies, these losses, calculated relative to specific regional trends, would have been 5.95%, including 8.15% in the European Union and 5% in the United States, similar in Emerging Market and Developing Economy (−5.95%) or Sub-Saharan Africa (−5.3%). These are massive numbers, quite different by areas of the world, and updated regularly with likely higher GDP losses.====In this paper, we are agnostic about the relative weight of the various restrictions, but argue that the combination of different measures jointly determines an economic loss as well as a containment of the epidemic. We call the measures “interventions”; sometimes, for convenience or when describing country measures, we refer to them as “lockdowns”, still referring to the combination of all measures. We provide here a framework to think about costs and benefits; we then simulate the initial scenario of the spreading of COVID-19 in western countries in March 2020 and evaluate several ====.====As indicators of the effects of the epidemic we adopt the number of COVID-19 fatalities, while GDP loss is used as a proxy for the economic effects of containment measures. The two indicators have been selected for their reliability: GDP is a standard economic indicator, while COVID-19 mortality is regularly monitored and made public in many countries; excess mortality would be an even more reliable indicator, where available.====For a functional description of the epidemic, we consider a compartmental model; models of this type are, in fact, relatively simple to set up and analyze, they provide reliable approximations of various epidemic features (====), and are widely used in modeling and relied upon in policy planning (====, ====, ====, ====). In particular, we develop in Section ====, representing the expectation of an available vaccine: in some prototypical simulations we take ==== as the end of the first quarter 2021. We then consider a proxy for containment policies that, as mentioned, encompasses all possible restrictions: the measures are summarized by a function ====, ==== being absence of any restriction and ==== being the complete shutdown of all non vital activities; ==== simultaneously reduces the chances of infection and GDP. As a normalization, ==== will be assumed to linearly affect the infection rate and has a concave effect on GDP.====We then focus on the social planner’s objective of containing both COVID-19 fatalities, evaluated in terms of an attributed value of life, and GDP loss over the estimated time frame. For both dimensions, the objective of the social planner discounts the future and optimizes the combined loss functional over the available lockdown policies. While the infection has started at some earlier time, ==== for convenience, the planner determines policies and acts at some time ====; in addition, the planner selects a time frame ==== for the evaluation, based on assessment of future technological development. The choice of ==== might seem restrictive at first, but the validity of our original guess of a decisive attenuation of the epidemic at the end of the first quarter 2021 indicates that the possibility of such a guess was actually well founded.====One of the main novelties of our research is that, to the contrary of the usual assumption that control policies can instantly vary with time (see ==== and various economic papers cited below), we restrict to more realistic policies that are constant over reasonably long periods of time (of the order of weeks, for instance): no shorter period is, in fact, allowed for the measures to be successfully implemented (====). In addition, since aggregate variables such as average behavior or total production cannot change instantaneously, we assume that there is some friction in implementing life changing policies, so that the lockdown level moves continuously during transitions (which last a few days). These restrictions are usually not considered in theoretical analysis but are first order constraints of policy making. We assume that a specific class of such intervention policies is available for the social planner.====Another novelty of our paper is that we carry out in Section ====, show, in general, existence of an overall optimal control in terms of possibly irregular functions, and then show some continuity properties of the control; we take a direct approach showing existence of a control ==== which is optimal within the preassigned class of realistic controls available to the social planner; we use for this a lower bound on the rate of change of the openness function ====.====To illustrate the main arbitrage present in our set up, ==== shows GDP loss and mortality of a single lockdown level ==== kept constant for the entire observation period, see ==== for a similar assessment of the trade-offs involved, implicit or explicit in most economic works discussed in next Section. The blue curve ==== is parametrized by the lockdown level, from laissez-faire ==== to ====, and represents the various options for the social planner: it can be thought of as a technical rate of transformation between economic activity and health safety. Generally speaking, it is best to be close to the origin, as otherwise very little advantage is gained in one variable, at a huge cost in the other. A specific choice of the intervention level, however, can be determined as optimal solution for each given value of life through the social planner welfare function: the indifference curves of the welfare function (light blue curve) define an optimal rule, when it exists. Its slope reflects the marginal rate of substitution (MRS) between mortality reduction and GDP losses and under simplifying assumptions, it is the inverse of the statistical value of life, as we will explain later.====There is, however, a surprising feature: in some scenarios, laissez-faire is the optimal solution for all values of life below a certain threshold, while immediately above the threshold a sizeable restriction policy is optimal. This determines that at the threshold itself two different optimal policies coexist (due to the non convexity of ==== for large ====). This situation, which we see appearing for various policy classes, is indicative of the variety of optimal responses depending on the value attributed to statistical life; it also clarifies some of the differences in the proposed policies (among which laissez-faire has been initially advocated by some governments). Although such discontinuity in the optimal policy takes place presumably only at isolated values of the value of life, we argue that a social planner particularly responsive to the population diverse views might be inclined to act according to a value of life near the critical one. In addition, discontinuity of controls around the critical value can have relevant social consequences in terms on how to evaluate potential alternatives to a given containment policy. We then provide some arguments indicating that, in general, the optimal control is likely to depend smoothly on the statistical value of life, provided that either the social cost attributed to COVID-19 mortality or the discount rate for future events is large enough.====In one prototypical case the social planner imposes a lockdown on March ====, Day ====, till May ====, Day ====, and then a reopening level till the end of the period on April ====, 2021, Day ====, at levels to be decided (cf. ====). With no restrictions the GDP loss of the four quarters would have been 1.41% with a 1.05% Covid-19 mortality. An optimization is then carried out assuming a value of life, in relation to Covid mortality, of around ==== (in Europe) to ==== (in US) million dollars; the optimal solution determines a 17.40% GDP loss and reduces mortality by 81.97%, see ====. Although the limitation to two lockdown levels in this specific simulation is unrealistic, the values we obtain are comparable with those observed at the end of Quarter ==== with a 13% GDP contraction in France and a 31.4% GDP reduction in US. The actual level of reopening in most countries has been much higher than that in our optimal solution, with the consequence of an exceptional economic rebound, but also a deadly second (and even third) wave: a situation described by a non optimal reopening level in our second simulation of Section ====.====In Section ====, we further show how to extend the approach to capital accumulation in a Solow and a Ramsey approach, and relegate their mathematical analysis in ====, ====.====Pairs of points in which ==== happens cannot overlap, hence indicate ==== be the smallest points of each pair, arranged in increasing order; let ====; and let ==== be the such that ==== for ==== for ====. We have shown that ==== satisfies ==== for these values of ====’s and ====’s. This finishes the proof.  □",Mortality containment vs. Economics Opening: Optimal policies in a SEIARD model,https://www.sciencedirect.com/science/article/pii/S0304406821000288,14 February 2021,2021,Research Article,116.0
"Atolia Manoj,Papageorgiou Chris,Turnovsky Stephen J.","Florida State University, United States of America,IMF, United States of America,University of Washington, United States of America","Received 3 August 2020, Revised 12 December 2020, Accepted 15 December 2020, Available online 12 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102481,Cited by (6),"Covid-19 has dealt a devastating blow to productivity and economic growth. We employ a ==== framework with heterogeneous agents to identify the tradeoffs involved in restoring the economy to its pre-Covid-19 state. Several tradeoffs, both over time, and between key economic variables, are identified, with the feasible speed of successful re-opening being constrained by the transmission of the infection. In particular, while more rapid opening up of the economy will reduce short-run aggregate output losses, it will cause larger long-run output losses, which potentially may be quite substantial if the opening is overly rapid and the virus is not eradicated. More rapid opening of the economy mitigates the increases in both long-run wealth and income inequality, thus highlighting a direct conflict between the adverse effects on aggregate output and its distributional consequences.","COVID-19 is a crisis like no other that has trapped the global economy in a Great Lockdown. As countries have tried to manage the associated health risks that required locking down their economies, what has followed has been an unprecedented collapse in economic activity, combined with dramatic turbulence in financial and commodity markets (====).====). In addition, lockdowns are hampering food distribution, and spikes in food prices are further increasing hardship among poorer households; this year, an additional 83 to 132 million may suffer from acute hunger due to the COVID-19 pandemic (====).====With countries preparing for the re-opening of their economies, as part of the recovery phase of the crisis, they are actively experimenting using a variety of containment measures that confront policy makers with a very challenging calculus about saving lives and livelihoods. The aim of this paper is to shed new light on key mechanisms and implications of measures used in the process of re-opening. The debate on how best to re-open the economies from the COVID-19 lockdown has an important long-run dimension that has received less attention in the literature. Our objective is to focus on this aspect. We show that while opening too fast may reduce short-run economic fallout, this comes at the cost of adverse long-run aggregate outcomes (output and consumption), while also introducing tradeoffs with respect to the impact on various inequality measures (wealth, health, income, and consumption).====This paper adopts the framework developed by ==== In the short run, however, this relationship is conditioned by the time path a given productivity change is expected to follow, and the differences in the consumption-smoothing motives it generates for rich and poor agents. A key feature of this labor allocation-relative wealth mechanism is that it introduces hysteresis in the dynamic adjustment characterizing the relative holdings of capital. Thus, a central insight of ACT is that the effects of a productivity change of a given magnitude on the long-run distributions of both wealth and income are crucially dependent upon the time path that the productivity change is assumed to follow. This is in sharp contrast to the dynamics of the aggregate economy, where the long-run equilibrium is independent of the transitional path.====To examine the consequences of the process of opening, we integrate the ACT framework into a recent two-sector Ramsey model of health ====, extended to include the evolution of the COVID-19 virus, albeit in a stylized way. An important characteristic of the model is that the interaction between the speed with which the economy is opened and the spread of the virus – the key issue in the current debate – will determine the nature of the post-COVID-19 steady state. In contrast to the basic ACT model, the interaction between the spread of the infection and the speed of opening up the economy may have long-run aggregate effects, as well as permanent distributional effects, depending upon the chosen speed. Moreover, the interaction between the speed of opening and the spread of the virus introduces various tradeoffs. Opening the economy more rapidly is likely to cause the virus to persist indefinitely, and while this may alleviate the short-run decline in aggregate economic activity, its permanence will exacerbate the long-run losses in production. In addition, more rapid opening with a faster transition will tend to mitigate the increase in long-run wealth and income inequality that the response to the pandemic generates. Our analysis also brings to the fore another source of inequality, namely health inequality, which is shown to be directly linked to wealth inequality.====Our numerical simulations suggest that, for what we view as a plausible rate of opening, the permanent effects on the economy are not inconsequential. For example, the loss in output after about four years is 2%–3% at annual rates, although asymptotically it is much smaller, while the inequality may increase by 1.5%, which already noted is not trivial. Our results also suggest that since poorer economies may lack the infrastructure to open as rapidly as more developed economies can, they are likely to suffer more adverse permanent distributional effects.====We should emphasize that, while much of the debate among policymakers identifies the increase in income inequality as an undesirable permanent consequence of the COVID-19 experience, the mechanism proposed in this paper is very different. Much of the debate attributes the inequality to small firms and businesses, temporarily closed during the pandemic, and being unable to recover. Our analysis generates the long-run inequality as a consequence of the intrinsic dynamics of the economy as it transitions in the process of re-opening. This stems from the differential abilities/desires of individuals, having different endowments, to save (or dissave to smooth consumption), together with their corresponding/associated response of leisure.====The rest of the paper is organized as follows. Section ==== reviews the literature with particular focus on the distributional impact of COVID-19. Section ==== sets out the components of the model, while Section ==== specifies and derives the alternative inequality measures. Section ==== describes the calibration and the numerical simulations, while Section ==== draws the main conclusions.",Re-opening after the lockdown: Long-run aggregate and distributional consequences of COVID-19,https://www.sciencedirect.com/science/article/pii/S0304406821000197,12 February 2021,2021,Research Article,117.0
"Ravid Doron,Steverson Kai","Kenneth C. Griffin Department of Economics, University of Chicago, USA,DCI Solutions, USA","Received 29 May 2020, Revised 25 September 2020, Accepted 28 December 2020, Available online 11 February 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102480,Cited by (4),"We study a static self-control model in which an agent’s preference, temptation ranking, and cost of self-control drive her choices among a finite set of options. We show it is without loss to assume the agent’s temptation ranking is the opposite of her preference. We characterize the model by relaxing the Weak Axiom of Revealed Preference (WARP), and exploit WARP violations to identify the model’s parameters.","Temptation’s impact on behavior has been the subject of many economic studies.==== These studies often view temptation as pushing the agent’s decisions away from his preferences. The worst temptation, therefore, is one that steers the agent towards his least preferred alternative. In this paper, we ask how can one identify such bad temptation from behavior, and how it differs from other forms of temptation?====In particular, we are interested in differentiating two ways of modeling an agent’s behavior. In the first model, the agent’s choices maximize ====where ==== is the agent’s choice set, ==== is the agent’s utility function, ==== is each alternative’s temptation value, and ==== is strictly increasing in both arguments. We refer to (1) as a ====. This models follows the standard approach of modeling temptation as an agent balancing his true preferences, ====, against the self-control cost of resisting the most tempting option available.====The second model specializes the general-temptation model to the case of ==== in which the agent’s temptation value directly opposes his utility; that is, ====We focus on one’s ability to differentiate and test these models in a finite, static choice environment. This focus sets us apart from the rest of the revealed-preference literature on temptation, which focuses on dynamic choices among an infinite set of possible alternatives. Thus, we avoid making any irrefutable continuity assumptions, while identifying temptation via a different channel than commitment preferences, that is, a preference toward smaller future choice sets. We consider the latter to be a particularly attractive feature of our approach, for two reasons. First, empirically, the extent to which people actually exhibit preferences for commitment is unclear (e.g., see  Laibson (2015)). Second, unlike the existing literature, which requires the agent to be sophisticated enough to foresee his own temptation, our approach also applies when the agent is naive.====Our analysis leverages the fact that the above models can violate the Weak Axiom of Revealed Preference (WARP). To illustrate, consider a dieting agent choosing an afternoon snack. The agent’s refrigerator contains some combination of the following three items: a healthy and non-tempting yogurt, a somewhat unhealthy and tempting sorbet, and a highly unhealthy and highly tempting ice cream. The agent always chooses yogurt when it is paired with exactly one of the other items—in these cases, the agent is willing to exert the costly self-control necessary to maintain his diet. However, resisting the very tempting ice cream is highly costly, and the agent is able to do so only by reminding himself how damaging the ice cream would be to his diet. Therefore, when all three items are available, the agent settles on the sorbet as a form of compromise: it does not require as much self-control as picking yogurt and is not as bad for his diet as ice cream. This set of choices violates WARP since the agent chooses yogurt over the other two items individually but not when all three items are available.====Proposition 1 proves a three-way equivalence between general temptation, bad temptation, and a relaxation of WARP: the Axiom of Revealed Temptation (ART). ART requires each menu ==== to contain at least one item ==== such that WARP is obeyed on the collection of subsets of ==== that contain ====. The option ==== can be interpreted as the most tempting item in ====. Our axiom, therefore, ties WARP violations to changes in the menu’s most tempting item.====One might wonder whether bad temptation is special. Specifically, can ==== and ==== be related in the general-temptation model such that the relation can never be refuted? This question seems particularly pertinent given our finite setting, which often leads to a multiplicity of representations. Bad temptation, however, turns out, to be unique in its ability to explain all behaviors consistent with ART. In particular, Proposition 2 shows that when at least three alternatives are available, a choice behavior exists that can be represented only with a general-temptation model in which ==== and ==== represent opposite rankings. In other words, although all temptations can be seen as bad, some temptations must be.====Taking bad temptation as a normalization, we show one can identify the ordinal ranking of ==== (and therefore ====) using two behaviorally defined relations on ====: an acyclic binary relation ==== and an equivalence relation ====. Proposition 3 shows ==== is a lower bound on the agent’s strict preference, and ==== is an upper bound on indifference in the agent’s preferences. More precisely, given a bad-temptation representation, ====, ==== if ====, and ==== only if ====. Moreover, these constraints are the ==== relevant ones—any preference relation with these properties will be represented by ==== in some bad-temptation model of the behavior. Hence, ==== and ==== reveal all the constraints placed on ==== through behavior.====Others have already noted WARP can be violated in the presence of temptation (e.g., Noor and Takeoka, 2010, Fudenberg and Levine, 2011, Fudenberg and Levine, 2012, Masatlioglu et al., 2014, Noor and Takeoka, 2015). Most closely related to our work are the papers by Noor and Takeoka (Noor and Takeoka, 2010, Noor and Takeoka, 2015), who use a two-period choice environment to study temptation models that lead to WARP violations. Noor and Takeoka (2010) study two models, one of which is a specialization of our general-temptation model to the case in which ==== for some convex function ====. Their second model is more general and nests the model of Noor and Takeoka (2015), but is neither a generalization nor a specialization of our model. Despite the generality of these models, one can show their induced static behavior satisfies ART. As such, Proposition 1 implies that, in a finite and static choice environment, these models are all equivalent to our bad-temptation model.====Another closely related model is the one by Masatlioglu et al. (2014), who study a model in which the agent has a fixed willpower budget. Since changes in a menu’s composition can change the willpower required to choose a certain alternative, their model can exhibit WARP violations. Unlike the rest of the temptation literature, Masatlioglu et al. (2014) do not rely on sophistication to identify their model. Instead, they assume the analyst can directly observe the agent’s “cold preference” – i.e., the preferences the agent would have if she were not affected by temptation. By contrast, we do not assume one can separately observe a “cold preference”. Without observing this “cold preference”, Masatlioglu et al.’s (2014) model turns out to be a strict special case of the bad-temptation model. We refer the reader to Appendix C for more details.====Our paper also belongs to the literature applying the revealed-preference methodology to study choice models violating WARP. Several prominent examples include choice under limited attention (Masatlioglu et al., 2012), choices with endogenous reference points (Ok et al., 2015), the rational- short-list method (Manzini and Mariotti, 2007), and the rationalization model (Cherepanov et al., 2013). Other than Ok et al. (2015), all these models use a revealed-preference approach similar to ours, relying on information that can be revealed using finite data. One can show the bad temptation model neither nests nor is it nested in the models of Manzini and Mariotti (2007), Ok et al. (2015), and Cherepanov et al. (2013). Bad temptation is, however, nested by Masatlioglu et al.’s (2012) model. See Appendix C for more details. Finally, in a concurrently written paper, Kıbrıs et al. (2018) provide axiomatic foundations for an endogenous reference-dependent model based on conspicuity. ART and their main axiom, called Single Reversal, turn out to be equivalent. As such, bad temptation and Kıbrıs et al.’s (2018) conspicuity-based reference-dependent models are indistinguishable.",Bad temptation,https://www.sciencedirect.com/science/article/pii/S0304406821000185,11 February 2021,2021,Research Article,118.0
Bonneuil Noël,"Institut national d’études démographiques, 9, cours des humanités, 93322, Aubervilliers cedex, France,École des hautes études en sciences sociales, 54, bld Raspail, 75006, Paris, France","Received 12 August 2020, Revised 12 November 2020, Accepted 16 January 2021, Available online 11 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102494,Cited by (4),"Triage protocols for intensive care units are based on priorities assigned to presents, but ignore patients about to arrive, so a priority newcomer may not find a ventilator and its associated nursing staff available because they are occupied by a lower-priority patient who however was present at the moment of assignment. Conversely, waiting too long leads to losing elderly patients who could have been saved by ventilators. As age and sex are major determinants of mortality by Covid-19 and have the merit, in contrast to other priority criteria, of being immediately available to health professionals, the criterion is the minimization of the mean mortality rate weighted by age- and sex-specific life expectancies. The dynamics is a queuing process involving mortality and return home flows and competition between ages. The result is the determination of an optimal threshold age that can guide triage.","The influx of patients during the Covid-19 crisis in hospitals with a limited number of ventilators and their associated nursing staff, beyond the tragedy, can lead, at the height of the crisis, to an excess of demand (by patients) over supply (in ventilators and their associated nursing staff). Economic calculation is challenged when it comes to maximizing the benefits produced by scarce resources, in the demand for a fair allocation. Indeed, the rationing of ventilators leads health professionals to make difficult choices about who to admit to a ventilator and who to put on hold. This choice is necessary in a pandemic period and should not be left to the discretion of clinicians in the heat of the moment (====).====This is the project of this article: to provide assistance in the management of the queue of Covid-19 patients in demand for ventilators in a situation of shortage. That is why I will reason with measures that are immediately available and accepted by health professionals, such as life expectancy, instead of the statistical value of life or Sepsis-related Organ Failure Assessment (SOFA) scores or Quality Adjusted Life Years (QALYs), which require medical tests and are not used in French hospitals. ====, referring to ====, used the “value of statistical life”, measured as a multiple of the annual GDP per head, for a theoretical discussion but without application to empirical data. These “values of statistical life” are not used in French hospitals and my wish in this article is to propose a procedure that can easily be used in hospitals. Quality Adjusted Life Years (QALYs) are a possible criterion for the selection of patients to ventilators, except that this is not practiced in French hospitals either. In addition, candidates to ventilators for Covid-19 are in such poor health that QALYs are not effective in assessing the survivability of Covid-19 patients (====).====Hospital queue management economists also propose priority measures based on what medical staff say: The Quick SOFA score (==== pointed out the lack of systematic admission criteria in intensive care units, arguing that the identification of the most unstable patients is difficult and depends too much on the training and subjectivity of the physicians in charge. These authors propose a criterion based on the regression of the patients’ final outcomes according to whether or not they were admitted to intensive care units, their clinical severity factors, their characteristics, and seasonal factors. However, this regression is true for the 15 selected hospitals (their geographical locations are not specified) and has no reason to be standardized to other hospitals. The regression reflects what was done in these hospitals, including the subjectivity of the physicians, and is therefore not a fair criterion.====The criterion of life expectancy by age and sex that I use has the merit of being easily calculable, of avoiding speculation on the behavior of Covid-19 patients under ventilator, and of being familiar to healthcare professionals. For example, Bruno Mégarbane, head of intensive care units at Lariboisière Hospital, France, insisted that what counts is “saving lives” (French radio ====, November 3, 2020), which I interpret as “saving years of life”. However, the procedure I propose can be adapted to other priority measures, if they are available for each patient in a timely manner at the time of admission to the hospital.====In the case of Covid-19, age and sex indeed have played an eminent role in mortality risk and in practice have served as a sorting criterion, even if not in the optimal way like the one I propose here. For example, on 28 April 2020, in France, according to an estimate provided by the French Information System for Monitoring Victims of Attacks and Exceptional Health Situations (SIVIC====), the average age of patients in intensive care was 62.1 years, compared with 84.6 for those in conventional hospitalization, 85.1 in follow-up care and rehabilitation facilities and services (====), 70.7 for the deceased, and 75.8 for those returning home. Patients on ventilation were therefore considerably younger, although the flow of patients on ventilators aged 80 years and over was not zero. In a press article dated 30 July 2020, ====, on the basis of data from a French parliamentary commission of inquiry, accused that old people were removed from hospitals at the peak of the influx of Covid-19 patients.====Biological age (====“Reserving” ventilators for patients who have not yet arrived amounts to setting a threshold age above which patients are excluded from ventilators, and below which patients who are present are assigned to ventilators. There would therefore be an optimal threshold age that takes into account laws of probability of arrival at the various ages, mortality rates with or without a ventilator, rates of assignment to ventilators, and rates of return home, out from the intensive care unit or from the waiting room.====The objective I propose is to maximize the total number of expected years left to live for the population of patients eligible for ventilation, which, in relation to the population entering the hospital system, amounts to minimize the mean (over time) mortality rate weighted by life expectancies. The result is the identification of an optimal threshold age, which allows for managing the queue in the best way possible for the present patients ==== the patients about to arrive.====My theoretical contribution is therefore to articulate competing queues with a finite or an infinite number of servers, age and sex competition, and to combine in an original way queue theory with age- and sex-based utility theory. I also estimate instantaneous rates of transition (to death, return home, or assignment to ventilators) from the data available. The possibility of implementing it practically is adapted to the case of intensive care units in Covid-19 time. The result is that I provide a threshold age based on arrivals, mortality, and returns home. The rates of all these movements are estimated based on early findings and are therefore specific to the epidemic and the means allocated. The threshold age, as called for by ==== for example, is no longer based on intuition and not only avoids the first-come, first-served rule, lottery, or questionable criteria such as priority to those who can attest to good deeds in the past or to priority given to patients claiming social utility, but also provides a normative framework for decision-makers.",Optimal age- and sex-based management of the queue to ventilators during the Covid-19 crisis,https://www.sciencedirect.com/science/article/pii/S030440682100032X,11 February 2021,2021,Research Article,119.0
"Augeraud-Véron Emmanuelle,Fabbri Giorgio,Schubert Katheline","GREThA, Université de Bordeaux, France,Univ. Grenoble Alpes, CNRS, INRAE, Grenoble INP, GAEL, Grenoble, France,Paris School of Economics, Université Paris 1 Panthéon-Sorbonne, France","Received 17 July 2020, Revised 18 December 2020, Accepted 3 January 2021, Available online 11 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102484,Cited by (2), towards future generations. The model is explicitly solved and the ,"The hopes of the post-war period that infectious diseases were behind us thanks to control and treatment improvements (Fisher, 1995) have proved to be false: the number of emerging infectious diseases (EID) has continued to rise since the 1950s (Smith et al., 2014). The fear of a pandemic has remained vivid in the scientific community, as shown by the Clade X exercise==== hosted at the Johns Hopkins Center for Health Security in 2018. Indeed, the number of outbreaks leading to epidemics and even pandemics has accelerated sharply over the last twenty years.====60% of these EID are caused by zoonotic pathogens, mainly (72%) of wildlife origin (Jones et al., 2008). Examples include AIDS, SARS (Severe Acute Respiratory Syndrome), MERS (Middle East Respiratory Syndrome), Nipah Virus, Avian influenza, Ebola virus and Influenza A virus subtype H1N1, as well as COVID-19. The large number of EID of zoonotic origin makes it natural to link the emergence of these diseases to the loss of biodiversity. The current COVID-19 pandemic, caused by the transmission of a pathogen from animals to humans, has put this link to the forefront of the stage.====While all the underlying mechanisms are not fully understood, a growing scientific literature documents this complex link (Morand et al., 2014, Morand, 2018). Both a dilution effect (decreasing relationship between biodiversity and EIDs, Civitello et al., 2015) and an amplification effect (increasing relationship between biodiversity and EIDs, Wood et al., 2017) are at work simultaneously (Rohr et al., 2019). The predominance of one effect over the other depends on the spatial scale at which the phenomenon is studied (Johnson et al., 2015). On a national or global scale, the dilution effect dominates (Halliday et al., 2020, Morand et al., 2014) and conserving biodiversity appears as a prevention against EIDs.====There are at least two reasons for the dilution effect (Keesing et al., 2010). Firstly, the decline in biodiversity leads to an increase in the prevalence and transmission rates at the local level and to a selection effect of the most harmful pathogenic strains. Second, habitat destruction brings species together and brings them closer to humans (Wolfe et al., 2005).==== The promiscuity between several species, in the wild, in captivity or in breeding, increases the risk of transmission and mutation of pathogens, and makes transmission to humans more likely (LoGiudice et al., 2003).====The economic cost of these diseases is very high (Sands et al., 2016). The West African Ebola epidemic of 2014 led to a 10% loss of GDP in Sierra Leone and Guinea (World Bank, 2014). The COVID-19 pandemic and the policy measures adopted to fight it, that, in the absence of a vaccine or effective treatment, led to the containment of more than half of the planet during more than 2 months, resulted in severe recessions in most countries, probably the worst since the Great Depression. In the Interim Report of September 2020 of the OECD Economic Outlook (OECD, 2020) global growth is projected to −4.5% in 2020, and growth in the Euro area to −7.9%, unusually large uncertainties around these figures being acknowledged. The economic costs are direct medical costs and mostly indirect costs coming from the disruption of supply, in the industry, in the transportation sector and particularly in aviation, in the arts and entertainment sector, in the tourism sector etc.====The basic question we are interested in is whether governments, at the world level, should coordinate to invest in biodiversity conservation as a prevention policy against the risk of emerging infectious diseases, or whether they should rely on mitigation policies reducing social interactions like lockdowns once the risk materializes, or also whether they should do both. There are two levels to this question. First, a choice has to be made between investing upstream (and how much) in biodiversity conservation, at the price of an economic loss, to reduce the probability of occurrence of pandemics, or not. Second, once a pandemic hits, the severity of the mitigation policy put in place to attenuate its negative effects on health and mortality has to be decided. We design a stylized model allowing us to shed light on these issues, first from a theoretical point of view, and then with a calibration of the model to data on COVID-19 and simulations. A huge literature in economics on the COVID-19 pandemic has emerged since March 2020.==== But the question of the links between biodiversity losses and EIDs we study here is, to the best of our knowledge, totally overlooked in this economic literature.====The theoretical framework we adopt is the one of a long-term macrodynamic economic model embedding biodiversity on the one hand, and epidemics, through a standard SIR model, on the other hand. The planner decides on the effort to be made to preserve biodiversity in order to reduce the likelihood of new EID. How much biodiversity is preserved depends on the size of land left undeveloped,==== that supports intact natural ecosystems such as primary forests. This size impacts the probability (and then the frequency) of occurrence of epidemic outbreaks. It comes at an economic cost, as land used for human activities (agricultural production, infrastructures, human settlements etc.) is a direct input in the production process.====In addition to this risk-reducing policy, the planner also has the possibility to decide on lockdown mitigation policies in the event of a pandemic, in order to reduce population mortality, at the cost of a reduction in productive activities.====Given the simplicity of the model we can solve it explicitly. Using a social welfare function embodying society’s risk aversion, aversion to fluctuations, degree of impatience and altruism towards future generations, we characterize the optimal mitigation policy and the optimal allocation of land to biodiversity conservation and study their behaviors in terms of the model’s parameters.====We show that the optimal lockdown is more severe in societies valuing human life more, and that the optimal biodiversity conservation is greater in more “forward looking” societies, with a small discount rate and a high degree of altruism towards future generations. We also show that societies accepting a large welfare loss to mitigate the pandemics are also societies doing a lot of prevention, not to have to incur the loss too often and so all the more since risk aversion or the risk of pandemics absent any biodiversity are higher.====To calibrate the model we use data from Gollier (2020) on the consequences in terms of mortality and economic activity of the ====, a “flatten the curve” mitigation strategy and a “crush the curve”, or suppression, strategy. We exhibit the terms of the trade-off between the loss of lives and the loss of GDP for the whole set of mitigation strategies, from ==== to suppression. We compute the optimal mitigation policy as a function of the relative value the planner assigns to life over the economy, and the optimal prevention strategy, depending on the former and on the risk parameters.====The idea of assessing the economic impact of disasters that arise from “environmental” causes is the source of a well-stocked literature, both in the empirical works (see for instance Noy, 2009, and the contained references) and from the theoretical point of view (see for instance Akao and Sakamoto, 2018 or Bakkensen and Barrage, 2016). The general idea of these contributions is to understand the impact of environmental disasters on growth and development. Particularly inspiring for our research are the papers by Bretschger and Vinogradova, 2016, Bretschger and Vinogradova, 2019 and Douenne (2020). The latter in particular studies the possibility of dealing with disaster of endogenous probability==== and then the idea of disaster prevention. A related contribution is the recent paper by Brock and Xepapadeas (2020) where the authors model the possible appearance of a shock linked to the arrival of an epidemic whose probability depends on the capital accumulated in the economy and the temperature anomaly caused by the use of non-renewable resources. In the present paper the focus is different since, for the first time we link biodiversity conservation, the risk of zoonotic pandemics, population and economic dynamics and mitigation policies.====The paper is organized as follows. Section 2 introduces the model’s assumptions and formulation. In Section 3 we describe the explicit solution of the model. Section 4 contains numerical simulations while Section 5 concludes. All the proofs are reported in Appendix A Proofs of Lemmas, Appendix B Proofs of.",Prevention and mitigation of epidemics: Biodiversity conservation and confinement policies,https://www.sciencedirect.com/science/article/pii/S0304406821000227,11 February 2021,2021,Research Article,120.0
Kirkegaard René,"Department of Economics and Finance, University of Guelph, Canada","Received 25 May 2020, Revised 25 November 2020, Accepted 14 January 2021, Available online 8 February 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102478,Cited by (0),"This paper compares the first-price auction and the second-price auction with several asymmetric bidders who are either weak or strong. The ranking of these auctions in terms of profit may flip as the exogenous reserve price or the number of weak or strong bidders change. Similarly, with endogenous reserve prices the ranking may depend on the seller’s own-use valuation. In other words, the ranking may be fragile to changes along these dimensions. Existing models rule out such ranking reversals by imposing substantial structure on type distributions. The current paper relies on simple mechanism design arguments that require less structure.","Starting with Vickrey’s (1961) seminal work, a central preoccupation of auction theory has been to rank the efficiency and profitability of different types of auctions. Vickrey (1961) himself was first to identify asymmetry among bidders as a critical factor. To this day, asymmetric auctions remain less than perfectly understood.====Vickrey (1961) demonstrated that there is no unambiguous profit ranking of the first-price auction (FPA) and the second-price auction (SPA). In a well-known paper, Maskin and Riley (2000) developed a few general principles for when the FPA outperforms the SPA, and vice versa. However, they concentrated on auctions with two bidders, one of whom is ex ante perceived as “strong” and the other as “weak”. Kirkegaard (2012a) generalized Maskin and Riley’s (2000) insights using a mechanism design approach. This literature centers on the role of bidders’ type distributions. Simply put, the lesson is that the profit ranking depends on the type distributions.====However, the type distributions need not be the only consideration for the seller when she contemplates different auction formats. How many weak and strong bidders are at auction? Is the reserve price predetermined by e.g. government regulation and, if so, at what level? On the other hand, if the reserve price is under the seller’s control then she must, in order to determine the optimal reserve price, ask herself what her own-use value of the object is in case it is not sold? Unlike previous papers, the current paper focuses on the role of these ==== of the problem. Given the type distributions, the primary objective is to examine whether the ranking of asymmetric auctions is generally robust to changes in the parameters. This is essentially the reverse of the exercise in Maskin and Riley (2000). They hold fixed the parameters and ask whether the distributions matter.====As mentioned, Maskin and Riley (2000) assume there are exactly two bidders. They also ignore reserve prices. Maskin and Riley (2000) and Kirkegaard (2012a) set out to identify configurations of type distributions where auctions can be ranked.==== This necessitates the use of fairly demanding proof techniques. As noted by Kirkegaard (2012a), these models are therefore heavily structured, and in fact so much so that the profit ranking is unchanged when reserve prices are allowed and when more weak bidders are present.====This paper does not impose such rigid structure on the configuration of type distributions. Thus, it will be shown that there are configurations where the profit ranking is sensitive to exogenous changes in reserve prices and the numbers of weak and strong bidders. Second, endogenizing the reserve price and recognizing that it depends on the seller’s own-use valuation likewise leads to the conclusion that the ranking may also depend on the latter parameter.====The research question is motivated by a large empirical literature on asymmetric auctions. This literature generally lumps bidders into two groups. Campo et al. (2003) divide bidders into solo bidders and joint bidders. In De Silva et al. (2003) bidders are either entrants or incumbents. The bidders in Flambard and Perrigne’s (2006) study are located in one of two areas. Brendstrup and Paarsch (2006) consider an application with major and minor bidders. Likewise, in Marion (2007) and Krasnokutskaya and Seim (2011) bidders are classified as either large or small. Finally, Athey et al. (2011) put loggers and sawmills in separate groups.====Given the sparsity of theoretical results, the empirical literature is forced to resort to numerical analysis in order to compare the performance of the observed auction format to that of some counterfactual auction format. However, it is unclear how robust the empirical findings are to changes in the parameters of the problem. The practical significance of the current paper is to highlight that the design question should be revisited following any change in parameters. For instance, when bidders are firms in the same industry, a change in the industry structure may make it optimal for the seller to switch auction format. Likewise, if the government’s opportunity cost of timber is to use the forest as a carbon sink instead, increased environmental awareness may cause the optimal reserve price to change in timber auctions, and with it the ranking of different auctions formats.====Motivated by the above literature, this paper considers auctions with two groups of bidders. Bidders in one group are strong compared to bidders in the other group who are weak. Formally, this is captured by the standard assumption that one type distribution dominates the other in terms of the reverse hazard rate.====There are two key assumptions that work in tandem to simplify the analysis. First, it is assumed that there are at least two strong bidders at auction. This is in contrast to much of the existing theoretical literature, such as Maskin and Riley (2000) and Kirkegaard (2012a). In most of the auctions considered in the empirical literature, however, there are more than one strong bidder present at auction. Hence, the assumption in the current paper is empirically justified.====The second key assumption is that the two type distributions do not have the same support. The highest type of a strong bidder strictly exceeds the highest type of a weak bidder. Then, there may exist a range of high bids that are only ever submitted by strong bidders in equilibrium. The reason is that competition among the strong bidders entice them to bid so aggressively that weak bidders cannot keep up. This phenomenon is referred to as ====.==== Note that bid-separation never occurs with only one strong bidder. Although bid-separation may seem to complicate the problem, the opposite is in fact that case. The core methodological insight is that bid-separation may make it possible to apply elementary mechanism design techniques that actually fail when there is only one strong bidder.====Following Myerson (1981), it is well understood that one auction is more profitable than another if it allocates the object to a bidder with a weakly higher “virtual valuation” with probability one. It is this basic mechanism design result that will be utilized here. This particular part of the approach is not claimed to be novel. Nevertheless, as emphasized by Maskin and Riley (2000), the argument does not in general have enough bite to compare the SPA and FPA. Thus, the novelty comes from identifying an instrument that can be leveraged to invoke the simple argument. To this end, the first observation is that the argument applies if bid-separation is sufficiently pronounced. Then, strong bidders with high virtual valuations separate away from weak bidders with mediocre virtual valuations. Thus, bid-separation may constitute an opening into the problem, yet the degree of bid-separation is endogenous.====Thus, the second and key observation is that the incidence of bid-separation depends on the size of the reserve price. In other words, the reserve price represents a lever that can generate enough bid-separation in equilibrium to permit the use of the fundamental mechanism design argument outlined above. Thus, a ranking can be obtained for some, but not necessarily all, reserve prices.====Evidently, reserve prices play a pivotal role. To start, assume that the reserve price is exogenous and the same for both auctions. Then, for ==== configuration of type distributions that satisfies the model’s sparse structure, there always exist ==== reserve price for which the FPA is strictly more profitable than the SPA. On the other hand, it is easy to show that there are configurations where the SPA outperforms the FPA for a subset of reserve prices. Hence, there are configurations in which the profit ranking changes as the reserve price changes. The ranking may likewise change as more bidders join the auction. A concrete example exhibiting these reversal properties is provided. This appears to be the first such example in the literature.====When it is endogenous, the optimal reserve price depends on the type distributions and the parameters as well as the auction format itself. The optimal reserve price in either auction is low enough to permit the weak bidders a chance of winning when neither the asymmetry between distributions or the seller’s own-use valuation is too large. Then, the FPA with an optimal reserve price is strictly more profitable than the SPA with an optimal reserve price when there are sufficiently many bidders. Similarly, for a fixed set of bidders, there are own-use valuations for which the FPA outperforms the SPA when the reserve price is endogenized. This latter result is used to show in another example that the profit ranking may flip as the seller’s own-use valuation changes. Together, the two examples thus demonstrate the economically important point that profit rankings may be sensitive to the changes in parameters.",Ranking reversals in asymmetric auctions,https://www.sciencedirect.com/science/article/pii/S0304406821000161,8 February 2021,2021,Research Article,121.0
"Baril-Tremblay Dominique,Marlats Chantal,Ménager Lucie","Paris School of Economics, Université Paris 1, France,LEMMA, Université Paris 2 Panthéon-Assas, France","Received 17 July 2020, Revised 6 January 2021, Accepted 16 January 2021, Available online 6 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102483,Cited by (6),"We analyze the spread of an infectious disease in a population when individuals strategically choose how much time to interact with others. Individuals are either of the severe type or of the asymptomatic type. Only severe types have symptoms when they are infected, and the asymptomatic types can be contagious without knowing it. In the absence of any symptoms, individuals do not know their type and continuously tradeoff the costs and benefits of self-isolation on the basis of their belief of being the severe type. We show that all equilibria of the game involve social interaction, and we characterize the unique equilibrium in which individuals partially self-isolate at each date. We calibrate our model to the COVID-19 pandemic and simulate the dynamics of the epidemic to illustrate the impact of some public policies.",", where ==== is the basic reproduction number==== of the disease, which is often estimated to be approximately 60% for COVID-19. The figure of 60% assumes that the population is homogeneous and passive, while it is well-documented==== that the herd immunity level varies between populations consisting of people with different behaviors. The object of the growing epi-economic literature is to analyze the two-sided interactions between the dynamics of epidemics and individual behaviors.====One of the many features of COVID-19 is the wide variety of responses to the infection in the population, with some individuals completely asymptomatic, and others developing fatal forms within a few days. As with symptomatic individuals, asymptomatic patients are a source of the spread of infection.==== Before being infected, there is no way of knowing whether one is of the asymptomatic type. Therefore, individuals form beliefs about their type, which they continuously update on the basis of how much they might have been exposed to the virus. At the same time, they decide how much to expose themselves to the virus in function of their updated belief. For instance, an individual who interacts with many people without developing COVID-19 symptoms becomes more optimistic about being the asymptomatic type. As a result, she may be tempted to meet even more people and forget about social distancing. The contribution of this paper to the epi-economic literature is to introduce learning into an epidemiological model, and to analyze the dynamics of an epidemic when individuals tradeoff the costs and benefits of self-isolation on the basis of their subjective beliefs.====To analyze this question, we amend the classical Susceptible–Infected–Recovered (SIR hereafter) model of ==== in two ways. In its classical version, the SIR model divides a homogeneous population into three groups: susceptible, infected and recovered, with individuals transiting from one group to another one at given, exogenous rates that depend on the size of each group. We depart from the homogeneity assumption by considering two possible types of individuals in the population: ==== and ====. Individuals of the severe type experience the symptoms of the disease immediately after being infected. In contrast, individuals of the ==== type do not have symptoms. Individuals with symptoms immediately self-isolate, but asymptomatic individuals can be contagious without knowing it. Therefore, the disease is spread in the population by asymptomatic types. Moreover, we assume that individuals without symptoms can influence the transition rate from susceptible to infected by strategically reducing the fraction of time they spend outside. Staying home prevents one from being infected, but comes at a cost (boredom, opportunity cost of not working, or of working in poorer conditions, lack of physical activity, etc.). Being infected is also costly for individuals of the severe type. Therefore, individuals continuously tradeoff the cost of self-isolation and the expected benefit of not having the symptoms on the basis of their belief of being the severe type. Finally, we assume that a vaccine will arrive at known time ====.====, i.e. such that individuals partially self-isolate at each date. We prove that there can be only one interior equilibrium, and that it is symmetric.====, who find a positive correlation between the number of cases before lockdown and the mortality rate in Italy. We also analyze the impact of policies aiming at mitigating the transmission of the virus such as mask distributions, messaging about hygiene measures, etc. We find that individuals compensate the decrease in the risk of infection by reducing social distances, but not to the point of accelerating the epidemic. Overall, we find that these policies reduce the number of deaths. We also show that a more performing health system results of less self-isolation but overall decreases the number of deaths. Finally, policies subsidizing self-isolation flatten the economic curve, but we find no substantial difference when self-isolation is encouraged at the beginning or at the end of the epidemic.==== Many papers have documented that individuals adapt their behavior when facing a risk of infection. For instance, ==== show that the demand for measles, mumps and rubella vaccines increases when there is a large increase in measles cases in a community, and ==== show that the demand for condoms increases in regions where HIV is prevalent. Some papers==== also prove that individual behaviors impact the spread of infectious diseases. In the case of COVID-19, ==== show that border restrictions and changes in individual behavior are partly responsible for reduced transmission in Hong Kong in February 2020.====In the theoretical literature, some models analyze the effect of social distance in SIR or SIS epidemiological models, either in a social optimum approach (e.g. ==== and ====) or also with strategic individual decisions (====, ====, ====, ====, ====, ====, ====, ====, ====, ====), or when older people are more likely to die from the disease (====, ====, ====, ====). The problem of individuals who tradeoff the costs and benefits of self-isolation in the SIR model has been studied notably by ====, ==== and ====. In an infinite horizon model, ==== characterizes the exposure level at the symmetric equilibrium and shows that self-isolation flattens the epidemic curve. ==== prove that individuals do not self-isolate enough with respect to what would be socially optimal, and ====The remainder of this paper is organized as follows. Section ==== sets up the model. In Section ====, we solve the best-response problem of a player, analyze some properties of the equilibrium and characterize equilibria in which there is no confinement at all, or always partial confinement. In Section ====, we calibrate our model to fit the COVID-19 pandemic, we simulate the dynamics of the epidemic in equilibrium and provide some policy analysis. Section ==== concludes and technical proofs are gathered in ====.",Self-isolation,https://www.sciencedirect.com/science/article/pii/S0304406821000215,6 February 2021,2021,Research Article,122.0
Guimarães Luís,"Queen’s University Belfast and cef.up, United Kingdom of Great Britain and Northern Ireland","Received 17 July 2020, Revised 23 December 2020, Accepted 16 January 2021, Available online 6 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102485,Cited by (9),"Antibody testing is a non-pharmaceutical intervention – not recognized so far in the literature – to prevent COVID-19 contagion. I show this in a simple economic model of an epidemic in which agents choose social activity under health state uncertainty. In the model, susceptible and asymptomatic agents are more socially active when they ==== they might be immune. And this increased activity escalates infections, deaths, and welfare losses. Antibody testing, however, prevents this escalation by revealing that those agents are not immune. Through this mechanism, I find that antibody testing prevents about 12% of COVID-19 related deaths within 12 months.","The COVID-19 pandemic has virtually stopped the world economy and has led to the death of more than one million people worldwide by October 2020. In the hope of constraining the pandemic, governments restricted movement, imposed lockdowns and quarantines, forced the closure of businesses, and increased the scale of viral testing and contact-tracing. In this paper, I show that there is another non-pharmaceutical intervention (NPI) to lower contagion: large-scale antibody testing.====There are two obvious reasons to support antibody tests. First, antibody surveys help in understanding the extent of the pandemic, its infection–fatality rate, the duration of immunity, and the proportion of asymptomatic. They have been conducted in several countries and have guided policy and the calibration of epidemiological models. Second, by identifying immune individuals, large-scale antibody testing may facilitate reopening the economy after lockdown (e.g., by issuing the so-called “immunity passports”; see ====). My paper identifies a new – not as evident – reason to support large-scale antibody testing. By revealing that susceptible and asymptomatic individuals are not immune, antibody testing reduces their social activity lowering the scale of the pandemic.====In the context of the COVID-19 pandemic, most individuals act without knowing their health state. More than half of the infected are asymptomatic (see references in ====) and, among the symptomatic, many only develop mild symptoms. Furthermore, more than 80% of immune individuals in France, Spain, and the UK are not identified.==== Thus, many individuals act without knowing whether they are susceptible, asymptomatic, or immune. Antibody tests end part of this uncertainty by revealing whether individuals are immune or not.====In this paper, I build a simple economic model in which agents maximize their lifetime utility under health state uncertainty. In the model, following ====, ====, ====, and ====, agents directly choose social activity, which is an overarching concept covering social and economic returns.==== Furthermore, agents can be susceptible, asymptomatic, symptomatic, recovered, or dead. Susceptible agents do not have antibodies against the virus, which puts them at risk of an infection. Asymptomatic and symptomatic agents are infected and may infect others but only symptomatic agents are aware of the infection; thus, asymptomatic agents behave as susceptible agents. Recovered agents have antibodies against COVID-19 and are, thus, immune. Individuals’ optimal social activity depends on their health state and uncertainty. In a world with perfect information, susceptible agents would constrain social activity to reduce exposure to the virus, while recovered agents would not. But, under health state uncertainty, agents rely on expectations of their health state to choose social activity: recovered agents may restrain themselves excessively while, crucial for contagion, susceptible and asymptomatic agents may be excessively active.====Numerical simulations of my model suggest that large-scale antibody testing substantially raises welfare and saves lives. In the model, optimal behavior critically depends on access to antibody tests. As the ==== of being immune builds up, susceptible and asymptomatic agents without access to antibody tests become more active than those with access because the former agents think they might be immune whereas the latter know that they are not. Therefore, increasing the scale of antibody testing lowers social activity and contagion by raising the share of agents that are sure of not being immune. Through this channel and using my preferred calibration, antibody testing avoids about 12% of COVID-19 related deaths in 12 months.====Further numerical simulations of my model also suggest that part of the benefits of antibody testing are due to postponing infections to gain time for a vaccine or cure to arrive. Yet, even if a vaccine or cure never arrives, antibody testing permanently lowers contagion; in the long run, antibody testing prevents about 3.4% of COVID-19 related deaths. Furthermore, my simulations also suggest that large-scale antibody testing still substantially lowers contagion and prevents COVID-19 related deaths even if the tests are imperfect.",Antibody tests: They are more important than we thought,https://www.sciencedirect.com/science/article/pii/S0304406821000239,6 February 2021,2021,Research Article,123.0
"d’Albis Hippolyte,Augeraud-Véron Emmanuelle","Paris School of Economics, CNRS, 48 boulevard Jourdan, 75014 Paris, France,GREThA, University of Bordeaux, 6 avenue Léon Duguit, 33600 Pessac, France","Received 8 July 2020, Revised 18 January 2021, Accepted 19 January 2021, Available online 6 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102487,Cited by (7)," is shown to be formally similar to an optimal growth model with endogenous discounting. The optimal dynamics then depends on the interplay between the epidemiological characteristics of the disease, the labor productivity and the degree of intergenerational equity. Phase diagrams analysis reveals that multiple trajectories, which converge to endemic steady-states with or without prevention or to the elimination of the disease, are feasible. Elimination implies initially a larger prevention than in other trajectories, but after a finite date, prevention is equal to zero. This “sooner-the-better” strategy is shown to be optimal if the pure discount rate is sufficiently low.","Infectious diseases constitute major health issues for which there is large consensus on the legitimacy of governments interventions. Yet few analysis has been undertaken to determine the socially optimal allocation of resources to control the evolution of the disease. In this article, we derive a welfare criterion from individual preferences and give precise foundations on a generally held belief, according to which intervention should begin as soon as possible and involve large expenses. We focus our analysis on expenditure that reduces the number of contacts relevant for infection transmission per unit of time and, consequently, reduces the spread of the disease. They include prevention campaigns that modify individuals’ behaviors, by e.g. the diffusion of masks or condoms that reduce the probability of a contact to be infectious, and any measures that reduce physical contacts such as lockdowns of populations. Throughout the article, all those policies will be named as prevention measures, and the aim will be to determine the optimal intensity over time of the prevention in an economy that faces an infectious disease.====Infectious disease’ prevention is a legitimate topic for economics since, as argued by Bloom and Canning (2006), there is little doubt that resource constraints play an important role in the spread of the disease. Moreover, as other diseases, they importantly affect labor and capital markets, and thus growth. However, and despite the fact that past diseases were recognized as economic tragedies, Gersovitz and Hammer (2004) pointed out that it is only since the 1990s that economists have entered the field. The interest has skyrocketed recently with the Covid-19 outbreak. Most articles in the literature adopt a positive approach focusing on private behaviors, such as individual choices on self-exposure to the risk (as in Geoffard and Philipson, 1996, Kremer, 1996), health expenditure (Momota et al., 2005), human capital accumulation (Bell and Gersbach, 2013, Corrigan et al., 2005, Boucekkine et al., 2016), fertility (Young, 2005), income distribution (Boucekkine and Laffargue, 2010) or biodiversity (Bosi and Desmarchelier, 2020). Some papers analyze the effect of public policies on private behavior (e.g. Geoffard and Philipson, 1997 ), while others consider optimal policy correcting for the obvious externalities produced by infectious diseases (e.g.  Gersovitz and Hammer, 2005, Francis, 2004, Gersovitz, 2013, Bethune and Korinek, 2020 or Eichenbaum et al., 2020).====While few papers in economics adopt a normative perspective, one observes there exists an abundant mathematical epidemiology literature, which dates back from Bernoulli (1760)! Following Sethi (1978) and Wickwire (1977), it is common to use optimal control techniques to define the desirable timing of vaccination, screening or health promotion campaigns. In most cases these studies use as a criterion a convex combination of the dynamic costs of the control and of the number of infected individuals. Moreover, the time horizon is usually finite and, in analytical models, the problem is linear with respect to the control. Based on this approach (Benhcke, 2000) finds that the optimal control of an epidemics is, in general, such that the prevention effort is maximal on some initial time interval and then set to zero (see also Buonomo et al., 2019, Morris et al., 2020). When the case of disease elimination is considered, the problem is more complex since terminal conditions are free (Barrett and Hoel, 2007). Some economic studies (in particular Gersovitz and Hammer, 2004, Francis, 2004, Feichtinger et al., 2004, Alvarez et al., 2020 or Rowthorn and Toxvaerd, 2020) have used a slightly more sophisticated criterion as they consider the present discounted value of total income net of the costs of the disease and of the control.====We propose an optimal control model in the tradition of Ramsey, 1928, Cass, 1965 and Koopmans (1965) in which the whole population is affected by an infectious disease, whose dynamics is rather general while admitting an endemic steady-state. The social welfare function is the present discounted value of the product of individual utility and the size of the population. We notably show how this criteria relies on preferences. The introduction of population in the objective is key to avoid the problem stressed by the optimal population literature (see notably Dasgupta, 1969) about maximizing the welfare of alive individuals only. By considering an endogenous population in the welfare function, our work distinguishes itself and extends important theoretical contributions of Delfino and Simmons, 2000, Boucekkine et al., 2008 and Goenka and Liu (2012) or Goenka et al. (2014), who in most cases consider specific epidemiological processes and study local dynamics. It also complement works such as those of Gersovitz and Hammer (2005) or Eichenbaum et al. (2020), who proceed by simulations.====A second important assumption we make is about the production structure which is of pure exchange, i.e. without physical capital, which allow us to completely characterize the model despite a general formulation for the dynamics of the infectious disease. The dynamics of the optimal prevention then depends on the interplay between the epidemiological characteristics of the disease, labor productivity and intergenerational equity. We believe it is important to characterize the global dynamics of the optimal system as it permits to visualize both the objective that should be reached in the short run, and some trajectories that could be hidden while performing local long-run analysis.====We find that it may be optimal to reduce the prevalence rate of the infectious disease in the long run only if labor productivity is above some minimal level. If this threshold is not reached, prevention is then at best temporary, simply slowing down the spread of the infectious disease. However, it may not be optimal to undertake temporary prevention. When instead labor productivity is sufficiently high, permanent allocation of resources to prevention is feasible though not necessarily optimal. If permanent prevention is socially optimal, the prevention effort monotonically increases with time for low initial prevalence rate, and is hump-shaped or decreasing otherwise. Hence, our paper establishes that under a welfare criterion for social intertemporal optimization a “sooner-the-better” strategy may not be the optimal one, in contrast to Benhcke (2000). This statement is however reversed when we consider paths that may yield to the elimination of the infectious disease. We first show that a simple modification of usual deterministic models describing endemic infectious diseases can be made to allow for elimination in finite time.==== Trajectories that drive to elimination are characterized by an increasing prevention for a finite interval of time and, once the infectious disease is eliminated, the prevention is zero. We show that upon existence, such paths are optimal if the pure discount rate is sufficiently small. In that case, it consequently is socially desirable that prevention should begin as soon as possible. Moreover, the effort initially devoted to prevention is larger for elimination than for any other dynamics. This is a theoretical foundation for a “whatever it costs” response to a disease outbreak.====We begin by presenting the dynamics of the population affected by an infectious disease in Section 2. The epidemiological assumptions are put forward and discussed using standard examples of HIV and flu. In Section 3 we set up the social planner’s problem, then prove the existence of a solution and characterize it. The dynamics of the optimal prevention is analyzed in Section 4. The question on whether it is socially optimal to eliminate the infectious disease is studied Section 5, and illustrated with a hand solved example. Section 6 concludes.",Optimal prevention and elimination of infectious diseases,https://www.sciencedirect.com/science/article/pii/S0304406821000252,6 February 2021,2021,Research Article,124.0
"Loertscher Simon,Muir Ellen V.","Department of Economics & Centre for Market Design, Level 4, FBE Building, 111 Barry Street, University of Melbourne, Victoria 3010, Australia,Department of Economics, Stanford University, United States","Received 18 July 2020, Revised 2 January 2021, Accepted 6 January 2021, Available online 5 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102482,Cited by (7), until such an event actually occurs.,"Without widespread immunization of the population, the road to recovery from pandemic-induced lockdowns requires sustained vigilance to ensure that the spread of the disease remains at a level that is manageable for a country’s or region’s healthcare system. At the same time, recovery ought to start as soon as possible to limit the reduction in liberty that such lockdowns impose, the mental and other health issues associated with social distancing and isolation, and to minimize the economic cost. If eradication is impossible or possible only at tremendous costs, keeping the pandemic under control without inducing economic and social hardship at a catastrophic scale requires finding a path through territory that is uncharted for both epidemiologists and economists. From a public health perspective, recovery requires the transition from a paradigm in which eradication of an epidemic is the goal to one in which the epidemic is ====. For economists, recovery requires plowing a path through a system whose dynamics are non-linear.====In this paper, we show how this can be done by providing a methodology that permits the return to some kind of normalcy, while keeping the spread of the disease at a level that even at the peak of the epidemic does not exceed the capacity constraint of the healthcare system. Specifically, we use a standard epidemiology model – a simple ==== – to predict the peak of the epidemic and treat the rate of transmission of the disease as the variable that the policymaker can influence by choosing the severity of a lockdown. We treat as a hard constraint the capacity of the healthcare system, that is, the maximum number of patients that it can handle at the peak of the crisis.==== Of course, this capacity constraint will need to be defined in such a way that patients with other – but no less severe – needs for care are still able to access treatment.====Our framework resonates with recent policy, as the capacity constraints-based approach allows policymakers to avoid explicitly trading off dollars against lives.==== In the U.S., as they approach the winter peak of the pandemic, the states of California and New York have both adopted policies that make lockdowns – and more generally restrictions on public life – contingent on hospital capacity utilization. For example, starting in December 2020, any region in California goes into lockdown as soon as the available ICU hospital capacity dips below 15%.==== In New York, the “state’s new approach focuses on maintaining sufficient hospital capacity instead of shutting down economic activity”, according to the New York Times article ====. Moreover, this new policy involves an element of prediction-based restrictions, precisely as implied by our approach: According to the same article, the “most complex element, which could prompt regionwide shutdowns, involves taking the rate of increase in an area’s hospitalizations and projecting forward to determine whether it would top 90% of capacity in three weeks. If so, restrictions will be introduced that include the closing of nonessential businesses, the limiting restaurants [sic] to takeout and delivery and a prohibition on nearly all gatherings”.====, we show that possibilities such as the capacity of the healthcare system increasing or a vaccine arriving at some point in the future do not substantively impact the optimal policy until such an event actually occurs. While the purpose of our model is to serve as a proof of concept that would need to be refined if applied, many of the key insights – such as the need to use epidemiology models to predict future healthcare demand and the non-trivial economic optimization problem when faced with a capacity constraint – will extend well beyond the confines of the specific setups we study.==== shows, the unemployment rate in the U.S. rose sharply from 5.2% in March 2020 to 19.5% in April 2020 as the nationwide lockdown hit the country and much of the rest of the world economy, and then steadily declined as the economy began to reopen. This steep incline and swift partial recovery reflects the peculiarity of the present economic downturn, which was ==== caused by a bad state of the economy. This is at the same time a source of hope and of concern: while the healthy underlying state of the economy at the onset may make for a relatively fast recovery, extended or repeated complete lockdowns can turn a public health shock into a deep and prolonged economic crisis. The firms that workers could return to in May and June may simply go out of business after further or extended lockdowns. Thus, the problem of finding a smooth path to recovery is particularly salient.====Our paper differs from the aforementioned SIR-based models in that the starting point of our analysis is not that the planner puts one weight on economic output and another on human life. Rather, the optimal policy is derived subject to a capacity constraint, so that the policymaker does not have to take an explicit stance on the value of human life at the outset of the analysis. As mentioned, ensuring that the capacity of the healthcare system is not exceeded is a key factor in preventing catastrophic health outcomes such as those that occurred in the first half of 2020 in Italy’s Lombardy region and in New York City. We also find that, under a capacity constraint, when the planner faces the possibilities that the capacity of the healthcare system will increase or a vaccine will arrive at some point in the future there are no dynamic complementarities of the nature explored in ====, as well as many other papers that utilize the same objective function involving lost output and deaths. While we make use of numerical methods to derive optimal dynamic policies, our paper differs methodologically from the previously discussed papers by also deriving analytical results.====The remainder of this paper is organized as follows. Section ==== describes our setup. Section ==== derives the dynamics of an epidemic and the optimal lockdown necessary to keep it at a level that respects the capacity constraint in a homogeneous population model. In Section ==== extends the analysis by deriving the dynamically optimal policy in a discrete-time version of the model. It also augments the model by allowing for stochastic capacity increases and stochastic arrival of a vaccine. Section ==== concludes the paper. All proofs omitted from the body of the paper can be found in the appendix.",Road to recovery: Managing an epidemic,https://www.sciencedirect.com/science/article/pii/S0304406821000203,5 February 2021,2021,Research Article,125.0
"Bhattacharya Joydeep,Chakraborty Shankha,Yu Xiumei","Department of Economics, Iowa State University, Ames, IA 50011-1054, USA,Department of Economics, University of Oregon, Eugene, OR 97403-1285, USA,Zhongnan University of Economics and Law, Wuhan 430073, China","Received 15 August 2020, Revised 19 December 2020, Accepted 22 December 2020, Available online 5 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102492,Cited by (6),"This paper offers a parsimonious, rational-choice model to study the effect of pre-existing ","Since the COVID-19 pandemic started in central China in late 2019, the disease has infected entire continents, ravaged the lives of millions and destroyed economies, lives, and livelihoods. Short of a safe and effective vaccine, no end appears in sight. The primary transmission route for COVID is believed to be human-to-human via respiratory droplets, small droplets that are ejected when speaking, coughing, or sneezing or via direct contacts. The most common symptoms of patients infected with the virus include a lower respiratory tract infection with fever, dry cough, and dyspnea. Some 40% of people, even when they are infected, show no symptoms and can unknowingly spread the infection to others.====This raises an interesting problem: When X goes out to meet Y, neither X nor Y knows whether they are infected and whether they can infect each other. Faced with infection risk, people, naturally, seek ways to avoid catching the virus. X may choose not to go out to visit with Y, or she may but wear a mask. What do X and Y decide to do? How do these choices relate to their affluence, infection risks, and costs and benefits? How does the income distribution of the economy in which X and Y live affect the infection risks these people face?====This paper is aimed at offering a minimalist, analytical framework in which to pose these questions. It permits each agent, X or Y, to rationally choose whether to “go out” and whether to engage in any preventive protection. Two elements set it apart from existing work. First, the model explicitly allows for some agents to unknowingly transmit infection without being symptomatic. And second, it considers two-sided prevention using non-pharmaceutical interventions: when X and Y meet, the ==== of X catching the virus from Y depends not just on what X does to protect herself but also on what Y does.====Most work on infectious-disease transmission start with some version of the ==== Susceptible-Infected–Recovered (SIR) model. That model assumes that the population in which a pathogenic agent is active comprises three subgroups: healthy individuals who are susceptible (S) to infection, infected individuals (I) who can infect the healthy ones, and individuals who are removed (R) from the infection cycle either due to death, immunity, or recovery. An infected individual randomly interacts with other susceptible agents and transmits the disease at a specific rate ====; an infected individual also transits from the infection cycle, say via death or acquired immunity, at a particular rate, ====) or (====)) by specifying a time-varying path for the basic reproduction number, ====.====). Some previous work by economists (====); and later (====, ==== for example) start with a version of the SIR model and augment it to allow people to make choices about prevention.==== These choices, in turn, affect the aggregate infection rate, producing a system of simultaneous feedback. Choices are influenced not only by budgetary limits and preferences but also by informational constraints: whether the individual knows she is infected or infectious, and whether the people with whom she socializes are themselves contagious (====). In short, ==== and ==== are endogenously determined.====We split ways from most of the existing economic-epidemiology literature by modeling how people, uninformed about their COVID-status, can spread the infection to others.==== In our model, a fraction of the population shows symptoms, gets enfeebled, and enters strict quarantine as symptomatic and infected (SI). The rest are either asymptomatic and infected (AI) or asymptomatic and uninfected (healthy, AH). Importantly, the non-SI do not know which group they are in. This is vital because the isolation of infected individuals is key to controlling transmission, and the decision to isolate often depends on observable symptoms (fever, cough, temperature). As ==== point out, the effectiveness of such symptom-based interventions depends critically on the fraction of asymptomatic infections. The generally accepted range of estimates for asymptomatic COVID transmission is between 17.9% to 30.8% of all infections.====There is another critical point of departure for us. Almost the entire economic-epidemiology literature, erstwhile targeted to study HIV infections, focus on one-sided prevention. As noted above, a significant route of transmission of COVID is via small droplets ejected when speaking, coughing, or sneezing. In our setup, a person from the non-SI group may invest in personal protective equipment (PPE), such as masks or face covers, to protect herself from infectious droplets ejected by others she meets. Such PPE also offers source control, blocking droplets ejected by the wearer. However, serious concerns about the acceptability and tolerability of such non-pharmaceutical interventions linger; we know that because not everyone who should be wearing a mask does so, at least not diligently. For us, the upshot is PPE offers two-sided protection, but not everyone likes using them; our model allows for choices to be made in this regard. Interestingly, source control is more critical because if people wear masks to dim their chance of innocently infecting others, everyone ends up being more protected.====. Even with that caveat, a lot is going on.====Agents, in the model, are heterogeneous in their endowment (income), ====. As the period starts, people know if they are in the SI pool or not. What they do not know is whether they are in the AI or the AH pools. Since these decisions are taken ====, any non-SI agent attaches a probability, ====, that they are in the AI group and uses that probability to compute the expected utility from a particular choice. Such people face two extensive-margin decisions: the decision to socialize (“go out”) and the decision to engage in prevention. Going out and meeting someone brings joy but may raise the infection risk.==== Similarly, protection activities, done diligently, entail goods and utility losses but reduce the chance of infection. To keep the analytics tractable, we make several non-critical, simplifying assumptions such as linear preferences, uniform income distribution, and so on.====In the standard SIR model, two assumptions are made. First, everyone knows their infection status. Second, at the start of the period, the entire pool of infected (====); each ==== meets every ==== and vice-versa, so there are ==== meetings. Afterward, a fraction ==== of the ==== has left their pool and joined the ==== pool. (Before the next period begins, some from the infected pool recover and may acquire immunity (or not) or perish. The membership in the various pools, at the start of the following period, is different from what it was when the previous period started.) In sharp contrast, we posit that entire pools do not mix: two people meet, bilaterally and only once, and neither is aware ==== of the other person’s infection status or protection choice. The probability X encounters Y depends on the choice of Y to go out. The probability that Y is infected depends, additionally, on ====. We introduce two-sided prevention by specifying that the likelihood of X (a member of the AH) getting sick from a meeting with Y depends multiplicatively on the person-specific, protection choices of both X and Y.====Agents under rational expectations take the masses of the three groups, SI, AH, and AI, as given. An individual determines her infection risk, and in turn, her going-out and protection choices, based both on these masses and her position on the endowment distribution. One compelling case has relatively poor people socializing without protection, the middle-income people socializing with protection, and the relatively affluent not socializing at all. Another case has the poor socializing without protection and the rest under self-quarantine. The big plus of our minimalist framework is that it allows precise, analytical determination of each of these entities.====Several equilibrium possibilities arise. Some equilibria represent extremes – all AI or AH go out or self-quarantine, and everyone socializing eschews/adopts PPE. Depending on deep parameters, more reasonable looking “interior” equilibria, are also possible. Central to our analysis is the following trade-off: All else same, less (more) use of PPE will raise (reduce) infection risk but may keep more (less) people home. In equilibrium, the overall effect will depend on the size of the groups which depends on who protects and who steps out. Our flagship result relates epidemiology with the income distribution. We find a mean-preserving increase in the spread of the distribution of income unambiguously increases the equilibrium proportion of socializing agents who are unprotected and may increase or decrease the percentage of agents who self-quarantine. Strikingly, while higher pre-COVID ==== may or may not raise the overall risk of infection is ambiguous, it definitely increases the risk of disease in social interactions.====A few recent papers are closely related to ours. ==== propose a similarly minimalist model of economic choices during a disease outbreak. In a two-period model, agents who have been tested and marked healthy choose whether to go out to work (in which case, they may get infected) or to stay at home, both today and in the future. The decentralized equilibrium is inefficient due to an externality: when making these choices, people do not take into account the impact of their choice on the spread of the disease.==== study the effect of COVID spread on future inequality in a SIR setup. In their setup, high and low-skill workers may choose to work onsite or remotely. Still, production activity in the high-skill occupations is such that the high-skilled find it easier to select the remote (hence, lower infection) option. ==== study pre-COVID inequality in the U.K. and document how the ability to continue working safely and through lockdowns is distributed very unevenly by gender, ethnicity, education, and earnings. Similarly, recent English data reveals that the most economically deprived areas have twice the rate of deaths involving COVID than the most affluent (====). ====, however, find that when it comes to getting tested, income disparities did not matter for residents of New York City.====We develop the model in Section ==== and study prevention decisions in Section ====. Section ==== constructs the equilibrium implied by those decisions. Section ==== delves into the model’s implications for disease transmission and inequality. Section ==== concludes. Proofs are relegated to the appendix.====Equilibrium ==== solves the quadratic equation ====with two candidate solutions. Suppose ====. When ====, there exists and only exists one equilibrium in ====.====Define ====, where ====, ==== and ==== are defined in the main text. We have ==== and ====when ====. Then a necessary and sufficient condition for a unique real solution to ==== in ==== is ====. This is true as long as ====
 Under this condition, the equilibrium ====is an interior solution.====Notice the condition for ==== and ==== to hold requires that ====Assuming so, what if ====?",A rational-choice model of Covid-19 transmission with endogenous quarantining and two-sided prevention,https://www.sciencedirect.com/science/article/pii/S0304406821000306,5 February 2021,2021,Research Article,126.0
"Bouveret Géraldine,Mandel Antoine","Nanyang Technological University, School of Physical and Mathematical Sciences, 21 Nanyang Link, 637371, Singapore,Paris School of Economics-Université Paris 1 Panthéon-Sorbonne, Centre d’Économie de la Sorbonne, 106 Boulevard de l’hôpital, 75013, Paris, France","Received 15 July 2020, Revised 9 November 2020, Accepted 10 December 2020, Available online 5 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102486,Cited by (5),", measuring the ratio between social welfare at a global and a local equilibrium. Overall, our results show that individual behaviours can be extremely inefficient in the face of epidemic propagation but that policy can take advantage of the network structure to design welfare improving containment policies.","In the context of the 2020 COVID-19 pandemic, very strong policy measures have been implemented to contain epidemic diffusion. State of emergency has been declared in certain countries and certain civil liberties (e.g. freedom of assembly) have been suspended. The implementation of such stringent policies, labelled as social distancing measures, has been justified by the role of social interactions in epidemic diffusion. In economic terms, the premise is that individual behaviour is extremely inefficient in the presence of disease/network externalities. Yet, there is, to our knowledge, no normative analysis of the challenges posed by the containment of epidemic spreading in a network. This is the issue we address in this paper.====The containment of epidemic processes defines a specific class of externality problems: through prophylactic investment, agents can reduce not only their own contamination risk but also reduce the risk of contagion of their peers in the network. The external effect hence created has certain features of a public good as the investment of each agent benefits to all the agents to whom it is connected. However, the magnitude of the effect depends on the specific connectivity between each pair of agents and thus on the structure of the network. In this setting, our first aim is to characterise, as a function of the network structure, individual and socially efficient behaviours. Second, we measure, using the notion of Price of Anarchy (PoA), the inefficiency induced by individual behaviours. Third, we investigate policy measures that can be implemented to overcome these inefficiencies.====We place ourselves in a setting where the network structure is given, each agent can be initially contaminated with a certain ====, and contagion spreads through network links proportionally to their contagiousness. Once infected, agents remain so permanently, i.e. we consider a susceptible/infected type of model according to the epidemiological terminology. In this context, agents aim at minimising their probability of contagion before a given date. In a narrow interpretation, this date can be seen as the expected date at which a treatment will be available. In a broader sense, the objective of each individual is to reduce the speed of incoming epidemic propagation. We assume that agents can invest in the network to reduce the speed of contagion. More precisely, they can decrease the contagiousness of links at a fixed linear cost. As the impact of individual investments depends on global contagiousness, and hence on the investment of other players, the situation defines a non-cooperative game. We consider two variants of the game. The local game in which an agent can only invest in the links through which it is connected. The global game in which an agent can invest in each link of the network. The local game naturally applies to settings where agents are individuals that can take individual and costly measures to limit their social interactions. The global game corresponds to a more complex setting where agents are usually organisations (regions, countries) that are involved in a scheme that allows one agent to subsidise, directly or indirectly, the investment of other agents in the reduction of contagiousness.====Our main results characterise the relationships between social efficiency, individual behaviours and network structure. First, we show that individually rational and socially efficient behaviours can be characterised using the notions of communicability and exponential centrality (====, ====). It is individually rational to invest in a link proportionally to the communicability between the investor and the edges of the link while it is socially efficient to invest in a link proportionally to the total communicability/exponential centrality of its edges. Second, we derive a quantitative measure of the inefficiency induced by individual behaviours using the notion of PoA. We show that in worst cases the level of inefficiency can scale up linearly with the number of agents. This strongly calls for public policy interventions. In this respect, we show the ====The remaining of this paper is organised as follows. Section ==== reviews the related literature. Section ==== introduces epidemic dynamics as well as our behavioural model of the containment of epidemic spreading. Section ==== provides our main results on the relationship between individual behaviours, social efficiency and network structure. Section ==== investigates the social efficiency of policy measures aiming at reducing epidemic diffusion. Section ==== concludes the paper. All proofs are given in the ====.",Social interactions and the prophylaxis of SI epidemics on networks,https://www.sciencedirect.com/science/article/pii/S0304406821000240,5 February 2021,2021,Research Article,127.0
"Bosi Stefano,Camacho Carmen,Desmarchelier David","Université Paris-Saclay, Univ Evry, EPEE, 91025, Evry-Courcouronnes, France,PJSE (UMR 8545), PSE, France,Université de Lorraine, Université de Strasbourg, CNRS, BETA, 54000, Nancy, France","Received 22 May 2020, Revised 24 January 2021, Accepted 26 January 2021, Available online 3 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102488,Cited by (18),"The recent COVID-19 crisis has revealed the urgent need to study the impact of an infectious disease on market economies and provide adequate policy recommendations. The present paper studies the optimal lockdown policy in a dynamic general equilibrium model where households are altruistic and they care about the share of infected individuals. The spread of the disease is modeled here using SIS dynamics, which implies that recovery does not confer immunity. To avoid non-convexity issues, we assume that the lockdown is constant in time. This strong assumption allows us to provide analytical solutions. We find that the zero lockdown is efficient when agents do not care about the share of infected, while a positive lockdown is recommended beyond a critical level of ====. Moreover, the lockdown intensity increases in the degree of altruism. Our robust analytical results are illustrated by numerical simulations, which show, in particular, that the optimal lockdown never trespasses 60% and that eradication is not always optimal.","On December 31st, the Chinese WHO office was informed of cases of pneumonia of unknown origin in the city of Wuhan. On the 11th and 12th January, the Chinese authorities identified a new type of coronavirus as the cause of the illness. On January the 23rd, there were 571 cases and 17 deaths. Dreading the rapid expansion of the illness, the Chinese government decided on that date to lock down the city of Wuhan and the neighboring region, affecting a total of about 57 million people. Only a share of healthy individuals of a household could go out, once a day, and only for essential shopping. The economic activity fell and the world feared a recession. Within one month, the rest of the world had to face the same problem. On the 11th of March 2020, the World Health Organization (WHO) declares that epidemic had become a pandemic. The difficult question all policy-makers need to face is the extent of the lockdown. Can a country stop an epidemic while maintaining some economic activities? All activities? The present paper proposes a series of three nesting stylized models of lockdown, establishing the feedback between the pandemic and production.====The urgency of the epidemiological issue and its economic consequences have led a number of specialists in economic dynamics in a race against time to recommend policy solutions. ====, ====, ==== and ====) and Removed/Recovered individuals (====). The SIR’s main assumption is that recovered individuals develop a lifelong immunity, that is, they cannot contract the disease again. In particular, in ==== build a multi-risk SIR model considering three age groups who suffer differently from the COVID-19 pandemic. Even further, their model also explores the impact of social distancing, testing and the arrival of a vaccine on optimal policies. Given the complexity of the problem and its stochastic nature, the authors are obviously obliged to resort to numerical simulations as well. They show that imposing targeted lockdown measures, social distancing and increasing testing minimize economic losses and deaths. In all scenarios, whether semi-targeted or uniformly targeted policies are chosen, a positive lockdown is always adopted while waiting for the vaccine. Finally, let us mention ====, who also analyzes a multi-risk SIR model with three population groups. Among the family of reasonable policies, ==== notices the existence of two polar solutions “potentially optimal“. In the first solution, a four months lockdown of 90% succeeds eradicating the pandemic. In the second, a five months lockdown of 30% allows to flatten the curve. The cost of both strategies is similar, a 15% of annual GDP. In all three papers, the policy maker takes into account deaths as a cost which is equal to a life’s statistical value.====Our paper aims at exploring other directions. In particular, we challenge the permanent immunity assumption made in the literature so far, remarking that the optimal lockdown policy is clearly sensitive to the duration of immunity after recovery. Nevertheless, regarding COVID-19 and to date, there is no consensus about the duration of immunity. However, for a majority of virologists the immunity period is plausibly short (see, for instance, the WHO COVID-19 daily press briefing on the 13th of April 2020). Published in September 2020, ==== This lack of sound knowledge about the duration of immunity should press the scientific community to open their research lines and search for robust policy recommendations under all possible scenarios for immunity.====In this context, the Susceptible–Infected–Susceptible (SIS) model represents an interesting alternative to the SIR framework. The SIS approach considers indeed the opposite case: recovery does not confer immunity. More precisely, the population is divided in two groups: Susceptibles (====) and Infectives (====). A susceptible can contract the disease after a contact with an infective and, then, get back to the group of susceptibles after recovery. Historically, the SIS model has been used to represent the spread of bacterial diseases as meningitis and plague, or the spread of protozoan diseases as malaria or the sleeping sickness (see ====). In the current pandemic of COVID-19, the choice of a SIS model may a priori seem a rather extreme choice. Nevertheless, we believe that in the context of policies concerning both the short and the long-term, the SIS model is better suited than the SIR model, in which recovery confers permanent immunity.====The hybrid literature combining economics and epidemiology dates back to the early Seventies. In one of the seminal contributions, ==== minimizes the social cost of an epidemic finding the optimal treatment in a SIS model. Because of the constant marginal cost of the treatment, a bang–bang solution is obtained: either the effort of the public health system is at its maximum and the disease eradicated; or the public health system does not make any effort and the disease grows out of any control (see also ====). The same minimization program was reconsidered in ==== and ==== with a vaccination protocol instead of a treatment effort. To the best of our knowledge, the first attempt to introduce the SIS hypothesis in an economic growth model is ====. Like them, we also consider an infinite-time-horizon model, but, instead of considering a centralized economy a la Ramsey, we work with a general equilibrium model based on market mechanisms in the spirit of ====.====The main objective of the present paper is to provide policy makers with robust optimal recommendations in face of an epidemic of the SIS type. Our determination to provide exact optimal policies will force us to assume that the lockdown is constant in time. As a result, we model here a government who chooses the lockdown level that maximizes a measure of inter-temporal social welfare over an infinite time horizon. Welfare is understood here in a large sense since it embraces empathy towards infectives. In particular, welfare depends, as usual, on households’ consumption but also on the share of infectives which is a negative externality: the more infectives, the less the household enjoys consumption. It is important to add a few words on empathy. Empathy is one of the key features of the COVID-19 pandemic and of the present paper. Without empathy the extreme lockdown measures imposed all over the world could not be understood. Certainly, the virus is fatal mainly for retired individuals: a 6% of all infected over 65 years dies (see ====). The economic loss that follows the lockdown would be way too high according the pure economic reasons. The recent literature mentioned above introduces fatalities in the policy maker’s objective as statistical economic losses. Among them, only ==== consider a measure of empathy, in this case, an emotional cost of death. We also believe that empathy towards the infected plays a major role in political and economic decisions, and as a consequence, we assume that individuals maximize their overall welfare, which depends as usual on consumption, and also on the share of infectives in the society.====As mentioned, the purpose of this paper is to provide with the lockdown rate which maximizes overall welfare. One particularity of our approach is that this lockdown rate is assumed to be constant in time while other recent contributions have considered a dynamic lockdown (e.g. ====). We have introduced this limiting assumption because, as in ====, interactions between the epidemiological model with a dynamic lockdown (control variable), makes the problem non-convex. Technically speaking, it means that given a candidate to optimal solution, one cannot verify the second order condition. As a result, it would not be possible to prove that the policy recommendation we provide regarding the lockdown rate is indeed maximizing welfare.==== Without addressing theoretically the convexity question, ==== and ==== resort to numerical simulations. By considering only constant lockdown policies, we ensure in this paper the optimality of our problem while providing an analytical and robust solution.====Next, we use the Cass–Koopmans criterion (1965) to describe the policy maker’s representation of welfare. Here, households’ utility is discounted in time so that future generations weight less in overall welfare. The Cass–Koopmans criterion is more difficult to solve, but still we are able to find the long-run solution. If we focus on maximizing long-run welfare, a positive level of lockdown remains optimal. However, the eradication of the disease is efficient only if households are empathetic towards infectives. When welfare is maximized along the transition, the optimal lockdown is positive only beyond a critical degree of altruism. Nevertheless, the optimal lockdown may be here insufficient to eradicate the epidemic, in contrast with some pure epidemiological models.====Finally, we introduce capital accumulation in the previous Cass–Koopmans model to appreciate the impact of the lockdown on the wealth of a nation. We obtain the same qualitative results as in the basic model without capital and, in this sense, our conclusions seem quite robust.====As already mentioned, we use numerical exercises to illustrate our theoretical results and optimal policy recommendations. We calibrate the models using the most updates COVID-19 data, in line with the recent literature. Among all results, let us advance that the optimal lockdown is always lower than 60%, as in ==== and ====. Although our frameworks are different, a SIR model with very low mortality and a SIS model, where mortality is zero, are relatively close.====The rest of the paper is organized as follows. Section ==== presents the standard SIS model under consideration, and it obtains the explicit trajectory for the share of infected under very general assumptions. Section ====, ==== present and analyze the epidemic-augmented infinite-horizon models without capital accumulation and the growth model. Finally, Section ==== concludes. All proofs are gathered in the ====.====The solution to Eq. ==== is given by ====. Using ====, we can rewrite ==== as ====Setting ==== in ====, it is straightforward to prove that there are two stationary states: ==== and ====. Since the share of infected lies between ==== and ====, ====, an endemic steady state ==== exists if and only if ====. Note that when ====, the endemic and the disease free steady state coincide (====).====According to the solution in ==== and using ====, we have the following dynamics. If ====, then the economy is disease-free forever. If on the contrary ====, then we have five cases.====(i) If ====, then the endemic steady state ==== exists and ====. According to ====, ==== if and only if ====. Indeed, if ==== (====), then ==== increases (decreases) continuously from ==== to ====.====(ii) If ====, then, according to ====, we have that ====. Then, ==== decreases continuously from ==== to ====.====(iii) If ====, then ==== and ====. Therefore, ==== decreases continuously from ==== to ====
 ====.====(iv) If ====, then substituting ==== in ====, we obtain that ====, with solution ====. Thus, ==== decreases continuously from ==== to ====
 ====.====(v) If ====, then ====. Therefore, ==== decreases continuously from ==== to ====.====Case (i) corresponds to the disease-free steady state of case (1) in ====. Cases (ii), (iii), (iv) and (v) generate the same qualitative dynamics and note that they correspond to ====. In all these cases, ==== decreases continuously from ==== to ====, which proves case (2) in ====.  ■",Optimal lockdown in altruistic economies,https://www.sciencedirect.com/science/article/pii/S0304406821000264,3 February 2021,2021,Research Article,128.0
Desbordes Rodolphe,"SKEMA Business School-Université Côte d’Azur , Campus Grand Paris, 5 quai Marcel Dassault, Suresnes, 92150, France","Received 14 August 2020, Revised 4 January 2021, Accepted 6 January 2021, Available online 3 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102493,Cited by (4),"We explore the space–time and mortality dynamics of recent infectious diseases outbreaks which have occurred in a large number of developed and developing countries. We fully acknowledge the heterogeneity of infectious diseases. We find that many outbreaks exhibit spatial dependence, due to the international movement of people and goods. All countries are exposed to these negative cross-border health externalities, which can be triggered by climate shocks. The mortality consequences are much more severe in developing countries. Paying attention to spatial dependence has important implications for economic research and international policymaking.","Globalisation is often perceived to have substantial economic and human benefits (Clausing, 2019) but an unfortunate consequence of the international movement of goods and people is that it facilitates the spreading of microbes across borders (Saker et al., 2004).==== This fact has long been recognised by trading nations which have tried through quarantine policies and, later, the adoption of International Sanitary/Health Regulations to prevent the global diffusion of diseases while limiting impediments to international travel and trade (Hein, 2020).==== Nevertheless, contemporary pandemics (e.g. 2002–2003 SARS; 2009 H1N1 Influenza; 2014–2016 Ebola; 2020 COVID-19) have demonstrated that the world remains unprepared to deal with the growing global threat of infectious diseases. Hence, while saying that ’diseases know no borders’ is almost a cliché (Ferhani and Rushton, 2020), the international community has evidently still not fully acknowledged this fact. More surprisingly, researchers in health-related medical or development fields also keep neglecting international health externalities. Recent empirical works on the determinants of life expectancy in developing countries have not considered spatial dependence (Barlow and Vissandjee, 1999, Hanmer et al., 2003, Kabir, 2008, Austin and McKinney, 2012, Lin et al., 2012). Paradoxically, this is also the case of studies which aim at investigating specifically the links between health and globalisation, such as Levine and Rothman (2006), Bergh and Nilsson (2010), Mukherjee and Krieckhaus (2012), Olper et al. (2018). This paper shows that this frequent omission is troublesome, from an economic and econometric perspective, because the local outbreaks of many infectious diseases are characterised by rapid spatial propagation.====More specifically, we examine the spatio-temporal dependence and mortality consequences of the top fifteen diseases which have caused the largest number of outbreaks in developed or developing countries during the period 1995–2015. To do so, we mainly rely on the under-exploited (at least in the Economics literature) GIDEON (Global Infectious Disease and Epidemiology Online Network) database, which provides a worldwide coverage of outbreaks of all infectious diseases.==== An outbreak, i.e. a sudden number of cases of a disease above what is normally expected, is an ideal outcome to study the short-run international spread of an epidemic. In addition, the use of this database allows us to highlight the variety of infectious diseases involved in outbreaks, to ensure that we do not mix heterogeneous spatial propagation patterns by working at a very granular level, and, crucially for a spatial analysis, to consider all countries. We model the international diffusion of diseases using spatial econometrics. It is a natural econometric approach to take into account international interdependency, and most important, to evaluate the consequences of spatial dependence since the direct (national) and indirect (international) effects of changes in the values of the explanatory variables in one location can be calculated. We thus investigate, in dynamic models with fixed effects, whether the links created by bilateral migration and trade facilitate the rapid diffusion across borders of various disease outbreaks. Subsequently, we look at the mortality consequences of disease outbreaks. Using here the Global Burden of Diseases (GBD) database, we relate, in each income group, the ‘unexpected’ number of new cases to the ‘unexpected’ number of deaths for different groups of infectious diseases.====For a diverse range of infectious diseases, we indeed often find spatial dependence. Our results indicate that factors fostering a disease outbreak in one country can quickly lead to the emergence of a disease outbreak in another country. For example, we highlight that climate disruptions, e.g. unexpected high rainfall, in one country can influence Dengue outbreaks locally and internationally in the same year. Both developed and developing countries are exposed to these international disease externalities but, so far, the mortality consequences are much more severe in the latter countries. Nevertheless, as we stress in the discussion of our findings, the spatial propagation of infectious diseases ought to be a strong self-interested motivation for developed countries to provide more international funding for health initiatives supporting various ‘global public goods’ such as infectious diseases control and slowing down antimicrobial resistance (Sandler and Arce M, 2002, Watkins et al., 2018, Bloom and Cadarette, 2019).====To our knowledge, this is the first study which has carefully investigated the international diffusion of the outbreaks of such a large number of different infectious diseases using the same econometric framework.==== The strong spatial dependence that we highlight for many diseases justifies the need to account explicitly for these spatial effects in the empirical modelling of health outcomes. Likewise, if the prevalence of infectious diseases is an explanatory variable, the probable spatial interdependence of the outcome, e.g. economic development, ought to be considered. Otherwise, as we explain later, failure to do so could produce estimated direct effects that are strongly biased (Pace and LeSage, 2010, Corrado and Fingleton, 2012, Betz et al., 2020). Fortunately, this study shows how spatial econometrics can be used to model spatial interdependence and, most interestingly, to estimate the strength of not only the direct but also spatial spillover effects. We illustrate that the indirect effects can be sizable.====The rest of this paper proceeds as follows. In Section 2, we present an overview of the major recent disease outbreaks. In Section 3, we investigate their space–time dynamics and, in Section 4, we look at their mortality consequences. Section 5 discusses the implications of our paper for econometric modelling and international policymaking. Section 6 concludes.",Spatial dynamics of major infectious diseases outbreaks: A global empirical assessment,https://www.sciencedirect.com/science/article/pii/S0304406821000318,3 February 2021,2021,Research Article,129.0
Herrenbrueck Lucas,"Simon Fraser University, Canada","Received 11 December 2020, Revised 23 January 2021, Accepted 26 January 2021, Available online 3 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102491,Cited by (6),"Economic recessions are traditionally associated with asset price declines, and recoveries with asset price booms. Standard asset pricing models make sense of this: during a recession, dividends are low and the marginal value of income is high, causing low asset prices. Here, I develop a simple model which shows that this is not true during a recession caused by ====, such as those seen during the 2020 pandemic: the restrictions drive the marginal value of income down, and thereby drive asset prices up, to an extent that tends to overwhelm the effect of low dividends. This result holds even if investors misperceive the economic forces at work.",None,Why a pandemic recession ,https://www.sciencedirect.com/science/article/pii/S030440682100029X,3 February 2021,2021,Research Article,130.0
"Goenka Aditya,Liu Lin,Nguyen Manh-Hung","Department of Economics, University of Birmingham, United Kingdom,Management School, University of Liverpool, United Kingdom,Toulouse School of Economics, INRAE, University of Toulouse Capitole, France","Received 21 September 2020, Revised 11 January 2021, Accepted 12 January 2021, Available online 2 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102476,Cited by (17),This paper studies an optimal growth model where there is an infectious disease with ==== dynamics with and without disease related mortality — a class of models which are non-convex and have endogenous discounting so that no existing results are applicable.,"The Covid-19 pandemic has brought the study of interaction of infectious disease with the economy, i.e. economic epidemiology models, to the frontier of economic research. The first generation of economic epidemiology models typically studied ==== models without disease related mortality.==== However, with Covid-19 the modeling of mortality has become important as this seems to be a driver of the policy responses adopted in many countries.====This paper analyzes the ==== model which has been used to model Covid-19 (Ferguson et al., 2020) when there is disease related mortality==== in a neoclassical growth model so that the model is fully general equilibrium.==== The literature has largely studied immediate effects of Covid-19 and in this paper we concentrate on the medium to long run effects, i.e. effects of the disease on the steady state.==== Households can save through investing in capital and production of the single consumption good uses capital and labor. Only those that are not infected (i.e. those who are susceptible and those recovered from the disease) individuals can work. There are two effects of the disease: there is morbidity, i.e. those who are ill do not work, and there is mortality, so that a fraction of those who have contracted the disease die due to it. The contact rate is endogenous in the model and is decreasing in health expenditures, which can also be interpreted as self-isolation costs (Eichenbaum et al., 2020).==== In our model the households are homogeneous and we do not model disease related externality where households do not take into account the effect of their decisions on the evolution of the disease in the population. The recent paper, Goenka and Liu (2020) explores in detail the effect of this health externality==== in a dynamic general equilibrium model with health expenditures and we abstract from it to concentrate on the role of mortality in modeling diseases of the ==== type. In the model we use an extended welfare function that depends on utility from consumption as well as a loss in welfare from disease related mortality as without it there can be counter-intuitive effects where increase in mortality is welfare improving.====There are two main methodological issues that we feel have not received adequate attention which we study in the paper. The first is examining different modeling choices for modeling mortality in the ==== model. We present a general model which encompasses the two canonical ways of modeling mortality in the epidemiology ==== model – early mortality so that there is immediate death of the infectives, and late in infection (delayed) mortality – where death takes place later so that those who succumb to the disease are those who are not circulating in the population transmitting the disease (see Busenberg and van den Driessche (1990) and Keeling and Rohani (2008)). This is consistent with the experience in Covid-19 where most of the mortality has taken place in hospitals and care homes so that these individuals are not effectively mixing with the general population of susceptibles. The timing of mortality affects epidemiology dynamics. We examine the implications of the epidemiology modeling choices on optimal choices and on the equilibrium steady state outcomes. When there is early mortality, the direct effect of higher deaths is drop in fraction of infectives which reduces infections, while with later mortality this does not happen. Thus, there is a self-limiting effect of increase in mortality. This has implications for the cut-off for persistence, ====, and the long run effect of the disease.====The second methodological issue is that we study the sufficiency conditions for the optimal control problem. The ==== epidemiology dynamics are non-convex. Endogenous mortality adds another problem as the population becomes endogenous. The economic epidemiology ==== model, thus, has endogenous discounting and is non-convex. As a result the standard Arrow or Mangasarian conditions do not apply. There are no results for the ==== model with and without mortality that can be used in economic models to our knowledge.==== We directly address this issue and given the special structure of the problem, we directly show the relevant transversality conditions and establish sufficiency by adapting the method of Leitmann and Stalford (1971) that was used for convex problems.==== The key to the proof in the current paper is to show the co-state variables associated with the bounded state variables converge to zero with time, and this implies a different transversality condition than in Goenka et al. (2020). As a special case, we obtain the sufficiency in the ==== model without disease related mortality.====The model is a fully dynamic general equilibrium model and we characterize the Euler equations that govern the evolution of the economy. As our interest is beyond the very short run, we show that there are two steady states for the economy: a disease free and a disease endemic steady state. The optimal health expenditure depends on a function of the parameters and the equilibrium values of the economic variables. This function is interpreted as the net marginal benefit of health expenditure (net of the marginal cost) and its position determines whether health expenditures (or non-pharmaceutical interventions such as ‘lock-downs that reduce economic activity) are positive or not in a steady state, and if they are, their magnitude. Thus, the equilibrium reproduction rate, ==== will depend on both the infectivity of the disease and endogenous economic choices. As the models are too complex to solve analytically we study how the steady state values of variables of interest change as the rate of disease related mortality changes (and thus, the number of disease related deaths in equilibrium). While the actual disease dynamics will typically have both early and late in infection mortality to understand how each affects equilibrium outcomes we analyze the polar cases where there is only mortality of one kind in detail. The endogenous economic choices interact with the inherent disease dynamics. With early mortality model the death of the infectives directly reduces infections and there is an indirect concentration effect as the population size also reduces. If the direct effect dominates then the per capita capital stock, output and consumption increase as those infected only consume but do not work. When there is late mortality, death does not directly reduce infections as those who succumb to the disease are not transmitting infection in the general population of susceptibles. Capital stock, output and welfare decline as infections are increasing — the fraction of the infectives in the population is increasing. Thus, the details of the epidemiology modeling will affect economic outcomes and how to think about optimal control of epidemics.====The plan of the paper is as follows: Section 2 studies the pure ==== model with early and late mortality. Section 3 introduces the economic epidemiology model, and characterizes the Euler equations and steady states, and Section 4 does comparative statics of equilibrium steady state outcomes when mortality is varied. Section 5 studies the transversality and sufficiency conditions, and Section 6 concludes.",SIR economic epidemiological models with disease induced mortality,https://www.sciencedirect.com/science/article/pii/S0304406821000148,2 February 2021,2021,Research Article,131.0
"Rabani Yuval,Schulman Leonard J.","The Hebrew University of Jerusalem, Jerusalem 9190401, Israel,Caltech MC305-16, Pasadena CA 91125, USA","Received 25 June 2020, Revised 11 December 2020, Accepted 14 January 2021, Available online 28 January 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102475,Cited by (1),"The “invisible hand” of the free market is remarkably effective at producing near-equilibrium prices. This is difficult to quantify, however, in the absence of an agreed model for out-of-equilibrium trade. Short of a fully reductionist model, a useful substitute would be a scaling law relating equilibration time and other market parameters. Even this, however, is missing in the literature.====We make progress in this direction. We examine a class of Arrow–Debreu markets with price signaling driven by continuous-time proportional-tâtonnement. We show that the connectivity among the participants in the market determines quite accurately a scaling law for convergence time of the market to equilibrium, and thus determines the effectiveness of the price signaling. To our knowledge this is the first characterization of price stability in terms of market connectivity. At a technical level, we show how convergence in our class of markets is determined by a market-dependent ====.====If a market is not isolated but, rather, subject to external noise, ==== has predictive value only to the extent to which that noise is counterbalanced by the price equilibration process. Our model quantifies this predictive value by providing a scaling law that relates the connectivity of the market with the variance of its prices.",None,The invisible hand of Laplace: The role of market structure in price convergence and oscillation,https://www.sciencedirect.com/science/article/pii/S0304406821000136,28 January 2021,2021,Research Article,132.0
"Calleja Pedro,Llerena Francesc,Sudhölter Peter","Departament de Matemàtica Econòmica, Financera i Actuarial, Universitat de Barcelona-BEAT, Av. Diagonal, 690, 08034 Barcelona, Spain,Departament de Gestió d’Empreses, Universitat Rovira i Virgili-ECO-SOS, Av. de la Universitat, 1, 43204 Reus, Spain,Department of Business and Economics, University of Southern Denmark, Campusvej 55, 5230 Odense M, Denmark","Received 4 April 2020, Revised 18 January 2021, Accepted 18 January 2021, Available online 26 January 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102477,Cited by (2),"We show that on the domain of convex games, Dutta-Ray’s egalitarian solution is characterized by core selection, aggregate monotonicity, and bounded richness, a new property requiring that the poorest players cannot be made richer within the core. Replacing “poorest” by “poorer” allows to eliminate aggregate monotonicity. Moreover, we show that the egalitarian solution is characterized by constrained welfare egalitarianism and either bilateral consistency à la Davis and Maschler or, together with individual rationality, by bilateral consistency à la Hart and Mas-Colell.","In the context of transferable utility cooperative games (games, for short), Dutta and Ray (1989) introduced the ====, which combines individual interests with the Lorenz criterion to promote equality. Although this solution lacks general existence properties, on the domain of convex games it selects the unique Lorenz maximal imputation within the core. On this domain, the first axiomatizations of the egalitarian solution were provided by Dutta (1990) by means of ==== or ====, that is, consistency with respect to (w.r.t) the reduced games proposed by Davis and Maschler (1965) or Hart and Mas-Colell (1989), respectively, together with ==== (CE), a prescriptive property that determines the solution for two-person games. Klijn et al. (2000) reformulated the above characterizations replacing CE by ==== (EF), also known as Pareto optimality, that requires the solution to distribute the entire worth of the grand coalition, ==== (EDS), which forces the solution to select an allocation in the equal division core (Selten, 1972), and ==== (BMP) imposing an upper bound for the payoffs of the players receiving most, and only requiring DM-consistency and HM-consistency when these richest players leave with their assigned payoffs. Considering the anti-dual properties==== in the characterizations of Dutta (1990) and Klijn et al. (2000), Oishi et al. (2016) and Dietzenbacher and Yanovskaya, 2020a, Dietzenbacher and Yanovskaya, 2020b obtain new axiomatizations.====
 Hougaard et al. (2001) described another axiomatization combining DM-consistency and EF with ==== (IR) and ==== (RS). IR guarantees that no single player is worse off compared to staying alone, and RS requires that the solution can only make a player ==== richer than another player ==== if the maximum surplus (in the sense of Davis and Maschler, 1965) of ==== over ==== is positive and larger than the maximum surplus of ==== over ====. Arin et al. (2003) reinterpreted the egalitarian solution providing a characterization without making use of any consistency property and invoking ==== (CS) requiring that each coalition receives at least what it can get by itself, ====, ====, and ====.==== Recently, Llerena and Mauri (2017) provided two characterizations imposing a weak version of DM-consistency and either IR together with ==== and ==== (over the core)==== or CS and BMP.====In this paper, we provide several characterizations with and without consistency. For the latter, we use ==== (AM) defined by Megiddo (1974), a very natural property requiring that no player suffers if only the worth of the grand coalition increases,==== and ==== (BR), imposing an upper bound for the payoffs of non-poorest players, together with CS. Up to our knowledge, AM has not been employed before in any of the existing characterizations of the egalitarian solution. Under EF, BR is equivalent to ==== introduced in Oishi et al. (2016), which is the anti-dual of BMP. This fact leads to a parallel characterization making use of CS, BMP, and the anti-dual of AM, called ====. Strengthening BR, replacing “poorest” by “poorer”, we introduce ==== (SBR) which allows to eliminate AM. At this point, it is worth to mention that Arin and Iñarra (2001) characterize the egalitarian solution on the domain of convex games by CS and RS. Therefore, RS and SBR are equivalent in the presence of CS. Hence, on convex games, a core allocation satisfies SBR if a positive difference in the payoffs between two players can only occur when any transfer from the richer to the poorer player produces an unstable allocation, i.e., an allocation that is not in the core.====Moreover, we prove that, on the domain of convex games, DM-consistency for two-person reduced games, called ==== DM-consistency (2-DMC), implies CS. Furthermore, we show that bilateral HM-consistency (2-HMC) together with IR imply EF. These logical implications among properties allow us to revisit some of the well-established characterizations. We also show that the egalitarian solution can be characterized by ==== (CWE) in the sense of Calleja et al. (2021) and either 2-DMC or 2-HMC and IR. We recall that CWE requires to distribute an additional amount obtained by the grand coalition to the poorer players making payoffs as equal as possible subject to nobody is worse off. Hence, CWE implies AM. However, egalitarianism may regard CWE, though stronger than AM, as even more appealing. Indeed, CWE prioritizes those players who received less before the grand coalition became richer.====To conclude, we investigate if our results are valid for some extensions of the egalitarian solution and on larger domains than convex games. Some incompatibilities arise when imposing the properties we adopt in our axiomatizations, highlighting the limits of the characterizations of the egalitarian solution presented in the literature as well.====The remainder of the paper is organized as follows. Section 2 contains preliminaries on games. In Section 3 we introduce properties of solutions. Section 4 contains our main results. Section 4.1 is devoted to the characterization results of the egalitarian solution with AM and without consistency. In Section 4.2 we provide alternative axiomatizations for a variable society of agents making use of 2-DMC and 2-HMC. In Section 5, we study the compatibility of the groups of properties used in our characterization results on some subdomains of balanced games.",Axiomatizations of Dutta-Ray’s egalitarian solution on the domain of convex games,https://www.sciencedirect.com/science/article/pii/S030440682100015X,26 January 2021,2021,Research Article,133.0
"Basile Achille,Rao Surekha,Bhaskara Rao K.P.S.","Dipartimento di Scienze Economiche e Statistiche, Università Federico II, 80126 Napoli, Italy,School of Business and Economics, Indiana University Northwest, Gary, IN 46408, United States of America,Department of Computer Information Systems, Indiana University Northwest, Gary, IN 46408, United States of America","Received 16 August 2020, Revised 18 December 2020, Accepted 11 January 2021, Available online 23 January 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102474,Cited by (3)," of alternatives. The study is conducted in the case where the voters/agents are allowed to express indifference among elements of ====, and the domain of the scfs consists of preference profiles ==== over a society ==== of arbitrary cardinality. A representation formula for the two-valued CSP scfs is obtained that provides the structure of such functions.","The purpose of this paper is to show a representation formula for strategy-proof social choice functions ====under the following assumptions:====We adopt the term ==== for a ==== with range of cardinality two. If ==== is itself of cardinality two and ==== is onto, we shall use the term ====. We consider social choice functions that are ====, i.e. no group of agents has incentives to form a coalition that can manipulate the social choice for their own advantage with false reporting (see Definition 2.3). To be concise, sometimes we shall write scf to mean social choice function, and CSP to mean coalitional strategy-proof.====Representation formulas are more specific than characterization results since they de facto characterize, but also furnish an operational way to produce the desired scfs. Therefore we are addressing a rather natural question and, more, we are considering this question even in the simplest case of functions whose range is of cardinality two. Despite all this, until now not too many representation formulas can be found in the literature for tackling the case wherein voters may express indifference between alternatives. As far as we know, there are no representation theorems at our level of generality.====When we assume that every voter can only express strict orderings, namely when we limit the domain of ==== by considering only strict profiles,==== we have the following representation theorems ( Theorems 1.1==== and 1.1====).",The structure of two-valued coalitional strategy-proof social choice functions,https://www.sciencedirect.com/science/article/pii/S0304406821000124,23 January 2021,2021,Research Article,134.0
Zhu Feng,"Laboratory of Behavioral Economics and Policy Simulation & School of Economics, Nankai University, China","Received 9 June 2020, Revised 3 January 2021, Accepted 5 January 2021, Available online 18 January 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2021.102472,Cited by (4),"I analyze the optimal favoritism in a complete-information all-pay contest with two players, whose costs of effort are weakly convex. The contest designer could favor or harm some contestants using one of two instruments: head starts and handicaps. I find that any given player’s effort distribution is ranked in the sense of first-order ==== according to how (ex post) symmetric the players are in terms of competitiveness. Consequently, as long as the designer values effort from both contestants, “leveling the playing field” is optimal regardless of which instrument is used.","Contests are widely used to allocate scarce resources among competing individuals. Examples include lobbying, college admissions, and competitions for job promotion opportunities (see Konrad (2009), Dechenaux et al. (2015), and Vojnović (2016)). In many situations, contestants are ex ante asymmetric in their abilities and positions. For instance, when considering a job promotion competition, the manager may notice differences in productivity levels and progress among employees. Therefore, she may want to tailor the competition to encourage more effort from employees.====Problems like this are commonplace: the contest organizer often has discretionary power in designing contest rules and takes advantage of them to induce more competition. Two general approaches are considered in the literature. One approach is to set individual-specific prizes, as in Gürtler and Kräkel (2010) and Pérez-Castrillo and Wettstein (2016), where the contest reward depends on the identity of the winner. Another approach is to set individual-specific contest success functions; in this case, when facing the same bidding profile, the two players have different probabilities of winning (see Drugov and Ryvkin (2017) and Fu and Wu (2020) for a general analysis).====This paper adopts the second approach and investigates the design of individual-specific contest success functions. Two commonly used instruments are considered: head starts, which are added to players’ efforts, and handicaps, which discount players’ efforts. In recent years, there has been growing literature exploring similar questions in various contest formats (see Konrad (2002), Epstein et al. (2011), Li and Yu (2012), Kirkegaard (2012), Franke et al. (2013),  Seel and Wasser (2014), Kawamura and de Barreda (2014), and Franke et al. (2018)). The conventional wisdom suggests that it is optimal to “level the playing field”: the contest designer prefers an unbiased contest when contestants are symmetric, but a biased contest favoring the weaker contestant when they are asymmetric. Nevertheless, in most of the aforementioned literature (with the exception of Kawamura and de Barreda (2014) and Drugov and Ryvkin (2017)), while costs are assumed to be linear with respect to effort, this assumption is not necessarily satisfied in many practical settings. For example, in a job promotion competition, much more effort is usually required to improve the work quality from excellent to perfect than from mediocre to good for any given employee.====From a theoretical perspective, this linearity assumption may cause us to overlook information that we should not have. Previous studies find the curvature of cost functions to be a decisive factor that cannot be ignored in contest design problems (see Moldovanu and Sela (2001), Drugov and Ryvkin (2017), Olszewski and Siegel (2020) and Fang et al. (2020)): Moldovanu and Sela (2001) find that it is optimal to allocate the entire prize to a single winner when cost functions are linear or concave; however, several positive prizes may be optimal when cost functions are instead convex. Similarly, Olszewski and Siegel (2020) find that the optimal number of prizes depends on the curvature of the costs in performance-maximizing large contests.====In light of this deficiency, the current paper investigates the optimal design of biased contests when cost functions are weakly convex. A related work is Drugov and Ryvkin (2017), which introduces a general class of biased contest success functions and studies optimal bias when players are ex ante symmetric. Drugov and Ryvkin (2017) provide conditions under which zero bias is optimal and prove through examples that biased contests may be optimal when such conditions fail. The key assumption in their model is that contest success functions are smooth: this includes Tullock (1980) lottery contests and Lazear and Rosen (1981) type tournaments. Nevertheless, one large class of contests is excluded by this assumption — that is, all-pay auctions, or more generally, all-pay contests.====In this paper, I use the framework in Siegel (2014a) and focus on the design of the optimal biased all-pay contest with complete information. A contest designer could influence the outcome of the contest by giving head starts to or handicapping players, whose costs of effort are weakly convex. For simplicity, I omit the word “weakly” hereafter: all relations are in the weak sense, unless explicitly stated as “strictly”. I find that, regardless of which instrument is used, any given player’s effort distribution is ranked in the sense of first-order stochastic dominance according to how (ex post) symmetric the two players are in terms of competitiveness. Consequently, for any objective functions increasing in efforts, it is optimal for the organizer to “level the playing field”. Comparing head starts and handicaps, I show that no instrument always dominates over the other, and I provide sufficient conditions for head starts to be more efficient, under the two most-studied objectives: total effort and maximum individual effort. Lastly, I study optimal combinations of the two instruments under the aforementioned two objectives and find that the designer benefits from using both instruments simultaneously; in fact, by doing so, she achieves her first best result when her objective is maximum individual effort.====This paper’s contributions are threefold. First, by allowing for convex cost functions, my result generalizes the conventional wisdom that a contest designer benefits from “leveling the playing field” and that (under certain conditions) head starts are a more efficient tool than handicaps. In this regard, a closely-related work is Li and Yu (2012), who find that, in revenue-maximizing all-pay auctions, “leveling the playing field” is optimal and that handicaps are less efficient than head starts. The current paper provides boundaries for their results to hold in more general cases: it may be suboptimal to “level the playing field” when cost functions are not convex, and sometimes handicaps can, in fact, be more efficient than head starts when cost functions are strictly convex.====Second, one conceptual element that sets this paper apart from the rest of the literature is that the current paper examines the distribution of efforts rather than a summary statistics, such as total effort. Consequently, I am able to show a stronger result: any player’s effort distribution is ranked in the sense of first-order stochastic dominance, according to how (ex post) symmetric the two players are in their competitiveness.====Finally, some results in this paper also contribute to the contest literature by deepening our understanding of optimal favoritism in contests with non-smooth contest success functions under complete information. For example, one implication of my result is that in a symmetric all-pay auction with complete information, the optimal head start is of size zero. This contrasts with the findings in Seel and Wasser (2014) that, with incomplete information, the optimal head start is always of a strictly positive size. This discrepancy emphasizes the role that information plays in the design of all-pay contests and is consistent with findings on the design of the optimal handicaps in lottery contests (see Fu (2006) and Kirkegaard (2012) for analyses in complete and incomplete information settings respectively). Also, my model yields contrasting results to contest models with smooth contest success functions. For instance, Drugov and Ryvkin (2017) show that a head start can improve aggregate effort supply in two-player Tullock contests with two symmetric players. As Section 3.1 shows in the current paper, though, this result fails to hold when contest success functions are non-smooth.====The remainder of the paper is organized as follows. Section 2 sets up the model. Sections 3.1 Optimal head start, 3.2 Optimal handicap, 3.3 Comparison between instruments identify the optimal head start and handicap and make a comparison between them. Section 3.4 discusses what happens when cost functions are not convex. Section 4 investigates the optimal combination of both instruments in two special cases, and Section 5 concludes. Omitted proofs are in Appendix.",On optimal favoritism in all-pay contests,https://www.sciencedirect.com/science/article/pii/S0304406821000100,18 January 2021,2021,Research Article,135.0
"La Torre Davide,Liuzzi Danilo,Marsiglio Simone","SKEMA Business School, Université Cǒte d’Azur, Sophia Antipolis, France,University of Milan, Department of Economics, Management and Quantitative Methods, Milan, Italy,University of Pisa, Department of Economics and Management, Pisa, Italy","Received 15 July 2020, Revised 2 November 2020, Accepted 22 December 2020, Available online 18 January 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102473,Cited by (17),"We analyze the determination of the optimal intensity and duration of social distancing policy aiming to control the spread of an infectious disease in a simple macroeconomic–epidemiological model. In our setting the social planner wishes to minimize the social costs associated with the levels of disease prevalence and output lost due to social distancing, both during and at the end of epidemic management program. Indeed, by limiting individuals’ ability to freely move or interact with others (since requiring to wear face mask or to maintain physical distance from others, or even forcing some businesses to remain closed), social distancing has on the one hand the effect to reduce the disease incidence and on the other hand to reduce the economy’s productive capacity. We analyze both the early and the advanced epidemic stage intervention strategies highlighting their implications for short and long run health and ==== outcomes. We show that both the intensity and the duration of the optimal social distancing policy may largely vary according to the epidemiological characteristics of specific diseases, and that the balancing of the health benefits and economic costs associated with social distancing may require to accept the disease to reach an endemic state. Focusing in particular on COVID-19 we present a calibration based on Italian data showing how the optimal social distancing policy may vary if implemented at national or at regional level.","Communicable diseases have historically played (and still do today) a major role in shaping economic development in both industrialized and developing countries, by determining till today 30% of deaths and accounting for 45% of the related years of life lost worldwide (====, ====). A variety of channels through which this may occur, including education attainment, labor force participation, life expectancy, income and civil conflict, have been identified and extensively documented in literature (====, ====, ====, ====, ====, ====). As health services are to a large extent publicly provided in most countries, the severity of such effects is amplified by the need to finance public health policies diverting resources from productive activities (====, ====). The relevance of these findings and arguments has over the last two decades given birth to a growing economic epidemiology literature to accompany the mathematical epidemiology literature in analyzing the implications and dynamics of communicable diseases. While mathematical epidemiology aims to characterize the epidemic dynamics according to the biological features of specific diseases (====, ====), economic epidemiology tries to understand the mechanisms through which health policy, in the form of preventive and treatment measures, may be used to contrast the spread of communicable diseases (====, ====; ====, ====), and only few works have tried to assess their macroeconomic implications (====, ====, ====, ====). ====, ==== and ==== focus on the short run effects of epidemics on income levels analyzing the feedback effects between the disease spread and the availability of resources to fight it, assessing whether preventive or treatment policy measures may be most effective. All these works, both those adopting a microeconomic and a macroeconomic perspective, share the same analytical framework to characterize the epidemic dynamics by relying on the susceptible–infected–susceptible (SIS) model, which is one of the simplest and most general mathematical epidemiology setups. In this paper we contribute to this literature adopting the same SIS framework to analyze the relation between epidemics and macroeconomic outcomes in light of the recent coronavirus pandemic experience. Indeed, since upon recovery from COVID-19 individuals return susceptible to a new infection (====), the SIS model can be applied also to characterize the dynamics of the coronavirus epidemic.====The recent COVID-19 epidemic has spurred a large and shared interest in understanding the mutual relations between communicable diseases, macroeconomic outcomes and health-economic policy. COVID-19 is a highly contagious virus-induced communicable disease, transmitted via droplets and contaminated objects during close unprotected contact between an infector and infectee (====). The virus is currently spreading fast from human-to-human as transmission simply occurs when healthy individuals meet respiratory droplets from coughs or sneezes of an infected person. Transmission is also possible via contaminated objects or materials which act as a carrier of the virus, such as cloths, utensils, and furniture (====). The outbreak of the disease has origin in China in late 2019 and has reached a pandemic status in only a few months, resulting thus far (at the time of writing, in early July 2020) in over 9.3 million confirmed cases and over 480,000 deaths globally (====). The implemented policy responses to reduce the spread of the disease have been countless (i.e., more than 13,000 in more than 195 countries), including traditional preventive and treatment measures but also social distancing (====). Social distancing refers to all those arrangements which aim at reducing the rate of transmission of a disease by limiting the exposure of single individuals to possible sources of infection. Social distancing is thus one of the most effective non-pharmaceutical disease control actions that can reduce the spread of a highly contagious disease (====). Understanding thus the health and economic consequences of specific disease control policies, and in particular those of social distancing, is essential in order to minimize their social cost and support policymakers in the difficult task of effectively managing the pandemic.====Over the last few months several works have tried to analyze the macroeconomic implications of COVID-19 and the policy measures implemented to control its spread (====, ====, ====, ====, ====). ==== analyzes a purely dynamic epidemiological setup in which the disease is described by a susceptible–infected–recovered (SIR) model, which is based on the assumption that recovery from the disease confers permanent immunity; through a comparative dynamic exercise he shows how different mitigation efforts affect the epidemic dynamics by reducing the disease transmission rate. Building on Atkeson’s epidemiological setup (2020), ==== analyze the links between economy and epidemics in a setting where the lockdown policy determines how much output agents can produce; by focusing on a social planner’s framework they determine the optimal duration and intensity of the lockdown showing in their baseline scenario that it may need to be initially severe and implemented shortly after the outbreak. ==== rely on a similar centralized economic setup enriching the epidemiological framework to consider a multi-group SIR model to distinguish the disease impact among young, middle-aged and old individuals, showing that intuitively increasing the severity and the duration of the lockdown for the old may allow to relax it for younger agents. ==== focuses on a multi-group SIR framework to distinguish between different severity levels of the disease in a competitive framework, analyzing the effects of different intensity of lockdown policies through comparative dynamics. ====Despite most of the works are based on a SIR setup, since as to date there is no evidence that ==== (====), it seems more reasonable to rely on a SIS framework in which after recovery people return to be susceptible and thus can get infected again, as in ====. This has also the advantage to allow for a direct comparison of our paper with the macroeconomic epidemiology literature (====, ====, ====, ====). Different from ====, since the epidemic management is a short run problem we abstract from saving and capital accumulation, considering a simple framework in which the disease prevalence determines the income level and disposable income is entirely consumed, as in ====. Different from both ==== and ==== in which the time horizon in infinite, we consider a finite time horizon to investigate the intensity and the duration of the optimal policy, along the lines of ==== and ====. Similar to them we focus on a centralized problem since in the real world the response to the disease has been mainly driven by public regulations and governmental measures rather than by behavioral changes. Different from them in our setup the social planner does not choose only the degree of social distancing but also that of therapeutic treatment: treatment is publicly provided and is financed through income taxation, thus the spread of the disease by lowering income reduces also the availability of resources to fund treatment measures, as in ====. In our setting the social planner aims to minimize the social costs associated with the disease prevalence and the strength of the policy tools employed to manage the epidemic, similar to ====. However, different from them, rather than relying only on numerical simulations we aim to determine analytically the duration and the intensity of the optimal policies as in ====).====Specifically, in our framework the social planner determines the optimal intensity and duration of social distancing by balancing its beneficial effects on health outcomes and its detrimental macroeconomic consequences. Indeed, by limiting individuals’ ability to freely interact with others social distancing allows to reduce the disease incidence but it also leads to a reduction in the economy’s productive capacity. Therefore, the planner wishes to minimize the social costs associated with the levels of disease prevalence and output lost due to the social distancing policy. Our setup is not strictly tailored to investigate the consequences of COVID-19 but it is applicable in more general terms, allowing us to consider the macroeconomic implications of a variety of communicable diseases. We show that in the early stages of an epidemic outbreak the disease dynamics can be proxied by a ==== and thus it is possible to explicitly determine both the optimal intensity and duration of social distancing. In later stages such an approximation is not applicable and the nonlinearity of the epidemiological dynamics precludes us from the possibility to derive analytical solutions. Through numerical simulations we illustrate how the optimal social distancing may largely vary according to the epidemiological features of specific diseases. For example, in the case of the seasonal flu the social distancing policy needs to be stricter and to last for longer than in the case of the common cold, because of its higher infectivity rate. We also show that our model is applicable to COVID-19 (even if by abstracting from the disease-induced mortality our results need to be taken with some grain of salt) by presenting a calibration based on Italian data, distinguishing between national- and regional-level policy in which an early stage and an advanced stage epidemic model applies respectively, illustrating thus how the early stage epidemic setup may be a useful approximation in concrete real world circumstances.====The rest of the paper proceeds as follows. Section ==== presents our macroeconomic–epidemiological framework in which the social planner seeks to determine the optimal intensity and duration of the mitigation policy (i.e., social distancing) to minimize the social costs of the epidemic management program. Section ==== discusses the advanced epidemic case, in which the number of infectives is already non-negligible with respect to the population size. In both cases we focus on the determination of the optimal intensity and duration of social distancing showing its implications on macroeconomic and health outcomes, and illustrating how social distancing might need to be implemented to control the diffusion of widespread diseases such as the seasonal flu and the common cold. Section ==== presents a calibration of our early and advanced epidemic setups to the Italian COVID-19 experience, showing how social distancing should be differently applied at national and regional level. Section ==== as usual concludes and presents directions for future research. Technicalities are postponed to ====, ====.",Epidemics and macroeconomic outcomes: Social distancing intensity and duration,https://www.sciencedirect.com/science/article/pii/S0304406821000112,18 January 2021,2021,Research Article,136.0
"Guo Jing,He Xue Dong","Department of Industrial Engineering and Operations Research, Columbia University in the City of New York, New York, NY, 10027, USA,Room 609, William M. W. Mong Engineering Building, Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong","Received 30 April 2020, Revised 23 November 2020, Accepted 21 December 2020, Available online 16 January 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2020.102470,Cited by (1),"Narrow framing is the idea that, when considering a monetary risk, the individual evaluates it to some extent in isolation and separately from her other risks. Originally documented in experimental settings, narrow framing has been widely applied to explain real-world investor behavior. We show that a prominent ==== of narrow framing presented in Barberis and Huang (2009) has some drawbacks that limit its applicability. We then propose a new model of narrow framing that overcomes these limitations and show its tractability in applications to choice over monetary gambles.","Tversky and Kahneman (1981) find that individuals can frame the same problem in different ways, and very often they use an approach called ====: when making multiple decision choices or evaluating multiple risks, they tend to consider one of them at a time, isolating it from other choices or risks. Since this seminal work, narrow framing has been found to operate extensively both in experimental settings and in real lives; see for instance Read et al., 1999, Rabin and Weizsäcker, 2009, Kumar and Lim, 2008, Haisley et al., 2008, Choi et al., 2009, and Baltussen and Post (2011).====More importantly, narrow framing can help explain many empirical findings in economics and finance that cannot be satisfactorily explained by classical models. For example, Barberis et al. (2006) find that a typical individual turns down independent, small gambles, such as a 50:50 bet to win $550 or lose $500, even with a wealth level of $1,000,000 and at the same time accepts some large, independent gambles, such as a 50:50 bet to win $20,000,000 or lose $10,000. As shown by Rabin (2000), the rejection of small gambles with positive expected values is difficult to explain under expected utility theory (EUT). Barberis et al. (2006) show that the above preferences can be explained by narrow framing. A second example is the stock market non-participation puzzle: many households refrain from investing in stocks even though the stock market offers a high average return (Mankiw and Zeldes, 1991); see e.g., Heaton and Lucas (2000). Barberis et al. (2006) offer an explanation for this puzzle based on narrow framing. Another example is the equity premium puzzle: the historical average return on stocks in excess of the risk-free rate (such as return on T-bills) is too high to be explained by the classical consumption-based capital asset pricing model with reasonable degree of risk aversion; see e.g., Mehra and Prescott (1985). Benartzi and Thaler (1995) and Barberis and Huang, 2001, Barberis and Huang, 2009 show that narrow framing can help to explain this puzzle. Narrow framing is also central to the leading explanations for the disposition effect (Barberis and Xiong, 2012, Ingersoll and Jin, 2013, Li and Yang, 2013b, Meng, 2010, Andrikogiannopoulou and Papakonstantinou, 2020) and to an important explanation for the momentum effect (Grinblatt and Han, 2005, Li and Yang, 2013b). In a recent paper, Barberis et al. (2020) consider an asset pricing model that combines narrow framing and prospect theory (Kahneman and Tversky, 1979, Tversky and Kahneman, 1992) and show that this model can explain 13 out of 22 prominent stock market anomalies. Narrow framing has also been applied to explain some stylized facts in insurance; see for instance Gottlieb and Smetters, 2012, Gottlieb and Mitchell, 2015, Brown et al., 2008, and Brown et al. (2019).====There was, however, little work on building a broadly applicable model of narrow framing until Barberis and Huang (2009); see also Barberis et al. (2006) and Barberis and Huang (2008).==== The model proposed by Barberis and Huang (2009), which we refer to in the following as the ====, is formulated by generalizing the classical recursive utility model (Kreps and Porteus, 1978, Epstein and Zin, 1989) in that (i) the risks that are evaluated in isolation by the individuals at the end of each period are assessed according to prospect theory so that the utility of gains and losses experienced by the individuals in these risks is calculated, (ii) the utility of gains and losses and the certainty equivalent of the individuals’ total utility from next period are added linearly with the weight for the former to be a positive constant in the linear addition, and (iii) the sum of the utility of gains and losses and the certainty equivalent of the total utility from next period is aggregated with the individuals’ consumption in the current period via an aggregation function, resulting in the individuals’ total utility at the beginning of the current period. The certainty equivalent and aggregation function are chosen so that the so-called relative risk aversion degree (RRAD) and elasticity of intertemporal substitution (EIS) are constant, and the time horizon is usually set to be infinite. The BH model provides an analytical framework to study the impact of narrow framing on decision making. Indeed, the aforementioned explanations for individuals’ attitudes to certain monetary gambles, non-participation in the stock market, and the equity premium puzzle offered in Barberis et al. (2006) and Barberis and Huang (2009) are based on the BH model. Further extensions and other applications of the BH model can be found in De Giorgi and Legg (2012), He and Zhou (2014), and Easley and Yang (2015).====In the present paper, we show that the BH model has some limitations in that the total utility process may not be uniquely defined in this model. More precisely, when (i) an agent consumes a constant fraction of her wealth and invests another constant fraction of her wealth into assets whose returns are independent and identically distributed (i.i.d.) over time, (ii) the EIS is less than or equal to one, and (iii) the utility of gains and losses experienced by the agent is negative, it is the case either that the total utility process of the agent in the BH model does not exist or that there exist two solutions to the recursive equation that defines the total utility process. Even in the finite-horizon setting, the BH model may fail to define the total utility process. Intuitively, the failure of the BH model arises from the restriction that the aggregation of the utility of gains and losses and the certainty equivalent of the total utility from the next period must be nonnegative. Imagine that in the BH model, because the weight for the utility of gains and losses is a constant, the magnitude of a negative utility of gains and losses can dominate the certainty equivalent of the total utility from the next period, and thus the sum of these two can be negative. In the literature, the applications of the BH model usually involve formulating a dynamic programming equation heuristically and solving it numerically. We show that this equation can have multiple solutions. When one applies the dynamic programming equation repeatedly, different starting points can lead to different solutions. Even worse, different solutions to the equation lead to different portfolios solved from the dynamic programming equation.====On the one hand, the limitations of the BH model do not diminish its value in the literature. Indeed, as mentioned above, the BH model has been successfully applied in decision making, portfolio selection, and asset pricing. On the other hand, the limitations of the BH model need to be fixed because the cases in which the BH model does not admit a unique total utility process are economically important. Indeed, in many applications of the BH model, the EIS takes a value that is less than one and the utility of gains and losses is negative; see for instance Barberis and Huang (2009) and De Giorgi and Legg (2012). Moreover, in those applications, the agents also consume constant fractions of their wealth and the asset returns are i.i.d.====To fix the above limitations of the GH model, we propose a new preference model of narrow framing, referred to as the ====. In this new model, the utility of gains and losses and the certainty equivalent of the individuals’ total utility from the next period are also added linearly, but instead of a constant, the weight for the utility of gains and losses is scaled to be proportional to the certainty equivalent of the total utility from next period per unit wealth. In other words, in the GH model a negative (positive, respectively) utility of gains and losses reduces (increases, respectively) the agent’s total utility ====. Thus, the GH model can be regarded as a modification of the BH model.====In addition to fixing the limitations of the BH model, we also incorporate in the GH model the features of probability weighting and diminishing sensitivity that are important components of prospect theory but are not considered in the BH model; see Section 3.3. Barberis et al. (2020) show that these two features are as important as loss aversion in explaining stock market anomalies. Thus, a model of narrow framing that features probability weighting and diminishing sensitivity can potentially have more applications than the BH model.====In the GH model, because the reduction of the total utility by a negative utility of gains and losses is proportional, the total utility remains positive after aggregating the utility of gains and losses. As a result, the GH model defines a unique total utility process whether the time horizon is finite or infinite. Moreover, the GH model in the finite-horizon setting with any utility specification at the terminal time converges to that in the infinite-horizon setting as the number of periods in the former goes to infinity. This result also implies that the total utility process in the infinite-horizon setting can be computed by the standard recursive algorithm of starting from any positive value and applying the recursive equation for the GH model repeatedly. Moreover, when the GH model is applied to portfolio selection, the resulting dynamic programming equation admits a unique solution and the solution can be computed by the standard recursive algorithm.====We then apply the GH model to study individuals’ attitudes toward risk. We show that, similar to the BH model, the GH model can explain an aversion to a small, independent, actuarially favorable gamble and acceptance of a large, independent gamble over a reasonable range of wealth levels. Moreover, the GH model is more tractable than the BH model.====Finally, let us highlight the difference of the present paper from Guo and He (2017). The contribution of the present paper is to propose a new model of narrow framing, namely the GH model, and to compare it extensively with the BH model. Guo and He (2017) is a particular application of the GH model, where the authors study the interaction of investors with recursive utility preferences and investors with loss aversion and the long-run behavior of these investors, assuming the preferences of the second type of investors to be represented by the GH model.====The remainder of the paper is organized as follows: In Section 2 we show the limitations of the BH model. In Section 3, we propose the GH model and establish the existence and uniqueness of the utility process under this model and the solution to the dynamic programming equation for portfolio selection under this model. In Section 4, we apply the GH model to explain individuals’ attitudes toward timeless gambles and show that the GH model is more tractable than the BH model. Section 5 concludes. All proofs are placed in Appendix.",A new preference model that allows for narrow framing,https://www.sciencedirect.com/science/article/pii/S0304406820301476,16 January 2021,2021,Research Article,137.0
"Bandyopadhyay Siddhartha,Chatterjee Kalyan,Das Kaustav,Roy Jaideep","Department of Economics and Institute for Global Innovation, University of Birmingham, Egbaston Birmingham, United Kingdom,Department of Economics, Penn State University, 504 Kern Building, University Park, United States,Department of Economics, Finance and Accounting, University of Leicester, Brookfield, London Road, Leicester, United Kingdom,Department of Economics, University of Bath, Claverton Down, Bath, United Kingdom","Received 17 July 2020, Revised 29 October 2020, Accepted 4 November 2020, Available online 9 January 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2020.11.008,Cited by (12),"We analyze a model where the government has to decide whether to impose a lockdown in a country to prevent the spread of a possibly virulent disease. If the government decides to impose a lockdown, it has to determine its intensity, timing and duration. We find that there are two competing effects that push the decision in opposite directions. An early lockdown is beneficial not only to slow down the spread of the disease, but creates beneficial ==== (such as social distancing, developing hygienic habits) that persists even after the lockdown is lifted. Against this benefit of an early lockdown, there is a cost from loss of information about the virulence and spread of the disease in the population in addition to a direct cost to the economy. Based on the prior probability of the disease being virulent, we characterize the timing, intensity and duration of a lockdown with the above mentioned tradeoffs. Specifically, we show that as the precision of learning goes up, a government tends to delay the imposition of lockdown. Conversely, if the habit formation parameter is very strong, a government is likely to impose an early lockdown.","The entire world has been severely affected by the emergence of a new respiratory illness, named coronavirus 2 (SARS-CoV-2), popularly known as Covid-19. Since the first recorded hospitalization in China on the 12th of December, ==== (====, ====), the disease has rapidly spread to other regions across the globe taking on the nature of a pandemic. Yet, because of its novelty, many decisions on how to respond to the possible pandemic had to be taken at a stage when much remained unknown about the disease, including its virulence and how to mitigate against that.====One of the common responses to preventing the spread of the disease has been through lockdown, a broad term by which we mean drastically reducing movement of individuals which includes severe restrictions on human to human contact. This has consisted of restrictions on when to leave one’s residence, where to go and whom to meet with, as well as shutting down non-essential parts of the economy. While these measures no doubt lower the spread of the infection, they come with a huge cost to the economy. Additionally, given that the disease is novel, a lockdown also leads to a ====.==== about the dynamics i.e. the spread and severity of the disease within the population had it been allowed to take its own course.====In this paper, we analyze a model of lockdown where the government (either a benevolent autocrat or a democratically elected leader) has to decide when (if at all) to impose a lockdown. Imposing a lockdown lowers the harm from the disease through lowering the rate of reproduction of the disease (the so-called R number). A lockdown also signals to the population the probable seriousness of the disease and makes them conscious of the need to take precautions against the disease via e.g. social distancing, wearing masks and practicing good hygiene. We posit that these precautionary actions often turn into habits even after a lockdown is lifted through inducing non-transitory changes in the behavior of the citizenry (see ====). This behavioral response is indeed being seen in many countries where a majority of citizens maintain social distancing and continue to reduce their social contact and take precautions against the disease even after the government eases some of the restrictions imposed earlier. As ==== point out, the use of lockdowns thus had an additional effect of signaling how important it was to change behavior, obviating the need for long drawn out draconian lockdowns for disease containment. This idea is formalized through our ==== parameter ==== which internalizes some of the messages that a lockdown conveys. Thus, an early lockdown has a benefit of inducing changes in behavior in the population that lasts beyond the lockdown.====Against this benefit of an early lockdown, there is a cost from loss of information (in addition to a direct cost to the economy). An early lockdown prevents the natural course of the disease and reduces its consequent public health cost but its suppression does not allow us to discover the dynamics of the disease and its consequent costs with any precision. Given the enormous cost to the economy of the lockdown, an early lockdown based on imperfect information on the severity of the disease may lead to a possibly inoptimal response e.g. imposing huge social and economic costs not warranted by the true severity of the disease.====While this effect has not been emphasized, there are a group of eminent scientists who have argued against a lockdown at least partly on the grounds that the data available was not precise enough to warrant a lockdown. For example, Sunetra Gupta, a professor of theoretical epidemiology at the University of Oxford, U.K. does not believe that the disease is likely to be virulent enough to justify a lockdown given its huge economic and social costs.==== Their model as well as a modification run by James Hay==== effect would push towards a delay in the timing of a lockdown. The present paper studies the interplay of these two independent aspects, namely, the informational loss and the behavioral benefit, and its net effect on the optimal timing and severity of a lockdown. We now describe the model in more detail, explain the main results and discuss what questions it allows us to answer.====We consider a three-period model, with the third period representing the end of the current planning horizon. We assume that the decision on whether and for how long to impose a lockdown is restricted to the first two periods. In period 1, the government gets information (via a signal with a certain level of precision) and forms its belief about the probability that the disease is virulent. If it decides to impose a lockdown, it has to determine for how many periods and with what intensity. The lockdown reduces the spread of the disease but we assume no further information is forthcoming on the virulence of the disease during the lockdown.==== If there is no lockdown in period 1, the government gets a further signal about the virulence of the disease and decides whether or not to impose a lockdown in period 2. Further, as mentioned earlier, an early lockdown changes the way people behave and induces a behavioral response whereby people continue to take voluntary steps to prevent the spread of the disease through habit formation. Lockdowns are however economically and socially costly and thus each lockdown period has a cost.====If the government is democratically elected, it may also face a cost from imposing a lockdown that is decreasing in the prior probability that the disease is virulent. We can think of this as a backlash from the economic hardship faced by the population. The strength of the backlash effect depends on various factors which we can think of summarily as representing trust in government. For a given belief about the probability of the disease being virulent, a lower trust is likely to cause people to impose higher electoral costs on the government. We denote this cost of public opposition to lockdown, particularly of consideration in a democracy by a parameter ====.====This model thus allows us to answer a number of important questions of relevance to a policy maker. Given the information in any period that the disease is virulent, we can analyze whether the government should impose a lockdown, how intense should the lockdown be and if imposed early, for how long should it be. We are further interested in understanding the cutoff probability that the disease is virulent at which a government decides to lockdown and how this shifts over time as a function of (i) the accuracy of the signal each period, (ii) the strength of the behavioral response, (iii) the cost to the economy, (iv) the magnitude of the backlash effect and (v) the expected time of arrival of a vaccine/treatment.====We characterize the tradeoff between information and habit formation. When the strength of the habit formation parameter (====) is high, early lockdown is optimal. In particular, we show that for any given ====, as long as the signal precision (====) is below a certain threshold, the cutoff belief for imposing a lockdown is lower in the first period than in the second period. Conversely, if the signal precision is higher than the threshold, the opposite holds as the possible information loss would make the government delay lockdown. Hence, the cutoff belief for imposing a lockdown is higher in the first period than in the second period. The democracy parameter ==== also plays a role, as this acts to delay the timing, duration and intensity of the lockdown given the possible public backlash against lockdowns. This also tilts towards waiting for a more precise signal that may reduce public opposition to the backlash. This suggests that democratic governments in populations distrustful of governments may delay lockdowns because of possible electoral consequences emanating from public backlash against a lockdown.====There are a number of other important comparative static results. The intensity of lockdown is increasing in ==== and the beliefs that the disease is virulent but decreasing in the loss to the economy. The parameter ==== represents the extent of the economic impact of the lockdown, with a higher value indicating a more negative impact. Consistent with the intuition in ====, we characterize how under some parameter values, optimal lockdowns are not draconian or long drawn out i.e. of low intensity and only for one period. Somewhat counterintuitively, while intensity of lockdowns go down as economic costs increase, we also see if ==== is high, the cutoff belief for early lockdowns is lower than that for later lockdowns. This is because if economic costs of a lockdown are high, conditional on imposing a lockdown, it is better to do so early than late to get the full benefit of disease reduction because of habit formation.====Our model’s assumptions on disease propagation and benefits of a lockdown are consistent with standard models in epidemiology and economics. Lockdowns in epidemiological models of disease propagation are a vital tool for slowing the incidence of the disease. The basic models (see ====) assume that the hazard rate is an increasing function of prevalence in the population. Economic models however assume people are rational and hence even without a lockdown, they update information about disease prevalence and take precautions to reduce their chances of infection through (individually) optimal social distancing (see ====). Nonetheless even in these fully rational models, the individual decision (i.e. choice of how much to socially distance) is not in general socially optimal as rational agents do not consider the external benefits of social distancing (see ====). Hence, imposing lockdowns to reduce human contact can be justified even in such models to reach the social optimum. Our model is agnostic about whether agents are fully rational and indeed abstracts from the decision of the population to focus on the government’s optimal timing of lockdown.====The economic costs of lockdown as well as the public health benefits are broadly consistent with both the epidemiological and economic models.==== Indeed in the Covid-==== context there are a large number of recent papers modeling the trade-off between the public health benefit (disease suppression) and the economy (see ====, ====, ====, ====).==== It assumes the disease is virulent and the reproduction number is uncertain. Its focus is on lockdown intensity allowing for some learning during a (partial) lockdown, though it assumes the virus is necessarily lethal. The model does not consider the trade-off between information acquisition versus allowing disease-reducing behavior to kick in early through behavioral changes.====To the best of our knowledge, our model’s insights on the trade-off between acquiring information and changing behavior have not been formalized before. It has some similarities with the literature on adoption of new technology where there is an optimal period at which one should adopt the technology where there is uncertainty in the costs and benefits of adoption. Waiting delays the potential benefits from the technology but allows us to learn more about the actual costs and benefits from adoption (see e.g. ====, ====, ====, ====). This literature however does not consider the tradeoff with habit formation.====At a more macro level, the impact of diseases on the economy has been looked at both empirically and theoretically including the endogeneity of diseases, labour supply and human capital accumulation and hence growth, including the possibility of a growth trap (====, ====, ====, ====). While these papers consider the impact of interventions on growth, they do not consider interventions in the context of a novel disease that has a trade-off between the informational value of delaying interventions and kicking in habit formation early.====The general expression for continuation payoff from waiting and not imposing a lockdown in period ==== is given by ====First, consider ====. We have argued above that for any signal ==== received in period ====, ====, the posterior in period ==== is less than or equal to ====. This means we have ====Substituting this in the general expression of ==== we obtain ====
 ====
 ====Next, consider ====. Only if a high signal is received in period ====, a lockdown is imposed. This gives us ====
 ==== and ====Thus, we have ====Finally, consider ====. Irrespective of the signal received in period ====, a lockdown is imposed. This gives us ====
 ==== and ====
 ====From this we can say ====From the above arguments, we can indeed confirm that ==== with strict inequality for ====. We can express ==== as ====
 ====
 ",Learning versus habit formation: Optimal timing of lockdown for disease containment,https://www.sciencedirect.com/science/article/pii/S0304406820301294,9 January 2021,2021,Research Article,138.0
"Karagözoğlu Emin,Sağlam Çağrı,Turan Agah R.","Bilkent University, Turkey,CESifo-Munich, Germany,Presidency Office of Strategy and Budget, Turkey","Received 10 December 2019, Revised 21 December 2020, Accepted 23 December 2020, Available online 6 January 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmateco.2020.102469,Cited by (1),"We study a tug-of-war game between two players using the lottery contest success function (CSF) and a quadratic cost (of effort) function. We construct a pure strategy symmetric Markov perfect equilibrium of this game, show that it is unique, and provide closed-form solutions for equilibrium strategies and values. In stark contrast to a model of tug-of-war with an all-pay auction CSF, players exert positive efforts until the very last battle in this equilibrium. We deliver a set of empirically appealing results on effort dynamics.","In a multi-battle or dynamic contest efforts in an individual battle and the corresponding battle outcome may influence future efforts and battle outcomes. Such an influence may have implications for parties’ behavior in earlier rounds (see Konrad, 2012). For instance, if losing the first battle makes the whole contest a write-off, then excessive efforts may be induced in the first battle. The presence of such dynamic linkages gives rise to some interesting questions such as “How do efforts vary across battles?”, “How do efforts vary with intensity of rivalry?”, and “Who exerts a greater effort: the leader or the follower?” (see Harris and Vickers, 1987).====The literature on contest games produced various models of dynamic contests such as ====, ====, ====, ====, and ==== (see Konrad, 2012 for a review). Harris and Vickers (1987) argued that tug-of-war is possibly the simplest framework to address questions regarding effort dynamics since this model has a single state variable — a measure of the distance between players. Along these lines, the current paper is concerned with the questions mentioned above, and it focuses on the model of ==== between two players.====A tug-of-war is a multi-battle contest game with a finite number of ordered states and potentially infinite number of battles. Players start at an initial state (neutral or non-neutral) and simultaneously exert effort in each battle to win the contest. Winning a battle moves the state towards the winning player’s favorite terminal state. This game can be illustrated by a horizontal line with an interior point that represents the initial state and two end-points that represent respective players’ favorite terminal states, resembling the sports competition after which the model is named. Players win the prize/award if they have won sufficiently many battle victories to pull the state to their terminal state. The winner of each battle is determined by a ==== (CSF for short), which takes players’ efforts as input. As the above description of the game reveals, what matters in tug-of-war is not the absolute number of battle victories, but the difference between the two players’ numbers of victories.====Two observations are worth mentioning here. First, we observe suspense and perseverance in real-life tug-of-war (see Deck and Sheremata, 2019 for recent experimental evidence) — an observation not completely captured by the theoretical literature. Second, we observe that exogenous noise is an essential feature of many real-life contests (see Thorngate and Carroll, 1987). Consequently, we study a tug-of-war game between two players, where the battle outcomes are determined by a ==== (a special case of the Tullock CSF; see Tullock, 1980). In our model, (i) there are no intermediate prizes, (ii) the cost of effort is quadratic, and (iii) players do not discount the future.====We completely characterize the symmetric Markov perfect equilibrium of this game under a regularity assumption. The equilibrium strategies are deterministic. Furthermore, this equilibrium is unique. We also offer a set of results on effort dynamics and some comparative statics. Our main results are as follows:====Some of our results resemble some of those in Harris and Vickers (1987), who also provided comparative static results. For instance, (iii) and (vi) above can be deduced from Property 3.1 in Harris and Vickers (1987). Similarly, (iv) and (v) can be deduced from Property 3.2 in Harris and Vickers (1987). Note that (ii), (vii), and (viii) listed above are novel. On the other hand, our first result is in stark contrast with the equilibrium of tug-of-war with all-pay auction CSF (see Konrad and Kovenock, 2005). Also note that the positive equilibrium efforts in all interior states and the stochastic nature of the Tullock CSF imply that there will be swings back-and-forth (i.e., the advantage may change hands). Finally, (iii), (iv), and (v) show that a partial discouragement is still present.====This paper contributes to the theoretical literature on dynamic contests. Our main contribution is to give a closed-form solution for equilibrium efforts and values in tug-of-war with a lottery CSF and a quadratic cost function. To the best of our knowledge, ours is the first paper to do that. We deliver empirically appealing predictions on effort dynamics. Our results are also of interest from a design perspective: a contest-designer who values neck-to-neck competition or suspense (a desirable feature from audience’s perspective in sports competitions) should prefer lottery CSF to all-pay auction CSF in tug-of-war. Finally, we believe that our model will be of practical value for experimental economists studying dynamic contests due to the existence of pure strategy equilibrium (easier to interpret/identify empirically) and the rich set of testable hypotheses it produces.",Perseverance and suspense in tug-of-war,https://www.sciencedirect.com/science/article/pii/S0304406820301464,6 January 2021,2021,Research Article,139.0
Gibbard Peter,"Research School of Economics, Australian National University, Acton ACT 2600, Australia","Received 8 September 2020, Revised 20 November 2020, Accepted 21 December 2020, Available online 4 January 2021, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102468,Cited by (2),"This paper presents a model of choice with limited attention. The decision-maker forms a consideration set, from which she chooses her most preferred alternative. Both preferences and consideration sets are stochastic. While we present axiomatisations for this model, our focus is on the following identification question: to what extent can an observer retrieve ","When an observer relaxes the assumption that a decision-maker (DM) has full attention, she may be unable to infer the DM’s preferences from observed choices. To take a simple example, suppose that a DM faces a choice of whether to switch from her existing pension plan to a new plan. Given an assumption of full attention, the DM’s choice reveals her preference: the observer attributes to the DM a preference for her existing plan over the new plan if she sticks to her existing plan. Once the observer allows for the possibility of limited attention, however, such an inference may be unwarranted. There are two hypotheses consistent with an observation that the DM chose her existing plan: first, it may be that she prefers the existing plan; but second, it may be that she simply failed to ==== the new plan. That is, the new plan was not in her ====. This illustrates the identification problem that may arise in the presence of limited attention: an observer may be unable to disentangle the influence, on a DM’s choices, of preferences and limited attention.====In a consideration-set model, a DM’s decision has two stages. In the first stage, she forms a consideration set, which is the subset of the choice set that she has considered. In the second stage, she selects the most preferred alternative from those alternatives in her consideration set. Since the 1970s, consideration-set models have been used frequently in applied research – especially in the marketing literature – but, more recently, economic theorists have introduced consideration sets into axiomatisations of choice. In their seminal article, Masatlioglu et al. (2012) present an axiomatisation of a consideration-set model that highlights the identification problem which potentially arises in the presence of limited attention. They find that, even if a plausible restriction is imposed on the DM’s dispositions to form consideration sets, an observer generally will be unable to retrieve the entire preference relation from observed choices, and, indeed, may be unable to any information about preferences.====In contrast to Masatlioglu et al. (2012), we model the DM’s preferences and her dispositions to form consideration sets as stochastic: preferences are assumed to be generated by a probability distribution in a manner similar to that of random-utility models; further, a DM’s consideration set is generated by a probability distribution over the subsets of her choice set. The probability distributions over preferences and consideration sets determine observed choice probabilities. While we present an axiomatisation for our model, our particular focus is the identification question. We say that a model is “fully identified” if, from the observed choice probabilities, we can retrieve (i) the preference probabilities (to the same extent as in a full-attention random-utility model) and (ii) the consideration-set probabilities. Our first conclusion is a negative one: Section 4.2 shows that, even if substantial assumptions are imposed on consideration-set probabilities, typically, consideration-set models are not fully identified. That is, an identification problem arises akin to that highlighted by Masatlioglu et al. (2012) for the deterministic case.====In order to solve the identification problem, one strategy is to use an enriched dataset. Section 4 presents a model in which the choice data includes observations of choices under several different frames. We refer to this enriched dataset as a ====. The recent theoretical literature on behavioural decision theory includes numerous examples of frame-dependent choice functions. For instance, in Masatlioglu and Ok, 2005, Masatlioglu and Ok, 2014, the frame is the DM’s initial endowment; in their models, the frame-dependent choice function records the DM’s choices under different endowments. A ==== arises if a change of frame alters the DM’s choices. Our paper is concerned with one variety of framing effect, which we term an ====. An attention effect arises if a change in frame alters the DM’s choices by influencing her attention — by changing her consideration sets. For instance, Caplin and Dean (2011) present a model with a frame-dependent choice function where the frame is the time of contemplation prior to the decision. In their model, a change in the contemplation time potentially alters the DM’s choices by changing her consideration set. Attention effects may also include changes in attention arising from (a) moving a product to the lowest shelf in a grocery store (b) changing the rankings in a search engine for an online bookstore. In contrast to Caplin and Dean (2011), however, the model that we develop does not assume that the attention effect is of a specific kind — our models are applicable to any attention effect.====Our central contribution is a finding that, assuming that the observer has access to such an enriched dataset, then the identification problem can be solved. Section 5 shows that our model of limited attention model is fully identified. The intuition is as follows: An attention effect occurs when a change in attention causes a change in choice, ====. So the observed dataset records the influence of attention on choice, ====. This allows us to isolate the influence of attention on choice. The intuition can also be couched in mathematical terms. In our framework, identification is the task of solving a system of equations (the rationalisation conditions) to obtain the unknowns (preference probabilities and consideration-set probabilities). When we do not use frame-dependent choice functions, an identification problem arises because the number of unknowns typically exceeds the number of equations. When we introduce frame-dependent choice functions, the number of equations increases, which solves the identification problem.====As noted above, if we do not use the richer dataset described by a frame-dependent choice function, typically an identification problem arises. But how does this finding square with Manzini and Mariotti (2014), and the strand of literature that builds on their seminal paper, including Brady and Rehbeck (2016), Aguiar (2017) and Aguiar et al. (2019)? This strand of the literature develops stochastic consideration-set models that do not use frame-dependent choice functions; however, these models are fully identified. Section 4.2 argues that the identification results in these papers derive from a restrictive assumption about the default option: in particular, these papers assume that the default option is chosen ==== none of the non-default alternatives are considered. In general, this is a restrictive assumption. For instance, in modelling the choice between an existing pension plan and new plans, it would clearly be restrictive to assume that the DM never chooses the existing plan when she considers at least some new plan. To make such an assumption is, in effect, to assume that the default option is the worst alternative. This is a cause for concern because, as Section 4.2 shows, the ability to retrieve consideration set probabilities follows immediately from this restrictive assumption. Further, this restrictive assumption is typically not made in the empirical literature on consideration sets.====While our paper is concerned specifically with attentioneffects, the question it addresses relates to the more general challenge that behavioural economics has posed to neoclassical economics: do framing effects undermine the ability to retrieve revealed preferences and, therefore, undermine standard choice-based welfare economics? For instance, this challenge is explicitly issued in Ariely et al. (2003). Responding to this challenge, Salant and Rubinstein (2008) and Bernheim and Rangel (2009) present models with frame-dependent choice functions in which ranking relations can be retrieved from the choice data. The current paper can be interpreted as responding to the same challenge, but focussing on the case in which the framing effect is an attention affect. The idea is that preferences may be retrievable from choices, so long as we use an enriched dataset comprising a frame-dependent choice function.",Disentangling preferences and limited attention: Random-utility models with consideration sets,https://www.sciencedirect.com/science/article/pii/S0304406820301452,4 January 2021,2021,Research Article,140.0
Nieto-Barthaburu Augusto,"Instituto de Investigaciones Económicas, Universidad Nacional de Tucumán, Argentina","Received 10 March 2016, Revised 11 December 2020, Accepted 14 December 2020, Available online 29 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102466,Cited by (0),"We identify sufficient conditions for existence of competitive equilibrium with network externalities and indivisibilities. Such combination of externalities and indivisibilities is present in many goods and services with network effects, and it makes existence of equilibrium non-trivial. We provide an ==== in a model with a measure space of consumers. Key conditions for existence are anonymity of network effects and dispersion in the economy’s income distribution.","Network externalities (NE in what follows) arise when the satisfaction that a consumer obtains from the consumption of a given good or service depends, usually positively, on the number of consumers that consume the same good or service.==== Typical examples are telephone, e-mail and social media: the fact that a new individual uses the service enhances its usefulness for existing users, who can then interact with an additional member. Most of the theoretical literature on markets with NE is based on partial equilibrium models, and it studies questions of competition among networks and compatibility decisions of network goods’ providers.==== The present paper studies the problem of existence of general equilibrium with external network effects.====On the General Equilibrium side, Starr (1999) proposes a model in which network goods are produced by firms using a technology characterized by set-up costs, hence displaying economies of scale. Starr (1999) proves existence of an Average Cost Pricing Equilibrium in that economy. In the language of the literature on external effects, ==== studies equilibrium existence in the presence of “pecuniary externalities”. In contrast, we consider the problem of equilibrium existence with ==== NE, that is, when the consumption of network goods by an individual enters other individuals’ utility functions.====A usual feature of markets with NE is tipping behavior: the tendency for one of the competing goods or protocols to pull away from its competitors and win a substantial share of the market. Indeed, in virtually all papers in which there is tipping in equilibrium it is implicitly or explicitly assumed that there is an indivisibility intrinsic to the network goods. We do not formally address the question of whether indivisibilities are necessary for tipping behavior to arise. However, we informally argue that the occurrence of tipping is due not only to NE, which make it advantageous for consumers to join popular networks, but also to an indivisibility in consumption of network goods. Indivisibilities encourage consumers to coordinate, to avoid having to join multiple networks of the same type. Motivated by this informal argument, in this paper we study the problem of equilibrium existence with NE and indivisibilities combined.====The general problem of externalities has been previously studied in the General Equilibrium literature. Arrow and Hahn (1971) prove existence of equilibrium with externalities assuming a finite number of agents with convex preferences and no indivisibilities. More recently, Balder (2004) obtains existence results in a model of an exchange economy with a measure space of consumers and externalities, but also without indivisibilities. Balder (2004) is the first to use an “externality mapping” to model external effects by means of a continuous mapping of aggregate consumption, which enters consumers’ utility functions. Such mapping is necessary to avoid the potential technical problems for existence of equilibrium with extenalities and a measure space of consumers first pointed out by Balder (2000).==== In the current paper we use an externality mapping which is a special case of the class proposed by Balder (2004), and using that device we show existence of equilibrium in an economy with indivisibilities in consumption, NE, and in which network goods are produced by a set of firms with convex technologies.====Indivisibilities have also been studied in the General Equilibrium literature before. For example, Mas-Colell (1977) and Yamazaki (1978) obtain existence of equilibrium in economies with a non-atomic measure space of consumers and indivisibilities/non-convex preferences. Their key assumption to ensure existence of equilibrium is dispersion in the economy’s income distribution. These papers do not, however, allow for externalities.====Externalities combined with indivisibilities and/or non-convex preferences can pose problems for existence of competitive equilibrium. These problems were illustrated by Noguchi and Zame (2006), which by means of an example shows that an equilibrium may not exist with a pollution externality and non-convex preferences. A key feature of the example in ==== is that households are impacted by consumptions of different consumers differentially.==== In our paper we rule out such differential external effects by assuming that households only care about the configuration of ==== in the economy. That assumption is key to our results. Indeed, we show that together with the assumption on dispersion of the income distribution mentioned above, this anonymity assumption is sufficient to avoid the existence problems identified by Noguchi and Zame (2006).====The rest of the paper is organized as follows. In Section 2 we introduce the model and state our main result, the existence theorem. In Section 3 we present an example to illustrate the type of economy discussed in the paper, and construct an equilibrium in this economy. Section 4 contains the proof of the existence theorem. Section 5 concludes.",Competitive General Equilibrium with network externalities,https://www.sciencedirect.com/science/article/pii/S0304406820301439,29 December 2020,2020,Research Article,141.0
"Yang Zhe,Zhang Xian","School of Economics, Shanghai University of Finance and Economics, Shanghai 200433, China,Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai 200433, China","Received 7 October 2020, Revised 9 December 2020, Accepted 12 December 2020, Available online 19 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102464,Cited by (4),"Inspired by Kajii (1992) and Askoura (2011, 2017), we introduce the notion of the weak ====-core. Finally, we establish the relations between normal-form games, games with nonordered preferences and games with pseudo-utilities.","In this paper, we formulate the weak ====-core of games with nonordered preferences and a continuum of agents. Observe that the notion of the core comes from the cooperative game and general equilibrium theory. Scarf (1967) first proved the nonemptiness of the nontransferable utility (NTU) core for an NTU cooperative game. Following the work of Scarf (1967) and Predtetchinshi and Herings (2004) gave a necessary and sufficient condition for nonemptiness of the core for an NTU game. Moreover, in the Arrow–Debreu production economy, the core is nonempty, since every competitive equilibrium allocation belongs to the core. Aumann (1961) first introduced the notion of the ====-core for a normal-form game, where the behavior of agents has external effects. Following the notion of Aumann (1961) and Scarf (1971) proved the existence of the ====-core by the result of Scarf (1967), and Ichiishi (1981) established a social coalitional equilibrium existence theorem by combining the concepts of Nash equilibrium and the core. Later, by developing the proof technique of Ichiishi (1981), some generalizations of Scarf (1971) to games with nonordered preferences were given by Border (1984) and Kajii (1992). Moreover, Florenzano (1989) used a different proof method to prove the nonemptiness of the core for a coalitional production economy. The work of Florenzano (1989) was extended to the fuzzy core of a coalitional production economy by Florenzano (1990) and the private core of a coalitional production economy with incomplete information by Lefebvre (2001). Recently, the work of Scarf (1971) was extended to games and exchange economies with incomplete information by Askoura et al. (2013), Askoura (2015) and Noguchi, 2014, Noguchi, 2018.====For markets with a continuum of players, Aumann, 1964, Aumann, 1966 analyzed the competitive equilibrium, and Hildenbrand, 1968, Hildenbrand, 1974 considered the core and equilibria of a large economy. Moreover, Kannai (1970) investigated the continuity property of the core of a market with infinitely many agents. Later, Noguchi, 1997a, Noguchi, 1997b followed the work of Bewley (1972) to analyze the market with infinitely many players and commodities, and Noguchi (2000) gave a fuzzy core equivalence theorem. More work on markets with a continuum of agents can be seen in Suzuki (2009) and Liu, 2017a, Liu, 2017b.====For the cooperative game with infinitely many players, Ichiishi and Weber (1978) analyzed some theorems on the core of a non-sidement game with a measure space of agents. Following the notion of ====-core in Weber, 1979, Weber, 1981 defined the weak core of an NTU game with a continuum of players, and proved the existence theorem. Moreover, Weber (1981) provided an example satisfying the conditions of the weak core existence theorem, but the core is empty. More work on ====-core can refer to Wooders (1983) and Wooders and Zame (1984).====Inspired by Weber (1981) and Askoura (2011) first introduced the notion of the weak core for a normal-form game with a continuum of players, and proved the nonemptiness theorem. Later, Askoura (2017) improved the result of Askoura (2011) by defining the equi-usc condition of functions. Observe that the set of agents is assumed to be a measure space in Askoura, 2011, Askoura, 2017. Unlike Askoura, 2011, Askoura, 2017 and Yang (2017) assumed that the set of agents is a compact Hausdorff topological space, and generalized the work of Scarf (1971) to a normal-form game with infinitely many players. Following the idea of Yang, 2017, Yang, 2018 provided a generalization of Kajii (1992) to games with infinitely many players. Inspired by Yang, 2017, Yang, 2018 and Zhao, 1992, Zhao, 1996 and Yang and Yuan (2019) proved the existence of weak hybrid solutions for games with nonordered preferences, infinitely many players and a partition of the set of agents. Recently, by improving the proof technique and using the social coalitional equilibrium existence theorem of Ichiishi (1981) and Yang (2020) analyzed the weak ====-core in an exchange economy with a continuum of consumers and pseudo-utilities.====In our paper, we follow the work of Askoura, 2011, Askoura, 2017. By assuming that the set of agents is a measure space, we extend the work of Kajii (1992) to games with a continuum of agents. We shall prove the existence of the weak ====-core in our model. Finally, we establish the relations between normal-form games, games with nonordered preferences and games with pseudo-utilities.====The rest of the paper is organized as follows. In Section 2, we extend the result of Kajii (1992) to games with spaces of strategies defined on Hausdorff topological vector spaces. Section 3 is the main result, and Section 4 is the conclusion.",A weak ,https://www.sciencedirect.com/science/article/pii/S0304406820301415,19 December 2020,2020,Research Article,142.0
Casajus André,"HHL Leipzig Graduate School of Management, Jahnallee 59, 04109 Leipzig, Germany,Dr. Hops Craft Beer Bar, Eichendorffstr. 7, 04277 Leipzig, Germany","Received 22 July 2020, Revised 30 October 2020, Accepted 4 December 2020, Available online 18 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102459,Cited by (2), one player gains or loses when another player leaves the game equals the ==== the latter player gains or loses when the former player leaves the game. Weakly balanced contributions: the ==== (sign) of the change of one player’s payoff when another player leaves the game equals the ,"The symmetric Shapley value (Shapley, 1953b) probably is the most eminent one-point solution concept for cooperative games with transferable utility (TU games or simply games). Besides its original axiomatic foundation by Shapley himself, alternative foundations of different types have been suggested later on. Important direct axiomatic characterizations are due to Myerson (1980) and Young (1985).====In order to account for asymmetries among players beyond the game itself, Shapley (1953a) already suggests weighted versions of his symmetric value, where these asymmetries are modelled by strictly positive weights for the players—the positively weighted Shapley values.==== There exist a number of axiomatic foundations for the whole class of positively weighted Shapley values (see, e.g., Kalai and Samet, 1987, Hart and Mas-Colell, 1989, Chun, 1991, Nowak and Radzik, 1995, Casajus, 2018, Casajus, 2019, Besner, 2020).====Myerson’s (1980) characterization of the symmetric Shapley value involves only two properties, efficiency and the balanced contributions property. Efficiency: the worth of the grand coalition is distributed among the players. Balanced contributions: ==== one player gains or loses when another player leaves the game equals ==== the latter player gains or loses when the former player leaves the game. Since the positively weighted Shapley values satisfy efficiency, they fail the balanced contributions property with exception of the symmetric==== Shapley value. Instead, they satisfy the weak balanced contributions property: ==== (sign) of the change of one player’s payoff when another player leaves the game equals ==== (sign) of the change of the latter player’s payoff when the former player leaves the game (Casajus, 2017a).====Casajus (2017a, Lemma 1) already establishes some joint implications of the weak balanced contributions property and efficiency. In particular, he shows that these properties imply the dummy player property and the dummy player out property. Dummy player: all her marginal contributions to coalitions not containing her coincide with her singleton worth. Dummy player property: a dummy player’s payoff equals her singleton worth. Dummy player out property: removing a dummy player from a game does not affect the remaining players’ payoffs. As our first result, we establish another joint implication of the weak balanced contributions property and efficiency (Proposition 1). In particular, they imply strong positivity: in any monotonic game, all payoffs are non-negative. In addition, a player’s payoff is zero if and only if she is a null player. Monotonic game: all marginal contributions are non-negative. Null player: a dummy player with zero singleton worth.====There exists a large class of solutions that satisfy efficiency and the weak balanced contributions property, among them not only the positively weighted Shapley values but a huge number of utterly implausible solutions (Casajus, 2017b). In particular, the latter solutions fail marginality due to Young (1985). Marginality: a player’s payoff only depends on her ==== marginal contributions. As our main result, we show that the positively weighted Shapley values are exactly those solutions that satisfy efficiency, the weak balanced contributions property, and marginality (Theorem 3). Given this characterization, the symmetric Shapley value can be “extracted” from the class of positively weighted Shapley values by either replacing the weak balanced contributions property with the standard symmetry property (Young, 1985) or strengthening the weak balanced contributions property into the balanced contributions property (Myerson, 1980). Whereas the former results in a non-redundant characterization of the symmetric Shapley value, the latter “extracts” the symmetric Shapley value by strengthening one property.====The remainder of this paper is organized as follows: In Section 2, we provide basic definitions and notation. In Section 3, we discuss the weak balanced contributions property. In Section 4, we provide our characterization of the class of positively weighted Shapley values. In Section 5, we conclude our paper with a comparison of our characterization with the recent characterization suggested by Casajus (2018). An appendix contains the proof of our main result.",Weakly balanced contributions and the weighted Shapley values,https://www.sciencedirect.com/science/article/pii/S0304406820301361,18 December 2020,2020,Research Article,143.0
"Aguiar Victor H.,Serrano Roberto","Department of Economics, The University of Western Ontario, Canada,Department of Economics, Brown University, United States of America","Received 12 September 2019, Revised 25 November 2020, Accepted 7 December 2020, Available online 17 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102462,Cited by (1),"We present a new notion of cardinal revealed preference that exploits the expenditure information in classical ==== environments with finite data. We propose a new behavioral axiom, Acyclic Enticement (AE), that requires the acyclicity of the cardinal revealed-preference relation. AE is logically independent from the Weak Axiom of Revealed Preference (WARP). We show that the Generalized Axiom of Revealed Preference (GARP), which characterizes the standard rational consumer, is logically equivalent to AE and WARP. We use our axiomatic decomposition to show, in experimental and scanner consumer-panel data sets, that AE explains the majority of the predictive success of GARP. Moreover, AE taken alone is superior in predictive success to both WARP and GARP.","Since the ground-breaking works of Afriat (1967) and Varian (1983), it is known that the rationalization of a list of price-demand observations by means of the maximization of a utility function subject to a linear budget constraint is equivalent to the Generalized Axiom of Revealed Preference (GARP).==== We say that ==== is revealed preferred to ==== whenever ==== is chosen when ==== is affordable at prices ====. This revealed-preference relation, due to Samuelson (1948), uses only ordinal information (i.e., it relies only on the fact that one commodity bundle is selected over another), and GARP requires that the relation be acyclic.====This result has allowed practitioners of revealed-preference analysis to test the null hypothesis of GARP-consistency. Indeed, the exercise has been performed in experimental budget allocation data sets,==== household consumption survey data,==== and scanner consumption panels.==== The empirical success of GARP in these different environments is usually limited, as quantified by predictive power measures such as those proposed by Beatty and Crawford (2011). In particular, the pass-rate of GARP is usually very small for high-powered environments (for detecting model inconsistencies) with substantial price variation (e.g., experimental and scanner data sets).====In the face of these facts, a natural question to ask is:====We propose two weaker and logically-independent consistency conditions that, taken together, are equivalent to GARP. The first condition is new: we call it ==== (AE), which underlies our main contribution; and the second one is the traditional ==== (WARP). From our characterization, it is striking that a nonrational consumer who obeys WARP must necessarily violate AE. While new axioms are sometimes justified on normative grounds, we see AE as valuable as part of a new and transparent characterization of rationality. Moreover, AE is empirically more successful than GARP.====We define the ==== (AE) condition as the requirement that a cardinal revealed-preference relation be acyclic. We use the (cardinal) expenditure information available in a traditional consumer environment. In particular, we define the expenditure premium of ==== over ==== under prices ==== as the additional amount of dollars that the consumer spends on ==== at prices ==== when ==== was affordable. We say that a bundle ==== is cardinally revealed preferred to ==== whenever (i) ==== is chosen when ==== is affordable and (ii) the expenditure premium of ==== over ==== under prices ==== is not smaller than the expenditure premium of ==== over ==== under prices ====.====The second weaker condition is the well-known ==== (WARP). WARP turns out to have an empirical performance similar to GARP. In fact, recent work by Cherchye et al. (2017) provides necessary and sufficient conditions on price variation under which WARP is equivalent to both GARP and the strong axiom of revealed preference (SARP). Empirically, survey data price variation usually satisfies these conditions (Cherchye et al., 2017), which means that in this type of data set, WARP and GARP are indistinguishable. In our application, we find in experimental and scanner data sets that WARP has low empirical success, and its performance is relatively equivalent to that of GARP.==== This finding supports the need for different primitive conditions from WARP, such as AE.====Thus, our results show that standard utility maximization is in the intersection of two classes of models, i.e., those that rely on transitivity of a (cardinal) revealed-preference and those that rely on pairwise consistent choices. This provides a fresh way to view the rationality paradigm. Pairwise consistency alone imposes enough discipline on transitive revealed preferences – or vice versa – to restore rational choice.====The only previous work we are aware of that has broken down GARP (or SARP) into more primitive conditions has done it exclusively in infinite data sets. Hurwicz and Richter (1979) shows that for demand functions (infinite data sets) SARP is equivalent to both WARP and an axiom called the Ville Axiom of Revealed Preference (VARP). VARP rules out the existence of a differential version of revealed-demand cycles.==== Moreover, VARP is logically independent from WARP, but it is not testable with finite data sets. In addition, due to the differential nature of VARP, it is not easily comparable with our AE condition. Nonetheless, VARP is implied by the strengthening of AE in the infinite data sets section. However, we show that demand functions that are consistent with VARP can generate finite consumption data sets that fail AE.====Section 2 establishes the axiomatic decomposition of GARP into AE and WARP. Section 3 extends the analysis to infinite data sets and provides an extension of AE equivalent to VARP. Section 4 presents an empirical application of our new axiom AE, along with WARP and GARP to both experimental and scanner consumer-panel data sets. Section 5 proposes an additive decomposition of the predictive success of GARP into the marginal contributions of AE and WARP; in the same experimental and scanner data sets, we find that AE explains the majority of the empirical success of GARP. Section 6 expands on the formal relation of the new AE condition with numerous models presented in others’ previous work. Finally, Section 7 concludes. The proofs not included in the main text are collected in an Appendix.",Cardinal revealed preference: Disentangling transitivity and consistent binary choice,https://www.sciencedirect.com/science/article/pii/S0304406820301397,17 December 2020,2020,Research Article,144.0
"Fabbri Giorgio,Gozzi Fausto,Zanco Giovanni","Univ. Grenoble Alpes, CNRS, INRA, Grenoble INP, GAEL, Grenoble, France,Department of Economics and Finance, LUISS University, Rome, Italy","Received 17 August 2020, Revised 12 November 2020, Accepted 26 November 2020, Available online 17 December 2020, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2020.102455,Cited by (12),In this paper we propose a macro-dynamic age-structured set-up for the analysis of epidemics/economic dynamics in continuous time.====The resulting ====Our main result is a verification theorem which allows to guess the feedback form of ====. This will be a departure point to discuss the behavior of the models of the family we introduce and their policy implications.,"The outbreak of the COVID-19 pandemic represents, in addition to an epidemiological historical event, an exceptional economic shock. Data from the ==== suggest that in many countries the loss of GDP due to the presence of the virus and the consequent containment measures will be at least 10%. For this reason, together with the obvious upsurge in medical scientific production on the subject, the phenomenon has had great echo in the economic literature with a strong pressure to merge economic and epidemiological models.====, ====, ==== and ====.====These articles focus on a series of questions essential to health and economic policy and they look, often numerically, at the trade-off between measures capable of containing contagion and those capable of avoiding economic collapse. However, they model the spread of the epidemic with age homogeneous epidemiological compartmental models so they cannot take into account one of the characteristic traits of the current epidemic, i.e. the great difference in the effects of the disease among people of different ages.====In order to address this limitation ====, ==== and ==== introduce models where the population is divided into a finite number of homogeneous “risk groups” and they study joint economic and epidemiological effects of introducing group-specific policies. Nonetheless in their formulations there is no possibility to move from one group to another and then this kind of approach can take into account the different effects of the disease on different age groups only if it is assumed that the duration of the epidemic is negligible compared to the age range contained in each group. However, this hypothesis is not very likely in the case of an epidemic lasting several years and it is inadequate in the case of diseases that become endemic in the population.====Instead of using age-homogeneous epidemiological compartmental models or epidemiological compartmental models with closed risk groups, it is possible, as we do in the present work, to describe more accurately the joint dynamics of the epidemic and of the age structure of the population by using explicit age-structured compartmental models, i.e. age-specific epidemiological models with aging process modeled ==== ====. This type of models was initially introduced by ==== and ==== and later adapted to numerous contexts and applications, see the books by ====, ==== and ==== for a structured and modern description of the matter.====, its dynamics needs to be formulated in an infinite-dimensional set-up.====In this, paper whose main aim is methodological, we initially present a class of macro-dynamic models (partly already introduced in the literature) that incorporates an epidemiological dynamics which generalizes the benchmark age-structured SIR model. Then we provide a general framework to study such optimal control problems through the dynamic programming approach; finally we present verification type results that hold for the whole class of problems.====The class of models that we study in the abstract form is rather general and is able, in the context of the epidemiological dynamic described by an age-structured SIR, to reproduce as special cases several of the settings proposed by the recent articles mentioned above (more details can be found in Section ====). Specific traits of the model are:====Since our results are proven for the abstract model they hold for any possible specification. Somehow related papers are ==== and ==== (which also contains a spatial spread modeling of the virus diffusion) where the authors present models with a complete age structure but with a simple cost structure and no factor accumulation.====, ====, ====, ====, ====, ====, ====, ====, ====, ====, ====.====The contribution of this work is (i) to propose a general fully age-structured macro-dynamic set-up in continuous time for analysis of epidemics and economic dynamic (Section ====); (iii) to prove verification type results (Section ====), see in particular ==== and ====, ====.====We must be clear on the fact that we do not solve the problem explicitly, nor numerically. Here our main goal is to provide a general ground which can be the departure point to attack special cases of our general model. In particular our main contribution is the proof of the verification type results of Section ====, ====) or of book chapters (e.g. Chapter 5 of ==== or Chapter 4 of ==== and that we treat using ====, ====).====The paper is organized as follows. In Section ==== we introduce the structure of the model: epidemiological dynamics, policies, structure of the economy and welfare functional. In Section ==== we show how to reformulate the model and the related optimal control problem in a suitable Hilbert space setting. Section ==== is devoted to dynamic programming while in Section ==== we provide the verification results. Section ==== concludes.",Verification results for age-structured models of economic–epidemics dynamics,https://www.sciencedirect.com/science/article/pii/S0304406820301324,17 December 2020,2020,Research Article,145.0
Toda Alexis Akira,"Department of Economics, University of California San Diego, 9500 Gilman Dr, La Jolla, CA 92093, USA","Received 8 November 2020, Accepted 7 December 2020, Available online 17 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102460,Cited by (9),"Carroll and Kimball (1996) have shown that, in the class of utility functions that are strictly increasing, strictly concave, and have nonnegative third derivatives, hyperbolic absolute ==== (HARA) is sufficient for the ==== of consumption functions in general consumption-saving problems. This paper shows that HARA is necessary, implying the ==== of consumption is not a robust prediction outside the HARA class.","The notion that the marginal propensity to consume decreases with wealth, or that the consumption function is concave, dates back at least to Keynes (1936). This observation is important in macroeconomics because the effect of a fiscal transfer of one dollar to a wealthy household is smaller than that to a poor household, implying that fiscal policies need to account for household heterogeneity. In an important contribution, Carroll and Kimball (1996) have shown that in the class of utility functions that are strictly increasing, are strictly concave, and have nonnegative third derivatives, hyperbolic absolute risk aversion (HARA) is sufficient for the concavity of consumption functions in finite-horizon consumption-saving problems without liquidity constraints. Their result has been extended in several directions:Carroll and Kimball (2001, Section 5) obtain concavity under finite-horizon, HARA utility, and liquidity constraints; Nishiyama and Kato (2012) obtain concavity under infinite-horizon, quadratic utility, and liquidity constraints; Ma et al. (2020, Proposition 2.5 and Remark 2.1) obtain concavity under infinite horizon, constant relative risk aversion (CRRA) utility, and liquidity constraints.====In all of these theoretical papers, the utility function is restricted to be HARA. Thus a natural question is whether it is possible to obtain the concavity of consumption functions under weaker assumptions: does concavity hold in a larger class than HARA, or is concavity a non-robust prediction that fails to hold outside the HARA class? This paper provides a definitive answer to this question, which is the latter. More precisely, I show that if the utility function is strictly increasing, is strictly concave, has a positive third derivative, but is not HARA, then there exists a finite-horizon (in fact, one period) consumption-saving problem such that the consumption function is not concave. Combined with the earlier results on the concavity of consumption functions just cited, my result shows that in the class of natural utility functions, HARA is both necessary and sufficient for the concavity of consumption functions in general consumption-saving problems.",Necessity of hyperbolic absolute risk aversion for the concavity of consumption functions,https://www.sciencedirect.com/science/article/pii/S0304406820301373,17 December 2020,2020,Research Article,146.0
Olszewski Wojciech,"Department of Economics, Northwestern University, United States of America","Received 18 June 2020, Revised 11 October 2020, Accepted 7 December 2020, Available online 17 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102461,Cited by (0),"The standard decision-theoretic model separates information and preferences. We argue that some behavioral phenomena in information processing (e.g., polarization) are closely related to this separation.====We propose a model in which agents receive vague information, which they refine when they are required to make probabilistic judgments, or to take an action. By doing so, they also refine their beliefs about some traits or states (e.g., their information-processing ability), which affects their utility.","In economics, we separate preferences and information. Agents’ preferences over consumption bundles are independent of their information, and their information is represented by a probability distribution over the state space. Any newly arrived information makes the agents update this probability distribution, what they do by using the Bayes rule. Although this approach is very convenient for conducting analysis, the separation was always somewhat controversial, and was the subject of some historically famous discussions.==== In practice, some settings make it difficult to distinguish preferences from information. For example, when people estimate the probability of survival of loved ones who are seriously ill, it is not clear whether they are conveying information or express preferences.====The social science literature has documented numerous instances of behavior inconsistent with Bayesian information processing, under given preferences. One example of such a phenomenon is polarization. The aim of our paper is to argue that the puzzle of polarization, and other puzzling behavioral phenomena in information processing, may be related to the separation of preferences and information.====More specifically, we suggest a model based on two observations: (1) in practice, people often receive vague signals, which leave room for interpretation; and (2) they interpret vague signals in the way which is most convenient to them, in the way which leads to the most desired beliefs about some traits or states they care about. That is, this interpretation of vague signals is not performed as separate from preferences.====We focus on one particular trait, namely, information-processing ability. In the psychology literature, this is related to self-enhancement theory (see Greenwald, 1980). According to self-enhancement theory, people have a desire to see themselves favorably as competent human beings; this increases their feelings of personal satisfaction and worth. Therefore, they distort the way they process information so as to interpret it in a way that supports a positive self-image. However, we wish to emphasize that many other traits and states also affect their information processing (e.g., people may wish to believe that their loved ones will live a long life, or that God exists).====We assume that agents obtain information as possibly vague, exogenous signals, modeled as sets of probability distributions. When agents interpret vague information, they refine their beliefs, possibly to a single probability distribution. And they can maintain or enhance their self-esteem by choosing an interpretation, or refining their beliefs, in a way that accomplishes this goal. However, this choice of beliefs and self-esteem is limited. First, it is limited to situations in which information is vague. In particular, when the information consists of a single probability distribution, agents’ beliefs are determined by the information they obtain, exactly as under the traditional Bayesian updating. In addition, the refinement of beliefs must be consistent with their actions, so it affects physical payoffs. This imposes some endogenous limits on information processing, as a trade-off between the physical payoffs and the self-esteem payoffs arising from the consequences of their actions.====We demonstrate that our theory generates what we call a ====. This is information processing biased toward prior views, driven by a preference for a high assessment of one’s own ability of information processing. This effect is obviously a potential source of polarization. We also demonstrate that our theory generates another effect, and a perverse one, which we call a ====. This is information processing biased against literal meaning of signals. As an example of comparative statics that can be performed within our model, we explore the effect of increased vagueness of information on the strength of these biases. We also discuss how the information contained in earlier signals affects the strength of these biases.====The rest of the paper is structured as follows. Below we discuss the relationship of this paper to the existing literature. Section 2 presents the main idea in the context of polarization, including its implications, and compares our explanation with existing ones. Section 3 provides a more general model. Section 4 describes the two types of biases which are captured by a basic version of our model, and explores the impact of an increase in the vagueness of information on the size of the two biases. Section 4 also delivers other comparative statics results. Section 5 discusses the implications suggested by our approach regarding biases other than polarization, and Section 6 concludes.====Some elements of our model are familiar from earlier papers. The use of sets, as opposed to single objects, as a representation of the vagueness of information dates from Arrow and Hurwicz (1972). More recent papers by Ahn (2008), Olszewski (2007) and Vierø (2009) recommend using sets of probability distributions to model ambiguity or vagueness of information.====Our paper is not the first which introduces self-assessment into utility function. Zábojník (2004) and Köszegi (2010) explore models in which agents’ utility is a function of self-assessment. The maintenance and enhancement of self-esteem is also emphasized in Bénabou and Tirole (2002) and Compte and Postlewaite (2004).====Finally, Brunnermeier and Parker (2005) provide a model in which agents have freedom in choosing their beliefs. In their paper, agents first choose their beliefs and then, taking their beliefs as given, they choose actions to maximize their utility.",Preferences and information processing under vague information,https://www.sciencedirect.com/science/article/pii/S0304406820301385,17 December 2020,2020,Research Article,147.0
"Askoura Youcef,Billot Antoine","Lemma, Université Panthéon-Assas (Paris 2), France","Received 28 October 2019, Revised 5 December 2020, Accepted 10 December 2020, Available online 17 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102463,Cited by (1),Preference aggregation is here investigated for a society defined as a measure space of individuals and called a ====. Individual preferences are represented through continuous v,"Harsanyi’s utilitarian theorem (1955) provides an axiomatic justification of the utilitarian rule when it is assumed that both individuals and society conform to the von Neumann and Morgenstern (v====) behavior model of decision under risk—====, when all individuals share the same prior.==== This theorem is based on a condition of Pareto indifference reflecting the fact that a pair of alternatives should be socially indifferent whenever every individual is indifferent between them. As a result, the social utility function is proved to be a weighted sum of individual utility functions. A lot of papers supply various proofs of this theorem (Domotor, 1979, Fishburn, 1984, Weymark, 1993, Mongin, 1995) while others (Gilboa et al., 2004, Alon and Gayer, 2016, Billot and Qu, 2020) provide extensions to the case of prior heterogeneity under variants of the Pareto condition (nobetting dominance, lottery or likelihood Pareto, beliefproof...). In any case, whether one considers homogeneous or heterogeneous beliefs, all models of preference aggregation always start with the explicit definition of a ====, that is the space of individuals. Insofar as a society or a social planner does not distinguish between any individual and her preference relation, aggregation theory does not make any conceptual difference between the set of preferences, the set of individuals and society itself. When the latter is finite, it is then defined as the set ====, for some positive integer ====. Note that most of the results in the contributions mentioned above require society to be finite.====Conversely to this approach, the model that we present here introduces the possibility for a society to be made of a measure space of individuals. It is based on the theoretic representation of an infinite set of agents as proposed by Aumann (1964). There are various motivations for such an assumption, from recent political conception of democracy (Vallier, 2020) to standard analysis of intergenerational social choice with infinite time horizon (Zame, 2007). In such cases, however, most of the results for preference aggregation consist of impossibility results. These impossibilities actually reflect the existence of a conflict between the Pareto condition and the common formalizations of anonymity or impartiality in infinite societies (Basu and Mitra, 2007). In this paper, we precisely aim to overcome this conflict and, consequently, to provide positive results for preference aggregation with a measure space of individuals.====Since Harsanyi’s seminal contribution, aside from the formal structure of society, the main ingredient for preference aggregation is the Pareto condition. This condition is rooted in the idea that individual utilities are defined as arguments for the Bergson–Samuelson social welfare function. Harsanyi’s utilitarian theorem establishes that, under Pareto indifference – ====, the fact that when individuals are unanimously indifferent between two options, society is also indifferent between them – the social welfare function is the weighted sum of individual utilities if society maximizes expected social welfare. Domotor (1979) shows that a strengthening of Pareto indifference to strong Pareto allows to aggregate individual utilities by means of a combination of ==== weights. Strong Pareto requires that an alternative ==== should be socially strictly preferred to another alternative ==== whenever every individual weakly prefers ==== to ==== and at least one person strictly prefers ==== to ====. Pareto indifference is certainly well-known in the social choice literature considering interpersonal utility comparisons. However, Arrow (1963) has changed the underlying perspective of the Pareto conditions within a social setting in proposing to use weak Pareto instead of Pareto indifference. Weak Pareto requires that an alternative ==== should be socially strictly preferred to another alternative ==== whenever every individual strictly prefers ==== to ====. This way, under weak Pareto, Harsanyi’s contribution is directly connected to the Arrovian tradition.====There is a small literature dedicated to preference aggregation for infinite measure societies. In this particular setting, Zhou (1997) considers a society endowed with a ====-algebra and a measure while the space of alternatives is assumed to be a space of lotteries defined on a finite set. The aggregation is performed through an adapted Pareto axiom. Analogous results are obtained for the ordinal aggregation case by Candeal et al. (1997).====The present paper goes deeper into the topic already addressed in Askoura and Billot (2018). Here, we describe society as a measure space of individuals—while Askoura and Billot (2018) just consider the case of an infinite but not measure society. We investigate Harsanyian utilitarianism for finite and infinite measure societies in considering continuous v==== utilities and deducing technical aggregation results as consequences of Hahn–Banach and Klee’s separation theorems. A more detailed comparison with Zhou’s results is established in Section 4.====This paper is organized as follows: in Section 2, we present a general set-up for a measure society and in Section 3, we prove several aggregation results for measure societies according to adapted Pareto conditions. In Section 4, finally, we comment the scope of the results and make connections with the literature. All proofs are brought together in the Appendix.",Social decision for a measure society,https://www.sciencedirect.com/science/article/pii/S0304406820301403,17 December 2020,2020,Research Article,148.0
"Belyakov A.O.,Kurbatskiy A.N.,Prettner K.","Central Economic Mathematical Institute of the Russian Academy of Sciences, Russia,Moscow School of Economics, Lomonosov Moscow State University, Russia,Vienna University of Economics and Business, Austria,Wittgenstein Centre for Demography and Global Human Capital (IIASA, OeAW, University of Vienna), Austria","Received 21 July 2020, Revised 2 November 2020, Accepted 29 November 2020, Available online 17 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102457,Cited by (0)," and raises economic growth through an increase in R&D incentives. Irrespective of the anticipation effect, the economic change at impact is not smooth but still features a kink in consumption.","One of the dominant economic challenges in industrialized countries over the coming decades is population aging triggered mainly by declining fertility and increasing life expectancy (Gruescu, 2007, Bloom et al., 2003, Bloom et al., 2007, Bloom et al., 2010). There are many concerns regarding the economic effects of aging that are related to (i) the sustainability of social security systems (Gruber and Wise, 1998, Attanasio et al., 2007, İmrohoroğlu and Kitao, 2012), (ii) the question of whether a declining workforce will be able to produce the goods and services that a growing number of dependents will consume (Gertler, 1999, Prettner et al., 2013), (iii) potentially declining average productivity levels of aging societies (Skirbekk, 2008, Bloom and Sousa-Poza, 2013, Mahlberg et al., 2013), and (iv) reductions in asset prices when retiring older workers sell their stocks (Mankiw and Weil, 1989, Schich, 2008). Irrespective of these concerns, at least up to now, a strong negative effect of aging on economic performance has not materialized (Acemoglu and Restrepo, 2017). This is widely regarded to be a consequence of the fact that aging societies have kept investing in new technologies that raised productivity, partly in anticipation of the expected economic challenges of the demographic changes (Irmen and Litina, 2016, Abeliansky and Prettner, 2017, Acemoglu and Restrepo, 2018). The corresponding theoretical analyses are typically carried out in endogenous and semi-endogenous growth models in which purposeful investments in R&D determine technological progress and thereby productivity growth (Prettner, 2013, Prettner and Trimborn, 2017, Baldanzi et al., 2019, Gehringer and Prettner, 2019).====Within this model class, increasing life expectancy fosters long-run growth. The reason is that individuals who live longer are more inclined to save and to invest. This raises the physical capital stock, which, in turn, puts downward pressure on the market interest rate. Since the future returns on innovation are discounted with the market interest rate, this raises the present value of a commercially successful R&D project and thus renders investments in R&D more attractive. As a result, resources are shifted to R&D, which raises technological progress and productivity growth (Prettner, 2013, Prettner and Trimborn, 2017, Gehringer and Prettner, 2019).====The described models consider the reactions of economic agents to ==== changes in demographic parameters, such as mortality and fertility, and either only their effect on the log-run balanced growth path (Prettner, 2013, Gehringer and Prettner, 2019) or on the convergence toward a new balanced growth path that follows the shock (Prettner and Trimborn, 2017). We aim to contribute to the literature by analyzing the effects of ==== population aging in this context. The effects of anticipation are particularly important because agents in many countries may predict demographic changes by observing the situation in forerunner countries of population aging – such as Japan – and adjust their consumption/saving decisions well in advance.====In order to describe such anticipated behavior one needs a model with rational expectations and a predicted demographic transition. To this end, we extend the overlapping generations (OLG) model of Prettner (2013) that is based on an R&D-driven growth model of Romer (1990)==== to account for anticipated changes of mortality and fertility. In our analysis, we first derive the balanced growth path analytically as the only feasible stationary solution. Then we utilize the central strength of the model – that it allows the numerical analysis of anticipated changes in mortality and fertility – to analyze the question of whether a drop of mortality and fertility that is anticipated leads to a higher per capita GDP in the long run than in the unanticipated case. We consider the Romer (1990) benchmark case in which population growth is zero such that fertility adjusts to equate mortality, ====. In doing so we sterilize the scale effect to get a meaningful description of the channels by which the reactions to the demographic changes unfold. To show how our results change when we follow the semi-endogenous growth model of Jones (1995) and allow for population growth, we provide a corresponding extension of the model.====The paper is organized as follows. In Section 2 we present the model and its building blocks, the basic assumptions on which it relies, and the aggregation rules that are based on the demographic properties of the household side. In Section 3, we derive the equilibrium solution and the balanced growth path of the model. Section 4 contains the central results on the growth effects of anticipated versus unanticipated aging. Section 5 discusses the robustness of our results with respect to different underlying modeling assumptions. Finally, in Section 6, we summarize, draw our conclusions, and sketch some skope for policymakers.",The growth effects of anticipated versus unanticipated population aging,https://www.sciencedirect.com/science/article/pii/S0304406820301348,17 December 2020,2020,Research Article,149.0
Wei Dong,"University of California, Berkeley, United States of America","Received 20 June 2020, Revised 11 October 2020, Accepted 13 November 2020, Available online 16 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.11.007,Cited by (4),"A Sender (seller) tries to persuade a rationally inattentive Receiver (buyer) to take a particular action (e.g., buying). Learning is costly for the Receiver who can choose to process strictly less information than what the sender provides. In a binary-action binary-state model, we show that optimal disclosure involves information distortion, but to a lesser extent than the case without learning costs; meanwhile, the Receiver processes less information than what he would under full disclosure. We also find that the Receiver can leverage his potential inattention to attain a higher equilibrium payoff than the perfectly attentive case. While the Sender is always worse off when facing a less attentive Receiver, the amount of information processed in equilibrium varies with learning costs in a non-monotone fashion.","Learning or information processing can be costly. Due to such costs, a decision maker may not want to take in all available information. For example, teaching a pupil quantum physics can hardly improve his/her understanding of the physical world, just as an adult may learn less from a book, an email, or a contract that contains too much detail. As Simon, 1971, Simon, 1996 puts it, “What information consumes is rather obvious: it consumes the attention of its recipients.…==== ====The real design problem is not to provide more information to people . . . but [to design] intelligent information-filtering systems”.====The burgeoning literature on persuasion games following Kamenica and Gentzkow (2011) studies the question of how to design optimal information-filtering systems. However, existing models almost always assume the information receiver to be a passive learner: he automatically processes whatever information that is revealed by the sender. Consequently, the design is driven not by learning costs or inattention, but by preference misalignments between the sender and the receiver.====How does inattention affect information disclosure and learning? Are features of optimal disclosure in standard models robust to inattention? To shed light on these questions, we study the information-filtering problem of a seller aimed at maximizing the purchase probability of a rationally inattentive buyer who finds it costly to learn. In our model, a Sender (seller) reveals information about an uncertain state of world (quality of the good) to persuade a Receiver (buyer) to take a particular action (buying). Processing information is costly for the Receiver, and he can learn strictly less information, in the sense of Blackwell, than what the Sender provides. Taking the Receiver’s optimal learning into account, the Sender designs an information disclosure policy aiming to maximize the probability of the Receiver taking such an action. Other applications of the model include a biased advisor persuading a rationally inattentive politician to vote for a reform, a prosecutor persuading a jury to convict a defendant, or a pharmaceutical company convincing FDA to approve a new drug.====We study this problem in a binary-action binary-state environment, where we can explicitly characterize incentive compatible information policies, that is, those to which the buyer willingly pays full attention. To get a flavor of optimal disclosure, suppose that the buyer’s optimal action absent any information is not buying. If information is ====, optimal disclosure is a recommendation rule that recommends the “buy” action whenever the quality is high and sometimes when the quality is low. The “buy” recommendation is obfuscated in such a way that the buyer finds it indifferent between buying and not buying upon receiving it. The said indifference is a common feature of optimal disclosure in standard persuasion models,==== but it no longer holds in our setting. Specifically, if learning is ====, such a disclosure rule becomes one of the worst because it fails to induce any attention from the buyer. Indeed, utility-wise the buyer does not benefit from paying any attention to such information, and thus he will ignore all of it without buying the good. To attract some attention, the seller now has to obfuscate the “buy” recommendation less often to increase the instrumental value of the available information. The seller does so by recommending the “buy” action less frequently when the quality is low, and as a result, the buyer will ==== prefer to buy when such a recommendation is made. Nevertheless, full disclosure (i.e., no obfuscation at all) is also suboptimal. Intuitively, starting from the buyer’s best response to full disclosure, obfuscating the “buy” recommendation a little more has a first-order positive effect on the selling probability, while its (negative) effect through incentive compatibility is negligible due to an envelope-theorem argument. Indeed, optimal disclosure still involves information distortion, and in equilibrium the buyer will learn strictly less than what he would under full disclosure.====We provide a set of comparative statics results with respect to learning costs. First, we show that the buyer can sometimes benefit from having a higher cost of learning. Intuitively, being too attentive induces the seller to push information obfuscation to its limit, which leads to very little useful information about the buyer’s payoff. Meanwhile, the presence of learning costs gives the seller an urgency to improve the value of information in order to avoid losing the sale and the buyer’s attention altogether. As a result, the buyer can leverage his potential inattention to obtain an equilibrium payoff that is strictly higher than the case with perfect attention.====Moreover, while the seller is always worse off when learning costs become higher, the amount of information processed by the buyer in equilibrium varies in a non-monotone fashion. Intuitively, when learning is more costly, in order to attract the buyer’s attention, the seller has to increase the instrumental value of information ==== by making the “buy” recommendation more informative about high quality. This effect tends to reduce her selling probability (as “buy” is recommended less frequently when quality is low) and increase the informativeness of the buyer’s learning. On the other hand, a higher learning cost worsens the buyer’s incentive to pay attention ====, which tends to shrink the set of incentive compatible policies and decrease the amount of information processed by the buyer. Both effects negatively impact the seller’s payoff while generating opposing forces on the buyer’s equilibrium learning, yielding our results.====The rest of the paper is organized as follows. Below, we discuss the literature. Section 2 sets up the model. Section 3 presents the main results on optimal disclosure and comparative statics. Section 4 solves an example with quadratic costs in closed form and illustrates explicitly the non-monotone variation of the buyer’s equilibrium learning. Section 5 discusses some extensions beyond the binary-action binary-state case. Section 6 concludes.==== This paper lies at the intersection of persuasion through flexible information (Kamenica and Gentzkow, 2011) and rational inattention (Sims, 1998, Sims, 2003, Matějka and McKay, 2015). The Bayesian persuasion framework has been extended to include costly information provision by the sender (Gentzkow and Kamenica, 2014) and costly ==== information acquisition by the receiver (Matyskova, 2018). The most related works, featuring information transmission under inattention, are Lipnowski et al. (2020a) and Bloedel and Segal (2020).====Lipnowski et al. (2020a) study the same setting with aligned decision preferences between the sender and the receiver, allowing for any finite state space and compact action space. It is shown that full disclosure is universally optimal if and only if the state of the world is binary.==== The current paper departs by studying a model with misaligned decision preferences and illustrates the interacting effect between persuasion and inattention on optimal disclosure. The binary-state environment in this paper assumes away multidimensionality of the belief space which is the main driving force (and complication) of the results in Lipnowski et al. (2020a).====Bloedel and Segal (2020) have almost identical motivations as this paper and Lipnowski et al. (2020a) with substantially different modeling assumptions, most notably regarding the nature of costly learning and the restriction to entropy costs on a particular signal space. In our model, the receiver incurs a cost to learn aspects of the underlying ====. In contrast, Bloedel and Segal (2020) have the receiver bear a cost to learn which ==== the sender has sent, while his interpretation of a learned message (to a posterior belief about state) is costless. Lipnowski et al. (2020b) provide a detailed discussion on the differences between these two cost specifications. Bloedel and Segal (2020) bypass the complications arising from a multidimensional belief space by defining signals as posterior means of the state, and their techniques and results hinge on the assumption of entropy costs. This paper does the same by focusing on binary-state environments, while our results apply to ==== posterior-separable costs.",Persuasion under costly learning,https://www.sciencedirect.com/science/article/pii/S0304406820301282,16 December 2020,2020,Research Article,150.0
Honda Edward,"Washington University in St.Louis, One Brookings Drive Campus Box 1208, St. Louis, MO 63130, United States","Received 17 February 2020, Revised 27 November 2020, Accepted 30 November 2020, Available online 11 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102458,Cited by (0),"We study a class of preferences that generalize substitutable ones and allows complementarities in a college admissions model. We show that a stable matching exists under this class of preferences by using an algorithm which we call the Conditional Deferred Acceptance Algorithm. Furthermore, we show that the algorithm is strategy-proof for the student side if we assume in addition a law of aggregate demand adapted to our setting.","In their seminal work, Gale and Shapley (1962) introduce the Deferred Acceptance Algorithm (DAA) and prove the existence of stable matchings in marriage markets. It is shown also that the algorithm could be used for college admissions with colleges accepting more than one student. However, it is assumed that the colleges take into account only the individual rankings of the students, and preferences over sets of students are not specified. In a model that allows for workers and firms to be matched by different job descriptions, Roth (1984) allows for preferences to be defined over sets of workers and job descriptions, but a substitutability condition is assumed on the preferences of the firms. A similar type of substitutability condition is assumed in many other matching models. For example, Roth (1985) considers preferences of colleges that are called “responsive” preferences, and Abdulkadiroğlu (2005) considers a generalization of these preferences, but these preferences necessarily satisfy the substitutability condition defined in Roth and Sotomayor (1990).====The substitutability condition says that if a student is chosen by a college, he/she is still chosen when a different student is removed from the set of students considered by the school. This means that the removed student cannot affect the way the college views the chosen student. This precludes choice functions with complementarities in which a student becomes more desirable to a school after some other students become available to that school. Although the assumption seems harmless for the college admissions setting, there are settings in which this is unrealistic. For instance, in a matching between universities and professors there are coauthoring professors that get along well and exhibit complementarities.====Of course, the theoretical appeal of being able to model realistic behavior is important, but it is important to allow complementarities for practical purposes as well. We illustrate this point with the following example of an application of matching algorithms to college admissions with affirmative action. We assume that some students qualify for a scholarship and that each college has a limited budget. Therefore, schools must limit the number of accepted students that qualify for a scholarship in addition to having a diverse group of students.====It is easy to see that the school’s choice in this example violates the substitutability condition. To satisfy substitutability and accept ==== by adjusting the choice from the four students, school ==== must do one of the following (we do not consider accepting more than two students because there are not enough seats to do that):====The problem is that the first option violates the budget constraint while the second accepts students of the same nationality. The third option satisfies the budget constraint and does not accept students of the same nationality, but this wastes an open seat.====It is reasonable to think that such a choice function arises in reality since any school has a limited budget. Or in the case of matchings between firms and workers, firms have a limited budget. As we will mention after we formally introduce our class of preferences, it turns out that the choice behavior of school ==== can be in a class of preferences that guarantees stability even though it violates substitutability. The key is to prioritize the groups of students and to have preferences that are conditionally lexicographic with respect to categories of higher priority, and also to have substitutability within the categories.====Violations of substitutability in the choice of schools potentially arise also when schools have constraints on the ratios of accepted students of different nationalities. In this situation, a student of nationality ==== might become desirable to a school only if enough students of nationality ==== are available and accepted. Hence, allowing complementarities seems important both from a theoretical and practical point of view.====This being said, our goal is to prove the existence of a stable matching for a class of preferences that generalize substitutable ones and allows for complementarities. This, however, is likely to be problematic since Hatfield and Kojima (2008) show that a condition that they call weak substitutes, which is equivalent to the substitutes condition in the college admission model, is necessary for the existence of a stable matching in a many to one matching model in the following sense: if there are at least two colleges and if the choice function of one of the colleges (they actually use matchings between hospitals and doctors instead) violates weak substitutability, there exists a preference profile of students and preferences of other colleges satisfying weak substitutability such that no stable matching exists.==== So the existence of stability can break down even if it is the preference of only one school that violates substitutability.====At first glance it could seem like generalizing substitutable preferences is impossible due to this proposition of Hatfield and Kojima (2008). We say this because if we take a preference of a college that is in this generalized class of preferences that violates substitutability, it is natural to think that the same preference profile of students and preferences of other colleges satisfying weak substitutability (and hence, within our generalized domain) that were used to prove the nonexistence of a stable matching in the proof of Hatfield and Kojima (2008) makes existence fail for this domain as well. However, the proposition of Hatfield and Kojima (2008) does not require the preferences among colleges to be related in any way or have a common structure. Thus, if we impose some common structure on the preferences of all the colleges, we may be able to guarantee existence of a stable matching. This is because even if the preference of a college violates substitutability, it could be impossible to construct preferences of the other colleges to make existence fail as in the proof of the proposition of Hatfield and Kojima (2008) because such preferences may be invalid under the imposed structure.====As becomes clear when we introduce the preferences, this common structure is the way in which the set of students are partitioned. Loosely speaking, every college puts students into different categories as in Example 1, and all of the schools use the same categorization. As we briefly mentioned after Example 1, we assume that the preferences of the colleges are conditionally lexicographic with respect to the more important categories and that the preferences over students in a category conditional on a fixed set of students for the higher categories are substitutable. Once we make these assumptions, we prove the existence of a stable matching using an algorithm which runs the DAA in multiple steps with the outcome of each step depending on the outcome of the previous step.====The rest of the paper proceeds as follows. We introduce the model and the class of preferences that we study in Section 2. Section 3 presents the results. We conclude with some additional related literature in Section 4.",A modified deferred acceptance algorithm for conditionally lexicographic-substitutable preferences,https://www.sciencedirect.com/science/article/pii/S030440682030135X,11 December 2020,2020,Research Article,151.0
Fukuda Satoshi,"Department of Decision Sciences and IGIER, Bocconi University, Milan 20136, Italy","Received 28 March 2020, Revised 27 November 2020, Accepted 29 November 2020, Available online 8 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102456,Cited by (3),"This paper studies unawareness in terms of the lack of knowledge in a model that generalizes both a non-partitional standard-state-space model and a stationary generalized-state-space model. The resulting model may not necessarily satisfy AU Introspection: an agent, who is unaware of an event, is unaware of being unaware of it. Yet, the paper shows that such agent does not know whether she is unaware of it, i.e., she is ignorant of being unaware of it. First, the paper asks when and how the generalized model (in particular, a standard-state-space model) has a non-trivial form of unawareness and sensible properties of unawareness. Second, the paper studies the implications of the violation of AU Introspection. An agent, when facing infinitely many objects of knowledge, may know that there is an event of which she is unaware. Treating new information only at face value can cause an agent to become unaware of some event.","Since 2009, November 12th has been declared as World Pneumonia Day to “raise awareness about pneumonia, the world’s leading infectious killer of children under the age of 5” (WHO, 2018); see also Greenslade (2020). If one is unaware of the fact that “pneumonia accounts for 19 per cent of all under-five deaths” (UNICEF and WHO, 2006, Wardlaw et al., 2006), the unawareness would refer to the lack of knowledge. One does not know the fact, and one does not know that one does not know the fact. In contrast, when it has been claimed that “only one in five caregivers in the developing world know the two key symptoms of pneumonia—fast and difficult breathing—” (UNICEF and WHO, 2006, Wardlaw et al., 2006), one could interpret it as “four in five caregivers in the developing world are unaware of the symptoms of pneumonia”. The unawareness would refer to the lack of conception. Four in five caregivers, presumably familiar with pneumonia, would have limited understanding on some aspects (i.e., the symptoms) of pneumonia.====Two different models of unawareness in the literature embody two different notions of unawareness: one as the lack of knowledge and the other as the lack of conception. In a standard-state-space model in which the state space consists of a single space, unawareness refers to the lack of knowledge as in pioneering papers by Modica and Rustichini, 1994, Modica and Rustichini, 1999: an agent is unaware of a statement if she does not know it and she does not know that she does not know it.==== In contrast, in a generalized-state-space model in which the state space consists of multiple subspaces as in Heifetz et al., 2006, Heifetz et al., 2008, an agent is unaware of a statement if she does not know the subspace within which the statement is described. Since multiple subspaces differ in how rich their vocabulary is for describing statements, the non-knowledge of the subspace is interpreted as the lack of conception.==== Unawareness as the lack of conception is stronger than one as the lack of knowledge: if an agent does not know the subspace within which the statement is described, then she does not know the statement and she does not know that she does not know it.====This paper provides a tractable model of unawareness on a generalized state space that nests the above two models. The model enables one to directly compare a “non-partitional” standard-state-space model and a “stationary” generalized-state-space model of Heifetz et al. (2006) regarding the following questions. What properties of unawareness do they satisfy? When do they represent a non-trivial form of unawareness? How do they differ in possible behavioral implications?====The resulting model defines unawareness as the lack of knowledge, and is understood as a generalized-state-space modelwithout the axiom of AU Introspection: AU Introspection, first postulated by Dekel et al. (1998), states that if an agent is unaware of an event, then she is unaware of being unaware of it. The paper asks when unawareness as the lack of knowledge coincides with one as the lack of conception. If the underlying state space reduces to a single space and if agents’ knowledge is induced by a possibility correspondence on the single state space, then the resulting model is the non-partitional standard-state-space model. If the agents’ knowledge is induced by a possibility correspondence on the generalized state space and if the model satisfies AU Introspection, then the model is the stationary generalized-state-space model. In fact, I characterize a property of a possibility correspondence on a generalized state space that yields AU Introspection. The model can also capture agents’ knowledge which is not necessarily induced by possibility correspondences.====Roughly, the paper answers the above questions as follows. First, although the model may not necessarily satisfy AU Introspection, whenever an agent is unaware of an event, she is ==== of being unaware of it: she does not know she is unaware of it, and she does not know she is aware of it. I call this property JU Introspection.====Second, an agent is unaware of an event through two channels: (i) whether unawareness satisfies AU Introspection; and (ii) whether the agent knows the subspace to which the event belongs (in a standard state space, whether the agent knows the entire space).==== The paper shows that AU Introspection is a property that connects two notions of unawareness, one as the lack of conception and the other as the lack of knowledge. In a non-partitional standard-state-space model in which the agent knows the entire space, the model has a non-trivial form of unawareness if and only if (hereafter, iff) it violates AU Introspection. In a stationary generalized-state-space model in which AU Introspection holds, unawareness is determined exclusively by whether the agent knows subspaces.====Third, suppose unawareness may not satisfy AU Introspection. Then, an agent may know that there exists an event of which she is unaware, i.e., she may know her self-unawareness. Also, more knowledge may not necessarily lead to more awareness. In a (standard or generalized) state-space possibility-correspondence model, AU Introspection fails when the agent processes (new) information only at face value. Suppose the agent receives new information (say, ====) so that she knows ==== at a state iff ==== holds at that state. Since she takes information only at face value, assume that, when new information does not hold, she may not know that new information does not hold (i.e., the negation of ====). Note that the former statement implies that, when new information does not obtain, the agent does not know ====. These two statements imply that if new information ==== does not hold then she may not know that she does not know ====.==== Thus, when new information ==== does not obtain, the agent does not know ==== and she does not know that she does not know ====, i.e., she is unaware of new information ====.====The paper is organized as follows. The rest of this section provides a technical overview. Section 2 defines a model. Section 3 demonstrates that the model nests any non-partitional standard-state-space model and any (non-)stationary generalized-state-space model. Section 4 studies properties of unawareness. Section 4.1 restates unawareness in terms of ignorance and possibility. Section 4.2 characterizes non-trivial unawareness. Section 4.3 investigates the existing axioms of unawareness. Section 5 studies knowledge of self-unawareness (Section 5.1) and non-monotonicity of unawareness in knowledgeability (Section 5.2). Section 6 provides concluding remarks. Proofs are relegated to Appendix A. Online Appendix provides supplementary discussions.",Unawareness without AU Introspection,https://www.sciencedirect.com/science/article/pii/S0304406820301336,8 December 2020,2020,Research Article,152.0
"Federico Salvatore,Ferrari Giorgio","Dipartimento di Economia, Università di Genova, Via F. Vivaldi 5, 16126, Genova, Italy,Center for Mathematical Economics (IMW), Bielefeld University, Universitätsstrasse 25, 33615, Bielefeld, Germany","Received 21 July 2020, Revised 18 November 2020, Accepted 20 November 2020, Available online 8 December 2020, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2020.102453,Cited by (30),"We study the problem of a policymaker who aims at taming the spread of an epidemic while minimizing its associated social costs. The main feature of our model lies in the fact that the disease’s transmission rate is a diffusive stochastic process whose trend can be adjusted via costly confinement policies. We provide a complete theoretical analysis, as well as numerical experiments illustrating the structure of the optimal lockdown policy. In all our experiments the latter is characterized by three distinct periods: the epidemic is first let to freely evolve, then vigorously tamed, and finally a less stringent containment should be adopted. Moreover, the optimal containment policy is such that the product “reproduction number ==== percentage of susceptible” is kept after a certain date strictly below the critical level of one, although the reproduction number is let to oscillate above one in the last more relaxed phase of lockdown. Finally, an increase in the fluctuations of the transmission rate is shown to give rise to an earlier beginning of the optimal lockdown policy, which is also diluted over a longer period of time.","During the current Covid-19 pandemic, policymakers are dealing with the trade-off between safeguarding public health and damming the negative economic impact of severe lockdowns. The fight against the virus is made especially hard by the absence of a vaccination and the consequent random horizon of any policy, as well as by the extraordinariness of the event. In particular, the lack of data from the past, the difficulty of rapidly and accurately tracking infected, and super-spreading events such as mass gatherings, give rise to a random behavior of the transmission rate/reproduction number of the virus (see, e.g., ====). In this paper we propose and study a model for the optimal containment of infections due to an epidemic in which both the time horizon and the transmission rate of the disease are stochastic.====, that studies numerically optimal containment policies in the context of a ==== (SIR) model (cf. ====); ====, which estimates the transmission rate in various countries for a SIR model with given and fixed transmission rate; ====, which combines a careful numerical study with an elegant theoretical study of optimal lockdown policies in the SEAIRD (susceptible (S), exposed (E), asymptomatic (A), infected (I), recovered (R), deceased (D)) model; ====, where – in the context of a multi-group SIR model – it is investigated the effect of lockdown policies which are targeted to different social groups (especially, the “young”, the “middle-aged” and the “old”); ====, in which a multi-risk SIR model with heterogeneous citizens is calibrated on the Covid-19 pandemic in order to study the impact on incomes and mortality of age-specific confinements and Polymerase chain reaction (PCR) tests; ====, which calibrates and tests a SEIRD model (susceptible (S), exposed (E), infected (I), recovered (R), deceased (D)) of the spread of Covid-19 in an heterogeneous economy where different age and sectors are related to distinct risks.====, ====). In ==== the authors study a deterministic SIR model in which the social planner acts in order to keep the transmission rate below its natural level with the ultimate aim not to overwhelm the national health-care system. The minimization of a social cost functional is instead considered in ====, in the context of a deterministic SIR model over a finite time-horizon. The resulting control problem is tackled via the Pontryagin maximum principle and then a thorough numerical illustration is also provided.====Inspired by the deterministic problems of ==== and ==== (see also ====, ====, among others), and motivated by the need of incorporating random fluctuations in the disease’s transmission rate, in this paper we consider a stochastic control-theoretic version of the classical SIR model of ==== From a technical point of view, the main difference between the models in ====, ====, ====, ==== and ==== and ours, is that we deal with a stochastic version of the SIR model, instead of a deterministic one. As a matter of fact, in the aforementioned works the transmission rate is a deterministic control variable, while it is a controlled stochastic state variable in our paper. Moreover, our formulation is also different from that of other stochastic SIR models where the random transmission rate is chosen in such a way that only the levels of infected and susceptible people become affected by noise, with the transmission rate itself not being a state variable (see, e.g., ====, ==== and references therein). To the best of our knowledge, ours is the first work considering the transmission rate as a diffusive stochastic state variable and providing the complete theoretical analysis of the resulting control problem.====In addition to its theoretical value, the determination of an optimal control in feedback form allows us to perform numerical experiments aiming at showing some implications of our model. For the numerical analysis we specialize the dynamics of the transmission rate, that we take to be mean-reverting and bounded between ==== and some ==== (cf. ====). This is quadratic both in the regulator’s effort and in the percentage of infected people.====An interesting effect which is in fact common to all our numerical experiments is that the optimal lockdown policy is characterized by three distinct periods. In the first phase it is optimal to let the epidemic freely evolve, then the social restrictions should be stringent, and finally should be gradually relaxed in a third period. We also investigate which is the effect of the maximal level ==== of allowed containment measures (i.e., the lockdown policy can take values in ====) on the final percentage of recovered, which in fact turns out to be decreasing with respect to ====. This then suggests that the case ==== – which leads in a shorter period to the definitive containment of the disease with the smallest percentage of final recovered — might be thought of as optimal in the trade-off between social costs and final number of recovered.====We observe that if the epidemic spread is left uncontrolled, then its reproduction number ==== fluctuates around 1.8 and the final percentage of recovered (i.e. the total percentage of infected during the disease) is approximately 72% of the society after circa 7 months (in all our simulations the initial infected were 1% of the population). On the other hand, when ====, under the optimal policy we have a relative reduction of circa 30% of the total percentage of recovered individuals, and the reproduction number drops below 0.6 in the period of severe lockdown (circa 60 days). Moreover, the optimal containment is such that the so-called “herd immunity” is reached as the product ==== (reproduction number ==== percentage of susceptible) becomes strictly smaller than the critical level of one, even if ==== oscillates at around 1.7 in the last more relaxed phase of lockdown. Finally, we observe that an increase of the fluctuations of the transmission rate ==== have the effect of anticipating the beginning of the lockdown policies, of diluting the actions over a longer period, and of keeping a larger level of containment in the long run. This can be explained by thinking that an higher uncertainty in the transmission rate induces the policymaker to act earlier and over a longer period in order to prevent positive larger shocks of ====.====The rest of the paper is organized as follows: In Section ==== we set up the model and the social planner problem. In Section ==== we develop the control-theoretic analysis and provide the regularity of the minimal cost function and an optimal control in feedback form. In Section ==== we present our numerical examples, while concluding remarks are made in Section ====. Finally, the ==== collects the proof of some technical results needed in Section ====.====Denote by ==== an arbitrary point of ====. For any multi-index ==== we denote by ==== and ====, with the convention that ==== is the identity.",Taming the spread of an epidemic by lockdown policies,https://www.sciencedirect.com/science/article/pii/S0304406820301300,8 December 2020,2020,Research Article,153.0
Nguyen Van-Quy,"Centre d’Economie de la Sorbonne, Université Paris 1 Panthéon - Sorbonne, 106-112 Boulevard de l’Hôpital, 75647 Paris Cedex 13, France","Received 21 April 2020, Revised 11 November 2020, Accepted 26 November 2020, Available online 2 December 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.102454,Cited by (0)," with endowment externalities. Consumers’ behaviors depend not only on their own consumption but also on the endowments of the other consumers. Applying the same method of analysis in Balasko (2015) about ==== concerns, we first show that almost all properties of equilibrium, including smooth equilibrium manifold and genericity of regular economies, can be directly extended to the economy where the demand function depends on the endowments of others and ==== of only one consumer. Next, we clarify the sufficient conditions under which those properties remain true in the economy with the most general form of endowment externalities. Finally, we generalize the above sufficient conditions to derive generic regularity results in the economy with both consumption and endowment externalities.","Debreu (1974) claims that: “The observed state of an economy can be viewed as an equilibrium resulting from the interaction of a large number of agents with partially conflicting interests”. Although general equilibrium theory is designed to describe and explain economic phenomena very broadly, the specific formulation of the economy may limit its application. The previous literature in the field so far has typically assumed that consumers’ preference depends only on their own consumption (selfish preferences) and not that of others. This self-regarding frame of reference ignores externalities that arise from the decisions of other consumers in an economy. The presence of externalities on consumer behavior is widely acknowledged in the economic literature and has been well documented from both empirical and experimental perspectives (for example Luttmer (2005) and Zizzo and Oswald (2001)). Therefore, the question of which properties of equilibrium generalize in the presence of externalities has become an essential and debatable question in the field of economics.====There is a growing number of literature on general equilibrium models in the presence of externalities. As mentioned in Laffont (1976) and Hammond (1998), externalities appear not only in preferences but also in consumption sets. Studying the same kind of externalities, Bonnisseau and del Mercato (2010) show that genericity of regular equilibria holds as long as second-order external effects on individual preferences are not stronger than the direct effect of household consumption. Bonnisseau (2003) considers a wide class of non-ordered preferences with external effects and shows that some desirable properties of the equilibrium manifold and regular economies remain true under a geometric assumption on preferences. Indeed, he shows the existence of equilibrium, an odd number of equilibria, and the genericity of regular economies remain true under the geometric assumption on preferences. Balasko (2003) shows that many properties of standard equilibrium models can be extended when preferences depend on prices and total resources are variable. Moreover, Fehr and Gächter (2000) and Sobel (2005) emphasize the appropriateness of unselfish preferences and introduced the popular “other-regarding preferences” (ORP). Additionally, a recent paper by Dufwenberg et al. (2011) shows that ORP does not affect market behavior when preferences are separable, and this entails the equilibria of economies with externalities coincide with those of the related classical economy without externalities.====In the present paper, building on the seminal contribution of Balasko (2015), we study a new kind of externalities termed as endowment externalities. Including externalities based on endowments is a more natural way to model several situations. In previous studies, many behavioral economists and decision theorists have agreed that individual behavior depends on a reference point. For example, Masatlioglu et al. (2014) and Maltz (2020) show that reference points belong to the class of endowments containing the initial endowments. Heap et al. (2016), Hopkins and Kornienko (2010), and Hopkins (2018) study the effect of the endowment inequality and the distribution of endowment on different actions of individuals, including contributing to a public good, making effort for a better reward, or engaging in gambling. These papers show the importance of endowment dependence in an economic model.====We make two main contributions. First, we extend Balasko (2015)’s result about wealth externalities when consumer behavior is described by demand functions. Balasko considered the case where demand functions depend on price and on the wealth of all consumers. We consider the more general case where the demand function depends on the endowments of others and the wealth of only one consumer. This asymmetric dependence allows us to obtain the same results as the one in Balasko (2015) using the same method of analysis. We show that under very general assumptions on individual demand functions, the following properties of equilibrium remain unchanged: the equilibrium manifold is smooth, the economy exhibits a ramified properness of the natural projection, and regular economies are generic. Moreover, regular economies have a finite (and odd) number of equilibria and possess local continuity of equilibrium selection mappings.====The second contribution generalizes the form of externalities in terms of endowments, which encompass the wealth concerns. In this part, consumers are characterized by utility functions which depend on the endowments of all consumers. Balasko’s method is not applicable in this case, because of the complexity of the Jacobian matrix. We take a different approach, the so-called extended approach where the equilibria are characterized by first-order conditions and market clearing conditions. This is based on the seminal work by Smale (1974) and Villanacci et al. (2002). As shown in an example by Bonnisseau and del Mercato (2010), an additional assumption on utility functions is necessary to carry out this type of analysis. We formalize this in Assumption 13. Precisely, we assume that the first-order effects of the endowments on the marginal rate of substitution are small enough (and vanishing) along the direction which keeps wealth constant. This condition is clearly satisfied under the separability condition of Dufwenberg et al. (2011) and when the external effect of one consumer on all others is a wealth effect only. With this assumption, we are able to derive the following nice properties of general equilibrium: the equilibrium set is a smooth manifold and the set of regular economies is an open and full Lebesgue measure subset of the parameter space. Moreover, we identify a natural relationship with the aggregate excess demand approach. This makes it possible to study genericity properties in the model with endowment externalities by applying the aggregate excess demand approach. We generalize Assumption 13 to include both endowments and consumption externalities and derive genericity of regular economies in the more general setting.====This paper is organized in the following order. Section 2 is devoted to definitions and notations regarding the economic environment of the exchange model. In Section 3, we consider the model with demand functions depending on the other endowments and wealth of one consumer. We prove that the equilibrium set is a manifold and we deduce the genericity of regular economies. In Section 4, we consider general endowment externalities, we state the key Assumption 13 and prove that it is sufficient to get the desired properties of equilibrium. Finally, in Section 5 we draw some conclusions and propose some lines for future research. All technical proofs are gathered in Appendix, except for Proposition 17, Lemma 18, Theorems 19, and 22.",Endowment-regarding preferences,https://www.sciencedirect.com/science/article/pii/S0304406820301312,2 December 2020,2020,Research Article,154.0
Cato Susumu,"Institute of Social Science, University of Tokyo, 7-3-1, Hongo, Bunkyo, Tokyo 113-0033, Japan","Received 26 April 2020, Revised 25 October 2020, Accepted 15 November 2020, Available online 28 November 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.11.002,Cited by (1),This paper examines the aggregation of preferences with a ,"Arrow’s impossibility theorem, which is a fundamental result for the aggregation of preferences, shows the incompatibility among three normatively desirable axioms: weak Pareto, independence of irrelevant alternatives, and non-dictatorship (Arrow, 1951). Arrow assumes that the set of individuals is finite. This finiteness assumption is crucial in the sense that there exists a social welfare function that satisfies Arrow’s axioms if the set of individuals is infinite, as shown by Fishburn (1970). In this study, we consider a measure space of agents, which is common in the field of general equilibrium theory.==== A pioneering work by Kirman and Sondermann (1972) provides basic results for Arrovian social choice theory with a measure space of agents. Kirman and Sondermann (1972) consider another version of non-dictatorship, ====, which requires a lower bound on the size of a decisive coalition, and show that in any ==== society, there exists no social welfare function that satisfies weak Pareto, independence of irrelevant alternatives, and coalitional non-dictatorship.====In this paper, we examine a ==== (or ====), and allow it to be ====.==== Our first result states that there exists a social welfare function that satisfies weak Pareto, independence of irrelevant alternatives, and coalitional non-dictatorship if and only if the mass of agents is atomic. In other words, the existence of an atom is a necessary and sufficient condition for the existence of a social welfare function that satisfies the three axioms. This implies that the Kirman–Sondermann result depends crucially on the assumption that the measure space of agents is atomless. A key observation behind our result is that non-dictatorship implies coalitional non-dictatorship in an atomless society, but they are independent of each other in general. Indeed, an atom tends to yield a dictator.====In our second result, we specify a necessary and sufficient condition for the finitely additive measure that guarantees the existence of a social welfare function that satisfies weak Pareto, independence of irrelevant alternatives, non-dictatorship, and coalitional non-dictatorship. The condition is that the mass is atomic, but that no singleton has a positive size. Such a measure can be constructed using a free ultrafilter. Although coalitional non-dictatorship is a natural normative property under an atomless society, it may not be so if there is an atom. We then formulate a new version of non-dictatorship, ====, which is normatively natural in the presence of an atom. According to atomic non-dictatorship, any atom cannot be decisive unless it coincides with the entire population. In our third result, we impose both atomic non-dictatorship and coalitional non-dictatorship on the social welfare function and obtain the requisite condition for the finitely additive measure space of agents. The result reveals that the space is extremely restricted with these non-dictatorship axioms. Because the standard non-dictatorship axiom follows from the other two versions under a certain condition on the measure, our result implies that all axioms are satisfied only under the extreme condition on the measure.====The interpretation of an atom is crucial to deriving the implications of our results. In an atomless society, all agents or participants are negligible. The existence of an atom implies that there exists some non-negligible agent in society. For an economy with a continuum of agents, Aumann (1964) emphasizes that an atom in a measure space can naturally capture the market power in an oligopoly. Then, Gabszewicz and Mertens (1971) and Shitovitz (1973) examine a “mixed” economy in which negligible and non-negligible agents exist simultaneously.==== Notably, there are at least two interpretations of collective decision-making with an infinite population. In the first, we can consider preference aggregation for multi-selves. One person can have different identities and preferences at different times. Thus, he or she aggregates his or her own different preferences to make a choice. Under this interpretation, an atom is some strong personality in this person. For example, the selfish/egoistic preference can be an atom, and may have strong power. In the second interpretation, we can consider social choice for an intergenerational conflict. There is a finite population at one time, but there are infinitely many individuals in the future. The future generations are unborn and, thus, it is not possible to know their preferences. However, it is meaningful to examine how we can resolve intergenerational conflict if the future generations’ preferences are hypothetically available; see Ferejohn and Page (1978), Bossert and Suzumura (2011), and Cato (2020) for attempts at Arrovian approaches of intergenerational social choice. In this interpretation, the group of individuals in the present can be an atom under collective decision-making.====Next, we discuss related works. First, it has been known that an ultrafilter is a basic mathematical tool for Arrovian social choice theory, as shown by Kirman and Sondermann (1972) and Hansson (1976). These two papers show that the family of decisive coalitions forms an ultrafilter; see also Monjardet (1983) for an early survey on this stream of works.==== The analysis is extended by Armstrong, 1980, Armstrong, 1985, Lauwers and Van Liedekerke (1995), Cato, 2013a, Cato, 2013b, Cato, 2017, Cato, 2018, Takayama and Yokotani (2017), and Bossert and Cato (2020a). Their results are applied to consider collective decision-making in a society with an infinite population.====Second, several scholars have introduced some measure structure to examine collective-decision making. Schmitz (1977) shows that a positive result is recovered when the size of the measure is infinite. For a finite measure, Nagahisa (1993) examines acyclic social choice in an atomless measure space of agents by imposing positive responsiveness. According to Nagahisa (1993), even if we allow social preferences to be intransitive, there is a very small, but decisive coalition with positive responsiveness. As is well known, positive responsiveness is quite common because it is a fundamental characteristic of the simple majority rule (May, 1952). Fey (2004) introduces an asymptotic density for the simple majority rule in an infinite-population setting; Fey’s result is not dependent on the measure structure. Subsequently, Surekha and Rao (2010) extend Fey’s analysis by introducing a measure space of agents.====The rest of this paper is organized as follows. Section 2 introduces our setting. Section 3 shows our results. Section 4 concludes the paper.",Preference aggregation and atoms in measures,https://www.sciencedirect.com/science/article/pii/S0304406820301233,28 November 2020,2020,Research Article,155.0
"Whitmeyer Joseph,Whitmeyer Mark","Department of Sociology, University of North Carolina at Charlotte, United States of America,Institute for Microeconomics and Hausdorff Center for Mathematics, University of Bonn, Germany","Received 8 May 2020, Revised 20 September 2020, Accepted 15 November 2020, Available online 27 November 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.11.006,Cited by (0),"Given any purely atomic probability distribution with support on ==== points, ====, any mean-preserving contraction (mpc) of ====, ====, with support on ==== points is a mixture of mpcs of ====, each with support on at most ","The mean-preserving contraction (mpc) of a probability distribution, in which a probability distribution is altered by collapsing portions of its measure to their respective barycenters, is an important concept in economics. In recent years there has been a surge of papers in which information structures are endogenous, and many works involve optimization problems in which one or many agents choose from the set of mpcs of a given distribution.====The purpose of this paper is to establish a result that is useful both in clarifying the economic intuition behind the solutions to, as well as simplifying the solving of, such problems. Our main finding, Theorem 1.4, establishes that given a purely atomic probability measure ==== on ==== with support on ==== points, any mpc of ==== with support on ==== points, ====, is the convex combination of two mpcs of ====, ==== and ====, each of which have support on at most ==== points. An important corollary of this result, Corollary 1.6, is that any mpc, ====, of ==== is a mixture of mpcs with support on at most ==== points.====These discoveries imply several compelling results related to Bayesian persuasion and information design,==== a literature that studies how a principal (or principals) can design information structures in various environments in order to achieve some objective. One particularly tractable class of problems are those in which the state is a random variable and the sender and receiver’s utilities are linear in the state (see e.g. Gentzkow and Kamenica, 2016, Kolotilin et al., 2017, Kolotilin, 2018; and Dworczak and Martini, 2019). Corollary 1.6 allows for an elementary proof for the upper bound of the number of messages needed by the persuader in an optimal mechanism in such problems (Proposition 2.1).====Corollary 1.6 also has useful ramifications for competitive Bayesian persuasion problems, in which multiple principals compete by designing information structures. Our result implies that when constructing equilibria, instead of checking deviations to ==== mpc of ====, one need check only deviations to mpcs with support on ==== points. In Section 2.2 we present a simple competitive persuasion problem in which this result is used, and Whitmeyer (2018) and Jain and Whitmeyer (2019) both appeal to Corollary 1.6 in similar settings to this example. The existence of a symmetric mixed strategy equilibrium with support on ==== (or fewer)-point mpcs (Proposition 2.2) also follows from this corollary.====This paper contributes to the literature in economics and mathematics on statistical experiments and majorization. The first foray into the area is the seminal work of Hardy et al. (1929) (see also Hardy et al., 1959), who establish a fundamental result on majorization, which was shortly followed and generalized by Blackwell (1953) and Strassen (1965).==== The equivalent notion of a mean-preserving spread (mps) – the opposite of an mpc in which portions of a probability measure are “spread” out – was subsequently introduced by Rothschild and Stiglitz (1970).====More recently, Elton and Hill (1992) introduce the concept of a “fusion” of a probability distribution. A fusion, ====, of a distribution ==== is any distribution that can be obtained by collapsing parts of the mass of ==== to their respective barycenters. In the setting we analyze in this paper, where ==== is a measure on ==== (with finite first moment), the notions of fusion and mean-preserving contraction are equivalent. A key contribution of Elton and Hill (1992) is the construction of the set of fusions: just as a measurable function can be approximated by simple functions, which are themselves finite linear combinations of indicator functions, a fusion can be approximated by simple fusions, which are themselves finite compositions of elementary fusions.====In this spirit, the contribution of our paper can be seen as it providing an alternate characterization for the set of fusions of a (finitely supported) probability measure ==== (on ====), one that we believe should be particularly compelling (and useful) to economists. Rather than taking elementary fusions as the building blocks, as Elton and Hill (1992) do, our basic objects are the fusions of ==== with support on ==== points. Consequently, Corollary 1.6 implies that any fusion can be approximated by finite ==== of fusions with ====-point support.====As we discuss later on in the paper, the set of all mpcs of a discrete probability measure is a compact and convex set. By the Krein–Milman theorem, such a set is merely the closed and convex hull of its extreme points. This is valuable in well-behaved linear optimization problems, in which the objective function obtains a maximum at an extreme point of the constraint set. This theorem (Krein–Milman) and its extension, Choquet’s theorem, provide part of the impetus behind the remarkable recent paper by Kleiner et al. (2020), who characterize the extreme points of the set of monotonic functions that majorize (or are majorized by) a given monotonic function such as, e.g., a cumulative distribution function (cdf). They thus characterize the set of mpcs and mpss for an arbitrary distribution.====We believe that our paper can be seen as complementary to Kleiner et al. (2020). Indeed their main result in which they characterize the extreme points of the set of mpcs of a random variable with cdf ==== – Theorem 2 – requires that ==== be continuous. In turn, Corollary 1.6 in this paper provides a necessary condition for a distribution to be an extreme point of the set of mpcs of a distribution with ====-point support—it must have support on ==== points or fewer.",Mixtures of mean-preserving contractions,https://www.sciencedirect.com/science/article/pii/S0304406820301270,27 November 2020,2020,Research Article,156.0
"Geng Sen,Özbay Erkut Y.","MOE Key Laboratory of Econometrics, Wang Yanan Institute for Studies in Economics, Xiamen University, China,Department of Economics, University of Maryland, United States of America","Received 17 June 2020, Revised 2 October 2020, Accepted 15 November 2020, Available online 25 November 2020, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jmateco.2020.11.003,Cited by (4),A ,"Say that a consumer wishes to purchase a perfume, but there turn out to be too many alternatives. According to the classical choice theory, the consumer considers all the perfumes and chooses the best one. However, it may not always be possible to consider all the products, so the consumer needs to eliminate some of the alternatives and consider only the remaining alternatives.==== Then, she chooses the best one among the ones she considers.====This two-stage procedure of the consider-then-choose process has been widely investigated, especially in the marketing and empirical IO literature. Hauser (2014) argues that consumers use the consider-then-choose decision process if the number of products is large; on the other hand, if there are few available products, a consumer might consider all of them. A consumer’s capacity determines whether the number of alternatives is large or small. In a sense, the capacity involves a switch between the classical choice theory and the consider-then-choose process. Numerous factors may contribute to the limited capacity. A physical limitation may apply. For example, a consumer may keep in her mind only a limited number of alternatives (Miller, 1956). Alternatively, the capacity may be constrained by some exogenous factors. For example, a decision maker (DM) may have limited time to decide, with full consideration of the alternatives being impossible within this limited time (Geng, 2016). The capacity may also be constrained as a result of a tradeoff between the search cost and benefits (Stigler, 1961, Hauser, 2014), or by limiting the capacity, the DM may reduce the decision cost (Ergin and Sarver, 2010, Ortoleva, 2013).====In our setup, a DM with a limited capacity considers all the alternatives when the number of alternatives is within her capacity. However, when the number of alternatives exceeds her capacity, she forms her consideration set by using a rationale to eliminate the dominated alternatives as in Manzini and Mariotti (2007). Additionally, the number of shortlisted alternatives must be within her capacity.==== We call this procedure a ====. The idea of using a rationale is to simplify the decision by reducing the number of alternatives. For example, if the products have multiple attributes, and, say, the value of an alternative is equal to some aggregation of these attributes, a DM may find it simpler to focus on some salient attributes and may consider only the undominated alternatives based on those attributes. A rationale may even be exogenously available, such as an online filtering tool, so that the DM reduces the number of alternatives by this tool.====In the classical choice theory that assumes full consideration for all set sizes, the binary choices reveal the preference if and only if the weak axiom of revealed preference (WARP) is satisfied. In the shortlisting procedure with capacity-====, since the full consideration assumption is relaxed, WARP violations are important. Indeed, it turns out to be that a WARP violation is enough to uniquely identify the capacity. To illustrate this point, say we observe a choice reversal (==== but that ====). This reveals that ==== is beyond the capacity of a DM. The largest set in which we do not observe choice reversals determines the so-called threshold capacity. Beyond the threshold capacity, it is too many for a DM so that she will use the shortlisting procedure. We show that the threshold capacity of the choices is the unique capacity of the DM. We also show that the capacity must be no less than two; in other words, a DM must consider binary choices fully. Hence, even if there is a WARP violation in the choice, the binary choice still reveals the preference as in the classical choice theory.====Additionally, we study how the choices reveal the rationale and the shortlisted alternatives. We also provide a choice-based characterization. We relax the axioms in Manzini and Mariotti (2007) to show the relation between their model and our shortlisting procedure with capacity-====. In the Discussion section, we revisit different choice reversals and compare the degree to which different models can explain these choice reversals.====We also study the link between the heuristics that are used to form consideration sets and the properties of these consideration sets. Observing the consideration sets has been particularly important for the marketing literature, especially when the entire choice data are not available (de Clippel et al., 2014, Dardanoni et al., 2020). Recently, the development of new tools, such as eye tracking, has made it possible to observe these sets (Reutskaja et al., 2011), which are also observable in many real-life situations, especially in online stores (Manzini et al., 2019). We provide the necessary and sufficient properties of the consideration sets to be derived by the shortlisting procedure with capacity-====. We find that the shortlisting procedure with capacity-==== refines the competition filter in Lleras et al. (2017).====Our paper contributes to the literature that incorporates limited capacity into limited consideration models. For example, in Salant and Rubinstein (2008), the capacity can be any number, but it is assumed to be observed.==== In Eliaz et al. (2011), under the consideration set interpretation, the DM narrows down the consideration set to two alternatives. In Bajraj and Ülkü (2015), the DM uses a shortlisting procedure to determine the top two alternatives to consider. To our knowledge, ours is the first paper that identifies the capacity from the choice data.====The rest of the paper is structured as follows: In Section 2, we introduce the limited capacity model. In Section 3, we show how the choices reveal the capacity, preference, conflicting rationale and shortlist. In Section 4, we provide the characterizations of the shortlisting procedure with a limited capacity. In Section 5, we discuss the related literature, and Section 6 concludes the paper. Proofs and examples illustrating independence of axioms appear in the Appendix A.",Shortlisting procedure with a limited capacity,https://www.sciencedirect.com/science/article/pii/S0304406820301245,25 November 2020,2020,Research Article,157.0
"Le Breton Michel,Shapoval Alexander,Weber Shlomo","Toulouse School of Economics, Toulouse, France,HSE University, Moscow, Russian Federation,China-Russia-Eurasia Research Centre (CREC), New Economic School, Moscow, Russian Federation","Received 20 May 2020, Revised 15 October 2020, Accepted 14 November 2020, Available online 21 November 2020, Version of Record 4 December 2020.",https://doi.org/10.1016/j.jmateco.2020.11.004,Cited by (1),"In this paper we examine a game-theoretical generalization of the landscape theory introduced by Axelrod and Bennett (1993). In their two-bloc setting each player ranks the blocs on the basis of the sum of her individual evaluations of members of the group. We extend the Axelrod–Bennett setting by allowing an arbitrary number of blocs and expanding the set of possible deviations to include multi-country gradual deviations. We show that a ==== landscape equilibrium which is immune to profitable gradual deviations always exists. We also indicate that while a landscape equilibrium is a stronger concept than ==== in pure strategies, it is weaker than strong ====.","This paper examines the class of strategic environments covered by the “landscape theory”introduced by Axelrod and Bennett 1993 (AB — henceforth). In the landscape setting the actors (countries) are partitioned into two mutually exclusive blocs on the basis of their propensity to work together with other bloc members on the bilateral basis. All players rank groups according to the sum of her individual evaluations of all members of the group. In this sense the AB approach to international interactions is related to Bueno De Mesquita (Bueno De Mesquita, 1975, Bueno De Mesquita, 1981) who constructed a proximity matrix for every pair of nations based on history of their defense cooperation.====Each actor ==== is characterized by the value of her size/strength/influence parameter ====. For each pair of countries ==== and ==== there is a parameter ==== (positive or negative), the value of which represents the propensity for collaboration between ==== and ====. Thus, the data of the model consists of a ====-dimensional vector of countries’ strength parameters and an ==== matrix ==== of pairwise proximity coefficients.====For an arbitrary partition of all countries into two blocs, AB define the ==== of a country ==== as the sum of the proximity coefficients ==== for all members outside her bloc weighted by their strength parameter ====. Obviously, the country frustration will be reduced if it avoids countries with whom it has a strong negative propensity to align. The ==== of any two-bloc partition is then determined as the sum of individual frustrations of all countries weighted by their size. The objective of the theory is to identify the configurations that yield, as AB call it, a local and ==== minimum of energy. To attain these outcomes, AB used the ==== or ==== approach by allowing single countries to switch their membership, one at a time, to generate a new configuration with the reduced energy level. Assuming the symmetry of the proximity matrix ====, i.e. ==== for all pairs of players ====, AB showed that for any initial bloc structure, the sequential gradual reduction of energy does not contain cycles and is terminated when a stable configuration is attained. Note that the symmetry of the proximity matrix ==== is essential to obtain stable configurations. Indeed, consider a game with two players, where player ==== prefers to join player ====, i.e., ====, whereas player ==== would like to avoid being together with ====, i.e., ====. The game obviously does not admit a stable partition, as the partition in two groups would be challenged by player ====, while the creation of a two-country bloc would be rejected by player ====.====AB provide a spectacular application of the landscape theory to European alliances prior to World War II. By using the Correlates of War data and estimating the propensity for cooperation based on ethnic and border conflicts, history, etc., AB calibrate a matrix ==== to conclude that there were two stable configurations. One is the expected partition to the Axis and Allies of World War II, while the other separates USSR, Yugoslavia and Greece from the rest of Europe! Axelrod et al. (1995) also illustrate and test the landscape theory by estimating the choices of nine computer companies to join one of two alliances sponsoring competing UNIX operating system standards in 1988.====Even though AB have not done that explicitly, it is natural to present their setting in game-theoretical terms. Each actor ==== is a player with two available pure strategies corresponding to two blocs, ==== and ====, and her payoff function is represented by her frustration level derived from the two-bloc partition. Thus, after minor adjustments, proper reformulation, and clarifications, AB in fact show the existence of a pure strategies Nash equilibrium in landscape games. As Bennett (2000, p. 51) points out: “A local optimum is defined as a configuration for which every adjacent configuration has higher (worse) energy. When the system reaches one of those points, no further improvement in energy is possible given a single step (change of coalition by one actor). This optimum is akin to a Nash equilibrium in game theory, wherein no single actor can improve its own payoff by choosing a different move”. Interestingly enough, the AB energy function ==== could be viewed as a potential, so that symmetric landscape games belong to the class of potential games examined by Monderer and Shapley (1996).====Notice that landscape games belong to the class of hedonic games pioneered by Banerjee et al. (2001) and Bogomolnaia and Jackson (2002). Hedonic games are coalition formation games, where the payoff of any player depends solely upon the composition of the coalition to which she belongs, and a strategic choice made by the coalition does not impact its members’ payoffs. This is the case for landscape games, where each player possesses a precise evaluation of every potential partner and then ranks groups according to the sum of her individual evaluations of all members of the group she may join. In the case of equal values of the strength parameter ==== for all countries, this model belongs to the class of ==== games in Banerjee et al. (2001). By constructing a potential function, as in AB, Bogomolnaia and Jackson (2002) show that the symmetric additively separable hedonic games, including the landscape games, admit a Nash stable configuration.====In this paper, we consider the class of landscape games that expands the AB framework in two aspects. First, we allow for an arbitrary number of blocs to form, without limiting ourselves to two-bloc configurations.==== ==== The configurations with more than two blocs have a place in various environments. In fact, during the Cold War between East and West that followed the end of the World War II, an important role has been played by the third bloc of non-aligned countries. And nowadays, when the world is often described as a multi-polar environment, the study of multi-bloc settings becomes even more relevant. Another distinction with regard to the AB model is that we expand the notion of incremental or gradual deviations in AB, in which only one country at a time was allowed to switch its bloc membership. In our framework we take the gradual approach further by allowing several countries to switch their blocs at the same time. However, the cost of absorption of new members from different blocs could be quite high. Thus, a switch will be allowed only for a subgroup from one bloc to another. We call such a deviation gradual and define a ==== as a configuration immune to gradual deviations. Note that individual deviations are obviously allowed under the umbrella of gradualism.====Our main result shows that, under the symmetry assumption, there is a landscape equilibrium. Our proof relies on the ==== approach. While our game itself is not a potential game, we show that it is strategically equivalent to a potential game. That is our game admits a weighted potential in Monderer and Shapley’s 1996 terminology. The application of the potential approach allows us to obtain even a stronger result, namely the existence of a Pareto optimal landscape equilibrium. It is important pointing out that the existence of a Pareto optimal landscape equilibrium rules out an emergence of prisoner’s dilemma type of situation, where countries acting in their own self-interest generate a suboptimal outcome. Interestingly, some aspects of Pareto optimality have been discussed by AB, who searched for the global optimum as the lowest energy level of any configuration. Since the concept of landscape equilibrium is stronger than Nash equilibrium, our result reconfirms the existence of a Nash equilibrium in landscape games. On the other hand, we also consider a more demanding notion of strong Nash equilibrium introduced by Aumann (1959), which requites immunity against any deviation by any group of players. However, as is implied by a result in Banerjee et al. (2001), a strong Nash equilibrium in landscape games may fail to exist. Thus, the unrestricted extension of the set of feasible deviations not only violates the concept of gradualism, but also diminishes the likelihood of obtaining a meaningful existence result.====The paper is organized as follows. In the next section we offer a brief review of the literature. In Section 3 we present a model and the necessary definitions. In Section 4 we state and prove our result on existence of a Pareto optimal landscape equilibrium. In Section 5 we discuss the links of our equilibrium concept with other modifications of Nash equilibrium.",A game-theoretical model of the landscape theory,https://www.sciencedirect.com/science/article/pii/S0304406820301257,21 November 2020,2020,Research Article,158.0
"Chen Siwei,Heo Eun Jeong","Lingnan College, Sun Yat-sen University, Guangzhou 510275, China,Department of Economics, Sungkyunkwan University, Seoul 03063, South Korea","Received 12 April 2020, Revised 14 July 2020, Accepted 31 October 2020, Available online 12 November 2020, Version of Record 21 November 2020.",https://doi.org/10.1016/j.jmateco.2020.10.008,Cited by (2),", we provide characterizations of these conditions. We also present the logical relations among the conditions by using our characterizations.","We study a standard school choice problem where there is a set of (public) schools whose seats are to be distributed to a group of students. Each student has a strict preference over the schools and each school has a priority ordering over the students. The priorities of the schools are determined by local or state law on the basis of residence proximity, the presence of siblings, and various other criteria (Abdulkadiroǧlu and Sönmez, 2003). For each preference profile and each priority profile, a rule assigns to each student at most one school subject to the schools’ capacity constraints.====For this problem, three rules have been extensively studied and used in practice: the student-proposing deferred acceptance (DA) rule (Gale and Shapley, 1962), the top-trading cycles (TTC) rule, and the immediate acceptance (IA) rule (Abdulkadiroǧlu and Sönmez, 2003). These rules take students’ full preference list over the schools as input, but they can also be implemented in practice by requiring students to report their “short-listed” preferences over a fixed number of schools (Haeringer and Klijn, 2009). Among these rules, unfortunately, no rule jointly satisfies three key axioms: ====, ====, and ====. Precisely, DA is not efficient; IA is neither stable nor strategy-proof; and TTC is not stable.====In an attempt to achieve these axioms, a list of restrictions have been imposed on priority profiles. Those restrictions are referred to as “acyclicity” or “homogeneity” conditions, which turn out to be closely related to each other. The conditions, however, are not easy to verify, as they all require the non-existence of a certain “cycle” involving a pair of schools and two or three students with certain relative priorities among them. Checking whether there exists such a cycle for every pair of schools and every pair (or triplet) of students is, however, quite demanding. Moreover, it is not easy to see the structure of the priority profile implied by these conditions. In this paper, we provide characterizations of these conditions to resolve these issues.====Ergin (2002) is the first paper that introduces this type of condition, which we call “E-acyclicity”. He shows that DA is efficient if and only if a priority profile is E-acyclic. Kesten (2006) identifies a similar restriction for TTC: a stronger condition than E-acyclicity is needed to achieve stability of TTC. He also shows that for these restricted priority profiles, TTC coincides with DA. Haeringer and Klijn (2009), on the other hand, focus on so-called ==== school choice problem, where each student reports his short-listed preference over the schools. That is, a student can only report a fixed number of schools in his preference list and the aforementioned rules take that as input. Since none of DA, TTC, and IA satisfies strategy-proofness, Haeringer and Klijn (2009) instead study the preference revelation game associated with each rule. They identify Nash equilibria of the game and check whether equilibrium outcomes are efficient or stable. Two acyclicity conditions are introduced in this context: one weaker acyclicity condition is necessary and sufficient for efficiency of the equilibrium outcomes of the game associated with TTC, while the other acyclicity condition is necessary and sufficient for efficiency of the outcomes associated with DA or IA. They also show that the equilibrium outcomes associated with DA are stable if and only if a priority profile is E-acyclic, while the equilibrium outcomes associated with TTC are stable if and only if a priority profile is K-acyclic.====This approach has also been adopted for other properties of rules or for more general school choice problems. For instance, Ehlers and Erdil (2010) allow for “weak” priority profiles so that a group of students may have the same priority in the ordering of a school. A condition that they introduce is necessary and sufficient for DA to be efficient or “consistent”. Kojima (2013), on the other hand, studies a variant of the problem where students may have multi-unit demands. DA is neither strategy-proof nor efficient in this setup. He introduces a condition called “essential homogeneity” and shows that it is necessary and sufficient for DA to be either strategy-proof or efficient.==== ====A list of various acyclicity conditions for priority profiles have been introduced to the literature, but it is difficult to use these conditions because it takes a long verification process to determine whether a priority profile satisfies them, even for a limited number of schools and students. Ergin (2002) thereby provides a characterization of his E-acyclicity and this result significantly improves visibility and verifiability of the condition. Such a result, however, does not exist for other acyclicity conditions.==== ==== Motivated by Ergin (2002)’s approach, we provide an analogous characterization of each condition and show how our results apply to a list of illustrative examples. These characterizations will be useful for the schools when they construct priorities, in particular, for a large number of students in practice. Our results also allow us to obtain direct and simplified proofs of the logical relations among the aforementioned conditions, previously identified by Kesten (2006), Haeringer and Klijn (2009), and Hatfield et al. (2016).====This paper is organized as follows: Section 2 introduces the model. Section 3 contains a list of restrictions and their characterizations. We conclude with an additional result for weak priorities in Section 4.",Acyclic priority profiles in school choice: Characterizations,https://www.sciencedirect.com/science/article/pii/S0304406820301130,12 November 2020,2020,Research Article,159.0
"Zhan Yang,Dang Chuangyin","Department of Management Science and Engineering, School of Management and Engineering, Nanjing University, Nanjing, China,Department of Systems Engineering and Engineering Management, City University of Hong Kong, Hong Kong, China","Received 12 September 2019, Revised 8 October 2020, Accepted 21 October 2020, Available online 10 November 2020, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jmateco.2020.10.006,Cited by (0)," techniques, and thus in the same framework as standard general equilibrium models. As a by-product, the existence of equilibrium is ensured for generic economies. Several computational examples demonstrate the effectiveness of the algorithm and show some quantitative features of equilibria in the model with default penalties.","Based on the standard model of general equilibrium with incomplete markets (GEI), Dubey et al. (2005) consider the mechanism of default and punishment, and build a broader model which incorporates the GEI model as a special case. In this model, households are allowed to default, bearing some punishments in utility functions. When the penalty rates are set to infinity, the model reduces to GEI. With the possibility of default, the equilibrium variables include anticipated delivery rates on assets. The asset markets are supposed to be competitive and pool all the buyers and sellers. At equilibrium, both commodity markets and asset markets are cleared, and the pool delivery rates are consistent with the fraction of promises that are actually delivered.====While this framework extends the scope of the GEI model and is more powerful in its ability to describe and analyze economic phenomena, it also brings significant difficulties in finding equilibria in such models. In the GEI model, the demand functions are not continuous at prices when the assets return matrix drops rank. Because of this discontinuity, equilibria may not exist, as shown in Hart (1975). This pathology is even accentuated by default. To avoid this difficulty in proving the existence of equilibrium, Dubey et al. (2005) need some bounds on asset sales.==== ==== Without these bounds, the existence can only be guaranteed if all the assets promise payoffs exclusively in the same good in each state, which is also a sufficient condition of equilibrium existence in the GEI model, see Geanakoplos and Polemarchakis (1986).====For GEI with generic parameters (endowments and asset payoffs), the existence of equilibrium is established in Duffie and Shafer (1985), by making use of Grassmannian manifolds. Since then, numerous computational attempts have been made for finding equilibria in the GEI model. DeMarzo and Eaves (1996) establish a continuous homotopy on the set of prices and the Grassmannian. The first algorithm without the use of the complex Grassmannian is developed by Brown et al. (1996), where a switching scheme is developed by allowing one household to introduce a new asset. It is shown that the discontinuity can be always (generically) avoided by switching to another homotopy. Although the scheme is effective theoretically, it would not be efficient numerically.==== ==== Schmedders (1998) develops a heuristic approach by adding to utility function a penalty term for transactions on the asset markets. Although this approach uses only a single smooth homotopy and is numerically efficient, its generic convergence has not yet been established. Recently, a smooth homotopy method is developed in Zhan and Dang (2020), which is shown to be efficient and can generically obtain an equilibrium. This can be done because the price vector is restricted in a smooth submanifold, rather than in the price simplex.====To obtain a better understanding of the impact of default and punishment on the behavior of economies with incomplete markets, it is necessary to compute equilibria in such models. However, to the best of our knowledge, there is no effective algorithm for finding equilibria in these economies.==== ==== Researchers often have to make very restrictive assumptions on economies to obtain closed-form solutions, such as symmetric households, only one commodity, and only one or symmetric asset. The purpose of this paper is to exploit the concept in Zhan and Dang (2020) and develop a homotopy path-following algorithm for computing equilibria in general equilibrium models with default and punishment.====We rewrite this model in a language that is consistent with the analysis for the GEI model, and define a delivery matrix which is a function of commodity prices and delivery rates. The pathological discontinuity happens when this delivery matrix drops rank. To avoid the singularity of the delivery matrix, we replace it with a new matrix, which is defined to have constant rank. We show that, with this replacement, the demand functions are continuous if the macro variables are restricted in a subset of the domains. Hence, a standard homotopy approach can be proposed for finding an equilibrium. The convergence of this homotopy method is ensured for generic economies, which naturally provides a constructive proof for the generic existence of equilibrium in the model with default penalties.====As shown in Dubey et al. (2005), although the default is socially costly, the benefits from permitting some default often outweigh all of the costs. To improve social welfare, the optimal default penalty is intermediate. One would naturally ask how harsh the default penalties should be. Our algorithm provides an effective technique in analyzing the model and answering this question.====The paper is organized as follows: Section 2 presents a basic description of the general equilibrium model with incomplete markets and default penalties. Section 3 introduces homotopy method and outlines the main ideas of our homotopy approach. In Section 4, the numerical performance of the proposed method is reported to further illustrate its effectiveness.",Determination of general equilibrium with incomplete markets and default penalties,https://www.sciencedirect.com/science/article/pii/S0304406820301117,10 November 2020,2020,Research Article,160.0
"Jiao Zhenhua,Shen Ziyang","School of Business, Shanghai University of International Business and Economics, Shanghai, 201620, China,Department of Economics, Columbia University, New York, NY 10027, USA","Received 23 February 2020, Revised 26 October 2020, Accepted 28 October 2020, Available online 10 November 2020, Version of Record 20 November 2020.",https://doi.org/10.1016/j.jmateco.2020.10.007,Cited by (1),"We propose a solution to the school choice problem with priority-based affirmative action. This solution is a special case of Kesten’s efficiency-adjusted deferred acceptance mechanism (henceforth, EADAM), specifically, we require that all minority students should give consent to priority waiving while none of the majority students consent. We formalize this solution as ==== (henceforth, EADAM====). While it is known that when all students consent, the EADAM is not responsive to the priority-based affirmative action, we show that the EADAM==== is ==== to the priority-based affirmative action. Inherited from the EADAM, the EADAM==== satisfies reasonable fairness but lacks strategy-proofness. We further show that there is no reasonably fair and strategy-proof mechanism that is also responsive to the priority-based affirmative action.","Both educators and economists are interested in how to design satisfactory school choice programs, which aim at giving students the opportunity to choose the school that they will attend. One of the central problems in school choice is that, among the students, there is a group of them who are usually under-represented and should be favored by the policies. This group of students is called ==== and those policies that are implemented to improve the minority students’ chances of attending their desired schools are called ====.====There are three popular types of affirmative action policies. The first one is the ==== affirmative action policy, which is implemented by limiting the number of admitted majority students at schools. The second one is the ==== policy, which is to reserve some seats at each school for the minority students and to require that a reserved seat at a school should be assigned to a majority student only if no minority student prefers that school to her assignment. Regarding the efficiency, fairness, and strategic issues, these two types of policies have been extensively studied by Abdulkadiroğlu and Sönmez (2003), Abdulkadiroğlu (2005), Bó (2016), Echenique and Yenmez (2015), Ehlers et al. (2014), Fragiadakis and Troyan (2017), Hafalir et al. (2013), and so on. The third type of policy is the ==== affirmative action policy, which favors minority students by means of promoting their priority ranking at schools. This type of policy is widely used in Chinese college admissions.====To measure how a matching mechanism performs in terms of a certain type of affirmative action policy, we use the notion of ====, which is first introduced by Kojima (2012) as ==== and is then formally defined by Doǧan (2016) as ====. A mechanism is ==== to some type of affirmative action if increasing the level of affirmative action does not result in a Pareto inferior assignment for the minority students. Since responsiveness only requires that a stronger affirmative action policy does not lead to perverse consequences, considering the purpose of affirmative action, it is a very weak requirement for an ideal matching mechanism. Besides, we still expect the mechanism to satisfy other desirable properties, including fairness, efficiency, and strategy-proofness.====For school choice problems, there are two popular matching mechanisms: the deferred acceptance (DA) mechanism and the top trading cycles (TTC) mechanism. Unfortunately, Kojima (2012) and Hafalir et al. (2013) show that, on the universal domain of school choice, neither DA nor TTC is responsive to any of the three types of affirmative action policies. Kesten (2010) proposes another matching mechanism for school choice problems, which is called the ==== (EADAM). Kesten (2010) shows that the EADAM weakly Pareto dominates the DA mechanism, and when all students give consent to priority waiving, it is Pareto efficient and satisfies a fairness property called ==== ==== which is weaker than ====. Tang and Yu (2014) further study the EADAM and introduce a simplified version of the EADAM which is outcome equivalent to the EADAM. However, when all students consent, the EADAM is not responsive to the priority-based affirmative action according to Jiao and Tian (2018).====In this paper, we focus on school choice problems with priority-based affirmative action and consider a special case of the Kesten’s EADAM as a solution. We call this solution ==== (EADAM====), that is, under EADAM====, we require that all of the minority students should give their consent to priority waiving while none of the majority students consent. In other words, the EADAM==== iteratively reruns the DA algorithm, and in each round, it first identifies the ====, and then for each of them, it modifies her preference by removing the corresponding school at which she is an interrupter from her preference list.==== ====In view of an insightful observation of Tang and Yu (2014),==== ==== it is easy to see that the last minority interrupters in the DA algorithm are constrained unimprovable. That is, conditional on not violating majorities’ priorities, the assignment of each last minority interrupter will remain unchanged with any Pareto improvement on the DA matching. This implies that it is reasonable to ask each minority student to waive her priority for the school at which she is a last minority interrupter.====Our main result is that, the EADAM==== is responsive to the priority-based affirmative action (Theorem 1). We also study the strategic properties of the EADAM==== and find that under the EADAM====, both types of students may have incentive to misreport their preferences (Example 4, Example 5).==== ==== Nevertheless, we cannot make a claim that the EADAM==== is bad because there is no reasonably fair and responsive (to the priority-based affirmative action) mechanism that can also achieve strategy-proofness (Theorem 2).====Next, we briefly illustrate our intuition. As pointed out by Jiao et al. (2020), the fact that DA is not responsive to the priority-based affirmative action is caused by the probable existence of new ==== in the DA procedure when there is a stronger priority-based policy. A rejection-cycle is a set of end-to-end ====, and a rejection-chain is a series of successive rejections which are initiated by the tentative acceptance of a ==== and ended by the rejection of a ====.==== ==== Specifically, we take a look at the following example.==== ====To avoid rejection-cycles, we consider the EADAM====. The EADAM==== iteratively reruns the DA algorithm, and in each round, last minority interrupting pairs of the previous round are identified. Roughly speaking, by iteratively selecting last minority interrupting pairs and modifying the preference profile accordingly, the EADAM==== finally rules out the existence of rejection-cycles, and hence can avoid leading to a Pareto inferior assignment for minority students when the level of affirmative action increases.====The rest of the paper is organized as follows: Section 2 presents the model. Section 3 introduces the EADAM====. Section 4 gives main results and Section 5 concludes the paper. All technical proofs are provided in the Appendix.",School choice with priority-based affirmative action: A responsive solution,https://www.sciencedirect.com/science/article/pii/S0304406820301129,10 November 2020,2020,Research Article,161.0
"Deng Shanglyu,Fu Qiang,Wu Zenan","Department of Economics, University of Maryland, 3114 Tydings Hall, 7343 Preinkert Dr., College Park, MD 20742, United States of America,Department of Strategy and Policy, National University of Singapore, 15 Kent Ridge Drive, Singapore, 119245, Singapore,School of Economics, Peking University, Beijing, 100871, China","Received 26 March 2020, Revised 14 October 2020, Accepted 15 October 2020, Available online 28 October 2020, Version of Record 21 November 2020.",https://doi.org/10.1016/j.jmateco.2020.10.004,Cited by (5),"This paper examines optimally biased Tullock contests. We consider a multi-player Tullock contest in which players differ in their prize valuations. The designer is allowed to impose identity-dependent treatments – i.e., multiplicative biases – to vary their relative competitiveness. The literature has been limited, because a closed-form solution to the equilibrium is in general unavailable when the number of contestants exceeds two, which nullifies the usual implicit programming approach. We develop an algorithmic technique adapted from the general approach of Fu and Wu (2020) and obtain a closed-form solution to the optimum that addresses a broad array of design objectives. We further analyze a resource allocation problem in a research tournament and adapt Fu and Wu’s (2020) approach to this noncanonical setting. Our analysis paves the way for future research in this vein.","In a contest, contenders sink irreversible effort or costly bids to vie for limited prizes, while their competitive outlays are nonrefundable regardless of the outcome. Such competitions can be exemplified by a plethora of examples, ranging from electoral competitions, lobbying, R&D races, and college admissions to sporting events. A voluminous economics literature has been developed to investigate contestants’ strategic behavior and the optimal design of contests for various goals.====This paper studies optimal contest design with contestants who differ in strength. Such heterogeneity affords the designer the flexibility to administer identity-dependent treatments that manipulate their relative competitiveness and bias the competition. Consider, for instance, the widespread practice of affirmative action in college admissions. Similarly, incumbent workers in firms are often ex ante preferred to external candidates when they compete for a vacancy.====We focus on the popularly adopted Tullock contest model and develop an algorithmic technique – which extends the general approach proposed by Fu and Wu (2020) – to solve for the optimally biased contest in closed form that addresses a wide variety of concerns. Imagine an asymmetric multi-player winner-take-all contest in which ==== contestants differ in their prize valuations. With effort entries ====, contestant ==== wins with a probability ====with ==== for all ====; the function ==== is typically labeled the ==== of the contest, and takes the form of ====
 ====, with ==== in Tullock settings. The set of weights ==== is a design variable or contest rule to be chosen by the designer prior to the competition.====The literature on optimally biased contests has typically focused on two-player settings and/or restricted design objectives. The conventional optimization approach requires an equilibrium solution of the contest under every possible set of biases ====. However, a solution is in general unavailable when three or more players are involved, except for the case of lottery contests, i.e., ====. Based on the equilibrium characterization of Stein (2002), Franke et al. (2013) make a pioneering contribution to solving for optimal biases that maximize total effort in a multi-player lottery contest that allows for ====. Fu and Wu (2020) introduce an alternative avenue for the optimization problem: They let the designer choose the equilibrium winning probability distribution as the design variable to maximize a more general objective function – with total effort maximization as a special case – and then show that every equilibrium winning probability distribution can be induced by a set of weights ====. This allows them to characterize the optimum without solving for the equilibrium.====Our paper extends Fu and Wu (2020) in two dimensions. First, they consider a general concave impact function ====, which limits their attention to the qualitative properties of the optimal contest. In contrast, we examine in depth a Tullock setting and develop an algorithm to obtain a closed-form solution to the optimal biases ==== and the set of active contestants – i.e., those who expend strictly positive efforts – in the optimum. This enables handy comparative statics of the optimum with respect to environmental factors. Second, we analyze a noncanonical setting for contest design that departs from the framework of Fu and Wu (2020), thereby expanding the scope of application for their approach.====We first construct a contest design problem in which the designer cares not only about total effort – which is commonly assumed in the contest literature – but also the selection efficiency and/or “closeness” of the competition.==== ==== Formally, she maximizes a convex combination of aggregate effort, the expected ability of the winner, and the variance of contestants’ equilibrium winning probability distribution. We detail an algorithm for the analysis, which paves the way for future studies of optimally biased Tullock contests.====We then analyze an optimal resource allocation problem in a research tournament à la Fullerton and McAfee (1999), which is strategically equivalent to a Tullock contest (see Baye and Hoppe, 2003, Fu and Lu, 2012). We allow a sponsor to split a fixed amount of productive resources – e.g., research funding – among firms to maximize the expected quality of the winning product. Lichtenberg (1990), for instance, documents the fact that extensive subsidies are provided by the United States Department of Defense (DoD) to assist private military technology firms that compete for defense procurement contracts.==== ==== The subsidy exemplifies a “technology-based” ( Kirkegaard, 2020) preferential treatment, as it not only varies firms’ relative competitiveness, but also affects the recipient’s actual productivity. This optimization problem differs from the setting delineated by Fu and Wu (2020), because the design variable – i.e., the resource allocation profile – influences actual output and thus directly enters the designer’s objective function.====The remainder of the paper is organized as follows: In Section 2, we describe the baseline model, lay out the optimal contest design problem, solve for the optimal contest, and present comparative statics. In Section 3, we extend the model to consider optimal resource allocation in R&D contests. Section 4 concludes the paper.",Optimally biased Tullock contests,https://www.sciencedirect.com/science/article/pii/S0304406820301099,28 October 2020,2020,Research Article,162.0
"Ramezanian Rasoul,Feizi Mehdi","Ferdowsi University of Mashhad, Iran","Received 3 February 2020, Revised 7 August 2020, Accepted 17 October 2020, Available online 26 October 2020, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jmateco.2020.10.005,Cited by (2),"We introduce a notion of efficiency, called ====, and prove that it coincides with a fairness notion of ====, in the sense of Harless (2018). We also prove that ==== implies ====, while it is not compatible with ====. Then, we provide an impossibility result which states that no mechanism meets ====, ====, and ====. Finally, we show that a modified eating algorithm satisfies ====.","An assignment problem seeks to allocate a finite set of indivisible objects (resources) to a set of agents with ordinal preferences. In such an assignment, the overall welfare of all agents is the basis for the usual approach to defining efficiency, while the fact that agents compare their allocation with others is the base to address fairness. Efficiency means that there is no alternative assignment that makes all agents (weakly) better off, while fairness indicates that each agent prefers his allocation to any others’ allocation. In other words, fairness is a criterion within two agents on whether one might envy the allocation of another while efficiency is criteria about all agents on whether at least some of them prefer to trade their allocation with each other.====While each of efficiency and fairness is possible to achieve separately, achieving them together is far from evident as they have been defined on different grounds. A well-known result of Varian (1974) shows that for the divisible setting, if the preferences of all agents are convex and strongly monotone, there always exists an allocation that is both fair, in the sense of envy-free (EF), and Pareto efficiency. Barman et al. (2018) exhibit that the fundamental result of Varian (1974) holds for indivisible goods with additive valuations. A relevant problem is which notions of fairness and efficiency are compatible in random assignments. Although envy-freeness is compatible with ex-post Pareto efficiency (e.g., the Probabilistic Serial (PS) mechanism of Bogomolnaia and Moulin (2001) is envy-free and ex-post efficient), it is trivial to show that envy-freeness could not guarantee ex-post Pareto efficiency. In this paper, we address this problem and construct a notion of efficiency which is always fair as well.====We introduce a notion of efficiency, called ====, in which no agent could increase his chance of getting some of his first best objects without decreasing the chance of other agents to receive some of their first best choices. We prove that SOE coincides with a strong fairness notion of ====, in the sense of Harless (2018), and implies ordinal efficiency (OE). The paper is organized as follows: In Section 2, we review the standard model of random assignment. In Section 3, we introduce SOE and examine its relationship with OE, Rank Efficiency (RE) in the sense of Featherstone (2020), and Interim Favoring Ranks (IFR) in the sense of Harless (2018). In Section 4, we show that a modified eating algorithm satisfies SOE.",Stepwise ordinal efficiency for the random assignment problem,https://www.sciencedirect.com/science/article/pii/S0304406820301105,26 October 2020,2020,Research Article,163.0
"Fontana Claudio,Runggaldier Wolfgang J.","Department of Mathematics “Tullio Levi - Civita”, University of Padova, Italy","Received 28 June 2020, Revised 19 September 2020, Accepted 13 October 2020, Available online 22 October 2020, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jmateco.2020.10.003,Cited by (2)," of asset pricing based on portfolio optimization arguments. By considering specifically a discrete-time setup, we simplify existing results and proofs that rely on semimartingale theory, thus allowing for a clear understanding of the foundational economic concepts involved. We exemplify these concepts, as well as some unexpected situations, in the context of one-period factor models with arbitrage opportunities under borrowing constraints.","The notions of arbitrage, market viability and state-price deflators are deeply connected and play a foundational role in financial economics and mathematical finance. Starting from the seminal works (Ross, 1977, Ross, 1978), the connections between these three concepts represent the essence of the fundamental theorem of asset pricing.==== ==== In frictionless discrete-time financial markets, if no trading restrictions are imposed, the appropriate no-arbitrage concept takes the classical form of absence of arbitrage opportunities (====). By the fundamental theorem of asset pricing of Harrison and Kreps (1979) and Harrison and Pliska (1981) (extended to general probability spaces in Dalang et al. (1990)), this is equivalent to the existence of an equivalent martingale measure, whose density acts as a state-price deflator. Moreover, always in the absence of trading restrictions, the results of Rásony and Stettner (2006) imply that no classical arbitrage is equivalent to market viability, intended as the solvability of portfolio optimization problems. No classical arbitrage thus represents the minimal economically meaningful no-arbitrage requirement for a frictionless discrete-time financial market.====In the presence of trading restrictions, these results continue to hold true as long as the set of constrained strategies is a cone, provided that equivalent martingale measures are replaced by equivalent supermartingale measures (see Föllmer and Schied (2016, Theorem 9.9) and Theorem 2.12). However, many practically relevant trading restrictions, such as borrowing constraints or the possibility of limited short sales, correspond to ==== constraints. In this case, as it will be shown below, market viability is no longer equivalent to no classical arbitrage, but rather to the weaker condition of ==== (NA====). Under convex trading restrictions, NA==== represents therefore the minimal economically meaningful concept of no-arbitrage and is equivalent to the existence of a ==== or, more generally, a ====.====The NA==== condition, introduced under this terminology in Kardaras (2010), corresponds to the absence of positive payoffs that can be super-replicated with an arbitrarily small initial capital and is equivalent to the no unbounded profit with bounded risk condition studied in the seminal work (Karatzas and Kardaras, 2007) (see also (Fontana, 2015) for an analysis of no-arbitrage conditions equivalent to NA====). In continuous-time, a complete theory based on NA==== has been developed in a general semimartingale setting starting with Karatzas and Kardaras (2007), also allowing for convex (non-conic) constraints. The connection between NA==== and market viability has been characterized in Choulli et al. (2015) in an unconstrained semimartingale setting (see also (Chau et al., 2017) for further results in this direction).====Scarce attention has, however, been specifically paid to NA==== in discrete-time models, despite their widespread use in economic theory. This is also due to the fact that, for discrete-time markets with conic constraints, there is no distinction between NA==== and no classical arbitrage (see Remark 2.3). To the best of our knowledge, the only works that specifically address discrete-time models by relying on no-arbitrage requirements weaker than no classical arbitrage are Elsinger and Summer (2001) and Korn and Schäl (2009). In a one-period model on a finite probability space, Elsinger and Summer (2001) show that limited forms of arbitrage may coexist with market equilibrium under convex constraints (see Remark 2.7 for a more detailed discussion). Closer to our setting, Korn and Schäl (2009) derive the central results of Karatzas and Kardaras (2007) on the numéraire portfolio in a one-period setting.====The present paper intends to fill this gap in the literature, in the framework of general discrete-time models with convex (not necessarily conic) constraints. Compared to Elsinger and Summer (2001) and Korn and Schäl (2009), we develop a complete theory of asset pricing based on NA====, also in the case of multi-period models with random convex constraints. We prove that market viability is equivalent to NA====, thereby showing that no classical arbitrage may pose unnecessary restrictions in the case of non-conic constraints. Building our analysis on this central result, we derive versions of the fundamental theorem of asset pricing, study the valuation of contingent claims and discuss non-trivial examples of our theory in the context of general factor models. We make a systematic effort to provide direct and self-contained proofs based on portfolio optimization arguments. The simplicity of the discrete-time structure allows for a clear understanding of the economic concepts involved, avoiding the technicalities of the continuous-time semimartingale setup.====The paper is divided into three sections, whose contents and contributions can be outlined as follows. In Section 2, we consider a general one-period setting. Extending the analysis of Korn and Schäl (2009), we prove the equivalence between NA==== and the solvability of portfolio optimization problems (market viability), thus establishing the minimality of NA==== from an economic standpoint. This enables us to obtain a direct proof of the characterization of NA==== in terms of the existence of the numéraire portfolio or, more generally, a deflator. We show that NA==== leads to a dual representation of super-hedging values and a characterization of attainable claims, and permits to rely on several well-known hedging approaches in constrained incomplete markets, even in the presence of arbitrage opportunities. Besides its pedagogical value, the one-period setting introduces several techniques that will be important for the analysis of the multi-period case.====Section 3 illustrates the theory in the context of factor models with borrowing constraints. We introduce a general factor model, where a single factor is responsible of potential arbitrage opportunities. In this setting, the NA==== condition and the set of arbitrage opportunities admit explicit descriptions in terms of the factor loadings. When NA==== holds but no classical arbitrage does not, we show the existence of a maximal arbitrage strategy. These results can be easily visualized in a two-dimensional setting, which enables us to provide examples of situations where, despite the existence of arbitrage opportunities, it is not necessarily optimal to invest in them. The analysis of this section clarifies the interplay between the support of the asset returns distribution, their dependence structure and the borrowing constraints.====Finally, Section 4 generalizes the central results of Section 2 to a multi-period setting with random convex constraints. We derive several new characterizations of NA====, showing that it holds globally if and only if it holds in each single trading period, and prove its equivalence to market viability. The most general result on the solvability of portfolio optimization problems in discrete-time was obtained in Rásony and Stettner (2006), relying on no classical arbitrage. Our Theorem 4.5 extends this result by introducing trading restrictions and weakening the no-arbitrage requirement to the minimal condition of NA==== (in turn, our proofs of Theorem 2.5, Theorem 4.5 are inspired from Rásony and Stettner (2006)). By generalizing the one-period analysis, we then give an easy proof of the equivalence between NA====, the existence of the numéraire portfolio and the existence of a supermartingale deflator, for general discrete-time models with random convex constraints.====We close this introduction by briefly reviewing some related literature, limiting ourselves to selected contributions that are specifically connected with the present discussion. Relying on the concept of no classical arbitrage, the fundamental theorem of asset pricing with constraints on the amounts invested in the risky assets is proved in Pham and Touzi (1999) in the case of conic constraints (see also (Köehl and Pham, 2000, Pham, 2000) for valuation and hedging problems in that setting) and in Brannath (1997) in the case of convex constraints. The specific case of short-sale constraints is treated in the earlier work (Jouini and Kallal, 1995). General forms of conic constraints have been considered in Napp (2003), extending the analysis of Pham and Touzi (1999). In the case of convex constraints on the fractions of wealth invested, as considered in the present work, versions of the fundamental theorem of asset pricing based on the usual notion of no classical arbitrage are given in Carassus et al. (2001), Evstignveev et al. (2004) and Rokhlin (2005). In comparison to the latter contributions, we choose to work with the weaker concept of NA====, due to its equivalence to market viability. In an unconstrained setting, the connection between no classical arbitrage and market viability is studied in Rásony and Stettner (2005) and Rásony and Stettner (2006), generalized in Nutz (2016) under model uncertainty. In the presence of model uncertainty and convex portfolio constraints, Bayraktar and Zhou (2017) prove a version of the fundamental theorem of asset pricing based on a robust generalization of the notion of no classical arbitrage. Finally, we mention the recent work Baptiste et al. (2019), where super-hedging has been studied under a weak no-arbitrage condition, called absence of immediate profits. However, the latter condition does not suffice to ensure market viability.",Arbitrage concepts under trading restrictions in discrete-time financial markets,https://www.sciencedirect.com/science/article/pii/S0304406820301087,22 October 2020,2020,Research Article,164.0
"Krishna R. Vijay,Sadowski Philipp","Florida State University, United States of America,Duke University, United States of America","Received 8 February 2017, Revised 3 August 2020, Accepted 30 September 2020, Available online 10 October 2020, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jmateco.2020.09.007,Cited by (0),We consider a ,"While taste is often modeled as a stable trait of the individual decision maker, tastes do evolve over time in many instances. For example, risk aversion tends to change over time (see Bekaert et al., 2010 and the references therein.) Accounting for evolving tastes, or taste shocks, is important in dynamic models of choice in macroeconomics and applied microeconomics, where data is usually noisy. The literature makes various assumptions about such taste shocks, including serial correlation. However, Magnac and Thesmar (2002) find that dynamic models with serially correlated (unobservable) taste shocks cannot be identified based on discrete choice data alone.====We consider a decision maker who perceives a particular type of Markov process that governs the evolution of his tastes, that is, his current taste is a sufficient statistic for his current beliefs over future tastes. We then analyze initial preferences over Infinite Horizon Consumption Problems as introduced in Gul and Pesendorfer (2004) (henceforth GP) where, in every period, choice is between lotteries over current consumption and a continuation choice problem for the next period. Theorem 1 fully characterizes the behavioral implications of our model for this dynamic choice data and shows that it is the appropriate data for full identification.====Consider, then, the dynamic behavior of a forward looking decision maker, who is aware that his tastes may change, and who embraces the future changes in tastes, in the sense that he evaluates future consumption based on his expectation of future consumption tastes. On the one hand, if tastes evolve randomly, then he prefers not to commit to consumption choice in advance. On the other hand, if tastes are correlated between subsequent periods, then the reluctance to commit will be reduced as the time of consumption draws nearer. That is, the decision maker prefers to delay necessary commitment.==== ====Importantly, however, the Markovian structure of the process that governs the evolution of tastes means that he is willing to commit to a continuation problem for the next period, ==== on the current taste. While this taste is not observable by the analyst, we argue that this willingness to commit should also hold contingent on current consumption ==== from a large enough menu. Our key novel axiom, ====, formalizes this notion.====Krishna and Sadowski (2014) (henceforth KS) model a decision maker in a dynamic environment who chooses over acts on an objective space of states of the world, and who has stable but noisy state-contingent tastes. A common special case of their model and ours features a stable and state independent underlying taste that is perturbed by iid noise (or transient taste shocks). Axiom N.1 relaxes their notion of unconditional ====.====Our other axioms, while many, are quite standard. Our ==== Axioms S.1–S.4, in particular, are well studied in the menu choice literature, and are independent of whether the problem is static or dynamic. Most of our ==== Axioms D.1–D.4 are discussed in KS. Those dynamic axioms are necessary for ==== model of dynamic preferences that is recursive, stationary, and Markovian. In addition, we impose a structural condition, ==== (Axiom N.2,) that ensures that the subjective states in the representation are neither transient nor absorbing.====According to Theorem 1, the Markov process that governs the evolution of tastes over time in our representation is uniquely identified from first period preferences, as is the only other preference parameter, the discount factor. To further explore the tight connection between observable behavior and the correlation of consumption tastes over time, note that for correlated tastes, knowledge of the taste at one point will reduce an individual’s uncertainty about future tastes. In that case, the individual’s aversion to commit to a consumption choice should decrease between one period and the next, more so the more correlated tastes are over time. For example, an investor will be less averse to commit to a more or less risky portfolio (say by accepting a penalty for reallocating his funds) given his current risk aversion, if his risk aversion is strongly correlated over time. This is independent of ex-ante uncertainty about the level of risk aversion. Theorem 2 provides comparative statics that formalize this intuition.",Randomly evolving tastes and delayed commitment,https://www.sciencedirect.com/science/article/pii/S030440682030104X,10 October 2020,2020,Research Article,165.0
"Braitt Milton dos Santos,Torres-Martínez Juan Pablo","Department of Mathematics, Federal University of Santa Catarina, Brazil,Department of Economics, Faculty of Economics and Business, University of Chile, Chile","Received 1 July 2019, Revised 1 September 2020, Accepted 2 September 2020, Available online 12 September 2020, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jmateco.2020.09.003,Cited by (2),"In matching problems with externalities, ==== measures the importance an agent gives to others’ potential reactions when she considers deviating and ","Externalities are a recurrent feature in many situations that can be modeled as matching problems. For instance, firms that consider the organizational structure of an industry before deciding to integrate; markets where household formation can impact prices through its effects on aggregate demand; or students internalizing peer effects on preferences to choose among ==== identical courses. A simple way to include externalities in the classical matching models introduced by Gale and Shapley (1962), ==== and ====, is to assume that individual preferences are defined over the set of admissible matchings instead of the set of potential partners.====In the presence of externalities, the welfare of an agent can be influenced by the characteristics of other couples formed. Thus, it is natural to assume that when an agent evaluates a deviation from a matching she considers the reactions of others: specifically, an agent will want to block a matching, by forming a new pair or choosing to be alone, only when her welfare is improved in any scenario compatible with her ==== about potential reactions. These beliefs can be heterogeneous and may depend on the matching from which deviation occurs. For instance, an ==== agent considers all the possible reactions of other individuals before implementing a deviation, while a ==== agent disregards them. In any case, two relevant pieces of information are incorporated in the beliefs of any individual: the share of potential reactions that she considers when evaluating a deviation – a measure of her ==== – and the number of potential reactions that other individuals may implement—a measure of their ====.====Sasaki and Toda, 1986, Sasaki and Toda, 1996 study the marriage problem, where social connectedness is intrinsically high, and show only in the case of extreme prudence are does a stable matching exist.==== ====
 Contreras and Torres-Martínez (2020) study the roommate problem, where social connectedness is intrinsically high, and show that extreme prudence may be insufficient to ensure the existence of a stable matching. Thus, it is intuitive that a matching problem may only have a stable matching when both social connectedness and prudence are high.====This work aims to deepen our understanding of matching problems with externalities, focusing on the role of ==== and ==== in stability. As pointed out above, prudence refers to the importance given to potential reactions by those who evaluate a deviation, while social connectedness quantifies the capacity to react to such deviations. Matching problems with externalities are modeled in such form that feasible pairs can be exogenously restricted. Thus, it is possible to capture barriers that are independent of preferences, such as the banning of same-sex couples in marriage markets or the limitation of geographical distance in school choice problems. In this context, each agent is characterized by a strict preference relation over the set of admissible matchings and by beliefs about other agents’ reactions to deviations. As usual, a matching is stable when there are no profitable bilateral or unilateral deviations. Note that, classical models with externalities are included as particular cases: roommate problems, when all pairs are feasible; marriage markets, when the population is divided into two groups and any pair composed of agents of different groups is feasible; or frameworks with extremely prudent or myopic agents.====Assuming that externalities are arbitrary (in the sense that agents may rank matchings in any order) and individuals’ preferences are independent and randomly determined, it is shown that the conflict between externalities and stability may disappear as the population grows. Indeed, asymptotic stability – a property that never holds in the absence of externalities – is achieved even in situations where social connectedness is low and prudence vanishes. In particular, for roommate problems and marriage markets with the same number of men and women (frameworks where social connectedness is relatively high), the set of preference profiles where the existence of a stable matching fails shrinks towards measure zero as the population grows, provided there is a minimal (potentially vanishing) amount of prudence. It is also shown that those who ignore the reactions of other individuals have a destabilizing effect on society: the probability of a matching being stable converges to zero as the number of myopic agents increases without bound.====More precisely, inspired by the works of Knuth (1976) and Pittel, 1989, Pittel, 1993 on classical one-to-one matching problems, it is shown that the probability of a matching being stable can be written as an integral with respect to the distribution of preference profiles. This ==== is an increasing function of agent-level measures of prudence and social connectedness, confirming that these attributes play a relevant role in stability when externalities are arbitrary (see Section 3).====In addition, a non-trivial lower bound for the probability of stability can be deduced from the integral formula for the probability of a matching being stable (see Section 4). This lower bound depends on global measures of prudence and social connectedness, allowing us to determine a sufficient condition for asymptotic stability: the product of the measures of prudence and social connectedness must grow at least exponentially. Therefore, when the share of the population that an individual can pair is bounded away from zero, asymptotic stability holds if either prudence is bounded away from zero or decays at most exponentially (see Section 5). These results are in sharp contrast to the findings of Pittel, 1989, Pittel, 1992 for classical marriage markets and Pittel (1993) for roommate problems. Indeed, without externalities, and when individuals’ preferences are independent and arbitrary, the probability of a matching being stable converges to zero as the population grows.====It is also shown that the solvability problems found by Sasaki and Toda, 1986, Sasaki and Toda, 1996 and Contreras and Torres-Martínez (2020) can be strongly mitigated as the population grows. In fact, since the ==== of a matching problem with externalities is greater than the probability of stability of any admissible matchings, it converges to one even in scenarios where prudence vanishes and social connectedness is low (see remarks at the end of Sections 4 The role of prudence and social connectedness in stability, 5 Main results of asymptotic stability).====However, those who ignore other individuals have a destabilizing effect on society, independently of the percentage of the population that they represent. Indeed, for roommate problems and marriage markets, if the number of myopic agents increases without bound, then the probability of a matching being stable has an upper bound that vanishes as the population grows (see Section 6). Intuitively, for any matching, the probability of stability is less than or equal to the probability that there are no blocking pairs among myopic agents, and the latter probability vanishes as the population of this type of agent grows.====It is important to remark that asymptotic stability may not hold when externalities are not arbitrary or individuals’ preferences are not independent of each other. For instance, when externalities are a secondary factor in the determination of individuals’ preferences, the probability of a matching being stable converges to zero as the population grows (see Remark 1). Furthermore, when preferences are correlated, some types of matchings might be always unstable, independently of the size of the population. However, when the correlation of preferences is not too high, it is possible to give examples where asymptotic stability holds for some subsets of admissible matchings (the size of this sets shrink as the correlation grows). Therefore, the probability of solvability still converges to one as the population grows. Unfortunately, it is difficult to go beyond examples without compromising the tractability of the model (see Section 7).====The rest of the paper is organized as follows. In Section 2 the characteristics of a matching problems with externalities are described, while Section 3 is devoted to the deduction of the integral formula for the probability of a matching being stable. Sections 4 The role of prudence and social connectedness in stability, 5 Main results of asymptotic stability analyze the effect of prudence and social connectedness on (asymptotic) stability. In Section 6 the destabilizing effect of myopic agents is discussed. Finally, Section 7 illustrates how the results of the paper may change when preferences are correlated. Some proofs are left in a final appendix.",Matching with externalities: The role of prudence and social connectedness in stability,https://www.sciencedirect.com/science/article/pii/S0304406820300999,12 September 2020,2020,Research Article,166.0
"Baik Kyung Hwan,Jung Hanjoon Michael","Department of Economics, Sungkyunkwan University, Seoul 03063, South Korea,Ma Yinchu School of Economics, Tianjin University, Tianjin 300072, China","Received 20 July 2019, Revised 27 May 2020, Accepted 26 June 2020, Available online 6 August 2020, Version of Record 19 January 2021.",https://doi.org/10.1016/j.jmateco.2020.06.004,Cited by (3),"We study contests in which there are multiple alternative public-good/bad prizes, and the players compete, by expending irreversible effort, over which prize to have awarded to them. Each prize may be a public good for some players and a public bad for the others, and the players expend their effort simultaneously and independently. We first prove the existence of a pure-strategy ==== of the game, then establish when the total effort level expended for each prize is unique across the Nash equilibria, and then summarize and highlight other interesting and important properties of the equilibria. Finally, we discuss the effects of heterogeneity of valuations on the players’ equilibrium effort levels and a possible extension of the model.","Common are contests in which there are multiple alternative public-good/bad prizes, only one of which will be awarded to a society of players; and the players compete, by expending irreversible effort, over which prize to have awarded to them by a decision maker.==== ==== Naturally, in such contests, each player has a valuation for each prize.====Examples of such contests are contests in which there are multiple alternative industrial policies, environmental policies, or trade policies to affect a group of firms, and the firms compete over which policy to have adopted by the government. In these contests, some of the firms may get benefits from the adopted policy, and others may be harmed by it. This means that each of the policies can be viewed as a public-good/bad prize for the firms. Another example is a contest in which there are multiple alternative economic policies to affect all the member countries in the European Union, and the member countries compete over which economic policy to have adopted by the Union. Yet another example is an election contest in which there are several presidential candidates, and lobbyists or rent seekers compete, by making contributions to the candidates’ election campaign, over which candidate to have elected. No doubt, the election result affects all the rent seekers.====Facing contests like the motivational examples above, we may well pose the following interesting questions. For which prizes do the players expend positive effort? How many prizes are there for which the players expend positive effort? Who expends positive effort? How many players are there who expend positive effort? How severe is the free-rider problem? What factors determine the effort levels expended by the players? Is there any player who expends positive effort for more than one prize?====Accordingly, this paper models a contest involving multiple alternative public-good/bad prizes as a strategic game, and addresses those interesting questions. It formally considers a game in which each player’s valuations for the prizes are publicly known, and the players choose their effort levels for the prizes simultaneously and independently.====This paper first proves the existence of a pure-strategy Nash equilibrium of the game. Then, it identifies cases where the total effort level expended for each prize is unique across the pure-strategy Nash equilibria.====In addition, this paper establishes the following interesting and important properties of the Nash equilibria. First, there are at least two prizes for which the players expend positive effort. Second, there are at least two players who expend positive effort. Third, if there are just two prizes in total, then each player never expends positive effort for both prizes. However, if there are more than two prizes, then some player may expend positive effort for more than one prize. Fourth, each player expends zero effort for every prize that does not give him the highest valuation; furthermore, he may expend zero effort for some or all of the prizes that give him the highest valuation. Fifth, if there are just two prizes, then a player whose valuation spread – that is, the difference between his valuations for the two prizes – is narrower than somebody else’s expends zero effort for both prizes and free rides; furthermore, a player whose valuation spread is the widest may expend zero effort for both prizes. Sixth, a player with the highest valuation for a prize (among all the players) may expend zero effort for that prize. Finally, a player with negative valuations for all the prizes may expend positive effort for some prize or prizes.====This paper is closely related to the literature on contests with identity-dependent externalities: See, for example, Linster (1993), Funk (1996), Jehiel et al. (1996), Esteban and Ray (1999), Das Varma (2002), Aseff and Chade (2008), Brocas (2013), and Klose and Kovenock, 2015a, Klose and Kovenock, 2015b. These papers study contests in which each player’s valuation for a private-good prize depends on who is selected as the winner, and each player may have a nonzero valuation for the prize even in case of his losing it. For example, Linster (1993) considers ====-player rent-seeking contests in which each player’s valuations for the single prize are represented by an ====-tuple vector, and each player’s contest success function is specified by the simplest logit-form function. Klose and Kovenock (2015b) consider ====-player all-pay auctions in which each player’s valuation for the single prize may depend on the identity of the winner, so that his valuations are given by an ====-tuple vector, and the winner is determined by the all-pay-auction selection rule.====The current paper differs from those papers in three ways. First, there are multiple prizes in the current paper, only one of which is to be awarded to the players, whereas there is a single prize in those papers. Second, each prize is a public-good/bad one in this paper, whereas the single prize is a private-good one in those papers. Third, this paper uses a general selection probability function (for each prize), which is different from the contest success functions (for the players) used in those papers.====There exist many papers which study contests with a group-specific public-good prize — that is, contests in which groups of players compete to win a prize to be awarded to a single group, and the prize is a public good only within the winning group. Examples include Katz et al. (1990), Baik, 1993, Baik, 2008, Baik and Shogren (1998), Baik et al. (2001), Epstein and Mealem (2009), Lee (2012), Kolmar and Rommeswinkel (2013), Chowdhury et al. (2013), Topolyan (2014), Barbieri et al. (2014), Chowdhury and Topolyan, 2016a, Chowdhury and Topolyan, 2016b, Barbieri and Malueg (2016), Chowdhury et al. (2016), and Dasgupta and Neogi (2018). In these papers, the number of groups and their sizes are exogenously given. These papers examine, among other things, the free-rider problem and the group-size paradox.====The model in the current paper strikingly differs from the ones in those papers in three respects. First, unlike in those papers, there are no groups (except the entire society of players) in this paper — that is, the society of players is not partitioned into groups. Second, there are multiple alternative prizes in this paper, only one of which is to be awarded to all of the players, whereas there is a single prize in those papers, which is to be awarded to a single group. Third, in this paper, each prize is a public good/bad for the players – precisely, it may be a public good for some players and a public bad for the others – whereas, in those papers, the single prize is a public good only within the winning group.====The current paper is closely related to Baik (2016). He studies contests in which there are two alternative public-good/bad prizes, and players compete over which prize to have awarded to or selected for them by a decision maker. The current paper differs from Baik (2016) in three respects. First, the current paper generalizes his model by not restricting the number of alternative public-good/bad prizes to two. Second, the current paper formally proves the existence of a pure-strategy Nash equilibrium in the case where there are multiple alternative public-good/bad prizes, whereas Baik (2016) proves its existence by constructing pure-strategy Nash equilibria in the case where there are only two public-good/bad prizes. Third, unlike Baik (2016), the current paper identifies cases in which the total effort level expended for each prize is unique across the pure-strategy Nash equilibria.====The rest of the paper is organized as follows. Section 2 presents a model and sets up a simultaneous-move game. In Section 3, we prove the existence of a pure-strategy Nash equilibrium of the game. In Section 4, we identify cases in which the total contribution or total effort level made for each prize is unique across the pure-strategy Nash equilibria. Section 5 summarizes and highlights other interesting and important properties of the pure-strategy Nash equilibria. Section 6 discusses the effects of heterogeneity of valuations on the players’ equilibrium effort levels and a possible extension of the model. Finally, Section 7 offers our conclusions.",Contests with multiple alternative prizes: Public-good/bad prizes and externalities,https://www.sciencedirect.com/science/article/pii/S0304406820300768,6 August 2020,2020,Research Article,167.0
"Boucekkine Raouf,Carvajal Andrés,Chakraborty Shankha,Goenka Aditya","Aix-Marseille Université, France,University of California, Davis, United States of America,EPGE-FGV, Brazil,University of Oregon, United States of America,University of Birmingham, United Kingdom","Available online 18 February 2021, Version of Record 9 March 2021.",https://doi.org/10.1016/j.jmateco.2021.102498,Cited by (13),None,"In December 2019, medical professionals hypothesized that clusters of a new respiratory disease were developing in the Chinese city of Wuhan. On January 3rd, 2020, the World Health Organization confirmed this hypothesis, as 44 cases were diagnosed as SARS-CoV-2 infections, the cause of the COVID-19 disease. Despite multiple measures to contain it, the virus spread beyond China’s borders, and in January, the first case outside the country was confirmed in Thailand. By the end of the month, around 8000 cases and 170 deaths had been confirmed in different countries. By mid-July, the need for ICU beds in the Italian region of Trento tripled their availability. At the time of writing this article, 13 months after the first confirmed cases of the disease, there have been more than 102 million cases, and while nearly 75 million patients have recovered from COVID-19, over 2,2 million people have succumbed to it.====On March 11th, 2020, the World Health Organization declared COVID-19 a global pandemic. Soon after, virtually every country in the world had implemented packages of policies intended to contain the disease. While these measures varied widely in form, intensity, and effectiveness,==== it would be challenging to overstate how fast, extensive and widespread the disruptions to socio-economic activities have been.====In the first quarter of the year, GDP had contracted by more than 5% in France, Spain, and Italy.==== By April 4th, unemployment insurance claims in the US had increased 30-fold, to over six million cases in just one week.==== For 2020, the World Bank estimates that the overall contraction in economic activity was around 10% in the European Union, 9% in India, and nearly 8% in the US.====Across countries, there has been some variation in the magnitude of the pandemic’s impact on macroeconomic variables. On the other hand, across households in a given country, the dispersion in the impacts is much broader. Two months after the first lock-down was effected in the UK in late March, 45% of individuals had experienced earning losses of at least 10%, but a more disaggregated analysis shows that for households in the first quintile of income, those losses were at least 60% for a quarter of them, while the same indicator was less than 2% for half of the households in the fifth quintile.==== And across sectors of economic activity, the variability in the impact of the pandemic is similarly wide: In the US, four weeks after the first human-to-human contagion was reported on January 30th, household demand for dried beans had increased by 37%, and job openings for interpreters and translators had increased three-fold====; three weeks later, sales at restaurants had decreased by 47%, and 70% of the restaurants had laid off employees, while demand for childcare workers had contracted by 36%.==== In their most ==== scenario, ==== predict that over 160 million jobs will be lost worldwide in the tourism sector.====Of course, non-economic variables are as important, if not more. One of the most widely implemented policy responses to the pandemic was the closure of schools. At its peak, in late April of 2020, 130 countries had closed their schools, affecting a total of 1.6 billion learners.==== In addition to the long-term costs of this disruption,==== there were immediate social consequences. In the US, for instance, 30 million children immediately lost access to school meals, and while programs were put in place to mitigate the severity of this situation, they mostly addressed the problem only during weekdays, leaving “millions of children hungry when schools aren’t in session”.==== In Senegal, less than 1% of school-age children engaged in on-line courses,==== while Bolivia and Kenya canceled the entire school year.==== In India, COVID-19 was found to increase pre-eclampsia and spontaneous abortion.==== In Ghana, pregnant women reported skipping prenatal care due to concerns about the disease,==== and a study on 118 low- and middle-income countries estimates that the decrease in perinatal care could cause over 1 million additional child deaths over a six-month period.==== In Peru, while a significant reduction in homicide and suicide was observed,==== the incidence of reported cases of intimate partner violence increased by 48%.====In the fall of 2020, the UK identified a new variant of SARS-CoV-2 and hypothesized that it is more deadly than the original virus. Another variant emerged independently in South Africa in October, and a third variant, originated in Brazil, was detected in Japan in early 2021.==== The three versions seem to be more contagious than the original virus.====On 31 December 2020, the World Health Organization granted emergency use approval to the ==== vaccine, which had shown an efficacy of 95% against COVID-19 on primary clinical testing. As of late January 2021, ten COVID-19 vaccines have been developed and are in use.==== Considering only the three vaccines whose companies have officially announced production plans,==== 5.3 billion doses will be made available during 2021.====That treatment for COVID-19 has been discovered so quickly after the disease first appeared is due primarily to the social recognition of the importance of science and scientific collaboration and communication.==== It is in this spirit that the ==== is publishing this special issue on The Economics of Epidemics and Contagious Diseases. The articles that follow address multiple topics where economic theory may help understand the evolution of a pandemic, the policies that may best deal with it, and its socio-economic implications.====For the sake of organization, the articles are presented in five thematic sections. The first section contains four pieces in the intersection of epidemiology, economics, and mathematics; they help us understand the spread of contagious diseases, even in the presence of lock-down policies. The five papers in the second section take a more microeconomic approach to the same topic, emphasizing the role that individual actions and decisions play in transmitting the disease. The next two sections answer the question of what the optimal response policy is. First, four papers address the problem from the point of view of containment policies, in Section ====, while the two of Section ==== tackle the issue from the perspective of health systems management. The volume concludes with some exploration of the macroeconomic implications of a pandemic—for the distribution of wealth, the value of economic assets, the location decisions of households, and the environment.",The economics of epidemics and contagious diseases: An introduction,https://www.sciencedirect.com/science/article/pii/S0304406821000367,18 February 2021,2021,Research Article,173.0
Noda Shunya,"Vancouver School of Economics, The University of British Columbia, 6000 Iona Drive, Vancouver, BC, V6T 1L4, Canada","Available online 4 December 2020, Version of Record 4 December 2020.",https://doi.org/10.1016/j.jmateco.2020.11.005,Cited by (0),None,None,Corrigendum to “Size versus truncation robustness in the assignment problem” [J. Math. Econ. 87 (2020) 1–5],https://www.sciencedirect.com/science/article/pii/S0304406820301269,4 December 2020,2020,Research Article,175.0
"Bambi Mauro,Gozzi Fausto","Durham University, Department of Economics and Finance, Durham, UK,LUISS University, Department of Economics and Finance, Roma, Italy","Received 5 May 2020, Revised 28 August 2020, Accepted 28 September 2020, Available online 16 October 2020, Version of Record 13 November 2020.",https://doi.org/10.1016/j.jmateco.2020.09.008,Cited by (7),"In a very influential model with internal habits, Carroll et al., (2017, 2000), establish that an increase in economic growth may cause a positive change in savings. The ","In a couple of very influential works, Carroll et al., 1997, Carroll et al., 2000 have studied the dynamics of an endogenous growth model with internal habits formation and they have shown that internal habit formation is a crucial channel to explain how an increase in economic growth may cause an increase in savings. This result was even more relevant since it was derived using a “multiplicative” form of habits instead of a “subtractive” form, whose drawbacks include an addictive behavior since consumption has to remain always higher than the habit stock and possibly an infinite negative utility (see Carroll, 2000).====Since then, several authors have observed how “multiplicative” habits can be used to enhance the explanatory power of their models in several dimensions. For example, Fuhrer (2000) has observed that habit formation improves significantly the effects of spending and inflation to monetary-policy actions; Diaz et al. (2003) have studied the role of habit formation in shaping precautionary savings and the wealth distribution in economies with heterogeneous agents; finally, multiplicative habits have been introduced also in portfolio choice models, see for example Gomes and Michaelides (2003) among others.==== ====However, other authors have recently questioned the optimality of the result found by Carroll et al. and, as a consequences, the optimality of all the previously mentioned contributions. In fact, it was observed that Carroll et al.’s result, as well as all the other results, was obtained in a parameters’ set, let us call it ====, where the multiplicative utility function is never jointly concave in consumption and habits. In particular, they have noticed that joint concavity is never possible under the (realistic) assumptions of a greater than one coefficient of relative risk aversion, i.e. ====, and of agents weighting habits less than consumption, i.e. ==== (e.g. Alonso-Carrera et al., 2005, page 1669, and more recently Yang and Zhang, 2018).==== ====As a consequence, the sufficiency conditions for optimality are not satisfied;==== ==== as clearly explained by Seierstad and Sydsaeter (see page 103 in Seierstad and Sydsaeter, 2002) the sufficiency conditions for optimality are crucial because “(...) the Maximum Principle cannot by itself tell us whether a given candidate is optimal or not, nor does it tell us whether or not an optimal solution exists”. In other words, the solution candidate could be either a maximum or a minimum or neither of them.====In addition, it is even not clear, without the strict concavity of the objective function, if the optimal control is unique or not (e.g. Acemoglu, 2009, Theorem 6.4 and Corollary 6.1, page 189–190). Other candidate optimal controls cannot be excluded only looking at the analysis done in Carroll et al., 1997, Carroll et al., 2000, because the authors have investigated the ==== around the steady state but not the global dynamics. Interestingly, there are several examples in the literature of models without a strictly concave Hamiltonian and multiple optimal controls, e.g. Dechert and Nishimura (1983) and Kamihigashi and Roy (2007) among others. Actually (Alonso-Carrera et al., 2005) show that in a variation of the model with multiplicative habits originally proposed by Carroll et al. non-monotonic and discontinuous consumption paths are consistent with the necessary conditions for optimality.==== ==== Such a finding was at odd with the smooth and monotonic consumption path found by Carroll et al. and reinforced the suspect that multiple optimal paths could actually exist.====In other words, Carroll et al., 1997, Carroll et al., 2000, by looking at the first order conditions from the Maximum Principle, have found a ==== optimal control but they have fallen short of proving that this candidate is indeed the unique optimal control. As previously explained, the candidate could be either a maximum or a minimum or neither of them.==== ==== Clearly, this is worrying especially because the model proposed by these authors have been extensively used and enriched by other authors and even worse some of these contributions give policy advices without actually knowing if the solution of their models is the unique optimum.====A simple way to restore optimality and uniqueness consists in imposing conditions on the parameters such that the utility function becomes strictly concave (e.g. Yang and Zhang, 2018). Unfortunately, for a coefficient of relative risk aversion greater than one, i.e. ====, concavity is restored only under the unattractive and rather implausible assumption that agents care about the habits more than about consumption, i.e. ====. Mostimportantly, under this assumption, Carroll et al.’s result on the relation between economic growth and saving does not hold anymore. It would be, therefore, important to understand whether optimality could be achieved even without this strong and implausible assumption.====In this short paper, we revisit this issue using a Dynamic Programming approach. The advantage of Dynamic Programming is that it investigates the ==== and, most importantly, it identifies ==== and it provides the optimal strategies in feedback form (see among others Salsa and Squellati, 2006, Bardi and Capuzzo-Dolcetta, 2008 Chapter III, section 2.5, Fabbri et al., 2017 section 2.5). Therefore, using a Dynamic Programming approach seems a very natural choice in this context.==== ====However, the application of the Dynamic Programming approach to our specific problem is not straightforward because it requires a non trivial adaptation of the existing literature on optimal control and viscosity solutions.==== ==== Although this literature was developed in the last four decades starting with the seminal work of Crandall and Lions, 1981, Crandall and Lions, 1983,==== ==== its application to macroeconomic problems is more recent (see e.g. the recent contribution by Achdou et al., 2017).====In our framework, this theory can be applied after taking the homogeneity properties of the problem into account (see e.g. Freni et al., 2006, Freni et al., 2008). Once this is done, we use the theory of viscosity solutions for the associated Hamilton–Jacobi–Bellman equation to prove the differentiability of the value function (see e.g. Cannarsa and Soner, 1989 or Bardi and Capuzzo-Dolcetta, 2008 for results of this type) and to solve the Closed Loop Equation (see e.g. Freni et al., 2008).====In particular, the existence and uniqueness of optimal feedback strategies can still be proved, even without a concave value function, because our problem has the following properties: linearity of the state equations, monotonicity of the utility function and differentiability of the value function.====Following this approach, we are therefore able to prove that a unique optimal control strategy exists. Such optimal strategy is given in feedback form and it is equal to the candidate optimal strategy found by Carroll et al. (2000). In this way, we provide a solid theoretical background to Carroll et al. (2000) result on the relation between economic growth and savings, and most importantly we answer to the raising concerns on the validity/optimality of this prediction as well as of the predictions of all the contributions in the literature using multiplicative habits. Contrary to what found in previous contributions (e.g. Alonso-Carrera et al., 2005, page 1684), we also confirm that the uniqueness and optimality of the solution still hold for relative risk aversion coefficient’s values lower than one and that such a solution converges without oscillations to the balanced growth path.====Moreover, our paper represents a neat example of how optimal control problems without a concave objective function can be dealt with using a Dynamic Programming approach. In particular, we show how much effective the dynamic programming approach can be in dealing with problems of optimality and uniqueness which would be otherwise unsolvable by simply looking at the Pontryagin’s Maximum Principle.==== ====The paper is organized as it follows. In Section 2, the optimal control problem is introduced and it is shown that standard sufficiency conditions for optimality do not hold in the parameters set usually assumed in the literature. In Section 3, we move to the Dynamic Programming approach and we explain the key steps to solve the control problem and to prove the existence and uniqueness of an optimal control. Section 4 concludes the paper. Appendix A contains more details on the derivations to check the sufficiency conditions of optimality, while Appendix B contains more details on the Dynamic programming approach followed to solve our problem. Finally Appendix C contains, for the reader’s convenience, a simple textbook example of an optimal control problem with a non-concave payoff function where a solution of the maximum principle’s necessary conditions is not optimal.",Internal habits formation and optimality,https://www.sciencedirect.com/science/article/pii/S0304406820301075,16 October 2020,2020,Research Article,176.0
"Ha-Huy Thai,Tran Nhat Thien","Université Paris-Saclay, Univ Evry, EPEE, 91025, Evry-Courcouronnes, France,TIMAS, Thang Long University, VietNam,DaLat University, VietNam","Received 10 June 2019, Revised 31 August 2020, Accepted 1 October 2020, Available online 10 October 2020, Version of Record 13 October 2020.",https://doi.org/10.1016/j.jmateco.2020.10.001,Cited by (0),This article considers an inter-temporal optimisation problem in a general form and gives conditions ensuring the convergence to infinity of the economy. These conditions can be easily verified and applied for a large class of problems in the literature. Some applications for different economies are given as illustrative examples.,"Initiated by Bellman (1957), the dynamic programming literature has rapidly become a workhorse of economic dynamic analysis. The traditional approach, culminating in Stokey and Lucas (with Prescott), gives a good explanation for and prediction of many economic phenomena. The theory of dynamic programming described in Stokey and Lucas (with Prescott) is based on a relatively strong structure of convexity. One of its implications is that in general, the economy converges to a steady state independently of the initial state.====Many studies have shown configurations where this strong convex structure is not satisfied. Clark (1971), Skiba (1978), Majumdar and Nermuth (1982), and Majumdar and Mitra (1983) consider economies where production functions exhibit an early phase of increasing returns, usually known as ==== functions. Dechert and Nishimura (1983) extend their works to a general non-concave production function. These works prove the existence of a critical level of capital stock, usually named the “====” point.==== ==== Beginning with a level capital stock under the ==== point, the economy shrinks and collapses to zero, otherwise it increases to a steady state.==== ====Kamihigashi and Roy (2006) extend the analysis to a larger class of production function, by assuming only the upper-semi continuity. They characterise the critical point below which the economy collapses in the long run and above which survival (bounded away from zero) is possible.====Another line of the literature studies conditions allowing the convergence to infinity of the economy. Jones and Manuelli, 1990, Jones and Manuelli, 1997 work with concave production function which keeps sufficiently high productivity even with a large accumulation of capital. Under this condition, the economy always converges to infinity.====Kamihigashi and Roy (2007) relax not only the concavity but also the continuity of production, and prove that under the condition that the productivity is sufficiently high for a large accumulation of capital stock, if the initial state of the economy is higher than a critical level, it will increase to infinity.==== ====Majumdar and Nermuth (1982), Dechert and Nishimura (1983), Mitra and Ray (1984), and Kamihigashi and Roy (2007) use the notion of ====, representing the discounted net returns on investment. They prove that the economy always evolves to increase the value of ====. It is interesting and surprising to see how the use of this notion provides such rich results, and it gives us deep insights in economic dynamics.====Roy (2010) studies an economy with ====, where the utility depends not only on the consumption but also on the capital level. He proves that the if the sum of the marginal rate of substitution between capital-consumption and the productivity overcomes the discount rate, beginning with a sufficiently high level of capital accumulation, the economy does not stop accumulating and, hence, converges to infinity.====In this article, we consider the same question about conditions ensuring sustained growth, in the generalised case, ==== where the dynamics of the economy can be characterised as a solution of ====where ==== is the discount factor and ==== denotes the payoffs function. Under mild conditions, the following condition is sufficient for characterising sustained growth: ====for every ==== large enough.==== ====The intuition for (1.1) is that, if choosing between saving and remaining in ====, the saving choice always prevails, then sustained growth is possible.====The results in this article allow us to gather a large class of cases studied in the literature under the same viewpoint. It can also be applied to situations where Kamihigashi and Roy’s (2007) techniques for one-sector economy cannot be used, for example, two-sector economies, the economy with ==== presented by Roy (2010) and the ==== of Kamihigashi (2008), or an economy with accumulation of human capital, presented in this article.====The article is organised as follows. Section 2 presents the fundamentals of the model. Under the ==== condition, an optimal solution exists and, under the ==== condition, its monotonicity is ensured. Section 3 studies the conditions ensuring sustained growth, with the main one being (1.1). Section 4 concludes. Examples and proofs are given in the Appendix.",A simple characterisation for sustained growth,https://www.sciencedirect.com/science/article/pii/S0304406820301051,10 October 2020,2020,Research Article,177.0
Feng Xin,"School of Economics, Nanjing University, 22 Hankou Road, Nanjing, Jiangsu 210093, China","Received 16 October 2019, Revised 1 October 2020, Accepted 1 October 2020, Available online 10 October 2020, Version of Record 13 October 2020.",https://doi.org/10.1016/j.jmateco.2020.10.002,Cited by (2),"In this paper, we study how to disclose the precision of the winner selection mechanism (i.e., the contest success function) that determines how effective a player’s effort is in determining his/her winning probability. Specifically, we focus on the disclosure of the adopted discriminatory power ==== of the Tullock contest. The discriminatory power ==== is exogenously given, and the contest organizer knows more than contestants about the true state of ","Contests are widely recognized as an effective device to promote efforts in practice. Employees work hard to compete for a promotion, firms spend time and resources in patent races, and athletes train for years to win medals. In these contest situations, players expend costly and irreversible efforts to influence their winning probabilities. For various reasons, contest outcomes are determined not only by players’ efforts, but also by many random factors, such as luck, limited observability of efforts, and imperfect evaluation of performance. This feature can be captured by imperfectly discriminatory contests in which the prize is not always awarded to the contestant who supplies the most effort, although a higher effort always increases one’s probability of winning. To formally model this, a contest success function (CSF) is introduced as a mapping from contestants’ efforts to their winning probabilities.==== ====Most literature on contests conventionally assumes that the contest organizer and contestants both know the adopted contest success function. We replace that assumption with the more plausible one that a contest organizer is better informed of the true state of the CSF than contestants, which applies to many real-world situations. For example, an experienced manager often knows more about how workers’ inputs affect their promotion opportunities than the workers themselves; an organizer is usually aware of more details about the competition policy and winner selection system in a research tournament than competing candidates; a race organizer is typically more cognizant than athletes about how their performance will be evaluated. Such an information policy is crucial, since the contestants’ bids are very sensitive to the discriminatory power ====.==== ====By possessing superior information, a contest organizer can manipulate players’ beliefs and their bids by disclosing information about the CSF, although the organizer presumably cannot control the noisy factors that affect the winner selection mechanism. Consider professional contests, e.g., the design competition, the R&D contest, and sports such as figure skating and gymnastics. In these contests, the contestants’ performances are often evaluated by the committee, which consists of experts in the respective fields. Although the final decision is made by the committee, the organizer could decide whether to disclose relevant information about the winner selection process to contestants. For instance, the organizer can pre-commit whether to reveal the name list of committee members, even before the members are selected. Equipped with the information on the winner selection mechanism, contestants update their own beliefs about the contest success function that determines how effective one’s bid is in determining his/her winning probability. Based on their own posterior beliefs, players spend efforts to compete.====In this paper, we study how an organizer discloses the contest success function (CSF) to boost players’ efforts. Specifically, we focus on the disclosure about the discriminatory power ==== of the adopted Tullock CSF. Tullock CSFs are referred to as the ratio-form contest success functions: Given ==== contestants and their effort profile ====, contestant ====’s winning probability equals ====. The exponent ==== measures the effectiveness of efforts, and it thus determines the returns to effort.==== ==== A greater ==== implies that the winner selection mechanism relates more to bids and less to various noisy factors.====In our model, the parameter ==== is drawn within the interval ==== exogenously, the contest organizer can observe the realization of ====, and contestants only know the distribution that ==== follows.==== ==== We use signal generating mechanisms to model information policies. In order to identify the optimal policy to disclose ====, we study public disclosure and private disclosure over groups with independent signals. By public disclosure, we mean that the signal realization would be revealed to contestants publicly. By private disclosure, we allow that contestants are partitioned into groups, signal realizations are revealed to groups privately and separately, and contestants within a group observe the same signal realization. In other words, the revealed information is public within a group, but remains private across groups. It is worth noting that signals can be generated by different mechanisms independently.==== ==== As a result, the private disclosure that we consider covers many information policies as special cases, including one-sided disclosure and private disclosure over individuals with independent and identical/non-identical signals.==== ====More information (e.g., full information) could be good news or bad news: When the true state of ==== turns out to be high, players would bid higher than when ==== is concealed; when ==== turns out to be low, players would bid lower than when no information is disclosed. To compare the information policies, we need to investigate how one’s equilibrium effort responds to a change in ====. This is fully captured by the implicit cost function, which is determined by the marginal bidding cost function.==== ==== If we interpret the marginal bidding cost as the shadow price of a bid, the implicit cost measures the total cost of the bid at its shadow price. Since players are sensitive to the bidding cost, the implicit cost function plays a crucial role in determining their equilibrium bidding behaviors.====When the implicit cost function is convex, i.e., its inverse function is concave in one’s bid, a contestant’s equilibrium effort supply is decreasingly elastic with respect to the value of ====. As a result, a contestant responds more sensitively to a decrease in ==== (by lowering his effort), but sluggishly to an increase in ==== (by increasing his/her effort). In particular, a contestant tends to bid more conservatively even when ==== is larger. Full concealment is thus optimal. Analogously, when the implicit cost function is concave, i.e., its inverse function is convex in one’s bid, a contestant tends to bid more aggressively when observing a larger ====. In other words, a larger ==== incentivizes a contestant to a greater extent than a smaller ==== disincentivizes him/her. On average, a player would exert more effort when ==== is disclosed. Depending on the convexity/concavity of the implicit cost function, the optimal policy is either full disclosure or full concealment.====To study public disclosure, we work on the posteriors by following the Bayesian persuasion approach. Specifically, we derive the players’ bidding strategies in terms of the posteriors. In particular, we find that a player’s equilibrium effort depends only on the posterior mean.==== ==== By aggregating players’ bids, we express the expected aggregate effort as a function of the posterior belief, which is induced by an arbitrary signal realization. According to Kamenica and Gentzkow (2011), the seller’s optimization problem is equivalent to maximizing the ex ante expected revenue by searching through all the feasible distributions of the posteriors. By making use of the covexity/concavity of the implicit cost function, we show that full disclosure/concealment (weakly) dominates public disclosure.====We next turn to private disclosure over groups with independent signals. We focus on the group-symmetric equilibria in pure-strategies, which requires that contestants within ==== would choose the same level of effort. However, we find that the conventional approach does not apply to analyzing private disclosure, since it requires explicitly solving for players’ equilibrium efforts from a non-linear system, which often lacks a closed-form solution. Nevertheless, we propose an alternative approach to circumvent the technical difficulty as follows. For a private disclosure policy with an arbitrary partition, we characterize players’ equilibrium effort strategies and rewrite the resulting expected aggregate effort in terms of players’ winning probabilities. With the properties of the equilibrium winning probabilities, we could establish an upper bound for the expected aggregate efforts induced by the private disclosure policy. We show that this aforementioned upper bound is always lower than the expected aggregate effort that results from the full-concealing/full-revealing policy. It is worth noting that the above argument is workable by altering the partitions over groups and the choice of disclosure policy for each group. Therefore, we establish the optimality of full disclosure/concealment by taking into account private disclosure over groups with independent signals.====This paper is related to the literature on imperfectly discriminatory contests.==== ==== Much of the contest literature takes the CSF as exogenously given, but a few of exceptions investigate how to determine the optimal exponent ==== associated with Tullock contests endogenously, e.g., Michaels (1988), Che and Gale (1997), Dasgupta and Nti (1998), and Wang (2010), among others. In these studies, both the contest organizer and contestants are assumed to have the same information about the adopted CSF. In contrast, asymmetric information naturally arises in our setting, since we assume that the contest organizer is informed of the parameter ==== associated with the adopted Tullock CSF and that the contestants are not. Under this assumption, we consider how the contest organizer optimally discloses the information about ====.==== ==== In this strand of literature, the paper closest to ours is Heijnen and Schoonbeek (2019), which analyzes equilibria of a two-player Tullock contest with an uncertain ==== that can be either high or low and is known to both, one or none of the players. In contrast, the current paper adopts an information design perspective using a Bayesian persuasion approach. It permits non-binary distributions, an arbitrary number of players, and both public and private disclosure.====This paper also belongs to the growing literature on information design/Bayesian persuasion. Rayo and Segal (2010) and Kamenica and Gentzkow (2011) study optimal persuasion between a sender and a receiver through a belief-based approach. Das and Kamenica (2015) and Mathevet et al. (2020) further develop this approach to a multiple-receiver setting. Bergemann and Morris, 2016a, Bergemann and Morris, 2016b, Bergemann and Morris, 2019 characterize the sender’s implementable information structures by utilizing the Bayes correlated equilibrium. In particular, the current paper is closely related to Kamenica and Gentzkow (2016) in the sense that a receiver’s action depends only on his/her posterior expectation of the state. The current paper differs from their work in that we also study private disclosure over groups. In addition, the set of actions is no longer finite in our model.====Many recent studies analyze information design problems in various settings, such as auctions (Bergemann et al., 2017); voting games (Alonso and Câmara, 2016, Chan et al., 2019, Wang, 2015); price discrimination (Bergemann et al., 2018); information hold-up and security design (Azarmsa and Cong, 2018); global games (Inostroza and Pavan, 2018); and contests (Zhang and Zhou, 2016); etc. Of those, our paper is most closely related to the works that study information disclosure in contests, which typically assume that contestants have limited information about other players’ types (e.g., abilities or valuations of the prize).====Information disclosure about contestants’ types has been analyzed by Serena (2017), Lu et al. (2018), Zhang and Zhou (2016), Zheng et al. (2018), Chen (2019), and Kuang (2020), among others.==== ====
 Serena (2017) studies information disclosure about valuations in a two-player Tullock contest with two-sided incomplete information. Lu et al. (2018) complement Serena’s study by considering an all-pay auction environment with two players. Both focus on type-contingent but deterministic disclosure policies. By exploiting the Bayesian persuasion approach, a growing number of studies investigate (stochastic) information disclosure in various contest settings. Zhang and Zhou (2016) study information disclosure in a one-sided incomplete information Tullock contest. They show that the optimal disclosure is either full disclosure or full concealment when the state is binary and partial disclosure could be optimal when the state goes beyond binary. Kuang (2020) generalizes Zhang and Zhou (2016) by allowing for more general information structures and diverse forms of prior distributions. It is shown that an organizer would not benefit from using private disclosure. Zheng et al. (2018) examine information disclosure in a two-player all-pay auction setting with two-sided private information and correlated types. Chen (2019) analyzes private and public disclosure in a two-player all-pay auction with two-sided private information and independent valuations. In contrast to these studies, the current paper focuses on information disclosure about the contest mechanism (i.e., CSF), which structurally differs from players’ types (valuations) in determining their payoffs. In addition to public disclosure, we investigate private disclosure over groups with independent signals, which departs from many existing studies.==== ==== In particular, the convenience of the structure allows us to work on a Tullock contest model with multiple contestants and a continuum of states.",Information disclosure on the contest mechanism,https://www.sciencedirect.com/science/article/pii/S0304406820301063,10 October 2020,2020,Research Article,178.0
Fisher James C.D.,"United Services Automobile Association, United States of America","Received 14 January 2020, Revised 13 July 2020, Accepted 11 September 2020, Available online 30 September 2020, Version of Record 8 October 2020.",https://doi.org/10.1016/j.jmateco.2020.09.005,Cited by (3)," or ====) and give a simple topological argument, which leverages this classic algorithm, to establish the existence of stable allocations when payoffs are upper-semicontinuous.","In classic matching markets, individuals choose their partners and each pair chooses a contract from a finite set of feasible contracts; the existence of stable allocations then follows from (a generalization of) Gale and Shapley’s 1962 celebrated Deferred Acceptance algorithm. Yet, in many modern matching markets the set of feasible contracts is not finite, rather it is infinite – e.g., the “Assignment Market” of Demange and Gale (1985) and the “Matching to Share Risk Market” of Chiappori and Reny (2016). This paper gives a novel topological argument, which leverages this classic algorithm, to establish the existence of stable allocations in infinite markets with upper-semicontinuous payoffs.",Existence of stable allocations in matching markets with infinite contracts: A topological approach,https://www.sciencedirect.com/science/article/pii/S0304406820301026,30 September 2020,2020,Research Article,179.0
"Fujinaka Yuji,Miyakawa Toshiji","Faculty of Economics, Kansai University, 3-3-35, Yamate-cho, Suita-shi, Osaka 564-8680, Japan,Faculty of Economics, Konan University, 8-9-1 Okamoto, Higashinada-ku, Kobe 658-8501, Japan","Received 31 October 2019, Revised 20 August 2020, Accepted 2 September 2020, Available online 23 September 2020, Version of Record 15 October 2020.",https://doi.org/10.1016/j.jmateco.2020.09.002,Cited by (1), and ====.,"We consider the housing market problem by Shapley and Scarf (1974). There is a group of agents, each of whom initially owns an indivisible good, called a “house”. We discuss the reallocation of houses to agents under the condition that each agent receives one and only one house. Money transfers are not allowed. A real-life application of the problem is the reallocation of university apartments (Abdulkadiroğlu and Sönmez, 1999).====We assume that there are two kinds of quality of a house; namely, high or low, and that the initial owner only knows the quality of his own house. That is, there is asymmetric information about the quality of a house between the initial owner (seller) and others (buyers), as in the Lemons markets by Akerlof (1970). The preference of an agent over the set of houses depends on the quality of the others’ houses. Thus, our model is a model of housing markets with interdependent values. In a real-life reassignment of rooms in a university dormitory, the present resident only knows how comfortable a room is or how compatible roommates and neighbors are, and each student’s preference depends on such information.==== ====This paper examines the implications of ==== and ==== within our model. ==== requires that for each type profile, each agent prefers to report his true type if the other agents report their true type. Individual rationality is important in housing markets because each agent owns a house and participation by all agents is desirable to expand the opportunity to exchange houses. ==== requires that no agent has regrets regarding participation even if any type profile is verified.====We obtain the following results. Theorem 1 states that if a type space satisfies a richness condition,==== ==== the no-trade rule is the only rule satisfying ==== and ====. The no-trade rule selects the initial endowment for each type profile. We can extend Theorem 1 to the case where random assignments are possible (Theorem 2). The no-trade rule, which selects the initial endowment with a probability of one for each type profile, is the only probabilistic assignment rule satisfying ==== and ====.==== ====Our theorem differs from the positive results in the private values model in which the reallocation of houses has been discussed.==== ==== When deterministic assignments only are possible, Ma (1994) shows that the strict core rule is the unique rule satisfying ====, ====, and ====.==== ==== Our theorem suggests that the interdependency of agents’ preferences leads to impossibility. Since we add just two kinds of the quality of a house to yield the interdependency, we would also see impossibility in more general interdependent values models, such as a model in which there are many signals on a house.==== ====Che et al. (2015) study an interdependent values model for the house allocation problem in which no agent has initial house property rights.==== ==== These authors show that no rule satisfies ==== and ====.==== ==== The authors are concerned with efficient rules, but we are concerned with individually rational rules since we consider housing markets in which each agent ==== have the initial house property rights. In addition, our assumption on the type space explicitly captures the asymmetric information that an agent only knows the quality of his own house. In contrast, Che et al. (2015) do not impose such an assumption.====The remainder of this paper is organized as follows. Section 2 provides a housing markets model with interdependent values. Section 3 presents our main theorem, and Section 4 considers an extension to random assignment problems. Section 5 concludes the paper. All the proofs of the theorems are relegated to Appendix A Proof of, Appendix B Proof of, Appendix C Proof of.",Ex-post incentive compatible and individually rational assignments in housing markets with interdependent values,https://www.sciencedirect.com/science/article/pii/S0304406820300987,23 September 2020,2020,Research Article,180.0
"Chen Zhi,Hu Zhenyu,Tang Qinshen","Department of Management Sciences, College of Business, City University of Hong Kong, Hong Kong,Department of Analytics & Operations, NUS Business School, National University of Singapore, Singapore,Division of Information Technology & Operations Management, Nanyang Business School, Nanyang Technological University, Singapore","Received 6 July 2020, Revised 1 September 2020, Accepted 14 September 2020, Available online 22 September 2020, Version of Record 28 September 2020.",https://doi.org/10.1016/j.jmateco.2020.09.006,Cited by (1),"This paper considers the problem of cost sharing, in which a coalition of agents, each endowed with an input, shares the output cost incurred from the total inputs of the coalition. Two allocations—average cost pricing and the Shapley value—are arguably the two most widely studied solution concepts to this problem. It is well known in the literature that the two allocations can be respectively characterized by different sets of axioms and they share many properties that are deemed reasonable. We seek to bridge the two allocations from a different angle–allocation ","Consider a coalition of agents ==== involved in a joint activity. An agent ==== is endowed with a nonnegative input ====. Through the activity, the aggregate input of the coalition ==== outputs a total cost ====, where ==== is a nonnegative and nondecreasing cost function. The cost sharing problem is then concerned with how to divide the total cost ==== among the participating agents.====The problem in the abstract form above has long been studied in economics in the context of sharing production costs of a single divisible homogeneous good. Specifically, each agent ==== requests for ==== quantities of the good and the total demand of the coalition ==== is met by the production activity at a cost of ====. Typical examples include sharing the cost of using a water supply system or setting up electricity lines among the set of users with different demands (Shubik, 1962) and sharing the inventory cost of using a centralized warehouse (Eppen, 1979). In the latter example—where our main motivation is coming from, each agent ==== faces random demand who seeks to decide an optimal order quantity to minimize expected holding and shortage costs. Suppose the random demands of the agents are independent and follow normal distribution with the variance of the demand of agent ==== being ====. Eppen (1979) establishes the “square-root rule” by showing that the minimum expected cost of the coalition ==== under inventory pooling is ====, where ==== is a positive constant. With ==== and ====, the classic inventory pooling problem can be cast as a cost sharing problem. Similar cost sharing problem also emerges in the risk sharing literature and we refer readers to Rüschendorf (2013) for a comprehensive account of the problem.====Numerous solution concepts have been proposed or applied to allocate the cost, ranging from the simple and easily implementable ==== to the sophisticated and computationally demanding ====. As its name suggests, average cost pricing rule uses the average cost ==== as a price that uniformly applies to all agents with agent ====’s cost being her input times the price, ====, ====. In the cost sharing literature, it is “viewed as the single compelling cost sharing method in the case of a single homogeneous good”(Moulin and Shenker, 1994, p.179), although there are also objections toward it due to different concerns (====, Moulin, 1996 and Moulin and Shenker, 1992, Moulin and Shenker, 1994). Nevertheless, its wide popularity is not only stemming from its simplicity but also from various axioms or desirable properties it satisfies. There is an extensive literature that focuses on the axiomatic characterizations of the average cost pricing rule and the comparison of its properties==== ==== with other allocations. We refer readers to Moulin and Shenker, 1994, Sudhölter, 1998 and the references therein.====While the average cost pricing rule is specifically proposed for the cost sharing problem, the Shapley value, originated from cooperative game theory, applies to cooperative situations beyond cost sharing problem. Indeed, the Shapley value allocates according to the marginal contribution made by agent ==== when joining coalition ====, ====, ====, averaged over all possible coalitions ====. Its definition depends on ==== only implicitly through the cost function ==== and is in general well defined as long as the cost of the coalition ==== is defined. The Shapley value has undoubtedly played a central role in the development of cooperative game theory. As Roth (1988) puts: “[The Shapley value] has been the focus of sustained interest among students of cooperative game theory... has been interpreted and reinterpreted” (p.1). The Shapley value is commonly believed to be an “equitable” allocation (Champsaur, 1975) and has been applied in practice to help, for example, find a fair split of fares for ride-sharing services (see Spliddit, 2018). There is also a huge amount of works devoted to the axioms and properties of the Shapley value. Among them, Moulin and Shenker (1994), Moulin (1996), and Sudhölter (1998) have specifically made a comparison between the Shapley value and average cost pricing in the cost sharing problem. While they share a common pool of desirable properties, each has its own limitations as well—certain property holds for one but not the other.====In this paper, instead of creating a dichotomy between the two, we bridge the two allocations by ordering them along the dimension of allocation inequality, a concept dating back to Lorenz (1905), who proposed the partial order—now known as the Lorenz order—to measure the income inequality in a society. In the cost sharing problem, we use Lorenz order or the equivalent concept of majorization (Marshall et al., 2011) to characterize inequality in the allocation of the total cost. Interestingly, the order between average cost pricing and the Shapley value depends on the convexity or concavity of the marginal cost. The common emphasis in the cost sharing literature is on the convexity of the cost function itself (====, Moulin and Shenker, 1992, Moulin, 1996, and de Frutos, 1998). To the best of our knowledge, the convexity of the marginal cost is not brought up before. Our main result states that when the marginal cost is convex, average cost pricing majorizes the Shapley value (or the Shapley value Lorenz dominates average cost pricing). In other words, the Shapley value allocates cost more equally than average cost pricing. The order is reversed when the marginal cost is concave.====Our result adds to the stream of literature that seeks to compare various allocation methods via Lorenz order. In the context of cost sharing problem, the serial cost sharing rules are proposed as a promising substitute for average cost pricing (Moulin and Shenker, 1992, de Frutos, 1998). The Lorenz order between average cost pricing and serial cost sharing rules is discussed in Chapter 2 of Hougaard (2009). Their order, however, depends on the convexity of the cost function rather than that of the marginal cost. Recently, Pham (2019) shows that the Lorenz order between the Shapley value and serial cost sharing rules also only depends on the cost function’s convexity. Combined with our main result, one can easily obtain a complete order of the Shapley value, average cost pricing, and serial cost sharing rules for many interesting cases: ====, the inventory pooling problem of Eppen (1979) where both the cost function and marginal cost are convex. In the more general context of cooperative game, Dutta and Ray (1989) consider the problem of finding the most egalitarian allocation that lies in the core of the game. When the game is convex, they provide an algorithm that computes an allocation that Lorenz dominates every other allocation in the core.====We formally define the cost sharing problem, average cost pricing and the Shapley value in Section 2. We also summarize here a partial list of properties shared by the two allocations that will be used in our proof. Our main result and its implications are stated in Section 3, followed by the proof of the main result in Section 4. We conclude our paper in Section 5.",Allocation inequality in cost sharing problem,https://www.sciencedirect.com/science/article/pii/S0304406820301038,22 September 2020,2020,Research Article,181.0
"Iwasa Kazumichi,Zhao Laixun","Faculty of Business, Economics & Statistics, University of Vienna, Vienna 1090, Austria,RIEB, Kobe University, Japan,Research Institute of Economic & Business, Kobe University, Kobe 657-8501, Japan","Received 2 August 2019, Revised 16 July 2020, Accepted 28 August 2020, Available online 17 September 2020, Version of Record 23 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.009,Cited by (3)," marginal impatience (DMI), an unequal society may be preferable for poor households than an egalitarian one in which every household owns an equal share of asset; (ii) poor households tend to benefit more under DMI than CMI (","Piketty’s popular book (2014, English edition) has revived wide interest in the relationship between social inequality and capital–asset ownership. He argues that the world today is returning toward “patrimonial capitalism,” in which much of the economy is dominated by inherited wealth: their power is increasing, creating an oligarchy. He thus proposes a global system of progressive wealth taxes, to avoid the vast majority of wealth coming under the control of a tiny minority.====However, some policy makers have also put forth the idea that tax cuts for the wealthy are stimulative, especially when the fraction of the rich is small, such as pre-takeoff in poor economies. Indeed, when China started its open-door policy about 40 years ago, the then-leader, Deng Xiaoping, in particular stressed to “allow a small fraction of the population to become rich first.”==== ==== This leads one to ask, is inequality necessarily bad, in particular, for the poor?====In the present paper, we offer an alternative explanation. As assets are accumulated and reinvested, diminishing returns kick in on the one hand and the demand for labor increases on the other hand, raising the wage rate. These effects are especially strong when we incorporate ====, under which households become more patient and thus invest more when they become richer, thereby increasing capital accumulation and eventually generating a trickle-down effect to the poor in the long run. The mechanism increases the capital stock, the productivity and the welfare of all households including the poor when the rich becomes richer. Such investment and production linkages between the rich and the poor are absent under constant marginal impatience (CMI). Hence, in contrast to the alarm caused by Piketty, we find that inequality may not be so bad after all; on the contrary, it might just be a “growing pain” or even a “necessary evil” on a country’s catching-up path, especially pre take-off when the fraction of rich people is small, and as a means to increasing the incentives for investment and eventually enlarging the total pie. However, the result could be reversed when the fraction of the rich becomes sufficiently large, as we shall demonstrate later.====Under endogenous time preference with DMI, a poorer household consumes a higher fraction of its income than a richer household. The assumption is motivated by a number of empirical studies which find strong evidence that households discount the future at different rates, which is important in explaining income inequality, see for instance, Hausman (1979), Becker and Mulligan (1997), Samwick (1997), and Barsky et al. (1997). Also, Lawrance (1991) and Warner and Pleeter (2001) find thatmore-educated households tend to have lower discount rates than less-educated ones, thus heterogeneous time preference may lead to inequality through long-term investment and human capital accumulation. In fact, some studies have found that the marginal propensity to save is considerably higher among wealthier people (Frederick et al., 2002). Recently, Dohmen et al. (2016) find a significant relationship between patience and development, with patience explaining a substantial fraction of development differences across countries. As for the microeconomic foundations of DMI, it may arise for the following reasons: the rich may invest more on health, beauty and education, enabling them to live longer and healthier, making them more optimistic for the future.====Also, the experiences of many developing countries provide good support for our study. In their ==== of economic development, widespread subsidies are provided to the rich–those fortunate enough to be business owners, such as the policies applied in the ==== where tax holidays, export, import and land subsidies are common, as a means to jump-starting economic development. Some of these countries have achieved great success with such policies; yet, their income inequality has also been rising rapidly. For instance, the Chinese wealth Gini coefficient remains well above the warning level of 0.4 set by the United Nations, peaking at 0.55 in 2002 (Knight, 2014).==== ==== Further, among the so-called BRICS countries (Brazil, Russia, India, China and South Africa), the Gini coefficient in South Africa was 0.67 in 2008, followed by Brazil (0.53 in 2015), India (0.51 in 2013), and Russia (0.483 in 1993).==== ====Given these stylized facts, one naturally asks the following question: could inequality be responsible for the high growth rates in these economies? A similar question was asked by Kuznets (1955) that led to the discovery of the Kuznets curve. Recent studies by Chang et al. (2015) and Gu et al. (2015) find that income inequality is a significant contributor to China’s savings glut, which has enabled the recent Chinese growth that is heavily dependent on investment (Song et al., 2011). Earlier, Banerjee and Duflo (2003) find that with cross-country data, changes in inequality in any direction are associated with reduced growth in the next period.====Based on the above empirical evidence and stylized facts, we consider a society without “equal opportunity” to begin with, as is a fact in many developing countries with strong traditional institutions (e.g., some Latin American countries, China and India, etc.), especially before their reform and take-off periods, when the fraction of rich households is very small.==== ==== To be specific, there exist two types of households that are symmetric in all aspects except that one type owns asset and can invest (e.g., “the rich”, “lenders”, “capitalists”), while the other type (e.g., “the poor”, “borrowers”, “workers”) is unable to own asset or does not have the technology to operate in the asset market and hence consumes all income at each point in time (i.e., “hand to mouth”). In such economies, the fraction of rich households is small and the financial market is inefficient so poor households can hardly obtain the skills to save and invest (Dupas and Robinson, 2013). And thus, in the present paper we simply assume poor households save nothing, and focus on examining how inequality evolves under globalization.====We find that ==== households tend to benefit more under DMI than CMI, because DMI generates a trickle down effect that is ====. Specifically, in standard models with CMI, a positive productivity shock always raises the income gap, since rich households benefit in more ways or more directly from such shocks while poor households only benefit through changes in the wage rate. This result is consistent with Acemoglu (2002), who studies the impacts of skilled labor-biased technology improvement and finds it to be a major cause for income inequality in the 20th century. In contrast, under DMI, an increase in productivity ==== due to the trickle down effect stemming from the rich investing more, and the level income-gap may also fall. These are exactly opposite to Acemoglu (2002).====Further, we demonstrate that under DMI, the welfare of an economy where everyone owns an equal share of capital is ==== than if only some people own capital. More surprisingly and perhaps politically incorrect, when the fraction of rich households is sufficiently small, ==== raises the poor households’ welfare in the steady state. The logic is, the lower the fraction of capitalists, the more they invest and the more capital the country accumulates under DMI. Hence, a country with higher inequality accumulates higher capital stock and enjoys higher levels of welfare per capita, ceteris paribus. On the contrary, if the same amount of capital stock is spread over more owners, each capitalist invests less and the steady-state welfare becomes lower. As such, the poor may accept inequality to a certain extent, as long as their income level rises.====In addition, we can clearly compare the welfare levels with even and uneven distributions of assets. Under CMI, the welfare per capita with even distribution is lower than the welfare of the rich but higher than that of the poor with uneven distribution; in contrast, under DMI, it can be lower than the poor’s welfare with uneven distribution. That is to say, under DMI, if society were forced to be egalitarian such that every household owned an equal share of asset, the steady-state welfare could become the lowest, i.e., lower than the poor’s welfare in an uneven-distribution steady-state. Hence, it may seem that we have assumed asymmetry of households to begin with, but given DMI, inequality of households turns out to be a natural consequence of endogenous time preference, and an even-distribution of assets would not be preferred by any group. Therefore, it is socially inefficient to eradicate inequalities if they are not out of reasonable bounds. The good news is though, we find that inequality exhibits an inverted-U shape under DMI, i.e., the Gini-coefficient first rises then falls as the share of population owning asset increases, in a similar shape to the Kuznets curve, albeit due to a different mechanism.====Finally, we examine policies such as a tax on capital earnings, to redistribute income from the rich to the poor, to keep inequality within boundaries.==== ==== The government could also increase expenditure on education such as scholarships and loans for low income families, and subsidies and loans for young entrepreneurs, etc. Nevertheless, we find that the effect of the tax can be reversed as the fraction of rich households rises: it reduces (raises) poor households’ income when the fraction is sufficiently low (high). As might be the case of present-day China (about 40 years after opening up to the world), the fraction of the rich has exceeded a certain level, especially in big cities and along the coast, and it might be time to impose a wealth or property tax as suggested by Piketty.==== ====In the theoretical literature, Bourguignon (1981) extends Stiglitz (1969) and shows that under convex saving functions, locally stable unegalitarian stationary distributions are Pareto superior to the egalitarian one. It follows that, the optimal asymptotic distribution of income and wealth is unegalitarian even though all individuals in the population are assumed to be identical. However, he admits that his result applies only to equilibria where all individuals have a positive wealth, which might not be the general case, especially in developing countries. In contrast, the present has a focus on developing countries and assumes poor households have zero savings (i.e., “hand-to-mouth”), especially in the early stages of catching up.====Elsewhere, Krusell and Smith (1998) demonstrate that introducing time preference heterogeneity can significantly improve the Aiyagari (1994) model in explaining income inequality, and Hendricks (2007) incorporates preference heterogeneity into the life-cycle model of Huggett (1996) to account for wealth inequality. Epstein (1987), Das (2003), Chang (2009) and Hirose and Ikeda (2012) investigate equilibrium stability and uniqueness issues under DMI. Different from the above, Uzawa (1968) and Kamihigashi (2000) examine cases of IMI (increasing marginal impatience) rather than DMI. In an economy with initial inequality, Ghiglino and Sorger (2002) show that redistribution of wealth may drive the economy from a steady state with strictly positive output to a poverty trap in which output converges asymptotically to zero. Benhabib et al. (2011) demonstrate that wealth follows a Pareto distribution in the right tail, driven by capital income risk rather than labor income, and subsequently, Benhabib et al. (2016) show that redistributive fiscal policy with idiosyncratic investment risk and uncertain lifetimes can generate a double Pareto wealth distribution. De Nardi and Fella (2017) survey numerous extensions and applications of the Bewley model 1977 on inequality due to various reasons. Closely related to us, Aghion and Bolton (1997) and Matsuyama (2000) examine inequality and credit market imperfections, generating a trickle-down effect of capital accumulation to the poor, which is in turn caused by changes in the interest rate. In contrast, in the present paper, we model the patience of asset owners. The poor is unable to save and borrow (impatient), and the trickle-down arises due to an increase in the wage rate. We focus on the effects of DMI preference, patience and total factor productivity. The interplays of the two types of households bring interesting results that are novel in the literature.",Inequality and catching-up under decreasing marginal impatience,https://www.sciencedirect.com/science/article/pii/S0304406820301002,17 September 2020,2020,Research Article,182.0
"Goenka Aditya,Nguyen Manh-Hung","Department of Economics, University of Birmingham, Birmingham B15 2TY, United Kingdom,Toulouse School of Economics, INRAE, University of Toulouse Capitole, Toulouse, France","Received 10 October 2013, Revised 26 June 2020, Accepted 28 August 2020, Available online 16 September 2020, Version of Record 19 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.004,Cited by (0),"We prove the existence of competitive equilibrium in the canonical optimal growth model with elastic labor supply under general conditions. In this model, strong conditions to rule out corner solutions are often not well justified. We show using a separation argument that there exist ==== that can be viewed as a system of competitive prices. Neither Inada conditions, nor strict ==== literature for which existence of a competitive equilibrium is not well understood. We give examples to illustrate the violation of the conditions used in earlier existence results but where a competitive equilibrium can be shown to exist following the approach in this paper.","The optimal growth model is one of the main frameworks in macroeconomics. While variations of the model with inelastic labor supply are used widely in growth theory, the version with elastic labor supply is the canonical model in business cycle models, both for exogenous and endogenous fluctuations.==== ==== Despite the central place of the model in dynamic general equilibrium models, existence of competitive equilibrium in general settings has proved to be a challenge. Results of existence of equilibrium for this model use strong conditions (see Coleman II, 1997, Datta et al., 2002, Greenwood and Huffman, 1995, Le Van and Vailakis, 2004, Yano, 1984, Yano, 1990, Yano, 1998, and paper establishes existence of equilibrium under very weak conditions: neither Inada conditions, nor strict concavity, nor differentiability, nor constant returns to scale (or more generally, homogeneity), nor restrictions on cross-partials of the utility functions, nor interiority assumptions. The recent paper by Kamihigashi (2015) shows that even if we make all the above assumptions (but not Inada or stronger assumptions) then there may be no interior optimal paths.==== ==== Understanding existence of both optimal and competitive equilibria in this model when we may not have interior paths still remains an open issue. Our results show that existence of both optimal and competitive paths can be established under very weak conditions, and whether the path is interior or not, is not important.====The approach taken in this paper is a direct method based on existence of Lagrange multipliers to the optimal problem and their representation as a summable sequence. The price of the good is the multiplier on the resource constraint. Thus, we not only know there exist equilibrium prices, but we can also calculate them in a given model. This is important as we would like to be able to characterize the equilibrium prices especially when we have non-interior equilibrium paths where existing methods do not apply. We give three examples where we can calculate equilibrium prices where the results in the literature are inapplicable.====The problem with inelastic labor supply was considered by Le Van and Saglam (2004). This approach uses a separation argument where the multipliers are represented in the dual space ==== of the space of bounded sequences ====.==== ==== The Le Van and Saglam (2004) approach uses a separation argument but imposes restrictions on the asymptotic behavior of the objective functional and constraint functions which are easily shown to be satisfied in standard models.==== ==== There is a difficulty in going from the inelastic labor supply to the elastic labor supply model: While one may be able to show that the optimal capital stock is strictly positive, one cannot be sure that the optimal labor supply sequence is strictly positive. Thus, the paper by Le Van and Vailakis (2004) which took the approach of decentralizing the optimal solution via prices as marginal utilities had to make additional strong conditions on the utility function (which fails in homogeneous utility functions such as those of the Cobb–Douglas class) to ensure that the labor supply sequence remains strictly positive. We extend (Le Van and Saglam, 2004) and show the Lagrange multipliers to the social planners problem are a summable sequence and one can directly use these to decentralize the optimal solution without having to make strong assumptions to ensure interiority of the optimal plan.==== ==== As the separation theorem does not require strict concavity or differentiability, these strong assumptions on utility functions can be dropped. This is of interest as an important specification of preferences in applied macroeconomics models are quasi-linear utility with linear utility of leisure where strict concavity and Inada conditions are violated. The linear specification also results in the planners problem in models with indivisible labor (see Hansen (1985), and Rogerson (1988)). Furthermore, for CES functions, Inada condition can be violated. In calibrated models the competitive equilibria essentially result in an interior solution but the problem is more fundamental: While for some examples we can calculate the equilibrium allocation, we still have to show that there always exist equilibrium prices that are summable. We give the main result on existence of a competitive equilibrium by showing that the price sequence constructed is an equilibrium one. Furthermore, there is no need to make any assumption on cross-partial derivatives of the utility function.==== ==== Thus, as one would expect, whether labor supply is backward bending or not, and whether consumption is interior or not plays no role in existence of equilibrium. As only convexity and not differentiability is required for the separation theorem we are also able to cover Leontief and more generally linear activity analysis models that are not covered by the existing results.====Yano, 1984, Yano, 1990, Yano, 1998 also studies existence of competitive equilibrium with endogenous labor under general conditions. There are both produced input/consumption goods (i.e. capital) and non-produced input/consumption goods (which can be interpreted as labor/leisure). While the conditions in these papers weaken the conditions used in Bewley (1982) they do not cover our existence result. Yano (1984) has the most general specification and is the closest to our assumptions. It does not use differentiability (and hence, Inada conditions). It also does not use interiority assumptions in Bewley (1982). However, it makes assumptions A.14-A.17 that we do not have to make. In our results as we are concerned only with the existence issue we allow for corner solutions. In particular, we give an example (Example 3) where the consumption of the produced good is zero (except in the initial period where the initial output is consumed) and the investment in capital is always zero. This is ruled out by A.14-A.17 in Yano (1984). Yano (1990) assumes continuous differentiability of the production function (A.1), utility function (A.5), and Inada conditions on the utility function (A.7). There is also an interiority condition (p.37) that says that all countries (firms) produce a positive output in equilibrium. Our paper does not use these conditions. In fact, in Example 4.2 we show under these conditions it is possible in a competitive equilibrium while there is positive output it is entirely consumed. Yano (1998) also assumes continuous differentiability and Inada conditions for utility (Assumption 1) and production functions (Assumption 2), which are not assumed in our paper.====There are other abstract proofs for existence of a competitive equilibrium in a neoclassical growth model, such as Aliprantis et al. (1997) which, in principle, could be adapted to show existence in a model with endogenous labor–leisure choice. There are two difficulties in using their approach for the model with endogenous labor–leisure. First, their approach prices the consumption good but it does not directly the equilibrium wage sequence. Second, they assume that the production function is strictly concave and satisfies the Inada condition, ====
 (p. 670). Their proof relies on showing that free disposal trajectory lies in a compact set (in the appropriate topology) (Lemma 3, p. 672). We do not rely on this argument, thus, we are able to dispense with both of their assumptions. We give examples where there is a competitive equilibrium where the assumptions of this paper are violated (see 4.1–4.3).====The organization of the paper is as follows. Section 2 describes the model. In Section 3, we provide the sufficient conditions on the objective function and the constraint functions so that Lagrangian multipliers can be represented by an ==== sequence of multipliers in optimal growth model with leisure in the utility function and prove the main result on existence of competitive equilibrium in a model with a representative agent by using these multipliers as sequences of prices and wages. Section 4 gives examples with corner solutions to illustrate that a competitive equilibrium will still exist using the main result of the paper. Section 5 concludes.",General existence of competitive equilibrium in the growth model with an endogenous labor–leisure choice,https://www.sciencedirect.com/science/article/pii/S0304406820300926,16 September 2020,2020,Research Article,183.0
"Graziano Maria Gabriella,Pesce Marialaura,Urbinati Niccolò","Università di Napoli Federico II, Dipartimento di Scienze Economiche e Statistiche and CSEF, Complesso Universitario Monte Sant’Angelo Via Cintia, 80126 Napoli, Italy,Università Ca’ Foscari Venezia, Dipartimento di Management, San Giobbe, Cannaregio 873, 30121 Venezia, Italy","Received 29 March 2020, Revised 6 July 2020, Accepted 28 August 2020, Available online 11 September 2020, Version of Record 17 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.008,Cited by (1),"We introduce new notions of bargaining set for mixed economies which rest on the idea of generalized coalitions (Aubin, 1979) to define objections and counter-objections. We show that the bargaining set defined through generalized coalitions coincides with competitive allocations under assumptions which are weak and natural in the mixed market literature. As a further result, we identify some additional properties that a generalized coalition must satisfy to object an allocation.","The core of an economy is defined as the set of feasible allocations that are not blocked or objected by any coalition. The possibility for other agents to react to this objection and propose a new counter-objection is not taken into account. Aumann and Maschler (1964) propose a new solution concept according to which objections that are counter-objected are not credible and therefore they should be neglected. Mas-Colell (1989) adapts this notion to atomless economies and defines the bargaining set as an enlargement of the core containing all the feasible allocations against which it is impossible to raise an objection with no counter-objections. Mas-Colell (1989) proves the equivalence between the set of competitive equilibria and the bargaining set under assumptions that are close to those used to prove the existence of competitive equilibria and the Core–Walras equivalence theorem respectively in Aumann, 1966, Aumann, 1964. The key idea of Mas Colell’s proof consists in characterizing credible objections as those that are price supported. This allows him to conclude that the set of competitive allocations and the bargaining set coincide and are equivalent to the core in atomless economies.====It is clear that when we move on to the case of finite economies the previous equivalences are no longer true. In this case, in fact, the core and, a fortiori, the bargaining set, strictly contains the set of competitive allocations. Furthermore, Anderson et al. (1997) show that, whereas the core shrinks to the set of competitive allocations after a sufficiently large number of replicas, the bargaining set does not. A similar investigation is conducted by Shitovitz (1989) in mixed markets, i.e. economies in which the measure space of agents have both atoms and an atomless sector. An atom of a measure space ==== is a set ==== with positive measure such that ==== or ==== for every other ==== and it represents a non-negligible agent in the market. For example, an atom can be representative of a trader who concentrates in his hands an initial ownership of commodities that is sufficiently large with respect to the total market endowment, as in the case of monopolistic or, more generally, oligopolistic markets. Or else, even though the initial endowment is spread over a continuum of negligible traders, an atom can be representative of a group of traders that decide to act as a single player, as in the case of cartels, syndicates, or similar institutions. Notice that in a mixed market the set of agent ==== is the disjoint union of an atomless section ==== and the atomic part ====. This allows to view as special case of mixed markets both atomless economies (once ==== is empty) and finite economies (when ==== is null and ==== finite). Shitovitz (1989) proves that, if in addition to certain assumptions there exists a commodity owned by only one of the atoms (====), then the core coincides with the bargaining set and it strictly contains the set of competitive allocations. He also illustrates an example of mixed market outside the class mentioned above and satisfying the sufficient hypotheses for the Core–Walras equivalence theorem (Shitovitz, 1973) in which the bargaining set is strictly larger than the core.====The previous conclusions seem to suggest that it is the core, rather than the set of competitive equilibria, to be compared with the bargaining set in models comprising atoms. In this work, instead, we go back to the original idea in Mas-Colell (1989), with the aim of characterizing the bargaining set also in mixed markets by means of competitive equilibria. Our approach consists in relaxing the class of coalitions that can form an objection and/or a counter-objection according to the veto mechanism of Aubin (1979). We allow agents to join a coalition with a partial participation rate, rather than to decide only whether to join or not. Basically, we enlarge the class of potential blocking coalitions to the so-called ====. A generalized coalition is a measurable function ==== from the space of agents ==== to the unit interval ==== with non-null support. Intuitively, the value ==== represents the share of resources employed by agent ==== in the formation of the coalition ====. We define four variants of the bargaining set depending on which class of coalitions is involved in objections and/or counter-objections and we study the relations among them (Proposition 2.13). In particular we show that all four bargaining set variants coincide when the economy is atomless, the familiar framework of Mas-Colell (1989) (Proposition 3.7). Our main result states the equivalence between the set of competitive allocations and a bargaining set in mixed economies in which atoms have convex preferences (Theorem 1), an assumption quite common in the literature of mixed markets (see for example Hildenbrand, 1974, Shitovitz, 1973, Greenberg and Shitovitz, 1986, Pesce, 2014, Bhowmik and Graziano, 2015, Avishay, 2019, among the others). Our theorem extends to mixed economies the Mas-Colell’s equivalence theorem since, as already noticed, once the set of atoms is null, a mixed market reduces to be an atomless economy. From a technical point of view, we closely follow the approach of Mas-Colell (1989), since we identify the notion of competitive objection as the one on which to focus attention. Indeed, even if competitive objections are defined as particular objections with a specific property and hence constitute only a part of the set of all possible objections, they are the only ones to consider when dealing with the bargaining set. Precisely, we prove that in order to show that an allocation belongs to the bargaining set it is enough to verify that there are no competitive objections against it. From this result we derive the existence of a competitive equilibrium in a mixed market under quite mild conditions (Corollary 3.6) as done by Mas-Colell (1989) for atomless economies. On the other hand, contrary to Mas-Colell (1989), since the measure space of agents we consider is not necessarily non-atomic, we cannot conclude that the correspondence defined as the integral of the demand net trade set has convex values. For this reason we work with its convex hull, a needless step in Mas-Colell’s setting thanks to Lyapunov–Richter’s Theorem. A further contribution of this paper is the identification of certain properties that a generalized coalition ==== has to satisfy to object. Indeed, we show that for an allocation ==== outside the bargaining set there exists a competitive objection characterized by full participation of negligible traders as well as of traders which are strictly better off (Proposition 4.3).====Summing up, our analysis contributes to two literatures: the one that studies bargaining sets in exchange economies and the literature on mixed markets. Recently Hervés-Beloso et al. (2018), Hervés-Estévez and Moreno-García (2018a) and Hervés-Estévez and Moreno-García (2018b) study the notion of bargaining set in finite economies. They allow generalized coalitions to form objections and counter-objections and obtain the Mas-Colell’s equivalence theorem for finite economies. Their result follows from ours since, as earlier observed, even a finite economy can be viewed as a special case of mixed market. At the same time, our work differs from the previous contributions in many respects. We consider four variants of the bargaining set among which only one is an extension to mixed markets of the definition they adopt. We obtain the equivalence theorem directly via the notion of competitive objections and we use existence and welfare theorem arguments, whereas in the above papers the equivalence is obtained by associating to the finite economy a continuum economy with a finite number of types of agents.==== ==== On the other hand, they also allow for production, they also investigate on the bargaining set of replica economies and analyze how the restriction on the formation of coalitions may impact on the bargaining set. We defer the analysis of our model to address these research questions to future works.====The paper is organized as follows: in Section 2 we introduce the economic model and the main definitions. Our main theorem is presented in Section 3 whereas further results and concluding remarks are stated respectively in Section 4 and Section 5. All the proofs are collected in the Appendix.",Generalized coalitions and bargaining sets,https://www.sciencedirect.com/science/article/pii/S0304406820300975,11 September 2020,2020,Research Article,184.0
Norman Thomas W.L.,"Magdalen College, Oxford, United Kingdom","Received 22 August 2019, Revised 30 June 2020, Accepted 31 August 2020, Available online 9 September 2020, Version of Record 16 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.006,Cited by (0), in favor of the target equilibrium.,"Sandroni (2000) and Blume and Easley (2006) offer a foundation for the “market selection hypothesis” (Alchian, 1950, Friedman, 1953, Cootner, 1964, Fama, 1965) that market forces will lead rational traders to flourish at the expense of irrational ones. They show that, if agents are equally patient and at least one has rational expectations, then a complete markets economy satisfying certain conditions will eventually be dominated by correct beliefs.==== ==== However, they take the true path of the economy as given, whereas many plausible models exhibit endogeneity in the economy’s state. Allowing this feature complicates the standard analysis of consumer survival by requiring solution of the economy’s dynamics. I show here that evolutionary game theory can be helpful in this respect; in particular, if utility is logarithmic and the economy’s state and beliefs are Markovian in the beliefs’ consumption shares, then market selection of those shares is described by the “replicator dynamics” (Taylor and Jonker, 1978), with beliefs flourishing if and only if they outperform the economy’s average belief in their ability to predict the evolving state. Whilst a Markovian state fits within the standard framework for market selection, assuming that beliefs are Markovian renders them endogenous, requiring them to be found in equilibrium; the notion of competitive equilibrium hence requires modification under this assumption, as in Dindo and Massari (2017).==== ====I stress that this is a fully rational model, with the replicator dynamics playing a descriptive rather than a behavioral role; there is no bounded rationality here other than the market selection literature’s relaxation of rational expectations in favor of heterogeneous beliefs. Whilst this evolutionary game theoretic representation has thus been implicit in the market selection approach since the seminal contribution of Blume and Easley (1992), here I make it explicit. Blume and Easley (1992, p. 10) note that: ====Since we can straightforwardly find the solution trajectories of the replicator dynamics, we can use them to analyze the effects of market selection with an endogenous state, as I illustrate with a simple exchange economy in Section 4.1.====  In macroeconomic general equilibrium models, it is common for the state of the economy to be endogenous, owing for instance to the role of agents’ expectations in shaping macroeconomic variables. This can often lead to multiple equilibria, in which case it is not clear what “correct beliefs” the market might select for; in this case, the replicator dynamics can also offer a method of equilibrium selection. I illustrate this in Section 4.2 by first establishing equilibrium convergence under market selection in a standard Taylor rule model with iid beliefs and a plausible form of inflation targeting. I then add a zero lower bound and consequent liquidity trap to the economy, and show that the market selects the target equilibrium over the liquidity trap. I begin, however, in the next two sections by outlining the model and its evolutionary game theoretic representation.",Market selection with an endogenous state,https://www.sciencedirect.com/science/article/pii/S030440682030094X,9 September 2020,2020,Research Article,185.0
"Fujimoto Junichi,Lee Junsang","National Graduate Institute for Policy Studies (GRIPS), 7-22-1 Roppongi, Minato-ku, Tokyo 106-8677, Japan,Sungkyunkwan University, 25-2 Sungkyunkwan-ro, Myeongnyun 3(sam)ga, Jongno-gu, Seoul, South Korea","Received 24 November 2019, Revised 28 June 2020, Accepted 28 August 2020, Available online 6 September 2020, Version of Record 16 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.007,Cited by (1),", we find that the optimal stationary allocation exhibits novel consumption dynamics: Borrower consumption begins at a relatively low level, converges toward a particular level when the participation constraint is slack, and jumps up when the participation constraint binds.  We then explore the role of limited commitment in generating such consumption dynamics and discuss the associated repayment profile.","This paper examines optimal long-term lending contracts between a single lender and a continuum of risk-averse borrowers. The lender wishes to maximize social welfare by offering lending contracts to the borrowers, which requires forming a lending relationship with the borrowers through a frictional matching process. The lender must self-finance the total cost of lending via proceeds from its loans. A borrower who is matched with the lender runs a project, which yields an output that is subject to idiosyncratic shocks. In any period, the lending relationship can terminate due to either the borrower reneging on the contract or an exogenous separation shock. In either case of match termination, the borrower’s past credit history is wiped out and the borrower seeks to be matched again. The main contribution of our paper is to analytically characterize the optimal stationary allocation in such an environment, which features endogenous outside option values of the agents as well as continual creation and destruction of principal–agent relationships.====Our key modeling assumptions are motivated by the situation faced by not-for-profit microfinance institutions (MFIs), as explained below. First, MFIs lend small amounts to large numbers of clients, and the relationships between MFIs and their clients are, while often long-term, not permanent, with the client base evolving over time. To capture these features, we build on the literature on search-theoretic models of credit markets==== ==== to introduce creation and destruction of lending relationships. Second, because MFIs pursue social missions not necessarily aligned with making profits, many MFIs depend on subsidies; thus, their financial sustainability is an important concern for society. By requiring the lender to self-finance its activities and not rely on external resources, we pursue optimal lending contracts that not only remove the need for subsidy but make the lender immune to pressure from investors as well as to interest rate and exchange rate risks.==== ====  Third, while severe penalties, such as permanent exclusion from future lending, may be effective in preventing borrower default, given such MFI missions as poverty reduction and welfare improvement, these penalties may be undesirable, especially when defaults are due to force majeure, as with natural disasters and burglaries. We therefore pursue allocations that can be achieved when the only penalty against exiting a lending relationship is, regardless of its cause, having to search for a new relationship.====Our paper relates to literature that theoretically explores the optimal lending arrangement between MFIs and their clients in an environment with moral hazard in terms of repayment. The closest work to our study is Tedeschi (2006) and Ghosh and Ray (2016), who endogenize borrowers’ value from default.==== ==== Tedeschi (2006) explores the problem of a single not-for-profit lender who faces borrowers with differing levels of risk and sets the interest rate and number of periods for which borrowers are excluded from loans after default. In Tedeschi (2006), however, borrowers are risk neutral and discount the future with the same rate as the lender. As a result, the optimal contract does not involve consumption smoothing across states and over time, which is central in the present paper. Ghosh and Ray (2016) analyze a model in which lenders know the credit history of previous clients but lack information on new clients. The model in Ghosh and Ray (2016) assumes risk-neutral borrowers and allows only one-period contracts, so again there is no issue of consumption smoothing.====From a theoretical standpoint, our paper belongs to the branch of the dynamic contracting literature that examines optimal risk sharing between a single principal and many agents. In particular, the environment of the model builds on the work of Krueger (2000), who adopts a limited commitment version of Atkeson and Lucas’s (Atkeson and Lucas, 1992, Atkeson and Lucas, 1995) model of private information and examines an optimal stationary allocation in an economy populated by a continuum of consumers who face idiosyncratic income risk.  Unlike most studies in this literature, both Krueger (2000) and the present paper assume that the principal has no access to external resources and is therefore subject to the aggregate resource constraint.==== ====  However, while in Krueger (2000) the relationship between lender and consumer is formed automatically and never terminates in any optimal allocation, in the present paper, the relationship is formed through a frictional matching process and is subject to exogenous separation shocks. This extension has two important consequences.====First, the extension enriches the contracting problem by endogenizing agents’ value of the outside option.  In Krueger (2000), the outside option of an agent is autarky, over which the lender has no influence. In contrast, in our paper, the lender must optimally choose the contract by taking into account how the contract affects borrowers’ value of the outside option. This feature, along with the lender’s resource constraint, generates novel consumption dynamics in the optimal allocation.==== ====  Second, the extension widens the scope of welfare analysis by enlarging the set of (incentive- and resource-) feasible stationary allocations. That is, with a permanent principal–agent relationship as in Atkeson and Lucas (1995) and Krueger (2000), stationarity is a strong requirement such that the Pareto criterion suffices to pin down the optimal allocation.==== ====  This intuition is better understood by noting that an allocation in which all agents’ consumption grows at a constant rate can never be stationary under a permanent principal–agent relationship. The situation changes dramatically once creation and destruction of the relationship are introduced. Our model exhibits a continuum of Pareto-optimal feasible stationary allocations, which enables discussion of how optimality depends on the social welfare function that the lender maximizes.====To analyze optimal allocations under different social welfare functions, we adopt an approach that differs substantially from the recursive approach adopted by Krueger (2000) and most of the dynamic contracting literature. This is because, in our environment, that borrowers’ value of the outside option is not only endogenous but controlled by the lender prevents the use of standard techniques to recursively formulate the lender’s problem. Further, deriving the necessary conditions for optimality from the Lagrangian of the sequential problem brings technical challenges due to the infinite dimensionality of the problem. We overcome these issues by using the sequential formulation of the problem and resorting to a variational argument. The idea here is to consider perturbations of the candidate optimal allocation that ensure that the participation constraints continue to hold at any history. Optimality of the original allocation requires that there are no such perturbations that achieve the same level of welfare with fewer resources (or achieve a higher level of welfare with the same level of resources). Such no-arbitrage conditions constitute the conditions that characterize the optimal allocation.====As a benchmark case, we assume a ==== lender who maximizes the sum of individual welfare or, equivalently, the expected discounted lifetime utility of all borrowers in the steady state. We then examine the corresponding optimal allocation and show that the consumption of matched borrowers features rich dynamics, which is novel in the limited commitment literature. That is, consumption is relatively low for newly matched borrowers. In subsequent periods, consumption moves toward a particular level until the participation constraint binds. When the participation constraint binds, consumption jumps up, and then evolves toward this level of consumption again. We show how such a consumption profile results from interaction between the lender’s goal of maximizing social welfare and the endogenous outside option value of borrowers. This optimal consumption profile requires repayment to be initially large and to decrease gradually, which suggests that from the welfare perspective, equal payment of principal may be more desirable than the equal installment payments common with MFIs; this also provides potential justification for the widespread practice of compulsory saving. The optimal contract therefore not only serves the role of microcredit, but also that of microinsurance and microsavings.====As an extension, we first explore the optimal allocation under a ==== lender who maximizes the welfare of the least well-off borrowers. We show that the consumption dynamics differ completely from the benchmark Benthamite case. This time, consumption is relatively high for newly matched borrowers and then falls whenever the participation constraint is slack. We show how the difference in the social welfare function generates such a different result from the benchmark case, and we go on to derive implications that apply beyond these two social welfare functions. We then present numerical examples that explore how borrower probability of being matched, which determines the ==== of the lender, in the benchmark optimal allocation varies with the model parameters.  We find that said probability increases as changes in parameters make the lending relationship more productive or dampen borrower incentives to exit the relationship.",Optimal self-financing microfinance contracts when borrowers have risk aversion and limited commitment,https://www.sciencedirect.com/science/article/pii/S0304406820300963,6 September 2020,2020,Research Article,186.0
Yang Zhe,"School of Economics, Shanghai University of Finance and Economics, Shanghai 200433, China,Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai 200433, China","Received 2 December 2019, Revised 14 July 2020, Accepted 31 August 2020, Available online 5 September 2020, Version of Record 8 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.005,Cited by (7),"In this paper, we introduce the notion of the weak ====-core.","The cooperative solution is another important concept in normal-form games. The idea of cooperative solutions of normal-form games is inspired by two concepts: core of exchange economies and core of cooperative games. The existence theorem of the core for transferable utility games was proved independently by Bondareva (1963) and Shapley (1967). Moreover, Scarf (1967) established the NTU core existence theorem of nontransferable utility games. Later, following the representation of Aumann (1961), Scarf (1971) proved the existence of the ====-core for a normal-form game by the work of Scarf (1967). Inspired by the work of coalitional production economies in Boehm (1974), Border (1984) proposed a class of games without ordered preferences, and proved the existence of the core by a line of argument analogous to Ichiishi (1981). By combining the work of Scarf (1971) and Border (1984), Kajii (1992) proved the nonemptiness of the ====-core for games with nonordered preferences. Furthermore, by using a different proof method from Ichiishi (1981), Border (1984) and Kajii (1992), Florenzano, 1989, Florenzano, 1990 proved the existence theorems of the core and fuzzy core for coalitional production economies without ordered preferences. Lefebvre (2001) generalized the work of Florenzano (1989) to a production economy with incomplete information.====Following the representation of ====-core in Aumann (1961) and Yannelis (1991) gave different definitions of ====-core for exchange economies. Observe that the ====-core concepts are more complicated than the notion of core, since every agent has the preference with externalities, a behavior assumption that is of significant practical relevance (e.g., Stiglitz (2019)). According to different choice cases of agents outside the coalition, there exist ====-core and ====-core in generalized games or exchange economies. Furthermore, following  Yannelis (1991) and  Kajii (1992), Holly (1994) analyzed the ====-core and ====-core of exchange economies comprehensively, and showed that the ====-core can be empty for an exchange economy with ==== persons.====Recent work on cooperative solutions focus on three aspects. First, Askoura et al. (2013) introduced the notion of ex ante ====-core for a normal-form game with incomplete information. Following Askoura et al. (2013), Noguchi, 2014, Noguchi, 2018 obtained more results for the ====-core of normal-form games with incomplete information. Moreover, Askoura (2015) analyzed the interim core of an exchange economy with incomplete information. Second, Uyanik (2015) extended the NTU ====-core of Scarf (1971) and TU ====-core of Zhao (1999) to games with discontinuous payoffs. Third, following the work on NTU core of NTU games with infinitely many players (Weber, 1981), Askoura (2011) first introduced the weak core of a normal-form game with a continuum of players. Later, by considering the equi-usc family of functions, Askoura (2017) improved the work of Askoura (2011). On the other hand, inspired by Askoura, 2011, Askoura, 2017, Yang (2017) generalized the work of Scarf (1971) to normal-form games with infinitely many players. Later, by improving the proof technique of Yang, 2017, Yang, 2018 provided a generalization of Kajii (1992) to games with nonordered preferences and infinitely many players, and Yang and Yuan (2019) proved the existence of weak hybrid solutions for games with infinitely many players, by following the work of Zhao, 1992, Zhao, 1996.====Our paper will focus on the weak ====-core of exchange economies with pseudo-utilities and a continuum of players by following the work of Askoura, 2011, Askoura, 2017. Generally speaking, there are a number of articles studying Nash equilibria of games and markets with a continuum of players, see Khan, 1989, Bewley, 1991, Khan et al., 1997, Balder, 1995, Balder, 1999 and Martins-da Rocha and Topuzu (2008). Recently, Askoura, 2011, Askoura, 2017 first analyzed the cooperative solution of games with a continuum of players.====Note that the weak cooperative solution based different strong-blocking concepts in Askoura, 2011, Askoura, 2017, Yang, 2017, Yang, 2018 and Yang and Yuan (2019), and there exist different ====-blocking definitions for exchange economies with externalities, see Yannelis (1991) and Holly (1994). Thus, by referring to the work of Holly (1994), we shall introduce an appropriate notion of the weak ====-core for an exchange economy with a continuum of agents and pseudo-utilities.====The paper is organized as follows. Section 2 recalls some results and definitions. In Section 3, we provide the existence theorem of the weak ====-core. Finally, Section 4 is the conclusion.",The weak ,https://www.sciencedirect.com/science/article/pii/S0304406820300938,5 September 2020,2020,Research Article,187.0
"Albizuri M.J.,Dietzenbacher B.J.,Zarzuelo J.M.","Faculty of Business Administration, Basque Country University, Bilbao, Spain,International Laboratory of Game Theory and Decision Making, National Research University Higher School of Economics, St. Petersburg, Russian Federation","Received 15 April 2020, Revised 8 July 2020, Accepted 11 August 2020, Available online 26 August 2020, Version of Record 3 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.003,Cited by (8),"This paper studies independence of higher claims and independence of irrelevant claims on the domain of bargaining problems with claims. Independence of higher claims requires that the payoff of an agent does not depend on the higher claim of another agent. Independence of irrelevant claims states that the payoffs should not change when the claims decrease but remain higher than the payoffs. Interestingly, in conjunction with ","A bargaining problem with multiple agents (cf. Nash, 1950) is described by a feasible set and a reference point inside this set. The feasible set consists of all payoff allocations in the utility space which can be jointly generated by the agents. The main question is which of these allocations will be selected by the agents or should be recommended by an arbitrator. The reference point, usually referred to as the disagreement point, serves as a lower bound, with the interpretation that it is implemented when the agents do not reach agreement.====Bargaining problems are typically studied from two perspectives. The positive or strategic approach studies solutions on the basis of their implementability, i.e. whether they result from a natural negotiation procedure. The normative or axiomatic approach studies solutions on the basis of their properties, i.e. whether they respect appealing fairness principles. Central solutions in bargaining theory are the Nash (1950) solution, which maximizes the product of the utility excesses with respect to the reference point, the Kalai and Smorodinsky (1975) solution, which maintains the ratios of the maximally possible utility excesses, and the Kalai (1977) solution, which equalizes the utility excesses.====More recently, Mariotti and Villar (2005) introduced rationing problems in which the agents share a loss instead of a surplus, described by a feasible set and a reference point outside this set. The reference point expresses rights, needs, demands, or aspirations, and serves as an upper bound for payoff allocations.====Herrero and Villar (2010) and Sudhölter and Zarzuelo (2013) merged bargaining problems and rationing problems into NTU sharing problems, where the reference point may be inside or outside the feasible set. The reference point reflects the entitlements of the agents which can be either satisfied or not.====Another line of research enriched original bargaining problems with a second exogenous reference point. Gupta and Livne (1988) analyzed problems where both reference points are inside the feasible set and serve as lower bounds. The first reference point has the classic conflict interpretation, while the second reference point emerges from pre-negotiation activities.====Chun and Thomson (1992) introduced bargaining problems with claims where one reference point is inside the feasible set and the other reference point is outside the feasible set. The inside reference point is a lower bound from which the utility excesses are measured. The outside reference point is an upper bound representing earlier commitments which cannot all be honored anymore. The main solution studied in this context is the proportional solution, which prescribes the efficient payoff allocation on the line connecting both reference points. Chun and Thomson (1992) and Lombardi and Yoshihara (2010) derived several axiomatic characterizations on domains with convex and nonconvex feasible sets, respectively. An alternative solution was studied by Bossert (1993) and Marco-Gil (1994).====This paper takes a further axiomatic approach to bargaining problems with claims. For convenience, we assume that the inside reference point equals the origin and we restrict to nonnegative feasible set allocations. In other words, we implicitly incorporate ==== and ==== into the definition of solutions. In this way, bargaining problems with claims can also be interpreted as bankruptcy problems with nontransferable utility (cf. Orshan et al., 2003). This model generalizes classic bankruptcy problems as introduced by O’Neill (1982) by allowing agents to have nonlinear utility functions over their monetary payoffs. The proportional solution of Chun and Thomson (1992) and Lombardi and Yoshihara (2010) corresponds to a generalized proportional rule for bankruptcy problems. The solution studied by Bossert (1993) and Marco-Gil (1994) corresponds to a generalized constrained equal losses rule for bankruptcy problems.====On the one hand, we focus on the property ====, originally appearing in the cost sharing literature (cf. Moulin and Shenker, 1992). On the domain of bargaining problems with claims, this property requires that for each pair of agents being symmetric within the feasible set, the payoff allocated to the agent with the lower claim does not depend on the higher claim of the other agent. This protects the payoffs of smaller claimants from being influenced by the big players. The proportional solution does not satisfy independence of higher claims. Interestingly, we show that, in conjunction with the standard axioms from bargaining theory, independence of higher claims characterizes a new constrained Nash solution, a constrained Kalai–Smorodinsky solution, and a constrained Kalai solution, obtained by explicitly bounding the original bargaining solutions by the claims. These three constrained bargaining solutions all correspond to a generalized constrained equal awards rule for bankruptcy problems.====On the other hand, we focus on the property ====, which says that the prescribed allocation should not change when the claims diminish but still dominate the allocation. For classic bankruptcy problems, this property was studied by Kibris (2012) and Stovall (2014). The proportional rule does not satisfy independence of irrelevant claims. Interestingly, we show that, in conjunction with standard axioms from bargaining theory, independence of irrelevant claims also characterizes the constrained Nash solution and the constrained Kalai solution.====This paper is organized in the following way. Section 2 provides preliminary notions for bargaining problems with claims. Section 3 formally introduces independence of higher claims and independence of irrelevant claims. In conjunction with standard axioms from bargaining theory, Sections 4, 5, and 6 characterize the constrained Nash solution, the constrained Kalai–Smorodinsky solution, and the constrained Kalai solution, respectively. Section 7 formulates some concluding remarks.",Bargaining with independence of higher or irrelevant claims,https://www.sciencedirect.com/science/article/pii/S0304406820300859,26 August 2020,2020,Research Article,188.0
Thomas Caroline,"Department of Economics, University of Texas at Austin, United States of America","Received 26 April 2018, Revised 27 July 2020, Accepted 4 August 2020, Available online 19 August 2020, Version of Record 7 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.001,Cited by (3),"This paper analyses a two-player stopping game with multiarmed bandits in which each player chooses between learning about the quality of her private risky arm and competing for the use of a single shared safe arm. The qualities of the players’ risky arms are independent. A player whose risky arm produces a success no longer competes for the safe arm. We assume that a player observes her opponent’s actions but not his realised payoffs. She is therefore never certain whether her opponent is still competing for the safe arm. When the players’ prior probabilities of success are sufficiently close, there exists no pure strategy equilibrium, and we characterise the unique mixed strategy equilibrium. Otherwise, the unique equilibrium is in pure strategies. The amount of experimentation performed in equilibrium is inefficiently low but, for many priors, higher than if successes are publicly observed.","Corporate growth can be achieved through various means. One avenue is acquisitions or mergers. Another is internal R&D or venture capital investments. Empirically, the two avenues have been shown to be substitutes.==== ==== Moreover, the two involve different sorts of uncertainty. In the case of acquisitions, the competition with other potential buyers plays a central role in determining payoffs. For instance, two firms might hold options to buy an asset, say a production plant. In contrast, in-house R&D is characterised by objective uncertainty about the profitability of an innovation. These considerations are captured in a two-player game where each agent has two actions, one characterised by strategic competition, the other by objective uncertainty.====We model the firms’ choices between internal R&D and acquisitions as a preemption game with private information. Two firms engage in independent experimentation – this represents in-house R&D – but compete for the use of a common outside option – this represents the acquisition of a production plant and implies abandoning the R&D project forever. Specifically, each player faces an exponential two-armed bandit problem, and must decide when to irrevocably stop experimenting with her risky arm. There is only one safe arm, which can be activated by at most one of the players – the new plant can only be bought by one of the firms, leaving the other firm no choice but to concentrate on its in-house R&D. Thus, the first player to activate the safe arm ends the game.====Each player’s risky arm can be either good or bad, and the qualities of the risky arms are independently drawn. A good risky arm yields a lump-sum payoff at each jump (“success”) of a standard Poisson process; a bad risky arm never produces a success. A player whose risky arm has produced a success is called “informed”. It is optimal for her to never switch to the safe arm.====Importantly, we assume that while the agents’ actions are public, the outcomes of their experimentation are only observed ====. Indeed, a success is interpreted as a marketable breakthrough in an in-house R&D agenda. The assumption is that a firm has some control over whether to publicise this success, and one of our results will be that a firm always wants to convince its rival that it has already produced a success, so this claim is not credible. Thus, in our model, whether a player has had a success is her private information. One crucial implication is that, as long as a player activates her risky arm, her rival does not know whether she is still a contender for the safe arm. Equivalently, as long as both players are experimenting, they must entertain the possibility that their rival has, in fact, already dropped out of the race for the safe arm.====Indeed, the players’ second-order beliefs play a central role. In equilibrium, conditional on neither player having stopped, a player revises her belief about her opponent’s private information, based on his strategy and on calendar time. As time goes by, the opponent is more likely to have produced a success. In addition, if an uninformed opponent’s strategy dictates that he stops with positive probability before a certain date, the opponent still being in the game constitutes additional evidence suggesting that he is informed. So the likelihood a player attributes to her opponent being the informed type depends positively on the weight his strategy attaches to stopping before that date.====We find that, qualitatively, the equilibrium strategies depend on the players’ priors that their risky arms are good – closer priors being interpreted as stronger competition for the safe arm. We establish the existence of a unique equilibrium for each pair of priors. The equilibrium is in pure strategies, except when the priors are sufficiently close (in a precise sense) and above the myopic threshold.====For such priors, we characterise the mixed strategy equilibrium. Both players stop with a positive intensity at every posterior belief in an interval bounded below by the single-agent threshold and above by the myopic threshold. A player stops for sure by the time her posterior belief attains the single-agent threshold. When the priors are asymmetric, i.e. when one player is ex ante more likely than her opponent to have a good risky arm, the mixed strategy of the more pessimistic player has an atom at the single-agent threshold. In contrast, the more optimistic player commits to losing access to the safe arm with positive probability: her equilibrium strategy assigns probability less than one to stopping before the pessimistic player’s belief attains the single-agent threshold.====The public information version of our stopping game is analysed in Thomas (2020). In equilibrium, conditional on no success, the more pessimistic player stops at the myopic threshold, whereas her opponent adheres to the single-agent policy. Both with public and with private information, competition for the safe arm leads to lower equilibrium experimentation than in the planner problem where the agents play as a team observing each other’s successes and maximising joint payoffs. Indeed, the planner solution has both players experimenting beyond the single-agent threshold. In the game with public successes, the players’ willingness to preempt one another unravels all the way to the myopic threshold. But when successes are private, a player’s uncertainty about whether her opponent is still in the race for the safe arm mitigates her fear of preemption, and leads to more equilibrium experimentation, conditional on no success. On the other hand, not being able to observe that her opponent had a success and therefore dropped out of the race means a player might stop experimenting unnecessarily early. We identify a set of priors at which the overall effect of private information is socially beneficial.====In summary, the main innovation of this stopping game is that experimenting agents observe successes ==== and compete for a ==== outside option. As a result, players are engaged in a preemption game where they are never sure that their opponent has not, in fact, unobservedly ====. This potential competition leads to inefficiently low levels of experimentation. But we show that (for certain priors) the inefficiency is less stark than when the players can observe their rivals’ successes.",Stopping with congestion and private payoffs,https://www.sciencedirect.com/science/article/pii/S0304406820300835,19 August 2020,2020,Research Article,189.0
Raghavan Madhav,"Department of Economics, University of Lausanne, Switzerland","Received 11 February 2019, Revised 20 July 2020, Accepted 27 July 2020, Available online 13 August 2020, Version of Record 29 August 2020.",https://doi.org/10.1016/j.jmateco.2020.07.006,Cited by (0),"A house allocation rule should be flexible in its response to changes in agents’ preferences. We propose a specific notion of this flexibility. An agent is said to be ==== over a pair of houses at a profile of preferences if the rule assigns her one of the houses at that profile and assigns her the other house when she instead reports preferences that simply swap the positions of the two houses. A pair of agents is said to be ==== over their assignments at a profile if the rule exchanges their assignments when they together report such ‘swap preferences’. An allocation rule is ==== if any pair of houses has a swap-sovereign agent, and is ==== if any pair of houses has either a swap-sovereign agent or mutually swap-sovereign agents. We show for housing markets that the ==== is the unique ====, ==== and ==== rule. In house allocation problems, we show that queue-based ==== are uniquely ====, ==== and ====. Varying the strength of non-bossiness, we characterise the important subclasses of ==== rules (additionally ====) and ==== rules (additionally ==== and ====).","We consider the allocation of indivisible private objects based on claimants’ preferences, and without monetary transfers. These problems are typified in the assignment of school seats to students, public housing to families, or office rooms to staff. In general, these indivisible objects are referred to in the literature as ‘houses’ (Shapley and Scarf, 1974). We consider two applications, differing only in the initial ownership of the houses. In ‘house allocation’, houses are initially collectively owned and an allocation rule distributes them among agents. In ‘housing markets’, each agent initially owns a house and an allocation rule redistributes the houses among them.====An axiomatic approach to these models identifies normative criteria that any allocation rule should satisfy, and tries to find rules that (perhaps uniquely) satisfy them. Over the years, a number of desirable properties for allocation rules have been proposed. Since agents must report privately-known preferences to the rule, no agent should find it beneficial to unilaterally misreport her preferences (i.e., the rule should be ‘strategy-proof’). ‘Pareto-efficiency’ is another desirable criterion, in that the final allocation reached should be undominated in welfare terms by any other allocation that could have been reached. ‘Consistency’ is an invariance condition which requires that a rule make coherent assignments for different groups of agents. A weaker requirement than consistency is ‘non-bossiness’, whereby an agent cannot change the assignment of other agents without changing her own assignment as well. Specific to housing markets, ‘individual rationality’ requires that no agent should ever be worse off from participating (i.e., she must always be able to at minimum leave with the house she came with).====We contribute to the literature by proposing a new ‘flexibility’ criterion for allocation rules. To motivate this exercise, consider for example the ‘constant’ rule, that produces the same allocation regardless of the preferences reported by agents. This rule is totally inflexible, in that it responds in no way to agents’ preferences. Consider also the ‘pure dictatorship’ rule that determines the assignment of each agent based on a single agent’s preferences.==== ==== This rule is almost entirely inflexible, in that it does not respond to any agent’s preferences except those of the pure dictator. Such rules may of course have merit in specific applications. However, it is somewhat unsatisfying that they ignore agents’ preferences in determining the allocation. Such inflexibility also results in such rules failing to be Pareto-efficient.====A desirable allocation rule thus needs to be flexible in its response to agents’ preferences. There are many ways to think about this flexibility; in this paper we propose a specific idea. Take some agent ====, fix the preferences of other agents, and suppose that this agent considers reporting some preferences (call it ====, denoting a strict ranking over houses) to the rule, which would give her an assigned house ====. Take some other house ====, and suppose that agent ==== reports instead preferences ==== to the rule, in which the houses ==== and ==== have simply exchanged their positions from ====, and the relative ranking of all other houses remains the same. We say that such preferences ==== are ‘swap preferences’ of ==== for ==== and ====. We are interested in whether agent ==== is assigned house ==== as a result of her changed preferences. If indeed the rule assigns agent ==== house ==== when she reports ====, and is assigned house ==== when she reports swap preferences ====, then we call her ‘swap-sovereign’ for ==== and ==== at this ‘profile’ of agents’ preferences.====This idea can also be extended to pairs of agents. Suppose that at a profile of preferences, agent ==== is assigned a house ====, and agent ==== is assigned some other house ====. Holding other agents’ preferences fixed, if it is the case that they swap assignments (i.e., ==== is assigned ==== and ==== is assigned ====) when they ==== report the swap preferences for ==== and ====, we say that they are ‘mutually swap-sovereign’ for that pair of houses at that profile. It should be noted that such preference changes are not necessarily strategic, in the usual sense of agents reporting false preferences in order to make themselves better off.==== ====We use these notions of swap-sovereignty to propose two versions of flexibility of an allocation rule. ‘Individual swap-flexibility’ requires a swap-sovereign agent to exist for each pair of houses. That is, at each profile of preferences, and for each pair of houses, there is required to be an agent assigned one of those houses who, by reporting swap preferences on that pair of houses, is assigned the other house.’ Our second condition, ‘mutual swap-flexibility’ requires mutually swap-sovereign agents to exist for each pair of assigned houses. In other words, it should always be possible for a pair of agents to exchange their assignments by together reporting swap preferences over that pair. These conditions depend on the profile of agents’ preferences, and so the identity of swap-sovereign or mutually swap-sovereign agents for a pair of houses could vary across profiles.====As illustration, we consider some well-known classes of allocation rules in our two applications. The ‘top-trading-cycles’ (TTC) rule for housing markets (Shapley and Scarf, 1974, Ma, 1994) is based on an iterative procedure in which agents ‘trade’ their initially owned houses. The TTC rule is also known as the ‘strict core mechanism’, since it always produces an allocation that no group of agents can better by trading only among themselves. The TTC rule is variously strategy-proof, individually rational, Pareto-efficient and non-bossy (Ma, 1994, Pápai, 2000a, Pycia and Ünver, 2017). We show that the TTC rule is not individually swap-flexible, and that indeed no individually rational rule in housing markets can be individually swap-flexible (Proposition 1). We show however that the TTC rule is the ==== rule that is strategy-proof, individually rational and mutually swap-flexible (Theorem 2). This result complements the existing characterisations of the TTC rule (see also Abdulkadiroğlu and Che, 2010).====We study priority rules in house allocation problems. Priority rules are a class of ‘queue-based’ rules which operate via an algorithmic procedure in which, given a preference profile, at each step a distinct agent is selected and assigned her top-ranked house from among those that remain from assignments made in previous steps. The simplest rule in this class is the ‘serial priority’ rule, in which the same ====th agent is selected in the ====th step for any preference profile. More generally, a ‘sequential priority rule’ allows the selection of subsequent agents to change as a function of ==== made to earlier agents. The identity of selected agents could thus differ substantially across preferences profiles. The most general rule we consider is a ‘priority rule’, where the selection of subsequent agents could change as a function of the ==== of earlier agents, and thus can vary even more significantly across preference profiles. Each serial priority rule is a sequential priority rule, which is in turn a priority rule, but the converse implications do not hold in general. As is well-known in the literature, both sequential and serial priority rules are non-bossy (see, e.g., Svensson, 1999, Pápai, 2001).==== ==== However, priority rules in general are bossy, since an agent could affect the ordering of agents who come after her in the ordering by suitably changing her preferences, even without changing her assignment.====In Theorem 1 we characterise the class of priority rules in terms of strategy-proofness, ‘envy non-bossiness’,==== ==== and individual swap-flexibility. To the best of our knowledge, this is the first characterisation of priority rules in the literature, and this is a useful contribution because this class features often in papers on house allocation as an example of rules that violate non-bossiness. We show that strengthening envy non-bossiness to non-bossiness gives us precisely the class of sequential priority rules. We also provide a new characterisation of the class of serial priority rules, based on the additional requirements of ‘pair-non-bossiness’==== ==== and ‘pair-sovereignty’.==== ==== We highlight that Theorem 1 is a unified treatment of the class of priority rules and the subclasses identified above. In particular, strategy-proofness and individual swap-flexibility are common to the entire class of rules, and subclasses are distinguished (essentially) in the strength of non-bossiness.====Our paper complements results in the literature on house allocation and housing markets. Serial priority rules are the only strategy-proof, ‘neutral’, and non-bossy house-allocation rules(Svensson, 1999). We do not assume neutrality for our results. Serial priority rules are the only Pareto-efficient, consistent, ‘conversely consistent’ and neutral rules (Ergin, 2000). Further, in a model with endogenous information acquisition, serial priority rules are the unique rules that are ex-ante Pareto-efficient, strategy-proof and non-bossy (Bade, 2015). Moreover, any Pareto-efficient allocation can be reached using a serial priority rule with a suitable ordering of agents (Abdulkadiroğlu and Sönmez, 1998, Bade, 2020). In housing markets, the TTC rule is the unique strategy-proof, individually rational and Pareto-efficient rule (Ma, 1994), and is the only rule that is Pareto-efficient, strategy-proof, ‘independent of irrelevant rankings’ and satisfies ‘mutual best’ (Morrill, 2013). Both the serial priority rule and the TTC rule can be embedded in unified classes of rules that are group-strategy-proof, Pareto-efficient (Bird, 1984, Pápai, 2000a, Pycia and Ünver, 2017) and ‘reallocation-proof’ (Pápai, 2000a). We do not use Pareto-efficiency for our results.",Swap-flexibility in the assignment of houses,https://www.sciencedirect.com/science/article/pii/S0304406820300823,13 August 2020,2020,Research Article,190.0
"Delacrétaz David,Kominers Scott Duke,Nichifor Alexandru","Department of Economics and Nuffield College, University of Oxford, New Road, Oxford OX1 1NF, United Kingdom,Entrepreneurial Management Unit, Harvard Business School, Soldiers Field, Boston, MA 02163, United States of America,Department of Economics and Center of Mathematical Sciences and Applications, Harvard University, Cambridge, MA 02138, United States of America,National Bureau of Economic Research, Cambridge, MA 02138, United States of America,Faculty of Business and Economics, University of Melbourne, 111 Barry Street, Parkville, 3010, Victoria, Australia","Received 17 February 2020, Revised 24 May 2020, Accepted 29 May 2020, Available online 6 August 2020, Version of Record 6 August 2020.",https://doi.org/10.1016/j.jmateco.2020.05.010,Cited by (0),"We prove a natural comparative static for many-to-many matching markets in which agents’ choice functions exhibit ====: reducing the extent to which some agent discounts additional partners leads to improved outcomes for the agents on the other side of the market, and worsened outcomes for the agents on the same side of the market. Our argument draws upon recently developed methods bringing tools from choice theory into matching.","Delacrétaz et al. (2019) introduced a general family of valuation functions with ====, under which each agent’s value for different sets of potential partners is given by an additive valuation over individual agents, discounted by a term that depends on the total number of agents in the set.==== ==== The additivity of ====’s valuation ensures that each agent with whom ==== may partner is evaluated independently from any other agent in the set; thus, there are ==== among potential partners. Meanwhile, the discount term captures the idea that agent ====’s marginal value for partners decreases as his number of partners increases; thus, there is ==== among potential partners.====We study many-to-many matching markets with size-dependent discounts and investigate the welfare implications of a discount reduction, under which an agent becomes more willing to accept additional trading partners. Our main result (Theorem 1) shows the intuitive comparative static that a decrease in one agent’s discounts makes all other agents on his side of the market worse off, and all agents on the other side of the market better off. To prove our main result, we draw upon recently developed methods that bring tools from choice theory into matching: First, we show that valuation functions with size-dependent discounts induce ==== choice functions; moreover, discount reductions lead to ==== of those choice functions. Then, to complete our proof, we invoke a powerful comparative static result of Chambers and Yenmez (2017) that applies to all expansions of path-independent choice functions.==== ==== We also show that our main conclusion continues to hold if the discounts of ==== agents on one side of the market decrease (Corollary 1). However, the effect of a simultaneous reduction in the discounts of agents on ==== sides of the market is ambiguous, even if the discounts decrease by the exact same amount (Example 1). Finally, we show that our main findings can be sharpened to cover deferred-acceptance-like mechanisms by comparing side-optimal stable matchings (Corollary 2).",Comparative statics for size-dependent discounts in matching markets,https://www.sciencedirect.com/science/article/pii/S0304406820300732,6 August 2020,2020,Research Article,191.0
"Lando Tommaso,Bertoli-Barsotti Lucio","University of Bergamo, Department of Management, Economics and Quantitative Methods, Via dei Caniana 2, Bergamo, Italy,VŠB-Technical University of Ostrava, Department of Finance, Sokolskà Trida 33, Ostrava, Czech Republic","Received 5 March 2020, Revised 16 May 2020, Accepted 18 July 2020, Available online 28 July 2020, Version of Record 8 August 2020.",https://doi.org/10.1016/j.jmateco.2020.07.005,Cited by (6),", namely ","In the theory of decision under uncertainty, decision makers measure their preferences regarding uncertain prospects by assigning different weights, to be interpreted either as misjudgements or as subjective revisions, to the outcomes of the corresponding random variable (RV) or to the corresponding probabilities. Mathematically, this weighting process may be formulated as a transformation of the values of the RV or of its cumulative distribution function (CDF), which, in turn, may be expressed, for instance, in terms of ==== (integrated CDFs, integrated quantiles, etc.), ==== (functions of the RV) or ==== (functions of the CDF). Based on different combinations of such transformations, the theory of stochastic dominance (SD) provides tools for representing preferences and risk attitudes.====In this context, the most commonly used SD relations are ==== and ==== (FSD, SSD, respectively), owing to their several applications in areas such as economics, finance and insurance. Basically, FSD represents any decision maker who prefers “more” to “less”, whereas SSD represents any decision maker who is also risk averse. Subsequently, weaker preferences can be represented by SD relations of higher degrees (Menezes et al., 1980, Muliere and Scarsini, 1989).====Although most decision makers are represented by FSD, it is generally not easy to establish whether one uncertain prospect is “bigger” than another. Thus, the discriminative power of FSD is generally poor. On the other hand, SSD might be limiting for those decision makers who are mostly risk averse but may have some degree of flexibility in their preferences and therefore exhibit a weak risk attitude, as discussed by Leshno and Levy (2002) or, more recently, by Müller et al. (2017). This may be illustrated by a simple example: consider a choice between (1) a sure gain of ==== and (2) the 50–50 lottery of gaining 0 or ====, in which we assume ==== to be positive and arbitrarily small compared with ====. Arguably, for small ====s, most decision makers (not only the risk averters) would choose (1). Hence, neither FSD nor SSD represents preference for the sure gain. Proper justifications for preferences of this type may be provided by a general approach, making it possible to interpolate FSD and SSD.====In the literature, this issue has been addressed in various ways. Fishburn, 1976, Fishburn, 1980 established continua of SD relations via fractional integration. Leshno and Levy (2002) defined the ==== of the first and second order, which allows for small violations of the FSD or SSD rules, whereby the weight of such violations is controlled by a real parameter. Tzeng et al. (2013) proposed an adjustment for the main theorem in the paper by Leshno and Levy (2002). Tsetlin et al. (2015) generalized second-order almost SD to dominance rules of a higher degree. Müller et al. (2017) introduced a new family of stochastic orders for expected utility maximizers, covering preferences from FSD to SSD and extend it to the risk-loving case as well, obtaining the almost SD as a special case. Huang et al. (2019) generalized SSD through the concept of SSD w.r.t. a function (Meyer, 1977) and filled the gaps between FSD, SSD and the higher degree SD relations.====In this paper we draw inspiration from these works, but we look at the problem from a different perspective. Whilst all the approaches cited above are related to the well known expected utility model, we refer to the dual model of Yaari (1987), in which utilities – transformations of the RVs – are replaced by distortions of the corresponding CDFs. We recall that a distortion is a nondecreasing function ==== such that ==== and ====
 (Wang and Young, 1998), generally interpreted as a subjective weighting of the original CDF, in which the choice of==== may represent different ways of measuring uncertainty. The expected utility approach (i) and the dual approach (ii) have similar purposes but differ from a mathematical point of view. In fact, SD relations may be characterized in terms of classes of ==== (order-preserving) functionals: following (i), we study isotonicity of expected utilities; following (ii), we study isotonicity of ==== (see for instance Chateauneuf, 1991, Wang and Young, 1998, Maccheroni, 2004, Sordo and Ramos, 2007), also known in the literature as ====, ==== or ====. Whilst FSD and SSD can be characterized dually through (i) or (ii), this is not true in general: approach (i) is related to SD rules based on integrated distributions, whereas approach (ii) is related to integrated quantiles (Muliere and Scarsini, 1989, Maccheroni et al., 2005).====Levy and Wiener (1998) studied the SD relations between distorted distributions and investigated the classes of distortions that preserve FSD and SSD. Following their approach, in Section 2 we compare RVs of which the CDFs are transformed through a common distortion function ====, where, for technical reasons, ==== is assumed to be an absolutely continuous CDF with support ====. Consequently we define a semiparametric family of stochastic orders, denoted by ====, ====. We study the relationships among the orders ==== for different choices of ====. Basically, the strength of ====-DSD is related to the shape of the distortion function and especially to its degree of concavity/convexity. If ==== is ==== than ==== in the sense of Chan et al. (1990), then ====-DSD implies ====-DSD. By varying the degree of convexity of ====, we may establish a continuum of SD relations, from FSD to SSD and beyond (i.e., weaker than) SSD. This can be achieved by focusing on a parametric family of distortion functions ==== and, consequently, by defining a parametric family of stochastic orders. In particular, the class of power distortions ====, gives rise to the ==== (====) of order ====, where the order determines the strength of the SD relation and ==== determines its degree of risk aversion. In Section 3, we show that PDSD satisfies some desirable conditions, yielding FSD as a limiting case and SSD as a special case.====Insofar as ====-DSD generalizes SSD, in Section 4, we apply the same approach to generalize the ==== (ICX), an order that is somewhat complementary to SSD (Shaked and Shanthikumar, 2007). Differently from SSD, the ICX represents decision makers who prefer “more” to “less” but are also “risk lovers”. We define a generalization of ICX, via a distortion function ====, and denote it as ====. Given two (possibly) different distortions ====, ====-DSD and ====-risk-loving-DSD can be combined to define the ====-==== order. Similarly to what has been undertaken recently by Müller et al. (2017), ====-mixed-DSD imposes constraints on aversion as well as attraction to risk, expressed in terms of ====.====In Section 5, we characterize DSD, risk-loving-DSD and mixed-DSD in terms of isotonic distorted expectations and generalize some results of non-expected utility theory (Chateauneuf, 1991, Wang and Young, 1998, Sordo and Ramos, 2007). We show that a distorted expectation preserves ====-DSD iff it is derived from a distortion that is less convex than ====, whereas risk-loving and mixed-DSD are preserved iff the distortion function satisfies some similar intuitive shape constraints.====Proofs and examples are given in theAppendix.",Distorted stochastic dominance: A generalized family of stochastic orders,https://www.sciencedirect.com/science/article/pii/S0304406820300811,28 July 2020,2020,Research Article,192.0
"Loubergé Henri,Malevergne Yannick,Rey Béatrice","Geneva School of Economics and Management, University of Geneva, Uni Mail, Pont d’Arve 40, 1211 Geneva 4, Switzerland,Université Paris 1 Panthéon-Sorbonne, PRISM Sorbonne EA 4101, F-75005 Paris, France,LabEx ReFi, F-75005 Paris, France,Université de Lyon, Université Lumière Lyon 2, GATE UMR 5824, F-69130 Ecully, France","Received 30 April 2020, Revised 17 July 2020, Accepted 17 July 2020, Available online 25 July 2020, Version of Record 8 August 2020.",https://doi.org/10.1016/j.jmateco.2020.07.004,Cited by (1), utility functions due to a particular feature of this family of utility functions.,"The work of Eeckhoudt and Schlesinger (2006) provided a refreshing perspective for the study of decisions under risk. Until the publication of their seminal article, and since Friedman and Savage (1948) at least, it was common to start from an economic decision under risk (insurance, gambling, investment, consumption and saving, production, protection, among others) and to analyze it using utility theory. As a result, specific properties of the von Neumann–Morgenstern utility function were associated to specific traits of attitudes towards risk. A negative second derivative of the utility function reflected risk aversion (Friedman and Savage, 1948, Pratt, 1964). A positive third derivative reflected prudence and was associated to precautionary behavior (Kimball, 1990). A negative fourth derivative was defined as temperance and invoked to explain the demand for risky assets in the presence of background risks (Kimball, 1992).==== ==== In the same vein, a positive sign of the fifth derivative was more recently associated to edginess to explain the effects of background risk on precautionary saving (Lajeri-Chaherli, 2004). Similarly, an ====-shaped utility function or a state-dependent utility function was used to rationalize simultaneous purchasing of insurance and lottery tickets, etc.====An alternative but less popular approach was initiated by Rothshild and Stiglitz (1970), who focused on properties of the probability distribution of outcomes and defined risk aversion as preference to avoid increases in risk, defined as a mean-preserving spread of the distribution. Their (second-order) approach was completed by Menezes et al. (1980) and by Menezes and Wang (2005) who introduced respectively downside risk aversion and aversion to outer risk by addressing third-order and fourth-order changes in the probability distribution of outcomes. All these authors provided also a link with utility theory, showing how their definitions could be associated to the signs of successive derivatives of the von Neumann–Morgenstern utility function.====Instead of focusing ==== on an economic decision model or on the shape of the probability distribution of outcomes, Eeckhoudt and Schlesinger (2006) started from the behavior of individuals exposed to simple equal-probability lotteries. Their choices were then used to define risk aversion, prudence, temperance, and so on, and the term “risk apportionment” was coined to describe how these behavioral traits were reflected in individual choices. For example, temperance reflects the preference for not associating an additional zero-mean risk ==== to a situation where the decision maker (DM) is already exposed to a prevailing zero-mean risk ====. The ====–==== lottery ==== is preferred to the ====–==== lottery ====. Preference for risk apportionment means preference for disaggregation of harms, given risk aversion. The beauty of this more primitive approach to attitudes towards risk is that it does not need familiarity with utility theory to be understood, although it is perfectly consistent with the traditional approach based on specific properties of the utility function. Preference for risk apportionment (or harms disaggregation) in successive increasingly complex lotteries translates into alternating signs of successive derivatives of the utility function (mixed risk aversion, as defined by Caballé and Pomansky, 1996).====As stressed by Eeckhoudt (2012), starting from these premises to analyze the direction of preferences under risk and to link them to successive derivatives of the utility function is appropriate. It is more robust than the traditional approach starting from an economic decision model (==== optimal saving), and deriving the sign of the ====th-order derivative of the utility function reconciling the model’s results with expected DM’s behavior. This latter approach may collapse as soon as changes are introduced in the structure of the economic decision model (see examples in Eeckhoudt, 2012).====The Eeckhoudt and Schlesinger (2006) approach, using zero-mean risks, received large support in experimental work (Deck and Schlesinger, 2010, Deck and Schlesinger, 2014, Deck and Schlesinger, 2018, Ebert and Wiesen, 2011, Trautmann and van de Kuilen, 2018, Attema et al., 2019). It was also generalized by Eeckhoudt et al. (2009b) to any couple of risks linked together by properties of increases in risk or stochastic dominance at any order.==== ==== Indeed, considering four mutually independent risks ====, ====, ==== and ==== such that ==== dominates ==== by ====-order stochastic dominance for ====, an expected utility maximizer with a mixed risk averse (MRA) utility function up to order ==== prefers the ====–==== lottery ==== to the ====–==== lottery ====.==== ==== In the former lottery risk apportionment holds. There is disaggregation of harms. In the latter lottery this is not the case. Instead of combining “good with bad” in the two possible lottery outcomes, the lottery yields “bad with bad” in the first outcome and “good with good” in the second outcome.==== ====Eeckhoudt and Schlesinger (2006), as well as Eeckhoudt et al. (2009b), consider additive risks. In the above lotteries, the final outcomes are either ==== and ==== on the one hand, or ==== and ==== on the other hand. Subsequent research addressed risk apportionment for multiplicative risks. The analysis is thus restricted to non-negative random variables. Multiplicative risks are observed in various circumstances in economic and social life. For example, investing in an asset denominated in foreign currency exposes the domestic investor to two multiplicative risks, the risk of the asset itself and the risk of variations in the domestic currency value of the foreign currency. Similarly, taking a job with a variable income in a firm exposed to bankruptcy results for the wage earner in a range of final outcomes where the two risks interact multiplicatively. Wang and Li (2010) addressed risk apportionment with multiplicative risks specifically. Building on results obtained by Eeckhoudt et al. (2009a) in a related context – see also Eeckhoudt and Schlesinger (2008) – they reached the conclusion that there is a direct relation between multiplicative risk apportionment at order ==== and the value of ====-degree relative risk aversion.==== ==== A similar result was obtained by Chiu et al. (2012) using a model combining two additive risks with a multiplicative effect on the first risk, an ====-degree shift of stochastic dominance on this risk, and a first-degree shift of stochastic dominance on the second risk. Again, the value of ====-degree relative risk aversion is critical to conclude whether risk apportionment in the sense of Eeckhoudt and Schlesinger (2006) is obtained or not.==== ====This literature was completed by works addressing risk apportionment in a bivariate context, a context where the decision-maker’s preferences are driven by the joint effects of two independent risks, for instance risks affecting wealth and health as in Eeckhoudt et al. (2007). In particular, Jokung (2011) and Denuit and Rey (2013) provided a new look at risk apportionment with additive or multiplicative risks by analyzing these cases as specific cases of risk apportionment in a bivariate context.====In this paper, we focus on additive and multiplicative risks and we start by providing homogeneous definitions for risk apportionment in these two risk contexts when the couples of risks are ranked by stochastic dominance. We then derive two theorems spelling out the exact conditions that a utility function must fulfill to yield additive or multiplicative risk apportionment. The first theorem, dealing with additive risks, provides an equivalence between risk apportionment and mixed risk aversion, instead of an implication as in Eeckhoudt et al. (2009b). The second theorem, dealing with multiplicative risks, provides also an equivalence between risk apportionment and a property of the DM’s utility function. Not surprisingly, this new condition is much more complex than the mere condition of mixed risk aversion obtained in the additive case.==== ==== It soon appears, however, that it is related to the value of the ====-degree relative risk aversion for ====, where ==== is a stochastic dominance order.====We then focus our attention on additive risks and we obtain a simple result for risk apportionment preferences of a DM in different situations. A DM displays a preference for risk apportionment when facing risk ==== dominated by risk ==== at order ==== and risk ==== dominated by risk ==== at order ==== if, and only if, he displays a preference for risk apportionment when facing two other couples of risks ==== and ==== related by stochastic dominance orderings at orders ==== and ==== respectively, provided that ====. The contribution of this result to the literature is to emphasize a correspondence between risky choices by a DM in two different circumstances involving additive risk combinations, as long as the sum of stochastic dominance orders linking the risk combinations in each circumstance is the same.====Our main motivation is then to check whether this result still applies when the two risks combine multiplicatively. We show that the answer is negative in general and we explain why. We then proceed in two steps. Firstly, we show that preference for multiplicative risk apportionment when the two pairs of risks are characterized by stochastic dominance orders ==== and ==== is a sufficient condition for the preference for multiplicative risk apportionment when the stochastic dominance orders are ==== and ====, whatever ====. Secondly, we derive a simple condition under which the converse relation also holds when ==== is either equal to one, ==== in case of first-order stochastic dominance (FSD), or two, ==== in case of second-order stochastic dominance (SSD). In both cases, relative risk aversion at order ==== plays a decisive role. In the second case, in particular, the property under scrutiny holds whenever relative risk aversion at order ==== is decreasing, for ====. Proceeding further, ==== dealing with ====, is technically challenging and does not lead to relevant economic interpretations.====As a consequence, we finally turn to the specific case where the DM’s preferences are reflected in a Constant Relative Risk Aversion (CRRA) utility function — the function most commonly used in the literature. We obtain that our result derived for additive risks extends to multiplicative risks, independently of the stochastic dominance orders relating the two couples of risks ==== and ====, for all CRRA utility function whose relative risk aversion exceeds one.==== ==== This strong result is driven by a simple property of the CRRA utility function that is often neglected in the literature.====Our paper is organized as follows: Section 2 provides our definitions for additive and multiplicative risk apportionment and states our core characterization theorems (Theorems 1 and 2). Section 3 focuses on additive risks and derives our result of equivalence between risk apportionment at orders ==== and ==== and risk apportionment at orders ==== and ==== when ====
 (Proposition 1). Section 4 turns towards risk apportionment in a multiplicative risks context and explains why our result from Section 3 does not generalize in the latter context. Section 5 first establishes that the preference for multiplicative risk apportionment at orders ==== and ==== is sufficient for the preference to hold at orders ==== and ====
 (Proposition 2). It then derives the converse result when ====
 (Proposition 3). Then, Section 6 turns to the specific case of CRRA utility functions and shows that our results from Section 3 and Section 5 hold generally in a multiplicative risks context, whatever ====, ====, ==== and ==== provided relative risk aversion is larger than one (Theorem 3 and Proposition 4). Section 7 concludes briefly. Most of the proofs are displayed in an Appendix A A reminder on stochastic dominance and the proof of Lemma 1(ii) in, Appendix B Proof of, Appendix C Proof of, Appendix D Proof of, Appendix E Proof of, Appendix F Proof of, Appendix G Proof of, Appendix H Proof of at the end of the paper.",New Results for additive and multiplicative risk apportionment,https://www.sciencedirect.com/science/article/pii/S030440682030080X,25 July 2020,2020,Research Article,193.0
"Lou Youcheng,Wang Shouyang","MDIS, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, China","Received 2 January 2020, Revised 3 June 2020, Accepted 8 July 2020, Available online 24 July 2020, Version of Record 3 August 2020.",https://doi.org/10.1016/j.jmateco.2020.07.003,Cited by (1),"This paper gives a new approach to show the existence and regularity of linear equilibrium established by Lou ⓡ al. (2019) for a noisy rational expectations economy. Different from the existing method which essentially requires to find a fixed point of a system of ====, the new approach is operated directly on an alternative form of market-clearing conditions. One main advantage of the new approach is that besides homogeneous-valuation economies, it can also handle the existence of equilibrium in economies with heterogeneous valuations where the existing method for dealing with homogeneous-valuation economies fails to work.","Rational expectations equilibrium (REE) economies have been widely studied since the pioneering works of Grossman (1976), Hellwig (1980) and Grossman and Stiglitz (1980). Grossman (1976) proposes a REE economy with a constant supply of the risky asset and show that the equilibrium price perfectly aggregates all private information of market participants. To prevent prices from becoming fully revealing, noise trading is introduced into the economy to make the equilibrium price only partially revealing in a finite-agent setting (Hellwig, 1980) and a continuum-agent setting (Grossman and Stiglitz, 1980).====Lou ⓡ al. (2019) generalize the finite-agent economy in Hellwig (1980) to a continuum-agent economy with general signal structure where traders’ signals are multidimensional and arbitrary correlation pattern between the components of traders’ signals and between the fundamental of the risky asset and signals is allowed.==== ====  Lou ⓡ al. (2019) transfer equivalently the equilibrium existence problem into a fixed-point existence problem. When there is no idiosyncratic noise, the authors develop a new technique to solve the equilibrium existence problem because in this case, the underlying fixed-point function is not uniformly bounded and consequently, Brouwer’s fixed-point theorem cannot be applied directly. Specifically, the authors first construct an auxiliary sequence of uniformly bounded functions, and then get a fixed point for every such a function. Taking the limit point of this sequence of fixed points (which is shown to be bounded) gets a fixed point of the original function which involves a system of nonlinear algebraic equations, coming from coefficient matching based on market-clearing conditions.==== ====This paper aims to present a ==== approach to show the ==== and ==== of linear equilibrium in the noisy REE economy in Lou ⓡ al. (2019). The existence of linear equilibrium is shown by applying Brouwer’s fixed-point theorem to a function constructed directly from an alternative form of (one-dimensional) market-clearing conditions, ==== to a system of nonlinear algebraic equations (which was done in Lou ⓡ al. (2019)). To be specific, we first construct an auxiliary price function in which an independent random variable is additionally introduced, and write the market-clearing condition in an alternative form where one side is the price function, while the other side is a term involving a variance-adjusted conditional expectation. We then construct two functions which map to the coefficients on signals and noise trade in the price function based on the alternative form of market-clearing conditions, and show the continuity and uniform boundedness of the two functions based on some elegant properties of conditional expectation and variance. Applying Brouwer’s fixed-point theorem to get a sequence of fixed-points. In addition, we show that any limit point of the coefficient on noise trade does not equal zero. Finally, taking the limit of both sides of the alternative market-clearing condition gives a linear equilibrium. Furthermore, in two cases when noise trading is large or signals take the classical form of the sum of the fundamental and an independent noise, the regularity of equilibrium prices (i.e., an increase in noise demand implies a higher price) is also established.====Working directly on the one-dimensional market-clearing condition (instead of a system of nonlinear equations) facilitates us to utilize some elegant properties of conditional expectation and variance, for example, the boundedness and monotonicity of conditional variance with respect to traders’ signals, the law of total variance, etc. It is worth remarking that our new approach can also be applied to show the existence and regularity of an extended version of the model in  Lou ⓡ al. (2019) where traders valuate the risky asset ====. However, the method in Lou ⓡ al. (2019) ==== be applied to solve equilibrium existence of economies with heterogeneous valuations because it depends crucially on one property, which does not hold for heterogeneous-valuation economies in general; see Section 4 for more illustrations.====Except for the work mentioned above, our work also relates to the literature on equilibrium existence and uniqueness of REE economies. Pálvölgyi and Venter (2015a) show that the linear equilibrium in Grossman and Stiglitz (1980) is unique in the class of all continuous price functions. Barlevy and Veronesi (2000) consider a setting where the fundamental is binomial and investors are risk-neutral instead of the classical normality-CARA assumption.  Breon-Drish (2015) analyzes the finite-agent economy of Hellwig (1980), but with an extension to more general signal structures of the exponential family. There is also literature that studies existence and uniqueness of an equilibrium for economies with multiple risky assets, for instance, Pálvölgyi and Venter (2015b), Chabakauri et al. (2017) and Carpio and Guo (2019). In addition, our work is also related to heterogeneous-valuation economies; see Rostek and Weretka (2012), Vives (2014) and Rahi and Zigrand (2018). Indeed, traders are possibly uncertain about the values of risky assets and valuate them heterogeneously. Heterogeneity of valuations can also be interpreted as arising from the fact that traders valuate their investment from different perspectives, for instance, short-term returns, long-term returns, and the volatility of prices, etc.====The paper is organized as follows: Section 2 introduces the model. Section 3 presents the new approach to show the existence of linear equilibrium, and regularity of linear equilibrium in two special cases. Section 4 offers an extension to economies with heterogeneous valuations. Section 5 concludes the paper. All preliminary lemmas are in the Appendix.==== We follow the notation in Lou ⓡ al. (2019). All vectors are column vectors by default. The operator ==== will stand for variance and ==== will stand for covariance. For any vector ==== (where each component ==== is ====-dimensional, and ==== denotes the transpose of a vector) and random variable ====, ==== is shorthand for ==== and ==== stands for ====, where ==== is the aggregate signal in the economy. For any random variable ==== and ====-dimensional signal ====, ==== is shorthand for the vector of covariances ====. For any multidimensional random vector ====, ==== denotes the variance–covariance matrix of ====. Finally, ==== denotes the 2-norm of a vector.",A new approach to the existence and regularity of linear equilibrium in a noisy rational expectations economy,https://www.sciencedirect.com/science/article/pii/S0304406820300793,24 July 2020,2020,Research Article,194.0
Heumann Tibor,"Instituto de Economía, Pontificia Universidad Católica de Chile, Chile","Received 3 December 2018, Revised 19 June 2020, Accepted 4 July 2020, Available online 23 July 2020, Version of Record 30 July 2020.",https://doi.org/10.1016/j.jmateco.2020.07.001,Cited by (1),"We study sender–receiver games in which a privately informed sender sends a message to ==== receivers, who then take an action. The sender’s type space ==== has finite cardinality (i.e., ====). We show that every equilibrium payoff vector (resp. every Pareto efficient equilibrium payoff vector) is achieved by an equilibrium in which the sender sends at most ==== (resp. ====) messages with positive probability. We also show that such bounds do not exist when two privately informed senders simultaneously send a message to a receiver.","We study a model that consists of a sender and ==== receivers. The sender has a type in ====, where ====. The sender sends a public message to the receivers, who then take action. We allow for the possibility that the receivers also have private information, which we call the receivers’ type. The players’ payoff is a function of the sender’s type, the message sent by the sender, the receivers’ type and the actions taken by the receivers. For a given strategy profile, a payoff vector ==== specifies the ex ante expected utility of the receivers and the interim utility of the sender. The ==== payoff vectors are the set of payoff vectors that can be achieved by a strategy profile that is a perfect Bayesian equilibrium.====We show that every equilibrium payoff vector can be achieved by a perfect Bayesian equilibrium in which the sender sends at most ==== messages with positive probability. The proof of both bounds uses Carathéodory’s theorem to show that for any sender strategy, there exists an alternative sender strategy that has support on only ==== messages that (i) induces the same expected payoff for all agents assuming the receivers’ strategies remain the same and (ii) conditional on any message, the belief about the sender’s type remains the same. The second condition guarantees that the receivers’ best-response function remains the same under the alternative strategy. We also show that every Pareto optimal equilibrium payoff vector can be achieved by a perfect Bayesian equilibrium in which the sender sends at most ==== messages with positive probability. One less message is needed to attain all Pareto optimal payoff vectors because the set of Pareto optimal payoffs has a dimensionality of one less the dimensionality of all the payoff vectors.====We then give an example to show that the bounds are tight. In this example, there are ==== passive receivers who do not take any action in the game, while the remaining receiver takes an action. In this game, every payoff vector can be achieved only if the sender sends at least ==== messages with positive probability; every Pareto optimal payoff vector can be achieved only if the sender sends at least ==== messages with positive probability.====Finally, it is natural to ask whether the bounds could be extended to situations in which there are multiple informed senders. We provide an example to show that such bounds do not exist in games with two privately informed senders who simultaneously send a message to a receiver. The intuition for why the result cannot be extended is that Sender 1’s best-response function depends on the distribution of posterior beliefs induced by Sender 2’s strategy profile. Hence, to guarantee that Sender 1’s best response function remains the same, it is not enough for the posterior belief after each message sent by Sender 2 to remain the same. This is in contrast with the receivers’ best response function, which only depends on the posterior belief induced by any given message (but not on the ex ante distribution over messages). Hence, in general, when two senders simultaneously send a message to a receiver, each sender requires a message space equal to the simplex of her type space because a sender can potentially have one message for each possible belief of her type space.",On the cardinality of the message space in sender–receiver games,https://www.sciencedirect.com/science/article/pii/S030440682030077X,23 July 2020,2020,Research Article,195.0
"Fahrenwaldt Matthias Albrecht,Jensen Ninna Reitzel,Steffensen Mogens","Maxwell Institute for Mathematical Sciences, Department of Actuarial Mathematics and Statistics, Heriot-Watt University, Edinburgh EH14 4AS, United Kingdom,Department of Mathematical Sciences, University of Copenhagen, Universitetsparken 5, 2100 København Ø, Denmark","Received 2 October 2019, Revised 4 July 2020, Accepted 8 July 2020, Available online 16 July 2020, Version of Record 23 July 2020.",https://doi.org/10.1016/j.jmateco.2020.07.002,Cited by (2),"Recursive utility disentangles preferences with respect to time and risk by recursively building up a value function of local increments. This involves certainty equivalents of indirect utility. Instead we disentangle preferences with respect to time and risk by building up a value function as a non-linear aggregation of certainty equivalents of direct utility of consumption. This entails time-consistency issues which are dealt with by looking for an equilibrium control and an equilibrium value function rather than a classical ==== and a classical optimal value function. We characterize the solution in a general diffusive incomplete market model and find that, in certain special cases of utmost interest, the characterization coincides with what would arise from a recursive utility approach. But also importantly, in other cases, it does not: The two approaches are fundamentally different but match, exclusively but importantly, in the mathematically special case of homogeneity of the value function.","We formulate a continuous-time dynamic consumption–investment problem where preferences with respect to risk and time variability are disentangled. In contrast to recursive utility which also builds on the idea of disentangling these preferences, our value function is based on a time-global objective with non-time-additive utility. This allows for working with certainty equivalents of direct utility of consumption rather than indirect utility. Time-inconsistency arising from non-time-additivity is dealt with by looking for a subgame perfect equilibrium among a continuum of selves. We consider a general incomplete market with coefficients driven by a non-hedgeable economic state process. In special cases that include the Merton market, we find a resulting behavior that coincides with that coming from recursive utility with Epstein–Zin preferences. Among these special cases, we also find closed-form solutions to new problem formulations beyond standard power utility, including non-hedgeable consumer price indexation and exponential utility.====Thus, our contribution to the literature is two-fold: We base – we believe as the first – the disentanglement on a time-global objective for a general financial market. Second we detect new and relevant solvable consumption–investment problems in incomplete markets within our problem formulation where the solution coincides with what would have been obtained by recursive utility. This opens for a new direction of studies of non-time-additive utility where calibration of preferences with observed consumption–investment patterns may prove superior compared to the established directions. It is beyond the scope of this presentation to perform this calibration. We simply propose a new and seemingly powerful pattern of thinking.====Recursive utility was developed by Epstein and Zin, 1989, Epstein and Zin, 1991 based on work by Kreps and Porteus, 1978, Kreps and Porteus, 1979. It is celebrated for disentangling preferences with respect to risk and time. Its continuous-time limit, spoken of as stochastic differential utility or, simply, continuous-time recursive utility, developed by Duffie and Epstein (1992b) has the same ability to allow for separate preference functions against variability over risk and (continuous) time. It is widely used to study optimal consumption–portfolio choice in various markets, see e.g. Schroder and Skiadas, 1999, Schroder and Skiadas, 2005 and Kraft et al. (2013). Also, recursive utility is used to examine ambiguity aversion and preferences for resolution of uncertainty, see e.g. Chen and Epstein (2002) and Skiadas, 1998, Skiadas, 2013. Issues with differentiability when going to continuous-time were addressed by Kraft and Seifried, 2010, Kraft and Seifried, 2014. A particularity of recursive utility is, of course, the definitional recursive building of the value function or indirect utility function. This means that, when locally aggregating present consumption with the utility of future consumption, the latter is represented by its indirect utility. In the recursion appears the certainty equivalent with respect to the representative indirect utility of wealth rather than the underlying future uncertain consumption.====Indirect utility appears to be the right representative of utility of future consumption, given that we start out with a recursive definition. Yet, here we suggest to start out with a time-global objective built up by certainty equivalents with respect to future uncertain consumption. Said differently, we suggest to replace the indirect utility representation of future consumption by the direct utility of future consumption itself. Apart from that, our objective remains the same: To separate preferences for risk and time. Once having formed certainty equivalents of future consumption at different points in time, we think of them as “certain” values attributed to these time points. This allows for a non-linear aggregation of these certainty equivalents which relates to preferences with respect to time only. Our objective becomes non-linear in time which, at first sight, dumps the idea for reasons of time-inconsistency issues that are completely avoided with recursive utility. There, the controls are, definitional from the recursive structure, time-consistent, so why bother about time-inconsistency issues? Because, we find the construction of a time-global objective based on direct utility of future consumption instead of indirect utility appealing and, by now, the complications with time-inconsistency can be overcome. That is, because we should and because we can.====Already in the definition of recursive utility, time-consistency issues are delicately avoided. First the certainty equivalent of the indirect utility is formed. Then this is non-linearly time-aggregated with present consumption. The alternative order is unfriendly: To first non-linearly time-aggregate indirect utility and consumption and then take the expected utility here-of. It is the non-linear time-aggregation under uncertainty that leads to time-inconsistency issues. Although we suggest a completely different formulation, we also have time-inconsistency issues, but for different reasons. We make non-linear time-aggregation of objects we can think of as certain like it is done for recursive utility. But we aggregate over a global time-horizon rather than a local (one-period in discrete-time and infinitesimal in continuous-time) time-horizon, as in the case of recursive utility.====Time-inconsistent behavior was initially formalized by Strotz (1955). Pollak (1968), Goldman (1980), and Laibson (1997) contributed to the understanding of the problem as an intra-personal game and looked for subgame perfect equilibria. Ekeland and Pirvu (2008) defined a continuous-time subgame perfect equilibrium in order to deal with the time-inconsistency arising from replacing exponential discounting of utility by hyperbolic discounting. We follow their definition and derive the equilibrium value and equilibrium strategy when the time-inconsistency arises from the non-linear aggregation of certainty equivalents as explained above.====Other more recent works that draw on the subgame perfect equilibrium approach to time-inconsistency include Björk and Murgoci (2014) and Björk et al. (2017) who work with rather general preferences but exemplify with hyperbolic discounting and the mean–variance criterion, and the linear-quadratic criterion and non-exponential discounting, respectively. Ekeland et al. (2012) solve for non-exponential discounting with different discount functions related to consumption while alive versus (inheritors’) consumption upon death and Pirvu and Zhang (2014) solve a problem with regime-shifting coefficients in both markets and preferences. Kryger et al. (2020) introduce additional non-linearity compared to Björk and Murgoci (2014), such that new versions of the mean–variance, the mean-standard deviation, and the linear-quadratic problems can be solved.====The idea of summing up certainty equivalents over global time was also pursued by Jensen and Steffensen (2015). They considered a consumption–investment–insurance problem in a Merton market for an investor with an uncertain lifetime and access to life insurance. The disentanglement of preferences for risk and time is, there, a starting point for the idea of also disentangling utility of consumption as alive and inheritors utility of consumption after the death of the investor. Already they show that in the special case of a Merton market the solution to our optimization problem coincides with that of recursive utility with Epstein–Zin preferences. We obtain the coincidence with the Merton market and recursive utility from a different angle. In contrast to the certainty equivalence approach introduced by Jensen and Steffensen (2015), the classical approach to recursive utility was generalized to include lifetime uncertainty and utility from bequest by Jensen (2019).====We start with a general diffusive, incomplete market with a risky, diffusive asset with price coefficients driven by another diffusive economic state process that cannot be perfectly hedged. But we also characterize solutions for much more general markets that have previously been studied under recursive utility. This unveils, in terms of resulting behavior, a fundamental difference between recursive utility and our approach. In general cases, studied under recursive utility by Chacko and Viceira (2005) and Kraft et al. (2013), the generalized Bellman equation that we find to characterize our equilibrium value, contains additional terms compared to the standard recursive utility Bellman-type equation. Only when we have complete separability in time, wealth, and the economic state process, we agree with recursive utility on the characterization of the solution. On the other hand, we study in detail such special cases leading to linearly homogeneous value functions that, to our knowledge, have not been studied before. They include cases with power utility where we scale consumption by the economic state process, interpreting this process as an only partly hedgeable consumer price index, and cases with exponential utility. We provide explicit solutions in these cases.====The outline of the paper is as follows. In Section 2, we present the model for the price and wealth processes. We motivate our problem formulation and relate it to standard recursive utility. In Section 3, we define the set of admissible controls and the concept of equilibrium and state our main theorem with sufficient conditions to determine equilibrium controls and the corresponding equilibrium value function. In Section 4, we present two non-trivial examples of the framework with incomplete markets. We consider two different choices of the utility functions, namely power utility and exponential utility. We provide explicit solutions, and we establish a connection to recursive utility.",Nonrecursive separation of risk and time preferences,https://www.sciencedirect.com/science/article/pii/S0304406820300781,16 July 2020,2020,Research Article,196.0
"Caplin Andrew,Leahy John","New York University, United States of America,N.B.E.R, United States of America,University of Michigan, United States of America","Received 5 June 2019, Revised 23 February 2020, Accepted 10 June 2020, Available online 23 June 2020, Version of Record 11 July 2020.",https://doi.org/10.1016/j.jmateco.2020.06.003,Cited by (1),"This paper builds upon Caplin and Leahy (2014), which introduced a new mathematical apparatus for understanding allocation markets with nontransferable utility, as such covering the housing market and other markets for large ","Among the most significant of all markets are those, such as the housing market, that allocate large indivisible goods to households. Two features of these markets make them difficult to study using standard economic analysis. The first is indivisibility. One generally has to purchase one house or another. Decisions are not made on the margin and hence marginal analysis is generally inappropriate. The second is cost. The purchase of a home consumes a significant fraction of the wealth of the typical buyer. Trade in these goods therefore has a large impact on agents’ marginal utility of wealth, rendering inappropriate the tools that have been developed to study allocation markets with transferable utility.==== ==== A small change in parameters can affect prices, which affect wealth, which feed back onto prices. Because of the interaction between the allocation, discreteness, and the marginal utility of wealth, small changes in market parameters can cause a global reallocation of goods. In spite of this apparent chaos, we show in this paper that generically there is a very simple structure to comparative statics. The response of the market to any change is the cumulative result of a small number of basic market transitions.====Caplin and Leahy (2014) (henceforth CL) introduced a new mathematical apparatus for understanding the minimum price competitive equilibrium in allocation markets with non-transferable utility. CL showed that these equilibria correspond to the solutions to a certain optimization problem. This optimization is over a set of mathematical structures, called GA-structures, that combine an allocation of goods to buyers along with a particular class of directed graphs that summarize key indifference relations among buyers and goods. CL showed that small changes in parameter values generally leave unchanged the allocation and the indifference relations that characterize the minimum price competitive equilibrium. Given that “local” comparative statics involve a fixed GA-structure, changes in model parameters that cause the price of a given good to change impact only those goods that “follow” it in the corresponding directed graph.====In this paper we build on CL and use GA-structures to characterize comparative statics when the change in parameter values is large enough to alter the allocation or the indifference relations that characterize the minimum price equilibrium. We use homotopy methods to characterize how the minimum price competitive equilibrium changes in response to arbitrary parameter changes. Given an initial equilibrium, we follow a path through parameter space, building up the discrete change in equilibrium from infinitesimal ones. We show that along the generic path equilibria evolve in a very controlled manner. While there are examples in which the most minor change in parameters may cause the entire structure of the equilibrium to change in arbitrary ways, we show such cases to be the exception rather than the rule. We introduce a natural definition of regularity for comparative static paths and show generically that when a small parameter change alters the allocation or the structure of indifference, the resulting change takes one of five and only five distinct forms of market transition. We use GA-structures to illustrate the precise nature of each such transition. For reasons that will be clear, the market transitions are labeled as: Graft; Prune; Prune and Graft; Cycle and Reverse; and Shift and Replant. The homotopy path decomposes all comparative statics into these five basic operations.====Markets for indivisible goods are characterized by multiple equilibria. Due to the presence of non-convexities, small changes in prices often do not alter the equilibrium allocation. We focus on the minimum equilibrium price for a number of reasons. First, we learn about the boundaries of the set of equilibrium. The minimum and maximal equilibrium have the same structure. The maximum equilibrium price is simply the dual of the minimum, in which the roles of buyers and sellers are reversed. Second, CL established that all equilibria are in a well-defined sense minimum price equilibria of some suitably perturbed model. Finally, Demange and Gale (1985) show that the minimal equilibrium price vector is not manipulable by buyers, making it a natural benchmark for multi-unit auctions with unit demand.====We see several potential applications for our homotopy paths. First, they are well suited to algorithmic use. They can be used in principle to compute minimum price equilibria from a starting point with a trivial equilibrium. The resulting algorithms may help to overcome computational barriers to application of the nontransferable utility allocation model.==== ==== Second, it is also not hard to envision adopting our methods to studying the dynamic properties of markets for indivisible goods by recasting the homotopy paths as sample paths which characterize the evolution of the market over time. Landvoigt et al. (2015) employ related arguments to study the evolution of housing markets over time.====This paper contributes to our understanding of allocation markets with non-transferable utility. Kaneko (1982) was first to establish conditions for existence of equilibria in such markets. Demange and Gale (1985) showed under much the same conditions that the set of equilibrium prices is a lattice with maximal and minimal elements. They also established that the minimum price equilibrium cannot be manipulated by buyers, as well as some basic comparative static properties of the minimum price equilibrium. There are several algorithms in the literature. Kaneko and Yamamoto (1986) apply algorithms based on Kakutani’s fixed point theorem. Alkan (1989) develops an algorithm which computes equilibria when agents’ utility is piecewise linear. Dutting et al. (2009) develop a test for whether a potential allocation forms the basis for a competitive equilibrium. Their algorithm searches through the set of allocations until one is found that passes this test. Zhou and Serizawa (2019) compute equilibria by adding one good at a time, using the minimum price equilibrium for ==== objects to find the minimum price equilibrium for ==== objects. Our homotopy paths illustrate the structure underlying minimum price competitive equilibria, and provide further insight into the comparative statics of allocation markets with non-transferable utility.====Section 2 presents the general model and summarizes relevant results from CL. The additional results in this paper rest in part on our ability to count the number of distinct GA-structures. The key cardinality results are in Section 3. They are derived by connecting the number of GA-structures with the structure of market demand at minimum equilibrium prices. We show that there is typically only one GA-structure, and that the most important points of market transition involve two and only two such structures. Section 4 introduces the domain in which we study comparative statics, which involves paths through a rich space of model parameters. Section 5 identifies the sense in which comparative static transitions are almost always “regular”, in that the replacement for a given GA-structure is a unique second element that appears at a point of transition. Section 6 identifies the five generic forms of market adjustment. Section 7 describes how to use path-following methods to algorithmically identify the minimum equilibrium price. Section 8 concludes. All proofs are in Appendix.",Comparative statics in markets for indivisible goods,https://www.sciencedirect.com/science/article/pii/S0304406820300756,23 June 2020,2020,Research Article,197.0
"Bergantiños Gustavo,Moreno-Ternero Juan D.","ECOSOT, Universidade de Vigo, Spain,Department of Economics, Universidad Pablo de Olavide, Spain","Received 19 February 2020, Revised 23 April 2020, Accepted 5 June 2020, Available online 16 June 2020, Version of Record 23 June 2020.",https://doi.org/10.1016/j.jmateco.2020.06.002,Cited by (13)," rule, the ==== rule and ====.","In the era of streaming, sports has become the cornerstone to television programming. The popularity of televised sports events keeps increasing and, for sports organizations, the sale of broadcasting and media rights is currently their biggest source of revenue. This sale is often collective, which generates an interesting problem of resource allocation, akin to well-known problems already analyzed in the game-theory literature. Instances are airport problems (e.g., Littlechild and Owen, 1973, Hu et al., 2012), bankruptcy problems (e.g., O’Neill, 1982, Thomson, 2019), telecommunications problems (e.g., Nouweland van den et al., 1996), museum pass problems (e.g., Ginsburgh and Zang, 2003, Bergantiños and Moreno-Ternero, 2015), cost sharing in minimum cost spanning tree problems (e.g., Bergantiños and Vidal-Puga, 2007, Trudeau, 2012), or labeled network games (e.g., Algaba et al., 2020a, Algaba et al., 2020b).====In a recent paper (Bergantiños and Moreno-Ternero, 2020a), we introduced a formal model to analyze the problem of sharing the revenues from broadcasting sports leagues among participating teams. Two main rules were highlighted therein. On the one hand, the so-called ==== rule, which splits the revenue generated from each game equally among the participating players (teams). On the other hand, the so-called ====, which concedes each player (team) the revenues generated from its fan base (properly estimated) and divides equally the residual. Among other things, we showed that both rules are similarly characterized by just three properties. Two properties are common in both characterizations. One (====) states that two teams with the same audiences should receive the same amount; another (====) that revenues should be additive on the audience table. The third property in each characterization comes from a pair of polar properties modeling the effect of ==== or ==== teams. The ==== property states that if each game played by a team has no audience, then such a team (called null) receives nothing. The ==== property states that if only the games played by one team have positive audience, then such a team (called essential) receives all its audience. In a follow-up paper (Bergantiños and Moreno-Ternero, 2020b) we show that a third axiom (====) stating that each team receives at most the revenue generated by its overall audience, together with ==== and ====, characterizes the family of all rules generated by convex combinations of the ==== rule and ====.====A natural third rule (outside from the previous family) can also be considered for this model. It is the rule that divides the overall revenues generated in the tournament equally among all participating teams. We refer to it as the ====. This rule is used quite often in practice. For instance, the football competitions of England, Italy and Spain divide around one half of the revenues generated by TV broadcasting equally among all teams.====In this paper, we further explore the axiomatic approach to this problem and derive new interesting results that uncover the structure of this stylized model further. To do so, we consider new axioms that formalize alternative ways of allocating the extra revenue obtained from additional viewers.====On the one hand, we consider a group of axioms stating different ways in which a rule should react when additional viewers of some specific team appear. More precisely, assume that a given tournament has more viewers than another tournament just because the games involving a specific team (====) have more viewers. How should a rule allocate those extra viewers? Our axioms consider three possible answers. The first axiom just ignores the fact that all viewers come from games involving team ====. Then, all teams should equally share the extra benefits. We show that this axiom, together with ====, characterizes the ====. The second axiom considers that team ==== and the rest of the teams are in a symmetric position because the audience of team ==== has increased the same amount as the audience of the rest of the teams (combined). Then, the extra benefits of team ==== should be equal to the sum of the extra benefits of the remaining teams. We show that this second axiom together with ==== characterizes the ====. The third axiom says that team ==== is fully credited for such an improvement. We show that this third axiom, together with ====, characterizes ====.====On the other hand, we consider an axiom referring to the incremental effect of adding additional viewers to a game. The axiom (====) states that the involved teams in the game should be affected in the same amount. The same should happen for the non-involved teams. Our last three results show that the combination of this axiom with some other basic axioms also characterize the three rules mentioned above. More precisely, ====, together with ==== (more aggregate revenues cannot hurt any team) and ====, characterize the ==== rule. If, instead, we add to ==== the ==== axiom (mentioned above), we characterize the ==== rule, whereas if we add the ==== axiom (also mentioned above), we characterize ====.====The rest of the paper is organized as follows. We introduce the model, axioms and rules in Section 2. In Section 3, we provide the characterization results. First, those involving ====. Then, those involving ====. We conclude in Section 4.",Allocating extra revenues from broadcasting sports leagues,https://www.sciencedirect.com/science/article/pii/S0304406820300744,16 June 2020,2020,Research Article,198.0
"Aouani Zaier,Chateauneuf Alain","Department of Economics, University of Kansas, USA,IPAG Business School and Paris School of Economics, University of Paris 1, France","Received 25 October 2019, Accepted 3 June 2020, Available online 14 June 2020, Version of Record 26 June 2020.",https://doi.org/10.1016/j.jmateco.2020.06.001,Cited by (0)," functions are consistent with multidimensional PD transfers and that ====, defined using inframodular functions, with the concave order in the unidimensional framework.","The literature on income inequality and its measurement is abundant. However, inequality is not only about differences in income as several other attributes, such as housing, education, health, or environmental quality, might be just as important as income. Hence the need for a multidimensional approach to inequality and its measurement; see, e.g., Kolm (1977), Atkinson and Bourguignon (1982), and Tsui (1995). Given different multidimensional distributions, in which each individual is described by a vector of attributes, the main concern of inequality measurement is to provide tools allowing to define what it means for one multidimensional distribution to be more or less unequal than another, and to determine the ranking of those distributions by order of increasing/decreasing inequality.====In the unidimensional setting, when a single attribute is involved, e.g., income, the Pigou–Dalton (PD) transfer principle (see Atkinson, 1970), whereby an order-preserving income transfer from an individual to a poorer one is considered inequality-reducing, is the most prominent example of inequality measurement tools, and the result in Hardy et al. (1934) and Marshall et al. (2009) establishing the equivalence between dominance of one income distribution by another according to a finite sequence of Pigou–Dalton transfers and dominance according to the concave order, that is, according to the class of additive utilitarian social welfare functions with a concave utility function, is the most prominent tool for comparing income distributions.====Motivated by the pertinence of PD transfers in the single attribute case, we show that inframodular functions; the appropriate extension of one-dimensional concavity to the multivariate setting (see, e.g., Marinacci and Montrucchio, 2005 and Müller and Scarsini, 2012), are consistent with multidimensional PD transfers, see Bosmans et al. (2009) and Basili et al. (2017), and that what we call weak inframodular functions fit more accurately with the traditional notion of PD transfers which are required to be rank-preserving. More precisely, our main theorem is an equivalence result between, on the one hand, dominance of one distribution by another according to a finite sequence of multidimensional PD transfers and, on the other hand, dominance according to the class of additive utilitarian social welfare functions with an inframodular utility function. We thus emphasize the similarities between the inframodular order, defined in the multivariate framework by the unanimity of ranking among all the additive utilitarian social planners endowed with an inframodular utility function, and the concave order in the univariate framework.====Magdalou (2018) establishes a similar, but distinct, result in a framework where individual outcomes are multidimensional but finitely divisible in each dimension, and a distribution simply counts the number of individuals having each possible outcome. Although Magdalou (2018)’s framework encompasses a variety of abstract sets of welfare-improving transfers, it falls short of covering the crucial multidimensional PD transfers considered in our paper.====Our main result, Theorem 1, is closer to Müller and Scarsini (2012). Dealing with multivariate distributions in the context of decision under risk, Müller and Scarsini (2012) show that if a random vector ==== is unanimously preferred to another random vector ==== by decision makers with an inframodular utility function, then the distribution of ==== can be obtained from the distribution of ==== through a sequence of transfers that mimic the mean preserving spread. The inframodular transfers we define in the proof of our Theorem 1 are similar to theirs.====The paper is organized as follows. Section 2 extends the notions of Pigou–Dalton transfer and Pigou–Dalton principle from the unidimensional setting to the multivariate framework. In Section 3, we show that inframodular functions are consistent with multidimensional Pigou–Dalton transfers. In Section 4, we provide a characterization of the multidimensional Pigou–Dalton principle in terms of the inframodular order, similar to the characterization of the unidimensional Pigou–Dalton principle in terms of the concave order. Finally, in Section 5, we relate the inframodular order to row majorization and we show that it is unrelated to the concave order when the number of attributes is higher than two.",Multidimensional inequality and inframodular order,https://www.sciencedirect.com/science/article/pii/S0304406820300720,14 June 2020,2020,Research Article,199.0
"Deng Liuchun,Mitra Tapan","Division of Social Sciences, Yale-NUS College, Singapore,Department of Economics, The Johns Hopkins University, Baltimore, MD 21218, United States of America,Department of Economics, Cornell University, Ithaca, NY 14607, United States of America","Received 11 February 2020, Revised 22 May 2020, Accepted 24 May 2020, Available online 9 June 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.jmateco.2020.05.008,Cited by (2),"This paper presents ==== for 3-period cycles in the two-sector Robinson–Solow–Srinivasan (RSS) model, taking as its point of departure an independently-(and simultaneously-) discovered exact discount-factor restriction for a general class of growth models by Mitra and Nishimura–Yano (MNY) in 1996. Our investigation of this remarkable result in the specificity of the RSS model enables a broadened inquiry that goes beyond the discount factor to parameters of labor-productivity and capital-depreciation. Since the RSS model, despite its concrete simplicity, is not covered by the general MNY model, the exact discount-factor restriction presented here does not follow from the MNY theorem, and necessitates new argumentation. Furthermore, we present a novel ==== ==== region as our second result.","In an instance of independent and simultaneous discovery of 24 years ago, Mitra (1996) and Nishimura and Yano (1996) presented the following result.==== ====This remarkable result is typically characterized as an “exact discount factor restriction” in optimal dynamics and understood and abbreviated as a necessary and sufficient condition for complicated dynamics for a general but precisely-specified class of models. It is a remarkable result in that by virtue of the fact that the technological specification of the considered class of models eschews explicit functional forms and is delineated only by assumptions expressed in a qualitative postulational form, a specific number for the discount factor can nevertheless be furnished — Mitra refers to this number as a “universal” constant. Furthermore, to the extent that complicated dynamics are represented by 3-period cycles, it is testimony to the importance of the Li–Yorke theorem (Li and Yorke, 1975) of chaotic dynamics on the unit interval. The result itself followed an earlier opening by Sorger (1994) and has led to a rich trajectory of work (Sorger, 2018) and by now has gone beyond the investigation of 3-period cycles into a body of work that Mitra and Sorger (1999) and their followers refer to as “rationalizability conditions for dynamic optimization problems”.====The question is how general is the general class of models that this literature addresses. To be sure, in keeping with the one-dimensional rubric of chaotic dynamics, the models are already limited to a single capital stock, but disregarding this, their assumptions on the technological specification do not go very much beyond the requirements of closedness, convexity, free-disposal, inaction and the impossibility of an output without an input; and those on the planner’s preferences beyond continuity, monotonicity and a version of strict concavity.====  The point is that this admittedly general setting rules out the two-sector RSS model.====  Even though not a special case, the question remains whether the tools and techniques pioneered by MNY carry over in a routine way to deliver an analogous result for the RSS case? And if so, is it at all obvious that the same universal constant would then be obtained? And indeed whether the inquiry itself could be broadened to a model with explicit functional forms to take the question beyond the discount factor to other explicit parameters of the model: to labor productivity and to the depreciation rate, the other two parameters of the RSS specification, for the example considered here?====In this paper we address all of these questions. However, a basic point regarding motivation needs clarification at the outset: our investigation is not simply to domesticate an erratic special case falling outside the general scope of the theory, but rather an elaboration and underscoring of May’s 1976 dictum of simple explicitly-specified models as being important litmus tests of general results in dynamical systems and economic dynamics.==== ==== And leaving aside any question as to the nature of the argumentation, what we find is indeed surprising. First, there is a universal constant for the RSS model but one different from the one discovered earlier for the “general” case: rather than ====, it is ====. This constitutes our first theorem. Second, and perhaps even more importantly, the specificity of the RSS conception allows one to devalorize the discount-factor; in short to emphasize labor productivity and capital durability in a measure equal to the rate of impatience. However, rather than a necessary and sufficient condition (in the sense of Theorem MNY above) for each of these parameters, we identify a necessary and sufficient ==== of the two parameters. This constitutes our second theorem: its formulation involves subtle differences from our first result, and we address them in some detail in the sequel.====In their investigation of the RSS model originally initiated in 2005, Khan–Mitra provided in 2010 an “explicit solution of the optimal policy function (henceforth OPF) when the discount factor is less than the labor-output ratio”, and used that solution to follow up their earlier result on the existence of optimal topological chaos for the model in a way that only a few qualitative observations of the OPF needed to be utilized for this purpose; see Khan and Mitra, 2005a, Khan and Mitra, 2005b. Using the more detailed information on the OPF subsequently available, they could establish optimal chaotic dynamics for a non-negligible parametric ranges of the model.==== ==== In that study, they also touched on the questions being posed here but in a diffused non-conclusive way. They considered both 3-period cycles and turbulence, as formalized by Block–Coppel–Misurewicz using Smale’s work as a point of departure,====  but what is of consequence and relevance to this paper is that in the earlier study, they could not obtain an ==== discount-factor restriction for 3-period cycles. However, they did obtain an exact restriction for the labor-output ratio, one of the other technological parameters of the model. Our Theorem 1 is the result that eluded them, and our Theorem 2 is a far-reaching generalization of their second result. After presenting what we see to be definitive results, we further discuss their relation to the earlier exploratory ones in Section 3 so that the marginal contribution of this paper relative to theirs can be fully gauged. We also discuss the novelty of the argumentation provided here as compared to the earlier proofs. After a brief recapitulation of the RSS model in Section 2, this is done in Section 5 on the proofs and the ancillary results that they rely on. We also include, for the convenience of the reader, several known results on the properties of the optimal policy from Khan and Mitra, 2007, Khan and Mitra, 2012, Khan and Mitra, 2020 in Section 5. Section 4 is a two-remark concluding section spelling out open questions.",Exact parametric restrictions for 3-cycles in the RSS model: A complete and comprehensive characterization,https://www.sciencedirect.com/science/article/pii/S0304406820300641,9 June 2020,2020,Research Article,200.0
Hu Ju,"National School of Development, Peking University, 5 Yiheyuan Road, Haidian District, Beijing, 100871, China","Received 27 February 2019, Revised 19 May 2020, Accepted 25 May 2020, Available online 4 June 2020, Version of Record 10 June 2020.",https://doi.org/10.1016/j.jmateco.2020.05.007,Cited by (1),"This paper studies symmetry among countably infinitely many agents who randomly enter into a stochastic process, one for each period. Upon entry, they observe only the current period signal and try to draw inference about the underlying state governing the stochastic process. We show that there exist random entry models under which agents are ex post symmetric. That is, all agents have identical posterior belief about the underlying states, although they are not ex ante symmetric. The form of the posterior belief is uniquely pinned down by ex post symmetry and a ==== condition. Our results provide a common prior foundation for the model studied in Liu and Skrzypacz (2014).","This paper studies symmetry among countably infinitely many agents who randomly enter into a stochastic process, one for each period. Upon entry, they observe only the current period signal and try to draw inference about the underlying state governing the stochastic process. We investigate whether symmetry among these agents is consistent with common prior of entry.====It is well known that no random entry model can make infinitely many entering agents ====, i.e., that they have identical beliefs about when they enter prior to entry. Nonetheless, our first main result proves that there do exist random entry models that make infinitely many entering agents ====, that is, they have identical posterior belief about the underlying state provided they have the same observation upon entry. The most important property of such entry models is that the length of entry can be unbounded but the whole process cannot last forever. Such ex post symmetric random entry models are not unique. But our second main result shows that if an additional stationarity condition is imposed, all these models are equivalent in the sense that they all result in the same form of posterior beliefs. This implies that in applications, the actual choice of such a random entry model is immaterial, as long as only posterior beliefs are concerned.==== ====We also show how our results can be applied to the reputation game analyzed in Liu and Skrzypacz (2014). In their model, an informed long-run player interacts with a sequence of uninformed short-lived players who enter the game at random times and only observe the long-run player’s actions in the recent few periods. In order to focus on the symmetric behavior of the short-lived players, they assume that all short-lived players hold identical beliefs about when they enter, which is inconsistent with common prior of entry. Our existence result of ex post symmetric random entry models provides a remedy for this discrepancy. An easy application of our results shows that their model and analysis are indeed consistent with the common prior assumption, because the symmetric behavior of the short-lived players can be guaranteed if they are ex post symmetric.====The rest of the paper is organized as follows. Section 2 presents the random entry and learning model. We introduce the key notion of ex post symmetry. Section 3 contains the analysis and results of the paper. Section 4 provides an application and discuss how our results can be applied to the reputation game studied in  Liu and Skrzypacz (2014). All missing proofs can be found in the Appendix.",On the existence of the ex post symmetric random entry model,https://www.sciencedirect.com/science/article/pii/S030440682030063X,4 June 2020,2020,Research Article,201.0
"Jang Hyo Seok,Lee Sangjik","Department of Mathematical Sciences, Seoul National University, Seoul, Republic of Korea,Division of Economics, Hankuk University of Foreign Studies, Seoul, Republic of Korea","Received 13 March 2019, Revised 10 January 2020, Accepted 18 May 2020, Available online 2 June 2020, Version of Record 15 June 2020.",https://doi.org/10.1016/j.jmateco.2020.05.005,Cited by (6),We prove the existence of a competitive equilibrium in a production economy with infinitely many commodities and a measure space of agents whose preferences are price dependent. We employ a saturated measure space for the set of agents and apply recent results for an infinite dimensional ==== such as Lyapunov’s convexity theorem and an exact Fatou’s lemma to obtain the result.,"The purpose of this paper is to prove the existence of a competitive equilibrium in a production economy with infinitely many commodities and a measure space of agents whose preferences are price dependent. In a seminal paper, Aumann (1966) demonstrated the existence of a competitive equilibrium for an exchange economy with a finite dimensional commodity space and a continuum of agents modeled as an atomless finite measure space by utilizing Lyapunov’s convexity theorem to dispense with convex preferences. Aumann’s model in Aumann (1966) was generalized to allow incomplete preferences by Schmeidler (1969) and to include production by Hildenbrand (1970).====As Shafer (1974)==== ==== and Balasko (2003a) pointed out, price dependent preferences have been traditionally explained by consumers taking relative prices as an indication of quality. In addition, we see other applications of price dependent preferences in the literature: Shafer (1974) showed the possibility of relating price dependent preferences to non-transitive preferences and Balasko (2003b) demonstrated the equivalence of a temporary financial equilibrium model with an Arrow–Debreu economy where preferences are price dependent.====Greenberg et al. (1979) first proved the existence of a competitive equilibrium in a large economy with price dependent preferences and a finite number of commodities. In Greenberg et al. (1979), the authors considered a large production economy with non-convex preferences. They reformulated the production economy as a three-person game and applied Debreu’s social equilibrium existence theorem to obtain a Walrasian equilibrium. In their proof, they applied Lyapunov’s convexity theorem and Fatou’s Lemma in several dimensions. In order to utilize Fatou’s lemma, Greenberg et al. (1979) assumed the compactness of the consumption sets, which differs from Aumann’s original model. Liu (2017) dealt with a coalition production economy based on Greenberg et al. (1979).====For infinite dimensional commodity spaces, Khan and Yannelis (1991) considered a large exchange economy and showed the existence of a competitive equilibrium. In Khan and Yannelis (1991), the commodity space is an ordered separable Banach space whose positive cone has a non-empty interior. Until recently, Lyapunov’s convexity theorem and an exact Fatou’s lemma for an infinite dimensional separable Banach space were not available. Therefore, the authors had to impose the assumption of convex preferences. They relied on the weak compactness of feasible allocations to extract a convergent subsequence of competitive equilibria for truncated subeconomies to obtain the existence of a Walrasian equilibrium. Now that the necessary mathematical tools are at hand, it is natural to ask as to whether equilibrium existence results for a large economy with an infinite dimensional commodity space, non-convex preferences and price externalities are available. We give a positive answer in this paper.====Saturated or super-atomless measure spaces have played an important role in recent mathematical economics.  Podczeck (2008) and  Sun and Yannelis (2008) successfully proved the convexity of Bochner integrals of an infinite dimensional separable Banach space valued correspondence on a saturated measure space. Based on saturated measure spaces, Khan and Sagara (2013) proved Lyapunov’s convexity theorem for vector measures taking values in an infinite dimensional separable Banach space and  Greinecker and Podczeck (2013) also showed it. Khan and Sagara (2014) established an exact Fatou’s lemma for an infinite dimensional separable Banach space. Khan et al. (2016) proved an exact Fatou lemma for Gelfand integrals which was also established via Young measures by Greinecker and Podczeck (2017). These results have already been applied to general equilibrium theory in several papers:  Khan and Sagara, 2016, Khan and Sagara, 2017,  Khan and Suzuki (2016) and Lee (2013). In Khan and Sagara (2017), the authors emphasized the importance of saturated measures by saying that “the significance of the saturation property lies in the fact that it is necessary and sufficient for the weak/weak* compactness and the convexity of the Bochner/Gelfand integral of a multifunction as well as the Lyapunov convexity theorem in separable Banach spaces/their dual spaces”.====In this paper, we consider a large production economy whose commodity space is that of Khan and Yannelis (1991) and whose agents have non-convex and price dependent preferences, similar to  Greenberg et al. (1979). We employ a saturated measure space of agents and hence, we can utilize the convexity of a Bochner integral of a Banach space valued correspondence, Lyapunov’s convexity theorem, and the exact Fatou’s lemma for an infinite dimensional Banach space. With these new results, we are able to relax the convexity of preferences and production sets, and apply Debreu’s social equilibrium existence theorem. Moreover, we can obtain a competitive equilibrium as the limit of a sequence of competitive equilibria for truncated subeconomies. We dispense with the uniform compactness assumption on the consumption sets and production sets, which was used in Greenberg et al. (1979) and in Liu (2017).====The paper proceeds as follows: Section 2 contains notations and definitions. We present our model in Section 3, and our main and auxiliary results are in Section 4. The proof of the auxiliary result is in Section 5 followed by the proof of the main theorem in Section 6. Section 7 concludes the paper with our remarks.",Equilibria in a large production economy with an infinite dimensional commodity space and price dependent preferences,https://www.sciencedirect.com/science/article/pii/S0304406820300616,2 June 2020,2020,Research Article,202.0
"Bervoets Sebastian,Faure Mathieu","Aix Marseille Univ, CNRS, EHESS, Centrale Marseille, AMSE, Marseille, France","Received 21 June 2019, Revised 12 May 2020, Accepted 18 May 2020, Available online 27 May 2020, Version of Record 1 June 2020.",https://doi.org/10.1016/j.jmateco.2020.05.006,Cited by (1),"In ==== game played on a network, where continua of Nash equilibria often appear.","The question of convergence of dynamical systems to some Nash equilibrium has often been explored in economics. Usually, convergence is discussed in contexts where the set of Nash equilibria is finite (see, for instance, the series of papers about convergence to the Cournot solution - Theocharis, 1960, Fisher, 1961, Hahn, 1962, Seade, 1980 among many others. In other games, see i.a. Arrow and Hurwicz, 1960 or Rosen, 1965). Yet continua of Nash equilibria may appear in several economic situations. As Seade (1980) points out when discussing convergence of dynamical systems, “Things would get trickier (...) if equilibria happened not to be regular, that is not even locally unique, isolated. This, one can dismiss as a non-generic, ‘unlikely’ occurrence, although that is often a risky stand to take”. Proving convergence in that case becomes problematic. In fact, to the best of our knowledge, no paper in economics addresses this issue.====When Nash equilibria are isolated, proving convergence amounts to showing that the distance between any solution curve and the set of Nash equilibria goes to zero. This is also necessary, but no longer sufficient, when equilibria are not isolated. Actually, the solution curve could very well approach the set of Nash equilibria, without ever converging to one specific element of that set. Convergence to an equilibrium when continua of equilibria exist has been explored in the dynamical systems literature (see for instance the book by Aulbach (2006) devoted to this problem). However, these techniques generally require strong regularity assumptions which fail to hold in most economic situations. In particular, they assume that the state space is an open set, while economic variables (such as prices, time allocation, efforts, quantities.) are typically defined on non-open sets.==== ==== This makes these convergence results inapplicable.==== ====Two other methods to prove pointwise convergence of a dynamical system in presence of continua of equilibria have been developed, in Bhat and Bernstein (2010) and in Panageas and Piliouras (2016). The first is based on arclength, where the idea is to prove that every orbit has finite arc length. The authors then exploit a well-chosen Lyapunov function in order to prove their result. In the second, the authors prove convergence of the replicator dynamics in potential population games in presence of continua of equilibria. Their method consists in constructing a local Lyapunov function in the neighborhood of a given omega-limit point of the replicator dynamics. In both cases, the technique crucially relies on finding an appropriate Lyapunov function. This technique might be difficult to generalize or even to adapt to other settings, since the Lyapunov functions are specific to each problem and there is no systematic way of finding one.====In this paper we present and adapt the ==== (introduced in Bhat and Bernstein, 2003), which does not rely on Lyapunov functions nor does it require regularity assumptions. We illustrate how it works by analyzing a standard dynamical system – continuous-time best-response dynamics – in the local public good game introduced in Bramoullé and Kranton (2007). This game has received considerable attention over recent years.==== ====In this game, players are placed on a network and interact only with their neighbors. The game has linear best responses and strategic substitutes, where individuals’ payoffs depend on the sum of their neighbors’ actions. It has been extensively studied in the recent literature, both for its great simplicity and for its rich structure of Nash equilibria. Of course the structure of the set of equilibria critically depends on the structure of the network, and in fact, Bervoets and Faure (2019) show in a companion paper that a substantial fraction of networks have continua of Nash equilibria. These ingredients combine to make this game the perfect candidate for our analysis of convergence.====The dynamical system that we consider is continuous-time best-response dynamics. We choose to focus on this specific dynamical system for various reasons. First, it is widely used in economics (see for instance the papers mentioned above about convergence to the Cournot solution). Second, it is related to many other dynamical system, in the sense that proving asymptotic properties of the solutions of this dynamical systems can be helpful for studying other dynamical systems (for instance replicator dynamics, see Hofbauer et al., 2009). Also many adaptive dynamics can be studied through a careful analysis of the best-response dynamics (for instance fictitious play, see Benaïm et al., 2005 or similar algorithms, see Leslie and Collins, 2006). And third, it is simple enough to allow for a clear exposition of the non-tangency technique, when a more complex system would necessarily interfere with the understanding of the proof.====In the next section we present the local public good game and describe the structure of the set of Nash equilibria. In Section 3 we define the continuous-time best-response dynamics and state our main result about convergence. We also provide an intuitive sketch of the proof, while the formal proof is in the Appendix.",Convergence in games with continua of equilibria,https://www.sciencedirect.com/science/article/pii/S0304406820300628,27 May 2020,2020,Research Article,203.0
"Liu Zhiwei,Song Xinxi","International School of Economics and Management, Capital University of Economics and Business, Beijing, China,Department of Economics, The University of Iowa, Iowa City, USA","Received 1 November 2018, Revised 29 February 2020, Accepted 12 May 2020, Available online 20 May 2020, Version of Record 25 May 2020.",https://doi.org/10.1016/j.jmateco.2020.05.004,Cited by (2),We generalize de Castro and Yannelis (2018) by taking into account the use of randomization. We answer the following questions: Is each efficient allocation of de Castro and Yannelis (2018) still ====? Are all efficient allocations still incentive compatible under the Wald’s maxmin preferences? We provide positive answers and give applications.,"One of the fundamental problems in mechanism design and equilibrium theory with asymmetric information is the conflict between efficiency and incentive compatibility. As shown in Myerson (1979) and Holmström and Myerson (1983), an efficient allocation may not be incentive compatible in the Bayesian framework.==== ==== However, when agents have the Wald’s maxmin preferences, de Castro and Yannelis (2018) showed that the conflict between efficiency and incentive compatibility no longer exists: all efficient allocations are also incentive compatible if and only if agents have the Wald’s maxmin preferences.====We generalize de Castro and Yannelis (2018) by taking into account the use of randomization. As pointed out by Raiffa (1961), and rigorously showed by Saito (2015) and Ke and Zhang (2020), a maxmin agent may achieve a higher payoff through the use of randomization, and hence prefers to randomize her choices.==== ==== The fact that randomization can improve maxmin agents’ payoffs leads to the following questions: Can an efficient allocation of de Castro and Yannelis (2018) be Pareto improved by a ====?==== ==== Are all efficient allocations still incentive compatible under the Wald’s maxmin preferences, when we take into account that each agent may randomize her choices (i.e., use a mixed strategy)? We answer these questions in this paper.====In particular, we explicitly take into account randomization (both lottery allocations and mixed strategies) to study efficiency and incentive compatibility in an ==== with asymmetric information. An ambiguous exchange economy with asymmetric information consists of a finite set of agents, each of whom is characterized by a finite ====, an ==== and an ====. More importantly, the agents have the ==== Gilboa and Schmeidler (1989) which include the Wald’s maxmin preference as a special case. The main contributions that this paper makes are:====First, we show that when the agents’ utility functions satisfy the standard concavity assumption, efficient allocations of de Castro and Yannelis (2018) cannot be Pareto improved by any feasible lottery allocation. Moreover, introducing lotteries over the commodity space enlarges the set of efficient allocations. That is, we show in Section 3 that the efficient lottery allocations set contains the efficient allocations set of de Castro and Yannelis (2018) as a strict subset.====Second, we show that this larger set, i.e., the efficient lottery allocations set, satisfies stronger incentive compatibility notions than the one in de Castro and Yannelis (2018). It follows that all efficient allocations of de Castro and Yannelis (2018) are still incentive compatible under the Wald’s maxmin preferences, even if we take into account that an agent may randomize her reports to get higher payoffs. Therefore, we strengthen and generalize the sufficiency part of de Castro and Yannelis (2018), and obtain as a corollary their related theorem. More specifically, there are different ways to define incentive compatibility under mixed strategies, since there are different and equally natural ways for a maxmin agent to evaluate her mixed strategy. Two frequently used ways are: ====, and ====. Evaluating a mixed strategy ex ante assumes that an agent learns the realization of her mixed strategy before nature draws a probability law from a set of probability laws (i.e., ambiguity) to minimize the agent’s expected utility. It follows that no mixed strategy can eliminate the effect of ambiguity.==== ==== Evaluating a mixed strategy ex post assumes that an agent learns the realization of her mixed strategy after nature draws a probability law from a set of probability laws to minimize the agent’s expected utility. Now, a mixed strategy can fully eliminate the effect of ambiguity.==== ====
 Saito (2015) introduced a more general way: a maxmin agent evaluates her mixed strategy by taking a weighted average of “evaluating a mixed strategy ex ante” and “evaluating a mixed strategy ex post”, where the weight captures the agent’s subjective belief that her mixed strategy eliminates the effect of ambiguity. We show that when agents have the Wald’s maxmin preferences, all efficient lottery allocations are ==== (Theorem 1). It follows that when agents’ utility functions satisfy the standard concavity assumption, all efficient allocations of de Castro and Yannelis (2018) are strongly mixed incentive compatible (Corollary 3), which is the strongest notion among all the mixed incentive compatibility notions considered in this paper (Proposition 2).====Third, we demonstrate the usefulness of our results through three applications, which are not covered by de Castro and Yannelis (2018). In the first application, we recast the key example of Prescott and Townsend (1984) with Wald’s maxmin preferences. Prescott and Townsend (1984) introduced lotteries on the commodity space in an exchange economy with a continuum of agents and the Bayesian preferences. They showed that an efficient allocation ==== may not be incentive compatible; however, the use of lotteries on the commodity space can achieve both incentive compatibility and the same utility for every agent as the efficient allocation ====. We show that in a Bayesian economy with a finite number of agents, Prescott and Townsend (1984)’s method does not always work: when an efficient allocation is not incentive compatible, using lotteries on the commodity space may not be able to achieve both incentive compatibility and the same utility for every agent as the efficient allocation. However, if agents have the Wald’s maxmin preferences, every efficient allocation is strongly mixed incentive compatible. In the second and third applications, we show that each ==== of de Castro et al., 2017a, de Castro et al., 2017b and each ==== of Angelopoulos and Koutsougeras (2015) are strongly mixed incentive compatible under the Wald’s maxmin preferences.====The paper is organized as follows. Section 2 defines the ambiguous exchange economy with asymmetric information. Sections 3 Randomization under ambiguity: efficiency, 4 Randomization under ambiguity: incentive compatibility discuss efficiency and incentive compatibility respectively, while taking into account the use of randomization. Section 5 shows that each efficient lottery allocation is strongly mixed incentive compatible under the Wald’s maxmin preferences. Section 6 discusses the three applications. Finally, we conclude in Section 7. The proofs of our main results are delegated to the Appendix A Proof of, Appendix B Proof of, Appendix C Proof of.",Randomization under ambiguity: Efficiency and incentive compatibility,https://www.sciencedirect.com/science/article/pii/S0304406820300604,20 May 2020,2020,Research Article,204.0
Ekici Özgün,"Özyeğin University, Istanbul, Turkey","Received 3 December 2019, Revised 22 March 2020, Accepted 4 May 2020, Available online 19 May 2020, Version of Record 19 May 2020.",https://doi.org/10.1016/j.jmateco.2020.05.003,Cited by (4),"We study the house allocation problem with existing tenants: ==== houses (stand for “indivisible objects”) are to be allocated to ==== agents; each agent needs exactly one house and has strict preferences; ==== houses are initially unowned; ==== agents initially do not own houses; the remaining ==== agents (the so-called “existing tenants”) initially own the remaining ==== houses (each owns one). In this setting, we consider various randomized allocation rules under which voluntary participation of existing tenants is assured and the randomization procedure either treats agents equally or discriminates against some (or all) of the existing tenants. We obtain two equivalence results, which generalize the equivalence results in Abdulkadiroğlu and Sönmez (1999) and Sönmez and Ünver (2005).","We consider the classical problem of allocating ==== indivisible objects to ==== agents. We assume that each agent needs exactly one object, agents’ preferences over objects are strict, and monetary transfers are not allowed. As real-life applications, consider the problems of allocating posts at hospitals to medical interns, dormitory rooms to college students, or kidneys for transplant to patients with kidney disease. In the literature it is conventional to refer to objects as “houses”, and in our paper we follow this convention.====This problem has three variants in the literature: In the so-called ==== it is presumed that houses are initially unowned. In the so-called ==== it is presumed that each agent initially owns one of the houses.==== ==== The third variant is the hybrid case, introduced by Abdulkadiroğlu and Sönmez (1999) and formulated in their paper as a ====: ==== houses are initially vacant; ==== agents are newcomers; and each of the remaining ==== houses is occupied by one of the remaining ==== agents (referred to as “existing tenants”). In our paper, we study this hybrid case and follow its formulation in their paper. Throughout, we assume that if they wish so, existing tenants can keep their occupied houses or even trade them with one another. Therefore, although doing so is an oxymoron, we will speak of existing tenants as the “owners” of their occupied houses.====In real-life applications in which there are no existing tenants, the allocation of houses is often carried out by a mechanism (allocation rule) called ====: First, a priority order of agents is chosen uniformly at random. Then the associated ==== is executed: The first agent in the order receives her top choice, the next agent receives her top choice among the remaining houses, and so on. The random priority has some very appealing properties. Most notably, it is simple, strategy-proof (i.e., it is immune to misrepresentation of preferences) and efficient (i.e., it always induces Pareto-efficient allocations). Alas, the presence of existing tenants poses two challenges not addressed under random priority. First, the allocation it induces is not always “group-rational”. This means that under this mechanism a subset of existing tenants is not assured that their coalitional allocation will always be (in the Pareto sense) at least as good as any coalitional allocation that they can attain by trading their occupied houses. Without this assurance, it is conceivable that these existing tenants opt out, which may lead to a loss in potential gains from trade. Therefore, to assure the voluntary participation of existing tenants, we take it to be the case that the allocation rule used always induces a group-rational allocation. Another feature of random priority is that under its random component (the random choice of a priority order) agents are put on an equal footing: The agents are ranked high in the priority order with equal probabilities. While this may be a desirable feature in real-life applications in which there are no existing tenants, when they are present it is conceivable that the mechanism designer deems it justified to discriminate against all or a subset of existing tenants. For instance, in kidney exchange practices, there are some patients (“existing tenants”) who already have compatible donors. The remaining patients however either have no donors (“newcomers”) or have incompatible donors (also “existing tenants”). For this latter group of patients, finding a kidney transplant from a compatible donor is a life-and-death matter. In the eyes of the medical authority, therefore, it may be justified to treat more favorably this latter group of patients, even if it means discrimination against patients with compatible donors.==== ====In this paper, to tackle the above-mentioned two challenges, we propose four random mechanisms. They are defined by means of two algorithms: The Y-I algorithm and the augmented TTC algorithm. The Y-I algorithm allocates houses to agents by means of a priority order of agents: The first agent in the order receives her top choice, the next agent receives her top choice among the remaining houses, and so on. However, if an agent ==== requests the occupied house of an existing tenant ==== and ==== has not been assigned a house yet, the remainder of the order is updated by moving ==== to the top (right above ====) and then we proceed.==== ==== As it turns out, thanks to this update protocol the Y-I algorithm always induces a group-rational allocation (see Ekici (2013)). The augmented TTC algorithm allocates houses to agents by means of an “augmenting function” ====: For each agent, ==== either assigns her the ownership of a vacant house or appoints her as the “inheritor” of an existing tenant. When our original problem is augmented by ====, this defines a private-ownership economy in which vacant houses are now also owned. Then the houses are allocated to agents by identifying (top trading) “cycles”: A cycle is a series ==== of agents such that the favorite house of ==== is owned by ====; the favorite house of ==== is owned by ====; and so on. In each cycle, the corresponding trades are performed, and then all the agents belonging to cycles are removed together with their assignments. If an existing tenant owns two houses, when she is removed one of her houses still remains. This house is then given to the inheritor of the existing tenant (the inheritor appointed by ====). If the inheritor has been removed, too, the house that remains is given to the inheritor of the inheritor, and so on. Then in the reduced private-ownership economy we proceed similarly: by identifying new cycles and performing the corresponding trades and so on.==== ==== As it turns out, the augmented TTC algorithm also always induces a group-rational allocation.====The first two random mechanisms that we propose are for real-life scenarios in which discrimination against existing tenants is deemed unjustified (perhaps, when allocating dormitory rooms to college students):====Note that the random components of these two mechanisms put agents on an equal footing: ==== and ==== are chosen uniformly at random.====The next two random mechanisms that we propose are for real-life scenarios in which discrimination against a subset ==== of existing tenants is justified:====Notice how the random components of these two mechanisms discriminate against the existing tenants in ==== (or, equally, notice how they favor the remaining agents): When the priority order ==== is chosen, they are placed at the bottom of the order and hence put at a disadvantage. When the augmenting function ==== is chosen, they never get a chance to receive vacant houses and hence are put at a disadvantage in the subsequent trade protocol. As an illustration of this discrimination, imagine the case where every agent prefers any vacant house to any occupied house. Then, under these two mechanisms, no existing tenant in ==== ever receives one of these coveted vacant houses.====Above, the four random mechanisms that we propose are defined by means of two seemingly different algorithms: (1) and (3) are defined by means of the Y-I algorithm, and (2) and (4) are defined by means of the augmented TTC algorithm. Even so, the main theoretical results of our paper show that they are closely related: In Theorem 1, we show that the random Y-I and the random augmented TTC mechanisms are equivalent. (That is, they induce any given allocation with exactly the same probability.) In Theorem 2, we show that the ====-discriminating random Y-I and the ====-discriminating random augmented TTC mechanisms are also equivalent. Indeed, it is Theorem 2 which is the main theoretical contribution of our paper. It helps unify and generalize the following equivalence results:====A mechanism is said to be deterministic if for each preference profile of agents its allocation choice is certain. The Y-I algorithm defines a class of deterministic mechanisms, each specified by the choice of the priority order ====. Similarly, the augmented TTC algorithm defines a class of deterministic mechanisms, each specified by the choice of the augmenting function ====. Since our random mechanisms randomize over these two classes of mechanisms, they inherit their nice theoretical properties: They are strategy-proof and efficient, and they always induce group-rational allocations. In showing Theorem 1, we show that there is indeed a one-to-one correspondence between these two classes of mechanisms: For each priority order ====, there exists a distinct augmenting function ==== such that the Y-I mechanism specified by ==== is the same as the augmented TTC mechanism specified by ==== (due to Lemma 1, Lemma 4). In other words, we show that these two classes of mechanisms coincide. These two classes of mechanisms are subsets of the more general class of “hierarchical exchange rules”, introduced by Pápai (2000) and which she characterized by a nice set of theoretical properties. We believe that the correspondence between the classes of Y-I and augmented TTC mechanisms assigns them a focal place in the larger class of Pápai’s hierarchical exchange rules. Also, to our best knowledge, the ====-discriminating random mechanisms in our paper are the first random mechanisms in the literature proposed for problems where it is deemed socially desirable to discriminate against only a subset of existing tenants (in order to favor the remaining agents). The choice between them turns out to be inconsequential since we show that they are equivalent.====As in our paper, AS and SU show their results by showing bijective mappings between mechanisms based on priority orders and those based on the TTC algorithm. A comparison of these bijections helps us better understand the nature of mechanisms based on priority orders: AS consider priority rules; SU consider the subclass of Y-I mechanisms in which existing tenants are placed at the bottom of the priority order; and in showing Theorem 1, we consider the Y-I mechanisms on the full domain. The bijective results in their papers and in ours show that when existing tenants are present, the randomization over the full domain of Y-I mechanisms may be the best alternative: In the case of a priority rule, the TTC-based mechanism that corresponds to it does not always treat an existing tenant as the owner of her occupied house. But then, if her occupied house is an appealing one, she may not take part in the random allocation process. In the case of the subclass of Y-I mechanisms considered in SU, in the corresponding TTC-based mechanisms, existing tenants only have their occupied houses to trade with. But then, if her occupied house is not an appealing one, an existing tenant may find it in her best interest to drop her occupied house and take part in the random allocation process as a newcomer. Even worse, in kidney exchange practices, for instance, a patient whose donor’s kidney is an unappealing one may not inform authorities about her donor, leading to a significant loss in welfare. Both these concerns are addressed if the allocation rule randomizes over the full domain of Y-I mechanisms. In the corresponding TTC-based mechanisms, an existing tenant is always treated as the owner of her occupied house, and she may also end up having some other house to trade with.====Much of the terminology in this paper is taken from AS. The proof strategy in showing Theorem 1 also follows the outline of their proof. In their paper, on the basis of the execution of the TTC algorithm, they construct a priority order. Their key observation is that a cycle formed by agents at some step is formed by some “chains” that arise at the preceding step. Their construction relies upon the order in which these chains appear. Our construction is similar, based on “chains” and the order in which they appear, but with one key difference. Under the augmented TTC algorithm, an existing tenant plays a role as part of a chain on two occasions: Once, when in the subsequent cycle, she trades a house; and once, when in the subsequent cycle, one of her inheritors trades a house that she inherits from her (or through her). To demarcate these two situations, we introduce the “clone problem”, in which for each existing tenant, a “clone” is added to the problem specification. The placement of existing tenants in the priority order then relies on when the existing tenants become part of chains (and not when their clones become part of chains). The clone problem turns out to be extremely useful in showing our results and it may be of independent interest.====The rest of the paper is organized as follows: Section 2 introduces the problem and the Y-I and the augmented TTC algorithms. Section 3 introduces the clone problem. Section 4 introduces the Y-I and the augmented TTC algorithms in the context of the clone problem. Section 5 introduces and explores the “chain order”, a construct that helps us link these two algorithms. Section 6 presents our results. The proofs of two lemmas are given in Appendix.",Random mechanisms for house allocation with existing tenants,https://www.sciencedirect.com/science/article/pii/S0304406820300598,19 May 2020,2020,Research Article,205.0
"Bach Christian W.,Perea Andrés","Department of Economics, University of Liverpool Management School, Chatham Street, Liverpool, L69 7ZH, United Kingdom,Epicenter, School of Business and Economics, Maastricht University, 6200 MD Maastricht, The Netherlands,Department of Quantitative Economics, School of Business and Economics, Maastricht University, 6200 MD Maastricht, The Netherlands","Received 12 November 2019, Revised 11 April 2020, Accepted 4 May 2020, Available online 18 May 2020, Version of Record 1 June 2020.",https://doi.org/10.1016/j.jmateco.2020.05.001,Cited by (3)," constitutes one of the basic solution concepts for static games with complete information. Actually two variants of correlated equilibrium are in circulation and have been used interchangeably in the literature. Besides the original notion due to Aumann (1974), there exists a simplified definition typically called canonical correlated equilibrium or correlated equilibrium distribution. It is known that the original and the canonical version of correlated equilibrium are equivalent from an ex-ante perspective. However, we show that they are actually distinct – both doxastically as well as behaviourally – from an interim perspective. An elucidation of this difference emerges in the reasoning realm: while Aumann’s correlated equilibrium can be epistemically characterized by common belief in rationality and a common prior, canonical correlated equilibrium additionally requires the condition of one-theory-per-choice. Consequently, the application of correlated equilibrium requires a careful choice of the appropriate variant.","Correlated equilibrium has been introduced by Aumann (1974) and represents one of the main solution concepts for static games with complete information. Two versions of this solution concept circulate in the literature and often no distinction is drawn between them. Indeed, both solution concepts are equivalent in terms of the (prior) probabilities assigned to choice profiles. Thus, both versions are rather perceived as substitutable. However, it turns out that the variation in defining correlated equilibrium can be significant from the so-called interim perspective once the probabilities are conditionalized on information. Both a player’s belief about the opponents’ choices as well as a player’s optimal choice in line with the two notions then becomes different. This discrepancy can be elucidated in terms of reasoning by unveiling the epistemic assumptions underlying the two solution concepts. Consequently, care should be exerted when applying correlated equilibrium. The use of the particular version of correlated equilibrium should be driven by deliberate reflection about which of the – distinct – underlying epistemic assumptions are more appropriate for the specific purpose at hand.====Formally, Aumann’s (1974) original solution concept of correlated equilibrium is constructed within an epistemic framework based on possible worlds, information partitions, and a common prior probability measure. Often, in scientific articles and game theory textbooks, a more direct definition of correlated equilibrium is used that simply models correlated equilibrium as a probability measure on choice combinations. The latter solution concept is sometimes called canonical correlated equilibrium (e.g. Forges, 1990) or correlated equilibrium distribution (e.g. Aumann, 1987) in the literature. The question arises whether these two definitions are actually interchangeable or whether they constitute two different solution concepts.====The analysis of games typically distinguishes three perspectives or stages: ex-ante, interim, and ex-post. From the ex-ante perspective players have not received any private information; epistemically players entertain prior beliefs in this stage of the game. Then, private information is unveiled to the players who update (or revise) their beliefs accordingly; the formation of these posterior beliefs as well as the subsequent choices take place in the interim stage of the game. From the ex-post perspective the outcome of the game as combination of the players’ choices ensues.====Besides, solution concepts can generally not be compared directly due to possibly being embedded in different structures. For instance, the formulation of correlated equilibrium uses an epistemic framework, while canonical correlated equilibrium lacks such structure. However, since solution concepts all induce for every player decision-relevant i.e. interim beliefs about his opponents’ choices, these beliefs as well as optimal choice in line with them can serve as a universal benchmark. In other words, the interim beliefs and subsequent optimal choices for every player can be viewed as the final output of a solution concept. It is thus always possible to compare any given solution concepts in the interim stage of a game.====The two versions of correlated equilibrium can be compared from an ex-ante as well as an interim perspective.==== ==== It is well-known that from the ex-ante perspective correlated equilibrium and canonical correlated equilibrium coincide. More precisely, the induced probability measure on choice combinations of a correlated equilibrium using the common prior only (and not the players’ information) is equal to some canonical correlated equilibrium, and vice versa. This fact together with the consequence that any correlated equilibrium can be represented by some correlated equilibrium distribution is also known as the ====. However, the relevant perspective for reasoning and decision-making in games seems to be interim. The posterior belief of a player about his opponents’ choices – conditionalized on his information in the case of correlated equilibrium and conditionalized on one of his choices in the case of canonical correlated equilibrium – constitute the outcome of the player’s reasoning and thus his decision-relevant doxastic mental state. In other words, the players’ posterior beliefs represent a solution concept ====. Optimal choice in line with a player’s reasoning then characterizes the respective solution concept ====. An appropriate comparison of solution concepts in terms of their game-theoretic semantics thus needs to address these two – doxastic and behavioural – dimensions.====Here, we show that correlated equilibrium and canonical correlated equilibrium are neither doxastically nor behaviourally equivalent in the interim stage of a game. Thus, the revelation principle even though valid from the ex-ante perspective does no longer hold from the interim perspective. First of all, inspired by the game in Aumann and Dreze’s (2008) Figure 2A, we illustrate that correlated equilibrium and canonical correlated equilibrium may induce different sets of first-order beliefs i.e. beliefs about the respective opponents’ choice combinations, from an interim perspective. Secondly, we construct an example where correlated equilibrium and canonical correlated equilibrium also differ behaviourally, i.e. in terms of optimal choice. In this sense, correlated equilibrium and canonical correlated equilibrium constitute two distinct solution concepts for static games.====In order to conceptually understand the difference of correlated equilibrium and canonical correlated equilibrium, a reasoning angle is taken using the standard type-based approach. First of all, transformations from Aumann’s epistemic framework to type-based models and back are defined. We show that these transformations turn correlated equilibria into epistemic models that satisfy a common prior assumption as well as contain types expressing common belief in rationality, and vice versa. An epistemic characterization of correlated equilibrium in terms of common belief in rationality and a common prior from an interim perspective consequently ensues.====We then introduce the epistemic condition of one-theory-per-choice. Intuitively, a reasoner satisfying this condition never uses in his entire belief hierarchy distinct first-order beliefs to explain the same choice for any player. We give an epistemic characterization of canonical correlated equilibrium in terms of common belief in rationality, a common prior, and the one-theory-per-choice condition from an interim perspective. In terms of reasoning, canonical correlated equilibrium thus constitutes a more demanding solution concept than correlated equilibrium. Conceptually, the one-theory-per-choice condition contains a correct beliefs assumption. Accordingly, the reasoner does not only always explain a given choice by the same first-order belief throughout his entire belief hierarchy, but he also believes his opponents to believe he does so, and he believes his opponents to believe their opponents to believe he does so, etc. Furthermore, the reasoner does not only believe any opponent to explain a given choice by the same first-order belief throughout his entire belief hierarchy, but he also believes his opponents to believe he does so, and he believes his opponents to believe their opponents to believe he does so, etc. In terms of correct beliefs properties, canonical correlated equilibrium thus is more demanding than Aumann’s original solution concept of correlated equilibrium.====In applications caution is required which solution concept – correlated equilibrium or canonical correlated equilibrium – is used, since they are genuinely different in terms of reasoning and the diacritic one-theory-per-choice condition does constitute a substantial assumption. In cases where correct beliefs conditions seem less plausible, correlated equilibrium rather than canonical correlated equilibrium appears to be adequate, while in cases where correct beliefs conditions seem more appropriate, the latter rather than the former solution concept appears to be suitable. Importantly, note that the interpretation of our characterizations of correlated equilibrium and canonical correlated equilibrium does not imply that one of the two solution concepts qualifies as superior, but that they can be concluded to be non-trivially distinct and the one-theory-per-choice condition sheds conceptual light on this difference in terms of reasoning.====We proceed as follows. In Section 2, the two definitions of correlated equilibrium within the framework of static games are recalled. It is then shown in Section 3 that the two solution concepts are neither doxastically nor behaviourally equivalent in the interim stage. In Section 4, a reasoning framework by means of type-based epistemic models is presented which is later used to analyse correlated equilibrium and canonical correlated equilibrium. Both solution concepts are characterized epistemically from the perspective of the interim stage in Section 5 and their difference in terms of reasoning thereby illuminated. Finally, some conceptual issues are addressed in Section 6. In particular, a philosophical discussion about the relation of the two versions of correlated equilibrium to Nash equilibrium based on the epistemic characterization results from Section 5 is offered.",Two definitions of correlated equilibrium,https://www.sciencedirect.com/science/article/pii/S0304406820300574,18 May 2020,2020,Research Article,206.0
"Aslan Fatma,Lainé Jean","Quantitative Social and Management Sciences Research Centre, Faculty of Economic and Social Sciences, Budapest University of Technology and Economics, Hungary,Cnam-Lirsa, Paris, France,Murat Sertel Center for Advanced Economic Studies, Istanbul Bilgi University, Turkey","Received 9 October 2018, Revised 3 February 2020, Accepted 4 May 2020, Available online 12 May 2020, Version of Record 26 May 2020.",https://doi.org/10.1016/j.jmateco.2020.05.002,Cited by (3),"We investigate the existence and properties of competitive equilibrium in Shapley–Scarf markets involving an exogenous partition of individuals into couples. The presence of couples generates preference interdependencies which cause existence problems. For both cases of transferable and non-transferable income among partners, we establish properties for preferences that are sufficient for the existence of an equilibrium. Moreover, we show that these properties define a maximal preference domain.","A Shapley–Scarf market (Shapley and Scarf, 1974) refers to a pure exchange economy without money, which involves finitely many individuals, each owning an indivisible good and having use of only one good. Shapley and Scarf (1974) prove the non-emptiness of the core in such markets and introduce the Top-Trading-Cycles (TTC) algorithm (attributed to David Gale), which always terminates at an allocation in the core. Roth and Postlewaite (1977) show that if individual preferences over goods are described by linear orders, the TTC outcome is the unique element of the strict core and the unique competitive allocation. The TTC algorithm also satisfies various desirable properties. Its outcome is Pareto efficient.==== ==== Moreover, it defines a strategy-proof allocation mechanism (Roth, 1982), and this mechanism is the unique one satisfying strategy-proofness, Pareto efficiency, and individual rationality (Ma, 1994).==== ====Beyond its classical interpretation as the housing market, a Shapley–Scarf market provides a relevant framework for the analysis of professional mobility, through which employees move from their current job (good) to another one made available by their employer or by the market. Many real-life mobility campaigns are organized as centralized procedures, in which individuals report to a central authority their preference list of available jobs, and the authority reassigns jobs so that each individual gets exactly one job.====A noteworthy fact in the design of a job mobility procedure is that some individuals live in couples. This leads to a significant departure from the classical Shapley–Scarf market, where individuals’ well-being only depends on the job they get. In contrast, individuals living in couple care not only about their own job but also about the one assigned to their partner. The consequences of externalities in preferences have been paid attention mainly for two-sided allocation problems.==== ==== Notable exceptions are Hong and Park (2018), and Massand and Simon (2019), where the existence of core-stable solutions is investigated for specific types of externality in Shapley–Scarf markets.==== ==== The disturbing role of couples for existence of certain solutions is not new. Indeed, Doğan et al. (2011) show that a Shapley–Scarf market with couples where partners have joint preferences may have an empty core.====This paper aims to investigate how the existence and properties of competitive equilibrium accommodate situations where individuals care not only about the good they receive but also about the good received by their partner. As equilibrium allocations closely relate to the TTC algorithm in markets without couples, a natural complementary issue is studying how the original properties of the TTC algorithm are impacted by this specific externality in preferences.====A competitive equilibrium in a Shapley–Scarf market without couples is defined as a situation where each individual is assigned to exactly one good, which she prefers the most among those with a price not exceeding the price of her currently owned good (interpreted as her income). Many real-life job mobility procedures are based on the computation of a score. Each applicant is assigned a number of points, or a score, which mimics a price. One example of such procedures is the one designed for French teachers in primary and secondary schools. In this procedure, the scores of applicants result from their professional history, as well as their private data (marital status, number of children, level of seniority as a teacher, duration of the last position, personal difficulties such as the care of a disabled child). A significant input in the computation method is also the nature of their current position. For instance, the wish to avoid teaching staff shortage in some district may motivate raising the score of teachers in that district. Applicants are ranked according to their respective scores, and this ranking determines the priority given to each applicant in the allocation procedure. Applicants with a high priority rank usually choose a popular position in terms of location or job quality. Hence, popular positions being accessible only to high score people, they are given a high market value (defined as the score of their tenants). A consequence is that a score of each applicant can be interpreted as a proxy for her income or the market value (price) of her currently held position. This motivates the relevance of competitive equilibrium as a solution to job mobility design.====We model a centralized procedure where each individual submits a preference list over bundles of two goods, the one she receives and the one her partner receives. This very general definition of preferences admits as special cases the one where couples submit joint preference lists, and also the one where each individual submits a preference list for herself, the central authority aggregating partners’ lists through some pre-specified device. Beyond the will of generality, this choice is motivated by the fact that most real-life mobility procedures for couples discard the possibility for couples of making joint claims. Exceptions may prevail for couples of civil servants whose careers are administrated by the same entity, and whose target is moving together to the same district. For instance, since 2018, a procedure of joint mobility allows French primary school teachers to condition their mobility to a given new area to the fact that their partners also get a position in that area. According to this procedure, both partners must report the same rankings of targeted positions. In contrast, the Scottish Foundation Allocation Scheme (SFAS) for medical school graduates stipulates that applicants who want to be assigned geographically close positions may require to be treated as a couple. However, such applicants are asked to submit separate preference lists, which are aggregated by considering their geographic concern to generate a joint preference list (for more details see Biró et al. (2011)).====Another critical issue in our model is whether income (or score level) is transferable among partners. In most real-life situations, the budget constraint prevails individual-wise. However, the aforementioned procedure of French joint mobility of primary schools teacher allocates to each partner the average score in the couple, which is a typical example of transferable income.====There is no natural definition of a competitive equilibrium for such an economy. We focus on a specific concept of market equilibrium, in which each couple gets a budget-constrained Pareto efficient bundle of goods. In the case where income is (resp. not) transferable among partners, we call weak (resp. strong) this equilibrium. Obviously, if partners submit the same preference list, Pareto efficiency resumes to maximizing the couple well-being under the relevant budget constraint.====We show that strong and weak equilibria may fail to exist with unrestricted preferences. Moreover, we identify two properties upon preferences which generate domains respectively maximal for the existence of strong and weak equilibria. Here, maximality means that existence is ensured at all profiles selected from the domain, but may fail when enlarging the domain with a preference that shows a minimal departure from the property. More precisely,====- The domain of ==== is maximal for the existence of strong equilibria. Responsiveness holds if each individual has two linear orders over goods, one for herself and one for her partner, and gets better off with a Pareto improving change with respect to these linear orders. In the case where the order over the partner’s good coincides with the partner’s order over her own goods, we get couple responsiveness. Interestingly enough, we show that with couple responsive preferences, the TTC algorithm always ends up at a strong equilibrium allocation. However, the TTC algorithm no longer defines a strategy-proof mechanism.====- The domain of ==== is maximal for the existence of weak equilibria. An individual has a weakly lexicographic preference if it satisfies the following property. For each subset of goods, there exists an element of this subset (called the priority good), and for each possible bundle of goods assigned to the couple, one of the two partners is given priority. Then, any bundle of goods assigning the priority good to the partner having priority for this bundle is preferred to any bundle not doing so. Moreover, since weak equilibrium allocations belong to the core, this existence result generalizes the main result of Doğan et al. (2011).====For each type of equilibrium, we provide a constructive proof of existence based on a modification of the TTC algorithm. As for the TTC algorithm for non-transferable income, this modified version is shown to be manipulable.====Both preference domains describe rather severe restrictions. In particular, they preclude complementarities between goods, which usually prevail when the distance between partners’ goods matters. Moreover, since responsiveness and the weakly lexicographic properties are logically independent, one cannot argue that the existence of equilibrium is made more or less problematic by income transferability. However, we show that under income transferability equilibrium allocations are Pareto efficient and are core-stable when coalitions are not entitled to break couples. In contrast, an equilibrium allocation without transferable income can be Pareto dominated and core-unstable.====The paper is organized as follows. Section 2 is devoted to preliminaries. We formalize Shapley–Scarf markets with couples in Section 2.1. Weak and strong equilibria are defined in Section 2.2. Alternative preference domains are introduced in Section 2.3. All results are stated in Section 3. Properties of equilibrium allocations (set-comparison, individual rationality, core stability, and Pareto efficiency) are studied in Section 3.1. Existence results are presented in Section 3.2. In Section 3.3, we establish the maximality of the responsive and weakly lexicographic preference domains. We conclude the paper with further comments, especially on alternative equilibrium concepts that may be considered and on the relationship between our model and models with multiple types of indivisible goods. Finally, all proofs are postponed to an Appendix.",Competitive equilibria in Shapley–Scarf markets with couples,https://www.sciencedirect.com/science/article/pii/S0304406820300586,12 May 2020,2020,Research Article,207.0
Mavi Can Askan,"Aix Marseille Univ, CNRS, EHESS, Centrale Marseille, AMSE, Marseille, France","Received 4 October 2019, Revised 2 April 2020, Accepted 29 April 2020, Available online 11 May 2020, Version of Record 16 May 2020.",https://doi.org/10.1016/j.jmateco.2020.04.005,Cited by (1),"This paper aims to present a new explanation for environmental traps through the presence of endogenous hazard rate. We show that adaptation and mitigation policies affect the occurrence of environmental traps differently. The former could cause environmental traps, whereas the latter could help society avoid such traps by decreasing the ==== of a harmful event occurring. As a result, we present a new trade-off between adaptation and mitigation policies different than the usual dynamic trade-off that is highlighted in many studies and is crucial to developing countries. Contrary to the literature, when an economy is in a trap, an economy with a high ==== equilibrium tends to be more conservative in terms of resource exploitation than an economy with a low ==== equilibrium, which implies a heterogeneous reaction against the endogenous hazard rate.","A social planner should consider methods for avoiding damage as a result of hazardous events. A direct response requires action to reduce the probability of a harmful event taking place. In many cases, mitigation activities reduce the risk of a harmful event by improving environmental quality.==== ==== Still, mitigation activities cannot eliminate risk completely. When risk is unavoidable, a possible action could be to alleviate the negative consequences of damage caused by a harmful event. The measures taken to reduce losses due to the harmful event can be categorized as adaptation activities. The management of adaptation and mitigation activities raises an interesting dynamic trade-off that can be described as an “adaptation and mitigation dilemma” in the field of environmental economics literature (Zemel, 2015, Tsur and Zemel, 2015, Crépin et al., 2012).====To elaborate on these concepts, we consider concrete examples. Improvements in energy efficiency through activities such as carbon capture and storage and reforestation address the root causes by decreasing greenhouse gas emissions and reducing the risk of a harmful event occurring. Therefore, such activities can be referred to as mitigation activities.==== ==== Differently, installing flood defenses and developing irrigation systems aim to reduce the damage inflicted by a harmful event rather than stop the event from occurring. Hence, such activities can be classified as adaptation activities. In this context, adaptation plays a ==== role, which means that it has no noticeable effect prior to the harmful event’s occurrence (Smit et al., 2000, Shalizi and Lecocq, 2009). In this example, the problem is to decide on an optimal combination of risk-reducing and damage-reducing measures within a given budget.====In this paper, we study the optimal management of natural resources and its link with adaptation and mitigation policies in a simple growth model under endogenous hazard rate depending on the environmental quality level.==== ==== For example, the rates of harmful events such as droughts, crop failures, and floods==== ==== are linked to the exploitation of natural capital.====The contribution of this paper is threefold. The first contribution of the paper is to analyze the implications of the endogenous hazard rate on the occurrence of the multiplicity of equilibria (i.e., the environmental trap). We show that one of the reasons for environmental traps is the endogenous hazard rate.====When an economy faces an endogenous hazard rate, a second trade-off arises between consumption and the endogenous hazard rate other than the usual intertemporal trade-off between present and future consumption. An economy with serious environmental quality problems is expected to be impatient due to the high endogenous hazard rate. Therefore, agents tend to increase their consumption at earlier dates, which again stresses the environmental quality over time. This trade-off between consumption and harmful events results in a vicious cycle of a “low level of environmental quality and consumption” in the long run and leads to an environmental trap. Since the multiplicity of equilibria can trap an economy to a lower welfare level, it is legitimate to try to avoid it. In that sense, differently from the existing literature, our paper aims to present qualitative implications of the endogenous hazard regarding the long term dynamics of an economy.====The possibility of multiple stationary equilibria in growth models with endogenous hazard==== ==== has been mentioned by Tsur and Zemel (2016) but not examined in greater depth. In this paper, not only do we offer an economic explanation of environmental traps, but we also prove the existence of the multiplicity of equilibria by giving explicit mathematical conditions (see Appendix A*).====Our study relates also to the substantial literature on resource exploitation under uncertainty. The consideration of uncertain events for optimal management began with the work of Cropper (1976), who finds that the depletion of resources is either faster or slower than expected if the available resource stock is uncertain. Clarke and Reed (1994) find that the endogenous hazard rate either increases or decreases the pollution stock when there is a single occurrence event that indefinitely reduces utility to a constant level.====Our paper differs substantially from these studies, since we treat an economy facing recurrent harmful events. Having recurrent events in the model allows us to offer a more realistic setup.====The existing literature attempts to determine the reaction of an economy to uncertain events. A common argument is that uncertainty pushes the economy to become more precautious by conserving natural resources (Polasky et al., 2011, de Zeeuw and Zemel, 2012, Ren and Polasky, 2014).====The second contribution of this paper is to show that there can be a heterogeneous reaction to uncertain events. We show that an economy with a high environmental quality level is more precautious than one with a low environmental quality level. In other words, an economy with a low environmental quality equilibrium adopts an “aggressive” exploitation policy relative to an economy with a high environmental quality equilibrium.====Apart from the literature on resource exploitation under uncertainty, recent contributions have been made regarding the implications of harmful events on the long-term behavior of an economy. A paper by van der Ploeg (2014) focuses on how a first-best optimal carbon tax should be adjusted over time when an economy faces harmful event probability. van der Ploeg and de Zeeuw (2017) extend the framework offered by van der Ploeg (2014) and show that a positive saving response can help the economy to dampen the discrete change in consumption at the time of the harmful event. A recent study by Akao and Sakamoto (2018) offers a general framework that can be used to justify the empirical evidence that shows the positive correlation between disasters and long-term economic performance.====Our study differs also from this branch of the literature, since these studies focus on the social cost of carbon stock but say nothing about the necessary policy response in terms of adaptation and mitigation policies in order to deal with harmful events.====The third (and most significant) contribution of this paper is that it presents a new trade-off between adaptation and mitigation different from the dynamic trade-off highlighted by numerous studies (Bréchet et al., 2012, Le Kama and Pommeret, 2016, Millner and Dietz, 2011). Zemel (2015) and Tsur and Zemel (2015) investigated the time profile of the optimal mix of adaptation and mitigation in a simple growth model with uncertainty. However, these studies did not consider the multiplicity of equilibria and its implications regarding adaptation and mitigation policies. Our main result is to show that adaptation increases and mitigation decreases the possibility of environmental traps.====What is the mechanism behind the new trade-off between adaptation and mitigation? Adaptation capital is shown to decrease the optimal steady-state level of environmental quality, as agents worry less about the consequences of a harmful event. Then, because the endogenous hazard rate increases, the trade-off between present consumption and the endogenous hazard rate becomes more important, which is likely to raise multiple equilibria. Contrary to this mechanism, mitigation activity improves environmental quality, and the trade-off between present consumption and harmful event probability becomes weaker.====Another contribution is our analysis of how the dynamic trade-off between adaptation and mitigation affects the occurrence of environmental traps. Recent papers by Tsur and Zemel (2015) and Zemel (2015) focus on the dynamic trade-off between adaptation and mitigation but overlook its qualitative implications regarding the multiplicity of equilibria. We show that the unit cost of adaptation changes the optimal mix of adaptation and mitigation policies over time but also affects the occurrence of the multiplicity of equilibria.====The remainder of this paper is organized as follows: Section 2 presents the benchmark model. Section 3 describes the model with adaptation and mitigation policies and explains in depth the effects of adaptation and mitigation on the occurrence of environmental traps. Section 4 provides numerical illustrations. Section 5 concludes the paper.",Can harmful events be another source of environmental traps?,https://www.sciencedirect.com/science/article/pii/S0304406820300483,11 May 2020,2020,Research Article,208.0
"Budish Eric,Reny Philip J.","Booth School of Business, University of Chicago, Chicago IL, 60637, United States of America,Department of Economics, University of Chicago, Chicago IL, 60637, United States of America","Received 14 April 2019, Revised 13 April 2020, Accepted 22 April 2020, Available online 30 April 2020, Version of Record 18 May 2020.",https://doi.org/10.1016/j.jmateco.2020.04.003,Cited by (1),"We provide an up to 30% improvement in the Shapley–Folkman theorem error-bound, and briefly discuss its consequences for the course allocation problem.",Budish (2011) considers the indivisible-goods combinatorial assignment problem in the context of an economy in which all goods are in fixed supply and agents are endowed only with fiat money. Budish establishes the existence of an approximate competitive equilibrium in which the agents have nearly equal money endowments and in which markets clear up to an error that is independent of both the number of agents and of the total supplies of all of the goods.==== ==== That money endowments can be made nearly equal is important for establishing a number of results on the fairness of the final allocation.====A careful look at Budish’s proof reveals that it provides an improved error bound for the Shapley–Folkman theorem. The purpose of this note is to provide an explicit statement and proof of this result. We also show that the improved bound can be up to 30% tighter and we provide an application to the course allocation problem in which this maximal improvement is nearly attained.,An improved bound for the Shapley–Folkman theorem,https://www.sciencedirect.com/science/article/pii/S030440682030046X,30 April 2020,2020,Research Article,209.0
Raghavan Madhav,"Department of Economics, Université de Lausanne, Switzerland","Received 21 October 2019, Revised 1 April 2020, Accepted 6 April 2020, Available online 22 April 2020, Version of Record 5 May 2020.",https://doi.org/10.1016/j.jmateco.2020.04.001,Cited by (0),"We reinterpret the ‘bossiness’ of a private-goods allocation rule (Satterthwaite and Sonnenschein, 1981) as the ability of an agent to ‘influence’ another’s welfare with no change to her own welfare. In applications where non-bossiness is not possible, we propose simple conditions on (1) which agents may have influence (==== and ====), and (2) the welfare consequences of influence (==== and ====). We apply these conditions to three well-known bossy rules: the ‘Vickrey rule’ in single-object auctions (Vickrey, 1961) (acyclic, positive), the ‘doctor-optimal stable rule’ in matching with contracts (Hatfield and Milgrom, 2005) (acyclic, positive, preserving) and ‘generalised absorbing top-trading cycles (GATTC) rules’ in housing markets with indifferences in preferences (Aziz and Keijzer, 2011) (acyclic, opposite, preserving). Under mild restrictions, we show how the nature of influence under a strategy-proof rule determines whether or not it satisfies ==== (requires acyclicity and either positivity or preservation), ==== (acyclicity and positivity) and ==== (acyclicity and oppositeness). In addition, we propose an influence-related generalisation of the ==== in school choice (Kesten, 2010), and characterise influence for strategy-proof GATTC rules in housing markets.","Private-goods allocation holds a distinguished place in the mechanism design literature. Allocation rules such as the ‘Vickrey rule’ for auctions (Vickrey, 1961), the ‘Gale–Shapley rule’ for school choice and college admissions (Gale and Shapley, 1962, Abdulkadiroğlu and Sönmez, 2003), the ‘doctor-optimal stable rule’ in matching with contracts (Hatfield and Milgrom, 2005), the ‘uniform rule’ for division problems (Sprumont, 1991), the ‘minimum demand rule’ for cost sharing (Serizawa, 1999), and rules based on ‘top-trading-cycles’ for house allocation and housing markets (Shapley and Scarf, 1974, Pápai, 2000, Pycia and Ünver, 2017, Aziz and Keijzer, 2011), are not only theoretically appealing, but have also found much success in practical applications. The ‘input’ to any such rule is some form of preferences over alternatives that are reported to it by agents, and the ‘output’ specifies each agent’s assignment, thus determining their welfare.====Notably, under some of these rules, an agent can change the welfare of other agents without changing her own welfare, by the expedient of unilaterally reporting different preferences to the rule. For instance, under the Vickrey rule, there is a bidder who does not win the auction (and pays nothing), but her bid sets the winning price.==== ==== A slightly different bid from this ‘price-setting’ agent could change the welfare of the winning bidders by changing the winning price, with no effect on her own welfare (she continues to lose and pay nothing). In another example, when the Gale–Shapley rule is used to match students to schools in centralised admissions programmes (Abdulkadiroğlu and Sönmez, 2003), some students may unilaterally alter the preferences they report to the rule in a way that changes the matched schools for other students, without changing the schools with which they are themselves matched (Kesten, 2010).====The reader may note that such rules are essentially ‘bossy’.==== ==== Bossiness is usually studied in the literature in its negation, i.e., as ‘non-bossiness’. Indeed, many applications admit non-bossy allocation rules, such as ‘hierarchical exchange rules’ (Pápai, 2000) or ‘trading-cycle rules’ (Pycia and Ünver, 2017) in object-allocation, the ‘you-request-my-house-I-get-your-turn’ rule in house allocation with existing tenants (Sönmez and Ünver, 2005, Sönmez and Ünver, 2010), the uniform rule (Sprumont, 1991) in division problems, ‘minimum demand rules’ (Serizawa, 1999) for cost sharing, ‘proportional trading rules’ (Barberà and Jackson, 1995) in exchange problems, and others.====But non-bossiness is not always achievable. For instance, it sometimes conflicts with other desirable criteria. In two-sided matching models, non-bossiness is incompatible with ‘stability’ (Kojima, 2010). In models of object reallocation, where agents might be indifferent between objects, non-bossiness is incompatible with Pareto-efficiency, ‘individual rationality’ and strategy-proofness (Bogomolnaia et al., 2005, Jaramillo and Manjunath, 2012). Rules in the class of Vickrey–Clarke–Groves are bossy (Vickrey, 1961, Clarke, 1971, Groves, 1973). However, little is known about the ==== of bossiness, or how agents influence each others’ welfare, even while its presence is utilised for welfare-gains (Kesten, 2010) or as a channel for collusion (Graham and Marshall, 1987).====We do not take a stand on the desirability or otherwise of non-bossiness.==== ==== Our methodological contribution is to analyse the ==== of bossiness when it is unavoidable, i.e., where non-bossiness is not possible. In particular, we study situations where, given some ‘profile’ of preferences, an agent can change the welfare of another agent by unilaterally reporting different preferences to the rule, with the additional restrictions that (1) she reports that her originally prescribed assignment is now worth relatively more to her than in her original preferences,==== ==== and (2) there is no change in the welfare of the agent herself (when viewed under her original preferences). If this is the case, we say that this agent ‘influences’ (the welfare of) the other agent at that profile, under that rule. One immediate consequence is that, if influence of this sort exists, we can check which agents influence which others at a given profile of preferences under a rule, and thereby determine the induced ‘influence relation’ on the set of agents. The influence relation – and, indeed, the notion of influence itself – provides a new analytical toolkit with which to study allocation rules.====As an illustration of the potential of this approach, we propose some new, intuitive and simple structural conditions on influence. We address two aspects. The first has to do with which agents are influential under a given rule at a given profile of preferences, and how the set of influential agents changes across profiles. The conditions are that: (1) the induced influence relation among agents is ‘acyclic’; and (2) the independence of agents from other agents’ influence is preserved across welfare-improving preference changes (‘preservation’). The second aspect concerns welfare consequences: Are agents better off or worse off when they are influenced by others? Here, the conditions are that (3) the welfare effects on influenced agents are always positive (‘positivity’); and (4) whenever there are influenced agents, there is always at least one who is better off and at least one who is worse off in welfare terms (‘oppositeness’). Notice that positivity and oppositeness are mutually exclusive.====These conditions are studied in three very different settings: (1) The doctor-optimal stable rule in matching with contracts; (2) The Vickrey rule in multi-unit single-object auctions; and (3) Generalised absorbing top-trading-cycles (GATTC) rules in housing markets with indifferences (Jaramillo and Manjunath, 2012, Alcalde-Unzu and Molis, 2011, Aziz and Keijzer, 2011). In Proposition 2, Proposition 4, Proposition 6, we show that the Vickrey rule and the doctor-optimal stable rule satisfy positivity but not oppositeness, whereas GATTC rules satisfy oppositeness but not positivity. We also show in the corresponding propositions that all three rules are acyclic, and only the Vickrey rule of the three fails to satisfy preservation. Thus we are able to show – for the first time – that the nature of bossiness can be structurally quite similar even for seemingly very different rules. We believe this offers a new direction for research on private-goods allocation.====We also show how the nature of influence is intimately related to various important properties of allocation rules. We show in Theorem 1 that an allocation rule that is strategy-proof and acyclic is weakly Maskin monotonic if and only if it is positive. This offers a new explanation for why the doctor-optimal stable rule and the Vickrey rule are weakly Maskin monotonic. In Theorem 2, we show that any allocation rule defined on a ‘rich’ domain that satisfies acyclicity and either positivity or preservation is weakly group-strategy-proof if and only if it is strategy-proof. This result is related to the equivalence result in Barberà et al. (2016), but provides new insights as it is based solely on the structure of influence under allocation rules. In Theorem 3, we show that a rule defined on a ‘top-rich’ domain that is strategy-proof, acyclic and ‘unanimous’ is Pareto-efficient if and only if it is opposite. Again, this provides a novel connection between GATTC rules and Pareto-efficiency. These properties of the corresponding rules are well-known, but our analysis provides a new methodological foundation for them. Moreover, we can pin down the precise gap between weak Maskin monotonicity and Pareto-efficiency — it results from the incompatibility between positivity and oppositeness.====Influence also helps illustrate the working of the ‘efficiency-adjusted deferred-acceptance mechanism’ (EADAM) in models of school choice (Kesten, 2010). Based on obtaining consent from certain students to waive potential priority violations, the EADAM generates efficiency improvements to the student-proposing Gale–Shapley rule, without hurting consenting students. In particular, the EADAM involves following a ==== over consenting students, tracing backwards the steps of the underlying student-proposing Gale–Shapley algorithm. Kesten (2010) shows by example that many other natural orders over students do not produce the same result, but the reason for using this specific backwards order is hitherto unexplained in the literature. Our analysis of influence provides a new justification. We show that the consenting students considered by the EADAM are the students with influence under the student-proposing Gale–Shapley rule. Moreover, we show that the order over consenting students used by EADAM is consistent with the acyclic influence relation induced by the Gale–Shapley rule: More precisely, in any round of EADAM, the consenting students considered are those that influence some other students, but are themselves independent of the influence of any other student.====We use this insight to propose a more general version of EADAM, which we called ‘influence-respecting’ EADAM, that considers in any round of the procedure ==== students who influence some other students but are themselves independent of the influence of any other student. Proposition 3 states that this ‘influence-respecting’ EADAM is outcome equivalent to EADAM for any profile of consent. In particular, EADAM is a special case of influence-respecting EADAM that simply traces backwards the steps of the underlying Gale–Shapley algorithm. We illustrate the difference between the two rules with an example.====We also characterise all instances of influence under strategy-proof GATTC rules in housing markets with indifferences (Proposition 5). We believe that this might help in characterising all strategy-proof, individually rational and Pareto-efficient rules in such environments, which is as yet an open question (see Bogomolnaia et al., 2005, Jaramillo and Manjunath, 2012).====The paper is organised as follows. In Section 2, we provide a general model of private-goods allocation that encompasses applications with or without monetary transfers. We introduce our notion of influence in Section 3. In Section 4, we define some commonly-used properties of preference domains and allocation rules. We prove our main results in Section 5. Section 6 contains our three applications, and Section 7 concludes. All proofs omitted in the main text are collected in the Appendix A Omitted proofs, Appendix B Examples.",Influence in private-goods allocation,https://www.sciencedirect.com/science/article/pii/S0304406820300446,22 April 2020,2020,Research Article,210.0
Karni Edi,"Johns Hopkins University, Department of Economics, United States of America","Received 21 November 2019, Revised 30 March 2020, Accepted 8 April 2020, Available online 22 April 2020, Version of Record 30 April 2020.",https://doi.org/10.1016/j.jmateco.2020.04.002,Cited by (1),"This is a study of probabilistically sophisticated choice behavior when the preference relation is incomplete. Invoking the analytical framework of Anscombe and Aumann (1963) and building on the work of Machina and Schmeidler (1995), the paper provides an axiomatic characterization of the general multi-prior multi-utility probabilistically sophisticated representation. In addition, the paper examines the axiomatic foundations for two special cases: complete beliefs and complete tastes. In the former case, the incompleteness is due to ambiguous tastes and in latter case it is due to ambiguous beliefs.","Choice-based definition of subjective probabilities presumes that, when called upon to decide among courses of action whose consequences are not known in advance, decision makers form of beliefs about the likely realization of the consequences, and that these beliefs are quantifiable by probabilities. Because the beliefs are personal, their representation is dubbed subjective probabilities. Borel (1924), Ramsey (1931) and de Finetti (1937) were first to propose the key idea that subjective probabilities may be inferred from the odds a decision maker is willing to offer when betting on events or the truth of propositions. This idea found its ultimate expression in the seminal works of Savage (1954) and Anscombe and Aumann (1963). A common feature of these works is that the subjective probabilities are defined in the context of expected utility theory. Consequently, these works confound the definition of subjective probabilities with the hypothesis that individual choice among uncertain prospects is representable by a functional that is linear in the probabilities. However, the representation of a decision maker’s beliefs by subjective probabilities and the notion of expected utility maximizing choice behavior are two separate ideas.====Machina and Schmeidler, 1992, Machina and Schmeidler, 1995 severed this connection by proposing a model, dubbed probabilistic sophistication, in which choice-based subjective probabilities are defined without requiring that the decision maker’s preferences respect the strictures of expected utility theory. According to Machina and Schmeidler subjective probabilities transform acts (that is, random variables on a state space that take their values in the set of consequences) into lotteries (that is, the corresponding probability distributions on the set of consequences) and preferences are represented by a utility function over the set of lotteries.====A central tenet of both the expected utility models and the probabilistically sophisticated models is that all alternative courses of action are comparable. That this presumption is not tenable as a general depiction of real-life decision making was recognized by von Neumann and Morgenstern who wrote “It is conceivable – and may even in a way be more realistic – to allow for cases where the individual is neither able to state which of two alternatives he prefers nor that they are equally desirable” (von Neumann and Morgenstern, 1947 p. 19). Aumann (1962) finds universal comparability not only an inaccurate description of real-life decision making but also lacking normative appeal. In his words, “Of all the axioms of utility theory, the completeness axiom is perhaps the most questionable. Like others of the axioms, it is inaccurate as a description of real life; but unlike them, we find it hard to accept even from a normative viewpoint” (Aumann, 1962 p. 446). Empirically, the main manifestation of incomplete preferences is indecisiveness or inertia.====Considering the restrictive nature of the completeness requirement, the objectives of this paper are to examine the implications of relaxing the completeness axiom in Machina and Schmeidler’s theory of probabilistically sophistication choice, and to study the representations of ambiguous beliefs and tastes in this model. More specifically, invoking the analytical framework of Anscombe and Aumann (1963), I explore conditions under which incomplete preference relations admit a multi-utility multi-prior in the probabilistically sophisticated representation.==== ==== In addition, I explore the conditions that characterize two special cases: Knightian uncertainty and single-prior multi-utility representation. The former case, first explored by Bewley (2002) in the context of expected utility theory, attributes the incompleteness to the decision maker’s ambiguous beliefs and the latter, explored by Shapley and Baucells (1998) and Dubra et al. (2004), to his ambiguous tastes.====The main analytical difficulty introduced by the incompleteness of the preference relation is due to the non-transitivity of the incomparability relation. When the preference relation is complete, indecisiveness arises only when the decision maker is indifferent among the alternatives under consideration. In Machina and Schmeidler (1995) the indifference is an equivalence relation, hence it is transitive. They exploit the transitivity of indifference to link general Anscombe–Aumann acts to constant acts that are convex combinations of the state-contingent payoffs of the original acts. This link is severed when the incomparability relation is non-transitive. The loss of transitivity requires that the preference structure is enhanced by the introduction of two new axioms dubbed replacement acyclicity and constant-act comparability. Replacement acyclicity requires that the incomparability relation restricted to replacement paths be acyclic. Constant-act comparability requires that one act be strictly preferred over another if and only if the induced constant acts that are non-comparable to the former are strictly preferred to those induced by the latter using the same reduction process.",Probabilistic sophistication without completeness,https://www.sciencedirect.com/science/article/pii/S0304406820300458,22 April 2020,2020,Research Article,211.0
Kranich Laurence,"Department of Economics, University at Albany, 1400 Washington Avenue, Albany, NY 12222, USA","Received 11 October 2019, Revised 8 February 2020, Accepted 13 March 2020, Available online 17 April 2020, Version of Record 25 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.009,Cited by (3),"I consider the problem of determining an equitable and efficient allocation of resources in production economies with factors which must be dedicated to production and cannot be consumed directly. First, I show that in such economies envy-free and efficient allocations exist under standard assumptions. However, I argue this notion of fairness is unsuitable for the present context. I then introduce a new notion of fairness, which I call ====. First, I associate with each consumption bundle its ==== consisting of the vector of factors used to produce it. I then show that preferences over consumption bundles can be extended to preferences over factor bundles. An allocation is resource-envy-free if no agent prefers another agent’s resource footprint to its own. The analysis of resource-envy-free allocations in production is exactly analogous to the analysis of envy-free allocations in exchange. I establish that resource-envy-free and efficient allocations exist under standard assumptions, and I demonstrate that such allocations are intuitively appealing.","In this paper I consider the problem of determining an equitable and efficient allocation in production economies in which factors must be dedicated to production and cannot be consumed directly.==== ==== ==== This includes most raw materials and extractive resources but it excludes labor.==== ==== In such an economy, the basic allocative questions are: (i) How should the factors be employed in producing consumption goods? and (ii) How should the output be allocated among consumers?==== ==== Here, I introduce a new notion of fairness, called resource-envy-freeness, and I establish that resource-envy-free and efficient allocations exist under standard assumptions. The novel aspect of the new notion is that it includes the equitable allocation of factors – question (i), above – in the evaluation of fairness.====Historically, the problem of formulating an appropriate ordinal==== ==== equity criterion for use in production economies has proven to be difficult. The most common notion is ==== (Foley, 1967, Tinbergen, 1946), where an allocation is said to be envy-free if no agent prefers the consumption bundle of another agent to its own. Under weak conditions, this notion of fairness is compatible with Pareto efficiency in exchange economies, that is, there are allocations which satisfy both criteria.==== ==== However, as demonstrated by Pazner and Schmeidler (1974), that need not be the case for economies involving production (and non-dedicated factors).==== ==== ==== Their result, that there are production economies in which envy-free and efficient allocations do not exist, was discouraging since both criteria seem appealing. In this paper I show that for economies with dedicated factors, envy-free and efficient allocations do exist under standard assumptions. However, I argue that this notion of fairness is unsuitable for the present context.==== ====To see the problem, consider an extreme example with two people: consumer ====, who likes only commodity ====, and consumer ====, who likes only commodity ====. If all of the resources were devoted to producing commodity ==== and consumer ==== were to receive the entire output, the allocation would be envy-free and efficient.==== ==== Nevertheless, I would argue that the allocation is unfair since a disproportionate amount of the resources are devoted to producing ====’s preferred good. Thus, it is not that consumer ==== is envious of ====’s consumption bundle, but rather it is envious of the resources devoted to producing ====’s bundle.==== ==== This leads to the notion of resource-envy-freeness. To be precise, with any consumption bundle one can associate its ==== consisting of the vector of factors used to produce it. First, I show that preferences over consumption bundles can be extended to preferences over factor bundles. An allocation is ==== if no agent prefers another agent’s resource footprint to its own. Interestingly, under standard assumptions on preferences and technologies, much of the discussion of no-envy in exchange can be translated directly to factor space and the analysis of no-envy of resources, in particular the simple argument for the existence of resource-envy-free and efficient allocations.==== ==== Finally, I point out that resource-envy-free and efficient allocations provide an appealing answer to the problem described above.==== ====The paper is organized as follows. In Section 2, I describe the basic model, and I discuss the standard notion of no-envy in this context. In Section 3, I introduce the notion of resource-envy-freeness. Section 4 considers the relationship between resource-envy-freeness and efficiency. Finally, Section 5 contains a brief conclusion.",Resource-envy-free and efficient allocations: A new solution for production economies with dedicated factors,https://www.sciencedirect.com/science/article/pii/S0304406820300409,17 April 2020,2020,Research Article,212.0
"Chambers Christopher P.,Miller Alan D.,Yenmez M. Bumin","Department of Economics, Georgetown University, ICC 580, 37th and O Streets NW, Washington, DC 20057, United States of America,Faculty of Law, Western University, 1151 Richmond Street, London, Ontario N6A 3K7, Canada,Faculty of Law and Department of Economics, University of Haifa, 199 Aba Hushi Avenue, Mount Carmel, Haifa, 3498838, Israel,Department of Economics, Boston College, 140 Commonwealth Avenue, Chestnut Hill, MA 02467, United States of America","Received 14 November 2019, Revised 11 March 2020, Accepted 12 March 2020, Available online 11 April 2020, Version of Record 17 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.008,Cited by (4),"We investigate the results of Kreps (1979), dropping his completeness axiom. As an added generalization, we work on arbitrary ====, rather than a lattice of sets. We show that one of the properties of Kreps is intimately tied with representation via a ====. That is, a preference satisfies Kreps’ axiom (and a few other mild conditions) if and only if there is a closure operator on the lattice, such that preferences over elements of the lattice coincide with dominance of their closures. We tie the work to recent literature by Richter and ==== (2015).","In behavioral decision theory, the term ==== refers to a bundle of alternatives, any of which an individual may consume at a future date. The menu choice literature refers to individual choice amongst menus. The interpretation is that an individual chooses a menu, from which she will be asked to choose at some later date. Usually, this second stage of choice is only implicit; we never get to see this second stage. The menu choice literature is predicated on the observation that many individuals seek to “leave their options open”, as they may be (informally) “uncertain” about what their future preferences over alternatives may be. For example, a typical preference discussed in the menu literature might feature choices such as: ====and ====Observe that a “rational” decision maker, who perfectly knows what her preferences will be at the later date when she will be expected to choose from the menu, would never exhibit such choices. She would either prefer the apple or the banana, and would therefore be indifferent between the menu consisting of both and ==== of the singleton menus.====The seminal work on menu choice is due to Kreps (1979). He establishes two classic decision-theoretic results on the theory of preferences over menus. First, he characterizes those preferences over menus that behave as what we will call ====. These are preferences for which there is an underlying preference over alternatives generating the preference over menus. Second, he characterizes those preferences over menus that admit a ==== representation.==== ==== Such a preference can be represented as if there is a collection of preferences over alternatives, and the preference over menus is monotonic with respect to these preferences.====Our aim in this note is to consider Kreps’ first result without the completeness property, and establish that several interesting examples from the theory of choice share a common structure. We do so in a broader framework than menu choice—the key observation is that the set of “menus” has an algebraic structure (namely union and intersection) that renders it a semilattice. To this end, and observing that there is nothing particularly special about the collection of menus, we work on more general partially ordered sets satisfying different algebraic properties.====In the context of menu choice, our primary contribution here is to establish (in a suitably generalized environment) that upon dropping completeness (the fact that any pair of objects can be ranked), we admit a vector ==== representation. This means that there is a family of indirect preferences, and one menu dominates another if and only if it does so for every indirect preference in the family. This is termed “vector-valued”, since if each indirect preference admits a real-valued representation, the ranking coincides with vector dominance of the imputed image of sets under the vector of representations. Such ideas are more or less standard in the theory of incomplete rankings (Szpilrajn, 1930, Ok, 2002). For example, dropping completeness from the remaining von Neumann–Morgenstern axioms admits a vector expected utility representation.====Much of the proof of this observation is already implicit in Kreps’ work. In the proof of his second result (mentioned above), he defines an auxiliary binary relation on menus. This auxiliary relation can be demonstrated to have all of the properties of a relation in his first theorem, with the exception of completeness.====We actually go further, and work on more general semilattices. Our first result does the following, for a complete join semilattice. We study the analogues of Kreps’ axioms in the first theorem, without completeness, and establish that there is a one-to-one correspondence between orders satisfying these axioms and ==== (Ward, 1942). Closure operators are objects from mathematics, but they have recently found much application in economics. For example, Richter and Rubinstein (2015) study the family of ====, which are a special type of closure. In another work, Nöldeke and Samuelson (2018) recently exploit the theory of Galois connections in mechanism design; Galois connections are intimately tied to the theory of closure.==== ==== There are many closure operators that are familiar in economics. Topological closure is a closure operator on the lattice of sets. The convex hull is a closure operator on a lattice of sets. The convex envelope of a real-valued function is a closure operator on the lattice of functions. Generally speaking, any object that can be defined as the “smallest” object of a certain type dominating another object serves as a closure. For example, the topological closure of a set is the smallest closed set containing that set. The convex hull is the smallest convex set containing that set, and so forth.====Kreps’ work and most subsequent work focus on the semilattice of menus. It might be thought that this is without loss of generality, as many semilattices or lattices are isomorphic to lattices of sets. In particular, the celebrated Birkhoff representation theorem (Birkhoff, 1937) claims that any distributive lattice can be homomorphically embedded in a lattice of sets with the usual union and intersection properties. Motivating our general exercise are two examples of rankings of ==== of a given set. The set of partitions is naturally ordered by the refinement relation, and is well-known to be non-distributive. In particular, these lattices cannot be mathematically modeled as lattices of sets. So the added generality of our results possesses implications for domains that need not “look like” lattices of sets.====On a lattice of subsets of a given set (with the usual union and intersection operations), it turns out that closure operators can be represented as the intersection of lower contour sets of weak orders.==== ==== This fact is implicit in Kreps and is entirely analogous to Richter and Rubinstein’s observation that a convex geometry (antimatroid) can be represented as the intersection of lower contour sets of linear orders.==== ==== Closely related as well is the famous decomposition result for path-independent choice functions of Aizerman and Malishevski (1981). This allows the general “vector” representation alluded to for families of sets. See also Richter and Rubinstein (2019) for an abstract definition of convexity based on these ideas.====Now, we can also investigate Kreps’ second result for an arbitrary lattice; positing the natural analogue of his axiom for an arbitrary lattice elucidates the structure of his second theorem. It allows us to define the same auxiliary relation defined in Kreps, which we are able to show satisfies the axioms of his first result. We can use these facts to establish a generalized version of the Kreps result: any preferences over an arbitrary lattice satisfying the adapted Kreps axioms can be represented as a strictly monotonic function of some closure operator. This is of practical relevance, as Chambers and Echenique, 2008, Chambers and Echenique, 2009 jointly establish another result: preferences satisfying Kreps’ axioms are those that have monotonic and submodular representations. Thus, it turns out that the ordinal content of submodularity is captured by the property of being strictly monotonic with respect to a closure operator.====Applications of these types of results can be found, for example, in Chambers and Miller, 2014a, Chambers and Miller, 2014b and Chambers and Miller (2018). Here we detail a few.",Closure and preferences,https://www.sciencedirect.com/science/article/pii/S0304406820300392,11 April 2020,2020,Research Article,213.0
"Yang Jian,Li Jianbin","Department of Management Science and Information Systems, Business School, Rutgers University, United States of America,School of Management, Huazhong University of Science and Technology, China","Received 16 September 2018, Revised 22 October 2019, Accepted 13 March 2020, Available online 9 April 2020, Version of Record 15 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.010,Cited by (2),"We study a non-traditional cooperative game where returns from coalitions are nondeterministic. The long-standing concept of core can be generalized to reflect players’ contentment with their allocations. It is now imperative to formalize the restrictions, such as those pertaining to information, on allocations. The latter are also at times more conducive to fractional representations. With probabilistic structures added, nondeterministic returns become random variables, utility functions attain risk-attitude connotations, and the timing of players’ allocation resolutions gains significance. Under various conditions for utility functions, we show how various core concepts of the general game can be related to its traditionally defined auxiliaries. These developments help pave the way for our illustrations, within two distinct settings, that players’ increased ==== would promote the formation of the grand coalition.","Cooperative games are formulated to model players’ cooperative behaviors. They are traditionally premised on deterministic returns. When nondeterministic returns are involved in a general game other than the special market-based type representing exchange economies, it is often their weighted averages or at the most some summary utilities that are interfaced with the games’ apparatuses. But this arrangement is inadequate in dealing with issues like the timing of allocation resolutions, and is incapable of factoring risk calculations in decision makers’ choices for large coalitions. As a matter of fact, we believe that at least part of what motivate lions to form prides, cavemen to bond in hunting groups, and lawyers to join partnerships, is the tendency for larger groups to shield individuals more forcefully from risk exposures. On average, a caveman’s contribution to a group may not be more than what he can do to himself. Even if the average meal size stays the same, joining the group will likely provide him steadier meals which would in turn dramatically improve his chance of survival. The traditional game seems ill equipped to deal with phenomena of this kind.====It is against this backdrop that we explore some generalizations. Our new definitions countenance both nondeterministic returns coming out of coalitions and players’ general preferences for their multi-dimensional gains in reflection of their ambiguity and risk attitudes. Risk will be our focus throughout. As is the case with the traditional game, we deal primarily with the concept of core. It hosts ways of dividing the return from the grand coalition in such a fashion that no smaller coalition would ever be able to improve all participants’ gains, with some improvements being strict. The nonemptiness of the core will incentivize players to form the grand coalition in the current multi-dimensional environment.====Our multi-dimensional setup could be viewed through the probabilistic lens. Then, nondeterministic returns would be understandable as random variables; also, utility functions would be identifiable with risk attitudes. The randomness in returns would push the hitherto irrelevant concern about timing to the fore. We introduce the concept of “contention profiles” in Definition 1 to model the situation where allocations in the core compete with a partial subset but not necessarily the complete set of alternative allocations. For instance, all members of the subset might be measurable with respect to a certain ====-field. Sometimes, it is also more convenient to describe allocations of the return from a coalition in fractional rather than absolute terms. This gives rise to the concept of the “fractional core” in Definition 2, which often has a one-to-one correspondence with its absolute counterpart.====We direct our effort to the task of linking the more general core concepts for a more general game involving nondeterministic returns to the ordinary core of the game’s traditional auxiliary involving scalar returns. As a bridge in between, there naturally emerges the concept of “programmable center” as laid out in Definition 3, Definition 4, respectively, for the absolute and fractional cases. With numbers associated with coalitions prepared through mathematical programming beforehand, a programmable center uses one inequality per coalition to define its member. This is more in line with the traditional core and much different from our new core definitions, where potentially complex Pareto-dominance expressions are required.====Connections between the traditional and new core concepts can be revealed under certain utility functions. When they are translation-invariant, Theorem 1 establishes a link for the absolute core that has repercussions for the fractional core as well. When all players share the same utility, the latter’s positive homogeneity would result in Theorem 2 for the fractional core. Further simplifications can be achieved by the utility’s super additivity and positivity. These results allow us to deal with two extreme cases regarding the timing of allocation resolutions. Under positive early settlement, players’ percentage gains are predetermined at a coalition’s formation stage before its return is realized; for instance, one player may claim to own 1% of the final coalition-wide return no matter what. In the late-settlement case, players’ gains can depend on a coalition’s actual return in arbitrary fashions; for instance, a caveman can stake a prime cut of a downed mammoth if he has delivered the fatal puncture and otherwise, be content with sharing the remaining spoil with other cavemen who have just played secondary roles. Even for the latter case, we insist that in our ==== regime, the allocation plan should have been set in stone by the time of coalition formation.====In two distinct settings, rigor can be achieved for the intuitively appealing notion that “risk aversion promotes cooperation”. The first one concerns the positive early-settlement case alone. We can introduce a partial order among games so that one might be deemed “less centripetal” than another based on a comparison between their utility functions and nondeterministic returns. The centripetal order is responsible for an order between fractional cores. Then in Definition 5, we give criteria on the collection of all coalitions’ nondeterministic returns being “auto-centripetal”. This new concept is somehow conjugate to the conditional value at risk ====. A player’s utility ==== may be ==== for some ====, so that ==== measures the average of the worst ====-portion of ====. If so, ==== is an aversion indicator with the risk-neutral case of ==== expressing the least risk aversion. For games sharing a common auto-centripetal return profile, Theorem 3 conveys a desired message: the higher an ==== that players use in their ====-induced utilities, the more likely they will agree on predetermined percentage allocations. This result can be used to justify our conjecture regarding the cavemen’s motives without us delving into potentially involved linear programs (LPs).====We can work on both the positive early-settlement and late-settlement cases in the second setting. In it, the total return achievable by a coalition is the sum of participants’ random contributions minus a deterministic cost term reflecting the difficulties and wastes inherent with such cooperation; players’ individual contributions are identically but not necessarily independently distributed; also, players share a common utility function ==== that is of the mean-deviation type: ====. The coefficient ==== in front of the deviation term again measures players’ aversion to risk. Despite potentially increasing difficulties associated with bigger coalitions, we show that a higher aversion indicator ==== will make players readier to join the grand coalition; see Theorem 4 for the positive early-settlement case and Theorem 5 for the late-settlement case. We suspect that the coherent risk measure, due partially to its various dual presentations, could provide a more fertile ground for risk-coalition connections.====In the remainder of the paper, we survey relevant literature in Section 2. The journey from the traditional formulation to our more general ones is made in Section 3. Under various assumptions on utility functions, we establish connections between the general notions and some of their traditional auxiliary counterparts in Section 4. For two distinct settings in Section 5 involving conditional values at risk and mean-deviation risk measures, respectively, we derive concrete results in support of the notion that “risk aversion promotes cooperation”. The paper is concluded in Section 6.",Cooperative game with nondeterministic returns,https://www.sciencedirect.com/science/article/pii/S0304406820300410,9 April 2020,2020,Research Article,214.0
Russell Jeffrey Sanford,"University of Southern California, United States of America","Received 22 November 2019, Revised 28 March 2020, Accepted 31 March 2020, Available online 9 April 2020, Version of Record 18 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.011,Cited by (1),"We prove a ==== for preference relations over countably infinite lotteries that satisfy a generalized form of the Independence axiom, without assuming Continuity. The ==== on abstract superconvex spaces.",None,Non-Archimedean preferences over countable lotteries,https://www.sciencedirect.com/science/article/pii/S0304406820300422,9 April 2020,2020,Research Article,215.0
"Eeckhoudt Louis,Courtois Olivier Le","CNRS (LEM, UMR 9221), France,Iéseg School of Management, France,LEM (UMR 9221), France,emlyon business school, France","Received 27 August 2019, Revised 16 March 2020, Accepted 24 March 2020, Available online 9 April 2020, Version of Record 15 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.007,Cited by (1), by Liu and Meyer (2013). We show that the intensity measure of the preference for bivariate risk apportionment is characterized by bivariate risk attitudes in the sense of Ross. The usefulness of our measures to understand economic choices is illustrated by the analysis of two specific decisions: savings under environmental risk and medical treatment in the presence of diagnostic risks.,"Bivariate risk apportionment is the preference for dispersing risks associated with two aspects of individuals’ well being into different states of the world. Although the term is quite recent (see for instance Jokung, 2011, Schlesinger, 2015, Li et al., 2016), the principle is not. Richard (1975) introduced it in the particular case of degenerate risks (==== sure losses) as follows: if for any ==== and any ====, the decision maker prefers the lottery that gives an even chance to ==== or ==== to the lottery that gives an even chance to ==== or ====, then the decision maker is considered multivariate risk averse. This preference – also known nowadays as correlation aversion – has later been generalized by Eeckhoudt et al. (2007), who extended the same dissociation principle from sure losses to higher order zero-mean changes in risks.==== ==== For instance, suppose again that ====, that ==== is a zero-mean risk and that the decision maker is risk averse, so that ====. Eeckhoudt et al. (2007) define cross-prudence as the preference for the lottery that gives an even chance to ==== or ==== to the lottery that gives an even chance to ==== or ====. Likewise, if we assume in addition that ==== is another zero-mean risk so that ====, cross-temperate decision-makers prefer the lottery that gives an even chance to ==== or ==== to the lottery that gives an even chance to ==== or ====. Therefore, cross-prudence and cross temperance are – along with correlation aversion – particular cases of bivariate risk apportionment.====Besides defining these concepts, Richard (1975) and Eeckhoudt et al. (2007) also established their connection in the expected utility model with properties of the utility function. Specifically, they indicated that correlation aversion (resp. cross-prudence; resp. cross-temperance) corresponds to==== ====
 ==== (resp. ==== or ====; resp. ====). This proves useful when characterizing a variety of economic decisions such as precautionary saving (Eeckhoudt et al., 2007, Courbage and Rey, 2007), insurance decisions (Cook and Graham, 1977, Rey, 2003), prevention choices (Xue and Cheng, 2013) or investments improving future random health/environmental quality (Denuit et al., 2011). For instance, correlation aversion (resp. correlation seeking) has been shown to increase (resp. reduce) the demand for insurance when losses occur along both the financial and the non-financial dimension (see Cook and Graham, 1977, Rey, 2003). Likewise, the introduction of a zero mean background risk on the second attribute of the utility function increases precautionary saving (Eeckhoudt et al., 2007) and the propensity to make efforts in order to reduce the probability of a given wealth loss (Xue and Cheng, 2013) if agents are cross-prudent.====However, other characterizations of preferences are required when interpersonal comparisons are made or when some comparative statics exercises are performed. For instance, the comparison of the intensity of precautionary saving of agents who face a risk on the second attribute of their utility function depends on the relative values of ==== across agents (Jouini et al., 2013). In the same vein, individuals in better health states hold more risky assets in their portfolios if ====
 (Crainich et al., 2017). In these two examples, identifying the signs of the cross derivatives of the utility function is not sufficient to capture or to compare the trade-offs dictating agents’ decisions. These trade-offs actually depend on ratios whose numerators and/or denominators are cross partial derivatives (of different orders) of the utility function. The papers of Jouini et al. (2013) and Crainich et al. (2017) actually suggest that the analysis of some economic decisions requires not only the identification of the ==== of preferences towards bivariate risks, but also the measurement of the ==== of these preferences.====In this paper, we propose a general measure of the intensity of preferences for bivariate risk apportionment. To do so, we adapt the technique introduced in the univariate case by Liu and Meyer (2013) to the bivariate one so that our intensity measure is a marginal rate of substitution between favorable and unfavorable changes in bivariate risks. In order to illustrate our index, suppose that an agent prefers the bivariate CDF ==== over ==== because he or she is correlation averse and ==== over ==== because he or she is cross-prudent. The value of ==== such that the agent is indifferent between ==== and ==== is a measure of the intensity of cross-prudence relative to correlation aversion. In the paper, we show how this technique can be extended to provide a general intensity measure of bivariate risk apportionment.====Then, we establish the relationship between our intensity measures of preferences for bivariate risk apportionment and properties of the utility function. Specifically, we highlight that these intensity measures correspond to ratios of cross partial derivatives of the utility function. As a result, they do not depend on utility scales and allow us to compare the intensity of individuals’ preferences for bivariate risk apportionment. These interpersonal comparisons are shown to be based on the concept of Ross higher order risk aversion applied in the bivariate context.====In the paper we also examine two economic decisions where our intensity measures play a central role. Precautionary saving when agents face future environmental risks is the first decision we analyze. Suppose that the environmental quality individuals enjoy results from policies implemented by public authorities. These policies determine how energy is supplied in the country by selecting a mix between different sources of energy that differ with respect to their environmental efficiency (==== their ability to provide energy while maintaining a high environmental quality) and their environmental risk (==== measured by the environmental quality distribution associated with the energy source). Do individuals increase precautionary saving if public authorities plan to modify their energy mix by increasing the share of a given energy (nuclear for instance) at the expense of the other (coal for instance) so that there is an improvement in the expected environmental quality but also a higher probability of catastrophic environmental risk? We show that one of our intensity measures of bivariate risk apportionment - namely, the intensity of cross-prudence relative to correlation aversion - is instrumental in explaining precautionary saving changes after such a policy and in comparing the intensity of these changes across individuals. More generally, this illustration (and another one analyzed in the paper, related to a treatment intensity in the presence of diagnostic risks) shows that our intensity measures of bivariate risk apportionment are useful in understanding any economic decision implying trade-offs between changes in bivariate risks of different orders.====Our paper is organized as follows. We define in Section 1 the concept of ==== degree changes in risk and associate preferences for these changes to the signs of various cross-derivatives of the utility function. We establish in Section 2 the measures of the intensity of preferences for ==== degree changes in risk. We show in Section 3 how these intensity measures characterize some economic choices.",Intensity of preferences for bivariate risk apportionment,https://www.sciencedirect.com/science/article/pii/S0304406820300380,9 April 2020,2020,Research Article,216.0
Lehmann Daniel,"School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel","Received 1 August 2019, Revised 25 March 2020, Accepted 30 March 2020, Available online 9 April 2020, Version of Record 15 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.012,Cited by (1),"This paper defines the notion of a local equilibrium of quality ====, ==== and ====. The quality ==== measures the fit between the allocation and the prices: the larger ==== and ==== the closer the fit. For ==== this notion provides a graceful degradation for the conditional equilibria of Fu, Kleinberg and Lavi (2012) which are exactly the local equilibria of quality ====. For ==== the local equilibria of quality ==== are ==== than conditional equilibria. Any local equilibrium of quality ==== provides, without any assumption on the type of the agents’ valuations, an allocation whose value is at least ==== the optimal fractional allocation. In any economy in which all agents’ valuations are ====-submodular, i.e., exhibit complementarity bounded by ====, there is a local equilibrium of quality ====. In such an economy any greedy allocation provides a local equilibrium of quality ",None,Quality of local equilibria in discrete exchange economies,https://www.sciencedirect.com/science/article/pii/S0304406820300434,9 April 2020,2020,Research Article,217.0
"Pivato Marcus,Vergopoulos Vassili","THEMA, CY Cergy Paris Université, France,Paris School of Economics, and University Paris 1 Panthéon-Sorbonne, France","Received 5 November 2019, Revised 18 March 2020, Accepted 23 March 2020, Available online 2 April 2020, Version of Record 13 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.006,Cited by (4),". Likewise, she can only perform acts that transform states ==== and outcome space ==== of regular open subsets of ==== — a “subjective” state space encoding the agent’s imperfect perception.","Suppose a doctor is considering several drug treatment options for a patient. The efficacy of each drug is a continuous function of the patient’s blood chemistry, blood pressure, and other physiological variables. But many of these variables are unknown, and either they are unmeasurable, or the available instruments are unreliable and imprecise. Alternately, suppose a firm wants to build a new factory. The factory could use one of several different production processes; the future profitability of each one is a continuous function of the unpredictable market prices of several raw materials and of the firm’s final products.====In both examples, an agent must make a choice under uncertainty. The Subjective Expected Utility (SEU) model is the standard paradigm for these kinds of decisions. In the classic axiomatic foundations of Savage (1954) (and most subsequent treatments), uncertainty is described by a state space, and “acts” are functions from this state space into a space of outcomes. But the Savage theory assumes that ==== possible functions from states into outcomes are feasible, so that an agent can meaningfully form preferences over them. This makes sense in decision problems where the state space has a ==== topology (e.g. bets on coins, dice, or urns; Arrow–Debreu economies). But in the examples above, only ==== functions are feasible, because of the underlying technological constraints. There is no drug that will transform the patient’s physiological state discontinuously into a health outcome. Likewise, there is no production process where profit is a discontinuous function of market prices. So it is ill-conceived and potentially misleading to suppose that an agent can form preferences over such unfeasible acts.====Another feature of these examples is the nature of the ==== available to the agent. In the standard Savage model, an agent can acquire information by observing an “event”—that is, a subset of the state space—and can form preferences conditional on this event. In the Savage model, ==== subset of the state space is a potentially observable event. But in the examples above, agents can only acquire information using unreliable and imprecise measurement devices, and many events remain unobservable.====For example, suppose the doctor has an instrument to measure the patient’s high-density lipoprotein cholesterol (HDLC) level. But the instrument only reports HDLC to the nearest milligramme per decilitre (mg/dL). If the instrument reports the cholesterol level as “8 mg/dL”, this means that the measured value is somewhere between 7.5 and 8.5 mg/dL. Furthermore, suppose the machine is susceptible to a random error of 0.3 mg/dL, so the doctor can only be sure that the true level is between 7.2 and 8.8 mg/dL. Perhaps if she carefully repeats the measurement several times, she can reduce this error to 0.05 mg/dL, in which case she will know that the true value is between 7.45 and 8.55 mg/dL.====Clearly, not every decision under uncertainty involves such technological constraints. But many do, often in important applications. One ==== apply the Savage approach directly to these kinds of decision problems. But this would require the agent to form preferences over unfeasible acts and to condition these preferences on unobservable events. This undermines the plausibility of the preference relation, the axioms, and the resulting SEU representation — whether interpreted normatively or descriptively. For these reasons, imperfect perception and continuity constraints require a substantial departure from the Savage framework; this is the topic of this paper. First, we introduce a new model of imperfect perception; then, we use this model to analyse decisions under uncertainty with imperfect perception and continuity constraints. We do this by enriching both the state space and the outcome space with ====. These topological structures are interpreted as objective features of the natural world, rather than the agent’s subjective experience. But they impose constraints on what she can do, and what she can perceive.====Our model of imperfect perception has three aspects. First, we assume that the agent is only able to acquire information about the state by observing it through a ==== of the state space. For example, the doctor is only able to measure the patient’s HDLC levels to the nearest mg/dL. It might be diagnostically useful to measure it to the nearest ==== per decilitre, but her instruments cannot do this. Second, only ==== finite partitions are observable. Finally, even for those measurements the agent ==== make, there may be a small amount of random error. The agent might be able to make this error arbitrarily small, but she cannot reduce it to zero. So the doctor can only really determine that HDLC levels are between ==== and ==== mg/dL, for some small ====.====To capture these three aspects, we suppose that the agent’s information about the state arrives through a ==== of the state space ====. To understand this, suppose that there is an open, continuous function ==== from ==== to a closed interval ==== of real numbers (representing some numerical “measurement”, e.g. the HDLC level in mg/dL), along with a finite set of threshold values ==== such that the agent can only observe events of the form “====” (for some ====), where ==== is the true state of the world and ==== is some measurement error. If she can make ==== arbitrarily small, the agent can, in the limit, observe events of the form “====”. In other words, she can observe the events ====, for all ====. These are closed subsets of ====, which overlap on their boundaries. For technical reasons, it is simpler (and for our purposes, equivalent) to represent these sets by their ====, which are the events ====, where ==== (because ==== is continuous and open). These are ==== of ====, and their union ==== is ==== in ====—we call this a ==== of ====. This is the basic unit of information available to the agent in our model.====Furthermore, we allow that only ==== regular partitions are available—for example, because only ==== real-valued functions ==== can be used in the above construction, because only certain “measurements” are technologically feasible. The collection of all feasible regular partitions generates a Boolean algebra ====. This is the algebra of all events that are observable, either directly or indirectly, by the agent. Any conditional preferences she forms must be conditional on events in ====. Meanwhile, our model represents technological constraints by requiring acts to be ==== functions from the state space to the outcome space. Indeed, the set ==== of feasible acts need not even contain ==== continuous functions; thus, our framework can incorporate further technological restrictions. The agent’s conditional preferences rank acts in ==== conditional upon events in ====.====Despite these limitations, our main results provide axiomatic characterizations of SEU representations for conditional preferences. In these representations, utility is a ==== function; thus, similar outcomes yield similar utility levels. This makes our representations particularly suitable for applications in financial economics, which typically require continuous utility functions (see e.g. Gollier, 2001). Moreover, beliefs are described by what we call a ====, a structure like a finitely additive probability measure on the Boolean algebra ====
 (Theorem 1). Finally, under an additional assumption on ====, beliefs can also be represented by a classical Borel probability measure on the ==== of ====—a “subjective” state space that extends the original state space (Theorem 3). Although our main motivation is the ==== analysis of decisions, our model and results are also appealing at the descriptive level, as we shall explain later in this section.====The aforementioned constraints create some obstacles for our axiomatic characterizations. For example, Savage’s axioms (e.g. the ====) and his construction of conditional preferences depend on the ability to splice any two acts on any binary partition of the state space. Furthermore, Savage obtains the subjective probability measure and utility function by restricting preferences to two-valued acts and finitely-valued acts respectively. But both spliced acts and finitely-valued acts are typically ====, and hence inadmissible in our framework. Furthermore, Savage’s axiom P6 (====) relies on a rich collection of classical partitions of the state space. But in our model, only certain ==== partitions of the state space are available.====The restrictions that we impose on acts and events might seem excessive. After all, even if they are unfeasible or unobservable, an agent can still ==== such acts and events. So she should still be able to form “counterfactual” preferences involving them. But this move is not as innocent as it appears. First of all, it is debatable whether the agent can even “imagine” many of the events and acts over which she must form preferences in the Savage model. (Consider the exotic entities appearing in real analysis textbooks.) Indeed, one could argue that the vast majority of events and acts are literally ====imaginable.==== ====Second, suppose ==== that the agent could still imagine ==== unfeasible acts and unobservable events, even if she cannot imagine all of them. Why can’t she form preferences with these ones? To see the problem, suppose the agent must choose between two feasible acts ==== and ====. Suppose that there is an unfeasible act ==== such that she prefers ==== to ==== and prefers ==== to ====. By transitivity, she must prefer ==== to ====. But why should her preferences between two feasible acts be determined by comparing them to an ==== act?====This question is apposite because many proofs in decision theory and social choice exploit preference transitivity along some chain of alternatives. If we break a link in this chain, such a proof might collapse. So it is not at all obvious that the same theorems would still be true on restricted domains. (This is why ==== is a popular workaround to impossibility theorems in social choice theory.) Do current axiomatic characterizations of subjective expected utility ==== on access to an unrealistically large domain of acts? Would these characterizations ==== on a smaller domain of acts, the same way that Arrow’s Impossibility Theorem breaks down in the domain of single-peaked preference profiles? If so, then what sort of SEU-like representation could replace them in such a domain? These questions are part of the motivation for the present paper.====Alternately, consider an unobservable event ====, and suppose we assert that the agent prefers act ==== to act ==== conditional on ====. Such an assertion is normally explicated by saying, “The agent ==== prefer ==== to ====, ==== she observed ====”. But what does such a sentence even mean, if ==== is an event ====? Even if we accepted that such conditional preferences were somehow meaningful, it is not clear that they should be related to unconditional preferences via a Separability axiom. This raises the question: do current axiomatic characterizations of SEU ==== on an unrealistically large algebra of events?====The previous three paragraphs explain our misgivings about preferences involving unfeasible or unobservable entities in a ==== context. In a ==== context, there is a simpler argument. In this case, we seek a scientific model to predict and explain an agent’s choice behaviour. This model should be consistent with actual human cognition. So it should not endow the agent with superhuman abilities to imagine and compare complex mathematical entities unlike anything she will ever encounter in reality. Furthermore, Occam’s Razor says that a scientific model should have the smallest possible number of components. But positing preferences over a menagerie of discontinuous functions would vastly multiply the number of components, yielding a less parsimonious and less attractive model of choice behaviour.====Finally, in some cases, the question of what is feasible or imaginable by an agent is irrelevant, because we are simply ==== a set of preference data over a restricted domain, and our SEU representation theorems must work with this limited dataset. For example, in empirical applications, we wish to construct an SEU representation from the revealed or elicited preferences of a human subject. If certain acts are unfeasible and certain events are unobservable, then we obviously cannot obtain revealed preference data concerning these acts and events. We might ==== such preferences through a questionnaire. But it is debatable whether such elicited preferences would really be credible (or even coherent).====Alternately, suppose we use an SEU representation theorem as a tool to prove a theorem about preferences over some other domain. In this case, the domain of preferences is determined by the hypotheses of the other theorem, not by the cognitive or perceptual capabilities of an agent. For example, Pivato (2020b) posits preferences on a vector space ==== of abstract “acts”, and obtains SEU representations with a purely subjective state space ====. The space ==== is either a Riesz space or a commutative Banach algebra, and ==== is a compact Hausdorff space obtained from the Kakutani or Gelfand representation theorems, with the elements of ==== corresponding to continuous real-valued functions on ====. Theorem 3 of the present paper is then invoked to obtain some of the desired SEU representations.====In some decision environments, our domain of continuous functions might be too small, because some ==== continuous functions are also feasible. For example, given a regular event ====, the agent might be able to implement a “====-contingent plan”, which agrees with one continuous function on ====, and a different continuous function on the complement of ====, but is discontinuous on the boundary of ====. Clearly, allowing such acts into ==== would remove some of the obstacles mentioned above, greatly simplifying our task. Thus, by restricting ==== to contain only continuous functions, we have actually solved a harder problem. Our axiomatic characterization extends to settings where ==== also contains piecewise continuous acts (Pivato and Vergopoulos, 2020a).====On the other hand, the domain of continuous functions and the algebra of regular events might still be ====. Continuous functions can still be far too complex to be physically realizable, or even easily imaginable (consider the Peano curve). Regular subsets can still be pathological and counterintuitive (Pivato and Vergopoulos, 2018 Proposition 6.9). Perhaps it is still unrealistic to posit preferences over arbitrary continuous acts, conditional on arbitrary regular events.====We forestall this objection, because our results apply even for much smaller domains of acts and events. We allow the set ==== of observable events to be any Boolean ==== of the algebra of regular events. Feasible acts in our model must be “measurable” with respect to ====, in addition to being continuous. Furthermore, we allow the set ==== of feasible acts to be a relatively small ==== of this set of measurable acts. This yields SEU representations even on rather small domains of acts and events (see Section 2.1 for examples).====The rest of this paper is organized as follows: Section 2 introduces our model of imperfect perception, and Section 3 introduces our model of decisions under uncertainty. Section 4 introduces the six axioms used in our results. Section 5 presents the SEU representation, in terms of a credence on a Boolean subalgebra ==== of regular sets, while Section 6 presents the Stonean SEU representation, which relies on a Borel probability measure on the Stone space of ====. Section 7 briefly discusses some of the axioms, and Section 8 reviews prior literature. All the proofs are in Appendix.",Subjective expected utility with imperfect perception,https://www.sciencedirect.com/science/article/pii/S0304406820300379,2 April 2020,2020,Research Article,218.0
Henry Chiu W.,"Economics, School of Social Sciences, The University of Manchester, Manchester, M13 9PL, UK","Received 17 October 2018, Revised 24 February 2020, Accepted 19 March 2020, Available online 2 April 2020, Version of Record 18 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.004,Cited by (5),This paper characterizes the stochastic deterioration resulting from taking a zero-mean financial risk in the presence of correlated non-financial background risk. We show in particular that it has an equivalent ==== as well as a necessary and sufficient “integral condition” that implies and is implied by a particular sense in which the stochastic deterioration can be decomposed into a “correlation increase” and a “marginal risk increase”. We further characterize a measure of aversion to the stochastic deterioration. These characterizations provide for a more general framework for formulating concepts of increases in risk and correlation and for better understanding risk management decisions governed by individuals’ attitudes to them.,"Economic decision making under uncertainty often takes place in settings where choices about endogenous risks must be made while simultaneously facing one or more exogenous background risks that are not under the control of the decision maker and can potentially be correlated with the endogenous risks. While considerable effort has been devoted to characterizing in the Expected Utility (EU) framework the conditions on the utility function under which changes in independent financial background risk have a definitive effect on risk aversion,==== ====
 McFadden (1974) argued that individuals tend not to view endogenous risk and background risk as independent or uncorrelated and it has been shown (by, among others, Doherty and Schlesinger (1983) and Tsetlin and Winkler (2005)) that if the endogenous risk is correlated with the uninsurable background risk, well-known results on optimal risk management decisions assuming uncorrelated background risk can be overturned.====The background risk in the presence of which economic decisions are made can moreover be non-financial as well as financial, one’s health status being an obvious and often-cited example.==== ==== The formal analytical framework for decisions in the presence of such background risk is the more general setting of multivariate distributions that encompasses those for multiplicative as well as additive financial background risks==== ==== as special cases. Epstein and Tanny (1980) show that extending analysis analogous to that of Rothschild and Stiglitz (1970) to a setting of bivariate distributions yields a particular sense in which a bivariate distribution exhibiting greater correlation than another and a utility function with a negative cross derivative is necessary and sufficient for aversion to such an increase in correlation. More recently, Muller and Scarsini (2012) show that a discrete multi-variate distribution is preferred to another by all expected utility maximizers whose utility functions have non-positive second and cross derivatives if and only if the former can be obtained from the latter by a finite sequence of “simple inframodular transfers”.==== ==== Notwithstanding these and other important contributions on multivariate stochastic dominance==== ==== as well as the existing results explicitly addressing financial risk taking in the presence of non-financial background risk,==== ==== little is known about the characteristics of the overall risk change induced by taking a financial risk in the presence of potentially correlated non-financial background risks. As a result, a decision maker’s attitudes toward such risk changes that determine her optimal economic decisions are not well-understood, still less the behavioral implications of the strength of such attitudes.====This paper defines and characterizes bivariate stochastic deteriorations, termed “correlation-increasing marginal risk increase” (CIMRI) and “correlation-decreasing marginal risk increase” (CDMRI). In the case of random vectors with finite support, a bivariate distribution is a CIMRI (CDMRI) of another if and only if the former can be obtained from the latter by a finite sequence of simple CIMRIs (CDMRIs), each of which can itself be decomposed into an “elementary correlation-increasing (-decreasing) transformation” as defined by Epstein and Tanny (1980) and a simple mean-preserving spread as defined by Rothschild and Stiglitz (1970). We show that a CIMRI (CDMRI) is what results from taking a zero-mean financial risk in the presence of a non-financial background risk that is correlated with the financial risk in the fairly weak sense of positive (negative) expectation dependence, as defined by Wright (1987), and is ==== stochastic deterioration disliked by all EU maximizers who are both correlation-averse (-loving) as defined by Epstein and Tanny (1980) and marginal risk averse, i.e., with a utility function concave in wealth. Furthermore, the stochastic deteriorations CIMRI and CDMRI each have a necessary and sufficient “integral condition” that is shown to imply and be implied by a particular sense in which a CIMRI (CDMRI) can be decomposed into a correlation increase (decrease) and a marginal risk increase. Together with a comprehensive characterization of attitudes toward financial risk-taking in the presence of a correlated non-financial background risk that we also offer, these results in turn provides an intuitive explanation for, as well as a significant generalization of, existing results on optimal choice of endogenous risk in the presence of financial and non-financial background risks. We further characterize a measure of aversion to CIMRI (CDMRI) and the concept of decreasing aversion to CIMRI (CDMRI) and demonstrate their behavioral implications in the context of risk management decisions. In short, the definition and characterization of CIMRI and CDMRI provide for a more general framework for formulating concepts of increases in risk and correlation and for better understanding risk management decisions governed by individuals’ attitudes to them.====The rest of the paper is organized as follows. Section 2 defines and characterizes the notions of CIMRI and CDMRI. Section 3 characterizes attitudes toward financial risk-taking in the presence of a correlated non-financial background risk and their implications for optimal decisions. Section 4 characterizes a measure of aversion to CIMRI (CDMRI) and the concept of decreasing aversion to CIMRI (CDMRI). Section 5 concludes the paper.",Financial risk taking in the presence of correlated non-financial background risk,https://www.sciencedirect.com/science/article/pii/S0304406820300355,2 April 2020,2020,Research Article,219.0
"Ando Sakai,Matsumura Misaki","International Monetary Fund, United States of America,Department of Economics, Columbia University in the City of New York, United States of America","Received 4 August 2019, Revised 24 January 2020, Accepted 20 March 2020, Available online 2 April 2020, Version of Record 9 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.005,Cited by (2),"We study the constrained efficiency of a competitive entrepreneurship model that features the occupation choice between entrepreneurs and workers. It is shown that, even when (1) the only friction is uninsurable entrepreneurial risks and (2) agents are risk-averse, the competitive market can generate too many entrepreneurs. We present a ==== that determines the constrained inefficiency and its direction (whether market generates too many entrepreneurs or too few) by exploiting the unique feature of the model where the equilibrium is characterized by an indifference condition instead of a marginal condition. The framework is also pedagogically useful to understand constrained efficiency analysis at intuitive level.","Competitive entrepreneurship models that feature the occupation choice between entrepreneurs and workers have been used extensively in the literature. Lucas, 1978, Kanbur, 1979a, Kanbur, 1979b, Kanbur, 1982, Kihlstrom and Laffont, 1979, Gine and Townsend, 2004, Cagetti and De Nardi, 2006, Cagetti and De Nardi, 2009, Vereshchagina and Hopenhayn, 2009, Buera et al., 2011, Buera and Shin, 2013 and Garicano et al. (2016) Despite the popularity, its constrained efficiency has not been analyzed systematically. In this paper, we characterize the constrained efficiency and present a sufficient statistic that determines constrained inefficiency and its direction.====The key feature of the competitive entrepreneurship models is the individual occupation choice. To present the essence of the argument, the baseline model is kept parsimonious and divided into two stages. In the first stage, ex-ante identical agents choose to become either entrepreneurs or workers. Becoming entrepreneurs incurs entrepreneurial risks in the sense that (1) agents are not sure about their entrepreneurial productivity but sure about wage as of occupation choice and (2) there is no insurance market for such risks. In the second stage, uncertainty resolves and entrepreneurs with heterogeneous productivity hire the profit-maximizing number of workers. In equilibrium, the wage clears the labor market. Mathematically, the occupation choice is a binary decision, so the equilibrium condition is an indifference condition instead of a marginal condition.====The efficiency of the occupation choice is studied by defining a constrained planner who can intervene in the first stage but cannot in the second stage. Specifically, the constrained planner maximizes the ex-ante utility by choosing the number of entrepreneurs subject to the same second stage equilibrium conditions as the market equilibrium. In this setup, we study the constrained efficiency of competitive entrepreneurship by comparing the number of entrepreneurs chosen by the planner and the market. The main results of the paper are two folds. The first result is that the competitive market can generate ==== entrepreneurs even when (1) the only friction is uninsurable entrepreneurial risks and (2) agents are risk-averse. For completeness, we also characterize a class of economic fundamentals with which the competitive market generates ==== entrepreneurs if and only if there are uninsurable entrepreneurial risks. The second result is that the intuition of constrained inefficiency and its direction can be reduced to a single sufficient statistic. Specifically, we show that the marginal risk premium of becoming an entrepreneur evaluated at the market equilibrium wage is a sufficient statistic to determine whether the market generates the efficient number of entrepreneurs or not, and if not, whether too many or too few.====The intuition of constrained inefficiency can be made transparent by highlighting the indifference condition. Specifically, at the market equilibrium, the wage makes sure that the certainty equivalents of the two occupations are equated, which implies that the marginal utilities of the certainty equivalents are also equated. Thus, whether the planner’s marginal intervention improves welfare or not can be reduced to the net gain in the certainty equivalents. Now, the planner’s intervention that changes the number of entrepreneurs affects the wage through labor market clearing, and therefore, triggers redistribution of certainty equivalents between occupations. When there are no entrepreneurial risks, certainty equivalents and consumption are the same, so the marginal redistribution does not generate a net gain. However, when there are entrepreneurial risks, the marginal redistribution through wage between the two jobs with different riskiness can have different impacts on the certainty equivalents of the two groups. When they differ, the planner can find a welfare-improving intervention, and thus, the market equilibrium is constrained inefficient.====For the direction of the constrained inefficiency, what matters is the sign of the net gain in the certainty equivalents. We show that the sign is determined by the relative importance of (1) the riskiness of the income and (2) the risk attitude of the agents. Intuitively, the market generates too many entrepreneurs if the risk aversion of entrepreneurs decreases sharply as their income increases. In this situation, if the number of entrepreneurs decreases and workers increases, the additional labor supply suppresses wage so the income of entrepreneurs increases. The resulting reduction of risk premium leads to higher ex-ante welfare.====Our paper belongs to the literature of the constrained efficiency analysis pioneered by Diamond (1967). Our result shares the same flavor as the generic constrained inefficiency of incomplete markets. Geanakoplos and Polemarchakis, 1986, Greenwald and Stiglitz, 1986 and Geanakoplos et al. (1990) However, the model we study is not covered in their framework and we exploit the specific model structure of occupation choice to characterize not only constrained inefficiency but also its direction and a sufficient statistic behind them. In this sense, our paper belongs to the group of papers that study the constrained efficiency of a particular model by leveraging the model specific structures, such as Farhi et al., 2009, Davila et al., 2012, Toda, 2015 and Gottardi et al. (2016) that study the constrained efficiency of Diamond and Dybvig, 1983, Aiyagari, 1994 and Krebs (2003). Compared to these papers, our paper is unique in its pedagogical value since the indifference condition makes the logic of constrained efficiency analysis more transparent than usual marginal conditions. The occupation choice model itself goes back to Lucas, 1978, Kanbur, 1979b, Kanbur, 1979a and Kanbur (1982). The difference from Kanbur (1981) and Kihlstrom and Laffont (1979) is that they study the inefficiency of the combination of both entrepreneurial risks and ex-ante labor decision, while we disentangle the implication specific to entrepreneurial risks.",Constrained inefficiency of competitive entrepreneurship,https://www.sciencedirect.com/science/article/pii/S0304406820300367,2 April 2020,2020,Research Article,220.0
"Yengin Duygu,Chun Youngsub","School of Economics, University of Adelaide, Adelaide, SA, 5000, Australia,Department of Economics, Seoul National University, Seoul 151-746, Korea","Received 2 December 2019, Accepted 15 March 2020, Available online 26 March 2020, Version of Record 4 April 2020.",https://doi.org/10.1016/j.jmateco.2020.03.003,Cited by (2),"Given a group of agents, the queueing problem is concerned with finding the order to serve agents and the monetary transfers they should receive. In this paper, we characterize interesting subfamilies of the VCG mechanisms by investigating the implications of either ==== or ==== requirements. First, we present a characterization of the ====and ====mechanisms. Next, we present characterizations of VCG mechanisms satisfying one of two different formulations of ==== or ====. Finally, we show that among the ==== and ==== mechanisms, the only ones that satisfy one of two formulations of ==== or ==== are extensions of the pivotal or the reward-based pivotal mechanisms.","A queue forms when jobs need to be processed one at a time (or resources are distributed sequentially). For instance, agents need to receive a service or a good, one agent at a time, or agents have to perform some task taking turns (e.g., researchers want to use a single supercomputer owned by a department or academic staff fulfills an administrative role in turns). Queueing problems encompass not only physical queues, but also waiting lists obtained when people apply for private mooring license, public housing, medical treatment/elective surgery, child care, private schools, etc.====The queueing problem is concerned with designing mechanisms that determine the queue position of each agent and the monetary transfer each agent should receive. We assume that serving each agent takes the same amount of time. Each agent incurs a waiting cost which is constant per unit of time, but agents may differ in their (unit) waiting costs.==== ==== The utility of an agent is equal to her monetary transfer minus her total waiting cost. This queueing problem has been analyzed by taking many different approaches (see Chun, 2016 for a survey).====In many queueing problems, waiting costs are agents’ private information and all agents are equally entitled for the service. Hence, incentive compatibility and fairness are central concerns. Perhaps the foremost requirements for incentive compatibility and fairness are ==== and ====, respectively.==== ====Even though ==== has been the object of numerous studies in many contexts, its analysis in the queueing problem has been limited. Chun (2006b) presents a condition that checks whether a mechanism is ====. This condition shows that the difference of the transfers of two consecutive agents in an efficient==== ==== queue must be bounded above and below by the waiting costs of the two agents. Since an agent’s waiting cost is her private information, truthful revelation of the waiting cost is essential in order to determine ==== allocations. Kayi and Ramaekers (2010) showed that the ==== (introduced by Suijs, 1996 and Mitra, 2001) are the only ==== and ==== (total transfer is zero) mechanisms. However, these mechanisms do not satisfy other desirable properties such as cost monotonicity and population monotonicity (see Section 5) or ====-welfare bound (including the identical-preferences lower bound) (see Chun and Yengin, 2017). If budget-balance is relaxed, what other interesting ==== and ==== mechanisms emerge? The characterization of the entire class of ==== and ==== mechanisms has not been addressed in the literature.====Our first contribution is to introduce and characterize a new parameterized class of mechanisms with ==== and ====. The characterization completely specifies the transfer amounts in each problem. Note that in the queueing problem, ==== implies ====. Hence, by ==== and ==== the class of ==== VCG mechanisms is characterized.==== ==== This class of VCG mechanisms is a new class which has not been the object of any study so far. It includes several prominent mechanisms in the queueing literature (i.e., the pivotal, the reward-based pivotal, the ==== -pivotal, and the symmetrically balanced VCG mechanisms).====Our second contribution involves solidarity requirements (see Thomson, 2013 for a survey). We investigate how the VCG mechanisms respond to changes in a given parameter of the problem, either the waiting cost or the set of agents. We require that upon the changes in the parameter, all relevant agents should be affected in the same direction, all weakly lose or all weakly gain. First, suppose the waiting cost of some agent increases. The aggregate waiting cost of an efficient queue in the new cost profile is at least as much as the initial one. This is bad news for the society. ==== (Maniquet, 2003) requires that no agent should be better off from the bad news. Alternatively, one may claim that each agent is responsible for a change in her own waiting cost, not any other agents. ==== (Chun, 2006a) requires that the other agents should not be negatively affected.====Second, suppose that new agents enter the queueing problem and a new queue has to be formed for the enlarged population. The aggregate waiting cost of an efficient queue in the enlarged population is at least as large as the initial one, which is bad news for the initial society. ==== (Thomson, 1983b, Thomson, 1983a) requires that no initial agent should be better off after the arrival of new agents. Alternatively, one may require new agents to bear the burden of the increase in the aggregate waiting cost of the society. ==== requires that the initial agents should not be worse off after the arrival of new agents.====We characterize the four subclasses of VCG mechanisms satisfying one of two different formulations of either ==== or ====. Next, we show that if budget-balance is not enforced, then it is possible to obtain ==== and ==== mechanisms that satisfy solidarity notions. In fact, we characterize two well-known mechanisms: the extended==== ==== pivotal mechanisms and the extended reward-based pivotal mechanisms. The pivotal and the reward-based pivotal mechanisms have been characterized with several different fairness and strategic axioms (Mitra and Mutuswami, 2011, Chun et al., 2014a). Hence, our characterizations here reinforce the importance of these mechanisms in the queueing problem. Our characterizations provide an alternative motivation for thesemechanisms based on ==== and either ==== or ==== Such motivation does not exist in the more general model of allocating indivisible goods which makes our results even more interesting.====The paper is organized as follows. Section 2 introduces the model. Section 3 introduces the class of VCG mechanisms. In Section 4, we characterize the class of ==== and ==== mechanisms. Section 5 investigates the joint implications of ==== and ==== together with one of two formulations of either ==== or ====. By combining the results in Sections 4 No-envy and strategy-proofness, 5 Solidarity and strategy-proofness, we obtain the characterizations of the pivotal and the reward-based pivotal mechanisms. Concluding remarks follow in Section 6. All proofs are given in Appendix.","No-envy, solidarity, and strategy-proofness in the queueing problem",https://www.sciencedirect.com/science/article/pii/S0304406820300343,26 March 2020,2020,Research Article,221.0
"Carlier Guillaume,Zhang Kelvin Shuangjian","CEREMADE, UMR CNRS 7534, PSL, Université Paris IX Dauphine, Pl. de Lattre de Tassigny, 75775 Paris Cedex 16, France,INRIA-Paris, MOKAPLAN, France,Center for Research in Economics and Statistics, ENSAE Paris-Tech, Campus Paris-Saclay, 5 Avenue Henry Le Chatelier, 91120 Palaiseau, France","Received 15 February 2019, Revised 5 March 2020, Accepted 11 March 2020, Available online 21 March 2020, Version of Record 31 March 2020.",https://doi.org/10.1016/j.jmateco.2020.03.002,Cited by (0),"We prove an existence result for the principal–agent problem with adverse selection under general assumptions on preferences and allocation spaces. Instead of assuming that the allocation space is finite-dimensional or compact, we consider a more general coercivity condition which takes into account the principal’s cost and the agents’ preferences. Our existence proof is simple and flexible enough to adapt to partial participation models as well as to the case of type-dependent budget constraints.","The principal–agent problem with adverse selection plays a distinguished role in modern microeconomic theory and has attracted a considerable amount of attention due to its numerous economic applications such as nonlinear pricing (Mussa and Rosen, 1978, Armstrong, 1996, Rochet and Choné, 1998), taxation theory (Mirrlees, 1976), regulation (Laffont and Tirole, 1993), to name just a few. In these problems, the principal cannot observe agents’ types; hence her profit maximization program is constrained by incentive compatibility. This leads to variational problems subject to global constraints which are difficult to solve in general. The goal of the present paper is to present a rather elementary approach to the existence of optimal contracts in adverse selection problems under minimal assumptions.====To fix ideas, let us consider a standard monopoly optimal nonlinear pricing problem. Denoting by ==== the set of products that are technically feasible for the monopolist and by ==== a certain range of prices, agents’ preferences are given by the utility ==== which depends on their (unobservable) type ====, the product ==== and the price ====. The monopolist knows the distribution of types ==== and has a cost function denoted by ====. Her problem then consists of maximizing her total profit ====among contracts ====, which are incentive compatible, i.e., ====and individually rational, i.e., satisfying the participation constraint ====where ==== is a certain outside option contract available to the agents. The multidimensional case ==== is considerably harder than the unidimensional case. Indeed, when ====, the standard single-crossing condition enables one to use specific arguments based either on optimal control (as in Guesnerie and Laffont, 1984) or monotonicity (as in Mussa and Rosen, 1978 or Jullien, 2000).====In higher dimensions, if ==== and ==== are compact, the existence of an optimal contract is well-known under general assumptions on the preferences and the cost. It follows for instance from the general results of Monteiro and Page (1998) (also see Carlier (2001) for the quasilinear case) or Ghisi and Gobbino (2005) who developed an elegant and original Gamma-convergence approach. More recently, Nöldeke and Samuelson (2018), McCann and Zhang (2019), and Zhang (2019) have established general existence results emphasizing the role of duality and generalized convexity. Nöldeke and Samuelson provide a general existence result assuming that the type and product spaces are compact, by a duality argument based on Galois connections. McCann and Zhang not only show a general existence result assuming a single-crossing type condition and boundedness of the agent-type and product-type spaces, but also generalize uniqueness and convexity results of Figalli et al. (2011) to the non-quasilinear case. In the vein of Carlier (2001), Zhang (2019) proves a general existence result using generalized convex analysis, under weaker assumptions on the product domain and without assuming the generalized single-crossing condition from McAfee and McMillan (1988).====Why should we bother with yet another existence result, then? Firstly, compactness of ==== and/or ==== is a severe restriction which rules out many important examples. In particular, upper bounds on prices should come as a result of the model rather than as an assumption. Secondly, for an optimization problem to have solutions, compactness of the admissible set can be replaced by a weaker assumption, which takes advantage of properties of both the objective and the constraints. For instance, in (1.1), ==== cannot be too negative and ==== cannot be too small because of the participation constraint. This gives extra restrictions on contracts; our assumption is that these restrictions are enough to force approximate optimizers to remain in a compact set.====Consider the monopoly pricing problem above in the extreme case where ==== is a singleton (so that the adverse selection problem disappears) and ====. Then the optimal contract simply corresponds to setting ==== and finding ==== by maximizing ====. The existence of such an optimal ==== is obvious if ==== is compact and ==== is lower semicontinuous. However, compactness of ==== can be replaced by an assumption, called coercivity, which requires compactness of the smaller set ==== (which is automatically bounded if ==== is superlinear for instance). This elementary example also shows that coercivity is indeed a minimal assumption.==== ==== This also strongly suggests that the natural condition for the existence of optimal contracts is the relative compactness of the set of ==== for which ==== and ==== for at least one type ====, rather than the compactness of ==== and ====. This is precisely the coercivity condition that we will consider (see (2.7)–(2.8)) and under which we will prove existence of an optimal contract. Another (more technical at first glance) advantage of our approach is that, contrary to the references listed above, the contracts we consider belong to a general Polish space, which can be infinite-dimensional (functions of time or random variables for instance).====Our proof involves two steps. The first step consists of showing that any feasible contract can be improved by another one which yields a larger benefit than the outside option to the principal for every type of agents. This key observation is not new, it appears in Monteiro and Page (1998) and Carlier (2001), but these authors did not take advantage of it to get rid of their compactness assumptions. Thanks to our coercivity assumption, the improved contracts remain in a compact set and the existence proof can be carried out along the same lines as, for instance, Monteiro and Page (1998). We then extend our existence result in two directions. The first extension considers the case where the outside option is not necessarily feasible for the principle which leads to partial participation models as in Jullien (2000). Our second extension concerns agents facing a type-dependent budget constraint as in the works of Monteiro and Page (1998) and Che and Gale (2000). Type-dependent budget constraints introduce possible discontinuities in preferences. While Monteiro and Page showed that the resulting difficulties may be overcome by a certain nonessentiality assumption, we will follow a slightly different route showing that a non-atomicity condition can be used instead.====The paper is organized as follows. Section 2 presents the basic model and main assumptions. In Section 3, we establish an existence result for the basic model. Section 4 shows how to extend the existence proof to models with partial participation, as in Jullien (2000). In Section 5, we generalize the analysis to the case of type-dependent budget constraints for the agents, as in Monteiro and Page (1998) and Che and Gale (2000). Finally, we have gathered in the Appendix several simple measurable selection results used throughout the paper.",Existence of solutions to principal–agent problems with adverse selection under minimal assumptions,https://www.sciencedirect.com/science/article/pii/S0304406820300331,21 March 2020,2020,Research Article,222.0
"Chadha Karan N.,Kulkarni Ankur A.","Department of Electrical Engineering, Stanford University, United States of America,Systems and Control Engineering, Indian Institute of Technology Bombay, India","Received 10 June 2019, Revised 26 February 2020, Accepted 29 February 2020, Available online 19 March 2020, Version of Record 31 March 2020.",https://doi.org/10.1016/j.jmateco.2020.02.006,Cited by (1)," networks, close approximations to the maximum social welfare and the maximum aggregate play in terms of only network characteristics such as the maximum degree and independence number. For the special case when the underlying network is a tree, we derive formulas which use only the number of nodes and their degrees. These results allow a system planner to assess aggregate outcomes and design interventions for the game, directly from the underlying graph structure, without enumerating all equilibria of the game, thereby significantly simplifying the planner’s problem. A part of our results can be viewed as a logical extension of Pandit and Kulkarni (2018) where the maximum weighted aggregate effort of the model in Bramoullé and Kranton (2007) was characterized as the weighted independence number of the graph.","In many settings of competition, the actions of only a subset of all agents directly affect the utility of an agent. The underlying structure of these interactions can be captured using a network or graph. The study of the resulting ==== provides insights into how interconnections, such as physical connections or economic and social interdependencies, affect the outcomes of interactions. These models have been applied for understanding diverse contexts such as crime, segregation, investment, research cooperation and so on.====In this paper, we consider noncooperative games of positive externalities where players are situated on nodes of a network whose edges represent preexisting interconnections throughwhich the actions of the neighbors of an agent influence the latter’s utility. Beginning with the work of Bramoullé and Kranton (2007) on the provision of local public goods in the presence of networks, and extending further to Bramoullé et al. (2014), many beautiful results have been found relating the nature of equilibria in such games to network structure. For instance, in the particular case of a public goods game on a network, assuming perfect substitutability, Bramoullé and Kranton showed that certain classes of Nash equilibria (Nash, 1950), called ====, are in one-to-one correspondence with maximal independent sets in the network. These were also shown to be the only ‘stable’ equilibria (Bramoullé and Kranton, 2007). Recently, Pandit and Kulkarni (2018) showed an even stronger result – that the maximum total effort over all equilibria of such a game was exactly the size of the largest independent set in the network, and they identified independent sets corresponding to ‘influential’ and ‘frugal’ equilibria that well approximated the maximum welfare in such games.====A common occurrence in such games, noticed by Bramoullé and Kranton (2007), is the vast multiplicity of equilibria. The number of specialized equilibria is itself exponential in the number of agents or nodes in the network. In addition to specialized equilibria, Bramoullé and Kranton (2007) identify ==== and ==== equilibria that are also prevalent in such games, further increasing the number of equilibria. Yet another recurring phenomenon is the dramatic change in the nature equilibria with small changes in the network structure. Addition of edges, for instance, affects the incentives of the agents that are joined by the edge, but this effect propagates through the network, resulting in more far-ranging effects whose total effect is not easy to ascertain. Our motivation in this paper is that these characteristics make it challenging for a principal or a system planner to ascertain outcomes of this interaction and significantly hampers the planner’s ability to design interventions that could alter outcomes. For instance, if the actions of the players are investment levels, a system planner may be interested in knowing the total aggregate investment that can result from such interactions, and may look to design interventions that improve the aggregate investment. If the actions of the players are the propensity for crime, a planner may be interested in devising structural changes to the network to reduce the total crime levels (Ballester et al., 2006). Bramoullé et al. observed in Bramoullé et al. (2014) that the minimum eigenvalue of the graph is a key characteristic that tells us when the equilibrium would be unique. However, despite this result, not much is understood about aggregate play in such interactions. Our central contributions in this paper are theorems that express maximum aggregate play and welfare in terms of graph characteristics. This allows maximum aggregate play and welfare (over all equilibria) to be assessed only from the graph structure, ==== enumeration of all equilibria, thereby significantly reducing the complexity of the planner’s problem.====We characterize the maximum aggregate play in strategic interactions on networks within a small band for the model discussed in Bramoullé et al. (2014). In this model, players are situated on nodes of a network and choose actions to maximize their benefit less the cost of their effort. The benefit a player derives is a function of the sum of its own effort and the total of the effort of his neighbors, where the latter is discounted by a substitutability factor ====. Thus, unit effort by a neighbor ==== of a player ==== is substitutable by ‘====’ units of effort by player ====, as far as the benefit of player ==== is concerned. Such a model applies to studying how firms involved in similar kinds of research choose the amount of investment and effort when the interaction structure is given by a network.====We view the case of perfect substitutability (====) as the base case and ==== as a small perturbation from this case. We show that for ==== greater than an explicit constant ==== that depends only on the underlying graph ====, the maximum aggregate play over all equilibria is within ==== and ====. Here ==== is the size of the largest independent set of the graph ==== (called ====), and ==== is the level of action at which marginal benefit equals marginal cost. We also identify the structure of the aggregate play maximizing equilibrium. In the special case when a unique maximum independent set exists in the underlying network, we establish the exact value of maximum aggregate play; indeed it is ====. We also obtain similar bounds on the maximum welfare attained by any equilibrium. Notably, these results apply to ==== network ====. When the underlying network is a tree (a graph without cycles), we obtain explicit formulas for the maximum aggregate play and welfare irrespective of the requirement that ====. Thus, using our results the planner need only calculate certain underlying graph parameters, such as the size of the maximum independent, size of the largest clique and know the degree distribution of the nodes, and these suffice to estimate maximum aggregate play and maximum welfare.====These results also show that alterations to the graph structure that do not change these graph parameters have limited effect on the maximum aggregate play and welfare. In particular, notice that deletion of nodes does not increase the independence number, and hence, is not likely to have a significant effect on the maximum aggregate play. However, deletion of edges can increase the independence number, making deletion of edges the more effective strategy at improving aggregate play. To be fair, there are other strategies that the planner can employ to channelize the behavior of players in the network such as imposing taxes or subsidies. In such a case, the mathematical analysis involving all equilibria becomes more complicated, and it is hard to give any concrete results about the effect of such interventions. We hope to explore these in future works.====Part of this paper can be viewed as a logical generalization of previous results in Pandit and Kulkarni (2018) which consider the case ====, and indeed for ====, our results reduce to those in Pandit and Kulkarni (2018). However, this generalization is significantly harder to accomplish mathematically. When ==== the nature of equilibria is drastically different from that when ====. The source of the hardness is that when ====, no canonical classes of equilibria (like specialized equilibria) are known. In particular, maximal independent sets do not, in general, correspond to equilibria of the game. We address this challenge by noticing that equilibria of this game correspond to solutions of a suitably defined ==== (LCP) (Cottle et al., 1992). In our recent work (Chadha and Kulkarni, 2018), we characterized ==== norm maximizing solutions of this linear complementarity problem and identified the existence of ==== ==== that support such solutions. Independent cliques are a set of cliques in the graph such that no two nodes from two distinct cliques are adjacent. They can be thought of as a generalization of independent sets and reduce to the latter when the said cliques are single vertices. We show in Chadha and Kulkarni (2018) via a constructive algorithm that such cliques that support an LCP solution necessarily exist in any graph. When translated to the game in consideration here, these correspond to Nash equilibria where agents that make a non-zero effort form a set of independent cliques and in each such clique, all agents have identical actions. Such equilibria, that we call ====, maximize aggregate play. When ====, these cliques become degenerate singletons and we recover the result of Pandit and Kulkarni (2018). The requirement of ==== is inherited from Chadha and Kulkarni (2018) and is necessary for the existence of such independent cliques.====For the case when the underlying graph is a tree, we provide bounds on the aggregate play and welfare which hold for a larger range of ==== and depend only on the number of nodes and their degree. We first provide such bounds for the case when the underlying graph is a line network. A ==== is a network whose nodes can be listed as ==== such that the links are ==== for ====. Then we consider ==== trees which are defined as trees with exactly one node of degree strictly greater than 2. Using these as the building blocks, we give tight bounds on maximum aggregate play and welfare for general trees.",Aggregate play and welfare in strategic interactions on networks,https://www.sciencedirect.com/science/article/pii/S0304406820300318,19 March 2020,2020,Research Article,223.0
"Pivato Marcus,Soh Arnold","THEMA, Université de Cergy-Pontoise, France","Received 13 November 2019, Accepted 3 March 2020, Available online 18 March 2020, Version of Record 25 March 2020.",https://doi.org/10.1016/j.jmateco.2020.03.001,Cited by (9),"We propose a new system of democratic representation. Any voter can choose any legislator as her representative; thus, different legislators can represent different numbers of voters. Decisions in the legislature are made by weighted majority voting, where the weight of each legislator is determined by the number of voters she represents. We show that, if the size of the electorate is very large, then with very high ====, the decisions obtained in the legislature agree with those which would have been reached by a popular referendum decided by simple majority vote.","Modern democratic governance almost always takes the form of ==== democracy — for a very good reason. Many policy problems are extremely complex; considerable effort is needed to understand their intricacies. Most voters do not have the time, the interest, or the educational background to make such an investment. Even if they did, they lack the incentive; as Downs (1957) argued, in a large electorate, a rational voter will not invest effort to cast a well-informed vote, because there is only a tiny chance that her vote will be pivotal. Deciding complex and technical policy questions through popular referenda engenders fiascos.====There are two broad families of representation systems: ==== and ====.==== ==== Both have major problems. In regional representation systems, the polity is divided into geographic districts; each district elects one or more representatives, typically via plurality vote or some variant. The chief advantage is that each voter can identify a specific member of legislature who is ==== representative, responsible for defending ==== interests. But the system has five major disadvantages. First, delimiting the electoral districts themselves (e.g. to prevent gerrymandering) is a nontrivial problem  (Ricca et al., 2017). Second, precisely because each member of legislature is beholden to a particular district, her policy choices are often parochial and myopic, and the resulting legislation is often socially inefficient due to logrolling and pork-barrel pathologies. Third, a legislator cannot credibly claim to represent voters who did not vote for her — and with the plurality rule, this is often the ==== of voters in her district. Fourth, and relatedly, the ideological distribution in the legislature often hugely misrepresents the ideological distribution in the electorate; typically, mainstream political views are wildly over-represented, while minority political views are under-represented or suppressed altogether. The structural disadvantage of a minor party is further magnified by the fact that even voters who are sympathetic to that party’s views will wish to not “waste their votes”, and so will vote for one of the larger parties which at least has some chance of winning. This leads to the fifth problem: smaller parties are inexorably squeezed out of the political system, leading to ideological homogeneity or political polarization (Duverger, 1959, Bol et al., 2019).====In proportional representation systems, voters vote directly for parties, each of which controls a proportion of the legislature roughly commensurate to its share of the popular vote. This eliminates the problems of regional representation described just above, but replaces it with five other problems. First, in contrast to regional representation, a voter can no longer identify a specific legislator as “her” representative. Second, the interests of geographic regions receive no representation in the legislature, creating the risk that peripheral, low-population regions are neglected or exploited. Third, to give each party a fraction of legislative seats proportional to its share of the popular vote, we must use an ====, and no apportionment rule is completely satisfactory (Balinski and Young, 2010, Kóczy et al., 2017, Ricca et al., 2017). Fourth, as noted by Ostrogorski (1902) and Rae and Daudt (1976), mediating legislative decisions through political parties can frustrate the will of the majority of voters; see also Besley et al. (2008). This leads us to the fifth and most important problem: proportional systems depend essentially on ====, and place them at heart of political process. Legislators may have more loyalty to their party than to the electorate (especially in “closed list” systems). Policy decisions may be determined by party affiliation rather than the strength of arguments. Although there is typically more ideological heterogeneity amongst parties in proportional systems than in regional systems, this heterogeneity is still limited. Voters whose views do not fit the platform of any of the major parties may become dangerously alienated.====Regional representation can be made to better approximate proportionality by using multimember districts — but this again requires the use of an apportionment rule, with all the problems that entails. Some hybrid systems combine elements of regional and proportional representation. For example, the ==== (MMP) system used in Germany, Japan, and New Zealand uses regional representation, but extra members are added to the legislature (drawn from party lists) to make the political distribution of the legislature roughly match that of the electorate. But the resulting legislatures can be unwieldy in size. Also, like all proportional representation systems, MMP depends on the existence of political parties.====Perhaps partly because of these problems, representative democratic institutions have recently suffered an erosion of legitimacy. Growing dissatisfaction with mainstream parties has led voters to flirt with populists, extremists, and rule by referenda. The inadequacy of plurality-based (“first-past-the-post”) regional representation has led to multiple (failed) attempts to replace it in Canada and Great Britain. Much recent work in social choice theory has focused on rules for electing committees to accurately represent the electorate (Skowron et al., 2016, Elkind et al., 2017, Faliszewski et al., 2017, Brill et al., 2018).====Dissatisfaction with conventional representative democratic institutions has also motivated recent ambitious proposals for radically different representation systems, based on proxy voting (Alger, 2006, Green-Armytage, 2015, Cohensius et al., 2017) and ==== (Paulin, 2014, Blum and Zuber, 2016; Christoff and Grossi, 2017a, Christoff and Grossi, 2017b, Brill and Talmon, 2018, Kahng et al., 2018). In these systems, the weight of each legislator is directly proportional to the number of voters she represents — an idea which can be traced back to early proposals by Sterne (1871, p.62), Tullock (1967, Ch.10 and 1992), Miller (1969) and Chamberlin and Courant (1983), and which has been recently proposed by Laslier (2017) and Abramowitz and Mattei (2019). Our proposal is similar, but with one key difference: the weight of a legislator is determined not only by the ==== of voters she represents, but also by a weighting factor that measures how effectively she represents them.====Legislatures in federal or multinational polities (such as the Council of the European Union) often combine regional representation with a weighted voting system: different legislators represent regions with very different populations, and receive concomitantly different weights. Typically, these weights grow sub-linearly with population size (this is called “degressive proportionality”). Several interesting recent papers identify the weighting formulae that optimize certain performance measures (such as expected total utility) assuming certain statistical distributions of voter preferences (see e.g. Barberà and Jackson (2006), Koriyama et al. (2013) and Macé and Treibich (2018)). Our proposal is similar, but with three key differences. First, in our system, legislators do not necessarily represent ====; they represent groups of voters united by ideological similarity rather than geographical proximity. Second, and relatedly, the weight of a representative in our system grows roughly linearly with the number of voters she represents. Finally, we do not measure the performance of the legislature in terms of expected total utility; instead, like Coffman (2016), we measure its performance by comparing the decisions made by the legislature will agree with those that ==== be made by a hypothetical direct democracy.====The rest of this article is organized as follows. Section 2 describes the new voting rule, which we call ====. Section 3 introduces the mathematical model we will use to analyse this system, and contains our first main result (Theorem 1), in the context of binary decisions. Section 4 extends this analysis to Arrovian preference aggregation, and Section 5 extends it to judgement aggregation. Section 6 discusses methods for calibrating the weighting factor for each representative. All proofs are in Appendix.",Weighted representative democracy,https://www.sciencedirect.com/science/article/pii/S030440682030032X,18 March 2020,2020,Research Article,224.0
"Ehlers Lars,Majumdar Dipjyoti,Mishra Debasis,Sen Arunava","Université de Montréal, Canada,Concordia University, Canada,Indian Statistical Institute, Delhi, India","Received 18 April 2018, Revised 15 September 2019, Accepted 27 February 2020, Available online 5 March 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jmateco.2020.02.004,Cited by (6),"In models without transfers, we show that every cardinal incentive compatible voting mechanism satisfying a continuity condition, can only take ====, but not ==== information into account. Our results apply to many standard models in mechanism design without transfers, including the standard voting models with ==== domain restrictions.","Many important models in mechanism design preclude the use of monetary transfers. This may be due to ethical (school choice problems, kidney exchange problems) or institutional (voting) reasons. A typical model in such an environment consists of a set of alternatives and ordinal preferences (strict linear orders) of agents over these alternatives. A mechanism or a social choice function (scf) selects a lottery over the set of alternatives. A notable feature of these models is that they only use ordinal information about preferences over alternatives. In this paper, we ask if there is any loss of generality in restricting attention to ordinal mechanisms in such environments.====Casual reasoning suggests that a mechanism that uses “a lot of” information regarding agents’ preferences is more susceptible to manipulation by agents. Two well-known results in this literature are illustrations of this informal principle. If all preferences are admissible, the Gibbard–Satterthwaite Theorem (Gibbard, 1973, Satterthwaite, 1975) states that the only information that an incentive compatible mechanism can use, is the preference information of a ==== (dictator) agent. If preferences are single-peaked, only ==== of agent preferences can be used in computing the social outcome at a profile (Moulin, 1983). Our results accord with this principle. Although cardinal information is available, we show that incentive compatibility in (private values) voting models implies that most of it cannot be used by the designer.====If the solution concept is dominant strategy and the mechanism is deterministic (i.e., lotteries produced by the scf are degenerate), then incentive compatibility immediately implies that the scf cannot be sensitive to cardinal information.==== ==== However, the answer to this question is not obvious if either (a) we allow for randomization and/or (b) we use the weaker solution concept of Bayesian incentive compatibility.==== ==== The central issue in such cases is that an agent has to evaluate lotteries based on cardinal utilities over alternatives. We follow the standard practice of evaluating lotteries using the expected utility criterion. Consequently, the evaluation of lotteries uses cardinal preference information of agents. Moreover, it implicitly imposes a domain restriction on agent preferences over the outcomes of the mechanism (i.e. lotteries). It seems natural therefore to believe that this cardinal preference information can be used by the mechanism designer to determine the value of the scf at various profiles.====Our main results (Theorem 1, Theorem 2, Theorem 3) show this to be false. We establish that every dominant strategy incentive compatible, cardinal and random scf that satisfies some version of continuity, is ordinal. We discuss two notions of ordinality and, correspondingly, two notions of continuity. To understand them, note that the ==== of an agent in our cardinal model is a utility vector over alternatives and agents use expected utility to evaluate lotteries. Now, consider an agent ==== and fix the utility vectors of other agents at ====.====Our first notion of ordinality requires that we treat two utility vectors representing the same ordinal ranking over ==== in the same manner. In other words, we say that an scf is ==== if ==== and ==== are two utility vectors which are affine transformations of each other, then the outcome of the scf is the same at ==== and ====.====To understand vNM-ordinality better, note that the outcome of an scf in our model is a lottery over the set of alternatives. Agents evaluate lotteries over alternatives using the expected utility criterion. By the expected utility theorem, the set of all utility vectors over alternatives can be partitioned into equivalence classes where utility vectors within each equivalence class rank lotteries in the same way. vNM-ordinality requires that scfs assign the same lottery to all utility vectors within an equivalence class.====Note however that even though these utility vectors rank the lotteries the same manner, they ==== assign them the same expected utility values. vNM-ordinality amounts to ignoring the expected values of the lotteries and only considering ordinal ranking of lotteries. Though this is without loss of generality in a model where the designer only cares about ordinal ranking of lotteries (i.e., utility is ordinal), as we show, vNM-ordinality is ==== implied by incentive compatibility. Hence, it is not an assumption that can be imposed without loss of generality when utility is cardinal.====Our second notion of ordinality, which we call ====, requires that the scf be sensitive to only ordinal ranking of ====. In other words, strong ordinality says that if ==== and ==== are two utility vectors which represent the same ordinal ranking over alternatives, then the outcome of the scf is the same at ==== and ====. Strong ordinality implies vNM-ordinality because if ==== and ==== are affine transformations of each other, then they represent the same ordinal ranking over alternatives.====We impose two versions of continuity. Fix the types of all agents except an arbitrary agent ====. If ==== varies the cardinal intensities of her preferences over alternatives without changing her ordinal ranking over the alternatives, then the outcome of the scf must vary continuously. Our notion of continuity is clearly weaker than the standard notion in that it applies only to sub-domains where the ordinal rankings over alternatives are unchanged. Thus, our formulation allows scfs to be sensitive to cardinal intensities but restricts them to be continuous in a limited sense. We call this requirement ====. We also consider a stronger version of continuity where we require the outcome of the scf to vary ==== continuously. We call this requirement ====.==== ====Our first result (Theorem 1) shows that every dominant strategy incentive compatible scf is vNM-ordinal ====. In other words, there is a dense subset of the domain where the scf is vNM-ordinal and the complement of this subset has measure zero. We give an example to illustrate that dominant strategy incentive compatibility does not imply vNM-ordinality (everywhere) - this is in contrast to the trivial conclusion in the deterministic scf case, where incentive compatibility implies (strong) ordinality. We use our main result to establish two striking results: (a) every c-continuous and dominant strategy incentive compatible scf is vNM-ordinal (Theorem 2) and (b) every uc-continuous and dominant strategy incentive compatible scf is strongly ordinal (Theorem 3). We emphasize here that incentive compatibility itself implies c-continuity of the scf almost everywhere. Our assumption implies that if we strengthen it to everywhere, we get vNM-ordinal scfs. Further, if we strengthen it to uc-continuity everywhere, we get strongly ordinal scfs. We discuss the implications of c-continuity and uc-continuity on our results in greater detail in Remark 3, Remark 4.====It is well known that there are incentive compatible scfs that are not strongly ordinal — we give some examples later. The uc-continuity condition requires that as the cardinal value of all the alternatives approach zero for the agent, the value of the scf must converge to any single value. This drives the strong ordinality result.====If the solution concept is Bayesian incentive compatibility, our conclusions are in terms of interim allocation probabilities.==== ==== In particular, we show that every cardinal Bayesian incentive compatible mechanism satisfying c-continuity property must be such that an agent’s interim allocation probability of each alternative does not change whenever she changes her type by using an affine transformation. Similarly, every cardinal Bayesian incentive compatible mechanism satisfying uc-continuity property must be such that an agent’s interim allocation probability of each alternative does not change whenever she changes her type such that the ordinal ranking over alternatives do not change. We note that these results for the Bayesian incentive compatibility case requires no assumptions on the priors of the agents.====As an application of our result, we show that the utilitarian scf is not Bayesian incentive compatible under mild conditions on the priors of the agents and the type space. Further, we show that among all weighted utilitarian scfs, except the dictatorship (which assigns positive weight on one agent and zero weight on all other agents), no other scf is Bayesian incentive compatible.====Our results apply to standard voting models but can also be applied to private good allocation problems (for instance, matching models) with the additional assumption of ====. Non-bossiness requires the following: a type change by an agent that does not change her allocation also leaves the allocations of all other agents unchanged. Introduced by Satterthwaite and Sonnenschein (1981), non-bossiness is a commonly used condition in the literature (Papai, 2000, Ehlers, 2002).====We believe that the paper contributes to the literature on mechanism design without transfers in several ways. It provides a foundation for the use of ordinal mechanisms. Moreover, we believe that our paper makes a methodological contribution. We use techniques from multidimensional mechanism design with transfers, particularly subgradient techniques used in that literature, to prove our results. Related methods have been used in some restricted one dimensional problems in the voting literature recently (Borgers and Postl, 2009, Goswami et al., 2014, Gershkov et al., 2017, Hafalir and Miralles, 2015). To the best of our knowledge, ours is the first paper to use the multidimensional versions of these results in such a setting.====We define the type space of an agent by specifying an arbitrary set of permissible linear orders over alternatives and considering all positive utility functions representing these orders. Thus our specification requires maximal cardinal richness consistent with an arbitrary set of ordinal restrictions. Our results therefore hold for standard unrestricted ordinal preferences, single-peaked ordinal preferences, and all standard domain restrictions studied in strategic social choice theory — here, domain restriction means restrictions on the ordinal ranking of alternatives (degenerate lotteries).",Continuity and incentive compatibility in cardinal mechanisms,https://www.sciencedirect.com/science/article/pii/S030440682030029X,5 March 2020,2020,Research Article,225.0
"Nöldeke Georg,Peña Jorge","Faculty of Business and Economics, University of Basel, Peter Merian-Weg 6, Postfach, 4002 Basel, Switzerland,Institute for Advanced Study in Toulouse, University of Toulouse 1 Capitole, 1 Esplanade de l’Université, 31080 Toulose Cedex 6, France","Received 26 August 2019, Revised 4 February 2020, Accepted 27 February 2020, Available online 4 March 2020, Version of Record 13 March 2020.",https://doi.org/10.1016/j.jmateco.2020.02.003,Cited by (10),"We consider how group size affects the private provision of a public good with non-refundable binary contributions. A fixed amount of the good is provided if and only if the number of contributors reaches an exogenous threshold. The threshold, the group size, and the identical, non-refundable cost of contributing to the public good are common knowledge. Our focus is on the case in which the threshold is larger than one, so that teamwork is required to produce the public good. We show that both expected payoffs and the ==== that the public good is obtained in the best symmetric equilibrium are decreasing in group size. We also characterize the limit outcome when group size converges to infinity and provide precise conditions under which the expected number of contributors is decreasing or increasing in group size for sufficiently large groups.","Ever since the publication of Olson’s “The Logic of Collective Action” (Olson, 1965), group size has been considered important in determining how successful a group will be in attaining its common goals. Specifically, Olson suggested that “[t]he larger a group is, the farther it will fall short of providing the optimal supply of any collective good, and the less likely that it will act to obtain even a minimal amount of such a good. In short, the larger the group, the less it will further its common interests” (Olson, 1965 p. 36). These suggestions have attracted much attention in economics and political science but providing firm theoretical underpinnings has proven challenging (Sandler, 2015). While such group size effects are well understood for some of the standard models of collective action (e.g., Chamberlin, 1974, McGuire, 1974, Andreoni, 1988), for other such models this is not the case.====In this paper we investigate group size effects for the class of participation games without refunds introduced in Palfrey and Rosenthal (1984) to model the private provision of a discrete public good. In such a threshold game, ==== group members decide simultaneously whether to contribute to a public good or not (to participate or not). All contributors pay a non-refundable cost. The public good is provided if and only if the number of contributors reaches an exogenous threshold ====, in which case all group members receive the same benefit from the provision of the public good. The threshold, the group size, and the identical cost of contributing to the public good are known to all players. We assume that the threshold is larger than one, thereby focusing on what Myatt and Wallace (2008b) call a “teamwork dilemma” rather than on the volunteer’s dilemma popularized by Diekmann (1985).====This threshold game is a stark model, which deliberately abstracts from asymmetries and incomplete information about costs and benefits by assuming that all group members are identical (Palfrey and Rosenthal, 1984 p. 172). For our purposes this is an attractive feature as it isolates the effects of changes in group size from the effects of changes in the composition of the group, thereby allowing us to focus on the former.==== ==== We further abstract from the possibility of role identification and thus restrict attention to symmetric (Nash) equilibria in which all group members employ the same (mixed) strategy and therefore obtain the same equilibrium payoff.==== ==== Of course, the assumption that there is no role identification makes what is already a stark model even starker. In particular, this assumption is not satisfied if group members interact repeatedly and therefore can condition their behavior on actions played in the past. Our approach to the investigation of group size effects is thus orthogonal to the one pursued in Myatt and Wallace, 2002, Myatt and Wallace, 2008a and Myatt and Wallace (2008b) who use stochastic stability arguments akin to the ones developed in Kandori et al. (1993) and Young (1993) to select among the pure strategy equilibria in repeated threshold games played by myopic players and study the composition affects arising in asymmetric games together with group size effects. Due to their focus on pure strategy equilibria, the papers by Myatt and Wallace find that the selected equilibrium features a probability of provision that is either zero or one. In contrast, we model a situation in which the probability of provision can change more gradually with group size. Over a large range of parameter values this is indeed what we find.====Generically (more precisely, for all but a countable set of cost parameters) our model has one or three symmetric equilibria. As it is always a symmetric equilibrium for no one to contribute, uniqueness obtains if and only if cost is so high and the group so large as to imply that the public good cannot be provided in a symmetric equilibrium. Hence, we face a multiplicity problem whenever the model allows for a non-trivial symmetric equilibrium, that is, an equilibrium in which the public good is provided with strictly positive probability. We resolve this problem by focusing on the – uniquely determined – best symmetric equilibrium, that is, the one which provides all agents with the highest expected payoff among the symmetric equilibria. This is also the symmetric equilibrium with the highest participation probability and the highest success probability (i.e., the highest probability that the public good is provided) among the symmetric equilibria and we thus refer to it as the ====.====We can think of two distinct reasons justifying our focus on the maximal equilibrium. First, it is of intrinsic economic interest to ask how the best equilibrium outcome that a group can achieve (under the coordination friction implied by the symmetry constraint that we presume to be unavoidable) depends on group size. Second, among the two non-trivial symmetric equilibria the one we consider is stable under the standard dynamics (e.g., the replicator dynamics) considered in evolutionary game theory, whereas the other one is unstable.==== ==== Consequently, whenever there are multiple symmetric equilibria only the maximal and the trivial equilibrium (which is always stable) are of relevance. In an evolutionary analysis of threshold games it is then natural to focus on the comparative statics of the maximal equilibrium (see Bach et al., 2006, Archetti and Scheuring, 2011). Nevertheless, it is of interest to understand how our comparative statics results depend on our focus on the maximal equilibrium. We therefore discuss how some of our key results would change if one were to consider the smaller of the two non-trivial symmetric equilibria instead.====Section 2 notes some fundamental facts about the symmetric equilibria of the threshold game we consider. All of these are immediate from Palfrey and Rosenthal (1984). We then begin our study of maximal equilibria by observing that (over the range of group sizes for which a non-trivial symmetric equilibrium exists) the participation probability in such an equilibrium is strictly decreasing in group size. We state this well-known result (e.g., Offerman, 1997, Hindriks and Pancs, 2002) as Proposition 1 in Section 3.1.====Our main results (Proposition 2 in Section 3.2 and Proposition 3 in Section 3.3) show that the expected payoff and success probability that are induced by the maximal equilibrium are strictly decreasing in group size until group size reaches a critical value (which may be infinite) beyond which the public good cannot be provided. We view Proposition 2 as confirmation of Olson’s maxim that “the larger the group, the less it will further its common interests,” whereas Proposition 3 formalizes his statement that the larger a group is, “the less likely that it will act to obtain even a minimal amount of such a good.” We complement these results by characterizing the limit as group size goes to infinity (Proposition 4 in Section 3.4) and by showing that for sufficiently large groups the expected number of participants is increasing in group size if and only if the contribution cost is sufficiently low (Proposition 5 in Section 3.5). We find the latter result interesting as it delineates the circumstances under which larger groups not only have a lower success probability but also incur higher expected aggregate costs than smaller groups.====The questions addressed in our Proposition 3, Proposition 5 have been previously considered in Hindriks and Pancs (2002).==== ==== In their Proposition ==== these authors claim that “the effect of group size on the probability of provision is indeterminate.” In contrast, our Proposition 3 shows that this effect can be determined and is negative for the maximal equilibrium we consider. Hindriks and Pancs (2002, Proposition 7) also consider the case of large group sizes. While three out of the four claims in that proposition are (as we prove) correct, their argument yielding these results is not. The problem is that Hindriks and Pancs (2002) take for granted that the error introduced by using the Poisson approximation to the binomial distribution can be ignored when studying comparative statics for sufficiently large groups. Our result on the expected number of contributors – which directly contradicts the corresponding claim in Proposition ==== from Hindriks and Pancs (2002) – shows that this is not so.====As we have noted before, the games we consider differ from the volunteer’s dilemma only in that we consider thresholds ====, whereas the volunteer’s dilemma corresponds to ====. The kind of group size effects we study are much easier to determine for the volunteer’s dilemma (see, for instance, the textbook treatment in Dixit et al., 2004, p. 454–458) because its unique symmetric equilibrium is easy to calculate explicitly. With the exception of Proposition 2 – in the volunteer’s dilemma the payoff in the symmetric equilibrium is independent of group size – all our propositions generalize corresponding results for the symmetric equilibrium of the volunteer’s dilemma.====We discuss some other related literature that considers symmetric equilibria in symmetric participation games to study group size effects in Section 4, where we also note how our Propositions 1–3 can be extended to the second class of games considered in Palfrey and Rosenthal (1984), namely participation games with refunds.",Group size and collective action in a binary contribution game,https://www.sciencedirect.com/science/article/pii/S0304406820300288,4 March 2020,2020,Research Article,226.0
"Du Ye,Lehrer Ehud","Southwestern University of Finance and Economics, China,School of Mathematical Sciences, Tel Aviv University, Tel Aviv 69978, Israel and INSEAD, Bd. de Constance, 77305 Fontainebleau Cedex, France","Received 27 September 2019, Revised 31 January 2020, Accepted 11 February 2020, Available online 24 February 2020, Version of Record 4 March 2020.",https://doi.org/10.1016/j.jmateco.2020.02.002,Cited by (1),". In this model the set of alternative strategies, with which a dynamic decision policy is compared, is the set of stationary mixed actions that satisfy all the constraints. We show that there exists a strategy that satisfies the following properties: (i) it guarantees that after an unavoidable deterministic grace period, there are absolutely no violations; (ii) for an arbitrarily small constant ====, it achieves a convergence rate of ====, which improves the ==== convergence rate of Mannor et al. (2009).",", also known as ==== or ====, is one of the central topics studied in game theory, computer science, and machine learning. The dynamic decision making model considers two players: a decision maker (henceforth, DM) and an adversary. At each round, the DM chooses an action to play while at the same time, the adversary chooses a state of nature. The combination of both determines a payoff received by the DM. The DM can take decisions based on the historical states of nature as well as on his own previous actions. The objective of the DM is to have ==== with respect to a set of alternatives. In other words, when the DM will examine the strategy he employed by comparing his performance to what he could achieve had he played certain alternative strategies instead, he will have no regret for not playing the latter. This is a non-Bayesian model: the DM has no prior probability distribution over the states of nature chosen by the adversary. Rather, he would like to play a strategy immunized against having regret, no matter how the adversary chooses the states.====We investigate a dynamic decision problem where the DM has exogenous constraints over the empirical frequency of actions actually played. The empirical frequency of actions must be kept within a pre-specified set, and in a case of violation the DM is penalized. It might be, for instance, that the DM is not allowed to play a certain action more than 50% of the time. In such a case, whatever the DM tries to achieve, he must do it without playing more than 50% of the time this action, otherwise he would be subject to a penalty.====This constrained model is motivated by quite a few real world applications. One motivation for this new model is the ==== (see Hazan and Kale, 2009). Here, a portfolio manager dynamically allocates fund across a set of assets (e.g., stocks, bonds and commodities). This manager’s objective is to perform at least as well as the best fixed portfolio in hindsight. Let us assume that market data (e.g., price and trading volume) are updated ==== times per trading year and the maximum number of assets that can be held in a portfolio is ====. In principle, a portfolio manager can adjust his allocation every time when new market data come in. In practice, however, he cannot do so due to trading constraints imposed by regulators or stock exchanges. For instance, stock exchanges, e.g. Shanghai Stock Exchange,==== ==== do not allow stocks to be purchased and sold out the same day. Thus, a portfolio manager can adjust his portfolio at most ==== times per year, where ==== is the number of trading days in a year. In other words, the constraint in this setting is that a portfolio manager cannot adjust his portfolio more than ==== percent of time.====Another motivating example is the ==== (see Abernethy and Kale, 2013). A market maker is a trading agency that provides liquidity to a market by quoting both the bid and asking prices of an asset simultaneousnessly. Due to the ==== or ==== (O’Hara, 1998), a market maker may be reluctant to provide quotes at unfavorable market scenarios. Nevertheless, in order to ensure enough liquidity, trading exchanges may impose constraints on the minimal number of quotes per day==== ==== that a market maker must provide.====Beyond exploring a non-Bayesian dynamic decision making, the theory of no-regret strategies is motivated by the connection it has to the study of economic equilibrium. It is well known that in the context of strategic interaction, when players use strategies that guarantee no (internal) regret, the empirical distributions of their joint plays converge to the set of correlated equilibria (see Hart and Mas-Colell, 2000). This result tells us that the statistics of the plays by non-Bayesian players converges to a solution concept where players hold a prior (correlated) belief about other players’ actions.====In this study the objective of the DM is to minimize his regret. For this purpose we introduce a model called ====, which generalizes the standard no-regret approach. In this learning model, there are finitely many linear constraints on the empirical frequency of actions actually played. A sequence of actions is considered ==== if its empirical frequency is within the limits imposed by the constraints. At any time the sequence of actions played is not legal, the DM gets a penalty.====In some real world scenarios, when rules or regulations are violated, penalties are imposed on the DM. Motivated by such examples, we introduce a penalty function into our model. As long as the sequence of actions played is legal, the penalty is naturally assumed to be zero. However, if the sequence is illegal, a penalty is deducted from the payoff of the DM. The penalty function could be very general. It could depend on time, be constant or proportional to the level of violation; it could be even a nonlinear function.====A study of a no-regret strategy in a dynamic decision problems with constraints should focus on three important issues. The first is to introduce of a proper definition of the set of alternative strategies. This set consists of the strategies to which the DM would eventually compare the performance of his own strategy. The second issue is whether there exists a strategy that guarantees no regret. In other words, is there a strategy that obeys all constraints and at the same time performs as well as any alternative strategy? The third issue is the extent of the regret as the time goes by. That is, how fast the gap between the DM’s actual performance and the performance of the best among the alternative strategies, shrinks to zero?====In the standard no-regret learning model the choice of alternative strategies is a set of all stationary pure actions, where a pure action is constantly played all the time. This set of alternatives would be inappropriate in a constrained model. The reason is that employing one of these strategies would necessarily violate one or several of the constraints. In the constrained model, the set of alternative strategies is defined to be the set of all stationary strategies that satisfy all the constraints. These stationary strategies are typically mixed, rather than pure.====Given the set of alternative strategies, we define the ==== that corresponds to a sequence of actions and states. It is defined in the spirit of the classical regret: the gap between the actual average payoff obtained (after penalties deducted) and the best payoff achievable by constantly playing one of the alternative strategies. A strategy in the dynamic model is ==== if, no matter what the penalty function is, its regret diminishes to zero with time. Our main result shows that such a strategy exists. In order to obtain no-regret independently of the penalty actually imposed, we design a strategy that guarantees that after a fixed grace period, absolutely no violation of any constraint occurs.====In the proof of the main result, we consider first an auxiliary dynamic problem. In this model the DM has a reduced set of pure actions: only those (mixed) actions in the original problem that obey the constraints. Suppose, for instance, that there is only one constraint, dictating that the frequency of playing a particular action cannot exceed twenty percent of the time. In this case, the set of actions in the auxiliary model is the set of mixed actions in which the probability of playing that action is less than or equal to 0.2. Employing Blackwell’s Approachability Theorem, we construct a no-regret strategy ==== in the auxiliary problem, where the DM may use all actions available without any limit.====The strategy ==== is then adjusted to fit the dynamic problem with constraints. This adjustment is done as follows. The time line is divided into intervals, the length of each depends on the total lengths of its predecessors. Each interval is further divided into three sub-intervals. In the first sub-interval, we ignore regret considerations and pay attention only to the constraint: we play a deterministic legal sequence of actions. In the second sub-interval, we follow ==== as long as we run no risk of violating anyconstraint. In case there is a chance of violation, we start playing the deterministic legal sequence again in the third sub-interval. This scheme is played repeatedly over and over in a carefully designed way so as to avoid any violation and at the same time minimize regret. In particular, we show that the probability of approaching a violation zone is slim, and therefore the probability to continue playing ==== without any interruption and thereby guarantee no-regret, is high. This way we obtain a constrained no-regret strategy. In all, our constrained no-regret strategy has the following nice properties: (i) it guarantees that after an unavoidable deterministic grace period, there are absolutely no violations; (ii) for an arbitrarily small constant ====, it achieves a convergence rate of ====, which improves the ==== convergence rate of Mannor et al. (2009).",Constrained no-regret learning,https://www.sciencedirect.com/science/article/pii/S0304406820300197,24 February 2020,2020,Research Article,227.0
"Chambers Christopher P.,Hayashi Takashi","Department of Economics, Georgetown University, United States of America,Adam Smith Business School, University of Glasgow, United Kingdom of Great Britain and Northern Ireland","Received 5 December 2017, Revised 16 January 2020, Accepted 10 February 2020, Available online 19 February 2020, Version of Record 7 May 2020.",https://doi.org/10.1016/j.jmateco.2020.02.001,Cited by (0),"This paper investigates whether there is an allocation rule for which innovation never hurts anyone. Existing studies provide possibility characterizations together with efficiency and a natural participation constraint, assuming the domain of one input good and one output good in which nobody prefers to consume more of the input good than what she has. We show that this possibility result does not survive and we lead to impossibility either when (i) somebody wants to consume the input good more than what she has; or when (ii) there are multiple input goods.","Technical innovation is widely understood as beneficial. It enlarges the possibilities of what society can achieve absent incentives. However, people are often rightfully concerned that they may lose out due to innovation. For example, technology often renders certain types of labor obsolete, leading to unemployment or decrease of wage income. Our aim in this note is to understand whether these concerns can be taken care of by appropriately adjusting the market mechanism; or perhaps, by considering another type of mechanism altogether.====In the following two examples, we formally demonstrate the known fact that innovation can hurt certain people in the market. The key to this observation is that innovation changes relative prices. The change in relative prices can hurt individuals through two channels:====We illustrate the first point with the following example.====The second point is illustrated by the following example.====Suppose instead that we do not take the market mechanism as given, but ask whether there are other methods of allocating resources which avoid the problems as in the previous examples. We propose to study this question axiomatically. To this end, we study ====. These objects map triples of preference profiles, endowment profiles and production technologies into feasible allocations.====We suppose that technologies exhibit ====. This restrictive hypothesis should, if anything, make positive results easier to obtain.====We consider three axioms. First is Technology Monotonicity, the primary requirement that innovation should not hurt anybody. Second is Efficiency, requiring that any selected allocation must be Pareto-efficient. Third is Free Access Lower Bound, which requires that nobody should receive a worse consumption bundle than what she could obtain by accessing the technology alone. Since in a production economy with constant returns the technology is replicable, the assumption that everybody can/should be able to freely access the technology is reasonable. Hence the lower bound condition is understood as a natural participation constraint. Alternatively, we may simply assume that each individual has the economy-wide production possibility set as her own production possibility set, and full ownership of (a) firm with this set. While we could generalize the model to allow individuals to possess firms with different technologies, this is not needed in order to demonstrate our impossibility.====In existing studies assuming the domain of one input good and one output good in which no individual prefers to consume more of the input good than she initially has (Moulin, 1990b, Moulin, 1987a, Fleurbaey and Maniquet, 1996, Maniquet, 1996), it has been shown that ==== satisfies the three axioms and is indeed characterized by them.====We show that this possibility result does not extend. We are led to impossibility when either (1) some individual wants to consume more of the input than she has; or (2) there are multiple input goods, each of which corresponds to the case illustrated above. In the real world, these two cases are rather generic.",Can everyone benefit from innovation?,https://www.sciencedirect.com/science/article/pii/S0304406820300185,19 February 2020,2020,Research Article,228.0
Stanca Lorenzo,"Department of Managerial Economics and Decision Sciences, Kellogg School of Management, Northwestern University, Evanston, IL 60208, USA","Received 10 August 2019, Accepted 29 January 2020, Available online 13 February 2020, Version of Record 21 February 2020.",https://doi.org/10.1016/j.jmateco.2020.01.007,Cited by (2),I provide a novel simplified approach to Savage’s theory of subjective expected utility. Such an approach is based on abstract integral ,"Savage’s (1954, 1972) axiomatization of subjective expected utility (SEU) is the cornerstone of decision making under uncertainty. However, the predominant approach in the subsequent literature on decision making under uncertainty is the so-called Anscombe–Aumann (AA) framework. This predominance is due to its analytical tractability compared to the Savage set-up. For example, Machina and Schmeidler (1995) referring to Anscombe and Aumann’s approach state ====In this paper, I show how one can obtain Savage-style representation theorems by means of a functional analytic approach as in the AA framework. I provide abstract integral representation theorems and then show how these can be used in a decision-theoretic framework. To illustrate these results, let ==== be a measurable space and consider a functional ==== on the space of simple ====-measurable functions. I provide conditions under which ==== can be written as==== 
 ====and ====for every bounded measurable function ====. The key condition for these results is the following additivity condition: if ==== are such that ==== then ====. In words, this assumption requires additivity restricted to functions that have disjoint support. These integral representation theorems are functional-analytic in the sense that they rely on the use of the Radon–Nykodym theorem.====The advantage of such an approach is that it can be used in axiomatic decision theory to readily obtain variations on Savage’s SEU model. Using the representation in (1), I obtain a representation theorem for SEU with state-dependent utility. Moreover, using (2), I obtain a version of Savage’s SEU model and show how non-atomicity of the prior can be easily relaxed. From a mathematical perspective, these functional-analytic techniques have the advantage of considerably simplifying Savage’s derivation of expected utility.====Finally, I discuss how these results can be used for axiomatic decision theory in different settings. To illustrate this point, I provide a general axiomatization of second order expected utility (Grant et al., 2009, Strzalecki, 2011). In Klibanoff et al. (2019), the representation in (2) is applied to obtain an axiomatization of the smooth ambiguity model.====This paper is closely related to the mathematical literature on integral representation theory, which extensively examined the additivity condition discussed previously. In particular, the present paper is related to the work of Martin and Mizel (1964). The condition that allows the relaxation of non-atomicity of the probability measure is taken from their paper. When translated into preferences, such a condition is equivalent to the unlikely atoms axiom from Mackenzie (2019). Section 4.3 briefly reviews this literature in mathematics.====In decision theory, a similar approach to mine was adopted by Wakker and Zank (1999) and Castagnoli and LiCalzi (2006). Section 4.2 discusses their work and connections with the axiomatic literature in decision theory.",A simplified approach to subjective expected utility,https://www.sciencedirect.com/science/article/pii/S0304406820300161,13 February 2020,2020,Research Article,229.0
"Demirkan Yusufcan,Kimya Mert","New York University, Department of Economics, 19 West 4th Street, 6th Floor, New York, NY 10013, United States of America,University of Sydney, School of Economics, Camperdown, NSW 2006, Australia","Received 17 May 2019, Revised 25 December 2019, Accepted 29 January 2020, Available online 12 February 2020, Version of Record 19 February 2020.",https://doi.org/10.1016/j.jmateco.2020.01.008,Cited by (1),"Manzini and Mariotti (2014) define the menu-independent random consideration set rule, where the ====. This equivalence is used to characterize the menu dependent random consideration set rules that correspond to (i) specific conditions on the ==== rule, and (ii) different stochastic choice models from the literature.","It is well known that individuals do not always consider all of the available alternatives when making a choice. There is also the observation that choices of the individuals are typically stochastic. Manzini and Mariotti (2014) (MM from now on) initiated the literature that combines these two findings. They analyze a stochastic model of consideration set formation in which the decision maker (DM) considers each alternative with a certain unobservable probability and then maximizes her stable preference among the alternatives she considers.====Given any subset of alternatives ====, the DM does not consider all of the available alternatives in ====. Instead, she forms a consideration set ==== and maximizes her stable preference on this consideration set. The composition of ==== is stochastic: each alternative ==== is considered with probability ====. ==== is referred to as the attention function. The modeler is able to observe the probability rule, which is a probability distribution ==== that indicates the probability of choosing an alternative from a given set. The preference and the attention function are assumed to be unobservable.====MM find the necessary and sufficient conditions for the probability rule to be consistent with this model when ==== is menu-independent, i.e. for any two sets ====, ==== and alternative ====, ====. We relax the assumption of menu-independence and allow for any restriction to be placed on the attention function. We call the resulting model menu dependent random consideration set rule (MDRCR).====One might restrict the attention function in a variety of plausible ways. For instance, there is evidence that consumers are less likely to consider an alternative in larger sets (see Caplin et al. (2011), Schwartz (2005) and Lleras et al. (2017)). Hence, one might impose the restriction that the probability of considering an alternative cannot increase as the set gets larger (see Example 3 in Section 2).==== ==== One can think of other plausible restrictions that could be appropriate under different circumstances.====We remain silent on the kind of restriction that should be imposed on the attention function. Instead, we develop a technique that helps considerably with the characterization. A probability rule will be consistent with the model under a particular restriction on the attention function if ==== one can find a preference relation ==== and an attention function that satisfy the restriction such that ==== for every set ====, and for every alternative ====, probability of choosing ==== from set ==== is equal to the probability of considering ==== and not considering any alternative that ranks better than ==== according to ====. We are able to simplify this problem to a great extent by establishing an equivalence between the attention function and the ====.====In our context, the hazard rate is the probability of choosing an alternative conditional on not choosing any of the alternatives that rank higher in DM’s preference. The equivalence between the hazard rate and the attention function means that any restriction on the attention function can be directly translated into a restriction on the hazard rate. That is, to check whether the probability rule is consistent with the model under some restriction, one need only find a preference relation whose hazard rate satisfies the restriction.====When the probability rule is consistent with an MDRCR under some restriction, an additional question that arises is the identifiability of the underlying parameters, i.e. the preference and the attention function. The equivalence between the attention function and the hazard rate implies that the separate parts of the identification problem are closely related. If the probability rule is consistent with a model in our framework, then the underlying preference can be fully identified from the probability rule if and only if the underlying attention function can be fully identified. Furthermore, if the preference is fully identified, then the attention function will be identified as the hazard rate corresponding to the preference.====The generality of the model and the equivalence between the hazard rate and the attention function imply that one can ==== find the exact set of MDRCRs that correspond to a specific restriction on the probability rule, and ==== find the exact set of MDRCRs that correspond to a stochastic choice model in the literature, that itself is not presented as a model with an attention function. We do both in turn.====First, we impose an intuitive condition on probability rules. The condition, which we call ====, requires that if in some set ====, ==== is more likely to be chosen than ====, then in any set ==== including both alternatives, ==== should still be more likely to be chosen. This condition is a strengthening of ====, one of the fundamental properties imposed on probability rules in the literature (see Section 4 for the formal definition). We show that ==== is completely characterized by the restriction that imposes a certain bound on the consideration probability of each alternative, where the bound on ==== is determined by the consideration probability of the alternative that ranks right above ==== in set ==== according to the underlying preference relation. Furthermore, if this condition is weakened to apply only in binary sets, then we get the characterization of the probability rules that satisfy ====.====To show that one can use the machinery developed to translate a model of stochastic choice that is not defined in terms of an attention function into a model in our framework, we find the counterpart of (i) ==== (defined in Echenique et al. (2018)), and (ii) random conditional choice set rules (see Brady and Rehbeck (2016)). We also find simple sufficient conditions for an MDRCR to be a random attention model (see Cattaneo et al. (2017)).====Section 2 formally defines the model. In Section 3, we provide the characterization of the model, and we investigate the identification properties. In Section 4, we find the restrictions imposed on the attention function by weak order independence and weak stochastic transitivity. In Section 5 we show that the machinery developed can be useful to translate a stochastic choice model into a model in our framework. Section 6 contains the literature review, and Section 7 concludes with a remark.","Hazard rate, stochastic choice and consideration sets",https://www.sciencedirect.com/science/article/pii/S0304406820300173,12 February 2020,2020,Research Article,230.0
Afacan Mustafa Oǧuz,"Faculty of Arts and Social Sciences, Sabancı University, 34956, İstanbul, Turkey","Received 18 November 2018, Revised 6 January 2020, Accepted 20 January 2020, Available online 31 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jmateco.2020.01.006,Cited by (5),"We formulate a graduate admission problem, which features different financial support options, in a matching with contracts setting. We introduce an algorithm, called “Minimum-Need Adjusted Cumulative Offer Process” (====). Under certain mild assumptions on students’ preferences, ==== is stable, fair, strategy-proof, and respects improvements. Moreover, it limitedly respects departments’ minimum number of teaching and research assistant (TA/RA) needs in the sense that no other stable mechanism honors those needs more than ====. It is also efficient within the class of stable mechanisms that limitedly respect the TA/RA needs. Lastly, we offer an axiomatic characterization: A mechanism is stable and strategy-proof, and limitedly respects the TA/RA needs if and only if it is ====.","Every year, students who wish to pursue graduate studies apply to graduate programs, and departments conduct admission processes to decide whom to accept and what kind of first-year financial support to offer to admitted students.==== ==== ==== There are four admission types, each associated with a different financial support option: admission with fellowship, admission with teaching assistantship (TAship), admission with research assistantship (RAship), and admission without financial support. The current U.S. graduate admissions markets run in a decentralized fashion, which causes problematic outcomes, as detailed later in Section 3.1. In this study, we propose a centralized algorithm that not only avoids all these current problems but also admits some other highly desirable properties.====Each admission type entails different conditions. While the admission types of with fellowship and without financial support do not require a workload, students with TAship and RAship admissions are required to supply a certain workload (in general, at most ====-h TA/RAship work per week). The advantages of TAship and RAship are that they lead students to improve their teaching and research abilities, respectively. Besides, they may differ in their respective stipend amounts.==== ==== Hence, we can say that each admission type has its advantages and disadvantages, and therefore, students may well have heterogeneous preferences over department-admission type tuples.==== ====Departments rank students based on their subjective evaluations. They are interested in their admitted student body as well as the financial support they offer to the admitted students. First, departments are willing to offer financial support to each of their admitted graduate (doctoral) students as long as they can do so.==== ==== Second, admission without financial support is the least prestigious admission type, and departments would rather offer it to the worst-ranked admitted student body whenever the capacities of all other admission types are exhausted. Another constraint making the admission procedure more challenging is that departments may need a certain number of teaching and research assistants (TA/RA, respectively). Therefore, while deciding who gets what kind of admission, departments need to take care of their TA/RA needs.====We first formulate a graduate admissions problem, including the above aspects, in a matching with contracts setting. While there are related studies on similar problems, to the best of our knowledge, ours is the first to incorporate the financial support term and the TA/RA needs into an admission problem. We introduce a choice function for departments, called “admission with financial support” (====) choices. We justify the ==== choices by observing that they internalize both the departments’ TA/RA needs and the appealing normative condition that the worst-ranked admitted student body should receive no financial support whenever the capacities of other admission types are exhausted. We then propose a twofold algorithm. Its first stage consists of the well-known “cumulative offer process (====)” Hatfield and Milgrom (2005) under the ==== choices. For each department with a TA or RA shortage (or both) with respect to its TA/RA need at the first-stage outcome, the second stage attempts to mitigate these shortages as much as possible by carefully changing the admissions of the admitted students within each department. One feature of the second stage is that TA shortages are prioritized in that it attempts to mitigate TA shortages first. Once TA shortages are eliminated, only then does it deal with RA shortages. We choose this approach because TA shortages are arguably more critical to departments in carrying out necessary academic activities.==== ==== However, this second stage can be modified to reverse the order of the adjustments at no cost. We refer to our twofold mechanism as the “Minimum-Need Adjusted Cumulative Offer Process” (====).====For our analysis, we impose certain assumptions on student preferences. First, we assume that students have a ranking list over departments, and each of them prefers any contract (a contract consists of a department and an admission-type) with the financial support of more favorable departments.==== ==== Second, if a student finds a contract with some financial support better than the outside option, then he continues to prefer contracts with any financial support of the same department to the outside option. Third , if a student would rather have a contract without financial support of a department instead of having a contract with some financial support of some other department , then he continues to prefer the former contract to any contract of the latter department. Lastly, every student prefers contracts that include some financial support to those without financial support of the same departments. While our results would disappear in the absence of these assumptions , as we will formally elaborate more in the Model section, they are empirically supported by several studies (e.g., Bersola et al., 2014, Poock and Love, 2001, Malaney, 1984); hence, we believe they are rather mild in the current context.====We study various properties of ====. We first show that ==== produces a stable Gale and Shapley (1962) and Hatfield and Milgrom (2005) allocation. Then, we consider its responsiveness to the departments’ TA/RA needs. To this end, we say that a mechanism ==== if, ==== for each problem and each department, the former’s outcome never induces a higher TA shortage than the latter’s, and moreover, if both of them do not yield a TA shortage for the department, then it is the case for RA needs as well, and ==== for some problem and some department, the latter’s outcome induces a higher TA or RA shortage (or both) than the former’s. We say that a mechanism ==== if no other stable mechanism respects the TA/RA needs more than itself. ==== limitedly respects the TA/RA needs.====Next, we study the efficiency, fairness, and incentive properties of ====. We show that no stable mechanism that limitedly respects the TA/RA needs efficiency-wise dominates ====. In other words, ==== is efficient within the class of stable mechanisms that limitedly respect the TA/RA needs. For our fairness analysis, by following Balinski and Sönmez (1999), we say that an allocation is ==== if no student envies someone else’s contract while, at the same time, he has a better ranking at the latter’s department. ==== is fair as well. In terms of incentives, ==== is both ==== and ====. While the former guarantees that no student ever benefits from misreporting his preferences, the latter makes sure that no student receives a worse contract after improving his rankings in departments.====In our last analysis, we offer an axiomatic characterization of ====. We show that a mechanism is stable and strategy-proof, and limitedly respects the TA/RA needs if and only if it is ====. The independence of these axioms is also obtained.====This study is the first that explicitly incorporates (soft) minimum needs – in other words, (soft) minimum capacities – into a matching with contracts setting. In addition to the current graduate admission problem, minimum capacities appear in many other real-life problems; hence, our algorithm may be useful to future studies.==== ==== One final remark relates to our proof techniques. Some of our proofs are partly based on the results of Kominers and Sönmez (2016). Specifically, we show that ==== choices fall within their domain of choice functions. This enables us to utilize some of their results in our findings on the first stage of ====, which is ==== under the ==== choices. We then use these auxiliary findings to prove our ultimate results about ====.",Graduate admission with financial support,https://www.sciencedirect.com/science/article/pii/S030440682030015X,31 January 2020,2020,Research Article,231.0
"Bossert Walter,Cato Susumu","Centre Interuniversitaire de Recherche en Économie Quantitative (CIREQ), University of Montreal, P.O. Box 6128, Station Downtown, Montreal, QC H3C 3J7, Canada,Institute of Social Science, University of Tokyo, 7-3-1, Hongo, Bunkyo-ku, Tokyo 113-0033, Japan","Received 14 October 2019, Revised 18 January 2020, Accepted 21 January 2020, Available online 30 January 2020, Version of Record 14 February 2020.",https://doi.org/10.1016/j.jmateco.2020.01.005,Cited by (2),"We analyze the decisiveness structures associated with acyclical collective choice rules. In particular, we examine the consequences of adding anonymity to weak Pareto, thereby complementing earlier results on acyclical ","The characterization of collective choice rules by means of their collections of decisive sets of individuals is a well-established approach that has turned out to be very useful over the last few decades. For instance, Fishburn (1970) observes that Arrow’s (1951; 1963; 2012) impossibility theorem is not valid if the population under consideration is infinite. This can be proven in an elegant manner by identifying the decisiveness structure of a social welfare function (that is, a collective choice rule that always generates complete and transitive social relations). According to Hansson (1976, p. 89), Fishburn credits Julian Blau for being aware of this result as early as 1960; however, Blau never published it. The collection of decisive sets for a social welfare function with an unrestricted domain that satisfies weak Pareto and independence of irrelevant alternatives forms an ultrafilter; see Kirman and Sondermann (1972) and Hansson (1976). If the population is finite, all ultrafilters are principal (that is, generated by a single individual) and thus dictatorial. In the infinite case, however, there also exist non-principal ultrafilters, and these do not correspond to dictatorships. In the same vein, filters can be associated with quasi-transitive social relations; see Hansson (1976) and Bossert and Suzumura (2010, Sections 10.1 and 10.2).====The case of acyclicity has been addressed by Brown, 1974, Brown, 1975 and Banks (1995), for instance. In analogy to the contributions of Kirman and Sondermann (1972) and Hansson (1976) who identify the decisiveness structures that underly complete and transitive social choice, Brown, 1974, Brown, 1975 and Banks (1995) show that if a collective choice rule generates acyclical social relations and satisfies weak Pareto, then the collection of decisive sets for this rule must be a prefilter—a structure that generalizes the notion of a filter. This observation is proven for two cases. In the first of these, the number of individuals is assumed to be finite and does not exceed the number of alternatives; in the second, the population may be finite or infinite but there must be infinitely many alternatives. Moreover, a reverse implication is established by Brown (1975) in the latter case. In particular, Brown (1975) shows that, for any given prefilter, there exists an acyclical collective choice rule that satisfies weak Pareto and Arrow’s (1951; 1963; 2012) axiom of independence of irrelevant alternatives and, moreover, the collection of decisive sets for this rule is equal to the given prefilter.====Our notation and basic definitions are presented in Section 2. We then recapitulate the observations of Brown and Banks, which form the starting point of our analysis, in Section 3. In the remainder of that section, we illustrate how their results can be strengthened. This is accomplished by means of a new structure that we refer to as a conditional prefilter—a weakening of a prefilter. We first show that a conditional prefilter emerges as the collection of decisive sets if we assume an acyclical collective choice function to satisfy the weak Pareto principle. Unlike the requisite results of Brown, 1974, Brown, 1975 and Banks (1995), our implication is not conditional on the cardinality of the population (which may be finite or infinite) or on the number of alternatives. We note that this implication (as well as others in a similar vein established here) does not rely on the property of independence of irrelevant alternatives. Because of this, scoring rules such as the Borda count are covered by these observations.====In addition, a reverse implication is formulated, again using the notion of a conditional prefilter. Specifically, we show that for any given conditional prefilter, there exists an acyclical collective choice rule which has this prefilter as its collection of decisive sets and, moreover, satisfies the properties of weak Pareto and neutrality (a strengthening of independence of irrelevant alternatives). As is the case for Brown’s (1975) reverse implication, our theorem applies to arbitrary (finite or infinite) populations and to arbitrary (finite or infinite) sets of alternatives. An interesting conclusion to be drawn from our results is that a conditional prefilter is well-suited to capture the decisiveness structure behind acyclical social choice.====Section 4 forms the main body of the paper. As our primary contribution, we investigate the consequences of imposing anonymity in addition to weak Pareto on an acyclical collective choice rule. In particular, we determine the additional structure of the collections of decisive sets that emerges in the presence of these two properties. As is the case for transitive collective choice rules, there is a difference between the finite-population case and the infinite-population setting.====We first consider finite populations. Our first major result establishes that the collection of decisive sets for a Paretian and anonymous acyclical collective choice rule must be a symmetric conditional prefilter; the reason for using this label will become clear once we introduce the requisite formal definition. The implication can be reversed in the sense that, for any symmetric conditional prefilter, we can define an acyclical collective choice rule that satisfies weak Pareto, anonymity, and neutrality and, moreover, the collection of decisive sets is given by this symmetric conditional prefilter. The relative magnitudes of the set of individuals and the set of alternatives is of importance in a finite-population setting. If there are at least as many alternatives as there are individuals, the only possibility is that the unique decisive set is given by the singleton that consists of the entire population. This is a quite strong implication because it proves that not only a prefilter but actually a filter results from our properties. On the other hand, if there are fewer alternatives than individuals, the collection of possible decisive sets is considerably richer. The distinction based on the relative cardinalities of the population and the set of alternatives parallels Bossert and Suzumura’s (2008) observations regarding a class of Suzumura-consistent collective choice rules. Suzumura consistency, introduced by Suzumura (1976), is a coherence condition that is intermediate in strength between transitivity and acyclicity. It has several appealing properties that are discussed in Suzumura, 1976, Bossert et al., 2005, and Bossert and Suzumura (2008), for instance.====Finally, we deal with the case of an infinite (to be precise, countably infinite) population. In this setting, a new subclass of prefilters plays a dominant role. We refer to these prefilters as Fréchet prefilters because they share an important property with the well-known Fréchet filter in that the complement of any set in such a prefilter is finite. It is important to note that we employ this concept only if the population is infinite; as is the case for the Fréchet filter, it is not suitable for finite populations. With an infinite population, there are two possible types of collections of decisive sets for an acyclical collective choice rule that satisfies weak Pareto and anonymity. In analogy to the finite-population case, the first possibility is the singleton collection that consists of the entire population. The second option is a decisiveness structure that corresponds to symmetric free Fréchet prefilters. We emphasize that this joint derivation does not require any assumptions regarding the cardinality of the set of alternatives—it is true for both the finite and the infinite case. Our final result establishes a reverse implication by showing that an acyclical collective choice rule that satisfies weak Pareto, anonymity, and neutrality can be defined for any symmetric free Fréchet prefilter, where the collection of decisive sets is given by this prefilter.====We work in an environment that does not require social relations to be transitive—the weaker property of acyclicity is all that is imposed. Moreover, the main axioms employed in this paper are weak Pareto and anonymity, two properties of a collective choice rule that are intuitively appealing and widely accepted. As a consequence, our results are applicable to some collective decision rules that are not covered in the earlier literature. For example, the contributions of Nakamura, 1979, Blair and Pollak, 1982, Kelsey, 1984, Le Breton and Truchon, 1995, Truchon, 1996, and Duggan (2016) deal with acyclic social choice but most of their results depend on the assumption of independence of irrelevant alternatives. On the other hand, the decisiveness structures established in this paper are compatible with scoring rules such as the Borda count and, more generally, voting methods that violate this independence condition.====One contribution of this paper consists of providing characterizations of rules based on Fréchet prefilters in a setting with an infinite population. The concept of a Fréchet prefilter is, as far as we are aware, new to the literature. The infinite-population framework is a natural analytical tool for intertemporal social choice. Our results suggest that Fréchet prefilters may turn out to be useful in an intertemporal context as well, thereby complementing earlier contributions such as those of Pazner and Wesley, 1978, Fey, 2004, and Surekha and Rao (2010).","Acyclicity, anonymity, and prefilters",https://www.sciencedirect.com/science/article/pii/S0304406820300148,30 January 2020,2020,Research Article,232.0
"Baharad Eyal,Ben-Yashar Ruth,Patal Tal","Department of Economics, Bar Ilan University, Ramat Gan 52900, Israel","Received 15 November 2017, Revised 26 November 2019, Accepted 8 January 2020, Available online 27 January 2020, Version of Record 10 February 2020.",https://doi.org/10.1016/j.jmateco.2020.01.002,Cited by (1),This study shows that independence between voters’ skills and states of nature improves the majority voting efficiency relative to the case when such independence does not exist. This implies that specialization (state of nature wise) is not advantageous under the simple majority rule.,"The simple majority rule is the most common voting method in democracy and hence a widely studied rule in voting theory. An important aspect of the majority rule is its efficiency, i.e. the probability to reach the correct decision, given that one exists. According to the common assumptions (i) voters’ goal is to reach this correct decision; (ii) voters vote independently, and (iii) their decisional skills can be represented in terms of probability.====The ==== probability of reaching a correct decision is based on the voters’ decisional skills. The seminal Condorcet Jury Theorem, CJT (Condorcet, 1785), is based on a fundamental assumption: every voter has a probability of choosing correctly, assuming a binary choice.==== ====However, the possible dependence of such skills on states of nature (where every decision is correct in a different state of nature) may also play a significant role in the collective probability. This dependence is represented by some probabilities of voting correctly, where each is associated with a different state of nature.====This paper studies the relationship between majority voting efficiency and the dependence of voters’ skills on states of nature. We show, for homogeneous sets of voters, that such dependence negatively affects the efficiency of the simple majority rule.==== ==== ==== According to this result, the collective probability of reaching a correct decision decreases when voters obtain different probabilities in the possible states of nature. Note that the weighted average decisional skills of each voter are assumed to remain unchanged. We conclude that specialization (that implies a higher probability of voting correctly in one state of nature relative to the other), is not advantageous under the simple majority rule. These results are applicable in the context of optimal investment in human capital: one should better invest equally in improving decisional skills under the two states of nature.",On the merit of non-specialization in the context of majority voting,https://www.sciencedirect.com/science/article/pii/S0304406820300112,27 January 2020,2020,Research Article,233.0
"Chatterjee Swarnendu,Storcken Ton","HE&OR, Novartis Healthcare, India,Department of Quantitative Economics, School of Business and Economics, Maastricht University, Netherlands","Received 14 February 2019, Revised 8 November 2019, Accepted 13 January 2020, Available online 23 January 2020, Version of Record 29 January 2020.",https://doi.org/10.1016/j.jmateco.2020.01.003,Cited by (2),A new property for collective aggregation rules called positive discrimination is introduced. This property is satisfied by many anonymous and neutral collective aggregation rules. We discuss unimodal profiles (or distributions) for which this property on its own determines the outcome as the mode of a ,"Collective aggregation rules involving a large number of voters, such as nationwide elections, are usually anonymous, neutral, and unanimous or at least they satisfy these properties to a large extent. Here we introduce another normative condition called positive discrimination and we show that it is satisfied by many collective aggregation rules, e.g., the score rules, the pairwise rules and the elimination rules. It is violated mainly by non-neutral or non-resolute rules.====Consider the set of linear orders over all candidates as the set of admissible preferences.==== ==== We assume that collective aggregation rules are anonymous, i.e., they treat voters symmetrically. Assuming that the collective aggregation rules are anonymous, a profile, i.e., a combination of voters’ preferences, can be interpreted as a frequency distribution. This particularly makes sense when there is a large number of voters.====Let ==== and ==== be two distinct candidates. For every linear order ====, let ==== denote the linear order obtained from ==== by swapping the positions of ==== and ====. For instance, if ==== is the third best and ==== is the second worst at ====, then ==== is the third best and ==== is the second worst at ==== and the positions of all other candidates have not altered. Consider a profile in which the frequency of any linear order ==== with ==== preferred to ==== is strictly higher than the frequency of ====. In such profiles ==== not only beats ==== in pairwise comparison but it beats ==== given any possible fixed ranking of the other alternatives with only ==== and ==== swapped. We say that in such profiles ==== ==== ====. Positional domination implies the “cover” and “pairwisely as good as” relations of Miller (1980) and Pérez-Fernández and De Baets (2018) respectively. A collective aggregation rule is said to be ==== if it ranks ==== higher than ==== whenever ==== positionally dominates ====. It is a condition that is satisfied by many well-known collective aggregation rules including the pairwise rules, the score rules and the elimination rules. Moreover, we show that positive discrimination is acyclic. Thus, it does not exhibit the drawbacks of, for instance, pairwise majority comparisons.====A natural question is whether or not there are meaningful profiles at which positive discrimination uniquely determines the outcome. Consequently, at such profiles, all positively discriminating collective aggregation rules have the same outcome. As there are many positively discriminating rules, one would expect less or no controversy about the outcome at such profiles. Unimodal profiles are examples of such profiles. In these profiles, frequency of one order, the mode, is the highest and frequencies of the other orders strictly decrease with their Kemeny distance from this mode.==== ==== Note that such unimodal profiles are not single peaked, as (almost) all linear orders appear in these profiles. Unimodality is obviously a strong restriction, but this simple and intuitive restriction delivers a coincidence result that applies to a very large class of rules. Also, it is clear that in a unimodal profile the number of voters is not small. For instance, at least ==== voters are needed to create a unimodal distribution over the set of linear orders on ==== candidates and at least ==== voters are needed in case there are ==== candidates.====Lemma 4.1 essentially shows that in a unimodal profile ====, with mode ====, candidate ==== positionally dominates candidate ====, whenever ==== is preferred to ==== at ====. Therefore, at such profiles the outcome of positively discriminating collective aggregation rules is equal to the mode ====. Apart from the fact that at unimodal profiles the mode is chosen by positively discriminating collective aggregation rules, there is also an intuitive interpretation. The Kemeny distance measures the difference between two orders by the pairs of candidates on which these differ. A unimodal profile may now be considered as a distribution where there is an ideal, norm, or average from which voters may deviate, where we assume that bigger deviations occur with less probability. These ideals, norms, or averages are known to the electorate possibly via, for instance, culture, (social) media or political campaigning. Considering a country-wide population of voters, these ideals or norms may not be unique. Therefore, we assume that each of these creates its own unimodal (sub)profile for a subgroup of voters, and that the addition of these yields the profile involving all voters. For positively discriminating collective aggregation rules, we show that the outcome at the addition of two unimodal distributions is between the two modes of these unimodal distributions. This result generalizes to additions of more than two unimodal distributions.====Next we investigate less restrictive distributions derived from a unimodal distribution, where some disturbances in the tails of the distributions are allowed. We show that the Condorcet consistent rules, the Borda rule, and the plurality rule choose the mode at the following type of perturbed unimodal distributions. Frequencies decline with the Kemeny distance from the mode until about half the maximal distance. Beyond this point frequencies are free, but bounded above by the frequency at half the maximal distance. Further, frequencies are constant at each specific distance from the mode. The combinatorial reasoning bringing about these results might work similarly for other collective aggregation rules, such as other score rules or some elimination rules. But we have not found a general method to deduce this, in particular, not even for all score rules. The results found here indicate, however, that the mode choice of the Condorcet consistent rules, the Borda rule, and the plurality rule is robust to considerable perturbations in the tail of a unimodal distribution.====This paper is part of a rather small literature (including Grandmont et al., 1978, Caplin and Nalebuff, 1988) showing how the ==== problem of social choice is alleviated under particular assumptions on the ==== of preferences rather than on the ==== of preferences. Our research differs from the stream of literature incorporating probabilities by which voters’ preference combinations are formed. For instance, impartial culture induces a uniform distribution where all profiles are equally likely. Under the assumption of uniform distribution, for instance, the probability of a majority cycle occurring has been calculated (see Gehrlein, 2006, Riker, 1982). Many authors have noted that the impartial culture is a significant idealization that almost certainly does not occur in real-life elections. Grofman et al. (2003) go even further arguing that impartial culture is a worst-case scenario in the sense that any deviation results in lower probabilities of occurrence of a majority cycle (see Grofman et al., 2006, for a complete discussion on this issue). A few other probabilistic models have been developed, for instance, multinomial likelihood models (Gillett, 1976, Gehrlein, 1978), dual culture (Gehrlein, 1978), maximal culture condition (Fishburn and Gehrlein, 1977). All these indicate that when voters’ preferences are homogeneous, there is an increased likelihood that a pairwise majority rule winner exists, meaning that Condorcet cycles are avoided. Our result goes further on this as at unimodal distributions not only Condorcet cycles are absent, but also most well-known rules have the same outcome: the mode. Our approach differs from the literature mentioned above, as the distributions considered here are based on the Kemeny distance. Merlin et al. (2000) have studied the probability of conflicts in a U.S. presidential type election. Their results partially resemble ours, but strictly speaking are not comparable, because of their assumption of impartial culture.====In Section 2 we introduce the model basics. Section 3 introduces the condition of positive discrimination. We show that many collective aggregation rules satisfy this condition. In Section 4 we define the notion of unimodal profiles and show that at these the outcomes of many collective aggregation rules are the same, the mode. Section 5 discusses disturbances in the tails of unimodal distributions. Section 6 is on multimodal distributions. A discussion in Section 7 concludes this paper. To make the paper self-contained we provide an Appendix on combinatorial results needed in this paper.",Frequency based analysis of collective aggregation rules,https://www.sciencedirect.com/science/article/pii/S0304406820300124,23 January 2020,2020,Research Article,234.0
"Kovach Matthew,Ülkü Levent","Department of Economics, Virginia Tech, 880 West Campus Drive, Blacksburg, VA 24061, United States of America,Centro de Investigación Económica, Department of Economics, ITAM, Río Hondo 1, Ciudad de México 01080, Mexico","Received 13 January 2019, Revised 2 October 2019, Accepted 28 December 2019, Available online 17 January 2020, Version of Record 30 January 2020.",https://doi.org/10.1016/j.jmateco.2019.12.005,Cited by (13),"We axiomatize a model of satisficing which features random thresholds and the possibility of choice abstention. Given a menu, the ==== first randomly draws a threshold. Next, using a list order, she searches the menu for alternatives which are at least as good as the threshold. She chooses the first such alternative she finds, and if no such alternative exists, she abstains. Since the threshold is random, so is the resulting behavior. We characterize this model using two simple axioms. In general the revelation of the model’s primitives is incomplete. We characterize a specialization of the model for which the underlying preference and list ordering are uniquely identified by choice frequencies. We also show that our model is a special Random Utility Model.","We study a stochastic variation of the celebrated satisficing model due to Simon (1955). A satisficing decision maker is commonly modeled using three psychological primitives: (1) a utility function, (2) a threshold utility level, and (3) a list ordering of alternatives. In any given menu, the decision maker’s choice is the first alternative in the list order whose utility beats the threshold. If the search process yields no such alternative, the decision maker chooses a maximal utility alternative (which is often explicitly assumed to be unique) or the last alternative in the list.==== ====Our variation features two important departures from this basic model. First, we operate in an environment with choice abstention. We interpret choice abstention simply as walking away from the menu, a commonly observed behavior when choice is ====, as is often the case outside of laboratory settings.==== ==== The second departure pertains to the nature of the threshold. We operate in an ordinal environment and our decision maker is endowed with a preference rather than a utility function. Concordantly, we assume that the threshold is given by an alternative rather than a utility level. We further assume that the threshold alternative is randomly drawn and its realization is only observable to the decision maker. Hence our model allows for the decision maker to have different thresholds for choices made from different menus, or for choices made from the same menu at different times.====Hence our primitives are (1) a preference, (2) a threshold probability for every alternative, and (3) a list order. Faced with any menu, the decision maker randomly draws a threshold, and chooses the first listed alternative which is at least as good as the threshold. If there exists no such alternative, she abstains from choice.====Our main results are as follows. In Theorem 1, we show that our model is a special case of the famous Random Utility Models. In Theorem 2, together with Lemma 4, Lemma 6, we characterize our model using two simple axioms. The revelation of the primitives of our model is generally incomplete. In Theorem 3, we characterize a special case which rules out zero-probability thresholds. The underlying preference and list of this specialization are uniquely identifiable from choice data. Section 5 contains results which study the consequences of menu-dependence in thresholds and correlation between the underlying primitives.====At the core of our model is the idea that thresholds used by decision makers in determining ==== alternatives may be random. There is empirical evidence for this in Caplin et al. (2011). We would like to suggest two potential reasons for variation in thresholds. First, thresholds may capture a decision maker’s changing psychological state or mood. Whether or not an alternative is good enough may be subject to changes in a variety of variables such as the decision maker’s ambition level or past choices, which may not be observable. Second, many decisions take place in the presence of an unobservable outside option, which renders any alternative inferior to it unacceptable.==== ==== A combination of these two reasons may be in effect in certain situations, whereby a decision maker’s mood depends on her outside option. Furthermore these reasons apply even when stochastic choice is meant to capture a population’s behavior, as moods and outside options may change across individuals. Our axiomatic analysis fully accounts for the behavioral consequences of randomness in thresholds within a satisficing framework, without taking a stance on its sources.",Satisficing with a variable threshold,https://www.sciencedirect.com/science/article/pii/S0304406820300033,17 January 2020,2020,Research Article,235.0
"Dietrich Diemo,Shin Jong Kook,Tvede Mich","Newcastle University, 5 Barrack Road, Newcastle upon Tyne, NE1 4SE, United Kingdom,University of East Anglia, Norwich Research Park, Norwich, NR4 7TJ, United Kingdom","Received 10 July 2018, Revised 24 October 2019, Accepted 30 December 2019, Available online 15 January 2020, Version of Record 21 January 2020.",https://doi.org/10.1016/j.jmateco.2019.12.004,Cited by (1)," unless the returns on money and capital production are identical for more productive consumers. However, printing money and distributing it to young consumers increase their incomes allowing young more productive consumers to produce more capital. Consequently money printing increases output."," Simple monetary policies in form of printing money can mitigate the effects of frictions in credit markets in situations like the financial crisis where agents had limited access to credit. How money transfers are distributed is important for the effectiveness of these policies. In the present paper we study economies in which consumers cannot commit to repay loans and examine the effects on output and welfare of monetary policies in form of printing money and distributing it to agents.====We consider overlapping generations (OG) economies with production. There are four commodities: a consumption good, capital, labor and money. Consumers live for two dates, consume both when young and old, and work when young. Moreover they produce capital by use of the consumption good. Some consumers are more productive than others in production of capital. Firms produce the consumption good by use of capital and labor. Productivities of consumers are common knowledge. Therefore from an efficiency point of view less productive consumers should leave capital production to more productive consumers.====We assume that there is a friction in the credit market: consumers cannot commit to repay loans. Since consumers live for two dates, usual penalties such as exclusion from taking loans or exclusion from both taking and making loans are ineffective. Hence debt is constrained to zero. Consequently there is no transfer of resources between consumers in the same generation so both kinds of consumers can very well be investing in capital production. The presence of money does not induce less productive consumers to move their savings from capital production by themselves to capital production by more productive consumers. However printing money and distributing it to more productive consumers enable them to buy consumption goods from less productive consumers. Thereby more productive consumers produce more capital and less productive consumers hold more money. Hence printing money mitigates the effects of the debt constraints.====Equilibria are prices, consumption plans and production plans such that consumers maximize utilities, firm maximize profits and markets clear. Money need not be valuable in equilibrium. Steady states are equilibria where consumption and production are constant across dates. In Theorem 1 we show that there are steady states where money is valuable. To study the impact of changing monetary policy from the passive policy with no transfers to an active policy with non-trivial transfers we consider a scenario with the passive policy before some date and an active policy after that date. There are two steps: first in Theorem 2 we show monetary steady states with active monetary policies exist and active monetary policies increase output compared to the passive monetary policy; and, second we show that output increases in the transition from a steady state with the passive policy to a steady state with the active policy. Active policies have several effects on consumers: additional income for consumers who receive transfers as well as higher wages and lower returns on savings for all consumers.====Our results in Theorem 1, Theorem 2 are obtained under reasonably general assumptions. In Section 5 we study an example to further illustrate our findings as well as the welfare implications of monetary policies. Consumers consume only when old. Thereby the negative effect of monetary policies on the returns on savings is maximized. Consumption goods are produced by use of Cobb–Douglas production functions. Scenarios with transfers to all young consumers and transfers solely to more productive young consumers are considered. If the elasticity of capital is larger than the fraction of more productive consumers, then printing money increases welfare of all consumers in both scenarios.====Returns on money and producing capital are determined in equilibrium. Unless these returns are equal for the more productive consumers, the two kinds of consumers face different returns on savings. Therefore it is possible to improve welfare by changing consumption at any two consecutive dates without changing production at any date or consumption at any other date than the two consecutive dates. In general, monetary policies in form of printing money and distributing it to young consumers cannot make returns equal. However these policies are simple and mitigate the real effects of debt constraints. Monetary policies involving money taxes eliminate the need for printing money, but enforcement of tax payment is needed. Hence institutions enforcing payment of taxes have to be available.==== Our model is a degenerate example of a model with endogenous debt limits in which debt limits are zero because consumers live for two dates. However if consumers lived longer and the penalty for not repaying loans was exclusion from both taking and making loans, then debt constraints would become endogenous and depend on future prices. Kehoe and Levine (1993) study pure exchange optimal growth economies where consumers cannot commit to repay loans and defaulting consumers are excluded from future trading in financial markets.====Kocherlakota (2009), Farhi and Tirole (2012), Liu and Wang (2014) and Miao and Wang, 2012, Miao and Wang, 2018 consider economies with endogenous debt constraints where some assets may be used as collateral. The assets used as collateral vary from worthless land to output of and shares in firms taking loans. The main finding is that bubbles in the assets used as collateral can mitigate the effects of debt constraints.====Kunieda (2008) and Martin and Ventura, 2012, Martin and Ventura, 2016 consider OG economies with production where consumers differ with respect to their productivity and face debt constraints. In Kunieda (2008) there is a continuum of different productivities and dynamic efficiency of steady-state equilibria is studied. With constant nominal money supply, too few consumers engage in capital production and too many consumers hold money. Monetary policies in form of distributing money to less productive old consumers work in that they decrease the return on holding money, so some of the less productive consumers move their savings to capital production from money. It is shown that there is a monetary policy that achieves constrained dynamic efficiency. However, the policy is not a Pareto improvement compared to constant money supply. Most of the analysis is carried out with Cobb–Douglas economies and focuses on steady states. We study a different channel through which monetary policy can work. Indeed, we show monetary policies in form of distributing money to young consumers work in that they enable more productive young consumers to produce more capital. The channel is that the monetary transfers increase incomes of more productive young consumers so they increase their savings. Since more productive young consumers have a comparative advantage in capital production, they use their increases in savings to increase their engagement in capital production. Most of the analysis is carried out for general economies and focuses on transitional dynamics as well as steady states.====In Martin and Ventura, 2012, Martin and Ventura, 2016, there are two productivities and bubbles on assets are studied. Bubbles are stochastic in that old bubbles may die and new bubbles may arise. These bubbles can be interpreted as bubbles in financial assets like shares, but they cannot be interpreted as money because they affect some assets and not all assets. Such bubbles affect the value of assets used as collateral in credit transactions. Their welfare analysis focuses primarily on steady states. In terms of policy, they look at a possible macro-prudential role of a Lender of Last Resort, which guarantees credit when collateral is scarce and taxes credit when it is excessive. While we consider a similar mechanism, we study equilibria where money is a deterministic bubble that is controlled by monetary policy. Also, our setup is more general and we analyze the existence of equilibria and the real effects of monetary policy in the steady state and in transition. Our aim is to provide a rationale for an active monetary policy.====One strand of the literature on the role of money emphasizes differences in liquidity of financial assets. Kiyotaki and Moore (2019) consider optimal growth economies with firms operated by entrepreneurs under endogenous debt constraints. Opportunities to produce capital arrive randomly. Entrepreneurs can finance production of capital by selling their portfolio of money and equity in other firms or by issuing equity. However they can sell only a fraction of their equity in other firms and commit to pay their shareholders only a fraction of future returns on capital. The fraction of future returns on capital that can be used as collateral varies randomly. Printing money and distributing it like helicopter drops have no real effects.====Another strand is formed by the New Monetarist literature. There, micro-founded search-theoretical models of money are often used to evaluate monetary policies. For these models Gu et al. (2016) find that money and credit are perfect substitutes and that this finding is robust to variations in policy, debt limit and use of collateral. Chiu and Molico (2010) find that mildly expansionary redistributive monetary policy can potentially be welfare improving by relaxing the liquidity constraint of some agents. Williamson (2008) finds that market segregation, where some agents have access to money markets and other do not, implies that printing money and distributing it through money markets have distributional effects. In our economies, the presence of money is not a perfect remedy for debt constraints, and the presence of perfectly functioning credit markets would not necessarily eliminate dynamic inefficiency of equilibrium allocations. Consequently money and credit are not perfect substitutes.====Sorger (2005) considers stability and determinacy of monetary steady states in OG economies with homogeneous consumers and without financial frictions. Monetary policy is described by inflation targeting or inflation forecast targeting. Unlike conventional wisdom, it is shown that active inflation forecast targeting can lead to indeterminacy. Monetary policy is implemented by printing money and distributing it to consumers when old. We consider heterogeneous consumers who cannot commit to repay debt. Monetary policy that distributes money to young consumers is shown to be welfare-improving. We show that real outcomes can be different for the same inflation rate, and that the real effect of printing money depends on how it is distributed.==== In Section 2 our model is introduced. In Section 3 equilibria, steady states and equilibrium dynamics are defined and studied for economies without money printing. In Section 4 the effects of printing money and distributing it to young more productive consumers are studied. In Section 5 an example is considered and the output and welfare effects of monetary policies consisting of printing money and distributing it to young consumers, both more productive and less productive, are worked out. Section 6 contains some final remarks.",Debt constraints and monetary policy,https://www.sciencedirect.com/science/article/pii/S0304406820300021,15 January 2020,2020,Research Article,236.0
"Chatterji Shurojit,Liu Peng","School of Economics, Singapore Management University, Singapore,Faculty of Economics and Management, East China Normal University, Shanghai 200062, China","Received 19 June 2019, Revised 22 November 2019, Accepted 31 December 2019, Available online 14 January 2020, Version of Record 18 January 2020.",https://doi.org/10.1016/j.jmateco.2019.12.003,Cited by (6),"We propose a model studying the random assignments of bundles with no free disposal. The key difference between our model and the one where objects are allocated (see Bogomolnaia and Moulin (2001)) is one of feasibility. The implications of this difference are significant. Firstly, the characterization of sd-efficient random assignments is more complex. Secondly, we are able to identify a preference restriction, called essential monotonicity, under which the random serial dictatorship rule (extended to the setting with bundles) is equivalent to the probabilistic serial rule (extended to the setting with bundles). This equivalence implies the existence of a rule on this restricted domain satisfying sd-efficiency, sd-strategy-proofness, and equal treatment of equals. Moreover, this rule only selects random assignments which can be decomposed as ==== of deterministic assignments.","We study the problem of allocating a finite set of objects to a finite set of agents, where money transfers are prohibited and each agent receives a bundle of objects. Each object has a certain number of identical copies. We refer to this number as the capacity of the object and require that it be smaller than the number of agents. A bundle is a subset of objects that contains at most one copy of each object. In order to preserve fairness, we adopt randomization in allocations.==== ====A central assumption in earlier studies on random assignments is that each agent gets at most one object.==== ==== However, for many relevant applications, it is more appropriate to allocate objects in bundles. One reason is that complementarity may require allocation in bundles in order to improve efficiency. Another reason is that the total number of objects may be more than the number of agents and free disposal may not be acceptable, as for instance is the case when a number of tasks exceeding the number of agents have to be accomplished. In this paper, we propose a model studying the random assignment of bundles in the absence of free disposal; by requiring that all copies of every object be allocated, we distinguish our model from that of other studies, for example, the course allocation problem, where seats of a course may be freely disposed of.==== ==== Another feature of our model is that we do not associate with objects priorities over agents, which distinguishes our model from the literature on school choice where priorities are essential (see Abdulkadiroğlu and Sönmez (2003)).====Under our formulation, a random assignment can be identified with a plane stochastic matrix which reflects the feature that feasibility in our model requires that groups of columns sum to fixed integers in a combinatorial fashion. The consequences of this more complex feasibility requirement are that not every random assignment is decomposable as a lottery over deterministic assignments, and that furthermore, the characterization of efficient random assignments for the setting of object allocation, (see Bogomolnaia and Moulin (2001)), is no longer valid.==== ====Our first result is a characterization of sd-efficiency using a condition called unbalancedness. We next extend the Probabilistic Serial rule (henceforth the PSB rule) and the Random Serial Dictatorship rule, (henceforth the RSDB rule) to the setting with bundles and observe that the RSDB rule satisfies sd-strategy-proofness, decomposability, and equal treatment of equals but violates sd-efficiency, while the PSB rule satisfies sd-efficiency and equal treatment of equals but violates sd-strategy-proofness and decomposability. As our principal finding, we identify a particular domain of preferences, the domain of essentially monotonic preferences, for which there exists a random assignment rule which selects only decomposable assignments and which satisfies additionally sd-strategy-proofness, sd-efficiency, and equal treatment of equals. Finally we demonstrate the non-existence of a sd-strategy-proof, sd-efficient, and sd-envy-free rule on the universal domain under a technical condition.==== ====In order to define the domain of essentially monotonic preferences, we first let agents sequentially take bundles which contain exactly one copy of each object that is still available. The set of bundles so dispensed is referred to as critical.==== ==== Essential monotonicity requires that a bundle contained in a critical bundle is less preferred to this critical bundle. In order to show the existence of a desirable random assignment rule, we show that the PSB and the RSDB rules are equivalent on the essentially monotonic domain and furthermore, degenerate to a constant rule. Given the disparity these two rules display in the setting with objects, their equivalence comes as a surprise as does the fact that sd-efficiency is preserved by a simple constant assignment on such a large domain, a finding not common in mechanism design.====The paper is organized as follows. Section 2 defines formally the model and axioms, the RSDB rule and the PSB rule, and introduces the domain of essentially monotonic preferences. Section 3 contains our results. Some discussions and examples that are omitted from the main exposition are gathered in Section 4. Section 5 contains all proofs except the proof of a Lemma which is of independent interest and appears in the Appendix.",Random assignments of bundles,https://www.sciencedirect.com/science/article/pii/S030440682030001X,14 January 2020,2020,Research Article,237.0
Noda Shunya,"Vancouver School of Economics, the University of British Columbia. 6000 Iona Drive, Vancouver, BC, V6T 1L4, Canada","Received 26 August 2019, Revised 1 December 2019, Accepted 9 December 2019, Available online 24 December 2019, Version of Record 28 December 2019.",https://doi.org/10.1016/j.jmateco.2019.12.001,Cited by (1),"We study the size of matchings (expected number of agents matched to some objects) generated by random mechanisms in the assignment problem. We show that no mechanism that satisfies two weak axioms, weak truncation robustness and weak regularity, achieves an approximation ratio better than ====) have the best approximation ratio among a broad class of mechanisms.","The ====, which is defined as the expected number of agents who are matched to some objects, is often an important policy concern. Matching size is a good measure for efficiency and fairness of mechanisms because it indicates the number of agents who receive extra utility from the allocation problem. Furthermore, when the assignment of goods exhibits positive externalities, the matching size itself can be a primary objective of mechanism design.====Indeed, in many real-world assignment problems, the institution attempts to (approximately) maximize the size of a matching. Maximization of the number of transplantation has been regarded as a primary design objective of the kidney exchange market (Roth et al., 2004). In the refugee resettlement problem, the institution wants to increase the total number of resettlements because it is socially undesirable to keep many refugees in temporary shelters (Andersson and Ehlers, 2016, Delacrétaz et al., 2016, Noda, 2018). Childcare services are rationed by the government using matching mechanisms in many countries. Due to a large excess demand, there often exist a large number of children who are waiting for childcare slots. In Japan, this leads to a large protest movement, and the government promised to improve the situation by increasing allocation of childcare slots (Kamada and Kojima, 2018).====This paper studies the assignment problem (also known as the one-sided matching problem), in which the central planner assigns a set of objects to a set of agents. Agents’ preferences over objects are assumed as private information. Some agents may insist that some objects are unacceptable to them. The central planner is not allowed to assign objects that are declared to be unacceptable (i.e., the assignment is required to be individually rational). Accordingly, to achieve a large matching size, the central planner must collect truthful information about the set of acceptable objects for each agent. Hence, the central planner must design a mechanism that satisfies strategy-proofness or other notion of incentive compatibility to induce truthful reporting about preferences.====Unfortunately, matching size and strategy-proofness are often in tension. If the mechanism always returns a large matching, agents are tempted to truncate their reported preferences to obtain a more preferred object. To see this, imagine that an agent is facing a mechanism that always attempts to maximize the matching size. Since the mechanism is required to be individually rational, by truncating his reported preference, an agent can make sure that he will not be assigned less preferred (but acceptable with respect to his truthful preference) objects. If the mechanism is designed to generate a maximum matching, it will instead assign the agent a more preferred object so as to increase the matching size. Accordingly, by truncating his reported preference, agents can increase the probability that they will be assigned more preferred objects (at the expense of the probability of being assigned less preferred but acceptable objects). Such manipulation is profitable under some cardinal preferences. Hence, the maximization of the matching size is not compatible with strategy-proofness. To induce truthful reporting, the mechanism must be designed to be robust against such preference truncation.==== ====Our aim in this paper is to characterize the trade-off between matching size and preference truncation. We evaluate mechanisms by their worst-case performance. The ==== of a mechanism is defined as the worst ratio of the matching size achieved by the mechanism to the maximum feasible size (which can be achieved if preferences were publicly observed).==== ==== In other words, if the approximation ratio of a mechanism is ====, then the mechanism always returns a matching whose size is not smaller than a factor ==== of the first-best. The worst-case analysis is a popular way to evaluate the performance of matching mechanisms. For example, Krysta et al. (2014) prove that the well-known random serial dictatorship mechanism (RSD) has an approximation ratio of ====, and Bogomolnaia and Moulin (2015) show that the probabilistic serial mechanism (PS, invented by Bogomolnaia and Moulin (2001)) achieves the same ratio.====This paper shows that the approximation ratio of ==== is not improvable. Specifically, we show that no mechanism satisfying ==== and ==== can achieve an approximation ratio better than ====.====The first axiom, weak truncation robustness, is originally introduced by Hashimoto et al. (2014). It requires that agents cannot obtain gain via truncation of preference; i.e., if preference ==== is obtained from ==== by shrinking the list of acceptable objects while preserving the relative rankings of those objects that remain acceptable, agent ====’s probabilistic assignments from ==== and ==== for still acceptable objects are always the same. Weak truncation robustness only requires that agents are not incentivized to truncate (or elongate) their preferences. Accordingly, it is weaker than (i.e., implied by) standard strategy-proofness, which discourages agents from taking any kind of misreporting. This property is necessary for collecting truthful information about the set of acceptable objects for each agent.==== ====The second axiom, weak regularity, is introduced by Liu and Pycia (2016). It requires that if the fraction of agents who changed their reports is small, the fraction of agents whose probabilistic assignments are significantly changed is also small. Roughly speaking, weak regularity requires the mechanism to be “continuous” in its input. Weak regularity is innocuous in that almost all of known mechanisms (including non strategy-proof ones) satisfy this axiom.====These two axioms are actually weak. Many standard mechanisms, including random and deterministic serial dictatorship mechanisms, top trading cycle mechanisms (Shapley and Scarf, 1974), the proposer-side of deferred acceptance mechanisms (Gale and Shapley, 1962), hierarchical exchange mechanisms (Pápai, 2001), Boston mechanisms, and PS, satisfy these two axioms. (Note that Boston mechanisms and PS are not strategy-proof but weakly truncation robust.) Furthermore, since the information about the set of acceptable objects is crucial for generating a large matching, weak truncation robustness cannot be relaxed further if preferences are private information and the central planner concerns the matching size. Therefore, our result indicates that we cannot hope for an approximation ratio better than ==== as long as preferences are private information. Accordingly, among a very broad class of mechanisms, RSD and PS are proven to be one of the best mechanisms for generating a large matching with respect to the worst-case criterion.==== ====The proposed upper bound is novel to the literature. Krysta et al. (2014) prove that (i) no Pareto efficient and strategy-proof mechanism achieves an approximation ratio larger than ====., and (ii) if a mechanism additionally satisfies non-bossiness, an approximation ratio cannot be larger than ====. Taking weak regularity as innocuous, this paper provides an approximation ratio from a weaker axiom in the following sense: (i) our upper bound, ====, is tighter than ====, (ii) we require neither Pareto efficiency nor non-bossiness, and (iii) weak truncation robustness is implied by strategy-proofness. Bogomolnaia and Moulin (2015) show that no envy-free mechanism achieves an approximation ratio better than ====. This paper complements their results by showing that the same upper bound can also be derived from weak truncation robustness, which is another property of PS.",Size versus truncation robustness in the assignment problem,https://www.sciencedirect.com/science/article/pii/S0304406819301211,24 December 2019,2019,Research Article,238.0
"Hodoshima Jiro,Miyahara Yoshio","Faculty of Economics, Nagoya University of Commerce and Business, 4-4 Sagamine, Komenoki-cho, Nisshin-shi, Aichi, 470-0193, Japan,Graduate School of Economics, Nagoya City University, 1 Yamanohata, Mizuho-cho, Mizuho-ku, Nagoya 467-8501, Japan","Received 29 July 2019, Revised 9 December 2019, Accepted 9 December 2019, Available online 17 December 2019, Version of Record 23 December 2019.",https://doi.org/10.1016/j.jmateco.2019.12.002,Cited by (8),A performance index based on the economic index of riskiness by Aumann and Serrano (2008) can be derived from an index based on the utility indifference price with the exponential utility function. The exponential utility function is a special utility function and relevant when the associated investor is risk averse as well as risk loving. The index based on the utility indifference price with the exponential utility function becomes an index for the random variable ==== of gambles with the property ==== and ==== when the investor is risk averse and an index for the random variable ==== of gambles with the property ==== and ==== when the investor is risk loving. We provide sufficient conditions for the existence and uniqueness of the index when the investor is risk averse and risk loving.,"Aumann and Serrano (2008) defined an economic index of riskiness for ==== where gambles stand for assets, cash flows, projects, etc. with uncertain outcomes. The concept of the economic index of riskiness by Aumann and Serrano (2008) has recently received a lot of attention in the financial economics literature (cf., e.g., Foster and Hart, 2009, Hart, 2011, Homm and Pigorsch, 2012a, Kadan and Liu, 2014, Schulze, 2014). Kadan and Liu (2014) considered a performance index based on the riskiness index by Aumann and Serrano (2008) and demonstrated the Aumann and Serrano (AS) performance index can take into account high moments and disaster risk to shed new light on various issues of financial problems. In their seminal paper of the AS riskiness index, Aumann and Serrano considered the random variable ==== of gambles with the property ==== and ==== and proved the existence and uniqueness of the AS riskiness index. So far, applications of the AS riskiness and performance indexes have been confined to gambles with ==== and ====. It is fair to say that studies originating from the AS index have been quite successful.====Certainly there exist plenty of these gambles. However, there also exist other gambles which do not have the above property. We consider the random variable ==== of gambles with ==== and ==== in this study. For example, commercial lotteries and gambles in casinos are appropriate to be categorized as gambles with ==== and ====. Projects such as oil and natural gas mining and investments in venture capital would be other examples of such gambles where it is difficult to expect to make money but there exist chances of a big success. There are also many uncertain cash flows and projects appropriate to be described as such gambles. That is, there seem to be plenty of gambles in the real world with the property ==== and ====. It may be appropriate to call gambles with the property ==== and ==== speculative or highly risky since ordinarily only risk loving investors would be interested in such gambles. The AS riskiness index excludes the important class of these gambles. The main reason why the AS riskiness index fails to take into account gambles with ==== and ==== is that they only consider risk-averse investors. However, there are risk loving investors who are willing to take risks in such highly risky gambles. Then, it seems relevant to provide an index of such gambles. In fact, a study of gambles with negative expected value was suggested in Aumann and Serrano (2008) as a topic of future research. So far, however, to the best of our knowledge no studies have been undertaken of gambles with ==== and ====. Any proper index of gambles should treat not only the class of gambles with ==== and ==== but also the class of gambles with ==== and ====. In this paper, we aim to fill the gap in the literature and provide an index for gambles with ==== and ====. In fact, ==== seem to be more often meant to be cash flows with the property ==== and ====, such as commercial lotteries and games in casinos in the real world. Therefore, it seems appropriate to extend gambles to include those with ==== and ==== in order to make the index more practical. In this study, we intend to consider such gambles from a different point of view.====Kadan and Liu (2014) defined new performance indexes as the reciprocal of the riskiness index by Aumann and Serrano (2008) and Foster and Hart (2009). In this paper, we only consider the AS performance index, defined as the reciprocal of the riskiness index of Aumann and Serrano (2008). The AS performance index is given as the solution ==== of the following implicit equation given by ====for a gamble ====. Kadan and Liu (2014) considered only a positive ==== as the original AS riskiness index ==== of Aumann and Serrano (2008) was defined in positive region.====Although the AS riskiness index has had a significant influence on subsequent studies in the financial economics literature, its connection has received little attention with other methods of evaluation such as certainty equivalence and utility indifference pricing in the literature of the so-called expected-utility approach. These methods are known as promising methods of evaluation of gambles in the expected utility approach (cf., e.g., Carmona, 2009). Miyahara (2014) independently proposed a performance index named as the inner rate of risk aversion (IRRA) that makes the utility indifference price with the exponential utility function ==== zero where ==== denotes the degree of risk aversion. The IRRA of a gamble ==== is given by the solution ==== of the implicit equation ====The utility indifference price of a gamble ==== is defined to be the solution ==== of the equation ==== where ==== denotes a utility function and ==== denotes expectation. When the utility function ==== is increasing, if ====, then ==== where ==== denotes the utility indifference price of ==== (i ====) where ==== (i ====) denotes a gamble. The property of ==== if ==== is called monotonicity, which a suitable evaluation function should satisfy. Hence, the utility indifference price is a value measure or a suitable evaluation function of gambles taking values corresponding to the value of gambles as we show below. The utility indifference price of a gamble ==== with the above exponential utility function is easily seen to be given by ====since ====We remark the above exponential utility function is the only utility function among ====-class of utility functions under certain conditions (cf. Theorem 3.2.8 of Rolski et al., 1999 for its proof and Proposition 2 of this study given below), where ====-class is the class of functions that possess continuous second derivatives. Therefore, the exponential utility function ==== is a special utility function. Miyahara (2010) proved the utility indifference price, in general, satisfies a desirable criterion called a ==== when the utility function is increasing and concave. And he named the above utility indifference price with the exponential utility function a ==== (RSVM) since it is sensitive to loss of the underlying gamble. The RSVM is equivalent to certainty equivalence and an evaluation by expected utility (cf. Miyahara, 2017) when the underlying utility function is ==== where certainty equivalence of ==== is defined by the solution ==== of the implicit equation ====. Therefore, the RSVM provides an evaluation conformable to expected utility, which implies the evaluation by the RSVM is desirable and proper. The method of evaluating assets, cash flows, projects, etc. with uncertain outcomes using the RSVM was introduced by Miyahara (2010) and developed in Miyahara, 2014, Miyahara, 2017. Miyahara restricted the degree of risk aversion ==== to be positive when he considered the IRRA and RSVM in Miyahara, 2010, Miyahara, 2014 since he was only concerned with risk-averse investors, where the underlying utility function is given by the above exponential utility function. However, the above exponential function continues to be a utility function when the degree of risk aversion ==== is negative, i.e., when an investor is risk loving. The exponential utility function is a convex utility function and hence its associated investor is risk loving when ====. And it is a concave utility function and hence its associated investor is risk averse when ====. The utility indifference price is to be defined for a utility function regardless of its risk preference, i.e., whether an investor is risk averse or risk loving. Thus it is quite natural to extend the Miyahara’s performance index IRRA originally defined when ==== to when ====. The IRRA based on the utility indifference price with the same exponential utility function when the degree of risk aversion ==== is negative, i.e., when the associated investor is risk loving, is the one we consider in this study as the performance index of gambles with the property ==== and ====. The IRRA continues to be a performance index of gambles when ==== as we show in this study. The IRRA was already applied in some areas (cf., Ban et al., 2016, Furukawa et al., 2018, Hodoshima, 2019) assuming the underlying investor to be risk averse. One may call the utility indifference price with the exponential utility function when the degree of risk aversion is negative a ==== since it is sensitive to gain of the underlying gamble instead of loss of the underlying gamble.====A sufficient condition of the existence and uniqueness of the positive IRRA given by Miyahara (2014), where the underlying investor is assumed to be risk averse, is given as follows:==== ==== ==== ==== ==== ====which is different from the sufficient condition ==== and ==== for finite gambles (cf. Aumann and Serrano, 2008), the sufficient condition for non-finite gambles given by Homm and Pigorsch (2012b), and the necessary and sufficient condition by Schulze (2014) for no-finite gambles. We remark Miyahara did not restrict gambles to finite gambles and considered non-finite gambles which include finite gambles. The assumption of finite gambles in Aumann and Serrano (2008) is replaced by the existence of the MGF in Miyahara (2014). The condition of the existence of the MGF in Miyahara (2014) is also different from the condition of the MGF in Homm and Pigorsch (2012b) and the necessary and sufficient condition in Schulze (2014).====We provide in this paper a sufficient condition for the negative IRRA, which is obtained using the relationship between concavity and convexity, where the underlying investor is assumed to be risk loving. The sufficient condition for the negative IRRA we present in this paper is given as follows:==== ==== ==== ==== ==== ====We remark the IRRA depends on the gamble only, i.e., on its distribution only, not on any other parameters such as the utility function of the decision maker or his wealth. This is the same as in the AS performance index.====In the above definition of the IRRA, when the IRRA ==== is not zero, the implicit equation of the IRRA is equivalent to the following equation: ====which is equal to the implicit equation of the AS performance index, i.e., equation (1) of Kadan and Liu (2014). This shows the IRRA is equivalent to the AS performance index, whether ==== is positive or negative. This is a further justification of the AS performance index. In other words, the AS performance index can be derived from the proper methods in the expected-utility approach, i.e., utility indifference pricing, certainty equivalence, and expected utility maximization, when the underlying utility function is the exponential utility function.====It is natural to extend the IRRA to the negative region since the underlying exponential utility function continues to be valid when ==== is negative as well as positive. As a result, we also enlarge the AS riskiness index to gambles with the property ==== and ==== since the AS performance index is the reciprocal of the AS riskiness index (cf. Kadan and Liu, 2014). We provide empirical examples of the negative IRRA, which indicate examples of the negative IRRA are abundant.====The rest of the paper is organized as follows. Section 2 presents definitions and properties of the RSVM, i.e., the utility indifference price with the exponential utility function. Section 3 presents a definition of the IRRA and a sufficient condition of the existence and uniqueness of the negative IRRA. Section 4 states the relationship between the IRRA and AS performance index. Section 5 presents empirical examples of the negative IRRA. Section 6 presents concluding comments.",Utility indifference pricing and the Aumann–Serrano performance index,https://www.sciencedirect.com/science/article/pii/S0304406819301223,17 December 2019,2019,Research Article,239.0
"Núñez Marina,Rafels Carlos,Robles Francisco","Departament de Matemàtica Econòmica, Financera i Actuarial and BEAT, Universitat de Barcelona, Av. Diagonal, 690, 08034 Barcelona, Spain,Departamento de Economía, Universidad Carlos III de Madrid, calle Madrid, 126, 28903 Getafe, Spain","Received 11 April 2019, Revised 6 November 2019, Accepted 9 November 2019, Available online 16 December 2019, Version of Record 31 December 2019.",https://doi.org/10.1016/j.jmateco.2019.11.002,Cited by (0),"We consider a package allocation problem in which a seller owns many indivisible objects and the rest of the agents, buyers, are interested in packages of these objects. Buyers’ valuations satisfy monotonicity and the gross substitutes condition (Kelso and Crawford, 1982). The aim of this paper is to analyze the following mechanism: simultaneously, each buyer requests to the seller a package by announcing how much he would pay for it; once buyers have played, the seller decides the final assignment of packages and the prices, as long as this assignment makes no buyer worse off than with his initial request. The subgame perfect equilibrium outcomes of the mechanism correspond to the Vickrey outcome (Vickrey, 1961) of the market."," are a subclass of resource allocation problems and commonly deal with situations where a set of buyers wish to acquire several indivisible objects from one seller. See for instance Bikhchandani and Ostroy (2002), Milgrom (2007) and Day and Milgrom (2008). In this paper, we approach the package allocation problem assuming that all parties take an active role in the allocation problem. This could be the case in the dissolution of a private company, where the main shareholder sells part of her/his stock to other shareholders.====We consider a situation where the seller owns many indivisible objects on sale and each buyer wants to buy a package of objects and has a non-negative valuation for each package. As usual in package allocation problems, preferences are assumed to be quasi-linear with respect to money and buyers’ valuations satisfy monotonicity and the gross substitutes condition.==== ==== An outcome for this allocation problem specifies an assignment of the objects to the buyers and the payment each buyer makes for his assigned package of objects.====We study the strategic interaction of all agents, by means of a simple mechanism we introduce. The mechanism works as follows: first, each buyer requests (for instance, bidding in a sealed envelope) a package he would like to buy and how much he would pay for it; second, the seller decides the final allocation of packages and their prices.====A requirement for allocating objects is efficiency. When buyers request packages of objects simultaneously, and the seller is restricted to choose only among requested packages, an overlapping problem may arise. As a consequence, the outcome of this equilibrium may not be efficient and do not belong to the core. In order to avoid this problem, in our mechanism, the seller is allowed to allocate non-requested packages as long as this does not make any buyer worse off than with his initial request. Our main result is that in any subgame perfect equilibrium (SPE), the final allocation of the objects is efficient and every SPE outcome coincides with the Vickrey outcome of the market.====Our work is related to Bernheim and Whinston (1986), where a set of completely informed buyers want to buy packages of heterogeneous objects. In the mechanism they propose, each buyer reports how much he would pay for each package and the auctioneer chooses an allocation of the packages. If a buyer receives a package, then he pays his bid. This game has multiple equilibria, some of them non-efficient. To overcome that, the authors restrict the strategies of the buyers, the so-called truthful strategies, to obtain Nash equilibria with good properties. In our mechanism all SPE lead to an efficient and core outcome, which is the Vickrey outcome, at the cost of assuming the seller plays a more active role.====The mechanism we propose is also inspired in the two-phase buying and selling procedure for assignment games introduced in Pérez-Castrillo and Sotomayor (2002), in the setting of the Shapley and Shubik (1972) assignment game, to implement the most favorable core allocation for the sellers. Both mechanisms have in common that there are two sides of the market, one side acts first simultaneously and the second side acts later sequentially. In their paper sellers act first setting prices for their objects (one object each), and buyers act later sequentially determining the matching (that assigns at most one object to each buyer). In our case the main differences are that buyers are willing to buy packages of objects and act in first place (each buyer demands a package at a price) and there is only one seller that owns all objects and acts secondly to determine who gets what. As in Pérez-Castrillo and Sotomayor (2002), the sector that moves first (in our case the buyers) has an advantage since the other sector has only the freedom to determine the matching. We also obtain that the sector that acts first obtains the maximum possible core payoff.====To sum up, the paper is organized as follows. The next section is devoted to an introduction of the market and the coalitional game associated with it. In Section 3, the mechanism is defined: Theorem 4 proves that any SPE produces an efficient allocation and finally Theorem 6 shows that the payoff of any SPE is the Vickrey payoff vector. The Appendix contains some technical lemmas needed to establish the main results.",A mechanism for package allocation problems with gross substitutes,https://www.sciencedirect.com/science/article/pii/S0304406819301107,16 December 2019,2019,Research Article,240.0
"Pham Ngoc-Sang,Pham Thi Kim Cuong","Montpellier Business School–Montpellier Research in Management, 2300 Avenue des Moulins, 34080 Montpellier, France,University of Strasbourg, CNRS, BETA, 61, avenue de la Forêt Noire, 67085 Strasbourg, France,FERDI, Clermont-Ferrand, France","Received 10 October 2018, Revised 18 April 2019, Accepted 13 November 2019, Available online 25 November 2019, Version of Record 5 December 2019.",https://doi.org/10.1016/j.jmateco.2019.11.004,Cited by (3),"We introduce an infinite-horizon ==== framework for studying the effects of foreign aid on the economic growth in a recipient country. Aid is used to partially ==== the recipient’s public investment. We point out that the same rule of aid may have very different outcomes, depending on the recipient’s circumstances in terms of development level, domestic investment, efficiency in the use of aid and in public investment, etc. Foreign aid may promote growth in the recipient country, but the global dynamics of equilibrium are complex (because of the non-monotonicity and steady state multiplicity). The economy may converge to a steady state or grow without bounds. Moreover, there are rooms for the divergence and a two-period cycle. We characterize conditions under which each scenario takes place. Our analysis contributes to the debate on the nexus between aid and economic growth and in particular on the ==== of aid effects.","Since the United Nations Summit in September 2000 at which the Millennium Development Goals (MDGs) were agreed, foreign aid, in particular, Official Development Assistance (ODA) has been continually increasing. For example, in 2015, development aid provided by the donors in the OECD Development Assistance Committee (DAC) was 131.6 billion USD, increased by 6.9% in real terms from 2014, and by 83% from 2000. At the same time, bilateral aid, provided by one country to another, risen by 4% in real terms.==== ==== Many issues are under debate regarding the effectiveness of aid in terms of economic growth, and extensive empirical investigations using different data samples show conflicting results.====On the one hand, some studies show that aid may exert a positive and conditional effect on economic growth. In a seminal paper, Burnside and Dollar (2000) use a database on foreign aid developed by the World Bank and find that foreign aid has a positive effect on growth only in recipient countries which have good fiscal, monetary and trade policies. Collier and Dollar, 2001, Collier and Dollar, 2002 use the World Bank’s Country Policy and Institutional Assessment (CPIA) as a measure of policy quality and show that aid may promote economic growth and reduce the poverty in recipient countries if the quality of their policies is sufficiently high. The findings in Guillaumont and Chauvet (2001), Chauvet and Guillaumont, 2003, Chauvet and Guillaumont, 2009 indicate that the marginal effect of aid on growth is contingent on the recipient countries’ economic vulnerability. While economic vulnerability is negatively associated with growth, the marginal effect of aid on growth is an increasing function of vulnerability.====On the other hand, other empirical studies, not rejecting the conditionality of aid effects, show a certain fragility of results and suggest a non-linear effect of aid on growth (Hansen and Tarp, 2001, Easterly et al., 2004, Islam, 2005, Roodman, 2007, Clemens et al., 2012, Guillaumont and Wagner, 2014). For example, Islam (2005) shows an aid Laffer curve in recipient countries with political stability. The effect of aid on growth may be negative at a high level of aid inflows. Hansen and Tarp (2001) find that the effectiveness of aid is conditional on investment and human capital in recipient countries and aid has no effect on growth when controlling for these variables. Their findings shed light on the link between aid, investment, and human capital and show that aid increases economic growth through its impact on capital accumulation. Using the same empirical specification like that in Burnside and Dollar (2000), but expanding the data set sample, Easterly et al. (2004) nuance the claim from that of these authors. The results on aid effectiveness seem to be fragile when varying the sample and the definition of different variables such as aid, growth and good policy (Easterly, 2003).====The aforementioned conflicting results in the literature raise a concern about the effectiveness of foreign aid. Our paper deals with this concern by investigating the following questions: (1) How does the recipient country use foreign aid to enhance economic growth? (2) What are the determinants of the effectiveness of foreign aid? (3) Why are the effects of foreign aid significant for some countries but not for others?====To address these questions, we consider a tractable discrete-time infinite-horizon growth model where public investment, which is financed by foreign aid and capital tax, may improve the total factor productivity (TFP) if it is large enough. Inspired by the empirical literature, we formulate aid flows taking into account the donor’s rules and the recipient’s need represented by its initial capital stock. In the case of a poor country, we also consider the efficiency in the use of aid and in public investment, then examine their impacts on the aid effectiveness. Our model allows us to find explicitly the dynamics of capital stocks, and provide a full analysis of equilibrium transitional dynamics. We show that if the initial circumstances of the recipient are good enough (high productivity and initial capital), the country does not need foreign aid to achieve its development goals. This result is in line with the findings in growth models with increasing return to scale. Consequently, our analysis focuses on the case in which the recipient country’s initial capital and productivity are not high. The main results can be described as follows:====First, when the foreign aid is very generous and/or the use of aid is efficient, and the recipient country has a high quality of circumstances (high and efficient public investment and/or low fixed cost in public investment), then the economy will grow without bounds for any level of initial capital stock. Consequently, the country will no longer receive aid from some date on.====The second case, corresponding to the richest dynamics of equilibrium, is found when foreign aid is not very generous and the recipient has an intermediate quality of circumstances. In this case, we distinguish 2 regimes: (R1) the recipient country focuses on its domestic investment, characterized by a remarkable level of capital tax financing public investment, and (R2) it focuses on foreign aid, characterized by the fact that the use of foreign aid is sufficiently efficient. In the first regime (domestic investment focus), if the country has sufficient initial capital or/and the foreign aid is quite high, the economy can grow. Otherwise, it would collapse (i.e., the capital level tends to zero) or stay at the unique steady state. In the second regime, the transitional dynamics are complex because of the non-monotonicity of capital dynamics. The non-monotonicity is due to the fact that the country focuses on foreign aid which decreases when the economy gets better. In this regime, there are two steady states: the lower one is interpreted as a middle-income trap while the second one as a poverty trap. Let us present our findings under this regime R2.====Several empirical studies on the effects of aid and conditionality of aid effectiveness in different recipient countries may illustrate our theoretical analyses. First, South Korea offers an illustration for our results in regime R1 and regime R2.3. Indeed, this country was a recipient country during the period of 1960–1990 (after the Korean War 1950–1953) and experienced a high domestic investment during this period. South Korea is now a developed country and has become a member of the OECD-DAC (since 2010). The average aid flows have decreased during the period of 1960–1980, from 6.3% to 0.1% of GDP. It became negative at the beginning of the 1990s.====  Second, our analysis in the regime R2.1 may be illustrated by the Tunisia case where aid flows have also decreased during 1960–2003, from 8.1% of GDP in the 1960s to 1.5% during 1990–2003.==== ====From a theoretical point of view, our paper is closely related to Dalgaard (2008) who considers that aid flows depend on the recipient’s income per capita and on the donor’s exogenous degree of inequality aversion. However, there are some differences. First, Dalgaard (2008) considers an OLG growth model while we use an infinite-horizon model à la Ramsey. Second, in Dalgaard (2008), public investments are fully financed by foreign aid while in our paper public investments are financed, not only by foreign aid but also by capital tax. In Dalgaard (2008), the transitional dynamics of capital stock are totally determined by the degree of inequality aversion on the part of the donor. Our contribution is to show that the transitional dynamics depend not only on the aid rule but also on the recipient country’s characteristics. In particular, our framework allows us to study whether a poor country can surpass, not only the low steady state but also the high one and then achieve growth in the long run.====Our theoretical results complemented by a number of numerical simulations, indicate that the effects of aid (in the short run and in the long run) are complex, non-linear and conditional on recipient countries’ characteristics. By the way, our paper is related to and complements the points in Charterjee et al. (2003) and Charteerjee and Tursnovky (2007). Indeed, Charterjee et al. (2003) examine the effects of foreign transfers on the economic growth of the recipient country given that foreign transfers are positively proportional to the recipient’s GDP but are not subject to conditions. They show that their effects on growth and welfare are different according to the type of transfers, untied or tied to investment in public infrastructures. Charteerjee and Tursnovky (2007) underly the role of endogeneity of labor supply as a crucial transmission mechanism for foreign aid.====Our paper is likewise related to the literature on optimal growth with increasing returns (Romer, 1986, Jones and Manuelli, 1990, Kamihigashi and Roy, 2007) and with the presence of threshold (Azariadis and Drazen, 1990, Bruno et al., 2009, Van et al., 2010, Van et al., 2016). Different from numerous papers in this literature, we point out the role of aid which can provide investment for the least developed country, this helps the recipient country to evade poverty and potentially obtain positive growth in the long run. Moreover, policy function in our framework may not be monotonic.====The remainder of the paper is organized as follows. Section 2 characterizes the case of a small recipient country. Section 3 presents the dynamics of capital, in particular, the poverty trap without international aid. In Section 4, we emphasize the role of foreign aid by analyzing the conditions for the effectiveness of aid. Section 5 studies effects of aid in a centralized economy. Section 6 concludes. Technical proofs are presented in the Appendix.",Effects of foreign aid on the recipient country’s economic growth,https://www.sciencedirect.com/science/article/pii/S030440681930120X,25 November 2019,2019,Research Article,241.0
"De Sinopoli Francesco,Meroni Claudia,Pimienta Carlos","Department of Economics, University of Verona, Verona, Italy,School of Economics, The University of New South Wales, Sydney, Australia","Received 23 July 2019, Revised 9 October 2019, Accepted 11 November 2019, Available online 18 November 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.jmateco.2019.11.003,Cited by (0),"-player games based on an ====-player game ====. A player meets each group of ==== opponents ==== times to play ==== in alternating roles. The winner of the tournament is the player who attains the highest accumulated score. We explore the relationship between the equilibria of the tournament and the equilibria of the game ==== and confirm that tournaments provide a refinement criterion. We compare it with standard refinements in the literature and show that it is satisfied by strict equilibria. We use our tournament model to study a selection of relevant economic applications, including risk-taking behavior.","The two-player game ====has the unique Nash equilibrium ====. Suppose that two players play this game twice, simultaneously, and in alternating roles, with the objective of maximizing the sum of the payoffs they get in both matches. That is, they play the symmetric game ====If they both play ==== – i.e. ==== when in the role of player 1 and ==== when in the role of player 2 – they both get a payoff of ====. None of them has an incentive to deviate, since any other choice leads to a lower payoff, and both players playing ==== is the unique equilibrium of such a game.==== ====Consider the same situation with the difference that the numbers that represented payoffs are now just scores, a prize is awarded to the player with the highest total score, and players want to maximize the probability of winning the prize. In the event of a tie, each player is the winner with probability ====. In this case ==== is no longer an equilibrium.====  Indeed, each player has an incentive to deviate, for instance, to ====. In doing so, the deviator loses two points but also lowers her opponent’s payoff by five points, thus winning the prize with probability ====. It is easy to see that the unique equilibrium of this game is ==== and that both players win with probability ====.==== ====We can think of the same competitive situation extended to any number ==== of players, where everyone meets all her opponents twice to play the same two-player game in alternating roles and where the prize is awarded to the player with the highest accumulated score. We call such a competition “double round-robin tournament”. Furthermore, we can extend this competition model to any possible ====-player base game, so that each player plays ==== with every possible group of ==== opponents ==== times, each time in a different permutation of the ==== roles. We call such a competition “====-tuple round-robin tournament”, or simply “tournament”.====This construction is based on ==== as introduced by Arad and Rubinstein (2013) (henceforth, AR13). An ====-player round-robin tournament ==== is a simultaneous game in which every player is matched with each one of her ==== opponents once to play the ==== game ====. AR13 assume that each player uses the same action every time she meets a new opponent. In turn, we define an ====-==== (or simply ====) ==== as a simultaneous ====-player game in which every player is matched with each group of ==== opponents ==== times and, in every match, a (possibly asymmetric) ====-player game ==== is played, where ====. In the ==== matches of the same group of players, they play ==== in all the possible permutations of the ==== roles. We assume that each player always uses the same action in all the matches that she plays in a given role, independently of the set of opponents of any given match and the roles in which they play. The winner of the tournament is the player with the highest accumulated score, which is given by the sum of the scores that she obtains in all the matches she plays. Players care only about maximizing the probability of being the highest scorer in the tournament and not about maximizing the total score.==== ====Like with the opening example, throughout our analysis we often consider the simplest case in which ==== is a two-player game. To this end we also define a ==== ==== as an ====-player tournament based on a (possibly asymmetric) two-player game ====. A player plays ==== twice with each opponent, once in the role of the first player and once in the role of the second player of ====. Using sports terminology, we say that a player plays once “at home” and once “away”. Thus, each player always uses the same action in all the matches she plays at home and the same action in all the matches she plays away.====We follow AR13 and use the symmetric Nash equilibrium as solution concept, so that every player uses the same strategy that specifies which action to play in each of the ==== roles.====  The vector of actions that a player will ultimately play in each role is the outcome of a randomization (mixed strategy) executed only once and before starting the interactions within the tournament. Consider again the initial example. Under the assumptions of our model, the tournament with three players has a symmetric equilibrium in which every player chooses ==== when playing at home and ==== when playing away. For ====, instead, the symmetric Nash equilibrium of the tournament prescribes players to choose ==== at home and ==== away, which are the Nash equilibrium strategies of the original two-player game. As with AR13, this is not a coincidence. As ==== grows, deviating from the equilibrium strategies of the base game becomes less and less profitable. The potential loss inflicted to each of the other players becomes negligible compared to the score foregone by not playing a best response to the strategy used by every other player in the tournament. The consequence is that the limit as ==== goes to infinity of a sequence of equilibria of ==== must be a Nash equilibrium of ====.====However, the opposite is not true. There may be equilibria of the base game ==== that are not limit points of equilibria of tournaments as the number of players grows to infinity. Thus, we study the conditions under which equilibria of ==== are an appropriate approximation of equilibria of the tournament. In this way, we extend to any ====-player (and possibly asymmetric) game ==== the criterion that selects a subset of Nash equilibria as “a stable distribution of actions over a large population of individuals who are occasionally matched to play ==== […] in environments where individuals are interested in maximizing ====” AR13, p.33. We call these equilibria “tournament-stable”.====The standard literature on equilibrium refinements has provided several solution concepts that restrict the set of Nash equilibria in order to satisfy rationality requirements, e.g. existence, admissibility, iterated dominance, backward and forward induction (see van Damme, 1991 for a survey). We compare the criterion provided by tournaments with some of these solution concepts. We show that strict equilibria (i.e. Nash equilibria such that, for every player, every deviation implies a strict loss) are always tournament-stable. We then present two classic economic applications, the ultimatum game and the entry game, in which tournaments and prominent equilibrium refinements lead to different results. In the ultimatum game the set of tournament-stable equilibria appears to be too restrictive, as it does not contain the equilibrium in which the first player makes an unfair proposal and the second player accepts it, which is a Kohlberg and Mertens (1986) stable set as a singleton.====  However, in the entry game, the concept of tournament-stable equilibrium proves to be too weak, as it sustains all the dominated Nash equilibria in which the potential entrant stays out of the market because the incumbent is going to behave aggressively with some positive probability.====AR13 list different interpretations of their tournament model. We note that most of them can be extended to also motivate our tournaments, with the additional advantage of being able to study any asymmetric interaction. According to the straightforward interpretation, players can participate in a tournament where changing action from match to match is costly enough so that they have to use the same action in all the matches. Double round-robin tournaments, for instance, are common in several sports competitions, where each team plays against every other team home and away. In such competitions, the coach, players and tactics have to be decided before the season starts and matches are typically asymmetric since the home team has the home court advantage. According to the social interpretation, agents can be occasionally involved in asymmetric interactions and only care about being the best achiever. Often, they play alternating roles in such interactions. For instance, they are both buyers and sellers, give and make interviews, make or receive the first offer when bargaining, etc. Finally, individuals can be randomly matched to interact in a biological contest. This is typically asymmetric, since individuals differ in qualities such as size, age, fighting ability, etc., which affect their relative probability of winning an isolated contest as well as how much they have to gain from it. The asymmetry can also be arbitrary and arise from the fact that one of the contestants arrives at the resource first and becomes the owner. Behavior can be conditioned on such asymmetries and can be summarized by a strategy that is genetically determined such as “if you are the owner of the resource attack, if you are the intruder retreat” (Maynard Smith and Parker, 1976). Evolution selects only the highest achieving strategies and eliminates the rest.====As argued by AR13, the refinement criterion provided by tournaments may be more appropriate to such an evolutionary interpretation of equilibrium. Indeed, every Evolutionary Stable Strategy is robust to AR13’s tournament model. The fact that evolution only selects the best performing strategies intuitively encourages risky behavior. Similarly to AR13, we also study whether tournaments favor risky strategies. We do this using a novel and very natural framework in which tournaments turn out to be an extremely useful refinement criterion. We consider a two-player game in which player ==== has to choose between a safe outside option or entering a constant sum game against player ====, whose Nash equilibrium payoff to player ==== coincides with that of the outside option. We demonstrate that the tournament structure incentivizes engaging in risk-taking behavior as, in the unique tournament-stable equilibrium, player ==== chooses to play the constant sum game with probability ====. We also show that this result is exactly driven by tournament incentives. Indeed we show that if players wanted to minimize the probability of being the lowest scorers then in the unique tournament-stable equilibrium player ==== would always choose the safe outside option.====The applications of tournaments that we present consist of games that have been traditionally studied by the experimental literature. In fact, AR13 (p.34) justify the tournament model also in the context of experimental design. To study behavior in the game ==== one may propose an experiment consisting of a group of players who are re-matched after each interaction and incentivized by the promise of a prize to the highest scorer. An obvious caveat to this design is that it creates incentives that are different from those present in ====, as evidenced by the fact already discussed that the set of equilibria of ==== and of the tournament can be different. We emphasize that our model allows to extend such a scope to all the possible games studied in experiments, which are typically asymmetric.====Our work expands AR13’s analysis to every possible game and to every possible equilibrium. In fact, not only can we study the equilibria of non-trivial extensive form games, but also the non-symmetric equilibria of symmetric games, which in many cases are the more economically relevant equilibria.====  In particular, we extend AR13’s analysis using a symmetrization of the game under study.====  We relate strategy combinations (and equilibria) of the base game with strategies (and symmetric equilibria) of such a construction by proving an analogue of Kuhn’s theorem (Kuhn, 1953). This allows us to generalize AR13’s Proposition ====, which establishes that every limit equilibrium of tournaments is an equilibrium of the base game. Moreover, we present an example which shows that there can be equilibria of a game that are not limit equilibria of tournaments built on it, replicating also AR13’s Proposition ====. Then, we discuss tournament-stability as an equilibrium refinement with classical examples of extensive form games in which the information structure is crucial. To complete the comparison with AR13 we conclude analyzing some two-player symmetric games, on which we can build both our double round-robin tournaments and their round-robin tournaments. The results obtained in the two cases show how AR13’s conclusions do not extend immediately to our setting.====An alternative tournament model is considered by Laffond et al. (2000). As in AR13 and in our model, each player chooses one action and employs it in all her interactions. However, a player’s payoff is given by the sum of the payoffs that she gets in all the (symmetric) games she plays, so players do care about their absolute total score. AR13 already explain the differences between the tournament model and the classic model of contests (see, e.g., Green and Stokey, 1983, Dixit, 1987, Konrad, 2009; among others), where players’ utilities depend on the probability of winning, which is a function of players’ efforts, and on the cost of their own effort. A particular contest model is that of the elimination tournament, which consists of several rounds in which individuals play pairwise matches (see, e.g., Rosen, 1986, Konrad, 2004, Groh et al., 2012). Differently from our model, the winner of a match advances to the next round of the tournament, while the loser is eliminated from the competition.====We formally describe the model in the next section. In Section 3, we analyze the interaction between any ==== players in the tournament. We examine the relationship between equilibria of the tournament and equilibria of the game on which it is built in Section 4, and we compare the refinement criterion provided by tournaments with standard refinements in Section 5. Section 6 explores risk-taking incentives in tournaments, while Section 7 extends the comparison between double round robins and AR13’s round robins.",Tournament-stable equilibria,https://www.sciencedirect.com/science/article/pii/S030440681930117X,18 November 2019,2019,Research Article,242.0
Raimondo Roberto,"Department of Mathematics, Universita’ degli Studi di Milano-Bicocca, Via Cozzi 8, 20126 Milan, Italy,Department of Economics, University of Melbourne, Parkville 3054 Victoria, Australia","Received 15 February 2019, Revised 18 October 2019, Accepted 28 October 2019, Available online 7 November 2019, Version of Record 14 November 2019.",https://doi.org/10.1016/j.jmateco.2019.10.006,Cited by (0),"
 ====
 ","In a typical strategic game it is true that Nash equilibria are socially inefficient (see Dubey (1986)). It is particularly interesting to study this inefficiency in the case of congestion games. In a few words we can describe a congestion game as a game where there is a ground of resources and each player can select a subset of them. Each resource has a cost which depends on the load induced by the players who use it. Of course, each player tries to minimize the sum of the resources’ costs used in his/her strategy. This type of games is simple but rich enough to capture situations very diverse and very interesting such as oligopoly models, network design, routing and even migrations of species. The original paper on this type of games is due to Rosenthal (see Rosenthal (1973)), while the further formalization (with more results and a simplified proof ) is due to Monderer and Shapley (see Monderer and Shapley (1996)), who introduced the notion of Potential Games. The most important result for this class of games is that there is always a Nash equilibrium in pure strategies. A simple, but very interesting, example was given by Pigou (see Pigou (1920)). It shows that Nash equilibria are suboptimal even in simple congestion games and the inefficiency grows with the degree of nonlinearity of the cost functions. For this class of games it is very useful, as a measure of inefficiency, the notion of the price of anarchy (====). This notion was introduced by Koutsoupias and Papadimitriou (see Koutsoupias and Papadimitriou (1999)). They defined, for a system in which noncooperative agents share a common resource, the ratio between the worst possible Nash equilibrium and the social optimum as a measure of the effectiveness of the system, and they computed upper and lower bounds for this ratio in a model in which several agents share a very simple network. In particular, along this line, in the class of congestion games the work of Roughgarden and Tardos has been very influential and surprising (see Roughgarden and Tardos (2002)) since, by building on the work of Koutsoupias and Papadimitriou, they studied more general networks and they computed specific bounds for different classes of cost functions. Many developments followed and very recently Roughgarden and Schoppmann have analyzed a class of games called the ====
 ====
 ==== where it is possible to quantify, in the worst possible scenario, such an inefficiency (====) with the help of a supplementary condition known as the local ====-smoothness condition (see Roughgarden and Schoppmann (2015) and Bhawalker et al. (2013)). The notion of local ====-smoothness is very useful in order to give such measures for the ====. For this reason, this notion has been presented in two different versions: a global version and a local one.====The main point for the introduction of the local version is to exploit to the fullest extent the local behavior. We think, given the importance of congestion games and the study of inefficiency, that it is of some interest to find new conditions to quantify the ====.====The are two main contributions in this paper. The first is to introduce a unified way to present local and global versions as special cases of a more general notion, that we suggest calling ====. The advantage of this new approach is twofold: not only does it shed some new light on previously developed constructions, but it also shows that we can use in a very systematic way a well-defined and easily computable family of linear maps in order to investigate the inefficiency of Nash equilibria in congestion games. This is particularly useful since, after all, we need to study the nonlinearity of cost functions because, as shown by Roughgarden, this nonlinearity is the crucial ingredient for the inefficiency, and not the topology of the network, as previously assumed. Of course, the use of linear maps is quite relevant when applicable in the study of nonlinearity. The family of linear maps depends on a family of paths which are to be chosen. It is shown that the previous known constructions due to Roughgarden and his coauthors are associated, in a certain sense, with the extreme points of the set of possible choices. Therefore, they are presented as different instances of the same construction. The second contribution is to present a simple mathematical criteria that allows us to easily compute an upper bound for ==== when players play a correlated equilibrium.====The paper is organized as follows. In the next section we will present the relevant definitions of congestion games and a new proposed notion of path structure. In the last section we prove a general theorem that will allow us to recover the previous notions and quantify the size of the ====.",Pathwise smooth splittable congestion games and inefficiency,https://www.sciencedirect.com/science/article/pii/S0304406819301089,7 November 2019,2019,Research Article,243.0
Arigapudi Srinivas,"Department of Economics, University of Wisconsin, 1180 Observatory Drive, Madison, WI 53706, United States of America","Received 19 March 2019, Revised 17 October 2019, Accepted 22 October 2019, Available online 4 November 2019, Version of Record 16 November 2019.",https://doi.org/10.1016/j.jmateco.2019.10.004,Cited by (12),"We study the effect of introducing a bilingual option on the long run equilibrium outcome in a class of two-strategy coordination games with distinct payoff and risk dominant equilibria under the logit choice rule. Existing results show that in the class of two-strategy games under consideration, the inefficient risk dominant equilibrium is selected in the long run under noisy best response models. We show that if the cost of the bilingual option is sufficiently low then the efficient payoff dominant equilibrium will be selected in the long run under the logit choice rule.","Suppose that we have a large number of anonymous myopic agents who are randomly matched to play a two-strategy symmetric coordination game with payoffs as shown in Table 1. We assume that agents choose strategies using a noisy best response model. In these models, best responses are chosen with a very high probability, and with a small probability, sub-optimal strategies are chosen due to noise in the underlying decision making process. It is these small probability events which will eventually lead to the breakdown of and transitions between equilibria, with some occurring more readily than others. This variation in the difficulties of transitions ensures that a single equilibrium, the stochastically stable equilibrium, will be selected in the long run.====Assume that ====. Strategy ==== is then the payoff dominant equilibrium, while strategy ==== is the risk dominant equilibrium.==== ====
 ==== ==== Existing results show that the risk dominant equilibrium (inefficient equilibrium, strategy ====) is selected in the long run under the noisy best response models (see Kandori et al. (1993) and Blume (2003)). This is because in the above class of games, it is relatively easier to disturb the efficient equilibrium compared to disturbing the inefficient equilibrium as the cost of transition from the risk dominant equilibrium to the payoff dominant equilibrium is greater than the cost of transition from payoff dominant equilibrium to the risk dominant equilibrium (see Section 6, mainly Eq. (6.9)). The work proposed here suggests a feasible way of selecting the efficient equilibrium. This is done by introducing a new strategy called a ==== as discussed below.====If we allow an extra option of adopting both strategies at an additional cost ==== in the game from Table 1, then we have a new game whose payoffs will be as shown in Table 2. These games are known as ==== and strategy ==== is called as the ====. For low adoption costs, the bilingual option can be thought of as a hedging strategy in the face of uncertainty about what other players choose. Once sufficient number of agents choose the bilingual option, playing the payoff dominant equilibrium becomes the unique best response. Our results confirm this intuition and show that the efficient equilibrium will be chosen in the long run for low adoption costs.====The seminal papers of Foster and Young (1990), Kandori et al. (1993), Young (1993) and Ellison (1993) model the noise using best response with mutations (BRM) in which the probability of a suboptimal choice is independent of its payoff consequences. This model eases the analysis of equilibrium breakdown, as the difficulty of transiting between two equilibria can be determined by counting the number of mutations needed for the transition to occur. In some applications however, it is more realistic to assume that costly mistakes are less likely to happen, i.e., it is more reasonable to assume that the errors in agents choices are payoff dependent as in the logit model of Blume, 1993, Blume, 2003. When mistake probabilities are payoff-dependent, the probability of transitions between equilibria becomes more difficult to assess, depending not only on the number of suboptimal choices required, but also on the unlikelihood of each such choice. As a consequence, general results on equilibrium breakdown and transitions are only available for two-strategy games in the ====, where only the noise level in agents decisions is taken to zero.==== In the case of two-strategy coordination games, a noisy best response model induces a birth–death Markov chain on the state space whose stationary distribution can be computed explicitly by standard methods. However for three-strategy games we no longer have this simplification. In order to be able to derive analytical results on equilibrium breakdown and transitions between equilibria in three-strategy games with payoff dependent error structure, we study behavior in the ====,==== ==== first taking the noise level in agents’ decisions to zero, and then taking the population size to infinity. Sandholm and Staudigl (2016) (henceforth SS16) show that in this double limit, transitions between equilibria can be described in terms of solutions to continuous optimal control problems. We formulate the problem of transitions between equilibria in bilingual games under the logit choice rule in the small noise double limit as an optimal control problem using the results of SS16. We solve this control problem using the results of Sandholm et al. (2018). The solution to the transition problem is then used to compute the set of stochastically stable states.====The previous literature on bilingual games mainly focused on cases where the agents either use deterministic best responses in a network setting (Immorlica et al., 2007, Oyama and Takahashi, 2015) or best responses with mutations (Galesloot and Goyal, 1997, Goyal and Janssen, 1997). To the best of my knowledge, this is the first attempt to study the long run equilibrium outcomes of bilingual games under a noisy best response model with payoff dependent error structure in a large population game framework.==== ====The work closest to ours is by Galesloot and Goyal (1997). They show that for a sufficiently large population size, the payoff dominant equilibrium in the bilingual game will be stochastically stable if and only if ==== in the ==== under the BRM model. They characterize the threshold adoption cost of the bilingual option, ==== in terms of the entries of the payoff matrix of the bilingual game. Our main result on the other hand shows that the payoff dominant equilibrium in the bilingual game will be stochastically stable if ==== in the ==== under the logit choice rule (see Remark 6.4 for comparison of results under the logit and BRM choice rules).====The paper proceeds as follows: Section 2 introduces our class of stochastic evolutionary processes. We formally define the unlikelihood function and describe it under the BRM and logit choice rules. In Section 3, we describe the formal modeling procedure of revision opportunities. We then set up the ==== problem in the small noise double limit and define ==== in this setting. Section 4 provides definitions and introduces matrix notation which will be used in our main analysis. In Section 5.1, we state the verification theorem from Sandholm et al. (2018) which provides a sufficient condition for a value function to be a solution in a certain class of optimal control problems. In Section 5.2, we formulate the ==== problem in bilingual games in the small noise double limit as an ==== problem and give a brief outline of how the verification theorem presented in Section 5.1 is used to solve the transition cost problem. In Section 5.3, we compute the costs of certain direct paths in the small noise double limit under the logit choice rule. These costs are used to present the solution of the transition cost problem and also compute the set of stochastically stable states in Section 6. Section 7 concludes.",Transitions between equilibria in bilingual games under logit choice,https://www.sciencedirect.com/science/article/pii/S0304406819301065,4 November 2019,2019,Research Article,244.0
"Harbaugh Richmond,To Theodore","Indiana University, United States,Bureau of Labor Statistics, United States","Received 19 May 2018, Revised 26 September 2018, Accepted 30 October 2018, Available online 28 May 2019, Version of Record 24 January 2020.",https://doi.org/10.1016/j.jmateco.2018.10.004,Cited by (5),"Is it always wise to disclose good news? Using a new statistical dominance condition, we show that if the receiver has any private receiver information then the weakest senders with good news gain the most from boasting about it. Hence the act of disclosing good news can paradoxically make the sender look bad. Nondisclosure by some or all senders is an equilibrium if standards for the news are sufficiently easy or if prior expectations without the news are sufficiently favorable. Full disclosure is the unique equilibrium if standards are sufficiently difficult or sufficiently fine, or if prior expectations are sufficiently unfavorable. Since the sender has a legitimate fear of looking overly anxious to reveal good news, mandating that the sender disclose the news can help the sender. The model’s predictions are consistent with when faculty avoid using titles such as “Dr.” or “Professor” in voicemail greetings and course syllabi.","If you have good news should you disclose it? According to the classic “unraveling” result, not only is it wise to disclose good news, but it is also necessary to disclose mediocre or even bad news to avoid the perception of having worse news (Viscusi, 1978, Grossman and Hart, 1980, Grossman, 1981, Milgrom, 1981, Milgrom and Roberts, 1986, Okuno-Fujiwara et al., 1990). This result on the power of voluntary disclosure has informed long-running debates over whether to mandate disclosure in consumer product information (Mathios, 2000), financial statements (Greenstone et al., 2006), environmental impact (Powers et al., 2011), and other areas.====However, as suggested by the above quote, inferences about modesty are not always so straightforward. People are sometimes unsure whether to reveal good news, and nondisclosure is often observed in practice (Jin, 2005, Xiao, 2010, Luca and Smith, 2015). Advertisers of high quality products frequently use a “soft sell” approach, donors sometimes make anonymous donations, members of a successful group do not always emphasize their group identity, overachievers are often understated, and offenders sometimes withhold mitigating information rather than “protest too much” or “make excuses.”====Most of the literature explains such anomalies by examining why the absence of good news is not treated as evidence of bad news.==== ==== But what if boasting about good news is itself treated as bad news? Consider whether a restaurant should post its hygiene grade from the local health department. The unraveling result implies that all restaurants should post their grades voluntarily, but many restaurants with even ==== grades choose not to, and indeed the best restaurants within the ==== category are less likely to disclose (Bederson et al., 2018). Such nondisclosure has led many communities to require mandatory disclosure rather than rely on voluntary disclosure (Jin and Leslie, 2003). Why is it necessary to require ==== restaurants to disclose their hygiene grade? Could showing off good news be a bad sign that the restaurant lacks confidence in diners’ opinions of it?====These same concerns are faced by individuals. For instance, in environments where titles such as “Dr.”, “Professor”, or “Ph.D.” are common, can use of a title be seen not just as redundant, but as a signal that the person has some reason to fear appearing unqualified? Table 1 summarizes data on the use of titles in voicemail greetings and course syllabi by Ph.D.-holding full-time faculty in 26 economics departments in the same U.S. state (details are in Appendix B). Even though it is costless to use a title, many faculty actively avoid them, for example substituting “instructor” for “professor” on course syllabi. In particular, faculty in the more prestigious universities with doctoral programs are significantly less likely to use a title.==== ====To understand why boasting about good news can be a sign of weakness, we analyze a standard costless disclosure game when there is a continuum of sender types and a finite set of verifiable messages. For instance, there is a pass–fail certificate or a system of letter grades. We show that the classic unraveling result is not robust to allowing noisy private receiver information about sender type.==== ==== Such information is likely to be present in most disclosure environments, and is equivalent to the sender facing a distribution of different receivers, where better sender types face a higher proportion of receivers with a more favorable prior about the sender. For instance, customers hear more favorable recommendations from friends about a good restaurant than a bad restaurant.====To analyze the interaction of the prior, the sender’s message, and the receiver’s private information, we develop a new statistical dominance condition that shows when private receiver information implies that the net gain from disclosure is decreasing in the sender’s type. This condition ranks the effects of truncations on conditional expectations, and always holds if either the private receiver information is sufficiently strong or sufficiently weak. For intermediate strength the interactions between the information sources can be more complicated, but the condition holds for common functional forms used in examples. The condition implies that skepticism about who discloses is self-confirming in that it gives the weakest sender types who meet a standard for good news the most incentive to show off.====If the standards for good news are sufficiently low or if the prior distribution of sender types is sufficiently favorable, such skepticism implies that there is a ==== where any type who deviates to disclosure is viewed skeptically. And, analogous to behavior in a (costly) signaling game (e.g., Feltovich et al., 2002), there can be a ==== where lower types who meet the standard disclose, but higher types who could disclose choose to rely on the private receiver information. These equilibria coexist with the full disclosure (unraveling) equilibrium and survive typical refinements such as the intuitive criterion, D1, and Pareto dominance.====Best response dynamics converge to the different equilibria depending on initial behavior.==== ==== If enough higher types initially countersignal by not disclosing, the better intermediate types are willing to take a chance and also stop disclosing, which further reduces the gains to disclosure. As more and more medium types join in, eventually the nondisclosure equilibrium is reached in which no types at all disclose. But if initially only the highest types countersignal, then nondisclosure tends to be associated with lower types who do not have good news to disclose. Hence lack of disclosure is more risky for the lower range of high types, and the disclosure region expands. Eventually all types disclose, including the highest types. If initial behavior is intermediate with a large but not too large share of countersignaling high types, play can converge to a countersignaling equilibrium, but only if the private receiver information is strong enough.====When does the classic result of full disclosure due to unraveling still hold? We find three sufficient conditions that ensure that there is unraveling up until some point in any equilibrium, including full unraveling. First, if a standard is sufficiently tough we find that disclosure occurs in any equilibrium since even the most skeptical beliefs about which types disclose still imply that any type meeting the standard does better by disclosing. Second, if prior expectations about sender quality are low enough we also find that disclosure is ensured since even an easy standard is then sufficiently above prior expectations. Third, if standards are sufficiently fine then full unraveling is the unique equilibrium.==== ====Although this paper does not consider optimal information design, the results offer new insight into several issues that have been debated from different perspectives. First, is the long-standing question of when disclosure should be mandatory. The existence of multiple equilibria implies that mandatory disclosure can help reveal information when senders and receivers fail to coordinate on an informative equilibrium. Similarly, having a third party disclose the news can reduce communication problems by allowing the sender to enjoy the benefits of favorable information without the negative inference from disclosing good news.====Second, is the issue of how difficult it should be to meet different standards such as those for school diplomas or other certificates of quality. We show that higher standards are less likely to allow the existence of a nondisclosure or countersignaling equilibrium, so higher standards can paradoxically induce higher certification rates.==== ==== Moreover, the more favorable the distribution of sender types, the higher the standard that is necessary to ensure disclosure. Hence setting lower standards for groups with an unfavorable distribution and higher standards for groups with a favorable distribution not only divides the conditional distributions in a more informative way, but it can also avoid strategic uncertainty due to multiple equilibria.====Third, the model offers new insight into the question of how fine or coarse standards should be. A large literature shows why an information structure that gives different types the same grade can benefit an information intermediary (e.g., Lizzeri, 1999), the sender (e.g., Kamenica and Gentzkow, 2011), or the receiver (e.g., Harbaugh and Rasmusen, 2016). Our results indicate that, unless disclosure is mandated, the gains from such coarseness may be undermined by the strategic reluctance of senders to disclose good news.====Finally, the model provides necessary and sufficient conditions for the existence of nondisclosure and countersignaling equilibria based on observable properties of the common knowledge distribution of types and standards. In a three-type costly signaling game, Feltovich et al. (2002) predict that higher types are less likely to signal, but do not address the effect of public information. In a two-type costly signaling game, Daley and Green (2014) predict that signaling is less likely when public information about type is more favorable. Here we predict that costless self-promotion is less likely by higher quality senders based on both their actual quality and public measures of quality.====In the following section we review the literature, in Section 3 we provide some simple illustrative examples and in Section 4 we develop a more formal model. For the case of a single standard, we derive conditions for when disclosure may fail or be incomplete in Section 5 and then we allow for multiple standards in Section 6 in order to reassess the classic unraveling result. In Section 7 we conclude the paper.",False modesty: When disclosing good news looks bad,https://www.sciencedirect.com/science/article/pii/S0304406818301216,28 May 2019,2019,Research Article,245.0
"Arcand Jean-Louis,Hongler Max-Olivier,Rinaldo Daniele","Department of International Economics, The Graduate Institute, Chemin Eugène-Rigot 2, 1202 Geneva, Switzerland,EPFL-IPR-LPM, Ecole Polytechnique Fédérale de Lausanne, BM 3142 Station 17, 1015 Lausanne, Switzerland","Received 3 May 2017, Revised 19 March 2018, Accepted 19 November 2018, Available online 17 December 2018, Version of Record 13 December 2019.",https://doi.org/10.1016/j.jmateco.2018.11.003,Cited by (2),We extend the celebrated Rothschild and Stiglitz (1970) definition of Mean-Preserving Spreads to a dynamic framework. We adapt the original integral conditions to transition ,"Comparing the riskiness of different random variables is a topic of central importance in economic research. The inadequacy of the variance as a measure of risk is well established, since this criterion is satisfactory in economic applications in a limited number of cases. To wit: an increase in risk increases the variance, but the converse is not necessarily true.====A milestone in the search for a more informative criterion was the series of articles by Rothschild and Stiglitz, 1970, Rothschild and Stiglitz, 1971, Rothschild and Stiglitz, 1972 which defined the concept of an increase in risk in the form of second-order stochastic dominance, often referred to as a Mean-Preserving Spread (MPS) , and applied it to various economic problems.==== ==== This concept has become a workhorse of microeconomic analysis, with applications ranging from finance to the study of inequality: see, for example, the standard textbooks by Laffont (1990), Levy (1998) or Gollier (2001).====The strength of Rothschild and Stiglitz’s result lies in a definition of comparative risk that can be summarized by means of four intuitively-appealing notions which are shown to be equivalent.====More explicitly, and for comparison purposes with what follows, we focus on Definition 1.3, which is given by the two following integral conditions==== :====The support of ==== may obviously also be compact, in which case the limits of the integrals and the upper bound of ==== correspond to the boundaries of the support. We present this form of the integral conditions for consistency with what follows.==== ====Definition 2 applies to a static framework: loosely speaking, the comparison of riskiness of random variables is done for a “snapshot” of their respective distributions taken at an arbitrary instant in time, as in a phase diagram for a dynamical system. In this paper, we provide the dynamic counterpart to mean-preserving spreads in the context of scalar diffusion processes. This allows us to parameterize the riskiness of a stochastic process throughout its evolution in the time domain. A remarkable feature of our dynamic counterpart is that it allows one to prove, for Brownian bridges, that a specific functional form, which corresponds to super-diffusive ballistic noise, constitutes the ==== process with non-constant drift that displays the dynamic version of the MPS property. In what follows we refer to this as a ====, or DMPS. While the functional form is non-Gaussian, its properties allow for simple closed-form solutions in a broad range of economic applications, of which we give four canonical examples below.====This paper is organized as follows. In Section 2, we derive our main results. First, in Definition 3, we give the two necessary integral conditions for a DMPS, which are straightforward dynamic generalizations of the standard Rothschild and Stiglitz conditions of Definition 2. The two conditions are essentially antisymmetry and positivity conditions on the derivative with respect to a risk parameter of the Radon–Nikodym derivative associated with the transition probability density that defines a family of risk-parameterized scalar diffusion processes. In Proposition 1, we provide a sufficient condition for a stochastic process to satisfy the integral conditions of Definition 3. This is followed by Lemma 1, which shows that Definition 3 allows one to characterize second-order stochastic dominance in terms of the preferences of a risk-averse agent, as in Definition 1.4.====Our most important result is given in Proposition 2 which proves that, within a broad class of processes, the diffusion process given by ====, where ==== is the standard Brownian motion, is the ==== process with non-constant drift that displays the DMPS property.====  It turns out that this process has an extremely simple representation in terms of the superposition of two drifted Wiener processes: we prove this in Lemma 2, which we call the Bernoulli Representation Lemma. This leads to particularly simple closed-form solutions in common applications. To give a first taste of this underlying simplicity, Proposition 3 then uses the preceding results to provide the marginal densities of DMPS-driven processes for three cases often used in economics: the drifted process with scalar coefficients, the mean-reverting (stationary Ornstein–Uhlenbeck) process and the geometric process. Section 2 concludes with Proposition 4 in which we derive Itô’s formula for a DMPS process.====Section 3 explores how driving a system with the DMPS noise process, and increasing its parameter of risk ====, differs from an increase in the variance of the Brownian motion for a general diffusion process. We do this in three ways. First, in Proposition 5, we study the curvature of the time-invariant probability measure for scalar processes, and show that the behavior obtained by driving the system with the DMPS process cannot be derived from a simple change in the variance of a Gaussian. To wit: varying the risk parameter ==== and the variance term induce very different effects on the stationary probability measure. Second, we study two simple optimal stopping applications (stopping at the ultimate maximum and stopping with a transaction cost), and show that the impact of an increase in risk on both the stopping threshold and the stopping region is different from that of an increase in variance. Third, we show that, contrary to an increase in variance, a DMPS may violate the Certainty Equivalence Principle used in optimal control theory. The upshot of Section 3 (as with the rest of the paper) is that, as in the static world of Rothschild and Stiglitz, risk and variance should not be conflated in a dynamic context.====Section 4 provides economic illustrations of our results and showcases the analytical tractability of this class of processes by revisiting four standard economic problems: (i) portfolio selection, (ii) investment under uncertainty as in Abel (1983) and Abel and Eberly (1994), (iii) asset dynamics à la Black–Scholes and finally (iv) firm entry and exit decisions under uncertainty following the Dixit (1989) framework. Our goal with these illustrations is not to propose new theoretical models, but to show how driving noise with a DMPS process instead of a Gaussian, and thereby disentangling risk and variance, modifies and often clarifies existing results. The Gaussian setup always emerges as a special, and sometimes misleading, case.",Increasing risk: Dynamic mean-preserving spreads,https://www.sciencedirect.com/science/article/pii/S0304406818301332,17 December 2018,2018,Research Article,248.0
Khan M. Ali,"Department of Economics, Johns Hopkins University, United States of America","Available online 17 August 2020, Version of Record 8 September 2020.",https://doi.org/10.1016/j.jmateco.2020.08.002,Cited by (0),None,None,Tapan Mitra: Mathematical Economist and Economic Theorist,https://www.sciencedirect.com/science/article/pii/S0304406820300847,17 August 2020,2020,Research Article,249.0
Kukushkin Nikolai S.,"Dorodnicyn Computing Centre, FRC CSC RAS, 40, Vavilova, Moscow 119333 Russia","Received 14 April 2020, Accepted 24 April 2020, Available online 18 May 2020, Version of Record 18 May 2020.",https://doi.org/10.1016/j.jmateco.2020.04.004,Cited by (0),None,None,Corrigendum to “Equilibria in ordinal status games” [J. Math. Econom. 84 (2019) 130–135],https://www.sciencedirect.com/science/article/pii/S0304406820300471,18 May 2020,2020,Research Article,251.0
"Hlouskova Jaroslava,Fortin Ines,Tsigaris Panagiotis","Macroeconomics and Economic Policy, Institute for Advanced Studies, Vienna, Austria,Ecosystems Services and Management, International Institute for Applied Systems Analysis (IIASA), Laxenburg, Austria,Department of Economics, Thompson Rivers University, Kamloops, BC, Canada","Received 3 January 2019, Revised 15 October 2019, Accepted 18 October 2019, Available online 1 November 2019, Version of Record 11 November 2019.",https://doi.org/10.1016/j.jmateco.2019.10.003,Cited by (1),"In this paper we analyze the two-period consumption–investment decision of a household with prospect theory preferences and an endogenous second period reference level which captures habit persistence in consumption and in the current consumption reference level. In particular, we examine three types of household depending on how the household’s current consumption reference level relates to a given threshold which is equal to the average discounted endowment income. The first type of household has a relatively low reference level (less ambitious household) and can avoid relative consumption losses in both periods. The second type of household (balanced household) always consumes exactly its reference levels. The third type of household has a relatively high reference level (more ambitious household) and cannot avoid to incur relative consumption losses, either now or in the future. Note that these households may act very differently from one another and thus there will often be a diversity of behavior. For all three types we examine how the household reacts to changes in: income (e.g., income fall caused by recession or taxation of endowment income), persistence to consumption, the first period reference level and the degree of loss aversion. Among others we find that the household increases its exposure to risky assets in good economic times if it is less ambitious and in bad economic times if it is more ambitious. We also find that in some cases more income can lead to less happiness. In addition, the less ambitious household and the more ambitious household with a higher time preference will be less happy with a rising persistence in consumption while the more ambitious household with a lower time preference will be happier if it sticks more to its consumption habits. Finally, the household’s happiness decreases with an increasing consumption reference level and thus not comparing at all will lead to the highest level of happiness. In addition, the sensitivity of happiness with respect to the reference level gets larger the closer the household moves to the threshold level, and it is smaller for less ambitious households than for more ambitious households due to loss aversion.","One of the most important decisions households face is consumption today versus consumption in the future. Households transfer current consumption into the future by allocating their savings into different types of assets some of which are riskier than others. These decisions are done with the knowledge that the future is risky. The expected utility theory (EUT) has been the cornerstone model for exploring these household decisions. This research deviates from the EUT model and explores, in a two-period model, the behavior of households which are characterized by reference dependent preferences (Kahneman and Tversky, 1979, Tversky and Kahneman, 1992) and by habit persistence (Abel, 1990, Alessie and Lusardi, 1997, Campbell and Cochrane, 1999, Constantinides, 1990, Flavin and Nakagawa, 2008, Pagel, 2017) when deciding on consumption, savings, and the portfolio allocation of savings. We explore the factors that influence a household’s consumption, savings and portfolio decisions when the second period reference level is assumed to depend on first period consumption and the first period consumption reference level. Households have been observed to show a habit for consumption that persists into the future, and hence a habit persistence model combined with prospect theory preferences will provide new insights on such important life cycle decisions.====By incorporating prospect theory type of preferences and habit persistence we will be able to address a number of issues on consumption and risk taking behavior that have not been explored in the literature previously. How does a household make intertemporal decisions under these two behavioral traits? Does the optimal solution depend on avoiding relative losses or not? Does the optimal choice depend on whether the household is sufficiently loss averse? Is the choice dependent on the household being less or more ambitious on targets? How do the second period reference level, consumption, risk taking, and happiness change when the first period reference level changes? Do the responses depend on the household’s level of ambition? What impact does the habit persistence in consumption have on consumption and portfolio choice? How will a household react to sudden income changes? Do happiness, current consumption and risk taking always increase when income increases? This paper will attempt to shed some light on the above questions.====The first reference levels ever used in economic research were developed by Stone (1954) and Geary (1950). The Stone–Geary utility preferences involve ==== and thus subsistence levels can be considered as a special type of reference points. Under such preferences households derive utility from consumption in excess of a subsistence level. Achury et al. (2012) explored portfolio-savings decisions with subsistence consumption,==== ==== where they use a Stone–Geary expected utility model to explain the empirical findings that rich people observe a higher savings rate, a larger proportion of risky assets in their personal wealth, and a higher volatility in consumption than poor people.====Another model that has been used is ====, where households are assumed to derive their utility from consumption relative to a reference level which depends on past consumption levels. Thus current consumption affects not only a household’s current marginal utility but also its marginal utility in the next period, which may explain why the more a household consumes today the more it will want to consume tomorrow.==== ==== The macroeconomics and finance literature uses habit persistence models to explain many puzzles, e.g., the equity premium puzzle (Abel, 1990, Constantinides, 1990, Campbell and Cochrane, 1999), excess consumption smoothing (Lettau and Uhlig, 2000), asymmetric reactions due to income uncertainty (Bowman et al., 1999)==== ==== and many business cycle patterns (Boldrin et al., 2001, Christiano et al., 2005).====Reference levels are also used to compare one’s own consumption levels to others (Falk and Knell, 2004, Hlouskova et al., 2017). Many households are influenced by the ==== motive while others are determined by the ==== motive. The self-enhancement motive applies when people want to feel they are better than their peers and set their references at low levels possibly reflecting the wealth of poorer people. Others with a high reference level place importance to the self-improvement motive and compare themselves with the ones who are more successful. Hlouskova et al. (2017) use a two-period life-cycle model with a sufficiently loss averse household to investigate the impact of these psychological traits on consumption, savings, portfolio decisions, as well as on welfare. They find that the optimal solution depends on whether the household’s present value of the consumption reference levels is below, equal to, or above the present value of its endowment income. When reference levels are below the endowment income the authors associate this with the self-enhancement motive. Under this motive the household wants to avoid relative losses in consumption in any present or future state of nature (good or bad). Hence the degree of loss aversion does not affect optimal first period consumption and risky asset holdings. When reference levels are equal to the endowment income this is linked to the belonging motive (i.e., the sufficiently loss averse household belonging to a similar social class). They find that the sufficiently loss averse household’s first period consumption is the exogenous reference consumption level and such households avoid playing the stock market. Finally, reference levels above the endowment income are connected with the self-improvement motive. Households with such high reference levels cannot avoid to consume below the reference level, either now or in the future. In this case loss aversion affects consumption and risky investment negatively. The current study differs from Hlouskova et al. (2017) in that it incorporates habit persistence into the household’s behavior.====Close to our work is also a recent paper by van Bilsen et al. (2017) who investigate optimal consumption and portfolio choice paths of a loss averse household with an endogenous reference level. The uncertainty arises from risky assets and it is assumed that the time is continuous. Mainly due to loss aversion, the household’s behavior is geared towards protecting itself against bad states of nature to avoid or to reduce losses. Consumption choices are found to adjust slowly to financial shocks. In addition, welfare losses are found to be substantial given consumption and portfolio selections are suboptimal. Curatola (2017) also analyzes optimal consumption-savings decisions of a loss averse household with a time varying reference level in a continuous-time framework and finds that a loss averse household can consume below the reference level (to the subsistence level) in bad economic times. This is done in order to invest in risky assets and increase the likelihood that in the future consumption exceeds its reference level. This behavioral approach can explain why investors increase their exposure to risky assets during financial crises. In contrast, standard habit persistence models do not allow consumption to be below the reference level. Our research complements the work by van Bilsen et al. (2017) and Curatola (2017) in that it provides additional insights: as our model is a two-period life-cycle model we can derive closed-form solutions which allow us to conduct comparative static analysis to detect why certain adjustments happen and also to conduct a welfare analysis.====In this paper, we find closed-form solutions for consumption and risk taking of a loss averse household whose endogenous second period reference level depends on current consumption (habit persistence) and on reference consumption. Households who have a relatively low first period reference level are more conservative (less ambitious), which allows them to achieve relative gains in both periods in both states of nature. Households who have a relatively high first period reference level and a low discount factor are more adventurous (more ambitious) and will thus face relative losses in the bad state of nature in the second period while they will achieve relative gains in the first period and in the good state of nature in the second period. On the other hand more ambitious households who value future consumption relatively more will have first period consumption below the reference level but will maintain future consumption in both states of nature above the endogenous second period reference level. We then conduct comparative statics and examine how these different types of households react to income changes, to changes in the first period reference level, to changes in loss aversion, and to changes in habit persistence.====The main difference with respect to Hlouskova et al. (2017), henceforth called HFT, is that this study considers also habit persistence. An increase in the consumption habit persistence will reduce current consumption but stimulate risk taking for less ambitious households, reduce both current consumption and risk taking for more ambitious households with a high time preference, and stimulate both current consumption and risk taking for more ambitious households with a low time preference. In addition, we analyze income effects, which are closely related to the effects of income taxes. Another difference between this study and HFT is that the response of first and second period consumption of less ambitious households to a change of the first period reference level is ambiguous. Finally, unlike in HFT we also consider here a scarcity constraint on consumption, i.e., the consumption in both periods cannot fall below a certain value.====Note that the household’s first period reference level may be interpreted to equal the first period consumption of a reference household, the Joneses. Then ==== ==== means that an increase of first period consumption of the Joneses will also trigger an increase of this household’s first period consumption.==== ==== In HFT the less ambitious household and the more ambitious household with a high time preference (low discount factor) do follow the Joneses, while the more ambitious household with a low time preference (high discount factor) does not. In this study the behavior of the more ambitious household is similar, while that of the less ambitious household may be similar or different, depending on the household’s time preference: for a lower time preference (larger discount factor) the household does follow the Joneses (like in HFT), while for a higher time preference it does not. The rest of the results are somewhat similar to HFT in terms of the impact of the exogenous parameters on the choice variables but differ in terms of magnitude.====Another interesting result that was not elaborated in HFT is the reaction of the choice variables of the household to income changes. When focusing, for instance, on risk taking then less ambitious households reduce risk taking when their income falls while more ambitious households increase risk taking when their income shrinks, which is consistent with the observation that investors increase their exposure to risky assets during financial crises (see Curatola, 2017). Finally, the same finding as in HFT is that the highest utility is achieved for the lowest current consumption reference level (while keeping everything else unchanged). Thus, not comparing at all (e.g., to others) leads to the highest level of happiness.====In the next section we present the model and lay out the methodology used to find the solutions. Section 3 presents the main results with a discussion and investigates the impact of income taxation. Finally, we offer some concluding remarks.",The consumption–investment decision of a prospect theory household: A two-period model with an endogenous second period reference level,https://www.sciencedirect.com/science/article/pii/S0304406819301053,1 November 2019,2019,Research Article,255.0
"He Ying,Dyer James S.,Butler John C.,Jia Jianmin","Department of Business and Economics, University of Southern Denmark, Odense, Denmark,Department of Information, Risk, and Operations Management, McCombs School of Business, The University of Texas at Austin, Austin, TX, USA,Department of Finance, McCombs School of Business, The University of Texas at Austin, Austin, TX, USA,School of Management and Economics, Shenzhen Finance Institute, The Chinese University of Hong Kong, Shenzhen, China","Received 21 January 2019, Revised 22 September 2019, Accepted 2 October 2019, Available online 21 October 2019, Version of Record 31 October 2019.",https://doi.org/10.1016/j.jmateco.2019.10.002,Cited by (0),"We extend the mean–variance (risk–value) tradeoff model to decision making under both risk and ambiguity. This model explicitly captures the tradeoff between the magnitude of risk and the magnitude of ambiguity. A measure that ranks lotteries in terms of the magnitude of ambiguity can also be obtained using this separation. By applying our model to asset pricing under ambiguity, we show that the equity premium can be decomposed into two parts: the risk premium and the ambiguity premium. Further, combining this model with the standard risk–value model, we build on the risk–ambiguity tradeoff to provide the value–risk–ambiguity preference model that does not rely on an approximation argument as the mean–variance model.","Markowitz (1952)’s mean–variance analysis is the foundation of portfolio selection theory and the capital asset pricing model (CAPM). For any act ====, the mean–variance preference is captured by a utility function based on the tradeoff between the mean ==== and the variance ==== of the distribution induced by ==== under probability measure ==== as below. ====It is well-known that the mean–variance preference fails to be monotone in general (Dybvig and Ingersoll, 1982, Jarrow and Madan, 1997, Maccheroni et al., 2009), which is inconsistent with von Neumann and Morgenstern (1947)’s EU model. Feldstein (1969) and Baron (1977) have shown that the mean–variance preference model is consistent with EU only when the utility function is quadratic.====Despite these limitations, the interest in mean–variance preference and its applications in finance and other areas has continued due to its mathematical tractability and intuitive foundation. In the economics and finance literatures, a typical approach is to restrict the distribution of the random variable to a small but interesting class without restrictions on preferences (Bigelow, 1993). In the decision analysis literature, an alternative approach is adopted. Rather than restricting distributions, scholars extended the mean–variance analysis framework to a risk–value model where value is measured by the utility of the mean and risk is measured by a more general concept than variance. Following the intuition of the mean–variance analysis, different models have been proposed to trade off risk and value in both the expected utility (EU) framework (Sarin and Weber, 1993, Jia and Dyer, 1996, Schmidt, 2003) and non-expected utility framework (Jia et al., 1999, Jia and Dyer, 2009).====In most real world decisions, objectively known probabilities are not available and many people exhibit an aversion to the consequences associated with subjective probability that cannot be uniquely specified and are therefore considered to be ambiguity averse. The objective of this paper is to extend the risk–value model to the context of decision making under ambiguity in an additive form.====Ambiguity may be created by missing information, by concerns about source credibility, or by expert disagreement about probabilities. As a result, the DM may be unable to specify a unique probability distribution for the uncertain consequence she is facing (Cerreia-Vioglio et al., 2013). Given this interpretation, we model the ambiguous choice problem by using a two-stage lottery where the DM assigns subjective probabilities to different possible probability distributions. The two-stage lottery is represented by an Anscombe–Aumann (AA hereafter) act ====, which maps states to probability measures over consequences. In each state there is a unique distribution representing the lottery faced by the DM. In the Subjective Expected Utility (SEU hereafter) framework, a subjective probability measure ==== over states can be derived from preferences, which is the second order probability. Under ====, each ==== induces a two-stage compound lottery, which represents the ambiguous lottery in this paper.====In Section 3 we develop the concept of a “standardized act” ==== to separate ambiguity from risk. As the main result of the paper, we obtain a two-component additive model to separate the evaluation ==== as ====where ==== represents the expected risk of ==== obtained by reducing the two-stage lottery induced by ==== under ====, which is called a reduced lottery in this paper; ==== represents the ambiguity of act ==== and ==== is the utility loss/gain from bearing the ambiguity which shares the same recursive structure as other second order expected utility (SOEU) models in the literature (Klibanoff et al., 2005; Nau, 2006, Grant et al., 2009, Neilson, 2010). Our approach to separating risk and ambiguity is new to the literature.====In Section 5 we utilize the risk–value model (Jia and Dyer, 1996) to express the utility from expected risk as: ==== where ==== is the mean of this reduced lottery under the first order probability ====, and ==== is a tradeoff factor. Substituting this decomposition into the above two-component additive separable model gives us a three-component model below. ====In this extended model, the utility of an act ==== is evaluated in an additive separable model with three components, the utility of value ====, a measure of standardized risk ====, and a measure of standardized ambiguity ====. This also introduces another novel contribution to the literature in Section 4 which is called the standard ambiguity measure defined as ====.====The preference represented by the above additive model and its three-component extension can be interpreted in the following way. Under ambiguity, the DM does not have a unique probability distribution over consequences. The expected distribution ==== obtained from applying the Reduction of Compound Lottery (ROCL) to the two-stage lottery represents the risk component of ====. After using this to evaluate the base utility level ====, the DM adjusts this base utility by the extra utility/disutility derived from bearing the ambiguity represented by ==== to form her overall evaluation. This two-component additive separable model is consistent with the ==== process proposed to model decision making under ambiguity in both the vector expected utility (VEU) model (Siniscalchi, 2009) and the mean-dispersion (MD) preference (Grant and Polak, 2013). The base utility ==== in our model is consistent with the baseline expected utility in VEU and the mean utility in MD. In contrast to VEU and MD, the adjustment based on ambiguity in our model differs from these models, which is consistent with the smooth ambiguity model in Klibanoff et al. (2005 hereafter KMM) and other SOEU models (Nau, 2006, Grant et al., 2009, Neilson, 2010).==== ====In the three-component additive separable model, the base utility ==== is further decomposed into the value and risk components to reflect the intuition that the DM adjusts the value ==== based on the mean of ==== by the standard risk ==== to evaluate the expected distribution ====. The three-component model may be interpreted as a two-step adjustment model in the family of the ==== preferences. Dawes and Corrigan (1974) have shown that additive linear models are often consistent with the actual outcomes of the decision making process when DMs make tradeoffs between factors. Therefore, our additive model and its three-component extension are intuitive models which reflect a decision making process when a DM makes tradeoffs among different factors. These models also enjoy mathematical tractability as the components in both models are as easy to compute as the parameters of the mean–variance model. In particular, we show that the ambiguity measure ==== can be approximated as the second order variance of the two-stage lottery induced from ====, which has similar computational tractability as the use of variance to measure risk in the finance literature. As a result, our three-component additive model facilitates the analysis of portfolio selection and asset pricing when ambiguity is involved in a manner that has been shown to be generally descriptive.====The notion of separating risk and ambiguity is not totally new and there have been some other efforts toward such a goal. Maccheronim et al. (2013, hereafter MMR) applied the Taylor expansion to the KMM smooth ambiguity model to obtain a mean–variance–ambiguity three component separable approximation to the certainty equivalent of an act evaluated by KMM. Although the KMM model achieves a separation between ==== toward risk and ambiguity (Chakravarty and Roy, 2009), the ==== are combined under integration. MMR (2013) applied this approximation to a portfolio selection problem involving assets with ambiguous returns to study how trading off between ambiguity and risk influences the allocation of money to different assets. The extension of our additive model can achieve the same three component separation effect without relying on an approximation based on the Taylor expansion. Further, our three-component model is based on an axiomatic approach, which clearly provides the conditions on preferences that justify the use of this model.====In another series of studies, Izhakian, 2017a, Izhakian, 2017b proposed a decision-making model which achieves the separability between risk and ambiguity and derives an ambiguity measure that can be applied in the asset pricing model. As highlighted in a recent Wall Street Journal article (Eisen, 2017), Brenner and Izhakian (2018) are considering extending the VIX model of the implied volatility of options on the S&P500 – a so-called fear gauge – with a measure of the ambiguity of the market returns to capture the “fear” of ambiguity. A separable ambiguity model can be used to derive such an ambiguity measure and to decompose the equity premium into a risk premium and an ambiguity premium in a consumption-based capital asset pricing model (CCAPM). These models explain investor behavior (e.g. Brenner and Izhakian, 2018; Izhakian’s 2017a; 2017b) and can be applied to investigate how ambiguity influences investors’ stock option exercise decisions (Izhakian and Yermack, 2017).====Our additive separable model under ambiguity can provide similar insights to Izhakian, 2017a, Izhakian, 2017b model but is both more tractable and general. Separation of risk and ambiguity is achieved by extending the SOEU (Grant et al., 2009, Neilson, 2010) and the final model is related to the well-known smooth ambiguity model in KMM (2005). Izhakian’s ambiguity measure is obtained by assuming that payoffs are “symmetrically distributed and probabilities of consequence are uniformly or elliptically distributed with the same expectation” (see Theorem 4, Izhakian, 2017a). In contrast, our ambiguity measure is derived for acts that induce general (first order) distributions of payoffs. Our ambiguity measure is also more intuitive as it can be approximated by the variance of the second order probability distribution over the first order utility induced by ====, which shares the same spirit as using the variance of first order distribution to measure risk.====Our paper contributes to the literatures of decision theory and financial economics in the following ways. First, our separable model provides an intuitive and tractable model to disentangle the influences of risk and ambiguity in decision making under ambiguity, which results in the standard ambiguity measure proposed in the paper. This separation is useful in financial economics. For instance, we show that the equity premium of an asset with an ambiguous return can be decomposed into the standard equity premium under risk plus an ambiguity premium relative to the standard ambiguity measure defined in the paper.====Second, our paper contributes to the family of decision-making models based on second order probability (SOP) with a recursive structure. The idea of using a SOP to represent ambiguity goes back to Marschak (1975) and Segal, 1987, Segal, 1990 and has reappeared in the recent literature (KMM, 2005; Nau, 2006, Ergin and Gul, 2009, Seo, 2009, Grant et al., 2009, Neilson, 2010). All of these models share a similar recursive utility representation to accommodate ambiguity aversion. But, none of them are additive separable, and therefore they cannot disentangle the ambiguity from risk in the sense proposed in this paper. By assuming extra conditions beyond the SOEU model, we can obtain an additive structure to model a linear tradeoff between utility from risk and from ambiguity. Such a separation in the SOEU model has not been studied in the extant literature. Also, our model allows preferences failing to satisfy the uncertainty aversion axiom (Gilboa and Schmeidler, 1989), which, however, is an axiom subscribed to by KMM and other SOEU models as we highlight in Section 3.1. In Section 3.2, we also point out that, in contrast to KMM and other SOEU models, one of the advantages of our model is that it explicitly reveals how DM trades off the risk component and the ambiguity component of an act in the spirit of the mean–variance analysis, which is not possible in the KMM.====Third, our model adds to the family of models with an additive form that captures decision making under ambiguity as ==== process including both VEU model in Siniscalchi (2009) and the MD preference in Grant and Polak (2013). Our model shares a similar motivation with these two models as the tradeoff between the components in our model can also be interpreted as an anchoring and adjustment process. However, it distinguishes itself from these two models by using the SOEU model to capture ambiguity, which results in an ambiguity measure that is intuitive and tractable.====The rest of paper is organized as follows. In Section 2, we motivate the additive risk–ambiguity model by considering a simplified Ellsberg problem and define two concepts that separate an AA act into a risky component and an ambiguous component. Section 3 presents the main theorem in the paper for the additive model. In Section 4, we discuss the standard ambiguity measure obtained from our additive model and show that under a Taylor approximation, this measure reduces to the variance of the second order probability distribution. We also show that, in the context of ambiguous returns, the equity premium consists of two parts: the risk premium and the ambiguity premium. In Section 5, we further decompose our model of risk and ambiguity to obtain the standard value–risk–ambiguity tradeoff model. Section 6 concludes the paper.",An additive model of decision making under risk and ambiguity,https://www.sciencedirect.com/science/article/pii/S0304406819301041,21 October 2019,2019,Research Article,256.0
Zheng Charles Z.,"Department of Economics, University of Western Ontario, London, ON, Canada, N6A 5C2","Received 21 February 2019, Revised 2 September 2019, Accepted 1 October 2019, Available online 18 October 2019, Version of Record 24 October 2019.",https://doi.org/10.1016/j.jmateco.2019.10.001,Cited by (5)," equilibrium. The class of distributions defined by this condition is larger than the class under the passive updating assumption in the mechanism design literature on collusion. The off-path posterior belief most conducive to collusion entails a continuation equilibrium as if the collusion vetoer were bidding against a naive, value-bidding rival.","The mechanism design literature on bidding collusion has provided a major framework to characterize, and thereby to design policies to preempt, the possibility of collusion among oligopolists. In that framework, competition among oligopolists is usually modeled as an auction, and oligopolists as privately informed bidders; before the auction, an outside mediator proposes a side contract to the bidders such that if all accept it then all abide by it during the auction. A crucial question, in calculating bidders’ incentive to collude, is what a bidder’s outside option is if he rejects the proposed side contract. The answer, other than simply assuming the outside option as an exogenous expected payoff (McAfee and McMillan, 1992), is a ==== assumption that, after the breakdown of collusion, the posterior beliefs remain to be the prior (Laffont and Martimort, 1997, Laffont and Martimort, 2000, Che and Kim, 2006, Che and Kim, 2009, Pavlov, 2008).==== ==== In a sequential setting, this assumption is consistent with Bayesian updating only if the side contract is either always accepted or always rejected. But then, in any case where occurrence of collusion is not completely null, the side contract must be always accepted in equilibrium; with breakdown of collusion an off-path event, posterior beliefs need not be the passive one. That means there might be other posterior beliefs more conducive to collusion than the passive one.==== ==== Thus, the design of collusion-preempting policies might have underestimated the possibility of collusion that the literature is set up to characterize and preempt.====To provide a fuller characterization of the possibility of collusion, this paper relaxes the passive updating assumption and examines bidders’ incentive to collude before a first-price auction with independent private values. I restrict the number of bidders to two, and the side contracts to type-independent side transfers.==== ==== The paper characterizes the entire set of bidders’ prior distributions given which there exists a type-independent side-transfer proposal that admits a perfect Bayesian equilibrium (PBE) where both bidders accept the proposed transfer for sure (Theorem 1). The set is strictly larger than the one based on the passive updating assumption (Corollary 3).====The condition that defines this collusion-implementable set is purely about the primitives (Ineq. (10) or (11)). Intuitively speaking, it requires that a bidder’s lowest possible valuation be large enough to be split between the two bidders so that each gets a share no smaller than a hypothetical surplus in bidding against a naive, value-bidding opponent (Remark 1). If this condition fails, no type-independent side transfer can be fully acceptable by both bidders. If the condition is met, whereas, the mediator can propose a side transfer equal to the hypothetical surplus and get accepted by both for sure at a PBE. In this PBE, the posterior belief about a bidder who vetoes the proposal is that his valuation is the highest in his type support. This posterior belief system is the most conducive to collusion, supporting the worst off-path continuation play for both the highest and lowest types of a bidder who unilaterally rejects a collusive proposal (Eqs. (8) and (9)), as if the vetoer were bidding against a naive, value-bidding opponent. An implication of our condition for collusion is that the prospect for both bidders to always agree on a collusive side transfer is higher when their prior distributions become more stochastically dominant (Corollary 2).====Thus, the idea amounts to incentivizing collusion up to the point of making a threat that should a bidder reject the collusive deal he would be penalized by a naive, level-zero, and hence fiercest competitor in the auction. Such a seemingly incredible threat is rationalized as a continuation equilibrium by the off-path posterior belief that bidder ====’s type is equal to the highest possible one. With such a belief, his rival ==== expects bidder ==== to play a mixed strategy that randomly selects a bid from a set. Let ==== denote the infimum of this set. Then bidder ==== would not bid below ==== (thereby getting zero winning probability) unless her type is below ====. Thus, if bidder ==== bids ====, he can outbid only those types of ==== that are below ====, as if ==== were at least as fierce as a level-zero bidder who does not shade her bid. While bidder ==== may submit other bids, expecting ==== to abide by this continuation equilibrium, to which his best response is to mix the bids down to ====, bidder ==== is indifferent about the bids. Hence his expected payoff is indeed equal to the one from bidding ====, which is no more than what he would get from bidding ==== against a naive, level-zero rival (Remark 2).====Eső and Schummer (2004) and Rachmilevitch (2013) have considered collusion between two bidders without the passive updating assumption, and analyzed pure-strategy equilibria where one bidder makes a type-independent proposal to the other and not all types of the latter are expected to accept it. I allow for all mixed-strategy equilibria. Focusing on the case where passive updating is not Bayesian inconsistent, this paper considers complete collusion, where almost all types of each bidder accept the collusive proposal.",Bidding collusion without passive updating,https://www.sciencedirect.com/science/article/pii/S030440681930103X,18 October 2019,2019,Research Article,257.0
Dosis Anastasios,"Department of Economics — ESSEC Business School and THEMA, 3 Av. Bernard Hirsch, B.P. – 50105, Cergy, 95021, France","Received 17 February 2019, Revised 4 September 2019, Accepted 19 September 2019, Available online 4 October 2019, Version of Record 11 October 2019.",https://doi.org/10.1016/j.jmateco.2019.09.003,Cited by (2),This paper studies general ==== markets and proposes a scheme of transfers among a regulator and insurers that discourages risk selection and promotes efficient competition. The proposed scheme conditions transfers on the ex post profits of insurers and requires the regulator to hold minimal information to implement it. Equilibrium exists and each equilibrium allocation is efficient in any environment with a finite number of types and states even if single-crossing is not satisfied. I argue that the proposed scheme features the characteristics of ex post risk adjustment.,"In their pioneering contribution, Rothschild and Stiglitz (1976) demonstrate that in insurance markets with adverse selection, insurers have an incentive to attract low-risk and healthy customers and avoid attracting high-risk and ill customers. This practice is known as risk selection and constitutes a major concern for policy makers worldwide. Various policies, such as risk adjustment, individual mandates, or/and community rating, have been implemented to correct risk selection. This paper proposes an alternative simple policy, which features the characteristics of ex post risk adjustment, and shows that this discourages risk selection and promotes efficiency in general private health insurance markets.====The analysis is performed in a market inhabited by a continuum of risk-averse consumers. Each consumer can be in a health state that requires costly health care. There is a finite set of possible consumer types that differ in their risk of requiring health care, wealth and cost of receiving care. Insurers compete in insurance plans but are unable to directly price discriminate among consumers due to unobservable characteristics correlated with risk and/or community rating.====The proposed scheme engages in transfers between the regulator and each insurer based on the realised profits of all insurers. The rationale of such a scheme results from the observation that an insurer that engages in risk selection imposes a negative externality on its rivals because the rivals bear the burden of serving high-risk individuals. Therefore, transfers force insurers to internalise this negative externality.====The main characteristic of the proposed scheme is that if an insurer realises strictly positive profits while at least one of the remaining insurers realises strictly negative profits, the profit-making insurer transfers its entire profit to the regulator; the regulator evenly distributes as a subsidy the sum of the profits to the loss-making insurers. I show that under this scheme, equilibrium exists in all possible parameters, and each equilibrium allocation is efficient.==== ==== This result has the following intuitive explanation. Suppose that efficiency requires cross-subsidisation.==== ==== Assume that all insurers offer an efficient menu of contracts. Would an insurer engage in risk selection? Engaging in risk selection harms its rivals (i.e., the fundamental negative externality mentioned above). If the transfer is set equal to the profits earned by such insurer, any gains from risk selection are outweighed by the transfer, rendering the deviation unprofitable.====A crucial point in the argument described above is that any proposed policy should not distort efficient competition. Thus, a regulator would like to ensure that companies compete towards the efficient (constrained Pareto) frontier. The main contribution of the paper is that, under the proposed scheme, each equilibrium allocation is efficient. To confirm that, suppose that all insurers offer an inefficient menu of contracts. For each such menu, another menu that attracts all types, i.e., not only the most profitable ones, exists, which yields strictly positive profits and does not impose any losses on rivals. Hence, no transfer is imposed, enabling effective market competition in accordance with the theory of efficient competitive markets.====Notably, because the regulator simply redistributes the transfers collected from insurers that realise profits to insures that realise losses in such a way that the budget is balanced, the scheme features the characteristics of ex post risk adjustment. In a conventional ex post risk adjustment scheme, the regulator subsidises insurers based on the (ex post) realised cost of health care.==== ==== Unlike conventional ex post risk adjustment, the regulator engages in transfers based on realised profits instead of costs.====In addition to its simplicity, the proposed scheme has another notable advantage because to be implemented, it only requires the regulator to possess information about the profits (or losses) realised by the insurers rather than the menus of contracts that the insurers offer.==== ==== One can unquestionably argue that the realised profits are public information, especially in large markets with insurance corporations.====The remainder of this article is organised as follows. In Section 2, I describe the model. In Section 3, I describe the transfers, game and equilibrium. In Section 4, I propose a scheme of optimal transfers and prove the existence and efficiency of equilibrium. In Section 5, I discuss the results and other aspects of the paper. In Section 6, I conclude the paper. The formal definitions are provided in Appendix A, and proofs are provided in Appendix B.",Optimal ex post risk adjustment in markets with adverse selection,https://www.sciencedirect.com/science/article/pii/S0304406819301028,4 October 2019,2019,Research Article,258.0
"Andonie Costel,Kuzmics Christoph,Rogers Brian W.","Harris School of Public Policy, The University of Chicago, United States of America,Department of Economics, University of Graz, Austria,Department of Economics, Washington University in St. Louis, United States of America","Received 5 October 2018, Revised 15 August 2019, Accepted 17 September 2019, Available online 28 September 2019, Version of Record 19 October 2019.",https://doi.org/10.1016/j.jmateco.2019.09.002,Cited by (1),How should we make value judgments about ,"Why might a social planner have a concern for inequality? We take up the following answer. As the positions in society become increasingly unequal, each individual will have increasingly strong preferences to occupy the more desirable positions rather than the less desirable positions. But, as those positions are scarce, incentives are brought into stronger conflict, resulting in wasteful competition. As competition for those positions intensifies, the actions that individuals take therefore result in larger inefficiencies, as individuals optimally compete more aggressively for the better roles. These inefficiencies lower the ex ante expected payoff to all.==== ====To model this intuition we propose a framework which focuses explicitly on this connection between the ==== in a society and the ==== of the outcomes it produces. Outcomes are generated through a contest, modeled as a non-cooperative game. Let society be identified with a list of wealth levels (or prizes), ====. The planner evaluates the desirability of ==== by the expected payoff of the contest’s symmetric equilibrium. That is, the planner prefers society ==== to society ==== precisely when the expected payoff from the contest is higher under ==== than ====.====We have two main goals with this approach. First, we demonstrate that the equilibrium properties of such a contest can be used to generate what we call ==== measures of the inequality in a given society of roles that are perfectly consistent with the axiomatic literature on inequality measures as in e.g. Fields and Fei (1978), Shorrocks (1980), Foster (1983), Shorrocks (1984), and surveyed in Sen and Foster (1997). In particular we show that the equilibrium payoff in different models of contests leads to different inequality measures with particular contests leading to, among others, the mean log deviation (MLD) measure introduced by Theil (1967) and Rawls’s (1971) maxmin measure, and even the Gini coefficient. Second, we use our efficiency-based measures to discuss the tradeoff between equity and efficiency in a society. In particular, we provide necessary and sufficient conditions (for different models of contests) under which a unilateral increase in one prize (keeping constant all other prizes) is detrimental to the overall efficiency in society.==== ==== The non-cooperative formulation on which we rely naturally addresses this question through the equilibrium payoff. Increasing an individual income in society is preferable, all else equal, but if it is already a large prize it also increases competition for it. The net impact in our framework is nontrivial. In this sense, one can view the equilibrium payoff as playing the role of a social welfare function, as advocated by Atkinson (1970), for the purpose of evaluating efficiency-inequality tradeoffs.====Before proceeding, we offer a brief remark on how we place our contribution in the history of thought on inequality. The “veil of ignorance” is, of course, a thought experiment with a long tradition, introduced to the economics literature by Harsanyi (1953) and expanded upon by Rawls (1971). Behind the veil, an individual is asked to evaluate a society without any knowledge of her own circumstances. Under Harsanyi’s (1953) formulation, an individual imagines she will be placed into one of society’s roles uniformly at random, whereas for Rawls (1971) the individual thinks as if she will be assigned to the least desirable place in society. Rather than adopting either of these notions of what we might call “fate”, we suppose instead that individuals believe they will come to occupy a position in society through the (equilibrium) outcome of a contest. In this sense, our framework is very much in the spirit of Harsanyi (1953), with the inclusion of an explicit non-cooperative model for fate, in which an individual’s preferences are dictated by an expected utility calculation at a symmetric ex ante stage.====In this paper we study three different models of a contest, each a class of non-cooperative games. We have looked to the modern theory of inequality to guide us somewhat in our choice of these games and the equilibrium concept that we employ. This literature has produced sharp characterizations of a number of measures, allowing us to understand their fundamental properties, see e.g. Fields and Fei (1978), Shorrocks (1980), Foster (1983), Shorrocks (1984) and Sen and Foster (1997). We connect our work to this approach in part to give some confidence that our approach is compatible with axiomatic considerations. The axiomatic literature has agreed on three key axioms that any measure of relative inequality must satisfy, which we will study vis à vis our efficiency-based measures.====The first key axiom is ====, i.e., the measure should depend only on the list of possible prizes and not on who gets which prize. In other words the inequality measure should not depend on the names or other characteristics of the individuals. This implies, in the case of efficiency-based measures, a focus on symmetric equilibria, in which each individual chooses the same strategy, so that the equilibrium payoff depends only on the list of prizes and not on the individuals’ names. We would also argue, beyond this technical point, that restricting attention to symmetric equilibria is more in line with the “veil of ignorance” view. The evaluation by a player in our contest is deliberately taken at the ex ante stage, before the individual learns anything about her type that might distinguish her from other individuals. She should, therefore, view the game in exactly the same way as any other individual at the ex ante stage. We therefore require the game to be played in a symmetric way, and so all of our efficiency-based measures of inequality satisfy the anonymity axiom.====The second key axiom is ====. We focus throughout on contests in which payoffs are affine functions of the monetary prizes mainly to allow our models to be compatible with homogeneity. Observe that, in this case, if payoffs are all multiplied by a given factor, then neither the symmetries nor the incentives in the game change   (see   e.g. Alos-Ferrer and Kuzmics (2013, Proposition 4)), and so the equilibria are invariant.==== ==== Accordingly, it is natural to construct an efficiency-based inequality measure that is homogeneous of degree zero, allowing one to focus attention on relative inequality.====There is an important auxiliary reason to focus on affine payoffs. The Harsanyi (1953) version of the veil of ignorance argument asks an individual to evaluate, according to her own utility function, a society, assuming that she wins each prize with equal probability. If the individual judges less equal societies to be worse, then it must be that her utility function for money exhibits risk-aversion. The affine payoffs in our models allow us to demonstrate that the efficiency-based measures deliver similar judgments about inequality even when all individuals are risk-neutral. In our setup, then, it is the nature of how the contest captures inefficiencies of competition, rather than individual preferences, that drives a judgment in favor of a more equal society. This illustrates how the non-cooperative view of inequality we advance here, even though it operates very differently from an axiomatic approach, permits similar judgments.====The third of these axioms is the ====, which is perhaps the single most important axiom that makes these measures really measures of inequality. It requires that the measure respond negatively when transferring wealth from a poorer position to a richer position (see Pigou (1920), Dalton (1920)). We provide conditions for each of our contest models characterizing under which the Pigou–Dalton axiom is satisfied.====We now describe the three models and the results we obtain for each. The first model is based on an “allocation game” taken from Kuzmics et al. (2014). In an allocation game, each of ==== players demands one of ==== prizes. If all demands are unique, then players receive their demanded prizes. Otherwise, if there is any mis-coordination, all players receive zero. An allocation game can be thought of as a discrete version of a Nash demand game, or alternatively as a generalized version of battle of the sexes. Allocation games represent, in a sense, the simplest model of a contest that admits the features we require and, therefore, provides a proof of concept for the approach taken here. In its unique symmetric equilibrium with positive payoffs, if prizes are all equal then the equilibrium mixture is uniform. As prizes become less equal, the equilibrium strategy places higher probability on better prizes, and this decreases the probability of successful coordination, lowering the expected payoff.==== ==== In fact, we show that the ex-ante payoff of this equilibrium is a monotone transformation of the mean log deviation (MLD) measure introduced by Theil (1967).==== ====The second model of a contest is a class of repeated allocation games, in which we focus on symmetric and stationary strategies. In this model, mis-coordination is followed by subsequent stages in which players again compete, until successful coordination is achieved. Our motivation for studying these games is twofold. The class of repeated allocation games is still very simple and based on the same ideas as the one-shot allocation games, but are parameterized not only by the prizes at stake but also by a common discount factor. This allows us to introduce a class of efficiency-based measures of inequality, one for each discount factor, nesting the MLD measure from the one-shot game. Moreover this class provides sufficient generality to nest Rawls’s (1971) maxmin measure, which occurs in the limit where players become unboundedly patient. Notably, this result obtains for a risk-neutral decision maker performing an (equilibrium) expected utility calculation. Using this model, in Proposition 1 we then show that the equilibrium payoff of the repeated allocation game satisfies Pigou–Dalton for every discount factor (and so the result applies to the one-shot allocation game as well).====The third model we study is more in spirit with the recent literature on contests (see e.g. Konrad (2009) for a survey), in that individuals compete for prizes by exerting unproductive, but costly, effort.==== ==== While we studied this third model in an attempt to make our results more general, we stress that neither this model nor those based on the allocation game are intended to be of practical importance. Rather, the models are meant to distill the forces of competition into a tractable framework. These contests are, it turns out, sufficiently general that their associated efficiency-based measures need not satisfy Pigou–Dalton. In Proposition 3, we provide an exact characterization of the condition on the contest’s allocation rule that corresponds to the Pigou–Dalton property. Essentially, the condition requires that an increase in one individual’s effort must lead to a probability distribution over prizes that ==== the original distribution in terms of the ====. We then show that certain parameterizations, in particular an ====-prize generalization of the Tullock contest, as in Clark and Riis (1996), Clark and Riis (1998) and Fu and Lu (2012), give rise to an equilibrium that ranks inequality identically to the Gini coefficient.====Having now described our models, let us turn attention to the equity-efficiency tradeoff. In the one-shot allocation game, we characterize the condition under which increasing the value of a prize leads to an increase in expected payoff, which captures a net positive effect of such a change to society’s roles. Our result shows that such a change is desirable provided that the prize is not too great compared to the average prize. For the repeated allocation game, it turns out that the answer depends non-trivially on the discount factor, such that for more patient players one may reach the opposite conclusion than is reached for the one shot allocation game. In Proposition 2, we characterize when it is that increasing the value of a prize results in a social improvement. Finally, for the allocation contests with effort exertion, we demonstrate a more specialized result, that increasing the value of the highest prize can have either a positive or negative effect on social welfare. For the case of the ====-prize Tullock contest, we develop a simple condition on parameters such that this increase leads to a reduction in expected payoff.====Our results, especially those from the allocation contests, have interesting connections to a line of work in the contest literature. For example, Clark and Riis (1998) and Schweinzer and Segev (2012) investigate the distributions of prizes in Tullock contests with complete information that maximize players’ effort. Moldovanu and Sela (2001) and Moldovanu et al. (2012) study the same question, but in contests with incomplete information, and where contestants can be punished for low effort. Other papers, e.g. Hopkins and Kornienko (2010), investigate how the distribution of prizes, or distribution of endowments (e.g. skills) affects contestants’ welfare in contests with incomplete information; or Hopkins and Kornienko (2004), Hopkins and Kornienko (2009) investigate analogous questions in contests where contestants obtain an extra “gratifying” payoff from being high-ranked in the distribution of consumption. Finally, and perhaps closest to the idea of our paper, we know of two papers that connect equilibrium properties of contests to known measures of inequality. First, Esteban and Ray (2011) propose a model of conflict among various groups in a society, and show that the“degree of conflict”, in equilibrium, is linearly connected to some measures of inequality including the Gini coefficient and the Herfindahl–Hirschman index. Second, Vesperoni (2015) studies a number of particular contests and shows that the players’ effort as a percentage of the sum of prizes is proportional to the Gini index, or a generalized variation of the Gini index, generated by the prize structure. The settings for which this property holds include the pair-swap contest, as developed in Vesperoni (2016), the best-shot or worst-shot contests, and possibly others.====The paper proceeds as follows. Section 2 defines a measure of inequality and presents the axioms of anonymity, homogeneity, and the Pigou–Dalton principle. We study the equilibria of allocation games in Section 3, while we explore the generalization to repeated allocation games in Section 4. Section 5 presents the model of a contest operating through the exertion of costly effort. We conclude and offer final remarks in Section 6. An appendix collects proofs omitted from the main text.",Efficiency-based measures of inequality,https://www.sciencedirect.com/science/article/pii/S0304406819301016,28 September 2019,2019,Research Article,259.0
Boonen Tim J.,"Amsterdam School of Economics, University of Amsterdam, Roetersstraat 11, 1018 WB, Amsterdam, The Netherlands","Received 9 May 2019, Revised 23 July 2019, Accepted 17 September 2019, Available online 25 September 2019, Version of Record 28 September 2019.",https://doi.org/10.1016/j.jmateco.2019.09.001,Cited by (14),"This paper studies optimal insurance in partial equilibrium in case the insurer is protected by limited liability, and the multivariate insured risk is exchangeable. We focus on the optimal allocation of remaining assets in default. We show existence of an equilibrium in the market. In such an equilibrium, we get perfect pooling of the risk in the market, but a protection fund is needed to charge levies to policyholders with low realized losses. If policyholders cannot be forced ","This paper studies optimal recoveries in insurance, and their effects on prices in equilibrium. We use an agency model, where a non-life insurer is protected by limited liability. In case of a default, the remaining assets of the insurer are (at least partially) allocated to the policyholders. In practice, proportional methods are very popular (Araujo and Páscoa, 2002, Sherris, 2006, Ibragimov et al., 2010, Laux and Muermann, 2010). we show that using a proportional method to allocate the recoveries may yield welfare losses in the economy. Moreover, when the multivariate insurance risk is exchangeable,==== ==== we characterize the optimal method instead. In the literature on deterministic bankruptcy problems, this optimal method is called a constrained equal loss (CEL) rule. We study stochastic bankruptcy problems that arise endogenously in insurance contract design.====In the literature on bankruptcy problems, a bankruptcy problem describes the deterministic situation in which we have to allocate a given amount (often referred to as estate) among a group of claimants when the available amount is not sufficient to cover all claims. A bankruptcy rule calculates shares for claimants such that (1) no agent gets more than its claim, and (2) all get a non-negative share. For an overview of bankruptcy problems in practice and bankruptcy rules, we refer to O’Neill (1982), Aumann and Maschler (1985) and Moulin (2000) or the overviews of Moulin (2000), and Thomson (2003). In a natural way, any default situation in insurance with limited liability is related to a bankruptcy problem where the realized multivariate insurance risk represents the claims and the realized asset value is the size of the estate. Then, any bankruptcy rule can be taken to define a solution to allocate the remaining assets to the policyholders ====.====Habis and Herings (2013) study a stochastic bankruptcy problem, where they show that stability among the claimants is possible. Moreover, Kıbrıs and Kıbrıs (2013) and Karagözoğlu (2014) study investment problems, where bankruptcy rules are applied in case of default. In all these papers, default is however an exogenous event, that is not affected by the investment decisions of the economic agents. We apply the concept of stochastic bankruptcy rules to a partial equilibrium setting in insurance with limited liability, where default occurs endogenously.====Doherty and Schlesinger (1990), Cummins and Mahul (2003), Bernard and Ludkovski (2012), and Peter and Ying (2019) study insurance contract design with limited liability by modeling default as an exogenous event, which may be correlated with the insurance risk of the policyholder. Moreover, Biffis and Millossovich (2011), Asimit et al. (2013), Cai et al. (2014), and Filipović et al. (2015) all study optimal insurance contracts with endogenous default risk. This means that default is affected by the design of insurance contracts. All these approaches however rely on the assumption that there is one insurer and one policyholder. We follow the approach of Filipović et al. (2015) to study optimal risk taking and premia of an insurer in equilibrium, where default occurs endogenously. We differ by allowing for multiple policyholders. In case there are multiple policyholders, the issue to allocate the remaining assets in default exists naturally. A bankruptcy rule is then applied ====, and we refer to such a stochastic bankruptcy rule as a recovery rule. A recovery rule is used to allocate, ====, the remaining assets in case of default. Such recovery rules affect the insurance premiums that are paid ====, and determined by the insurer. Rees et al. (1999) study optimal insurance regulation with a given recovery rule. Moreover, Sherris (2006), Ibragimov et al. (2010), Laux and Muermann (2010) and Bauer and Zanjani (2016) all assume a proportional recovery rule. An exception is Araujo and Páscoa (2002), who focus on existence of general equilibria with a continuum of policyholders. There are frequent real life deviations from the proportional rule, and some are actually contemplated by law (Araujo and Páscoa, 2002).====This paper extends the model of Mahul and Wright (2004) to the setting of partial equilibria in case the insurer is protected with limited liability. Mahul and Wright (2004) study optimal risk-sharing among insurers via pools in the context of catastrophe insurance. Their objective is to maximize a weighted utility of all insurers. Then, all insurance risk is pooled ====, and then redistributed among the insurers. The premium is allowed to be decided ==== as well. This problem is similar to the classical Pareto optimal risk-sharing problem in Borch (1962), but it now includes exposure constraints. Mahul and Wright (2004) describe the constrained equal loss recovery rule and characterize it via an ==== participation constraint. Our focus is different as we study the effect of rules to allocate default losses in equilibrium, and their effects on insurance premia and the risk taking behavior of the insurer.====Our key assumption is that the multiple policyholders are ==== identical via an exchangeability condition on the multivariate insurance risk. Popular examples of exchangeable risk are the case where risk is independent and identically distributed (====), and the case where risk is formulated as a common shock model (Marshall and Olkin, 1967). We show in this paper that a partial equilibrium exists under some regularity conditions. Moreover, we find that it is optimal for the insurer to force some policyholders to pay ==== levies to cover losses in default. This leads to a partial equilibrium with perfect pooling of the insurance risk. If the insurer cannot force policyholders to pay ==== a levy, we find that the constrained equal loss (CEL) recovery rule is the optimal recovery rule in equilibrium. Proportional bankruptcy costs do not affect optimality of the CEL rule, but it may lead to a different insurance premium and risk taking behavior of the insurer. Our results also hold in the absence of a regulator (monitoring device). Without a regulator, the insurer will invest in such a way that it maximizes its own expected profit — not taking into account the utility of the policyholder. Then, in the absence of leverage, we show that the insurer will always invest all its assets in the risky technology.====This paper is set out as follows. Section 2 defines the model set-up. Section 3 characterizes the optimal pooling and recovery rules. Section 4 shows existence of a partial equilibrium. Section 5 studies incentive compatibility. Finally, Section 6 concludes. All proofs are delegated to Appendix A.",Equilibrium recoveries in insurance markets with limited liability,https://www.sciencedirect.com/science/article/pii/S0304406819301004,25 September 2019,2019,Research Article,260.0
"Gentry Matthew,Komarova Tatiana,Schiraldi Pasquale,Shin Wiroy","Florida State University, United States of America,London School of Economics, United Kingdom of Great Britain and Northern Ireland,London School of Economics and CEPR, United Kingdom of Great Britain and Northern Ireland,Korea Institute for Industrial Economics and Trade, The Republic of Korea","Received 21 December 2018, Revised 12 August 2019, Accepted 29 August 2019, Available online 19 September 2019, Version of Record 11 November 2019.",https://doi.org/10.1016/j.jmateco.2019.08.006,Cited by (2),"We explore existence and properties of equilibrium when ==== bidders compete for ==== objects against many local bidders who bid for single objects only. We then consider monotone ==== building on Jackson, Simon, Swinkels and Zame (2002), and demonstrate that these exist in general. These existence results apply to many auction formats, including first-price, second-price, and all-pay.","Simultaneous bidding for multiple objects is a commonly occurring phenomenon in many real-world auction markets, but surprisingly little is known about the properties of equilibria in games involving simultaneous auctions when bidder payoffs are non-additive.==== ==== For example, when auctioning drilling rights in the US Outer Continental Shelf, the US Minerals Management Service typically offers (and bidders typically bid on) a large number of drilling tracts simultaneously. Prior empirical work (e.g. Hendricks and Porter, 1988, Hendricks et al., 2003) suggests that economically important complementarities may exist between tracts in close proximity. Yet little is presently known – either theoretically or empirically – about how such synergies might affect equilibrium behavior in such markets.==== ====This paper analyzes equilibrium within a class of mechanisms we refer to as ==== for complementary goods. In this setting, a collection of ==== objects are offered for sale to a set of ==== bidders. Bidders have independent private valuations over combinations of objects, where objects are complements in the sense that bidders’ valuations are supermodular in sets of objects won. Auctions are simultaneous in the sense that bidders may bid on each object individually but may not submit contingent or combinatorial bids, and ==== in the sense that each object ==== is allocated to the highest bidder in auction ==== and payments in auction ==== depend only on bids in auction ====. So long as all auctions are standard, auctions for different objects may have different formats. For simplicity, we frame discussion in terms of a single auctioneer, although this is inessential for our results.====The simultaneous standard auction game raises a number of significant theoretical challenges. Even assuming independent private types, each bidder’s preference structure could in principle be as complex as a complete ====-dimensional set of valuations assigned by that bidder to each of the ==== possible non-empty subsets of objects. Meanwhile, the simultaneous standard auction permits bidders to submit (at most) ==== individual bids on the ==== objects being sold. Furthermore, as usual in auctions, payoffs in the resulting game may be discontinuous in bids. The end result is a discontinuous Bayesian game with high-dimensional types for which even basic properties – such as existence of Bayesian Nash equilibrium – are challenging to establish in general.====Section 2 introduces the model and describes what we mean by a standard auction. The framework of standard auctions includes many auction formats, such as first-price, second-price, and all-pay. Section 3 introduces a partial order on bidder types characterized by a finite number of linear inequalities on marginal valuations. These inequalities define a cone (with a nonempty interior) strictly contained in the first-orthant cone of the ====-dimensional type space, which is the cone describing the usual coordinatewise order. We show that even the (strong) assumption of supermodular valuations is insufficient to ensure monotonicity of best replies with respect to the usual coordinatewise order on types—in fact, a strict coordinatewise increase in type can induce a strict coordinatewise decrease in best-reply bids. Our stronger partial order, however, is sufficient for monotonicity in that each bidder ==== has an interim best reply such that an increase in ====’s type with respect to our partial order will imply an increase in ====’s bids with respect to the usual coordinatewise order.====Equipped with this preliminary result, Section 4 builds on the methodology of Athey (2001), McAdams (2003) and Reny (2011) to establish existence, in any simultaneous standard auction, of pure strategy equilibria on finite bid spaces which are monotone in the sense above. This result turns on one additional condition, which clarifies the relationship between the partial order cone and the support of bidder’s joint distribution of valuations. Namely, we show that this relationship is “sufficiently rich” if this distribution is absolutely continuous with respect to the Lebesgue measure in the ====-dimensional space and the support of this distribution is regular enough. While the existence of a pure strategy Bayesian Nash Equilibrium follows from Milgrom and Weber (1985), the monotone characterization of the equilibria under a suitable partial order on types is novel in this setting.====We then proceed to consider continuous bidding spaces. First, in Section 5.1, we consider a special case similar in spirit to Krishna and Rosenthal (1996), in which a single global bidder bids in simultaneous first-price auctions for ==== objects against a collection of local bidders who bid for single objects only.==== ==== Building on proof techniques in Reny (2011), we show existence of a pure strategy Bayesian Nash equilibrium which is monotone with respect to our partial order. To the best of our knowledge, both existence and monotonicity are novel in this setting. Moreover, monotonicity is here pivotal in establishing existence; the proof turns on passing from a sequence of monotone equilibria on discrete spaces to the limit of this sequence in a continuous space, which is feasible only because the space of monotone strategies is known to be compact in an almost-everywhere convergence metric if the partial order is sufficiently rich (Reny, 2011).====While we believe that this finding is of interest in its own right, this example also serves to highlight a subtle challenge arising in settings with more than one global bidder. Specifically, the interaction between strategic overbidding by global bidder ==== and dependence across auctions of bids by ====’s global rivals leads to failure of a key technical property – better-reply security of Reny (1999) – needed to complete the existence proof. In Section 5.2, we therefore turn to an alternative solution concept, ====, building on the work of Jackson et al. (2002, henceforth JSSZ). JSSZ define the ==== ==== to a given game ==== as the game arising when, in addition to their actions under ====, players also submit cheap-talk indications of their types which the auctioneer may use (only) to resolve ties. A ==== ==== is a strategy profile for bidders plus a tiebreaking rule such that strategies are a Bayesian Nash equilibrium given the tiebreaking rule. Starting from a class of discontinuous games ==== which includes ours, JSSZ establish existence of solutions to ==== in which bidders play ==== as defined by Milgrom and Weber (1985) and communication is truthful.====In the context of simultaneous auctions for complementary goods, we show that these general conclusions can be sharpened in at least three respects. First, rather than permitting bidders to communicate their full ====-dimensional types, we allow bidder ==== to submit (in addition to her bid vector ====) only an ==== vector of cheap-talk signals ====; we refer to this as a ==== to distinguish it from the ==== of JSSZ. Second, in any simultaneous standard auction, we show the existence of a solution to the signaling extension in which the auctioneer’s tiebreaking rule can be characterized by a set of ==== weakly monotone tiebreaking precedence functions ====, where the auctioneer randomizes object ==== independently among the set of high bidders in auction ==== with the highest tiebreaking precedence: i.e. among the set of bidders ==== with ==== and ====. This characterization of tiebreaking sharpens that in JSSZ, implying in particular existence of a solution where allocations and payments in auction ==== depend only on bids and signals in auction ====. Furthermore, whereas JSSZ consider only existence in distributional strategies, we obtain existence in pure strategies which are additionally monotone in a suitable partial order sense.====Section 6 contains several numerical examples of bidding behavior in simultaneous auctions for complementarities, which illustrate our main theoretical results. Proofs of all propositions and lemmas are collected in Appendix.====Our analysis relates to an important phenomenon arising in simultaneous auctions known as the exposure problem. Exposure can occur when bidders try to win several objects without opportunities to express their preferences on combinations of the objects. In this situation, bidders might strategically underbid or overbid in an attempt to avoid winning undesirable bundles. As a result, the auctions can fail to construct efficient allocations, leading some bidders to incur ex-post losses. The exposure problem has been analyzed in various settings within the simultaneous auctions literature, although to the best of our knowledge the available results remain sparse.==== ==== We do not analyze the exposure problem formally in the paper but we do highlight this problem in our examples.",On monotone strategy equilibria in simultaneous auctions for complementary goods,https://www.sciencedirect.com/science/article/pii/S030440681930093X,19 September 2019,2019,Research Article,261.0
Barbos Andrei,"Department of Economics, University of South Florida, Tampa, FL, United States of America","Received 25 August 2018, Revised 18 June 2019, Accepted 22 July 2019, Available online 16 September 2019, Version of Record 20 September 2019.",https://doi.org/10.1016/j.jmateco.2019.07.008,Cited by (4),"In contractual relationships where the agent executes numerous independent tasks over the lifetime of the contract, it is often infeasible to evaluate his performance on all tasks that he is assigned. Incentives under moral hazard are instead provided by randomly determining whether or not to monitor each of these tasks. We characterize optimal contracts implemented with such random monitoring in a stochastic dynamic environment where the agent’s cost type varies over time. We show that the compensation terms the agent is promised for contingencies where monitoring reveals compliance are as good as those for when no monitoring takes place, and for some cost types are better; these latter types receive a monitoring reward. As time passes and the agent becomes richer, the size of the monitoring reward decreases. Compensation on the equilibrium path exhibits downward rigidity, a feature elicited empirically by earlier literature.","The standard literature on dynamic contracting under moral hazard examines situations where the principal makes a noisy observation of the action chosen by the agent in every period of the game.==== ==== In certain contractual environments, it is however infeasible or too costly to acquire information about an agent’s performance on ==== tasks that he executes over the lifetime of a contract. Typical such environments are those where the principal interacts with a large number of agents who each performs numerous independent tasks, or those where monitoring the agent’s performance is a complex activity that requires expending significant resources. In such situations, it may be more convenient to provide incentives by randomly determining in each period whether or not to monitor the agent, thus evaluating his performance only on a random subset of the tasks that he executes.==== ====In this paper we study dynamic contracting under moral hazard in an infinitely repeated game where contracts are implemented with this type of ==== technology. Specifically, we assume that the principal employs a monitoring instrument that, in every period, reveals with some ==== probability the precise effort level chosen by the agent, while with complementary probability reveals no informative signal of it. As we will argue later, this modeling choice of a perfectly observable effort level when monitoring is executed can alternatively be thought of as capturing situations where the principal assigns a certain level of publicly ==== performance to the agent in each period, which the agent then chooses whether to meet or not by exerting a necessary amount of effort that in our model will be assumed to be random. The main goal of our analysis is to derive features of optimal dynamic contracts with random monitoring. As a second key contribution, it also elicits a mechanism through which an empirically observed positive association between compensation and on-the-job tenure can emerge on the equilibrium path of an optimal contract under moral hazard.====In practice, experience with a task is key to uncovering the cost of performing it. We capture this feature in our framework by assuming that the marginal (opportunity) cost of the resources (effort, time, etc.) spent by the agent in executing a current period action depends on a state of nature, referred to as his cost type, which evolves stochastically over time. The realizations of the agent’s cost type across periods are independent, identically distributed, and in our main modeling specification, publicly observed and contractible.====An example of a situation captured by our model is that of a service provider whose workers execute a large number of tasks.==== ==== One such worker, when dealing with a customer, chooses the level of customer satisfaction to deliver, interpreted as his action, while the variable complexity of the issue being addressed determines the cost of delivering that level. Monitoring is often random in these situations. For instance, most car dealerships request feedback from the customers of their service center by asking them to complete a survey about the quality of service performed by various personnel; the imperfect response rate in such surveys means that information about an agent’s performance on any given task is acquired with probability less than one. Another example is that of a project manager whose actions can generate a private benefit at the expense of the payoff created by the project under his administration. Frequently, the magnitude of the noise in the project’s payoff is significantly higher than the magnitude of the potential resource diversions, and thus the manager’s behavior can only be incentivized through a direct auditing of his actions. If such auditing is a complex and therefore costly activity, or if it can only be the result of a whistle-blower’s action, monitoring is or can be performed only at random times. The manager’s actions that are outside the scope of an audit are fully unobservable and thus monitoring is random.====In the recursive formulation of contracts that we employ in our analysis, the agent starts every period with a promised continuation value that captures the equilibrium path expected present value of the stream of future utilities. Accounting for this value, the contract specifies a recommended action for each possible realization of the agent’s type, and a current period transfer and next period continuation value for each contingency that may emerge during the period of play.====When the agent is risk averse, efficiency dictates that the current period transfers that play no role in inducing effort be set at a common value. These transfers are those corresponding to contingencies where monitoring is not performed or where zero effort is implemented. The next period continuation values take a common value across these contingencies as well; moreover, to minimize the intertemporal variance in the agent’s income, this common value is the current period continuation value. The compensation variables specified for contingencies where a positive effort is recommended and monitoring reveals compliance are set at the same common levels whenever these levels are sufficient to provide the agent with incentives to exert effort. For the remaining contingencies, incentive provision requires that the compensation variables be set at higher levels, while efficiency that the incentive compatibility constraints bind. Complying agents of certain types are thus promised a ==== that comes in the form of a higher current period transfer and next period continuation value than the corresponding contract variables with no monitoring.====The monitoring reward promised to each type of agent decreases over time and eventually vanishes. The explanation for this fact is as follows. As the agent’s type==== ==== fluctuates over time, the optimal choice of the next period continuation values, presented above, implies that the current period continuation value weakly increases on the equilibrium path. For any given type, the incentive compatibility constraint thus binds at the beginning of a contractual relationship when the compensation variables are small. While this constraint binds for a type, the effort level specified for that type and the corresponding compensation terms that induce it are stationary, since the marginal cost of implementing effort is unaffected by the level of the ==== period continuation value. To deliver the increasing continuation value, the compensation terms promised for contingencies where monitoring is not executed must improve over time. As the continuation value increases, the monitoring reward thus decreases and is eventually eliminated when the agent is rich enough that the threat of forgoing the accumulated continuation value is sufficiently strong to incentivize effort.====Under the optimal contract from our model, the current period transfer only adjusts upwards on the equilibrium path. A significant amount of empirical evidence points to such a positive relationship between compensation and length of on-the-job tenure.==== ==== Several dynamic models have been constructed where, in equilibrium, compensation exhibits this ====. Some papers proposed that the downward wage rigidity is a consequence of improved job matching over time (Jovanovic, 1979), of learning about the worker’s ability (Harris and Holmstrom, 1982), or of on-the-job learning (Farber and Gibbons, 1996). Other papers argued that it is meant to incentivize effort (Lazear, 1979), to preempt job turnover (Stevens, 2004), or to deter applications from low ability workers (Guasch and Weiss, 1982). On the other hand, under ====, the standard ==== monitoring technology assumed in earlier models typically implies that the equilibrium wage schedule presents downward adjustments whenever realizations of the exogenous noise are sufficiently unfavorable to the agent. One notable exception is Grochulsky and Zhang (2017) who construct a model under moral hazard where the agent is motivated by a mix of endogenous contractual incentives and exogenous market based incentives. Compensation in this environment exhibits downward rigidity on a range of the agent’s continuation value above a certain market-based benchmark where the wage increases whenever the agent’s participation constraint binds. He (2012) considers optimal contracts in environments with moral hazard where effort generates outcomes according to a Poisson process and where the agent can save privately. Because of this latter feature, the agent cannot be incentivized with a decreasing wage schedule during time intervals with no successes. Instead, the threat is that of contract termination, which would keep compensation constant thereafter, but forgo the possibility of future promotions. These result in a downward rigid wage schedule. Besides the key feature of the agent’s saving opportunity, the downward rigidity also hinges on the assumption of a zero probability of success when shirking, which is strategically equivalent to our assumption of a perfectly observable action with monitoring.==== ====The downward rigidity is particularly surprising in our model. While the property hinges on the assumption that the effort is perfectly observable when monitoring is executed, it emerges under the randomness generated by the idiosyncratic realizations of the agent’s cost type and by the stochastic nature of the monitoring technology. The key elements that generated downward wage rigidity in earlier principal–agent models, such as those from Harris and Holmstrom (1982)==== ==== and Grochulsky and Zhang (2017), are the agent’s risk aversion and a participation constraint that changes stochastically over time due to exogenous shocks. In our model, the latter is replaced with an incentive compatibility constraint, generated by the random nature of the monitoring, also stochastically evolving. Together with the earlier models, our paper points thus out to one possible foundation of environments where principal agent models generate downward wage rigidity that consists of a risk averse agent and a stochastically changing constraint.==== ==== In such environments, optimal contracts aim at minimizing the agent’s wage volatility across time and states of the world due to the agent’s risk aversion, but whenever the relevant participation or incentive constraint is not satisfied by current compensation terms under a new realization of the state of the world, these terms need to be updated. The update must occur upwards to satisfy the relevant constraint.==== ====The downward wage rigidity property hinges on the underlying assumption that the principal acquires a ==== informative signal of the agent’s action when monitoring is executed. While one goal of employing this assumption in our model is to construct a benchmark where the imperfect nature of the monitoring is solely due to its randomness, this modeling specification also captures contractual situations where the agent has complete control over the level of his publicly ==== performance. In contrast, an implicit assumption underlying a noisy monitoring technology is that the agent does not observe this measurable level ==== on the resources to expend since the realization of the noise is learned only later. However, in a variety of situations, the agent can in fact choose the level of his measurable performance. For instance, workers can often adjust the intensity of their effort depending on how far they are from meeting a certain expected performance level. Similarly, a manager who has the option to divert resources for private benefit, can choose the extent of a potential diversion which may then be perfectly observable with an audit. In other words, instead of thinking of the output as random for a given level of effort exerted by the agent, as in standard models of noisy monitoring, our model captures situations where the cost of effort is random for a given level of the measurable performance that the agent wants to attain. Our paper suggests that in such situations, the compensation will exhibit downward rigidity even if the recommended effort level varies over time in response to exogenous cost shocks and even if the agent’s performance is not always evaluated by the principal.====This paper belongs to the vast literature on dynamic contracting under moral hazard. Some of the early seminal papers from this literature are Radner (1981), who examined a situation where the publicly observed period outcome depends on the agent’s action and some state of the world which are both privately observed by the agent, Townsend (1982), which studied dynamic contracts under adverse selection, Rubinstein and Yaari (1983), which elicited the role of temporal incentives in mitigating the inefficiencies induced by moral hazard in repeated relationships, and Rogerson (1985), which showed that in a repeated moral hazard model where the agent can neither borrow nor save, the inverse marginal utility of consumption evolves as a martingale. Spear and Srivastava (1987) were the first to employ recursive methods for solving dynamic contracts. Some of the more recent contributions to this literature are Sannikov (2008), which studies contracting under moral hazard in a continuous time model, Biais et al. (2013), which consider a model where the agent exerts effort to preempt large losses and his limited liability hinders risk sharing, and Kovrijnykh (2013) which studies the case where the principal has a limited ability to commit to enforce a contract. Our paper makes a contribution to this literature by studying contracts implemented with random monitoring. Some of the insights that our paper uncovers are familiar from earlier papers, such as a positive relationship between the level of promises about future compensation, as captured by the continuation value, and the current compensation, or a negative relationship between the same variable and current effort, including a retirement of the agent at high enough levels of the continuation value. Other findings, such as the presence of a monitoring reward that is decreasing in the level of the continuation value and over time, or the downward wage rigidity are distinct from most of the earlier literature.====Optimal contracts with random monitoring are investigated in a ==== framework in Barbos (2019).==== ==== An implicit assumption underlying static modeling of contractual environments is that a contract is signed and executed for every action that the agent performs. However, in many real-world situations where random monitoring is employed, the sort of physical constraints or cost structures that preclude monitoring all of an agent’s actions are also likely to preclude ==== of each action. Instead, contracts are usually signed at the outset of a long-term relationship (for instance, at the time when a worker is hired), and following that, the agent executes a series of actions over the period of the contract. In these situations, the principal can design contracts that incentivize the agent to expend resources on an action by means of both current period transfers and promises about future compensation. The goal of this paper is to derive properties of optimal contracts implemented with random monitoring in such dynamic environments.====We are aware of two earlier papers that consider random monitoring in a dynamic setting. Piskorski and Westerfield (2016) considers a contractual model under moral hazard in continuous time where in addition to the standard continuous noisy observation of the agent’s action, the principal also employs a monitoring instrument that stochastically reveals deviations from a prescribed action. The likelihood of detecting a deviation is a function of the costly monitoring intensity and of the extent of the deviation. They analyze the optimal mixture of monitoring and performance-based compensation, and in their main result show that this mixture is generically non-monotonic in the agent’s promised utility. Fraser (2012) considers a two-period contracting problem under random monitoring in the context of agri-environmental policy. He analyzes numerically the effect of various model parameters on the incentives of the agent to deviate from the prescribed policy and illustrates the potential use by the principal of inter-temporal incentives. Another paper related to ours is Halac and Prat (2016), who consider a model in which, as in our paper, the principal does not observe the output generated by the agent, by may receive informative signals of it via a Poisson process with a parameter that depends on the effort exerted by the agent and the investment made by the principal in an attention technology. The quite different environments make their results not directly comparable to ours.",Dynamic contracts with random monitoring,https://www.sciencedirect.com/science/article/pii/S0304406819300813,16 September 2019,2019,Research Article,262.0
"Fu Haifeng,Wu Bin","International Business School Suzhou, Xi’an Jiaotong-Liverpool University, Jiangsu, 215103, China,International School of Economics and Management, Capital University of Economics and Business, Fengtai District, Beijing, 100070, China","Received 25 October 2018, Revised 14 July 2019, Accepted 26 August 2019, Available online 12 September 2019, Version of Record 1 October 2019.",https://doi.org/10.1016/j.jmateco.2019.08.005,Cited by (0), under the two sets of conditions comes naturally as a corollary.,"Endowed with a continuum of agents where each agent has a measure of zero, a large game embodies the essence of a free and competitive economy and has various applications including being used as a good approximation of large finite economy.==== ==== Recently, Khan et al. (2013) generalize the conventional large game model named large game with traits (henceforth LGT) by assigning each player a bio-social trait and show the existence of pure-strategy Nash equilibria. This new game model echoes the consideration of players’ social identities as advocated in Akerlof and Kranton (2000) and Brock and Durlauf (2001).====A Nash equilibrium together with the trait function of an LGT can induce a joint distribution on the product space of traits and actions, which is called in this paper a ====.==== ==== Such a trait–action distribution in equilibrium contains the very essential information of a Nash equilibrium. Now, an important question arises: if we are given an arbitrary distribution on the product space of traits and actions, how can we tell if it is a trait–action distribution in equilibrium? In other words, what are the characteristics of a trait–action distribution in equilibrium?==== ==== We aim to answer this question first and then explore further related issues.====We start with characterizing the trait–action distribution in equilibrium of an LGT under the assumption that both the traits and actions of the game are ====. We show that if this countability assumption is satisfied, then a given distribution is a trait–action distribution in equilibrium if and only if it satisfies the following ====: for every finite subset within the product space, the proportion of players having traits and playing actions in the set is no more than the proportion of players having both traits and at least one best response in it.==== ====We also show via two counterexamples that this result is quite sharp in the sense that if the countability assumption on any one of the two spaces is relaxed, then such a characterization result can fail to work. Based on these findings, it is natural to ask if the countability assumption on the two spaces is both sufficient and necessary for the characterization result to work. In order to answer this question, we firstly show that the characterization inequality actually exists under quite general conditions without relying on the countability assumption. This interesting result helps us give a confirmative answer to the above-mentioned question: given the trait and action spaces, if every trait–action distribution in equilibrium in any LGT can be characterized by the characterization inequality, then both trait space and action space must be countable. This result thus deepens the usual characterization of a trait–action distribution in equilibrium and is novel to the best of our knowledge.====However, the countability assumption on the two spaces can sometimes be quite restrictive. Hence, it is worthwhile to find a replacement for this assumption. Towards this end, we turn to an assumption used by He et al. (2017), which is called the ==== condition.==== ==== This condition requires that the original ====-algebra defined on the agent space is strictly richer than the ====-algebra generated by the game function and trait function on any non-trivial collection of agents. Equipped with this condition, we are able to characterize the trait–action distribution in equilibrium in a similar way.====Moreover, we also show in Theorem 5 that the nowhere equivalence condition is not only sufficient but also necessary to characterize trait–action distribution in equilibrium in any LGT without the countability assumption. This result also enriches the characterization of the nowhere equivalence condition given in Theorem 2 of He et al. (2017). Finally, the existence result of the characterization inequality, together with the previous characterization results, leads directly to the existence of a trait–action distribution in equilibrium and hence also a Nash equilibrium in the two settings of LGT. Thus, it is utterly clear that the crucial point for the existence of a Nash equilibrium is the validity of the characterization result.====The paper is organized as follows. Section 2 introduces the LGT model. Section 3 presents characterization results under countability assumption and shows the existence of the characterization inequality. Section 4 presents characterization results under the nowhere equivalence condition and also shows the existence of a pure-strategy Nash equilibrium for the LGT. For ease of reading, all the proofs are placed in the Appendix.",Characterization of Nash equilibria of large games,https://www.sciencedirect.com/science/article/pii/S0304406819300928,12 September 2019,2019,Research Article,263.0
"Bonetto Federico,Iacopetta Maurizio","School of Mathematics, Georgia Tech, United States of America,SKEMA Business School, Université Côte d’Azur (GREDEG), France,OFCE, Sciences Po Paris, France","Received 2 January 2019, Revised 16 April 2019, Accepted 3 August 2019, Available online 29 August 2019, Version of Record 3 September 2019.",https://doi.org/10.1016/j.jmateco.2019.08.002,Cited by (1),"We analyze the rise in the acceptability ==== in a Kiyotaki–Wright economy by developing a method that can determine dynamic ==== for a class of search models with genuine heterogeneous agents. We also address open issues regarding the stability properties of pure strategy equilibria and the presence of multiple equilibria, numerical experiments illustrate the liquidity conditions that favor the transition from partial to full acceptance of ====, and the effects of inflationary shocks on production, liquidity, and trade.","One central question of monetary economics is how an object that does not bring utility ==== becomes accepted as a means of payment. It is well understood that the emergence of money depends on both trust and coordination of beliefs. While some recognize this observation and simply assume that money is part of the economic system, others have tried to explain the acceptance of money as the result of individuals’ interactions in trade and production activities.==== ==== Among the best-known attempts to formalize the emergence of money in decentralized exchanges is Kiyotaki and Wright (1989, henceforth, KW). The static analysis in KW provides important insights on how the specialization in production, the technology of matching, and the cost of holding commodities condition the emergence of monetary equilibria. Nevertheless, important issues are yet to be resolved. First, one would like to know if and how convergence to a particular long run equilibrium occurs from an arbitrary initial state of the economy. Historical accounts describe the different patterns that societies have followed in adopting objects as a means of payment.==== ==== What are the dynamic conditions that lead individuals in a KW economy to accept either commodity or fiat money? Second, static analysis gives little guidance about the short run consequences of a shock that causes, for instance, a sudden rise in inflation. How does the degree of acceptability of commodity and fiat money change with inflation?====The determination of dynamic equilibria in a KW environment is challenging. In an effort to improve its tractability, new classes of monetary search models have been proposed. These have incorporated some features of centralized exchanges but have also eliminated others, most notably the genuine heterogeneity across individuals and goods, and the storability of goods (see Lagos et al. (2017), for a recent review). Restoring these features turns out to be a useful exercise for characterizing the rise of money as a dynamic phenomenon.====The study of money acceptance in a KW environment requires a departure from the conventional set of tools employed to characterize the dynamics of an economy with centralized markets. Our method combines Nash’s (1950) definition of equilibrium with Perron’s iterative approach to prove the stable manifold theorem (see, among others, Robinson (1995)). This is the first work, to our knowledge, that shows how to determine pure strategy dynamic Nash equilibria in a KW search environment with fiat money. Previous works on the subject considered economies without fiat money, and often assumed bounded rationality.==== ==== An exception is Iacopetta (2019) that also studies dynamic Nash equilibria in a KW environment, but does not consider fiat money. The work presented here extends the KW environment of Iacopetta (2019) in that it introduces fiat money and also considers seignorage (Li, 1994, Li, 1995). In the present environment it is possible to explicitly address how the distribution of individuals’ characteristics affects the emergence of a partial or full monetary equilibrium.====Steady-state results echo those of the inventory-theoreticmodels of money (e.g., Baumol (1952), Tobin (1956), and Jovanovic (1982)). For instance, higher levels of seigniorage may induce some to keep commodities in the inventory instead of accepting money, as a way to minimize the odds of being hit by seignorage tax. The dynamic analysis, however, generates novel results: it shows how changes in the liquidity of assets other than money can alter the proportion of individuals who accept fiat money in transactions. For instance, it reveals that an economy that converges to a long run equilibrium in which all prefer fiat money to all types of commodities (full acceptance) may go through a phase in which only a fraction of individuals do so (partial acceptance).==== ====The remainder of the paper is organized as follows: Section 2 describes the economic environment, characterizes the evolution of the distribution of inventories and money and defines a Nash equilibrium. Section 3 overviews steady-state Nash equilibria for some specifications of the model. Section 4 presents a methodology to determine Nash equilibria. Section 5 illustrates the acceptability of money and discusses multiple steady states through numerical experiments. Section 6 contains welfare considerations. Section 7 comments on future research. Appendix A contains proofs and mathematical details that are omitted in the main text. Appendix B explains how the stable manifold theorem is related to our solution algorithm.",A dynamic analysis of nash equilibria in search models with fiat money,https://www.sciencedirect.com/science/article/pii/S0304406819300886,29 August 2019,2019,Research Article,264.0
"Nunez Manuel,Schneider Mark","School of Business, University of Connecticut, 2100 Hillside Road Unit 1041, Storrs, CT 06269-1041, United States of America,Culverhouse College of Business, University of Alabama, 361 Stadium Drive, Tuscaloosa, AL 35487, United States of America","Received 2 April 2019, Revised 2 August 2019, Accepted 9 August 2019, Available online 23 August 2019, Version of Record 30 August 2019.",https://doi.org/10.1016/j.jmateco.2019.08.004,Cited by (0),"A popular approach to modeling ambiguity aversion is to decompose preferences into the subjective expected utility of an act and an ambiguity index, or an adjustment factor, or a ","One popular approach to modeling attitudes toward ambiguity is to decompose preferences into the expected utility of an act and an ambiguity index (Maccheroni et al., 2006a, Maccheroni et al., 2006b), or an adjustment factor (Siniscalchi, 2009), or a dispersion function (Grant and Polak, 2013). The most general of these specifications is the class of mean-dispersion preferences, axiomatized in the  Anscombe and Aumann (1963) framework by Grant and Polak (2013). The mean-dispersion class is quite large and includes the multiple priors model (Gilboa and Schmeidler, 1989), Choquet expected utility (Schmeidler, 1989), invariant bi-separable preferences (Ghirardato et al., 2004), the variational representation of preferences (Maccheroni et al., 2006a, Maccheroni et al., 2006b), and vector expected utility (Siniscalchi, 2009), as special cases. Mean-dispersion preferences, as characterized by Grant and Polak (2013) can be represented as: ====where ==== is the mean utility of act ==== with respect to a vector probability distribution ==== across all states of nature, that is, ====and ==== is the vector of deviations from the mean utility (i.e., ====, where ==== is the expected utility of ==== in state ====). The function ==== is a measure of (aversion to) dispersion. The dispersion function in the Grant–Polak representation, like the ambiguity index for variational preferences and the adjustment factor in vector expected utility, has very little structure imposed on it.==== ====
 Grant and Polak (2013) remark, “Typically, we will be interested in mean-dispersion preferences that at least partially tie down the admissible probabilities and that put more structure on the dispersion functions” (p. 1367). Indeed, even if one found that the axioms of variational preferences, or vector expected utility preferences, or mean-dispersion preferences are satisfied, one would be at a loss to determine which dispersion measure to use in applications, and the choice of any particular dispersion measure would be rather arbitrary.====In this paper, working in the Anscombe–Aumann framework, we provide a simpler axiomatic characterization of mean-dispersion preferences, which uniquely selects the dispersion function from an infinite class of possible alternatives and which uniquely determines the subjective probability distribution over states. In particular, we characterize preferences of the form: ====with ====and ====where ==== is a unique subjective probability distribution across states and ==== is a scalar in ==== that represents a decision maker’s attitude toward the dispersion of expected utility across states, ranging from dispersion neutral (====) to very dispersion averse (====). We also note how one may characterize preferences for which ==== is in ====. The parameter ==== may alternatively be interpreted as an index of ambiguity attitude, and this interpretation is supported by our analysis in Section 4. The dispersion function ==== is the mean absolute deviation function.====The mean absolute deviation has applications in economics and finance where it has been used to formulate a linear program for portfolio optimization (Konno and Yamazaki, 1991) that can be solved more efficiently than the classic quadratic program in mean–variance analysis. Our representation provides a rational preference foundation for this approach to portfolio selection. Indeed, Konno and Koshizuka (2005) remark: “we have summarized important properties of the MAD model, which after more than a decade since its inception, is still considered to be a mere computational scheme without solid theoretical foundation”. (p. 899). The mean absolute deviation also has applications in other diverse areas in business and engineering including project management, forecasting, statistical process control and risk analysis.====We provide an axiomatic foundation for this measure of dispersion in a model of choice under uncertainty. These preferences also admit a vector expected utility representation, an invariant bi-separable representation, and a multiple priors representation. Hence, in addition to uniquely identifying the dispersion function, our axioms simultaneously pin down specific vector expected utility and multiple prior representations from the large class of preferences admitted by these models.====A somewhat different mean-dispersion model has been developed in a companion paper (Schneider and Nunez, 2015), in which a parallel approach to that of Klibanoff et al. (2005) is followed through the use of second order acts. In addition, the paper in Schneider and Nunez (2015) explicitly assumes the existence of the coefficient in the dispersion function. There is some concern in the literature that the use of second-order acts is problematic since second-order acts may not be observable. Al-Najjar and De Castro (2010) formalize this concern and show that there is “no verification mechanism to determine what the decision maker receives under a second-order act ... even in idealized repeated experiments where infinite data can be observed”. As such, there has been considerable work aimed at axiomatizing a smooth ambiguity aversion model as in Klibanoff et al. (2005) but without relying on second-order acts. Other axiomatic characterizations of second order expected utility have been provided in many alternative setups including Nau (2006), Ergin and Gul (2009), Grant et al. (2009), Seo (2009), and Neilson (2010). For the same reason, it is desirable to axiomatize a model of mean-dispersion preferences that uniquely identifies the dispersion function without invoking second-order acts.====In this paper, we show that it is possible to axiomatize a model of mean-dispersion preferences which uniquely derives the dispersion function, the coefficient representing attitudes toward dispersion, and the subjective probability distribution over states in the standard Anscombe–Aumann setting. This approach also makes the proof much more involved than in the paper by Schneider and Nunez (2015). Moreover, we arrive at our representation using standard axioms in the literature plus two new axioms. We thereby provide a model which uniquely characterizes the dispersion function, in contrast to more general approaches in Maccheroni et al., 2006a, Maccheroni et al., 2006b, Siniscalchi (2009), and Grant and Polak (2013), which admit an infinite set of possible dispersion measures.====Sections 2 Objective and subjective lotteries, 3 Axioms and main results develop an axiomatic foundation for preferences in (2), where the absolute deviation dispersion function ====, the existence, uniqueness, and range of ====, and the existence and uniqueness of ====, can be derived from the axioms. We show that the model explains aversion to ambiguity in Ellsberg’s paradoxes in Section 4. Section 5 concludes.",Mean-dispersion preferences with a specific dispersion function,https://www.sciencedirect.com/science/article/pii/S0304406819300916,23 August 2019,2019,Research Article,265.0
"Liu Peng,Zeng Huaxia","Faculty of Economics and Management, East China Normal University, Shanghai 200062, China,School of Economics, Shanghai University of Finance and Economics, Shanghai 200433, China,The Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai 200433, China","Received 26 January 2018, Revised 28 January 2019, Accepted 22 April 2019, Available online 16 August 2019, Version of Record 21 August 2019.",https://doi.org/10.1016/j.jmateco.2019.04.010,Cited by (8),"We address a standard random assignment problem (Bogomolnaia and Moulin, 2001). A weakly connected domain admitting an sd-strategy-proof, sd-efficient and equal-treatment-of-equals rule is characterized to be a restricted tier domain. Conversely, on such a domain, the probabilistic serial rule is uniquely characterized by either sd-strategy-proofness, sd-efficiency and equal treatment of equals, or sd-efficiency and sd-envy-freeness. Moreover, we provide an algorithm to construct unions of multiple restricted tier domains, each of which admits an sd-strategy-proof, sd-efficient and sd-envy-free rule.","We consider the problem of allocating several indivisible objects to a group of agents, each of whom receives at most one object.==== ==== Each agent reports a strict ordinal preference on objects to the planner, and then the planner assigns a lottery over objects to each agent. The profile of lotteries agents receive is called a ====. To extend a preference on objects to an assessment on lotteries, the ==== introduced by Gibbard (1977) is widely adopted: A lottery is viewed at least as good as another one if the former (first-order) stochastically dominates the latter according to the ordinal preference over objects.==== ==== Equivalently, under the von-Neumann–Morgenstern hypothesis, a lottery stochastically dominates another one if and only if it delivers an expected utility weakly higher than that delivered by the opponent for ==== cardinal utility representing the ordinal preference.====With the stochastic dominance extension, several axioms are defined for designing random assignment rules which associate each profile of reported preferences to a random assignment. First, ==== requires that no reassignment can be arranged such that all agents are at least as well as before, and someone receives a strictly better lottery. Second, random assignment rules should provide incentives for agents to truthfully reveal their preferences. Accordingly, ==== is introduced, saying that for each agent, the lottery delivered by truth-telling stochastically dominates the lottery induced by any preference misrepresentation, regardless of others’ preferences. In addition, ex ante fairness in the sense of either ==== or ==== is imposed. As suggested by the names, ==== requires that agents reporting the same preferences receive the same lottery, while ==== is stronger, and requires that an agent always weakly prefers her own lottery to others’.====Two classic random assignment rules have been widely studied in the literature: the ==== (Abdulkadiroğlu and Sönmez, 1998) and the ==== (Crès and Moulin, 2001, Bogomolnaia and Moulin, 2001). On the one hand, the random serial dictatorship rule is ==== and ====, but not ==== (see Abdulkadiroğlu and Sönmez, 2003, Kesten, 2009). On the other hand, the probabilistic serial rule is ==== and ====, but fails ====. Moreover, an impossibility result has been established by Bogomolnaia and Moulin (2001): When the numbers of objects and agents are identical and at least four, and agents’ preferences are from the ==== with no restriction, no random assignment rule satisfies ====, ==== and ====. Recently, this impossibility has also been established on some restricted preference domains, e.g., single-peaked domains and single-dipped domains (see Kasajima, 2013, Altuntaş, 2016, Chang and Chun, 2017).====These impossibilities raise a natural question: Is there a reasonable restricted preference domain which admits an ====, ==== and ==== random assignment rule? Furthermore, if the answer is in the affirmative, what are the admissible random assignment rules? This paper provides answers to these two questions.====We execute our investigation in a class of rich domains, ==== domains, which occupies a prominent position in the literature (see Remark 1). Two preferences are called ==== if across these two preferences, several pair(s) of contiguously ranked objects are locally switched, and all other objects are identically ranked. A domain is then said ==== if any two distinct preferences are connected via a sequence of preferences in the domain which are consecutively neighbored. This implies that the difference of any two preferences in a ==== domain can be reconciled via a sequence of local switchings. Our first main result (Theorem 1) shows that a ==== domain admitting an ====, ====, and ==== rule must be a ====. To construct a ====, objects are first partitioned into several blocks, each of which contains one or two objects. Then, all preferences are required to respect a common ranking of blocks, referred to as a restricted tier structure. As an example, consider a skyscraper with two apartments on each floor. A restricted tier structure can be elicited according to floors from the top down to the bottom: All agents prefer higher apartments to lower ones. Between two apartments on the same floor, the preferences are arbitrary across agents. The second main result (Theorem 2) searches for the desirable rules on a ====, and characterizes the probabilistic serial rule by either ====, ==== and ====, or ==== and ====.====Logically, our domain characterization result identifies, within the class of ==== domains, the exact boundary between the ones admitting desirable rules and the ones not. Normatively, we treat it as a negative result since a ==== is so restrictive that agents are almost required to have the same preference. However, we believe that our domain characterization result is critically different from and more informative than all existing impossibility results alluded above. First, it implies every existing impossibility result. One can see this by simply verifying ==== and the failure of the restricted tier structure. Second, our domain characterization result implies the nonexistence of ====, ==== and ==== rules on some important domains that have not been studied by the literature of random assignment (see Remark 1). Third, our domain characterization is potentially useful in distinguishing possibility and impossibility when one in the future studies a specific interesting assignment problem, and encounters with a particular restricted preference domain. Last, it suggests that to find a reasonable restricted domain which admits a desirable rule, we have to go beyond the ==== domains. More specifically, given an arbitrary domain (not necessarily ====), we partition it into multiple ==== subdomains which are mutually disconnected.==== ==== Then, the existence of an ====, ==== and ==== rule implies that each subdomain must be a ====. In other words, any domain admitting an ====, ==== and ==== rule must be a union of ====. Following this direction, we provide an algorithm which gradually excludes preferences from the universal domain, and eventually generates a union of ==== which is not ====. More importantly, we show that every domain generated by the algorithm is equivalent to a ==== of Liu (2019), and hence admits an ====, ==== and ==== rule (see Proposition 1).====The rest of the paper is organized as follows. Section 2 introduces the model. Section 3 presents the two main results.Section 4 studies the union of ====, while Section 5 concludes. The Appendix gathers the omitted proofs.",Random assignments on preference domains with a tier structure,https://www.sciencedirect.com/science/article/pii/S0304406819300515,16 August 2019,2019,Research Article,266.0
"Marinacci Massimo,Massari Filippo","Department of Decision Sciences and IGIER, Bocconi University, Italy,School of Banking and Finance, UNSW, Australia","Received 21 May 2019, Revised 16 July 2019, Accepted 30 July 2019, Available online 14 August 2019, Version of Record 16 August 2019.",https://doi.org/10.1016/j.jmateco.2019.07.012,Cited by (9),"We model inter-temporal ambiguity as the scenario in which a ==== learner holds more than one prior distribution over a set of models and provide sufficient conditions for ambiguity to fade away because of learning. Our conditions apply to most learning environments: iid and non-iid model-classes, well-specified and misspecified model-classes/prior support pairs. We show that ambiguity fades away if the empirical evidence supports a set of models with identical predictions, a condition much weaker than learning the truth.","Let ==== be a family of models and ==== a set of prior distributions on ====. If ==== contains more than one prior distribution, its multiplicity represents the ==== ambiguity perceived by a Bayesian decision maker (DM). This setting has been used to highlight the interaction between learning and ambiguity.==== ====Marinacci (2002) formalizes the intuition that if a DM observes repeated draws (with replacement) from the same ambiguous urn, ambiguity fades away over time because he eventually learns the true composition. If the learning problem is well-specified — in the sense that the true probability belongs to the model-class/prior support pair adopted by the DM — ambiguity fades away because all posterior distributions converge to a Dirac distribution on the true model.====Here, we generalize the result in Marinacci (2002) to the case in which the DM does not learn the true probability because his prior view of the world is incorrect — that is, when the learning problem is misspecified in the sense that the model-class/prior support pair does not contain the true model/parameter. We show that ambiguity fades away if the data clearly designates a unique most accurate model (or a set of models with equivalent predictions), a condition that is always satisfied in well-specified learning problems. In a nutshell, ambiguity fades away in all cases in which the empirical evidence eventually dominates the effect of heterogeneity in the prior distributions. On the contrary, ambiguity persists in those sequences in which two or more models with different predictions have comparable likelihood infinitely often. When this happens, the posteriors are “split” between these models with weights that depend on the priors, and the DM perceives ambiguity.====Our key contribution is to formalize sufficient conditions for the posteriors obtained from all priors to concentrate on the same model. Our findings rely on and generalize standard results in statistical learning theory. With a unique prior, a sufficient condition for the Bayesian posterior to concentrate on the true model (consistency) is that the prior ==== attaches a positive mass to the true parameter(s) (Doob, 1949, Freedman, 1963). In a multiple priors setting, this result continues to hold: if all priors give positive mass to the true model, then all posteriors concentrate on it and ambiguity fades away (Marinacci, 2002). On the other hand, in an iid setting and if the true parameter set does not belong to the prior support, the posterior concentrates on the model that is the closest in terms of K-L divergence to the truth if it is unique (Berk, 1966, White, 1982). In a multiple priors setting, this result suggests that if the minimizer of the K-L divergence, ====, is unique and all priors give it a positive weight, then ambiguity fades away because all posteriors concentrate on ====.====Theorem 2 proves this conjecture and generalizes it to the non-iid setting. Theorem 1 provides an empirical condition for the posteriors derived from all priors to concentrate on a unique model which does not depend on a priori knowledge of the truth. Theorem 3 shows that ambiguity fades away when all posteriors concentrate on models with identical predictions and provides an empirically verifiable sufficient condition for the above to occur.==== We prove that a Bayesian agent with multiple priors does not suffer from long-run ambiguity in all those cases in which the data support a unique model (or a set of models with identical predictions). How common are these situations? A precise answer to this question is hard to give because it depends on the true probability measure, which is typically unknown. If ==== counts finitely many iid probabilistic models, then the set of parameters characterizing an iid data generating process such that at least two models in ==== have identical average K-L divergence (a situation that violates all our sufficient conditions and may generate long-run ambiguity) is nongeneric, thus suggesting that ambiguity should be the exception, rather than the norm. However, we are cautious about concluding that ambiguity typically fades away in real world situations because models and parameters are hardly iid and chosen at random. For example, consider the standard problem of predicting stock market returns. Several models have been proposed and, to date, it is not clear which model is the closest to the truth — there is no definite statistical test that favors one unique model over another. Because the empirical evidence does not support a unique model, an investor with a set of priors on available models of stock market returns suffers ambiguity despite the large amount of available financial data.====Our, multiple-prior, learning model describes the attitude of a DM that holds more than one prior distribution over a set of parameters and incorporate new information by independently updating each prior according to Bayes’ rule. On the other hand, the multiple-likelihoods model (e.g., Epstein and Schneider, 2007, Epstein and Seo, 2015) describes a DM who believes that signals have multiple, hence uncertain, interpretations. Such signals can generate ambiguity even where none is present a priori. Learning models that accommodate such a possibility generate posterior sets different from those defined in this paper, and they lead to different results regarding if/when ambiguity fades away.",Learning from ambiguous and misspecified models,https://www.sciencedirect.com/science/article/pii/S0304406819300850,14 August 2019,2019,Research Article,267.0
Slinko Arkadii,"Department of Mathematics, The University of Auckland, Private Bag 92019, Auckland 1142, New Zealand","Received 11 February 2019, Revised 29 July 2019, Accepted 2 August 2019, Available online 14 August 2019, Version of Record 20 August 2019.",https://doi.org/10.1016/j.jmateco.2019.08.001,Cited by (9)," alternatives are isomorphic but we found a rich variety of maximal Arrow’s single-peaked domains. We discover their recursive structure, prove that all of them have cardinality ====, and characterise them by two conditions: connectedness and minimal richness. We also classify Arrow’s single-peaked Condorcet domains for ==== alternatives.","The famous Condorcet Paradox==== ==== shows that if voters’ preferences are unrestricted, the majority voting can lead to intransitive group preference in which case the Condorcet Majority Rule (Marquis de Condorcet, 1785) does not determine a winner. ==== ==== are sets of linear orders that are immune from this paradox which means that, whenever the preferences of all voters are drawn from a Condorcet domain, the majority voting never yields cycles. The first maximal Condorcet domains – one for each number of alternatives – were produced by Black, 1948, Black, 1958, who called them the domains of the single-peaked preferences. Such a domain for ==== alternatives contains ==== linear orders. And although larger maximal Condorcet domains have been discovered (Kim and Roush, 1980, Abello and Johnson, 1984, Fishburn, 1996, Fishburn, 2002), the domain of single-peaked linear orders remains by far the most popular among social choice theorists and political scientists. The reason is that it is often plausible to assume that voters’ preferences are ‘one-dimensional’ and determined by a single parameter, e.g., the location of this voter on the left–right political spectrum.====Arrow (1963), however, used a condition weaker than single-peakedness requiring only that single-peaked condition is satisfied on triples, i.e., he required that Black’s single-peaked condition is satisfied only locally and not globally. He noted that domains that satisfy this weaker condition are still Condorcet domains and Inada (1964) showed that this condition is indeed more general and does not imply Black’s single-peakedness. Monjardet (2009) calls it Arrow–Black’s single-peakedness but we prefer to follow Raynaud (1981) and call it Arrow’s single-peakedness.====Dasgupta and Maskin (2008) claim that in practical elections voters are rarely choosing their orders from the universal domain and that ideology is the primary reason for that. In particular, they note that in 2002 French presidential elections and in 2000 U.S. presidential election voters were selecting their orders from two different Condorcet domains.====In this paper we investigate the size and the structure of maximal Condorcet domains that satisfy Arrow’s single-peakedness. Unlike Black’s single-peakedness, where all maximal domains (on the sets of alternatives of the same cardinality) are isomorphic, we find a rich class of maximal Condorcet domains satisfying Arrow’s single-peakedness. However, surprisingly, all of them – as the maximal Black’s single-peaked domain – contain ==== orders despite the restrictions on Arrow’s single-peaked domains are local and not global as with Black’s single-peakedness. We show that maximal Arrow’s single-peaked domains have a nice recursive structure, namely, a maximal Arrow’s single-peaked domain on ==== alternatives is constructed from two maximal Arrow’s single-peaked domains on sets of ==== alternatives with isomorphic subdomains which are maximal Arrow’s single-peaked domain on ==== alternatives. Our structural results allow to obtain an if and only if characterisation of Arrow’s single-peaked domains by two conditions: connectedness and minimal richness. Finally, we classify all maximal Arrow single-peaked domains on a set of ==== alternatives.====The paper is organised as follows. In Section 2 we discuss the relation of this paper to the closest papers in the literature. Section 3 introduces the main concepts. In Section 4 we formulate and prove our main structural results. In Section 5 we obtain an if and only if characterisation of Arrow’s single-peakedness and show how all the existing characterisations of Black’s single-peakedness follow from it. In Section 6 we classify, up to an isomorphism, maximal Arrow’s single-peaked domains on sets of four and five alternatives, respectively. In Section 7 we provide a polynomial algorithm for checking if a set of never-bottom conditions is consistent and, hence, defines a Condorcet domain. Section 8 concludes.",Condorcet domains satisfying Arrow’s single-peakedness,https://www.sciencedirect.com/science/article/pii/S0304406819300874,14 August 2019,2019,Research Article,268.0
Lahkar Ratul,"Department of Economics, Ashoka University, Rajiv Gandhi Education City, Sonipat, Haryana 131029, India","Received 26 October 2018, Revised 24 June 2019, Accepted 31 July 2019, Available online 14 August 2019, Version of Record 16 August 2019.",https://doi.org/10.1016/j.jmateco.2019.07.013,Cited by (10),"We study evolution of preferences in large population aggregative games. In such games, all agents in society interact with each other. The material payoff or fitness of agents is entirely individualistic. Subjective payoffs, which represent preferences, depend upon a non-individualistic component which is adjusted through a type-dependent preference parameter. Using the indirect evolutionary approach, we show that the individualistic type enjoys fitness dominance under any type distribution in such games. Hence, under a class of evolutionary dynamics that satisfy monotone percentage growth, all non-individualistic types are eliminated. We apply this analysis to two classes of non-individualistic preferences-altruistic and Kantian. Altruistic preferences put a positive weight on the externality imposed on society while Kantian preferences incorporate the Kantian categorical imperative. In two important economic models, the tragedy of the commons and the public goods game, we show that both these classes of preferences are eliminated.","While conventional economic analysis regards individual preferences as entirely self-interested, there is experimental evidence that agents do show concern for the well-being of others (Andreoni et al., 2010). Such findings have generated attempts to provide theoretical foundations to more general types of preferences. One approach in this direction has been the argument that the material payoffs induced by other—regarding (non-individualistic) preferences may be sufficiently high as to make them evolutionarily stable in the face of mutant self-interested preferences. This is the indirect evolutionary approach towards explaining non-standard preferences introduced by Güth and Yaari (1992) and Güth (1995). Papers in this field have studied the evolutionary stability of altruistic preferences (Bester and Güth, 1998), spiteful preferences (Koçkesen et al., 2000a, Koçkesen et al., 2000b), preferences that combine both altruism and spite (Bolle, 2000, Possajennikov, 2000, Sethi and Somanathan, 2001; Heifetz et al., 2007, Alger, 2010) and Kantian preferences (Alger and Weibull, 2013, Alger and Weibull, 2016).==== ====Most models on the evolutionary stability of non-individualistic preferences rely on agents in a large population being matched into small groups within the population, either randomly (Bester and Güth, 1998, Heifetz et al., 2007) or assortatively (Sethi and Somanathan, 2001, Alger, 2010, Alger and Weibull, 2013, Alger and Weibull, 2016).==== ==== Non-individualistic preferences can then serve as credible commitment devices. This may enable agents with such preferences to obtain a higher level of material payoff when matched against each other than the payoff obtained by individualistic agents against each other thereby making them evolutionarily successful.====The assumption that individual interaction happens through matching in small groups may not be applicable in certain important situations. For example, certain tragedy of the commons problems like environmental pollution and depletion of natural resources involve interaction among all members of the society. Similarly, there are problems of public goods provision on a nationwide scale. In this paper, we examine the possibility of survival of non-individualistic characteristics in such “playing the field” models, i.e. models where all members of society have to interact with each other.====We consider the class of large population aggregative games for our analysis. These are games in which payoffs depend upon the individual strategy of an agent and the aggregate strategy level in the society (Corchón, 1994). Aggregative games allow us to construct a playing the field model tractably and also include interesting economic applications like the tragedy of the commons and the public goods game.==== ==== The large population characteristic of our model is a convenient abstraction of a large society and also provides the necessary setting for the rigorous application of evolutionary dynamics to the question of preference evolution. To the best of our knowledge, this is the first large population model of preference evolution in which all agents interact with each other.====We first develop a general framework for the analysis of preference evolution in our setting of aggregative games. Following the standard method in the indirect evolutionary approach, we make a distinction between material payoffs and subjective payoffs. Material payoffs are entirely individualistic and capture the self interest of agents. Subjective payoffs, however, also include a non-individualistic component, which agents adjust for by applying a type-specific preference parameter. We divide society into a finite number of types including an individualistic type. We establish conditions such that the aggregative game generated by the subjective payoffs has a unique Nash equilibrium at every type distribution. We then follow the standard assumption in the preference evolution literature that agents can coordinate upon that unique equilibrium instantaneously.====The material payoff that each type obtains at this Nash equilibrium determines the evolutionary success of that type. For our evolutionary analysis, we consider the class of dynamics that satisfies monotone percentage growth rate (Nachbar, 1990), which includes the replicator dynamic (Taylor and Jonker, 1978). We show that at the Nash equilibrium under every type distribution, the individualistic type’s material payoff dominates the material payoff of every other type. Hence, applying existing results on the elimination of dominated strategies under this particular class of dynamics (Nachbar, 1990, Samuelson and Zhang, 1992), we conclude that every type other than the completely individualistic type becomes extinct in the long run. Hence, the only preference that survives in our setting of aggregative games is the standard individualistic preference.====We apply our general analysis to two questions of interest; the survival of altruistic preferences and the survival of Kantian preferences. Altruistic preferences are of interest because, starting with Bester and Güth (1998), they are arguably the most widely analyzed topic in the indirect evolutionary literature. The analysis of Kantian preferences, on the other hand, is of more recent vintage (Alger and Weibull, 2013, Alger and Weibull, 2016). But such preferences are of interest because they incorporate the fundamental moral principle of Kant’s (1785) categorical imperative which urges individuals to consider the consequences of everybody behaving likewise. Also, from the methodological point of view, these preferences satisfy the important requirement of generating aggregative games.====We analyze these preferences in the context of the tragedy of the commons and the public goods game. These two canonical models from economics, both of which are aggregative games, represent a wide variety of social problems that arise due to the interaction of a large number of agents. The standard Nash equilibrium in these games is inefficient due to the self-interested behavior of agents. In the case of altruism, the non-individualistic (altruistic) component in agents’ subjective payoffs is the weight they attach to the social impact of their actions, which can also be equivalently interpreted as the extent to which they internalize the externalities they create. Such externalities are negative in the tragedy of the commons whereas they are positive in the public goods model. In the case of Kantian preferences, non-individualistic agents put a certain weight on the Kantian categorical imperative. In either case, non-individualistic behavior can alleviate the problem of inefficiency at the standard Nash equilibrium. However, our general analysis implies that any preference that puts any weight on altruism or Kantian morality gets eliminated. Only individualistic preferences survive resulting in the inefficient Nash equilibrium.====Bester and Güth (1998) pioneered the literature on the indirect evolutionary analysis of altruistic preferences. That paper also considers the survival of altruistic preferences under positive and negative externalities. There are, however, significant differences between the two papers. First, Bester and Güth (1998) consider preference evolution in an environment of pairwise random matching while in our paper, all agents interact with each other. Second, since they consider pairwise random matching, they define altruism as the extent to which an agent accounts for the impact of her action on the other agent’s payoff, this impact arising from the externalities present in the payoff functions. In contrast, given our playing the field environment, we need to interpret altruism as the extent to which an agent internalizes the externality imposed upon the entire society. Finally, in terms of conclusion, whether altruism survives or not in Bester and Güth’s (1998) model depends upon the nature of externalities. They find that if externalities are positive, then a partial (but not complete) level of altruism is evolutionarily stable while if externalities are negative, then the only evolutionarily stable preference is the completely selfish preference. In contrast, in our model, only the completely selfish preference survives under both positive and negative externalities. The key ESS condition in Bester and Güth (1998) for altruism to survive is that when matched with an altruist, the fitness of a selfish agent should be less than the payoff of an altruistic agent. This condition is satisfied only when externalities are positive. In our model, the selfish type always enjoys fitness dominance and, hence, evolutionary advantage, irrespective of the nature of externalities. Our analysis, therefore, suggests that while altruism may develop in small group matchings, it is unlikely to do so when everybody in society interacts with each other.====Since Bester and Güth (1998), the indirect evolutionary approach has been extended to consider the evolutionary success of not just altruistic preferences but also preferences bearing more malevolent traits like envy or spite (Bolle, 2000, Possajennikov, 2000, Heifetz et al., 2007, Alger, 2010). All these papers consider pairwise matchings, define altruism or spite in terms of the impact on the other player and establish the stability of some type of altruistic or spiteful preferences under some form of externality. Hence, our paper differs from these papers on the same grounds as from Bester and Güth (1998). Ours is a playing the field model, we define altruism with respect to impact on the entire society, and such altruism is eliminated. We also note that in certain applications, we can extend our approach to incorporate spite. In such situations, as we note towards the end of Section 5, spiteful preferences also get eliminated.====Similar differences arise between our analysis of Kantian preferences and that of Alger and Weibull, 2013, Alger and Weibull, 2016. They model a finite number of agents matched assortatively under incomplete information. Agents incorporate the Kantian categorical imperative according to their degree of morality. In such models, one particular type of preference incorporating moral concerns, which they call ====, emerge as evolutionarily stable. In contrast, in our field environment, Kantian moral preferences are eliminated entirely. Alger and Weibull, 2013, Alger and Weibull, 2016 also allow for an arbitrary trait space while our formal model allows the presence of only one non-individualistic trait (such as altruism or Kantian morality) in a given situation, although the intensity with which agents hold that non-individualistic trait may vary. Nevertheless, as we note towards the end of Section 6, it should be possible to extend our approach to incorporate a mixture of different traits.====Apart from matching in small groups, most results about the survival of non-individualistic preferences require another important assumption. This is that within each group, preferences are common knowledge. This is true, for example, in Bester and Güth (1998). If this assumption is not met and matchings are uniformly random, then the individualistic type enjoys the highest fitness at the Bayesian Nash equilibrium of the subjective payoffs game. This is because the Bayesian Nash equilibrium strategy of the individualistic type is the best response to the distribution of strategies in the population, while the equilibrium strategy of the other types is not. Hence, only individualistic preferences can then be evolutionarily stable in such matching models (Ely and Yilankaya, 2001, Ok and Vega-Redondo, 2001, Dekel et al., 2007). In a playing the field model like ours, the distinction between complete and incomplete information is irrelevant. Nevertheless, the logic behind elimination of non-individualistic types is similar. At the Nash equilibrium of the subjective payoffs game, the individualistic type’s strategy is dominant and, therefore, enjoys the highest fitness.====However, this does not mean that the extinction of non-individualistic preferences in our model follows from existing results on the elimination of such preferences under incomplete information. This is because such results still depend upon matchings in small groups. As Ok and Vega-Redondo (2001) note, if we drop the assumption of matchings in small groups, then non-individualistic preferences may survive even under incomplete information. Examples of such models are Koçkesen et al., 2000a, Koçkesen et al., 2000b who establish the stability of a class of envious or spiteful preferences in a playing the field environment. The key reason why such non-individualistic preferences survive in these models is that these are finite player models. Hence, it may be possible for a single spiteful mutant to deviate from a Nash equilibrium and reduce the payoffs of other players more severely than his own payoff. In contrast, ours is a large population model where each agent is of measure zero. Hence, deviations by single agents cannot have any impact on the payoff of other agents. Of course, mutations by a small measure of agents may still create conditions for non-individualistic agents to survive in a large population model. However, this does not happen in our model due to the fitness dominance enjoyed by the individualistic type.====In terms of methodology, the main contribution of this paper is that it suggests a general framework for analyzing preference evolution in a large population field environment. The use of aggregative games for this purpose is also significant because certain features that are important in the study of preference evolution such as uniqueness of best response and uniqueness of Nash equilibrium arise naturally in such games. The dynamic analysis in this paper, which is based on the dominance of the equilibrium material payoff of a certain type, is similar to that of Heifetz et al. (2007). That paper also applies dominance solvability, albeit in a pairwise random matching environment, to show global convergence of the type distribution under payoff monotone selection dynamics.==== ====
 Sandholm (2001b) also applies such dynamics to study evolution of preferences with idiosyncratic biases under random matching in two-strategy games. The main difference is that our dynamic analysis is for a playing the field model instead of in a random matching model. Such fully dynamic approaches of these papers as well as ours are significant because in most models of preference evolution, the dynamic process is not made explicit. For example, Bester and Güth (1998) and Alger and Weibull, 2013, Alger and Weibull, 2016 use the static notion of ESS to determine the evolutionary stability of a certain type of preference. Other models such as Koçkesen et al., 2000a, Koçkesen et al., 2000b and Ok and Vega-Redondo (2001) characterize equilibrium material payoffs and heuristically argue for the proliferation of certain preferences on grounds of payoff monotonicity. We also note that notions like ESS are local stability concepts while individualistic preferences in our model are globally evolutionarily dominant.==== ====The rest of the paper is as follows. In Section 2, we introduce the model and formalize the distinction between material and subjective payoffs. Section 3 characterizes Nash equilibrium of the subjective payoffs game. Section 4 analyzes preference evolution under evolutionary dynamics that satisfy monotone percentage growth. In Section 5, we apply our general results to show the elimination of altruistic preferences in the tragedy of the commons and the public goods game. Section 6 considers Kantian preferences. Section 7 concludes. Some proofs and technical material are in Appendix.",Elimination of non-individualistic preferences in large population aggregative games,https://www.sciencedirect.com/science/article/pii/S0304406819300862,14 August 2019,2019,Research Article,269.0
"Chattopadhyay Subir,Mitka Malgorzata M.","Department of Economics, University of York, York, YO10 5DD, UK,School of Economics and Finance, University of St Andrews, KR16 9AR, UK","Received 11 February 2019, Revised 23 July 2019, Accepted 27 July 2019, Available online 13 August 2019, Version of Record 6 September 2019.",https://doi.org/10.1016/j.jmateco.2019.07.011,Cited by (2),We study a ==== model of trade with two goods and many countries where each country sets its distortionary tariff non-cooperatively to maximize the payoff of the representative household. We prove the existence of pure strategy ====.,"The ability to impose a tariff is arguably the tool most commonly used by a government to influence foreign trade. This is done to benefit the country as it moves the equilibrium allocation in an appropriate direction away from free trade. Since retaliation is only to be expected, the resulting strategic equilibrium becomes the object of analysis; importantly, the allocation induced is, quite generally, inefficient.==== ==== This sets the stage for a role for institutions that regulate and promote international trade to attempt to mitigate the inefficiency by facilitating the negotiation of multilateral agreements.==== ==== The heterogeneity across countries, particularly in terms of their relative size and tastes, is likely to play a key role in the determination of the rules of multilateral engagement used by these institutions to achieve the desired mitigation. An essential element of any analysis that provides the foundation for such rules would be to clarify the manner in which heterogeneity interacts with the number of countries in consideration, and our aim is to contribute to that analysis.====We study a model with many countries in which the prices that domestic agents face are the world prices distorted by a tariff, and where the revenue from the tariff is distributed by the government to the agents as a lump-sum transfer. Trade in competitive markets results in the determination of world prices for goods in general equilibrium, and each government acts non-cooperatively to set tariff rates to maximize the utility of the agents. The equilibrium concept used is pure strategy Nash equilibrium.====The literature on optimal tariffs in the presence of retaliation has drawn attention to the importance of solving for the Nash equilibrium of a non-cooperative tariff game.==== ==== However, the quote from Costinot et al. (2016), who present a parametric “new trade model” and study the problem faced by a single country, that “future research should strive to characterize the Nash equilibrium in which all countries attempt to manipulate their terms of trade”, confirms the paucity of results on existence and characterization of pure strategy Nash equilibrium.====Our paper contributes to the literature by studying a tractable multi-country framework; it addresses the issue of the existence of a Nash equilibrium and also provides qualitative results, and it does so analytically rather than numerically/computationally.==== ====Our model is sufficiently general in that we do not impose any restrictions on trade patterns and we are able to characterize the equilibrium in terms of conditions on countries’ endowments. This is in contrast to the literature where results can often be traced to simplifying assumptions, like symmetry, on the structure of the model.==== ====Evidently, the introduction of tariffs enriches the model whilst creating additional technical difficulties. Sontheimer (1971) draws on Foster and Sonnenschein (1970) to note that if some good is not normal then, in the presence of a tariff, the demand set could fail to be convex. Hence, existence of a general equilibrium requires that in each country every good is normal. It is also clear that continuity of behaviour and payoffs requires that, given tariff rates, the Walrasian equilibrium is unique. These two requirements immediately restrict the set of economies that one can work with. In addition one must face the main difficulty in proving existence of Nash equilibrium in such a framework: one has to establish that each payoff function is quasiconcave in the player’s own choice.====In view of the technical problems that the richness of the model forces us to confront, there is a trade-off in how general a model one can work with. We would also like to adopt a framework that allows the theory to be taken to the data. The linear expenditure system, which has been fruitfully applied in many areas, and preferences that generate it, provides a compromise that is an attractive specification for a model of tariffs. The fact that these generalized Cobb–Douglas preferences induce a demand function that is the same as the aggregate demand function induced by heterogeneous agents with sufficiently heterogeneously distributed characteristics (see Grandmont, 1992) is a bonus.====We consider an exchange economy with an arbitrary number of countries and with two goods. Preferences are restricted to be in the Cobb–Douglas class but are otherwise arbitrary—no assumptions of symmetry are made and countries are allowed to be different. Endowments are also arbitrary other than being nonnegative.====We study non-cooperative tariff equilibria in the induced strategic form game.====We first show that if for each good at least two countries have a positive endowment of the good, a mild assumption, then there are common upper and lower bounds on tariff rates such that no country’s best response is on the boundary of the strategy sets that are induced. These bounds are consistent in that all potential equilibrium points are interior.====We then show that if at least two countries have a positive endowment of the good on which the tariff is imposed then each country’s payoff function has the property that its second derivative is negative at any point at which the first derivative of the function is zero. When combined with the behaviour of the payoff function at the boundaries of the strategy sets, the local second order property ensures that the payoff function is quasiconcave. That suffices to prove the existence of pure strategy Nash equilibria.====Next we show that the tariff game is a game of strategic complementarity as the best response functions are strictly increasing. We also provide robust examples to show that the game can fail to be supermodular; it follows that the existence proof cannot be simplified by appealing directly to lattice theory, a simplification that would make redundant the difficult step in which we verify quasiconcavity.==== ====We then use the properties developed to provide results on payoff comparisons that culminate in a policy implication. We show that, in our model, a country’s payoff increases when a competitor raises its tariff if and only if the country has set a positive net tariff rate. Therefore, the standard condition in the literature under which payoff comparisons across Nash equilibria become possible must be violated in our model since that condition (which is not implied by supermodularity) requires that the country’s payoff always responds in the same direction whenever a competitor raises its tariff. We then show that a country’s payoff increases as we move away from the free trade point in either direction along the best response. We are also able to show that the following surprising result holds very generally: the participation of at most two countries in negotiated tariff changes suffices to induce a Pareto improving allocation relative to a Nash equilibrium with the direction of tariff rate change easily determined; this is despite the fact that changing any tariff rate affects the payoff of every country. In view of the fact that our model is parametric and can therefore be easily taken to the data, this result on welfare improvements has the potential to play an important role in policy deliberations in the current global environment where large economies are adopting protectionist measures.====We also provide results that relate the position of a country’s best response function to the trade pattern in the absence of tariffs, and that specify lower bounds on the arithmetic and harmonic means of equilibrium tariff rates; this last result implies that there is at most one symmetric equilibrium and it must be free trade. Our final result is that there is no trade if and only if the equilibrium allocation is Pareto optimal. We then comment on the extent to which comparative statics exercises can be carried out, and on the possibility of obtaining a result on uniqueness.====We reiterate that our analysis is free of any restrictions on trade patterns and our minimal assumptions are transparent since they are on the fundamentals of the economy and not on elasticities. We observe that, although a key goal of this literature is to determine the welfare effects of the introduction of non-cooperative tariffs, we do not pursue that goal analytically. This is because we know from Kennan and Riezman (1988) that, even in the two country world with identical symmetric Cobb–Douglas preferences, the pattern of endowments determines one of the three outcomes—there is a cigar shaped region around the line of slope ==== that delineates a region in which, in Nash equilibrium, both lose relative to free trade, and outside the region the bigger country wins.==== ====One imagines that in our model with many countries welfare effects of tariffs will depend intricately on the combination of preference parameters and endowments. This suggests the use of numerical methods which are placed on firmer ground since in our tariff games solutions to the first order conditions characterize all possible interior pure strategy Nash equilibria.====One must ask whether our framework is too restrictive. In the literature one sometimes finds expression of the belief that the issue of the existence of pure strategy Nash equilibria in the general model described has been settled; yet, as we now argue, very little is known about it.==== ====
 Wong (2004) considers the 2 × 2 (two good and two country) pure exchange model and provides an example to show that existence can fail if for some country, and a tariff rate set by the other country, the country’s offer curve fails to enclose a convex set; this confirms that the result in Otani (1980) on the existence of compensated equilibrium in a general model with production is driven by his Assumption 11 (b), one that he refers to as “most uneasy”, which convexifies the problem. Wong (2004) then proves existence with the normal goods assumption and the assumption that the area enclosed by each offer curve is a convex set for each level of the tariff chosen by the opponent, and makes the argument that his proof cannot be extended to the case with more than two countries.==== ====The first order conditions of the 2 × 2 pure exchange economy with Cobb–Douglas preferences have been studied by Otani (1980) as an example, and by Kennan and Riezman (1988) who revisit Johnson’s original question and provide the solution described earlier.==== ====To summarize, the existence results that are known require strong restrictions and are not known to extend to the case with more than two countries.==== ====We make one final comment. A trivial modification, which amounts to no more than relabelling the variables, allows one to view the model as one of multiple tax jurisdictions. Once local tax rates are set and treated as parameters, a standard Walrasian equilibrium is played. Our model provides an “off-the-shelf” well-received parametric framework in which existence of a pure strategy Nash equilibrium is guaranteed and in which qualitative as well as quantitative analysis can be easily carried out.====We present the model in Section 2, and discuss, in order, existence in Section 3, strategic complementarity in Section 4, and properties of the solution in Section 5. Concluding comments are in Section 6, and all proofs are collected in Section 7.",Nash equilibrium in tariffs in a multi-country trade model,https://www.sciencedirect.com/science/article/pii/S0304406819300849,13 August 2019,2019,Research Article,270.0
"Araujo Aloisio,Gama Juan Pablo,Pascoa Mario Rui","IMPA, Rio de Janeiro, Brazil,FGV EPGE, Rio de Janeiro, Brazil,Department of Economics, Federal University of Minas Gerais, Belo Horizonte, Brazil,School of Economics, University of Surrey, Guildford, UK","Received 16 March 2019, Accepted 21 July 2019, Available online 4 August 2019, Version of Record 16 August 2019.",https://doi.org/10.1016/j.jmateco.2019.07.005,Cited by (0),"Efficiency is not commonly related to the crash of bubbles. However in the presence of wary agents, infinite-lived agents that are worried about distant losses, efficient bubbles may occur and, in a stochastic setting, these bubbles can crash. In this paper we characterize the Arrow–Debreu (AD) price and establish the relationship between the agents’ concern about distant losses and the existence of pure charges in the AD price. We show that this pure charge induces efficient bubbles in the positive net-supply assets that complete the markets and that, as we enter some sub-tree, that pure charge may no longer present in the AD price for the sub-economy, implying the crash of the bubble. Finally, we give an example in which there is an efficient bubble with infinitely many crashes.","Rational bubbles have been extensively studied since the late 70’s pioneering work by Blanchard (1979) and Blanchard and Watson (1982). Santos and Woodford (1997) made a theoretical and systematic study of rational bubbles in a general equilibrium model with finite and infinite-lived agents. However, efficient bubbles in positive net-supply assets seemed to be ruled out by the same portfolio constraints that avoided Ponzi schemes. Araujo et al. (2011) showed that this is no longer the case when standard impatience assumptions do not hold. More precisely, if agents are ====, that is, not willing to neglect losses at distant dates, the efficient allocations may be implemented by trading positive net supply assets with speculative prices. The drawback of this surprising result is that it was established in a deterministic setting, where the occurrence of a bubble implies that it will be present at all dates. Araujo et al. (2019) showed how it can be implemented sequentially these efficient allocations in economies with money or other long-lived assets with taxes.====In a stochastic economy, bubbles can burst some time later, as we enter some sub-tree where the reason for the occurrence of the bubble does not prevail. In this paper we address whether efficient bubbles generated by wariness may burst in a stochastic economy. Wariness is a lack of impatience that consists in neglecting distant gains but not distant losses. The lack of impatience can be interpreted in terms of ambiguous beliefs, in the sense of Gilboa (1989) and Schmeidler (1989), making the agents worried about their losses among the different dates and state of nature. And, as secondary objective, we provide an analysis of possible increments in asset price variation due to the crashing of bubbles in some states of nature.====We provide an example where the occurrence of a pure charge in the Arrow–Debreu price may happen and then disappear, depending on whether the infimum of consumption is a cluster point of the consumption sequence or not, within the relevant sub-tree. This induces the asset price bubble or the burst of that bubble, respectively, and the bubble may even occur and then crash in infinitely many different period depending on the path. In all cases, bubbles cannot reappear in the economy.====The article is organized as follows: In Section 2, we define the notation used along the article, and we define the type AD economy that we will work on. In Section 3, we characterize the superdifferential of the type of consumers that we defined. In Section 4, we implement the AD allocation sequentially, and we analyze the existence of efficient bubbles in this framework and how they affect the variation of the asset prices, and we conclude this section with an example with a positive bubble that the probability of crashing from a very distant date onwards is positive. In Section 5, we give some concluding remarks.",Crashing of efficient stochastic bubbles,https://www.sciencedirect.com/science/article/pii/S0304406819300783,4 August 2019,2019,Research Article,271.0
Kukushkin Nikolai S.,"Dorodnicyn Computing Centre, FRC CSC RAS, 40, Vavilova, Moscow 119333, Russia,Moscow Institute of Physics and Technology, 9, Institutskiy per., Dolgoprudny, Moscow region 141701, Russia","Received 17 January 2019, Accepted 29 July 2019, Available online 2 August 2019, Version of Record 7 August 2019.",https://doi.org/10.1016/j.jmateco.2019.07.010,Cited by (4),"Several agents choose positions on the real line (e.g., their levels of conspicuous consumption). Each agent’s utility depends on her choice and her “status,” which, in turn, is determined by the number of agents with greater choices (the fewer, the better). If the rules for the determination of the status are such that the set of the players is partitioned into just two tiers (“top” and “bottom”), then a strong ","This paper investigates a class of strategic games with discontinuous utilities. From a purely technical viewpoint, those games are interesting because no general conditions for equilibrium existence, see, e.g., Reny, 1999, Reny, 2016, McLennan et al. (2011), Prokopovych (2013), or Kukushkin (2018), are applicable to them, even though they display features such as aggregation, monotonicity, and strict quasiconcavity.====From the viewpoint of economic theory, those games model how concerns for relative social status influence people’s decisions. This topic has been present in the literature since, at least, Veblen (1899), and has attracted ever growing attention in recent decades (Frank, 1985a, Akerlof, 1997, Clark and Oswald, 1998, Becker et al., 2005, Arrow and Dasgupta, 2009). Bilancini and Boncinelli (2008) stressed the importance of distinguishing between ordinal and cardinal approaches: in the first case, the status of a player is determined by the ==== with other players’ choices; in the second case, by the ==== between them.====The starting point for this paper is the model of Haagsma and von Mouche (2010), henceforth, an “HvM status game”, which belongs to the ordinal strand of the literature. In contrast to, say, Frank (1985b) or Becker et al. (2005), an HvM status game has a finite number of players; each player’s utility depends on her choice and her “status”, which is just the order rank of her choice among all choices (simply put, the number of players with the same or lesser choices). The utility function strictly increases in status and is strictly quasiconcave in own choice. The discrete nature of status in such games generates unpleasant discontinuities of the utility functions. Nonetheless, the best responses exist and, typically, exhibit the “keeping up with the Joneses” effect in the literal sense: a player may choose above her intrinsically most preferred alternative in order to become equal with somebody else and obtain a higher status thereby. (This should be contrasted, e.g., with the Status Model of Akerlof (1997, p.1008), where the optimal choice of an agent does not depend on what the others are doing.) Examples show that Nash equilibria may be or not be Pareto efficient.====The weakest point of Haagsma and von Mouche (2010) is the absence of any general result on the existence of Nash equilibrium. The only exception is the two-person case, where it is geometrically obvious that the graphs of the best responses must intersect.====As often happens in mathematics, to make an advance, one has to modify the original posing of the problem. Kukushkin and von Mouche (2018) considered “binary status games”, where the number of the players may be arbitrary, but there are only two status levels: a player belongs to the top tier if her choice is the maximal of all, and belongs to the bottom tier otherwise. Every such game possesses a Nash equilibrium; moreover, every best response improvement path, regardless of where it was started and in what order the players act, inevitably reaches a Nash equilibrium after a finite number of improvements.====Here, a step further in the same direction is made. In an “====-consolidated HvM status game”, there may be fewer potential status levels (====) than players, so players with different, but close, order ranks may have the same status. For instance, the games of Kukushkin and von Mouche (2018) are 2-consolidated HvM status games. As another example of such a game, the top tier may consist of the players whose choices are greater than or equal to the median choice, while everybody else is relegated to the bottom tier.====The main findings of this paper are as follows. Every 2-consolidated HvM game possesses a strong Nash equilibrium that weakly Pareto dominates all other Nash equilibria (Theorem 1). Moreover, the main result of Kukushkin and von Mouche (2018), about the convergence of all Cournot paths, remains valid in this broader context even with an extension to simultaneous tâtonnement (Proposition 2).====Generally, a 3-consolidated HvM game need not possess an equilibrium. A Nash equilibrium existence result (Theorem 3) is proven under an additional assumption, which could be called “local single crossing” and which holds, e.g., when each player’s intrinsically most preferred choice does not depend on her status. However, there may be no Pareto efficient Nash equilibrium, to say nothing of a strong equilibrium. With more than three possible status levels, there may be no Nash equilibrium at all even under that additional assumption. From the viewpoint of equilibrium existence results of reasonable generality, ==== in this context.====Section 2 provides formal descriptions of both HvM status games and our “consolidated” HvM status games. Section 3 contains definitions and auxiliary, technical results concerning the Cournot tâtonnement processes. Our main results are in Section 4 (two possible status levels) and Section 5 (three possible status levels). A brief discussion of possible and impossible extensions in Section 6 concludes the paper.",Equilibria in ordinal status games,https://www.sciencedirect.com/science/article/pii/S0304406819300837,2 August 2019,2019,Research Article,272.0
"Chevallier Claire Océane,El Joueidi Sarah","CREA, University of Luxembourg, Luxembourg, Luxembourg,American University of Beirut, Beirut, Lebanon","Received 6 July 2018, Revised 22 July 2019, Accepted 24 July 2019, Available online 1 August 2019, Version of Record 6 August 2019.",https://doi.org/10.1016/j.jmateco.2019.07.009,Cited by (2),This paper develops a dynamic general equilibrium model in infinite horizon with a regulated ,"The Great Recession of 2007–2009 has highlighted the importance of the banking sector in the economy and its role in the propagation of the crisis. Both academics and policy makers acknowledged the failure of banking regulation. The U.S. Federal Reserve chairman, Ben Bernanke, in his 2008 speech on the financial crisis, suggested that inadequate financial regulation had contributed to the severity of the crisis.==== ==== In particular, banking regulation failed to prevent valuation and liquidity problems in the U.S. banking system, which have contributed to the propagation of the financial crisis (Miao and Wang, 2015). Banks’ values were tied to the real estate pricing, which sharply decreased during the bursting of the U.S. housing bubble. Banks experienced large losses, leading to concerns about banks’ solvency, reducing investors’ confidence and banks’ profits further (Miao and Wang, 2015). This has highlighted the importance of understanding the role of banking regulation on financial crises, in particular its role in banks stock price changes that are not due to economic fundamentals, i.e. the emergence and existence of banking bubbles.====Most countries rely on the Basel banking regulation framework to regulate their banking system. The first Basel Accord goal was to prevent international banks from growing without adequate capital.==== ==== Therefore, the committee imposed minimum capital requirements which were calculated based on credit risk weighted assets. Credit risk weights take into account possible losses on the asset side of a bank’s balance sheet. The idea is that banks holding riskier assets had to hold more capital than other banks in order to ensure solvency. This approach has been criticised by researchers and regulatory agencies because it only considers credit risk and does not encompass market risk.==== ==== Market risk refers to the risk of losses from changes in market prices, which increases banks’ default risk. The Basel committee has recognised this problem and released the Basel II Capital Accord.==== ==== This new accord considers market values into the banking regulation framework in order to take into account market risk of the trading book. It allows banks to use an internal model based on ==== to quantify their minimum capital requirements. The idea of capital requirements based on Value-at-Risk is to impose a solvency condition for banks which requires that the maximum amount of debt that banks can hold does not exceed the market value of banks’ assets in the worst case scenario. Value-at-Risk capital requirements remained in Basel III.==== ==== Fig. 1 shows sharp movements in the bank real stock price index for the period from 1973 to 2016. This index includes 168 banks listed in Europe. The theory on asset prices argues that stock price changes are hardly explained by fundamentals alone. Several authors suggest that changes in stock prices can be due to the existence of bubbles. For instance, Kocherlakota (1992) and Miao and Wang (2015) show that bubbles may emerge when agents’ portfolios are constrained. As the stock price index boom in 2004 coincides with the release of Basel II capital requirements, and although there might be other explanations, we investigate if Basel capital requirements based on Value-at-Risk (Basel II and III) might help to explain the increase in prices observed from 2004, which can be partially interpreted as a bubble by the theory on asset prices.====This paper combines the economic theory of banking regulation with the one of asset price bubbles. It aims at determining if Basel regulatory recommendations may generate and affect banking bubbles. It provides policy implications of banking regulation by evaluating their effect on banking bubbles, and as a consequence on the economy. A bubble on a bank stock price is defined as a temporary deviation of the bank stock price from its fundamental value. The bank fundamental value is the value of the bank value without a bubble. Positive bubbles are defined as banks stock prices being above their fundamental values. In contrast, negative bubbles are defined as banks stock prices being undervalued. In addition, as in Blanchard and Watson (1982) and Weil (1987), bubbles have a constant probability of bursting.====We develop a dynamic general equilibrium model with three types of infinitely lived agents, banks, households, and firms, as well as a regulatory authority. Banks raise funds by accumulating net worth and demanding deposits (supplied by households) to provide loans to firms. Firms produce the consumption goods using capital. The regulatory authority imposes two banking regulations. The first requires that banks keep a fraction of deposits as reserves. These reserves cannot be used to invest in loans (risky assets). The second measure is the capital requirement based on Value-at-Risk. It ties banks’ deposits to banks’ market value, as recommended in Basel II and III accords.====We show that bubbles emerge if agents believe that they exist. Thus, expectations of agents are self-fulfilling. Results suggest that when banks face capital requirements based on Value-at-Risk, two different equilibria emerge and can coexist: the bubbleless and the bubbly equilibria. In contrast, under a regulatory framework where capital requirements are based on credit risk only, as in Basel I, banking bubbles are explosive and, as a consequence cannot exist. The bubbly equilibrium is characterised by positive or negative bubbles depending on the tightness of capital requirements. We find a maximum value of the capital requirement based on Value-at-Risk below which bubbles are positive. Below this value and until the bubble bursts, the bubbly equilibrium provides larger welfare than the bubbleless equilibrium. The intuition is that, when agents consider that a bubble exists, lower capital requirements lead to optimistic beliefs about bank valuation. Banks demand more deposits and make more loans. This effect reduces the lending rate and provides higher welfare. Thus, profits of banks rise which increases the value of banks. As a consequence, initial beliefs about the value of banks are realised. In contrast, above this maximum capital requirement, bubbles are negative leading to a credit crunch and thus, reduce welfare. Therefore, our model shows that a change in regulation might lead to a crisis, by shifting the economy from higher to lower welfare. This can explain the existence of crises without external shocks. We also show that the equilibrium with positive (negative) bubbles exists if the probability that bubbles collapse is small (large). This is consistent with Weil (1987) and Miao and Wang (2015).====This paper is related to two strands of literature. First, it is related to the literature on banking regulation. As in Dangl and Lehar (2004) and Tomura et al. (2014), we study the impact of banking regulation on the economy. Dangl and Lehar (2004) compare the effect of capital regulation based on Basel I and the Value-at-Risk internal model approach. They find that the latter reduces risk in the economy. Tomura et al. (2014) introduce asset illiquidity in a dynamic stochastic general equilibrium model and show that capital requirements based on Value-at-Risk can lead banks to adopt a macro-prudential behaviour. We contribute to this literature by showing how capital requirements may generate and affect banking bubbles.====Second, this study is related to the literature on the existence and the effect of rational bubbles in infinite horizon. For instance, the literature on the existence of bubbles in general equilibrium models with infinitely lived agents is scarce and marked with few important contributions (Miao, 2014). Tirole (1982) shows that bubbles under rational expectations with infinitely lived agents cannot exist. In addition, Blanchard and Watson (1982) argue that “the only reason to hold an asset whose price is above its fundamental value is to resell it at some time and to realise the expected capital gain. But if all agents intend to sell in finite time, nobody will be holding the asset thereafter, and this cannot be an equilibrium”. Such behaviour implies that agents over save so that they do not consume everything they could. This cannot be an equilibrium since agents would deviate to increase their consumption levels and, thus, the so called ==== (TVC) would not be satisfied. In contrast, Kocherlakota (1992) demonstrates that bubbles may exist in an infinite horizon general equilibrium model with borrowing or wealth constraints. These constraints limit the agent arbitrage opportunities by introducing some portfolio constraints. Santos and Woodford (1997) show that bubbles do not exist in standard infinite horizon settings. Nevertheless, they prove that they might emerge under some specific conditions. The setting presented in this paper is a close version of the example 4.5 in Santos and Woodford (1997) which illustrates the existence of bubbles in incomplete markets, following the terminology of Giménez (2003) and Santos (2006). We contribute to this literature by showing that banking bubbles may emerge with banking regulation based on Value-at-Risk in an infinite horizon general equilibrium framework.====This paper is mostly related to Miao and Wang (2015). They insert an endogenous borrowing constraint and show that bubbles can emerge in an infinitely lived general equilibrium framework without uncertainty. Our model contrasts with Miao and Wang (2015) regarding three major characteristics. First, we introduce the aspect of banking regulation to analyse how it may generate and affect banking bubbles. We provide policy implications by evaluating the role of regulatory parameters on banking bubbles, and as a consequence on the economy. Second, Miao and Wang (2015) consider an agency problem to justify a minimum dividend policy that links dividends to net worth. A dividend policy constraint is not assumed in our model. Third, negative bubbles as well as positive bubbles can arise in this model, while Miao and Wang (2015) study positive bubbles. There are few authors that study negative bubbles in the literature. For instance, Weil (1990) shows that negative bubbles defined as persistent undervaluations of asset prices may arise. In particular, as in our model, pessimistic beliefs might lead to persistent undervaluations. Allen and Gale (2004) define negative bubbles as “asset prices that fall below their fundamentals”. They show that they can occur when banking crises lead banks to simultaneously liquidate their assets. They provide the 1990 stock prices collapse in Japan as an example of such phenomenon. Acharya and Naqvi (2019) use the same definition and show that negative bubbles on safe assets can arise when the monetary policy is loose and intermediaries’ liquidity is abundant.====The present paper is organised as follows. Section 2 presents the model. Sections 3 Bubbleless general equilibrium, 4 Bubbly general equilibrium analyse, respectively, the bubbleless and the bubbly general equilibrium. Section 5 analyses the impact of banking capital requirements on the emergence of bubbles and on the economy. Section 6 presents a numerical example and the local dynamics around the equilibria. Finally, Section 7 concludes.",Capital regulation and banking bubbles,https://www.sciencedirect.com/science/article/pii/S0304406819300825,1 August 2019,2019,Research Article,273.0
Harless Patrick,"University of Arizona, Tucson, United States","Received 21 February 2019, Revised 18 July 2019, Accepted 23 July 2019, Available online 31 July 2019, Version of Record 5 August 2019.",https://doi.org/10.1016/j.jmateco.2019.07.006,Cited by (0),"We propose an algorithm to construct ==== allocations, we apply the technique to obtain a surprising and “decentralized” representation of the serial rule. Further applications construct new ==== rules.","Familiar assignment problems such as tasks to employees, dorm rooms to students, and school seats to children restrict or prohibit monetary compensation. Centralized procedures nevertheless attempt to improve welfare by accounting for individuals’ preferences. Also motivated to treat individuals fairly, procedures may contrive lotteries over assignments to overcome the asymmetries inherent in deterministic assignments. Individuals report only preferences over objects, however, so lottery comparisons are ambiguous. Nevertheless, stochastic dominance comparisons permit conservative inference of these preferences. Following this approach, we study probabilistic rules.====Our primary interest is efficient assignment: It should not be possible to make some agents better off without making others worse off. Several symmetric rules, including the leading random priority and serial rules,==== ==== ensure efficient final assignments. Extended to lotteries, however, the principle demands more: An allocation is ==== if there is no alternative allocation such that each agent’s new assignment stochastically dominates her original assignment from the perspective of her own preferences. While the serial rule meets this strengthening, the random priority rule does not, leaving just one symmetric and ==== rule that has received serious attention.==== ==== This narrow focus leaves little scope for policy makers to tailor solutions to specific contexts. Our techniques expand the range of desirable alternatives.====We propose an algorithm to identify and select ==== allocations and define ==== rules. The algorithm specifies a discrete, sequential process that proceeds through a fixed number of rounds. In each round, each agent points at her most preferred object among those that remain. Some of these objects are selected for distribution, with shares of these objects assigned to those agents pointing at the object. Unless all pointing agents can be satiated, the shares of an object are fully distributed and the object is removed.==== ==== A fully specified algorithm, called an ordered-claims-algorithm, includes a selection rule for each round and a distribution rule for each object in each round.==== ==== Applying a particular ordered-claims-algorithm to each economy defines an allocation rule that is guaranteed to be ====. For the purpose of constructing ==== rules, our ordered-claims-algorithms provide two natural levers for variation: Selection of objects and distribution of shares. Among the myriad combinations of selection and distribution rules compatible with the general algorithm, a natural subfamily applies a single pair of selection and distribution rules. This additional structure still admits a rich diversity of allocation rules with desirable properties.====Our approach contrasts with “consumption processes” that identified the serial rule.==== ==== A consumption process proceeds in continuous time with all agents simultaneously. At each moment, each agent consumes probability shares from her most preferred among the remaining objects until the available shares are exhausted, then switches to her next most preferred object and resumes consuming shares. The natural choice of uniform consumption rates defines the serial rule.====Consumption processes can define rules, though moving away from uniform consumption rates introduces jarring asymmetries. Even when appeal to the properties of rules overcomes their descriptive distaste, the consequences of modifying consumption rates remain opaque and difficult to trace. By contrast, many different choices of selection and distribution rules avoid knee-jerk objections to asymmetries. Moreover, the implications of these choices are often transparent, with a tight relationship from properties of selection and distribution rules to properties of allocation rules. An additional feature of ordered-claims-algorithms is that at least some assignments are finalized in each round, which enables partial decentralization by objects. These advantages strongly recommend ordered-claims-algorithms as tools for defining and identifying rules with desirable properties. We illustrate the process with several natural selection and distribution rules, each leading to a new and intuitively appealing probabilistic assignment rule.====Concomitant with our search for new appealing rules, we ask whether an ordered-claims-algorithm represents the serial rule. A surprisingly simple ordered-claims-algorithm succeeds==== ====: The ordered-claims-algorithm that selects in each round the most over-demanded objects and distributes shares of each object by equalizing unfilled shares among agents (Theorem 2).==== ==== The convenient ordered-claims-algorithm representation, featuring partial decentralization, deepens our understanding of this central rule. Beyond illustrations, we prove an important technical result that further recommends ordered-claims-algorithms: An allocation is ==== in a given economy if and only if it is the outcome of an ordered-claims-algorithm (Theorem 1).====Our study adds to the broad literature on probabilistic assignment pioneered by Hylland and Zeckhauser (1979) and reinvigorated by Bogomolnaia and Moulin (2001). Theorem 1 relates to several results on ====, most notably providing a parallel to the known coincidence of ==== allocations and outcomes of consumption processes (Bogomolnaia and Moulin, 2001). Additional results provide analytic definitions of ==== in terms of a partial ordering over objects (Bogomolnaia and Moulin, 2001), dominated support sets (Abdulkadiroğlu and Sönmez, 2003),==== ==== and welfare maximization.==== ==== As with ==== allocations, the serial rule admits diverse representations including as the solution to a flow-sharing problem (Katta and Sethuraman, 2006) and an adaptation of top-trading cycles from equal division (Kesten, 2009) as well as the outcome of consumption processes (Bogomolnaia and Moulin, 2001, Bogomolnaia and Heo, 2012) and now an ordered-claims-algorithm (Theorem 2).====In addition to building on the probabilistic assignment literature, our paper relates to school choice. Recent work moves beyond ====, introducing notions of “ex ante” efficiency (Featherstone and Niederle, 2008), “rank” efficiency (Featherstone, 2013), and stochastic stability (Kesten and Ünver, 2015). Tightening the connections to school choice, algorithms similar to those presented here lead to a probabilistic version of the immediate acceptance rule (Harless, 2018). Through our algorithms, we also link probabilistic assignment to the problem of adjudicating conflicting claims (O’Neill, 1982).==== ==== We draw on this literature for attractive distribution rules, hopefully motivating further exchange between these strands of literature.====Turning to results, Section 2 introduces the model and Section 3 presents our general algorithm and efficiency theorem. Section 4 provides our new representation of the serial rule and Section 5 illustrates structured versions of the algorithm. Section 6 concludes while the Appendix collects all proofs.",Efficient rules for probabilistic assignment,https://www.sciencedirect.com/science/article/pii/S0304406819300795,31 July 2019,2019,Research Article,274.0
"Yang Zhe,Yuan George Xianzhi","School of Economics, Shanghai University of Finance and Economics, Shanghai 200433, China,Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai 200433, China,Guiyang Institute for Bigdata and Finance, Guizhou University of Finance and Economics, Guiyang 550025, China,School of Financial Technology, Shanghai Lixin University of Accounting and Finance, Shanghai 201209, China,Business School, Sun Yat-Sen University, Guangzhou 510275, China,Center for Financial Engineering, Soochow University, Suzhou 215008, China,BBD Technology Co., Ltd. (BBD), 21F Palm Springs Building, No. 199, Tianfu Avenue, Chengdu 610093, China","Received 30 October 2018, Revised 20 July 2019, Accepted 23 July 2019, Available online 29 July 2019, Version of Record 1 August 2019.",https://doi.org/10.1016/j.jmateco.2019.07.007,Cited by (17),"Inspired by Zhao (1992), we first define the hybrid solution of games with nonordered preferences and prove its ==== in ==== ","Nash equilibrium and core are two important solution concepts in game theory. The concept of Nash equilibrium represents the noncooperative behavior of players, while the core captures cooperative behavior among them. Aumann (1961) first introduced the notion of the ====core based on the ====blocking concept, and the existence of the ====core was then proved by Scarf (1971) in a normal-form game with continuous and quasiconcave payoff functions. Later, Kajii (1992) provided a generalization of Scarf’s theorem to games without ordered preferences. This result is also an extension of Border (1984). Unlike Border (1984) and Kajii (1992), another proof method was considered in Florenzano (1989) by defining the group preference of each coalition. Applying Gale and Mas-Colell fixed point theorem, Florenzano (1989) proved the nonemptiness of the core of a coalitional production economy without ordered preferences. By adopting the proof technique analogous to Florenzano (1989), Lefebvre (2001) stated a nonemptiness result for the private core of an economy with differential information, and Martins-da-Rocha and Yannelis (2011) generalized the work of Kajii (1992) in Hausdorff topological vector spaces by combining the methods of Scarf (1971) and Florenzano (1989). Recently, the work of Scarf (1971) was extended to games with incomplete information (Askoura et al., 2013, Noguchi, 2014, Noguchi, 2018, Askoura, 2015), games with discontinuous payoffs (Uyanik, 2015) and games with infinitely many players (Askoura, 2011, Askoura, 2017, Yang, 2017, Yang, 2018).====To unify two solution concepts (i.e., Nash equilibrium and core), Ichiishi (1981) introduced the notion of social coalitional equilibria in a model of societies, and proved a social coalitional equilibrium existence lemma. Existence theorems of Nash equilibria and ====cores can be obtained as two special cases of social coalitional equilibria. Later, Zhao (1992) introduced a partition of the set of players and assumed that all players will cooperate within each coalition of the partition and compete between different coalitions of the partition. Following the above idea, a hybrid solution was defined by Zhao (1992) in a general ====person cooperative game. By Kakutani fixed point theorem, Zhao (1992) proved the existence of hybrid solutions for a general ====person cooperative game. As a consequence, Zhao (1992) obtained the existence of hybrid solutions for normal-form games. Later, Zhao (1996) introduced a hybrid equilibrium concept in an exchange economy with externalities, and proved its existence by the proof method analogous to that of Zhao (1992).====Inspired by Zhao (1992) and Kajii (1992), we attempt to generalize the work of Zhao (1992) to games without ordered preferences. Following the idea of Zhao (1992), we introduce the notion of hybrid solutions for games with a partition of the set of players and nonordered preferences, and prove its existence theorem in Hausdorff topological vector spaces. Observe that Yannelis and Prabhakar (1983) introduced majorized correspondences to generalize the equilibrium existence theorem of Shafer and Sonnenschein (1975). Later, Deguire et al. (1999) and Yuan (1999) analyzed the existence of equilibria for qualitative and generalized games by considering L-majorized correspondences, and Tan and Yuan (1993) proved the existence of equilibria for generalized games with ====-majorized preference correspondences. Recently, Prokopovych, 2013, Prokopovych, 2016 used ====-majorized and L-majorized correspondences to analyze games with discontinuous payoffs. Inspired by the idea of L-majorization, we shall introduce the open graph L-majorized game and prove the existence of hybrid solutions for open graph L-majorized games.====Furthermore, we generalize above results to games with infinitely many players. Weber (1981) first showed that the core can be empty for a non-side-payment game with infinitely many players. Thus, Weber (1981) introduced a strong blocking concept and defined the weak core approximating the core, which is nonempty under some regular conditions. Following the idea of Weber (1981), Askoura (2011) defined the weak core for a normal-form game with a continuum of players, and proved the existence theorem. By introducing the condition of equi-usc for a family of functions, this result was improved by Askoura (2017). On the other hand, following Weber (1981) and Askoura (2011), Yang (2017) also defined the weak ====core for a normal-form game with a compact Hausdorff topological space of players. Yang (2017) proved the existence of weak ====cores and gave a characterization of the weak ====core, that is, the weak ====core coincides with the closed-coalition ====core. Yang (2018) then extended the result to games with nonordered preferences, which is also an infinite-player extension of Kajii (1992). Inspired by Weber (1981), Askoura, 2011, Askoura, 2017 and Yang, 2017, Yang, 2018, we shall define the weak hybrid solution of games with nonordered preferences and infinitely many players. By developing the proof techniques in Yang, 2017, Yang, 2018, we prove the existence of weak hybrid solutions for games with infinitely many players.====In brief, there are four contributions in this paper. First, we generalize the work of Zhao (1992) to games with nonordered preferences. Second, we obtain an infinite dimensional version of the first result. Third, inspired by the idea of domain L-majorization in Yannelis and Prabhakar (1983), Yuan (1999), Deguire et al. (1999), we introduce the open graph L-majorized game and prove the existence of hybrid solutions for open graph L-majorized games. Fourth, we introduce the notion of the weak hybrid solution for games with nonordered preferences and infinitely many players, and prove the existence theorem.====The rest of this paper is organized as follows. In Section 2, we recall the model and results from Zhao (1992). Section 3 provides the main results, and Section 4 is the conclusion.",Some generalizations of Zhao’s theorem: Hybrid solutions and weak hybrid solutions for games with nonordered preferences,https://www.sciencedirect.com/science/article/pii/S0304406819300801,29 July 2019,2019,Research Article,275.0
"Loertscher Simon,Mezzetti Claudio","Department of Economics, University of Melbourne, Level 4, FBE Building 433, 11 Barry Street, Parkville, Victoria 3010, Australia,School of Economics, University of Queensland, Level 6, Colin Clark Building 39, Brisbane St. Lucia, Queensland, 4072, Australia","Received 19 October 2018, Revised 16 July 2019, Accepted 17 July 2019, Available online 26 July 2019, Version of Record 2 August 2019.",https://doi.org/10.1016/j.jmateco.2019.07.004,Cited by (0),We prove that the deficit on each trade in a Vickrey double auction for a homogeneous good with multi-unit traders with multi-dimensional types is at least as large as the Walrasian price gap. We also show that as the number of traders grows large the aggregate deficit is bounded below by the ratio of the Walrasian price and the ==== of excess supply at the Walrasian price.,"For the classic, private-value model of a market for a homogeneous good with buyers and sellers who are privately informed about their demand and supply schedules, Vickrey (1961) showed that a two-sided version of the multi-unit auction that has become to bear his name, which we here refer to as a ====, always runs a deficit in the aggregate when the quantity traded is positive. Subsequent work has shown that the deficit result continues to hold for any ex post efficient, dominant strategy mechanism that satisfies the agents’ individual rationality constraints, provided the agents’ type spaces are smoothly connected, and has extended the result to (expected) deficit under any Bayesian incentive compatible and interim individually rational mechanism when agents’ types are independently drawn from continuous distributions with identical, compact supports.==== ==== Discussing his proposed mechanism, or scheme, (Vickrey, 1961, p. 13, emphasis added) further noted: ====In this paper we strengthen Vickrey’s conclusion by proving two additional results. First, we show that the Vickrey (or VCG)====  double auction incurs a deficit on every unit that is traded and that this deficit per trade is at least as large as the Walrasian price gap. This result is important because it improves economists’ understanding of the working of the Vickrey double auction (and its relation to Walrasian prices).====  It also provides a proof that the Vickrey double auction incurs a deficit in the aggregate. Such proofs have been elusive. Vickrey’s argument rested on a graphical illustration and much of the subsequent literature has established (im)possibility results relating the sum of the agents’ marginal products, ==== to social welfare ====, where ==== is social welfare without ====, concluding that a deficit is inevitable if for all type realizations ==== is the case; see, for example, McAfee (1991), Makowski and Mezzetti (1994), Williams (1999), Yenmez (2015), Loertscher et al. (2015), or Segal and Whinston (2016). Although the generality of this approach has its obvious merits, it also makes it hard to see under which conditions on the primitives of the model (such as homogeneous goods with multi-unit demands and supplies) the conditions are satisfied. Recently, Delacrétaz et al. (2019) showed that homogeneous goods models are “assignment-representable”, which implies ==== and thereby proves impossibility as a function of the primitives of the model. However, compared to our first result (stated as Theorem 1), the chain of reasoning this involves is certainly longer, and there is no connection to the Walrasian price gap.====To establish our second result, we assume that as the number of traders goes to infinity the probability measure generating values and costs guarantees the existence of a well behaved aggregate excess supply, with an interior Walrasian price and quantity.==== ==== Our second result, which generalizes Corollary 3 in Tatur (2005), is that as the number of traders grows large the aggregate deficit is bounded below by the ratio of the Walrasian price and the elasticity of the excess supply at the Walrasian price. Thus, while the deficit on each trade goes to zero, the aggregate deficit remains bounded away from zero.====Section 2 introduces the setup, which is essentially the same as the one we study in Loertscher and Mezzetti (2019), where we propose a dominant strategy, ex post individually rational, double clock auction that is deficit free and, under additional assumptions, asymptotically efficient. Section 3 states and proves our first result that each trade generates a deficit. Section 4 derives the lower bound on the aggregate deficit with a large number of traders. Section 5 concludes the paper by connecting our deficit result for private goods to the deficit generated in the problem of providing a public good.",The deficit on each trade in a Vickrey double auction is at least as large as the Walrasian price gap,https://www.sciencedirect.com/science/article/pii/S0304406819300771,26 July 2019,2019,Research Article,276.0
"Bhowmik Anuj,Centrone Francesca,Martellotti Anna","Indira Gandhi Institute of Development Research, Gen. A.K. Vaidya Marg, Santosh Nagar, Goregaon (East), Mumbai 400 065, India,Dipartimento di Studi per l’Economia e l’Impresa, Università del Piemonte Orientale, Via Perrone 18, 28100 Novara, Italy,Dipartimento di Matematica e Informatica, Università di Perugia, Via Vanvitelli 1, 06123 Perugia, Italy","Received 11 April 2018, Revised 12 July 2019, Accepted 15 July 2019, Available online 24 July 2019, Version of Record 30 July 2019.",https://doi.org/10.1016/j.jmateco.2019.07.003,Cited by (1)," has possibly empty interior. The result is based on a new cone condition, firstly developed in Centrone and Martellotti (2015), called coalitional extreme desirability. We also formulate a notion of incentive compatibility suitable for coalitional models and study it in relation to equilibria.","Since the seminal paper of Radner (1968) a huge literature has grown in the area of Equilibrium Theory under Asymmetric Information, which allows for the possibility of having differently informed agents. From the mathematical point of view, the classical Arrow–Debreu exchange economy representation is thus enriched by taking into account the informational aspects; namely, if ==== is a set of states of the world, each agent is endowed with a probability measure on ==== representing the agent’s prior beliefs, an ==== utility function which depends on the possible states of the world, an initial endowment which specifies the agent’s resources in each state, and a partition of ==== which represents the agent’s initial information. The notion of a Walras equilibrium, called ====, is adapted to include the aforesaid informational aspects. The second notion of our paper, the ====, allows for the possibility of cooperation among agents and is usually associated with Edgeworth. It is well recognized that the asymmetric context gives rise to different possibilities of sharing information among members of coalitions and thus, accordingly, different notions of core have been developed (Wilson, 1978, Yannelis, 1991).====In individual models, both the cases of a finite and an infinite dimensional commodity space have been treated, with various degrees of generality; most of these models assume anyway a countably additive measure space of agents, and a finite-dimensional commodity space or a commodity space whose positive cone has a nonempty interior, in order to apply classical separation theorems to support optimal allocations with nonnegative prices, refer to Angeloni and Martins-da Rocha, 2009, Einy et al., 2001, Graziano and Meo, 2005. Only recently, Bhowmik (2013) has adapted Rustichini and Yannelis’s (1991) additivity condition and extremely desirable commodity assumption==== ==== to the asymmetric information framework, in a way, to obtain a countably additive individualistic core-Walras equivalence theorem with an infinite dimensional commodity space, without assumptions on the positive cone.====Anyway, in the literature, Vind’s (1964) model is well established, where the author proposed to replace the individualistic representation of a complete information economy with a coalitional one: the rationale of this choice is the fact that only the bargaining power of coalitions matters. Hence, coalitions are used as primitives, preferences are defined directly on coalitions and allocations are countably additive measures. Later, Armstrong and Richter (1984) abandoned the countable additivity setting to assume finite additivity for the sake of realism, claiming that a countable union of coalitions need not be a coalition. Under this hypothesis and considering infinitely many commodities, we can also recall the work of Cheng (1991). Basile et al. (2009) then introduced asymmetric information into this framework, working with a Boolean algebra and a Euclidean space of commodities. As pointed out by the authors themselves another advantage of the f.a. approach is that it allows us to include the case of a countable set of asymmetrically informed agents, as in limit economies. We refer the reader to Basile et al. (2009) for further motivations.====The use of coalitional models in the presence of asymmetric information to prove core-Walras equivalence theorems, finds a justification in the criticism of Forges et al. (2002): they in fact claim that, being the notion of the core based on agents making agreements to trade among themselves and not through an anonymous market, this involves communication among agents, and “it is then unreasonable to impose the restriction that an agent cannot entertain a contract which varies with information he does not possess”. Indeed, the main point which allows to overcome this criticism lies in the definition of the private core introduced in Basile et al. (2009), as in coalitional models private feasibility of allocations depends just on the information of the coalitions themselves (requiring measurability just with respect to information of the coalitions) regardless of the one available to single individuals.====Up to now, to our knowledge, there are no core-Walras equivalence results covering both the cases of a finitely additive coalitional model with an infinite dimensional commodity space. The aim of this work is to try to fill this gap, by introducing in the asymmetric context a Banach lattice as the commodity space, the notion of ====, which is the extension of that in Centrone and Martellotti (2016) given for complete information. We point out that the use of properness conditions in asymmetric information models already appeared in the literature in the context of individualistic models (see for example Aliprantis et al., 2000, Angeloni and Martins-da Rocha, 2009).====In this paper we obtain a coalitional asymmetric core-Walras equivalence result in a framework whose commodity space is ====, the positive cone of a Banach lattice ==== having the Radon–Nikodym property (see Diestel Jr. and Uhl (1977)) and feasibility is defined as free disposal; note that this allows for a great variety of infinite dimensional commodity spaces interesting for economics and finance, for example, all the ==== spaces for ====. The idea underlying our properness condition is linked to the familiar idea to see an economy with asymmetric information where uncertainty is captured by ==== states of nature and having ==== as a commodity space, as a complete information economy having ==== as a commodity space. In individualistic models, when feasibility is defined with free disposal it is however well known that equilibria may not be incentive compatible and hence contracts may not be enforceable (see Glycopantis et al. (2003)). So this is the main motivation to try to give results assuming exact feasibility (see Angeloni and Martins-da Rocha (2009)). The same problem can thus be faced in the coalitional setting. Under suitable hypothesis, we are not only able to prove a core-Walras equivalence without free disposal, but we also propose a suitable notion of ==== and show that private core allocations are incentive compatible. We also point out that the introduction of asymmetric information and of the arising informational constraint made it necessary to adopt new techniques with respect to those in Centrone and Martellotti (2016).====The rest of the paper is organized as follows: Section 2 deals with the description of our model, some assumptions and the necessary concepts. In Section 3, we introduce the notion of ==== in the asymmetric information framework and prove some technical lemmas that play central roles in the proofs of our main results. We also compare our properness notion with the one of Aliprantis et al. (see Podczek and Yannelis (2008)). In Section 4, we present our coalitional core-Walras equivalence theorem under the free disposal feasibility condition. In Section 5 we face the question of exact feasibility and prove a core-Walras equivalence theorem under this assumption. Moreover, we define a suitable notion of coalitional incentive compatibility and prove that core allocations are incentive compatible. Section 6 is devoted to some asymmetric individualistic results, deriving from our coalitional ones in the spirit of comprehensiveness of Armstrong and Richter (1984). Lastly, we summarize and compare our results in Section 7.",Coalitional extreme desirability in finitely additive economies with asymmetric information,https://www.sciencedirect.com/science/article/pii/S030440681930076X,24 July 2019,2019,Research Article,277.0
"Bossert Walter,Peters Hans","Centre Interuniversitaire de Recherche en, Economie Quantitative (CIREQ), University of Montreal, P.O. Box 6128, Station Downtown, Montreal QC H3C 3J7, Canada,Department of Quantitative Economics, Maastricht University, P.O. Box 616, 6200 MD Maastricht, The Netherlands","Received 17 January 2019, Revised 27 June 2019, Accepted 9 July 2019, Available online 19 July 2019, Version of Record 23 July 2019.",https://doi.org/10.1016/j.jmateco.2019.07.002,Cited by (0),"One unit of a good has to be divided among a group of agents who each are entitled to a minimal share, and these shares sum up to less than one. The associated set of choice problems consists of the unit simplex and all its full-dimensional subsimplices with the same orientation. We characterize all choice rules that are independent of irrelevant alternatives, continuous, and monotonic; the last condition means that if an agent receives its minimal share and that share increases, then no other agent benefits. In line with Kıbrıs (2012) we show that these rules are rationalizable and representable by a real-valued function. On the issue of rationalizability, we also consider weakenings of our conditions. In particular, we show that in general, excluding cycles of any fixed length does not imply the strong axiom of revealed preference, that is, the exclusion of cycles of any length. For continuous three-agent choice rules, however, excluding cycles of length three implies the strong axiom of revealed preference.","In this paper we consider the following division problem: one unit of a perfectly divisible good is to be divided among ==== agents who each are entitled to a minimal share of this good, where these shares sum up to less than one; in other words, there is a surplus. Thus, the set of division problems consists of the unit simplex and all its full-dimensional subsimplices with the same orientation. Our approach to this division problem is choice-theoretic: we interpret a chosen division to be superior to those that are feasible but not chosen, and therefore assume that it should again be chosen if it is still available when the minimal entitlements increase. Expressed more formally, we consider choice rules that are independent of irrelevant alternatives in the sense of (assumption no. 7 in) Nash (1950). As is the case in all choice environments, this independence property is a necessary (but, in general, not sufficient) condition for rationalizability. We think of it as a fundamental requirement because it represents a coherence condition that is compelling in many situations. Note that, again in agreement with other models that involve the division of objects among a group of agents, the decision to impose this independence axiom reflects that we do not focus on strategic aspects of the problem under consideration. This observation illustrates a parallel with Nash’s (1950) axiomatic approach to the bargaining problem; other than that, however, the two types of problem have different specifications, and results obtained in one setting cannot be directly imported into the other.====Another basic property we impose on choice rules is continuity with respect to the minimal shares. When combined with independence of irrelevant alternatives, the condition has an additional powerful consequence: if an agent receives more than the minimal share in a given division problem and this minimal share decreases, then the chosen division does not change (Lemma 1). We use the terms division problem and choice problem interchangeably in this paper.====For the general ====-agent choice problem we add a third condition, called monotonicity. Suppose that, in a given division problem, an agent receives its minimal share at a chosen division. If this minimal share increases, then none of the other agents should benefit. This is a plausible fairness condition and facilitates the description of possible choice rules.====Our first main result (Theorem 1, Theorem 2 in Section 3) is the characterization of all choice rules satisfying independence of irrelevant alternatives, continuity, and monotonicity. We refer to the resulting choice rules as path rules. A path collection is a set of monotonic and continuous curves for the set of agents ==== and for all subsets of ====, in the simplex that represents the choice problem when all minimal shares are zero. These curves additionally satisfy a permutation independence condition, meaning that if the order of following different path pieces is permuted, then the same point is reached. The associated rule assigns to the subsimplex corresponding to a choice problem the point where one of these curves enters the subsimplex; permutation independence ensures that this is well-defined. For the case ====
 (Section 4) we obtain a larger class of path rules by dropping the monotonicity condition and imposing, instead, a nonintersecting path condition (Theorem 3).====In the final part of the paper (Section 5) we investigate the issues of rationalizability and representability. In our setting, independence of irrelevant alternatives is equivalent to the weak axiom of revealed preference, which requires that in the preference relation induced by a choice rule there are no cycles of length two. The strong axiom of revealed preference demands that there are no cycles of any length. We first show that independence of irrelevant alternatives and monotonicity imply rationalizability (Theorem 4); adding continuity, we also obtain representability of the choice rule ( Theorem 5). These results are closely related to Theorem 2, Theorem 3 in Kıbrıs (2012). That paper considers shortage rather than surplus sharing and focuses on rationalizability and representability. The proof of our Theorem 4 is completely analogous to the proof of Theorem 2 in Kıbrıs (2012). Our proof of Theorem 5 uses the characterization of rules in terms of paths but the representing real-valued function – assigning to a point ==== the largest volume of a simplex in which ==== is chosen – is analogous to the one used in Theorem 3 of Kıbrıs (2012).==== ====We also show that, without the monotonicity condition, excluding cycles of length ==== or less does not necessarily exclude cycles of length ====
 (Theorem 6)—thus, in particular, independence of irrelevant alternatives does not guarantee rationalizability, that is, the strong axiom of revealed preference. Finally, we show that for the case of three agents adding continuity and non-existence of cycles of length three implies the strong axiom of revealed preference ( Theorem 7). An extension of the latter result for ==== remains an open question.====The choice problem examined in this paper can be seen as the counterpart of the familiar bankruptcy problem, where the shares are claims which sum up to more than one and thus cannot all be satisfied. See Thomson (2015) for an overview of the literature on bankruptcy problems. Kıbrıs (2012) and Stovall (2014) consider the bankruptcy problem from a choice-theoretic perspective similar to ours. Kıbrıs (2012) focuses on rationalizability and representability and, as already explained, our Theorem 4, Theorem 5 are closely related to his main results. A difference with both Kıbrıs (2012) and Stovall (2014) is that we fix the surplus to be divided at one, while they consider (in their case) the estate to be divided as variable. Our results could be extended to other values of the surplus by assuming homogeneity, but that is more restrictive. Stovall (2014) also considers monotone path division rules but assumes a variable number of agents and imposes consistency—a condition which does not directly fit within our model.====Our paper contributes to the literature on revealed preference and rationalizability, starting with the seminal work of Samuelson (1938) and later Arrow (1959), Uzawa (1960), and Richter, 1966, Richter, 1971). We refer to Bossert and Suzumura (2010) for a recent overview of this literature. Apart from the contributions mentioned above, the present paper is perhaps most closely related to Bossert and Peters (2009), where choice sets are compact and convex subsets of ====. Here, the domain of choice sets – the standard simplex in ==== and all its subsimplices with the same orientation – is much smaller. One of the common denominators in the literature (see, for instance, Rose, 1958, Peters and Wakker, 1991, Peters and Wakker, 1994, Bossert, 1994, Blackorby et al., 1995, Bossert and Peters, 2009) is that while in two dimensions (which corresponds to the case ==== in the present paper) cycles of any length are excluded if cycles of length two or three are excluded, this observation does not carry over to more than two dimensions. Thus, as mentioned above, the case of more than three agents remains an only partially solved problem.",Choice on the simplex domain,https://www.sciencedirect.com/science/article/pii/S0304406819300758,19 July 2019,2019,Research Article,278.0
Fukuda Satoshi,"Department of Decision Sciences and IGIER, Bocconi University, Milan 20136, Italy","Received 6 December 2018, Revised 9 April 2019, Accepted 9 July 2019, Available online 18 July 2019, Version of Record 23 July 2019.",https://doi.org/10.1016/j.jmateco.2019.07.001,Cited by (7),This paper formalizes an informal idea that an agent’s knowledge is characterized by a collection of sets such as a ====-algebra if and only if the agent is additionally introspective about what she does not know.,"Economic agents base their decisions on their knowledge about uncertainty that they face. Where does their knowledge come from? Researchers often represent an agent’s knowledge by a (sub-)====-algebra on an underlying state space. For each element ==== of such ====-algebra, the agent is supposed to “know” whether the set ==== obtains at a realized state ==== (i.e., ==== lies in ====) in an informal sense. This informal understanding of the ====-algebra as representing the agent’s knowledge helps researchers to connect various measure- and probability-theoretical formal apparatus and their informal ideas behind economic problems at hand.====The main objective of this paper is to provide epistemic foundations for representing an agent’s knowledge by a collection of sets in terms of the agent’s logical and introspective properties of knowledge and of the structure of an underlying state space. Conceptually, this paper fully characterizes the hidden assumptions on knowledge when researchers use a particular form of a set algebra. The paper also clarifies the formal sense in which a given collection of subsets of the underlying states has an informational content. Especially, it fully answers the question of when a ====-algebra is an adequate tool for representing one’s knowledge.====To fix an idea, consider an agent named Ashley. The underlying space of uncertainty is a pair of a set of states and a collection of subsets of the states (i.e., events) about which she reasons. An event describes a certain aspect of the states. To describe her knowledge most generally within state space models, represent it by her knowledge operator that maps each event ==== to the event that she knows ====.====Under what conditions on Ashley’s knowledge, if possible, can the analysts represent her knowledge by a set algebra such as a ====-algebra or a topology? The main result of the paper (Theorem 1) shows that one can represent Ashley’s knowledge by a set algebra if and only if her knowledge satisfies the ====: Truth Axiom, Monotonicity, and Positive Introspection. By Truth Axiom, if Ashley knows an event at a state, then the event is true at that state. Thus, Ashley’s knowledge is truthful in contrast to her beliefs, as she may believe something false. By Monotonicity, if Ashley knows an event ==== and if ==== implies an event ====, then she knows ====. By Positive Introspection, if Ashley knows an event ====, then she knows that she knows ====.====Roughly, the main result is stated as follows. Under the basic conditions, the collection of events ==== which Ashley knows whenever ==== obtains (i.e., ==== is self-evident) satisfies the “maximality property:” for any event ====, the self-evident collection contains the maximal event included in ====. Conversely, let ==== be a collection of events with the maximality property. Then, one can construct a knowledge operator satisfying the basic conditions and whose self-evident collection coincides with ==== as follows: an event ==== is known at a state ==== if and only if there is an event ==== which is true at ==== and which implies ==== (i.e., ====). If ==== is taken as Ashley’s self-evident events, then it recovers her original knowledge operator.====Moreover, additional properties of knowledge are represented as set-algebraic properties of self-evident events. I fully characterize when Ashley’s knowledge forms such a set algebra as a ====-algebra or a topology depending on her additional properties and on the structure of the underlying states. When the underlying space is sufficiently rich, Ashley’s knowledge forms a topology if and only if she additionally knows a tautology and conjunctions of what she knows. Her knowledge forms a sub-algebra of the underlying space (e.g., a sub-====-algebra of a measurable space) if and only if her knowledge additionally satisfies Negative Introspection: if she does not know an event then she knows that she does not know it. Interestingly, Negative Introspection, together with Truth Axiom and Monotonicity, implies the conjunction property (Corollary 1). This representation is convenient as one can introduce fully-introspective knowledge on a probability space in a well-defined manner. For example, epistemic analyses of dynamic games often call for both knowledge and probabilistic beliefs (e.g., Dekel and Gul (1997)). However, standard partitional knowledge, defined for any subset of states, may not always be measurable.====The  maximality  property  is originally studied by Salonen (2009) and Samet (2010), which examine the relation between an information partition and a fully-introspective knowledge operator on an algebra of sets. The technical contribution of the main result is mild. Conceptually, however, it fully identifies: (i) the minimum conditions for knowledge to be represented by a collection of events (i.e., Truth Axiom, Monotonicity, and Positive Introspection) irrespective of the structure of underlying states; and (ii) when knowledge can be summarized by such a different set-algebra as a ====-algebra or a topology, depending on properties of knowledge and underlying states. Further discussions will be given in the next subsection.====Does there exist the smallest collection of events ==== including a given collection ==== and satisfying the maximality property? Call it the ==== of ====. Proposition 2 provides the condition under which a given collection has an informational content in that its information closure exists. This proposition makes it possible to compare collections of events according to informativeness (Corollary 2).====To demonstrate that information closures preserve informational contents of collections, consider a collection of information sets. Proposition 1 characterizes when Ashley’s knowledge is induced from her information sets on an arbitrary state space such as a measurable space. Her information set associated with a given state is the set of states that she considers possible at that state, even though she cannot identify the realized state. Thus, Ashley knows an event ==== at a state if her information set at that state implies (i.e., is included in) the event ====.====Proposition 3 states that Ashley’s self-evident events coincide with the information closure of her information sets. That is, her knowledge is induced by information sets if and only if the information sets have the informational content. Also, the information closure turns out to be the smallest ====-algebra or topology depending on assumptions on knowledge (i.e., the nature of information sets) and the state space. For example, information sets form a partition if and only if knowledge satisfies Negative Introspection in addition to the basic conditions. In this case, the information closure forms a sub-algebra (e.g., a sub-====-algebra of a measurable space). If the information partition is countable, then its information closure coincides with the smallest ====-algebra. Moreover, Corollary 3 states that the finer the information sets are, the larger (or, the more informative) the information closure is. To complete the discussion that information closures preserve informativeness, Proposition 4 establishes the Blackwell-type relation, as in Dubra and Echenique (2004), between preferences over signals (i.e., mappings defined on underlying states) and information closures generated by signals.====The paper is organized as follows. The rest of this section discusses the technical contributions of this paper, especially, how it resolves the problems documented in the literature on the sense in which ====-algebras represent knowledge. Section 2 sets up a framework. Section 3 presents the main results. Section 3.1 establishes the equivalence between a knowledge operator and a collection of events. Section 3.2 studies the informational content of a given collection. Section 3.3 studies its implications such as how a collection that represents knowledge is generated from information sets. Section 4 provides concluding remarks. Proofs are relegated to Appendix.",Epistemic foundations for set-algebraic representations of knowledge,https://www.sciencedirect.com/science/article/pii/S0304406819300746,18 July 2019,2019,Research Article,279.0
"Erlanson Albin,Kleiner Andreas","Department of Economics, Stockholm School of Economics, Sweden,Department of Economics, W.P. Carey School of Business, Arizona State University, United States of America","Received 5 July 2018, Revised 11 May 2019, Accepted 24 June 2019, Available online 15 July 2019, Version of Record 17 July 2019.",https://doi.org/10.1016/j.jmateco.2019.06.002,Cited by (4),We revisit the problem of a principal allocating an ,"A crucial part of designing mechanisms is to elicit private information. It is often assumed that private information cannot be verified in any way. However, there are many real-life situations when information indeed is verifiable as it may be based on hard information. In a recent paper, Ben-Porath et al. (2014) (henceforth called BDL) analyzed costly verification in a model where a principal allocates an indivisible good to privately informed agents. They showed that the optimal Bayesian incentive compatible mechanism for the principal is in the class of “threshold mechanisms”.==== ==== In a threshold mechanism, provided that some report is above the threshold, the agent with the highest reported value is verified and gets the object if he was truthful, and the object is randomly allocated according to a given probability distribution otherwise. If an agent is caught lying he does not receive the object.====BDL point out that, somewhat surprisingly, the optimal mechanism is ex-post incentive compatible. Thus, the optimal mechanism does not use the extra flexibility that a Bayesian mechanism offers. We explain this observation by establishing a more general equivalence: for any Bayesian incentive compatible mechanism there exists an “equivalent” mechanism that is implementable in ex-post equilibrium and induces the same expected verification costs. Proving the equivalence might help to analyze this or related models because it allows to optimize over a smaller class of ex-post incentive compatible mechanisms. Since these mechanisms are conceptually simpler the corresponding analysis might sometimes be more tractable. Similar equivalence results exist in the standard one-dimensional mechanism design setting with single-crossing and quasi-linear utilities (Mookherjee and Reichelstein, 1992, Manelli and Vincent, 2010, Gershkov et al., 2013).====In the second part of this note we provide an alternative proof for the optimality of threshold mechanisms==== ==== by using insights from the literature on interim allocation rules.==== ==== To prove the optimality of threshold mechanisms we observe that the relevant incentive constraints are formulated in terms of interim allocation and verification rules. Thus, we can restate the optimization problem using only interim rules and optimize over them directly, which is significantly easier. A characterization of feasible interim allocation rules is readily available due to Border (1991), and we can show that threshold mechanisms are optimal. Our approach of using interim allocation rules to solve for optimal mechanisms is one example among several recent papers, for other examples see Mierendorff (2016), Mylovanov and Zapechelnyuk (2017) and Pai and Vohra (2014).====The rest of the note is organized as follows. In Section 2 we introduce the model. In Section 3 we formalize and prove the equivalence between Bayesian and ex-post incentive compatible mechanisms. In Section 4 we provide our alternative proof for the optimality of threshold mechanisms.",A note on optimal allocation with costly verification,https://www.sciencedirect.com/science/article/pii/S0304406819300734,15 July 2019,2019,Research Article,280.0
"Roy Souvik,Storcken Ton","Economic Research Unit, Indian Statistical Institute, Kolkata, India,School of Business and Economics, Maastricht University, The Netherlands","Received 2 August 2018, Revised 1 May 2019, Accepted 13 June 2019, Available online 27 June 2019, Version of Record 9 July 2019.",https://doi.org/10.1016/j.jmateco.2019.06.001,Cited by (9),"We consider domains in strategic voting problems which satisfy three properties, namely top-connectedness, pervasiveness, and richness. We prove the following two results for such a domain: (i) it admits non-dictatorial, unanimous, and strategy-proof choice functions if and only if it has an inseparable top-pair, and (ii) it admits anonymous, unanimous, and strategy-proof choice functions only if it does not have any top-circuit. Finally, we establish the practical relevance of our results by applying them in the context of locating a public good or a public bad, preference aggregations, policy making, etc.","This paper examines typical, economically relevant collective choice problems where an alternative has to be chosen based on the preferences of the individuals in a society. Examples of such problems include electing a parliamentary candidate, deciding on which national policy to implement, finding the spatial distribution for provision of necessary public goods such as hospitals, schools, bus-stops, etc., or public ‘bads’ such as nuclear plants, garbage dumps, etc.====These collective choice problems are modeled by choice functions assigning to every possible combination of individual preferences a feasible alternative. A choice function is called unanimous if, whenever all individuals agree on their best alternatives, that alternative is chosen. It is called strategy-proof if no individual can change its outcome in his/her favor by misreporting his/her (sincere) preference. It is called dictatorial if there is a particular agent, also known as the dictator, such that at all combinations of individual preferences it chooses the best (most preferred) alternative of that agent. We call a set of admissible preferences a ‘possibility domain’ if it admits at least one non-dictatorial, unanimous, and strategy-proof choice function, otherwise we call it an ‘impossibility domain’.====Gibbard (1973) and Satterthwaite (1975), in their seminal contribution, prove that if there are at least three alternatives, then the unrestricted domain is an impossibility domain. The unrestricted domain is one which contains ==== possible strict preferences over the alternatives. In response to this result, the assumption of unrestricted domain has been relaxed and its consequences have been investigated extensively in the ensuing literature. It is worth noting that whether a domain is an impossibility domain or not depends on the presence or absence of certain combinations of preferences within it, and not on the number of preferences. For instance, if the number of alternatives ==== is at least three, then one can construct an impossibility domain with even six preferences (see e.g. Ozdemir and Sanver, 2007 or  Storcken, 1985), whereas a possibility domain can be constructed with as many as ==== preferences (see Aswal et al., 2003).==== ====Various sufficient conditions for possibility or impossibility domains are known in the literature. However, such conditions are often limited by a lack of transparency (see e.g. Sen et al., 1969, Kalai and Muller, 1977, or Kalai and Ritz, 1980), which makes it hard to find their applications. In this paper, we present a complete characterization of all possibility domains which satisfy three conditions, namely top-connectedness, pervasiveness, and richness. It is worth mentioning here that these conditions can be considered as mild in the sense that many domains of practical importance such as single-peaked (Moulin, 1980), single-dipped (Peremans and Storcken, 1999), single-crossing (Saporiti, 2014), circular (Sato (2010), Chatterji and Sen (2011)), etc., as well as the applications in Section 4 satisfy these conditions. We prove that such domains are possibility domains if and only if there is an inseparable top-pair. The notion of inseparable top-pair is related to that of an inseparable pair introduced in Kalai and Ritz (1980). A pair of alternatives ==== is called an inseparable top-pair in a domain if, whenever ==== appears as the best alternative in a preference, ==== is preferred to every alternative ==== with the property that ==== appears as the best alternative in some preference in the domain (see also  Aswal et al., 2003 and Bochet and Storcken, 2008). Aswal et al. (2003), Sato (2010), and Pramanik (2015) provide sufficient conditions for a domain to be an impossibility one, however our result is independent of their results. It is worth mentioning that these papers assume the domains to be ‘regular’ (that is, every alternative appears as the best alternative in some preference in the domain), where our results apply to the non-regular domains as well. In Section 5, we provide a formal discussion on the connection of our result with these closely related papers.====Note that possibility domains do not guarantee choice functions that have other desirable properties such as anonymity. In fact, there are possibility domains that admit unanimous and strategy-proof choice functions for which a particular agent behaves like a dictator for all but one type of profiles (see Example 3.1). This motivates us to provide a generalized structure of domains which admit choice functions which are anonymous as well as unanimous and strategy-proof. Theorem 2 of this paper provides a necessary condition for this by showing that a domain admits such a choice function only if its top-graph does not contain a cycle. In the case of Euclidean preferences, the restriction that there is no cycle in the top-graph boils down to the fact that the domain is a set of single-peaked preferences on a tree as defined in Demange (1982). Thus, for domains of Euclidean preferences, Theorem 2 provides a necessary and sufficient condition for admitting an anonymous, unanimous, and strategy-proof choice function (see Corollary 2).====Finally, we establish the practical relevance of our results by detailing some scenarios where our results can be readily used. Such situations include the problem of policy making, locating a public good or a public bad, preference aggregations, etc.====The rest of the paper is organized in the following way. Section 2 presents the basic model and the conditions that we impose on the domains. Section 3 presents the main results of the paper and Section 4 discusses some applications of those. Section 5 concludes the paper with a discussion on the connection of our results with the existing literature. All the proofs are relegated to the appendices.",A characterization of possibility domains in strategic voting,https://www.sciencedirect.com/science/article/pii/S0304406819300722,27 June 2019,2019,Research Article,281.0
"Liebrich Felix-Benedikt,Svindland Gregor","Department of Mathematics, University of Munich, Germany","Received 30 November 2018, Revised 12 February 2019, Accepted 20 May 2019, Available online 8 June 2019, Version of Record 14 June 2019.",https://doi.org/10.1016/j.jmateco.2019.05.002,Cited by (7),We study the problem of optimising the aggregated utility within a system of agents under the assumption that individual utility assessments are ====: they rank Savage acts merely in terms of their distribution under a fixed reference probability measure. We present a unifying framework in which optimisers can be found which are ,"A substantial driver in the development of mathematical economics has been the theory of general equilibrium, surveyed, among many others, by Debreu (1982) and Mas-Colell and Zame (1991). It mainly analyses whether and how a ==== populated by a finite number of agents can share a commodity in an efficient way. Efficiency is always to be understood against the backdrop of potentially varying ==== the agents have concerning the shares of the commodity they receive.====We shall refer to such sharing schemes as ====. The most prominent notion of their efficiency is ====, a systemic notion of stability and efficiency of an economy which means that no agent can improve her share without worsening the share of another agent. Formally, suppose the agents are represented by the set ====, share a common good ====, and entertain preferences ====, ====, concerning the share they are to receive. Then a sharing ==== is Pareto efficient if any other sharing scheme ==== which satisfies ==== for some agent ==== necessarily satisfies ==== for another agent ====.====Pareto efficient allocations can be analysed particularly well if the individual preferences ====, ====, admit a numerical representation: if ==== denotes the set of commodities agent ==== accepts as her share, a function ==== is a numerical representation of the preference relation ==== if ==== weakly prefers ==== to ==== if, and only if, ====. We will refer to ==== as a ====.==== ====Let us assume now that the commodity space involved in such a problem is a vector space ====. Given a good ==== which is to be shared, an allocation of ==== is any vector ==== with the property ====. This assumption of perfect substitution means that in principle, any sharing of ==== is hypothetically feasible for the agents. Suppose furthermore that each individual preference relation ==== can be numerically represented by a utility function ====, ====.====A key observation which will be the guiding thread of our investigations, initially due to Negishi, is the following: suppose that suitable positive weights ==== can be found such that the allocation ==== satisfies ====where ==== is an arbitrary allocation of ====. Then ==== is indeed a Pareto efficient allocation of ====.====Let us abstract this example which we shall get back to at a later stage of the paper. The allocation ==== is a maximiser for the optimisation problem ====where ==== is the set of all allocations ==== with the property ====, whereas ==== denotes the vector of individual utilities resulting for the agents from the sharing ====. These individual utilities are aggregated to a single quantity using the aggregation function ====and the optimal value is finite. As the aggregation function ==== in problem (1) may be chosen freely, it introduces substantial flexibility which we shall exploit in Section 5. Given a parameter ====, we will use the aggregation function ====to obtain ==== allocations as maximisers for problem (1). Similarly, if we choose the aggregation function ====maximisers will be so-called ====, which reflect certain notions of fairness in game theory. The reader should keep in mind that both the optimal value and the optimisation problem itself will be of secondary importance.====We shall assume throughout our study that all goods are risky future quantities or ====, i.e. real-valued random variables contingent on a measurable space ==== of future states of the world. One may also think of them in the interpretation ofMas-Colell and Zame (1991) as consumption patterns. Riskiness in the realisation of the states ==== is assumed to be governed by a reference probability measure ==== such that the probability space ==== is ====.==== ==== As usual, we shall identify two Savage acts ==== and ==== if the event ==== has full ====-probability. Substantial results have been achieved solving problem (1) with aggregation function ==== chosen as in (2) in a framework of Savage acts and involving  ==== preferences, c.f. Acciaio, 2007, Carlier and Dana, 2008, Chen et al., 2018, Dana, 2011, Filipović and Svindland, 2008, Jouini et al., 2008, Ravanelli and Svindland, 2014.====We adopt the assumption that the agents involved have law-invariant preferences, i.e. the values of the utility functions ====, ====, only depend on the ==== of the commodity under the reference probability measure ====: if two Savage acts ==== and ==== induce the same lottery over the real line under ====, i.e. if the Borel probability measures ==== and ==== on ==== are identical, then ==== holds for all ====, the reasoning being that utility only depends on statistical properties of the commodity. Along the lines of  Dana (2011) and Jouini et al. (2008), we shall refer to such utility assessments as law-invariant. Under the name of ==== it is a well-known property of preference relations which was introduced by Machina and Schmeidler (1992); we refer to Cerreia-Vioglio et al. (2012) as well as the references in Cerreia-Vioglio et al. (2012, footnote 2); however, these references typically study preference relations in an Anscombe–Aumann framework with general sets of consequences. Strzalecki (2011), on the other hand, studies probabilistic sophistication for general finitely additive reference probabilities. We use the term law-invariance to emphasise that we are working in a Savage setting with a numerical representation of a preference relation whose values only depend on the law under a countably additive reference probability measure.====Normatively, law-invariance can be interpreted as a form of ==== of the agents in that they are indifferent between two Savage acts yielding the same ==== — by inducing the same lottery under the reference probability measure ====. Practically, this consequentialism mostly relies on the fact that Savage acts can be grasped only in terms of empirical distributions of certain quantities, an observation which also explains the requirement of non-atomicity of the state space ====. There is a one-to-one correspondence between law-invariant utility functions over Savage acts contingent on a non-atomic space and preference relations on (suitable sets of) lotteries on the real line.====Preferences expressed by law-invariant utility functions have another economically appealing feature. Under a mild continuity assumption and quasi-concavity of the utility function ==== – that is, convexity of the preference relation expressed by ==== – law-invariance of ==== is equivalent to two standard notions of risk aversion: (i) monotonicity in the concave order which was introduced to the economics literature by Rothschild and Stiglitz (1970): if every risk averse expected utility agent weakly prefers ==== to ====, agent ==== with utility ==== weakly prefers ==== to ====; (ii) dilatation monotonicity: if ==== is a finite measurable partition of the state space, agent ==== weakly prefers the act associated to more information encoded by ====, i.e. the conditional expectation ====, to ==== itself. This will be discussed in detail in Theorem 18, to the best of our knowledge the most general version of this result in the literature and one of the main results of the paper.====Our established equivalence between law-invariance and concave order monotonicity has the important consequence that, in many situations, ==== maximisers for (1) can be found. An allocation ==== of ==== is comonotone if there are ==== non-decreasing functions ==== summing up to the identity — ==== holds for all ==== — such that ====, ====. The commodity ==== can be interpreted as a contract contingent on the common risk driver ====. Such comonotone allocations are desirable and have been widely studied. Empirical investigations of comonotonicity as a property of (optimal) allocations can be found in Attanasio and Davis (1996) and Townsend (1994). According to Carlier et al. (2012), who study multivariate comonotonicity, it is a property which – statistically – is “testable and tractable”. Key to solving (1) are so-called ==== results as given by Landsberger and Meilijson (1994), Ludkovski and Rüschendorf (2008), Carlier et al. (2012), and  Filipović and Svindland (2008). For comonotonicity in a multivariate setting we refer to Carlier et al. (2012) and the references therein. For its use beyond the risk sharing problem, see Cheung et al. (2014) and Jouini and Napp (2003) as well as the references therein.====Before we outline our main contributions, we give a brief overview of the rich existing literature of sharing problems as described above. For its treatment in general equilibrium theory, we refer to the survey articles by Debreu (1982) and Mas-Colell and Zame (1991) as well as the Khan and Yannelis (1991). More closely related and involving law-invariant criteria are the problems studied by Carlier and Dana (2008), which focuses on Rank Dependent Expected Utility agents and uses additional conditions, and Dana (2011), which studies optimal allocations and equilibria for concave, monotone and law-invariant preferences with strong order continuity properties on bounded wealths. Jouini et al. (2008) and  Acciaio (2007) study the problem for law-invariant utility functions under the additional assumption of cash-additivity. The comonotonicity of solutions to such sharing problems has been subject of, e.g., Chateauneuf et al. (2000) for Choquet expected utility agents, Strzalecki and Werner (2011) in the case of more general ambiguity averse preferences, and Ravanelli and Svindland (2014) who study agents with ==== as axiomatised by Maccheroni et al. (2006). There is also a rich strand of literature on sharing problems when the objective is not to maximise utility, but to minimise risk. The functionals involved are thus not utility functions, but ====. The case of agents with convex law-invariant and cash-additive risk measures has been studied by Filipović and Svindland (2008) on Lebesgue spaces, and by Chen et al. (2018) on general rearrangement invariant spaces. While Acciaio and Svindland (2009) treat the case of law-invariance for ==== reference probability measures, Liebrich and Svindland (2018) consider the problem for convex risk measures beyond law-invariance of the involved functionals. Finally, Mastrogiacomo and Rosazza Gianin (2015) study weak Pareto optima involving quasi-convex risk measures.====Our main contribution is to prove the existence of comonotone maximisers in (1), and thus of economically desirable allocations, in a wide range of situations by laying the groundwork in clear-cut meta results and then applying these in concrete cases which encompass, but go beyond Pareto efficiency, such as the application in game theory mentioned above. We prove that maximisers in problem (1) exist for agents with heterogeneous preferences as long as their utilities are law-invariant with respect to the reference probability measure ==== and suitable bounds hold on the one-dimensional subspace of riskless commodities. This approach distinguishes it from other contributions in this direction which restrict preferences to certain classes of law-invariant utilities. It therefore qualifies as unifying. We would like to point out a few noteworthy directions in which we were able to obtain general results:==== In Section 2, we thoroughly describe the setting in which we shall study the sharing problem. In Section 3, we study the problem on the commodity space ==== of all ====-integrable random variables. We isolate the core difficulties of the problem, find powerful meta results applicable in a range of situations as wide as possible, and give a set of straightforward criteria guaranteeing that problems of the shape (1) have maximisers. Section 4 has two parts: Section 4.1 collects the main contributions of our paper on the structural properties of quasi-concave functions on general rearrangement invariant Banach lattices of integrable functions. These findings are of interest beyond the existence of efficient allocations. In Section 4.2 we provide suitable local versions of the results in Section 3 on such general rearrangement invariant commodity spaces. Section 5 illustrates the range of economically relevant allocations which can be obtained with our method. Technical but straightforward estimates necessary for the applications are relegated to Appendix.",Efficient allocations under law-invariance: A unifying approach,https://www.sciencedirect.com/science/article/pii/S0304406819300606,8 June 2019,2019,Research Article,282.0
Zierhut Michael,"Institute of Financial Economics, Humboldt University, Dorotheenstrasse 1, 10117 Berlin, Germany","Received 13 November 2018, Revised 21 April 2019, Accepted 28 April 2019, Available online 6 June 2019, Version of Record 6 June 2019.",https://doi.org/10.1016/j.jmateco.2019.04.011,Cited by (2),"Any normative theory is based on a standard of social welfare. When markets are incomplete, the usual standard is constrained efficiency: A planner who may only use traded assets for transfers of future income cannot achieve a Pareto improvement. This paper points out that constrained efficiency is a weak basis for the normative theory of the firm. Unless short sales are restricted, a constrained Pareto optimum need not exist. This ==== problem is robust to perturbations of endowments and leads to surprising economic outcomes: Even though Drèze equilibria are the only candidates for constrained efficient plans, all of them can be Pareto dominated by equilibria with alternative objectives of the firm.","Normative economic theory judges economic outcomes by standards of social welfare and describes how economic agents should behave if a standard is to be met. The most common standard is the Pareto criterion: No agent can be made better off without harming another agent. As a pure criterion of efficiency it disregards other welfare considerations such as equity. In production economies with complete markets, the benchmark is a fictitious planner with the ability to choose production plans and reallocate consumption goods freely. A plan of production and consumption is Pareto optimal if such a planner cannot achieve a Pareto improvement. When markets are incomplete, this standard is too high: Some transfers that are feasible for the planner are not feasible for consumers who can only exchange marketed assets. To avoid this difference in abilities, it has become customary to constrain the feasible set of the planner such that only assets and present consumption can be reallocated. A constrained efficient plan is a Pareto optimal point in the constrained feasible set. The present paper shows that such a point need not exist. This raises the question in how far constrained efficiency should be used as a standard in the normative theory of the firm when markets are incomplete.====The concept of constrained efficiency is introduced by Diamond (1967), albeit in a somewhat restrictive setting: the set of feasible outputs is a ray for each firm, and short sales are not permitted. Drèze (1974) drops the former restriction in his normative approach to a general objective of the firm. He defines the objective in such a way that the resulting equilibrium equations satisfy the first order conditions of constrained efficiency. In the present paper, the second restriction is dropped as well. After all, short sale restrictions are not a technological constraint but an institutional arrangement, and standards of social welfare should be institution-free. This relaxation aside, the setting is identical to the partnership economy of Magill and Quinzii (1996, §31.): There are two dates, and all firms are organized as partnerships. At date 0, consumers may become partners and pay their share of the production costs. At date 1, all partners receive their share of the output. Due to constant returns to scale there are no incentives to exclude partners. Firms use the Drèze criterion to choose their production plans. All other assets are traded only at date 0 and pay off at date 1. Since there is only one consumption good, there is no need to reopen the market. By construction, Drèze equilibria are the only candidates to implement a constrained efficient plan. Nonetheless, and this is illustrated by means of an example in Section 3, it may well be that deviations from the Drèze criterion lead to Pareto superior plans. This paradoxical behavior points to a conceptual weakness.====Several shortcomings of the Drèze criterion are already known: Drèze (1974) himself points out the possibility of coordination problems between firms. Multiple firms may entail multiple Drèze equilibria, including some at which firms ignore their comparative advantages. The underlying problem is that the constrained feasible set is not convex. Therefore, the first order conditions are not sufficient for constrained efficiency, and some Drèze equilibria may be constrained inefficient. In the example, such coordination problems do not arise because there is only one firm. Nonconvexities also exist in the presence of income effects: Dierker et al. (2002) present an example with a unique Drèze equilibrium that is constrained inefficient. Even though a constrained efficient plan exists, it can only be implemented by the planner who must simultaneously change the production plan and make large transfers of present consumption to compensate the losers. These compensation payments could not be carried out in the market. The underlying problem is that the constrained feasible set is much larger than the set of possible market outcomes, and the constrained efficient plan lies outside the latter. In the example of the present paper, there is no such issue because all consumers have quasilinear utility, and thus there are no income effects.====Complications occur if short sales take place but the preferences of short sellers are not taken into account. In this case, Drèze equilibria are not only inefficient but may not exist at all: Momi (2001) documents a robust nonexistence problem when changes in the dimension of the asset span cause discrete changes in the ownership structure of a firm. In the example of the present paper, there is no such issue because all consumers have a positive share in the partnership. Whether the concept of Drèze equilibrium has, besides its normative character, also a foundation in positive theory is still an open question. The results so far are negative: Zierhut (2017) shows that a constrained efficient production plan would generically be rejected by a majority vote of shareholders. The outcome is similar if shareholders play a noncooperative bargaining game. In the partial equilibrium model of Britz et al. (2013), Nash bargaining results in a production plan that satisfies the Drèze criterion. However, the bargaining solution generically involves compensation payments between shareholders, but no such payments are made at a Drèze equilibrium. While these results challenge the positive foundations of Drèze equilibrium, the present paper challenges its normative foundations.====The focus on a two-date finance economy avoids the difficulties that arise when the Drèze criterion is extended to an economy with more than two dates. If there are multiple periods or multiple consumption goods, markets open again in the future, and present choices influence future prices. Price-taking consumers and firms do not take this influence into account. In a series of examples, Diamond (1980), Loong and Zeckhauser (1982) and Stiglitz (1982) show how constrained inefficient plans are realized in equilibrium. The inefficiency in these examples is not caused by the inability to resolve conflicts between shareholders; in fact, the technologies are so simple that no such conflicts exist. The problem is that exchange fails to result in constrained efficient consumption plans, regardless of how production plans are chosen. A planner who is aware of the price effect could attain a Pareto improvement by choosing different portfolios. Geanakoplos and Polemarchakis (1986) and Geanakoplos et al. (1990) show that this is a generic problem. A multiperiod Drèze criterion is developed by Dierker (2015) on the basis of minimal efficiency, which is a weaker standard. The concept rests on a planner whose power is limited to choosing production plans before the markets opens and reallocating present consumption after the market has closed. Since this planner cannot choose portfolios, his choice set is smaller than the constrained feasible set. However, the two sets are susceptible to the same problem: In the example of the present paper, neither a constrained efficient nor a minimally efficient plan exists.====The reason behind this nonexistence is that production plans simultaneously determine two variables of relevance: the asset span and the aggregate quantity. This causes a dilemma for the planner: If he chooses the optimal asset span, the optimal quantity cannot be achieved. If he chooses the optimal quantity, the dimension of the asset span drops, and an optimal distribution is no longer possible. A similar but more fragile phenomenon is documented by Werner (1991) in a different economic setting: There is no production, but assets pay off multiple consumption goods. In such a setting, the constrained feasible set is not independent of market prices. On the contrary, the prices in future spot markets determine the set of possible transfers. Therefore, the type of discontinuity in consumers’ budget correspondences that causes the nonexistence of exchange equilibria in Hart (1975) also appears in the feasible correspondence of the planner. However, the nonexistence problems described by Hart and Werner both do not survive perturbations of endowments, which is shown by Duffie and Shafer (1985) and Pan (1995) respectively. By contrast, the nonexistence of constrained efficient plans in partnership economies is robust to such perturbations.====The agenda is as follows: Section 2 describes the economic model. Section 3 presents the example. Section 4 extends the insights from the example to the general settings. Unproblematic economies are characterized, and it is shown that these are not generic. The section ends with a discussion of alternative approaches to the normative theory of the firm. Section 5 concludes.",Nonexistence of constrained efficient production plans,https://www.sciencedirect.com/science/article/pii/S0304406819300588,6 June 2019,2019,Research Article,283.0
"Rusinowska Agnieszka,Taalaibekova Akylai","CNRS – Paris School of Economics, Centre d’Economie de la Sorbonne, 106-112 Bd de l’Hôpital, 75647 Paris Cedex 13, France,Université catholique de Louvain, CORE, Voie du Roman Pays 34, B-1348 Louvain-la-Neuve, Belgium,Université Paris 1, Centre d’Economie de la Sorbonne, 106-112 Bd de l’Hôpital, 75647 Paris Cedex 13, France","Received 25 September 2018, Revised 7 April 2019, Accepted 20 May 2019, Available online 29 May 2019, Version of Record 4 June 2019.",https://doi.org/10.1016/j.jmateco.2019.05.003,Cited by (7)," in the game played by the persuaders with equal impacts. This characterization depends on influenceability and centrality of the targets. We discuss the effect of the centrist persuader on the consensus and symmetric equilibria, compared to the framework with only two persuaders having the extreme opinions. When the persuasion impacts are unequal with one persuader having a sufficiently large impact, the game has only equilibria in mixed strategies.","Social networks play a central role in most of our everyday activities, communicating and exchanging information, sharing knowledge, research and development, advertisement, among many others. A process that can perfectly be modeled by social networks is the one of opinion formation. The opinions result from interactions with other individuals that hold views on given issues. In the seminal model on opinion formation introduced by DeGroot (1974), individuals update their opinions by taking weighted averages of their “neighbors”, i.e., people that they are connected to in the network. An accompanying question being particularly important, e.g., in lobbying, political campaigning, marketing, or counter-terrorism, is how to identify optimal targets to achieve social impact. Indeed, the reliance on others to form opinions lies at the heart of advertising (Bimpikis et al., 2016), efforts to make people aware of different issues, preventing criminal social groups and organizations (Ballester et al., 2006), or attempts of capturing votes in elections. In economics such models are used to study competition between firms and product differentiation. In political science, they are applied for determining equilibrium outcomes of electoral competitions.====We consider a game of competitive opinion formation in a society played by three competing persuaders that have different opinions on a certain issue. The society consists of individuals having their own opinions on that issue and updating them like in DeGroot (1974), i.e., by taking weighted averages of individuals’ opinions that they listen to. The opinion is a real number between 0 and 1, and can be interpreted as the intensity of the opinion “yes”. Our point of departure for the present paper is Grabisch et al. (2018) who extend the DeGroot model by introducing two persuaders (called external players in their paper) with the extreme opinions 0 and 1. In the present paper, we introduce a third persuader which has the centrist opinion ====. Each persuader chooses one individual to target. Targeting in this setting means forming a link with that individual in order to make the average opinion in the society as close as possible to the persuader’s own opinion. The persuaders are characterized by (possibly unequal) persuasion impacts. The higher the impact of a persuader targeting an individual, the more this individual takes the persuader’s opinion into account when updating his own opinion.====The main objective of the present work is to study the effects of entering the additional centrist persuader into competition between the two extremist persuaders. First, we examine the opinion convergence and consensus reaching in the society targeted by the three persuaders. Is it possible to obtain a limit opinion vector? Can the society reach a consensus meaning that every individual has the same opinion? If so, how does such a consensus look like? More specifically, how does the presence of the centrist persuader change the convergence and consensus reaching in the society? In order to consider competition between the three persuaders, we define a noncooperative game played by the persuaders with strategies being target individuals and study the existence and characterization of pure strategy Nash equilibria. Grabisch et al. (2018) obtain a constant sum game where players have opposite interests. Our extended game cannot be considered as a constant sum game anymore, and hence we derive new expressions for the payoffs, appropriate for the extended setting. A number of new questions arise. How can the centrist persuader affect optimal strategies of the extreme persuaders determined in Grabisch et al. (2018)? How do characteristics of the key (i.e., targeted) individuals change when the third persuader enters into the play? Which network structures appear to be consistent with the equilibrium in pure strategies?====The extension of Grabisch et al. (2018) by introducing the third persuader with the centrist opinion has a number of consequences on consensus reaching in the society and Nash equilibria of the noncooperative game played by the persuaders. The presence of the centrist persuader preserves opinion convergence but changes the long run opinions of the society and the consensus. When the three persuaders choose the same target, a consensus exists and is determined by the three persuasion impacts. If the impact of the centrist persuader is vanishing, we recover the consensus with only two extreme persuaders. When the impact of one of the persuaders is much larger than those of the others, the consensus approaches the opinion of the high-impact persuader. Moreover, when all three persuaders target the same individual, the presence of the centrist one improves the situation of the weaker extreme persuader in the sense that consensus moves closer to the opinion of the smaller-impact persuader. Although in the paper we focus on the one-target framework, we also briefly discuss the case when the persuaders can target more than one individual. When the number of targets is the same, the convergence result is preserved, and when additionally the persuaders target the same individuals, the consensus result remains valid, independently of the common number of targets.====By using some notions and definitions given in Grabisch et al. (2018), we characterize equilibria in our three-persuader setting. The two key concepts are centrality (also called influence or intermediacy) and influenceability. An individual is more central than another individual if his influence on others reaches the network before the influence of the another individual. Influenceability of an individual means that he listens less to others, and hence it can be easier to influence him by an additional opinion. We focus our analysis on the case with equal impacts and find that both centrality and relative influenceability are important, and the target individuals are completely characterized by these two notions. More precisely, conditions for the existence of symmetric Nash equilibria of the game played by the three equal-impact persuaders are that the relative influence of a potential target must be at least twice higher than the one of any other individual in the network. Strong-impact persuaders must take into account the presence of the new centrist one. When comparing the results to Grabisch et al. (2018), the persuaders are demanding higher centrality from their potential targets to compensate the impact of the new persuader. However, when the persuaders have weak impact, the conditions for Nash equilibria are the same as for the case with only two extreme persuaders. If the persuasion impacts are unequal and one persuader’s impact is sufficiently large, then the game has only equilibria in mixed strategies. Besides symmetric equilibria, we also examine non-symmetric Nash equilibria. In particular, we deliver some necessary conditions for the existence of a non-symmetric equilibrium when the persuaders are equally strong. Moreover, we present many numerical examples that illustrate our results.====The leading assumption of the paper that each persuader targets only one individual covers many real-life situations with one target who is a kind of outstanding and influencing master. It is a well-known practice when a celebrity (actor/actress, sportsman/sportswoman, singer, model, showman, etc.) becomes an ambassador or a “face” of the brand. In this way, companies take into consideration the activity of the potential advertiser, his or her popularity, and the number of followers in social media. A good example is Ambassador Marketing that is a form of “word of mouth” marketing, where a person with specific influence or expertise participates in a brand’s marketing strategy, by presenting the brand in a way that encourages the audience to purchase a product. Usually the ambassador leverages his or her own popularity on social media platforms to drive the value.====The introduction of the centrist persuader with a specific position that involves balance, neutrality, and equal combination of the extreme positions makes the theoretical framework richer. It can give a more realistic explanation of the political issues, where ordering election candidates on a line and the presence of a centrist candidate is a quite usual assumption. A good example comes from the last French Presidential Elections with the current President seen as centrist. Also some economic spectrum can be covered by our modeling, where three parties can be seen as three main firms that differ from each other by production, work, and distribution. They can compete over marketing campaigns, product adoption, firm allocations, etc. While the framework of the persuaders with extreme and centrist opinions can find many real-life applications, an extension to many more persuaders does not seem so appealing in reality. There are numerous examples with a small number of persuaders, in particular with three persuaders, e.g., when well-known people use only iOS/Linux/Android software, drive German/British/Japanese cars, wear American/Italian/French brands, etc.====Our model can be applied to mobile operators. In most of the countries, the market of firms that provide mobile services is restricted to three or four large companies. In particular, they can be divided into three categories: company A with excellent coverage and high quality services (it settles high prices compared to competitors, but offers the best connectivity, strongest reliability, and highest average speeds across both urban and rural areas), company B (for average price, it gives unlimited talk, text and data with a very high speed, but company’s rural coverage is substandard), and company C (low-cost provider, i.e., it settles low price with weak network coverage and its services are not so good as its competitors’ products). Following our model, companies A, B and C can be seen as of three different categories: 1 stands for the best and expensive product, ==== is seen as average and affordable, and 0 reflects non-reliable and cheap services. The example exhibits several specific features relevant to our model. The number of consumers is finite, i.e., restricted by network coverage zone and long-term users. Also, people interacting with each other decide which company to choose. Their decision is mostly driven by the recommendations and opinions of their friends. Moreover, the market is saturated, i.e., almost every person is already a customer of at least one mobile company. Hence, competitors target a small number of potential consumers, and therefore targeting the same agent is a common practice.====Consider Adidas, Nike and Puma, which are three main football equipment manufacturers. Following our model, we can assume that Adidas and Nike are extreme persuaders since they are in daily battle with each other, and Puma can be seen as the centrist persuader. Each of these brands has representatives from the football world: Adidas has a contract with Lionel Messi, Nike — with Cristiano Ronaldo, and Puma — with Antoine Griezmann. In real-world examples, usually important persuaders do not target the same “face” of the brand. We notice a similar feature in our theoretical investigations, as there is no symmetric equilibrium in a perfectly symmetric network with three equally strong persuaders.====In the paper, we assume that the network is fully observed. While this assumption somehow restrict the model applicability to large size networks, it is quite realistic for smaller networks, where the relationships between individuals are easily observed. For instance, this can be the case in committees and smaller institutions whose members work together for a longer period and establish trust relations that become known to everybody.====The paper is organized as follows. The model is introduced in Section 2. Section 3 concerns the opinion convergence and consensus reaching. In Section 4 we define the noncooperative game played by the persuaders and present the equilibrium analysis. More precisely, we determine the equilibrium conditions for the case when the persuasion impacts are the same and briefly discuss the case of the unequal persuasion impacts. The related literature is surveyed in Section 5. Section 6 presents concluding remarks. The Appendix A, Appendix C, Appendix B present proofs of the main results (Appendix A), a discussion of the case when the persuaders target more than one individual (Appendix B), where we briefly discuss the convergence and consensus reaching in such a multi-target extended framework, and an elaboration on targeting different individuals (Appendix C).",Opinion formation and targeting when persuaders have extreme and centrist opinions,https://www.sciencedirect.com/science/article/pii/S0304406819300618,29 May 2019,2019,Research Article,284.0
"Florig Michael,Rivera Jorge","Département d’Economie, École Polytechnique, 91128 Palaiseau Cedex, France,Departamento de Economía, Facultad de Economía y Negocios, Universidad de Chile, Diagonal Paraguay 257, Torre 26, Santiago, Chile","Received 22 November 2017, Revised 19 November 2018, Accepted 12 May 2019, Available online 22 May 2019, Version of Record 3 June 2019.",https://doi.org/10.1016/j.jmateco.2019.05.001,Cited by (0),"This paper investigates the limit properties of a sequence of competitive outcomes existing for economies where all commodities are indivisible, as indivisibility vanishes. The nature of this limit depends on whether the “strong survival assumption” is assumed or not in the limit economy, a standard “convex economy”. If this condition holds, then the equilibrium sequence converges to a Walras equilibrium for the convex economy; otherwise it converges to a hierarchic equilibrium, a competitive outcome existing in this economy despite the fact that a Walras equilibrium might not exist.","The “discrete economy” proposed by Florig and Rivera (2017) is a private ownership economy where the indivisibility of commodities matters at an individual level, but is negligible at the level of the entire economy. The continuum of individuals that participate in this economy is partitioned into a finite number of types of agents. Individually, consumption and production sets are discrete sets (the same subset for agents of the same type), while their aggregate by type of agent is the convex hull of the individual set. Consumers of a given type are identical, except for a continuum parameter with which we initially endow them. This parameter could be identified as “fiat money” (see Drèze and Müller (1980)), whose sole role is to facilitate the trade of indivisible commodities. Despite the fact that fiat money has no intrinsic value whatsoever, since it does not enter into consumer preferences, it plays a fundamental role in the assignment of resources.==== ====Under mild conditions, Florig and Rivera (2017) prove the existence of a “rationing equilibrium” for discrete economies, a competitive outcome in which fiat money has a strictly positive price.==== ==== Moreover, when the distribution of fiat money is such that different consumers are initially endowed with a different amount of it, then a rationing equilibrium is a “Walras equilibrium with fiat money” for the discrete economy.==== ==== The proof of these results uses a “weak survival condition”, i.e., the initial endowment of resources belongs to the convex hull of the consumption set.====The aim of this paper is to investigate the limit properties of rationing equilibrium as indivisibility vanishes. This limit is an element of a “convex economy” (with fiat money), a standard economy with a finite number of agents, where both consumption and production sets are polyhedral, and where consumers are initially endowed with fiat money. Since a convex economy can always be approximated by a sequence of discrete economies,==== ==== the question in this paper actually refers to the limit properties of a rationing equilibrium sequence, whose elements belong to a sequence of discrete economies that converges to a convex economy.====Our main finding is that the nature of this limit is strongly dependent on the type of survival condition assumed in the convex economy. Under a “strong survival condition” the limit is a Walras equilibrium with fiat money for the convex economy. When fiat money has a strictly positive price in the convex economy, then the Walras equilibrium with fiat money corresponds to a “dividend equilibrium” (or “equilibrium with slack”), a generalized notion of the Walras equilibrium that allows for the possibility that some agents spend more than the value of their initial endowment (see Kajii (1996) and Mas-Colell (1992)). A situation like this may occur when, for instance, local satiation holds for some consumers, or when some price rigidities are present in the convex economy. Otherwise, fiat money becomes worthless in the convex economy (its price is zero at equilibrium), thus the Walras equilibrium with fiat money is a standard Walras equilibrium.====In our opinion, a most interesting situation occurs when a “weak survival assumption” is assumed for the convex economy. When the initial endowment of resources of each consumer does not belong to the interior of consumption set, the indivisibility of commodities could matter, regardless of how small it is. It may then occur that not all consumers have access to all goods, i.e., a commodity may be so expensive that some consumers who do not own expensive goods would not be able to purchase a single unit by selling their entire initial endowment. When the consumption goods become “more divisible”, i.e., if the minimal unit per commodity decreases, then the equilibrium price may react so that the situation persists.====Following Gay (1978), based on a generalized concept of price, several authors have proposed generalizations of the Walras equilibrium existing in the convex case, even when the Walras equilibrium does not exist due to a failure of the strong survival assumption (see, for instance, Danilov and Sotskov (1990),  Marakulin (1990) and Mertens (2003)). Backed up by several examples, Florig (2001b) proposes an interpretation of those generalized prices in terms of small indivisibilities, introducing the concept of “hierarchic equilibrium”.==== ====The main result of this paper is the proof of that when the weak survival condition holds in the convex economy, then the rationing equilibrium sequence converges to a hierarchic equilibrium of that economy. This result formalizes the interpretation of hierarchic equilibria in terms of small indivisibilities given in Florig (2001b).====A direct consequence of our convergence results is that when both the strong survival assumption and the local non-satiation hypothesis are satisfied, then a Walras equilibrium does exist for a convex economy.==== ==== We highlight, however, that this fact certainly does not inhibit the possibility of the existence of a Walras equilibrium, or a related competitive outcome, in convex economies maintaining one of these conditions, while relaxing the other. For instance, using variants of the “irreducibility” condition introduced by Gale, 1957, Gale, 1976, which is a weaker condition than the strong survival condition, but stronger than the weak survival condition, several authors have studied the existence of competitive outcomes in economy (see Baldry and Ghosal (2005), Florig (2001a), Gottardi and Hens (1996), Hammond (1993) and McKenzie (1959), Spivak (1978), among others). The existence of competitive outcomes for a convex economy when the non-satiation condition is relaxed was studied in, for instance, Drèze and Müller (1980), Marakov (1981) and Sato (2010). It is worth mentioning that all of these contributions maintain the perfect divisibility of goods.====This paper is organized as follows. In Section 2 we introduce preliminary concepts and notions, while Section 3 presents the model of economies and equilibria notions used in this paper. Therein we also define the notion of convergence of a sequence of discrete economies to a convex economy. In Section 4 we present the main contributions of this paper, the convergence of equilibrium results (namely, Proposition 4.2 when the strong survival condition holds, and Theorem 4.1 for a general case). Finally, most of the proofs are provided in the Appendix.",Walrasian equilibrium as limit of competitive equilibria without divisible goods,https://www.sciencedirect.com/science/article/pii/S030440681930059X,22 May 2019,2019,Research Article,285.0
"Melindi-Ghidi Paolo,Seegmuller Thomas","Economix - University of Paris Nanterre, France,Aix-Marseille University, CNRS, EHESS, Centrale Marseille, AMSE, France","Received 26 July 2017, Revised 4 February 2019, Accepted 18 April 2019, Available online 16 May 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jmateco.2019.04.009,Cited by (0)," and love for children. We show that independently from the type of altruism, a multiplicity of equilibria might emerge if the degree of love for children is high enough. We refer to this condition as the ====. Then, the ==== is determined by expectations on the future growth rate and the dynamics are not path-dependent. Our model is able to reproduce different fertility behaviours in a context of completed demographic transition independently from fundamentals, preferences, technologies and initial conditions.","Many countries have almost surely completed their demographic transition. For instance, in France, the total fertility rate of all women has reached its ====. Data from Insee==== ==== show that, from 2006 to nowadays, the total fertility rate of French women is on average stable at the replacement level of two children per woman.==== ==== Even though nowadays the total fertility rate of French women is on average stable at replacement level, if we focus on small geographical areas, such as regions or departments, very different fertility patterns emerge. The demographic literature has already highlighted the existence of these geographical differences in France dating back to the previous centuries.==== ====Fig. 1 maps the total fertility rate of French women in 2014 at department level. It clearly shows that differentials in fertility rates still persist nowadays. More precisely, if we concentrate on fertility rates in French departments belonging to the same regional entity, we can clearly observe very different fertility behaviours. The coefficients of spatial autocorrelation of Moran and Geary at department level confirm the existence of a strong fertility heterogeneity across departments.==== ====This is, in our opinion, a very interesting phenomenon that deserves to be analysed in detail. Indeed, in a context of completed fertility transition without heterogeneity of individuals’ preferences, abilities, norms or initial conditions, such as in departments belonging to the same regional entity, we would expect a convergence of fertility rates across departments. However, this expectation is not confirmed by the empirical evidence. The case of Alsace is emblematic.==== ==== Even though this region is composed by two departments, Bas-Rhin and Haut-Rhin, with similar, if not the same, fundamentals and initial conditions, we can observe the existence of an important fertility differential. We cannot ascribe this phenomenon to heterogeneity of preferences of individuals populating two neighbourhood departments, since their cultural fundamentals, religious values, historical heritages and social norms are the same. Of course, this fact is not specific to Alsace and can be observed in other adjacent French departments. At the same time, there also exists some French departments, adjacent and belonging to the same region, that do not exhibit any differentials but a convergence of fertility rates.====To understand if these different current fertility behaviours have persisted over time, we use time-series data for French departments. We observe that time-persistence might arise in different geographical areas of France. Fig. 2 clearly shows this statement.==== ==== In particular, it depicts the dynamic evolution of the total fertility rate of all women over the period 1975–2014 for four couples of two adjacent departments belonging to a particular region for which this persistence of fertility behaviours is pronounced.==== ====Looking at other adjacent departments in Fig. 1 we can clearly observe that differences in fertility rates at department level is not the only possible empirical outcome observable in French data. We also find patterns characterized by convergence of fertility rates across adjacent departments and over time. To support this claim, we present in Fig. 3 four cases of adjacent departments belonging to the same regional entity in which we do not report any persistence of fertility differentials over time.==== ==== The fertility behaviour of women in each department tends to be much more homogeneous with respect to the case of persistent fertility gap. Indeed, in all four cases in Fig. 3, fertility rates at department level fluctuate around very similar values and cross each other several times.====These important empirical facts we observe in French departments ask to answer to the following theoretical questions: how can we explain, on the one hand, the persistence and, on the other hand, the absence of fertility differentials in a context of completed fertility transition in which households’ preferences, such as altruism, norms, religious beliefs, technologies and initial conditions are homogeneous? Can an economic model of fertility explain this diversity when is not assumed the heterogeneity of fundamentals?====We claim that a standard fertility model with homogeneous preferences, fundamentals, technologies and initial conditions cannot explain this diversity of configurations because it would necessarily predict the convergence of fertility rates. Therefore, the main motivation of this paper is to fill the gap between economic theory on fertility and the empirical evidence we have highlighted in this section. To this end we propose a new theoretical model based on the role of expectations that might help to understand the variability of fertility behaviours across French departments. Put differently, the main objective of this paper is to develop a new model of fertility that is able to reproduce theoretically the evidence we observe in the French data at department level in a context of completed fertility transition. In the next section, we will present an economic model with endogenous growth, fertility, love for children and either paternalistic or dynastic altruism that is able to provide an explanation, based on the multiplicity of equilibria, of the different stylized facts presented in this section.====In the literature, several papers have explained the existence of fertility differentials, because of heterogeneous endowments in human capital and/or skills to work (see for instance Dahan and Tsiddon (1998), De la Croix and Doepke, 2003, De la Croix and Doepke, 2004, Kremer and Chen, 1999, Kremer and Chen, 2002). However, differences in fertility collapse in the absence of heterogeneities across households, as it seems to be the case among the French departments we focus on in Section 2. Other contributions interesting for our research questions develop models with endogenous fertility and traps due to a multiplicity of steady states. This multiplicity may be explained by the net return of capital which is no more always decreasing with respect to capital (Palivos, 1995, Cai, 2002) or the difference between the returns of investing in capital and having children (Becker et al., 1990, Galor and Weil, 1996). In these approaches, one converges to a steady state with high or low fertility depending on the initial condition on capital. The dynamics are typically path-dependent. This means that with the same initial conditions, as in the French departments we consider, one converges to the same equilibrium and fertility rate. Therefore, these different types of analysis cannot explain the diversity of configurations we highlight among French departments, such as in the region of Alsace. In addition, both the existence and the absence of fertility gaps across adjacent departments cannot be explained by such models in which individuals are homogeneous in terms of preferences, as we believe it is the case for individuals living in neighbourhood departments within the same regional entity.====For these reasons, we look at another explanation based on the multiplicity of equilibria. Depending on whether agents living in two adjacent departments coordinate their expectations on the same or on different equilibria, the model might reproduce the absence or the persistence of disparities in fertility rates. In other words, we develop a model which is able to replicate both empirical evidences without considering heterogeneity of preferences or different initial socio-economic conditions. We obtain these conclusions considering an endogenous growth model with fertility, love for children and either paternalistic or dynastic altruism. We recall that in the first case, utility depends on the amount of bequest for each child, while in the second one, utility of parents depends on utility of children. To have a model with growth as tractable as possible, we consider an ==== technology. We show that whatever the type of altruism, there exist two balanced growth paths under similar conditions. Therefore, our results do not depend on the type of altruism considered, which highlights the robustness of the findings.====Above all, our analysis emphasizes the crucial role of love for children, formalized by a utility that depends on the number of children ====. As we show, the multiplicity of equilibria emerges if love for children is significant, referred as the ====, and the marginal utility of having children does not strongly depend on the number of children, meaning that households are quite indifferent between two (stationary) fertility rates. In this case, the fertility rate is determined by expectations on the future growth rate. Indeed, depending on their expectations, agents coordinate on a high fertility rate or rather a lower one.==== ==== This result can be related to the Matsuyama (1990) model with real money balances in the utility in which multiplicities can occur. Indeed, in our framework, the number of children which is an endogenous argument of the utility function is also the price of capital. This may explain the diversity of configurations we observe among adjacent French departments belonging to the same region. If we associate our economy to a French department, households of two departments sharing the same fundamentals can coordinate their expectations on the same or on different equilibria. Finally, on the theoretical ground, our result is new for two reasons. First, it does not depend on a form of heterogeneity among agents (preferences, skills, initial conditions). Second, our economy can jump on one equilibrium or the other one because, in contrast to what we usually find in growth models with fertility, dynamics are not path-dependent. Because of love for children, the fertility rate is determined by expectations on the future growth rate.====Our analysis is also related to the ==== theory formulated in the sociological literature by the pioneering work of Hoffman and Hoffman (1973). These authors develop a sociological approach based on the idea that the combination of socio-economic and normative factors is able to influence fertility decisions. The main assumption of their sociological model is that the value that parents give to their children determines the intra-family relationships and the fertility behaviours. Empirical studies have tested if the value of children hypothesis is able to explain fertility differentials within and between countries. The results indicate that this hypothesis is quite predictive of fertility intentions (Mares and Mozny, 2005, Nauck, 2006, Nauck and Klaus, 2007). To our knowledge, no economic contributions have tried to give an explicit theoretical formulation of this theory. We show that this can be done extending basic growth model of rational fertility choice to the idea that the marginal utility of having children does not strongly decrease with the number of children. In this case, expectations on the value of children could explain the appearance of fertility differentials between households and geographical areas characterized by the same fundamentals. However, in an economic setting with altruism and love for children, it seems to us more appropriate to refer to this theoretical approach as ====, that is, a welfare that strongly weights utility for the number of children.====This paper is organized as follows. In the next section, we develop our model with endogenous growth, fertility, love for children and either paternalistic or dynastic altruism that exhibits a multiplicity of equilibria. We discuss and interpret our results in Section 3. We conclude in Section 4, while technical proofs are relegated to an Appendix.",The love for children hypothesis and the multiplicity of fertility rates,https://www.sciencedirect.com/science/article/pii/S0304406819300503,16 May 2019,2019,Research Article,286.0
"Hayashi Takashi,Lombardi Michele","University of Glasgow, United Kingdom","Received 10 November 2018, Revised 15 March 2019, Accepted 18 April 2019, Available online 16 May 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jmateco.2019.04.007,Cited by (4),"In many situations, agents are involved in an allocation problem that is followed by another allocation problem whose optimal solution depends on how the former problem has been solved. In this paper, we take this dynamic structure of allocation problems as an institutional constraint. By assuming a finite number of allocation problems, one for each period/stage, and by assuming that all agents in society are involved in each allocation problem, a dynamic mechanism is a period-by-period process. This process generates at any period-==== history a period-====, this process selects for each state a period-==== socially optimal outcome conditional on the complete outcome history realized up to period ====. Heuristically, the SCF is one-step-ahead implementable if there exists a dynamic mechanism such that for each state and each realized period-==== onwards. We identify a necessary condition for SCFs to be one-step-ahead implemented, one-step-ahead Maskin monotonicity, and show that it is also sufficient under a variant of the condition of no veto-power when there are three or more agents. Finally, we provide an account of welfare implications of one-step-ahead implementability in the contexts of trading decisions and voting problems.","The theory of implementation investigates the goals that a planner can achieve when these goals depend on private information held by various agents. The problem of the planner is to design a mechanism or game form in which the agents’ incentives dovetail to an equilibrium outcome that coincides with the planner’s goal. When such a mechanism exists, his goal is fully implementable. This paper studies full implementation problems in a dynamic environment, in which:====Many real-world allocation problems have the above dynamic structure. For example, in democratic societies, the identity of governments may change over time due to periodic elections, the policy variables chosen by the current government affect the optimal decisions of future governments and, moreover, the current government handles its decision problem without being able to commit to future policy variables (see, e.g., Persson and Tabellini, 2002, Krusell et al., 1997).====  Also, in a market, today trading affects future trading activities, and the role of the market maker is to facilitate trade period-by-period but not to make a commitment related to future trading activities, or to enforce them over time (see, e.g., Radner, 1972, Radner, 1982, Prescott and Mehra, 1980). More generally, the above set-up is justified by the fact that in many real-life situations, agents are involved in an allocation problem that is followed by another allocation problem, whose optimal solution depends on how the former problem has been solved. This dependence is evident when agents have non-separable preferences.==== ====In this paper, we take this dynamic structure of allocation problems as an institutional constraint. Given that the goal of implementation theory is to study the relationship between outcomes in a society and the mechanisms under which those outcomes arise, it is important to throw light on how such an institutional constraint affects outcomes in society. In this paper, we ask the following question: If we take the described dynamic structure as an institutional constraint, can one describe the requirements on social choice functions (SCFs) that are equivalent to subgame-perfect Nash implementability by a sequence of period-==== mechanisms?====This paper answers the above question by assuming that every agent in society is involved in each period-==== mechanism and that there are ==== periods. Moreover, it does it by assuming that the SCF is a (complete) contingent plan of action: In every period ====, the SCF specifies a period-==== socially desirable outcome conditional on the outcome history realized up to period ====. We make this assumption due to our institutional constraint and due to the notion of subgame perfection. More precisely, given that the sequence of allocation problems can only be solved period-by-period, given that the optimal solution to period-==== allocation problem depends on how the previous allocation problems have been solved, and given that there is a positive, albeit small, probability that agents make mistakes when they carry out their intended actions — this is one of the assumptions on which the notion of subgame perfect equilibrium is based, it is compelling to assume that the SCF is a period-by-period process that assigns an optimal solution to period-==== allocation problem that depends on the complete outcome history realized up to period ====. Therefore, the SCF ==== is defined as a list of period-==== SCFs, ====, one for each period ====, such that each period-==== SCF ==== depends on the state ==== and on the outcome history realized up to period ====. Observe that period-1 SCF ==== depends only on the state ====.====Given that the sequence of period-==== mechanisms can be thought of as one “large” dynamic mechanism, let us denote this dynamic mechanism by ====. Before introducing our notion of implementation, it will be useful to develop some terminology. A period-==== history ==== is a sequence of choices made by agents from period 1 to an intermediate period ====. A period-==== history ==== is consistent with a period-==== history ==== when ==== is the initial part of ====. Since by assumption every agent is involved in each period-==== mechanism, an agent’s (pure) strategy ==== assigns a feasible action to every period-==== history ====. This paper uses subgame-perfect Nash equilibrium as the equilibrium concept for solving, after every period-==== history ====, the dynamic game that the dynamic mechanism ==== may lead to. Since each agent’s strategy is a complete contingent plan of action, a subgame-perfect Nash equilibrium strategy profile ==== of the dynamic game ==== generates a period-by-period outcome process, in the sense that ==== specifies an equilibrium outcome for every period-==== history that is consistent with ====.====The following notion of one-step-ahead implementation is adopted. A SCF ==== is ==== if there exists a dynamic mechanism ==== such that for each state ==== and each period-==== history ====, the following two requirements hold. (====) The period-by-period outcome process generated by ==== at state ==== from period ==== onwards coincides with the period-by-period outcome process generated by at least one (pure) subgame-perfect equilibrium strategy profile of the dynamic subgame ====. (====) Every subgame-perfect Nash equilibrium strategy profile of ==== generates a period-by-period outcome process that coincides with the period-by-period outcome process generated by SCF ==== at state ==== from period ==== onwards.====Under our notion of implementation, we provide a necessary condition, called ====. This condition is an adaptation to our framework of the fundamental property for Nash implementation, now widely referred to as Maskin monotonicity (Maskin, 1999). Maskin monotonicity says that if ==== is socially optimal at ==== but not at ====, then the outcome ==== must have fallen strictly in someone’s ordering at the state ====. To introduce our variant of Maskin monotonicity, note that in every period-==== environment each agent ranks period-==== outcomes according to her period-==== reduced preferences, which are induced by means of backward induction. This means that a period-==== reduced preference ordering over the set of period-==== outcomes depends on past decisions as well as on the socially optimal path that the dynamic process ==== will bring about in the future. Thus, one-step-ahead Maskin monotonicity requires that every period-==== feasible SCF must be Maskin monotonic with respect to period-==== reduced preferences. A period-==== SCF is feasible when it is Maskin monotonic, after every feasible outcome history.==== ====Furthermore, if for every period ====, the period-==== SCF satisfies the condition of no veto-power with respect to period-==== reduced preferences, we show that the necessary conditions are also sufficient. The dynamic mechanism we construct to achieve the full implementation uses the Maskin mechanism in each period, not only on the equilibrium path but also out-of-equilibrium path.====  The reason is that period-==== planner can neither punish agents over time nor compensate agents in the future when they deviate from a socially undesirable period-==== outcome. Though the implementing dynamic mechanism may look complicated, the idea behind it is very simple. Indeed, it can be thought of as a tree (finite directed graph) in which a Maskin mechanism corresponds to each node and in which each node corresponds to a history. Each branch emanating from a node can be thought of as a possible outcome that players can achieve via the mechanism. Then, given a node, the corresponding Maskin mechanism associated with this node simply asks players to report their ranking of branches (plus some tie-breaking information). Note that when a player points out his best branch, she reveals only a partial information about which sequence of outcomes she wants to achieve. For instance, in a consumption-saving model, the period-==== planner asks agents to report how much they would like to save/consume in the current period but not to report the full time sequence of consumption/saving. The implementability of a SCF is determined by whether such a one-step-ahead manner is enough for extracting information necessary for it.====Finally, we provide an account of welfare implications of our sufficiency result in the context of trading decisions and voting problems.====Firstly, we consider a borrowing–lending model with no liquidity constraints, in which agents trade in spot markets and transfer wealth between any two periods by borrowing and lending. In this set-up, intertemporal pecuniary externalities arise because today’s trade changes tomorrow’s spot price, which, in turn, affects its associated equilibrium allocation. The quantitative implication of this is that every agent’s reduced preference concerns not only her own consumption/saving behavior but also the consumption/saving behavior of all other agents. Under such a pecuniary externality, we show that the standard sequential competitive equilibrium (or Radner) solution is not one-step-ahead implementable because it is not one-step-ahead Maskin monotonic (see Claim 1). We have also identified preference domains – which involve no pecuniary externalities – for which the sequential competitive equilibrium solution is definable and one-step-ahead implementable. It is worth emphasizing that when we focus on non-contingent SCFs (or correspondences), the sequential competitive solution reduces to the Walrasian solution under certainty, and this solution is implementable in Nash equilibrium, and so in subgame-perfect Nash equilibrium, when Walrasian equilibrium allocations on the boundary of the feasible set are ruled out.====  The reason is that in this case the Walrasian solution satisfies Maskin monotonicity, which is a necessary condition for implementation in Nash equilibrium (Hurwicz et al., 1995). This means that the additional requirement of the paper that implementation should be achieved in a one-step-ahead manner is indeed important and binding (see Section 1.1. for a more elaborated discussion).====Secondly, we consider a bi-dimensional policy space where an odd number of agents vote sequentially on each dimension and where an ordering of the dimensions is exogenously given. We assume that each voter’s type space is unidimensional, that a majority vote is organized around each policy dimension and that the outcome of the first majority vote is known to the voters at the beginning of the second voting stage. This dynamic resolution is common in political economy models (see, e.g., Persson and Tabellini, 2002). In this environment, we show that the simple majority solution, which selects the Condorcet winner in each voting stage, is one-step-ahead implementable. In this process, we explicitly state the conditions on the utility function of each voter that are needed for this SCF to be well-defined and show that this is the case. As established by De Donder et al. (2012) for the case where there is a continuum of voters, the assumption that both dimensions are strategic complements, as well as the requirement that the induced utility of both dimensions is increasing in the type of the voter, are particularly important for guaranteeing the existence of a Condorcet winner in each voting stage.",One-step-ahead implementation,https://www.sciencedirect.com/science/article/pii/S0304406819300485,16 May 2019,2019,Research Article,287.0
Mavi Can Askan,"CEE-M, Univ. Montpellier, CNRS, INRA, SupAgro, 2 Place Pierre Viala, 34060 Montpellier, France","Received 20 December 2017, Revised 15 December 2018, Accepted 18 April 2019, Available online 11 May 2019, Version of Record 11 May 2019.",https://doi.org/10.1016/j.jmateco.2019.04.008,Cited by (4),"This paper aims to analyze the overlooked link between catastrophic events and sustainability through a limit cycle analysis. We use and extend a well known Calvo and Obstfeld (1988, Optimal Time-Consistent Fiscal Policy with Finite Lifetimes, Econometrica) framework in order to distinguish individual’s and social planner’s discount rates and show that ==== occurs at two critical values for individual discount rate, only if the economy is exposed to catastrophic event risk. This result is important because the role of individual discount rate on aggregate long-term dynamics has been overlooked in the literature. More importantly, the existence of limit cycles implies that consumption and natural resource stock are exposed to cycles in the long run, meaning that the path of utility does not conform to the prominent Sustainable Development criterion. Lastly, we analyze the economic reasons behind limit cycles and show that protecting the environment decreases the likelihood that limit cycles will occur.","It is widely recognized that uncertain catastrophic events could cause large scale damages (Alley et al., 2003, Field et al., 2012). Many studies have focused on decision-making regarding the exploitation policy of natural resources under uncertainty (Bretschger and Vinogradova, 2017, Tsur and Zemel, 1998, Tsur and Zemel, 2007, Tsur and Zemel, 2016, Clarke and Reed, 1994). In addition, some recent studies concentrate on determining the optimal environmental policy to deal with uncertainty. For this purpose, adaptation and mitigation policies and their implications under uncertainty are a major point of interest (Zemel, 2015, Tsur and Zemel, 2015, Mavi, 2017).====Apart from studies on uncertainty and resource exploitation, another branch of economics literature concentrates on the relationship between discounting and sustainability, which is a long-standing and important debate. The debate has been intensified in the context of climate change (Stern, 2006, Weitzman, 2007, Heal, 2009). Some of these studies are related to the role of individual time preferences (see Endress et al. (2014), Schneider et al. (2012), Marini and Scaramozzino, 1995, Marini and Scaramozzino, 2008, Burton (1993)). The presence of individual time preferences in an economic model is appealing because the infinitely lived agent model (ILA model hereafter) is criticized for not considering consumer’s preferences.==== ====The articles cited above which incorporate individual discount rate are based on the seminal framework proposed by Calvo and Obstfeld (1988). The authors introduce individual time preferences (i.e. individual discount rate) in an overlapping generations model (OLG hereafter). Then, they find the aggregate consumption level of all generations at a given time. Once the aggregation is made, the model reduces to the representative agent framework. This framework has been used in environmental economics literature, including the above-cited papers to study some important topics, such as inter-generational equity==== ==== by the above-cited papers. However, these papers do not analyze the role of individual time preferences on long-term aggregate dynamics. This clearly introduces a dichotomy between the OLG part and ILA model, as the implications of individual discount rate on the long-term dynamics in the ILA model are unknown. One of the aims of this paper is to resolve this dichotomy and analyze in depth the effects of individual discount rate on aggregate dynamics.====How does the present study fit into the literature? On the one hand, studies investigating the long-term impacts of uncertainty on resource exploitation policies do not consider sustainability and intergenerational equity (see Bommier et al. (2015), Zemel (2015)). On the other hand, the strand of literature on sustainability and intergenerational equity does not consider uncertain events (see Marini and Scaramozzino, 1995, Endress et al., 2014, Burton, 1993). To the best of our knowledge, the link between sustainability and catastrophic events is overlooked in the environmental economics literature. This paper aims to fill this gap.====In this paper, we have two important motivations: First, we aim to explain the role of individual preferences on the occurrence limit cycles (Hopf bifurcation). Second, we show that limit cycles do not conform to the prominent Sustainable Development criterion. Then, we argue that the Sustainable Development criterion should be revised by policymakers to encompass limit cycles. Otherwise, one should avoid these cycles from a normative point of view.====The contribution of this paper is twofold: First, by extending the Calvo and Obstfeld (1988) framework to account for uncertain events, we show that, for two critical parameter values for the individual discount rate, endogenous cycles (Hopf Bifurcation) arise in the economy in the long run. The mechanism behind limit cycles can be summarized as follows: On the one hand, the economy accumulates physical capital and creates waste. In this sense, the environment is used as a “sink” in the economy. This can be considered the economic goal. On the other hand, because the damage inflicted after a catastrophic event is proportional to the remaining natural resource stock after the event, the economy wishes to protect natural capital. This is the environmental goal. When it becomes difficult to steer between these conflicting goals, it may be optimal to cycle around the optimal steady state==== ==== (see Wirl (2004)).====What effect does catastrophic event probability have on limit cycles? Note that when there is no chance of a catastrophic event occurring, the above-mentioned trade-off between the economic and environmental goal disappears because the environment has only a proactive role, meaning that the utility from the environment becomes effective only once the catastrophic event occurs. Therefore, individual preferences cannot cause limit cycles due to the absence of the trade-off between the economic and environmental goal.====To better understand our motivation for using an OLG model, some additional clarifications regarding the link between individual preferences and the occurrence of limit cycles are offered here. In fact, the existence of limit cycles is possible even without an OLG model. One can easily show that limit cycles take place in a representative agent model (see Wirl (2004)). In other words, the main source of the bifurcations is the above-mentioned trade-off, not the structure of the population. However, this is not to say that individual discount rate does not matter for bifurcations. Individual discount rate is important in the sense that it can make it more or less difficult to steer between the economic and environmental goal. For some levels of individual discount rate, it becomes difficult to decide between the environmental and economic goal. It is this difficulty, which depends on individual discount rate, that makes cycles appear. Therefore, we feel it is important to focus on individual discount rate in this study.====Indeed, because our aim is to focus on the importance of individual preferences regarding the sustainability of the economy, we think it is helpful to use the well-known Calvo and Obstfeld (1988) framework, as it allows us to distinguish individual discount rate from the social planner’s discount rate.====One may argue that the trade-off between the economic and environmental goal, which is the source of limit cycles, is common in growth models that contain environmental aspects. In such a framework, the occurrence of limit cycles can be understood through the growth condition (i.e., the state grows but below the social planner’s discount rate) as shown in Wirl (2004).==== ====Indeed, limit cycles have been studied extensively in environmental economics. Wirl, 1999, Wirl, 2004 and Bosi and Desmarchelier, 2016, Bosi and Desmarchelier, 2017 studied the existence of limit cycles in models with representative agent frameworks. None of their studies link limit cycles to equity across generations and sustainability as described by the Sustainable Development criterion.====At this point, the question to be addressed is: What are the implications of limit cycles regarding sustainability? Note that the Sustainable Development criterion requires that the utility of consumption has a non-decreasing path (i.e. ====). If the economy is exposed to limit cycles due to the trade-off between the environmental and economic goal and/or due to the complementarity of preferences, the Sustainable Development criterion is not respected, as the utility and the dynamics of natural resource stock exhibit cyclical behavior in the long run.====Secondly, contrary to the Calvo and Obstfeld (1988) framework and the articles using this framework, we show that individual discount rate can change the stability properties of the model when the economy is exposed to catastrophic event uncertainty. This result disproves the conventional statement that aggregate dynamics are governed solely by the social planner’s discount rate (see Endress et al. (2014), Schneider et al. (2012), Marini and Scaramozzino, 1995, Marini and Scaramozzino, 2008, Burton (1993)). Indeed, we show that individual discount rate has an important role regarding long-term aggregate dynamics and, hence, the sustainability of an economy.====Because the first part of the model is an OLG model, there is an intra-generational allocation of consumption which is stable over time. We also show that intra-generational equity can conform to sustainability, as we show that a more even allocation of consumption between generations ensures a stable equilibrium in the long run.====As stated above, we argue that there are two options: Either a policymaker should revise the Sustainable Development criterion to encompass limit cycles, or one should avoid these cycles in order to respect this criterion. One can argue that the second option is better, as sustainability and intergenerational equity are generally perceived as normative inquiries (Solow, 2005, Solow, 2006). As a result, a social planner who pays attention to sustainability and intergenerational equity should seek to avoid limit cycles. We show that the social planner can avoid limit cycles through an environmental policy that aims to protect the environment. This is made possible by the fact that a higher natural resource stock implies a lower marginal utility of consumption. As a result, different levels of individual discount rate are expected to have a relatively small effect on the trade-off between the economic and the environmental goal. Consequently, it would be less likely that an economy would become trapped in limit cycles in the long run.====The remainder of the paper is organized as follows: Section 2 presents the benchmark model and explains all economic mechanisms behind the limit cycles in detail. Section 3 focuses on thresholds and multiple equilibria. Section 4 explains the model with environmental protection that can avoid limit cycles. The last Section 5 concludes the paper.",What can catastrophic events tell us about sustainability?,https://www.sciencedirect.com/science/article/pii/S0304406819300497,11 May 2019,2019,Research Article,288.0
Pištěk Miroslav,"The Czech Academy of Sciences, Institute of Information Theory and Automation, Pod Vodárenskou věží 4, 182 08, Prague 8, Czech Republic","Received 3 January 2019, Revised 3 April 2019, Accepted 9 April 2019, Available online 29 April 2019, Version of Record 14 May 2019.",https://doi.org/10.1016/j.jmateco.2019.04.006,Cited by (2), and opens a way for a non-probabilistic interpretation of the algebraic theory. Note finally that our method of using powerful topological techniques to derive purely algebraic result may be of general interest.,"Many systematic violations of the ==== (Von Neumann and Morgenstern, 1953) have been observed, see e.g. Tversky (1969), stimulating the development of alternativedecision-making  theories   (Fishburn, 1988, Starmer, 2000, Machina et al., 2004). In particular, the axiom of ==== of preferences, nowadays understood as an intuitively appealing cornerstone of rationality, is not always supported by empirical evidence (Bar-Hillel and Margalit, 1988, Butler et al., 2016). A concise mathematical model of non-transitive decision-making has been proposed in Kreweras (1961) and Fishburn (1982), representing preferences with a skew-symmetric bilinear (SSB) functional. Note that from the mathematical point of view, such representation is closely related to the regret theory (Loomes and Sugden, 1982), see Blavatskyy (2006).====Denoting ==== an asymmetric relation of strict preferences on a non-empty convex set ====, we say that a functional ==== on ==== is an SSB representation of ==== if ==== is SSB and ==== for all ====. Let ==== and ==== be indifference and preference-or-indifference relations defined in a standard way using ====. Then, the axioms of (algebraic) SSB representation stated for all ==== and all ==== are as follows: ====
 ==== If ==== is, moreover, a set of probability measures, axioms (C1)–(C3) hold if and only if there exists an SSB representation of ====, see Fishburn (1982, Theorem 1). ====Recently, a variant of SSB representation of preferences has been proposed in a topological vector space. For a non-empty convex subset ==== (being equipped with the relative topology) the axioms for all ==== and ==== are the following: ==== An asymmetric binary relation ==== on ==== satisfies (F1), (F2) and (F3) if and only if there exists an SSB representation of ==== that is, moreover, separately continuous in each variable, see Pištěk (2018, Theorem 3.6 and Theorem 5.3). ==== ==== Further, the existence of a maximal element of ==== with respect to ==== has been shown, assuming compactness and convexity of ====, see Pištěk (2018, Corollary 3.4 and Theorem 3.6). In the algebraic setting of SSB representation, a similar result has been shown only for a (finitely generated) polyhedral set (Fishburn, 1988 Theorem 6.2).====In this article, we show that algebraic SSB representation may be, somehow surprisingly, considered only as an application of the above introduced topological theory. By using the so-called ==== (Jarchow, 1981, Bogachev and Smolyanov, 2017) for an underlying linear vector space, we show that axioms (C1) and (F2) imply axiom (F1) with respect to such topology, see Proposition 4.1. This step is essential to prove that axioms (C1), (F2) and (F3) are equivalent to the existence of algebraic SSB representation of preferences on ==== in a fully abstract setting, see Theorem 4.2. As a consequence, axioms (C3) and (F3) are equivalent given axioms (C1) and (F2), thus we have generalized Fishburn (1982, Theorem 1) that has been stated for a set of probabilistic measures using a stronger convexity axiom (C2). Further, we propose a generalized existence result for a maximal element w.r.t. ====, see Theorem 4.3. Note finally that the technique used may be of general interest since it permits one to use topological tools to obtain relatively stronger results that may be finally transposed to a purely algebraic setting employing the inductive linear topology.====The article is organized as follows. Section 2 presents the basic notation and preliminary results. In Section 3 we introduce the notion of the inductive linear topology and discuss its basic properties. The main theorem of the algebraic SSB representation is presented in Section 4 together with all the related results. When working on this article, several inaccuracies in Pištěk (2018) have been discovered; all of them are corrected in Appendix.",SSB representation of preferences: Weakening of convexity assumptions,https://www.sciencedirect.com/science/article/pii/S0304406819300473,29 April 2019,2019,Research Article,289.0
"Kitahara Minoru,Okumura Yasunori","Department of Economics, Osaka City University, Japan,Department of Logistics and Information Engineering, TUMSAT, 2-1-6, Etchujima, Koto-ku, Tokyo, 135-8533, Japan","Received 8 June 2018, Revised 7 January 2019, Accepted 9 April 2019, Available online 24 April 2019, Version of Record 6 May 2019.",https://doi.org/10.1016/j.jmateco.2019.04.004,Cited by (6),"This study analyzes the number of matches in stable and efficient matchings. The benchmark number of matches is the largest one among the matchings in which no agent can be better off by itself. We show that, in the one-to-one matching model, the number of matches in any stable matching is more than or equal to the smallest integer that is not less than half of the benchmark number. This result is satisfied even if “stable matching” is replaced by “efficient matching”. We extend the model to the many-to-one matching one and provide the sets of preference profiles in which each of the above results continues to hold.","We consider a matching model where the agents are divided into two groups. An agent in one group is called a ==== and that in the other group is called a ====. A worker is matched to (or employed by) one firm or unmatched (not employed by any firm) but a firm can be matched to (or employ) multiple workers. Of course, this model can be interpreted as, for example, a student–school or a doctor–hospital matching model, and so on. Many previous studies also consider this model. See, for example, Roth and Sotomayor (1990) for a survey.====We restrict our attention to the number of matches (or the number of employed). There are several reasons for increasing the number of matches in matching markets. First, external effects exist on the matchings. Many empirical studies observe the positive relationship between unemployment and crime, for example, Box (1987). Moreover, several empirical studies such as Lochner and Moretti (2004) show that education has a significant crime-reducing effect. Thus, matching a worker (or a student) to a firm (or a school) benefits not only the worker and the firm (or the student and the school) but also outsiders by decreasing crime. In addition, the doctor shortage in a hospital prevents patients living nearby from receiving good medical care. Second, wealth disparity between matched and unmatched individuals would be a serious issue in not only labor markets but also school–student matching markets. This is because the education level of individuals is significantly related to their income. Thus, decreasing the number of the unmatched is expected to reduce inequality in several matching markets.====In fact, several central matching authorities want to increase the number of matches in real matching markets. First, according to Abdulkadiroğlu et al. (2005), one of the reasons why the New York City Department of Education required a new matching system in 2003 is that many (approximately 30,000 per year) students were not assigned to any school that they had applied to in the old system. Second, in the matching system of Japanese medical residency, the Japanese government introduced a regional cap that restricts the total number of residents matched within the prefecture. This is because of a shortage of doctors in several regional areas. See Kamada and Kojima (2015) with regard to this restriction. Third, in Japan, over 20,000 children are waiting for admission into publicly certified nursery schools, which are schools or daycare centers for the preschool children whose parents cannot provide full-time care at home.==== ==== See, for instance, Okumura (2018), regarding this aspect. The Japanese government has announced several times recently that it is working on eliminating childcare waiting lists.==== ====We specifically focus on the number of employed in a stable matching and an efficient matching. We use the number of employed in a maximal individually rational matching as a benchmark. In a maximal individually rational matching, the number is the largest among those in which any agent cannot be better off by itself. We use this benchmark because it is difficult to enforce any matching where the number of employed is higher than this.==== ====Several recent studies focus on the number of matches. First, when weak preferences are allowed in a matching model, two different stable matchings may have a different number of matches even in a simple one-to-one matching model. Several previous studies such as Iwama et al. (1999), Irving and Manlove (2009), McDermid (2009), and Király (2011) discuss the method for finding a stable matching in which the number of matches is not less than those in any other ones. We consider a class where the preference profiles of the agents are assumed to be strict, but two different stable matchings may have different numbers of matches in the class.==== ====
 Biró et al. (2010) consider the problem of finding a maximum matching that admits the smallest number of blocking pairs in a one-to-one matching model. Note that any maximum matching that admits the smallest number of blocking pairs may not be a maximal individually rational matching. Arnosti (2016) discusses the factors that determine the number of (un)matched agents when the matching is determined by a stable matching mechanism and shows how the number is dependent on correlations among agent preferences.====Our main results are as follows. First, we restrict our attention to the set of preference profiles of the agents in which the model corresponds to a one-to-one matching model. In this class, we show that the number of employed in an efficient matching is not less than ==== of that of a benchmark matching. Furthermore, there exists a preference profile such that the number of employed in the stable matching is equal to the smallest integer that is more than or equal to half of that in a benchmark matching. Since a stable matching is efficient in this case, we obtain the same results even if “efficient matching” and “stable matching” are interchanged.====Second, we consider a general many-to-one matching model. The preference of each firm satisfies the law of aggregate demand. This property is originally introduced by Hatfield and Milgrom (2005). If the preference of each firm satisfies the law of aggregate demand and substitutability, the stability versions of the above results are satisfied. However, the efficiency versions of the results do not hold. Hence we provide an original property of the firm’s preference called general ====-separability, which is stronger than the law of aggregate demand. This property is a generalization of (quota) ====-separability introduced by Martínez et al. (2000). Then, the efficiency versions (as well as the stability versions) of the results are satisfied if the preferences of all firms satisfy general ==== -separability.====Finally, we provide some specific models to compare the difference between the expected values of the numbers of a stable matching and a benchmark matching in some context. We consider that the workers and firms are located at a Hotelling linear city and each worker incurs a transport cost that is linearly dependent on the distance between the worker and the firm.",On the number of employed in the matching model,https://www.sciencedirect.com/science/article/pii/S030440681930045X,24 April 2019,2019,Research Article,290.0
"Bommier Antoine,Kochov Asen,Le Grand François","ETH Zurich, Switzerland,University of Rochester, United States,emlyon business school, France","Received 14 December 2018, Accepted 2 April 2019, Available online 22 April 2019, Version of Record 4 May 2019.",https://doi.org/10.1016/j.jmateco.2019.04.001,Cited by (5),". The first delivers a novel axiomatization of endogenous discounting without restricting beliefs to be expected utility. Leveraging our analysis of ambiguity aversion, the second result delivers a maxmin representation of beliefs.","A recent paper by Kochov (2015) considered the implications of ambiguity aversion for intertemporal behavior. A key lesson is that an ambiguity averse agent would seek to take different, negatively correlated bets in different time periods. As Fig. 1 illustrates, doing so reduces the overall uncertainty faced by the agent as it implies that a bad outcome in some period ==== would be compensated by a good outcome in period ====. Combining such behavior, which Kochov (2015) called ====, with a notion of stationarity, which we call path stationarity and preview momentarily, Kochov (2015) axiomatized the following dynamic version of the maxmin model of Gilboa and Schmeidler (1989): ====where, as usual, ==== is a set of beliefs over the state space ====, representing the agent’s perception of ambiguity.====A limitation of Kochov’s analysis is that it depends critically on a third assumption which is evident from (1.1), namely, that the ranking of nonstochastic consumption streams be time separable. Without this auxiliary assumption, the behavior in Fig. 1 need no longer be indicative of ambiguity aversion. To see this, consider the Uzawa (1968) and Epstein (1983) model of endogenous discounting,==== ==== which relaxes time separability by allowing the rate of time preference to vary with the consumption path: ====It is known from Epstein (1983) that this model, which has found applications in the theory of optimal growth and the study of small open economies, exhibits intertemporal hedging if and only if ==== is a decreasing function of the consumption level ====. The latter property, known as ====, is assumed in virtually all applications of the model as it insures the stability and uniqueness of steady states.====The main goal of this paper is to disentangle the intertemporal implications of ambiguity aversion from those of endogenous discounting. We do so by focusing on a special kind of uncertainty that can arise in a dynamic setting. Imagine an agent expecting a tax refund. The agent knows the amount to be refunded and plans to consume it as soon as the refund arrives. The uncertainty is ==== the tax refund will arrive. We show that seeking an intertemporal hedge against this special type of uncertainty, ====, is indicative of ambiguity aversion whether discounting is exogenous as in (1.1) or endogenous as in (1.2). In particular, such behavior cannot be rationalized by the expected utility model in (1.2).====The analysis is supplemented by two representation theorems. The first one concerns a general class of preferences which includes the models in (1.1), (1.2) as special cases. The defining property of this class is an axiom which we call ==== and which is assumed in both Kochov (2015) and Epstein (1983).==== ==== The axiom extends Koopmans’ classical notion of stationarity to a setting of uncertainty by positing the following implication. Consider an event ==== resolving in period ==== and note that ==== may affect contemporaneous consumption as well as consumption in a more distant period ====. Path stationarity requires that the agent’s attitudes toward uncertainty do not depend on the date on which consumption takes place and, in particular, on ====. This restriction on behavior turns out to be remarkably powerful. Its first implication is that the utility of a non-stochastic consumption stream is computed as in (1.2): ====This implication sets the stage for our analysis of ambiguity aversion vis-à-vis endogenous discounting, which requires the existence of such a utility function. The second implication of path stationarity concerns the ranking of stochastic consumption streams. As the lifetime utility of any such stream could be random, the agent needs to assign an expectation ==== to each random variable ====. We show that the mapping ====, which we call the agent’s ====, must be translation-invariant and positively homogeneous, which means that for all ====, ====Certainty equivalents of this form are known as ==== and have been studied extensively in the context of static choice under ambiguity. See Ghirardato et al. (2004) and the references therein. In contrast, we characterize such certainty equivalents in terms of path stationarity, a property of intertemporal behavior.====Combining path stationarity with our notion of intertemporal hedging, our second result shows that the certainty equivalent ==== is also concave and, hence, takes the maxmin form ====Thus, we not only identify a more robust prediction of ambiguity aversion, we are also able to put this prediction to use and generalize Kochov’s (2015) characterization of the maxmin model.==== ",Ambiguity and endogenous discounting,https://www.sciencedirect.com/science/article/pii/S0304406819300424,22 April 2019,2019,Research Article,291.0
"Yu Meng,Zhang Junnan","Institute of Mathematics, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China,School of Mathematical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China,Research School of Economics, Australian National University, Australia","Received 27 November 2018, Revised 16 February 2019, Accepted 4 April 2019, Available online 18 April 2019, Version of Record 29 April 2019.",https://doi.org/10.1016/j.jmateco.2019.04.002,Cited by (1),"In this paper, we extend and improve the production chain model introduced by ","Over the past several centuries, firms have self-organized into ever more complex production networks, spanning both state and international boundaries, and constructing and delivering a vast range of manufactured goods and services. The structures of these networks help determine the efficiency (Levine, 2012, Ciccone, 2002) and resilience (Carvalho, 2007, Jones, 2011, Bigio and La’O, 2016, Acemoglu et al., 2012, Acemoglu et al., 2015a) of the entire economy, and also provide new insights into the directions of trade and financial policies (Baldwin and Venables, 2013, Acemoglu et al., 2015b).====We consider a production chain model introduced by Kikuchi et al. (2018) that examines the formation of such structures. They connect the literature on firm networks and network structure to the underlying theory of the firm. In particular, the model in Kikuchi et al. (2018) formalizes the foundational ideas on the theory of the firm presented in Coase (1937), embedding them in an equilibrium model with a continuum of price taking firms, and providing mathematical representations of the determinants of firm boundaries suggested by Coase (1937).====A single firm at the end of the production chain sells a final product to consumers. The firm can choose to produce the whole product by itself or subcontract a portion of it to possible multiple upstream partners, who then make similar choices until all the remaining production is completed. The main reason for firms to produce more in-house is to save the transaction costs of buying intermediate products from the market. In fact, Coase (1937) regards this as the primary force that brings firms into existence. An opposing force that limits the size of a firm is the costs of organizing production within the firm.==== ==== A price function governs the choices firms make and is determined endogenously in equilibrium when every firm in the production chain makes zero profit.====Considering that all firms are ex ante identical, a notable feature of this model is its ability to generate a production network with multiple layers of firms different in their sizes and numbers of upstream partners. The source of the heterogeneity lies solely in the transaction costs and firms’ different stages in the production chain. This feature provides insights into the formation of potentially more complex structures in a production network. Kikuchi et al. (2018) prove the existence, uniqueness, and global stability==== ==== of the equilibrium price function restricting every firm to have only one upstream partner. In this case, the resulting production network consists of a single chain.====There are however, several significant weaknesses with the analysis in Kikuchi et al. (2018). First, while they provide comprehensive results on uniqueness of equilibrium prices and convergence of successive approximations in the single upstream partner case, they fail to provide analogous results for the more interesting multiple upstream partner case, presumably due to technical difficulties. Second, their model cannot accurately reflect the data on observed production networks because their networks are always symmetric, with sub-networks at each layer being exact copies of one another. Real production networks do not exhibit this symmetry.==== ==== Third, they provide no effective algorithm for computing the equilibrium price function in the multiple upstream partner case.====This paper resolves all of the shortcomings listed above. As our first contribution, we extend their existence, uniqueness, and global stability results to the multiple partner case. To avoid the technical difficulties faced in their paper, we employ a different approach utilizing the theory of monotone concave operators, which enables us to give a unified proof for both cases.====Theoretically, the concave operator theory ensures the global stability of the fixed point, so the equilibrium price function can be computed by successive evaluations of the operator. In practice, however, the rates of convergence can be different for different model settings. This leads to unnecessarily long computation time in most cases. As a second contribution, we propose an algorithm that achieves fast computation regardless of parameterizations and is shown to drastically reduce computation time in our simulations.====A third contribution of this paper is that we generalize the model to a stochastic setting. In the original model, the equilibrium firm allocation is symmetric and deterministic: firms at the same stage of production choose the exact same number of upstream partners. In reality, each firm faces uncertainty in the contracting process and cannot always choose the optimal number of partners. We model the number of upstream partners as a Poisson distribution and let the firm choose its parameter, which can be seen as a search effort. Using the same approach, we prove the existence and uniqueness of equilibrium price function as well as the validity of the algorithm. We further use simulations to analyze how production and transaction costs determine the shape of a production network. This generalization provides a new source of heterogeneity in the equilibrium firm allocation and can be a potential channel for future research on size distribution of firms.====As briefly mentioned above, the method we use to establish the existence, uniqueness, and global stability of the equilibrium price function draws on the theory of concave operators. A competing method traditionally used for the same purpose is the Contraction Mapping Theorem, which has been an essential tool for economists dealing with various dynamic models ever since Bellman (1957). So long as the operator in question satisfies the contraction property, we can quickly compute a unique fixed point by applying the operator successively. This property, simple as it may be, is not shared among a number of important models, urging us to find new tools to tackle fixed point problems in economic dynamics.====The theory of monotone concave operators originally due to Krasnosel’skii (1964, Chapter 6) is another simple yet powerful tool. The idea behind it is intuitive: imagine an increasing and strictly concave real function ==== such that ==== and ==== with ====. Then it must be true that ==== has a unique fixed point on ====, and by the concavity of ====, the fixed point can be computed by successive evaluations of ==== on any ====. No contraction property is needed here while we still get all the results from the Contraction Mapping Theorem. A full-fledged theorem owing to Du (1989) for arbitrary Banach spaces is stated in Theorem 3.1. For similar treatments==== ==== in the math literature, also see Krasnosel’skii et al. (1972), Krasnosel’skii and Zabreĭko (1984), Guo and Lakshmikantham (1988), Guo et al. (2004), and Zhang (2013).====Apart from Theorem 3.1, there are other similar techniques that utilize concavity to show uniqueness==== ==== of the fixed point. Krasnosel’skii (1964) shows that a monotone operator on a positive cone has at most one nonzero fixed point if the operator satisfies a concavity condition (====-concave). For applications of this technique in the economic literature, see, for example, Lacker and Schreft (1991) and Becker and Rincón-Zapatero (2017). Following Krasnosel’skii (1964), Coleman (1991) proves uniqueness under slightly different concavity and monotonicity conditions (pseudo-concave and ====-monotone). See also Datta et al. (2002b), Datta et al. (2002a), and Morand and Reffett (2003) for other economic applications along this line.====Marinacci and Montrucchio, 2010, Marinacci and Montrucchio, 2017 link concavity to contraction in the Thompson metric (Thompson, 1963), which allows one to apply the Contraction Mapping Theorem to operators that are not contractive under the supnorm. In a similar vein, Marinacci and Montrucchio (2017) establish existence and uniqueness results for monotone operators under a range of weaker concavity conditions using Tarski-type fixed point theorems and the Thompson metric. Among all of these results, the theorem by Du (1989) turns out to be the most suitable for our work.====The monotone concave operator theory has seen some recent success in the economic literature. Lacker and Schreft (1991) study an economy with cash and trade credit as means of payment and show that the equilibrium interest rate is a unique fixed point of a monotone concave operator. Coleman, 1991, Coleman, 2000 studies the equilibrium in a production economy with income tax and proves the existence and uniqueness of consumption function by constructing a monotone concave map. Following this approach, Datta et al. (2002b) prove the existence and uniqueness of equilibrium in a large class of dynamic economies with capital and elastic labor supply. Similar work in the same vein includes Morand and Reffett (2003) and Datta et al. (2002a). Rincón-Zapatero and Rodríguez-Palmero (2003) exploit the monotonicity and convexity properties of the Bellman operator and give conditions for existence and uniqueness of fixed points in the case of unbounded returns. Balbus et al. (2013) study the existence and uniqueness of pure strategy Markovian equilibrium using theories concerning decreasing and “mixed concave” operators. More recently, this theory has been applied extensively to models with recursive utilities since Marinacci and Montrucchio (2010); other contributions==== ==== along this line include Balbus (2016), Borovička and Stachurski, 2017, Borovička and Stachurski, 2018, Becker and Rincón-Zapatero (2017), Marinacci and Montrucchio (2017), Pavoni et al. (2018), Bloise and Vailakis (2018), and Ren and Stachurski (2018).====Our work connects to this literature in that the operator which determines the equilibrium price is shown to be increasing and concave but does not satisfy any contraction property. To prove existence and uniqueness, Kikuchi et al. (2018) use an ad hoc and convoluted method for the case when every firm can only have one upstream partner but fail to generalize it to the multiple partner case. Using the monotone concave operator theory, we are able to extend their results and give a much simpler proof.====Section 2 describes the model in detail. Section 3 introduces the monotone concave operator theory and gives existence and uniqueness results. The algorithm is described in Section 4. Section 5 generalizes the model, allowing for stochastic choices of upstream partners. Section 6 concludes. All proofs can be found in the Appendix.",Equilibrium in production chains with multiple upstream partners,https://www.sciencedirect.com/science/article/pii/S0304406819300436,18 April 2019,2019,Research Article,292.0
Rota-Graziosi Grégoire,"Université Clermont Auvergne, CNRS, CERDI, F63000 Clermont-Ferrand, France","Received 19 June 2017, Revised 2 April 2019, Accepted 8 April 2019, Available online 18 April 2019, Version of Record 29 April 2019.",https://doi.org/10.1016/j.jmateco.2019.04.003,Cited by (6),", more precisely ====-concavity, and supermodular games, this paper offers a simple yet unifying perspective on the fundamental forces that shape tax competition. The main results characterize sufficient conditions on the marginal productivity of tax competing jurisdictions to predict a “race to the bottom.” These conditions bind the curvature of the demand for capital of each tax-competing jurisdiction. Quadratic production function respects these, while Cobb–Douglas form requires an additional condition. We deduce several results: at least one pure-strategy ","Is tax competition harmful? Can tax coordination be Pareto improving? These questions among others have been addressed in the literature of tax competition initially formalized by Zodrow and Mieszkowski (1986), Wilson (1986) or Wildasin (1988). One of the main conclusions of the literature reviewed by Keen and Konrad (2013) is that international tax competition would trigger a “race to the bottom”. In other words, the Nash equilibrium of the standard tax competition game would be characterized by too low tax rates and consequently an under-provision of public goods with respect to the social optimum. This result, which is widely held beyond the academic circle (OECD, 1998, OECD, 2013==== ====) is far from obvious to establish in a general framework with ==== asymmetric countries in interaction.====The “race to the bottom” (or “to the top”) may be viewed as the result of two properties of the tax competition game: a positive tax spillover and the strategic complementarity of tax rates. The first property means that any decrease (increase) in the tax rate of one country reduces (improves) the payoff of the other countries. The second property characterizes the similarity of countries’ reaction in any change in the tax rate of one of them: a decrease in one country would induce a similar reply from the other. In contrast, in presence of negative tax spillovers, any decrease in the tax rate of one country improves the payoff of the others. Tax competition would then have a positive impact on countries’ payoffs. Such a view is in line with the ==== school, which considers tax competition as a way to tame the ==== (see Brennan and Buchanan, 1980). If tax rates are strategic substitutes, any change of the tax rate in one country would imply an opposite reaction by the others and neither a “race to the bottom” nor a “race to the top” may take place, the need for some tax coordination becoming dubious.====On the empirical side, a significant number of works, reviewed in Leibrecht and Hochgatterer (2012), Devereux and Loretz (2013) or Costa-Font et al. (2014), focus on the existence of tax competition and its nature. A large body of this literature establishes the existence of positive slopes of the tax reaction function==== ==== or equivalently the strategic complementarity of tax rates. However, it is worth to remark that some recent analyses (Chirinko and Wilson, 2007; Parchet, 2014) display downward sloped reaction functions (respectively, among US states and Swiss municipalities) leaving the question of the nature of tax competition open for further empirical investigations. The degree of tax spillovers has been investigated in the study from the International Monetary Fund (IMF, 2014), which establishes positive tax spillovers based on panel data of corporate income tax for 103 countries for the period 1980–2013. This result may be also related to the literature on tax planning activities by multinational companies initiated by Hines and Rice (1994) and reviewed by Dharmapala (2014). These works appreciate the impact of tax rates differential between jurisdictions on income shifting between affiliates and parents of multinational companies. A decrease in the tax rate of one jurisdiction triggers some income shifting towards this jurisdiction, which corresponds to a positive tax spillover at the macroeconomic level.====Positive tax spillovers and strategic complementarities of tax rates are critical for a “race to the bottom”. These properties are often implicitly assumed in the literature or derived from the analytical specification of the production function used by the authors. We study here sufficient conditions to obtain plain and strategic complementarities of tax rates with general production function, which may differ among jurisdictions. If the first property is immediate in our framework where countries maximize their tax revenue, the second one is more delicate to establish. We follow the standard approach as originated by Zodrow and Mieszkowski (1986) and Wilson (1986) to apprehend tax competition: capital is perfectly mobile; its net return is then equal across countries. We consider ==== countries, which differ by their respective production function. Countries choose simultaneously their tax policy, here their respective tax rate, looking for maximizing their tax revenue under the constraint of the perfect mobility of capital. While Zodrow and Mieszkowski (1986), Wilson (1986), and Wildasin (1988) consider welfare maximizer, we restrict our analysis to tax revenue maximizer. Our approach is very simple given countries’ actual tax systems. However, first we can interpret tax rates as average effective tax rates, which encompass statutory tax rates, tax base’s definitions, and even tax law enforcement’s dimension. Second considering tax revenue maximization allows us to focus on the building block of the tax competition game and more broadly of fiscal competition. Establishing general sufficient conditions for the supermodularity of the tax competition game in the presence of welfare maximizers raises multiple issues: (i) the nature of public goods and their degree of substitutability or complementarity with respect to private consumption; (ii) the weight of capital owners in the welfare function, the distribution of capital, and the intra- and inter-jurisdiction redistributive impact of tax competition. Finally, our analysis may be viewed as a preliminary step in the understanding of tax systems’ competition by introducing a powerful tool: supermodularity to deal with the multidimensionality of tax systems (see Slemrod and Gillitzer 2014).====In the tax competition game, we study here, equilibrium tax rates are implicitly defined through the first order condition of the constrained maximization program of each country. The marginal production function plays a crucial role since it corresponds to the inverse demand for capital. And the curvature of the demand function is decisive for the supermodularity of tax competition as it is in oligopoly theory. Given that this demand function is not directly defined in our framework in contrast to oligopoly theory, we will have to use a concept of generalized concavity (see Avriel et al. 1988), more specifically this of ====-concavity introduced in economics by Aumann (1975). Then, we identify sufficient conditions for the supermodularity of the tax competition. Supermodular games, which have been mainly applied in industrial organization==== ==== display several nice properties: first, they encompass many analytical specifications, allowing appreciation of the robustness of the results; second, the existence of at least one pure-strategy Nash equilibrium is immediate, and many solution concepts yield the same prediction; finally, these games tend to be analytically appealing by significantly simplifying the analysis.==== ==== These three qualities are particularly relevant in the context of tax competition, where the formalization of the problem differs among authors and the existence of a Nash equilibrium remains an issue.====We establish that the ====-concavity of marginal production function with ==== (where ==== is the maximum tax rate) is a sufficient condition for the supermodularity of the tax competition game, when countries maximize their tax revenue and under standard assumptions on the production function. These conditions allow us to bind the curvature of the marginal production function. Quadratic production function respects all these conditions, while an additional condition relating the maximum tax rate to the lowest level of production for ==== (the total stock of capital) and the extreme values of the output elasticity is necessary for Cobb–Douglas production function. Our set of sufficient conditions may appear stringent, but the tax competition game brings into play two opposite effects resulting from any variation in the tax rate of one country on the other countries’ tax policy: (i) an increase in the tax rate of country ==== involves a decrease in the worldwide net return of capital, which allows the other countries to increase their own tax rate ====; (ii) this increase triggers also an outflow of capital from country ==== to the others, which improves the tax base of the latter, but decreases their marginal productivity and induces them to reduce their own tax rates.====Our analysis contributes to the literature in twofold. First, it establishes sufficient conditions for the strategic complementarity of tax rates. In this regard, it provides some theoretical backgrounds to the seminal works of Zodrow and Mieszkowski (1986) and Wilson (1986). In particular, it complements Laussel and Le Breton (1998), who address the issue of the existence and uniqueness of Nash equilibrium in tax competition.==== ==== We also establish a sufficient condition for the strategic complementarity of tax rates with Cobb–Douglas production functions. From the supermodularity of the tax competition game we deduce the existence and the uniqueness of a pure-strategy Nash equilibrium. Second, we apply several results of supermodular games with positive spillovers to tax competition. These results hold for any payoff function especially welfare function, which displays the two properties: the plain and strategic complementarity of tax rates. We distinguish tax coordination from tax cooperation. Following the literature on macroeconomic coordination failures, we consider that there is a tax coordination problem, when countries fail to coordinate on the Pareto dominant Nash equilibrium, while tax cooperation consists of reaching a Pareto superior outcome, which does not have to be a Nash equilibrium of the initial tax competition game. With these definitions and given the property of positive tax spillovers we deduce that: (i) tax coordination is unambiguously Pareto improving; (ii) the highest Nash equilibrium is coalition-proof.==== ==== These results hold with welfare maximizers or in other forms of tax competition (e.g., commodity or excise tax competition).==== ====Before concluding, we highlight the role of capital supply, which jeopardizes seriously the conventional view of a harmful tax competition even when countries are looking for maximizing tax revenue only. First, we relax the implicit assumption of an inelastic worldwide stock of capital by considering saving decisions. This induces a positive relationship between interest rate and the total stock of capital. The supermodularity of the tax competition game holds if the saving function is convex or if its concavity remains moderate. Second, capital ownership, its distribution, and its potential concentration in some countries lead us to consider a new kind of player in the tax competition game: offshore finance centers or tax havens. Characterized by a zero capital tax rate and no real economic activity,==== ==== tax havens are singular players displaying negative tax spillovers and eventually strategic substitutability of their tax rates. They may modify drastically the international tax competition, which is not supermodular anymore.====The paper is structured as follows: Section 2 is a preamble introducing some results regarding generalized concavity; Section 3 presents the tax competition game and sufficient conditions for its supermodularity; in Section 4 we deduce some consequences of the positive spillovers property and the supermodularity of the tax competition game in particular in terms of tax coordination and cooperation; Section 5 discusses the role of capital on the nature of tax competition; Section 6 concludes.",The supermodularity of the tax competition game,https://www.sciencedirect.com/science/article/pii/S0304406819300448,18 April 2019,2019,Research Article,293.0
"Mauleon Ana,Roehl Nils,Vannetelbosch Vincent","CEREC and CORE, UCLouvain, Belgium,Department of Economics, University of Paderborn, Paderborn, Germany","Received 28 May 2018, Revised 21 December 2018, Accepted 9 April 2019, Available online 17 April 2019, Version of Record 29 April 2019.",https://doi.org/10.1016/j.jmateco.2019.04.005,Cited by (2),"We study the stability of overlapping group structures where each group possesses a constitution that contains the rules governing both the composition of the group and the conditions needed to leave the group and/or to become a new member of the group. We provide a ==== on preferences that guarantees the existence and the emergence of constitutionally stable group structures. We show that although more blocking power for the individuals might enlarge the set of constitutionally stable group structures, it could happen that the society will never reach a stable group structure.","The understanding of how and why groups form and the precise way in which they affect outcomes of social and economic interactions has been apprehended assuming that each individual can only be a member of one of these groups.==== ==== However, there are many situations in which individuals might be a member of more than one group. Free trade agreements are signed among overlapping collections of countries. Joint ventures are formed among overlapping collections of firms. Overlapping groups of individuals may be involved in relationships involving public-goods provision, reciprocity or information-sharing.====Mauleon et al. (2018) provide a theoretical framework that introduces the notion of constitution to model for each group the rules governing both the composition of the group and the conditions needed to leave the group and/or to become a new member of the group. They propose the concept of constitutional stability to predict the group structures that are going to emerge at equilibrium when the deviating coalition has to take into account the constitution of the group it wants to modify. Mauleon et al. (2018) examine both the existence of constitutionally stable group structures as well as whether the society will reach one of these stable group structures. They provide requirements on constitutions and individuals’ preferences guaranteeing that, from every initial group structure, there always exists a sequence of feasible deviations that are not blocked leading to a constitutionally stable group structure.====In the present paper, we investigate two fundamental questions that were not answered in Mauleon et al. (2018). Does there exist an alternative way of guaranteeing stability without restricting both the constitutions and the individuals’ preferences? We show that the existence of a common ranking in the society guarantees that from every initial group structure a constitutionally stable group structure will be reached. In the spirit of Banerjee et al. (2001) and Farrell and Scotchmer (1988) the common ranking property guarantees the consensus of all players in the society with respect to the ranking of two subsequent group structures that result from the deviation of a feasible deviating coalition that has not been blocked.====Notice that the constitutions grant the group members a certain level of blocking power. That is, the members of each group might have certain property rights or degree of authority allowing them to inhibit changes in the composition of the group that do not conform to their own preferences. What happens in terms of stability if more blocking power is given to the individuals? We show that the relationship between higher blocking power and stability is not monotone and depends on the adopted perspective of stability. Although the set of constitutionally stable group structures might become larger, it could happen that the society will never reach one of these stable group structures because all sequences of feasible deviations that are not blocked do not necessarily lead to them.====Up to now, very little theoretical work exists on the stability for overlapping coalition formation. Myerson (1980) describes how the outcome of a cooperative game might depend on which groups of players hold cooperative planning conferences, and studies allocation rules, which are functions mapping conference structures to payoff allocations. Shenoy and Kraus (1996) and Dang et al. (2006) propose heuristic algorithms for overlapping coalition formation without addressing the question of the stability of overlapping coalitions. Conconi and Perroni (2002) introduce a cooperative game of multi-dimensional agreement formation and define the notion of stable agreement structure, a core-like equilibrium concept where subsets of players can put forward objections to a certain proposed configuration of agreements. Albizuri et al. (2006) propose an extension of the Owen’s coalitional value (1977) to an overlapping coalition formation setting. Allouch and Wooders (2008) analyze from a general equilibrium perspective club economies where clubs may overlap. Under the assumption that there is small communication cost of deviating from a given outcome, they prove that for all sufficiently large economies the core is nonempty and the set of price-taking equilibrium outcomes is equivalent to the core. Chalkiadakis et al. (2010) introduce a model for cooperative games with overlapping coalitions that is applicable in situations where agents need to allocate different parts of their resources to simultaneously serve different tasks as members of different coalitions. They explore the stability concept of the core. Our concept of constitutional stability generalizes previous stability concepts in the literature in which the rules governing the composition of each group as well as the exit of current members and/or the arrival of new members were exogenously given (see Jehiel and Scotchmer, 2001, Drèze and Greenberg, 1980, Caulier et al., 2013a, Caulier et al., 2013b) or not explicitly considered (like in the core where it is assumed that the deviators only form coalitions among themselves and, thus, no composition and/or admission rules are considered).====An important stream in the literature studies whether a decentralized process of successive blocking leads to a stable outcome in different models that can be seen as special cases of the present model where groups or coalitions do not overlap. See Roth and Vande Vate (1990), Klaus and Klijn (2007), Kojima and Unver (2008) and Herings et al. (2017) for two-sided matching problems; Chung (2000) and Diamantoudi et al. (2004) for roommate problems; Koczy and Lauwers (2004), Koczy (2006), Yang, 2010, Yang, 2011, Yang, 2012 and Béal et al. (2013) for cooperative games, and Diamantoudi and Xue (2003) for hedonic games. We extend these results on paths to stability to the framework of overlapping group structures, a difficult framework for which there were no general results regarding necessary and sufficient conditions guaranteeing the convergence of the society to a constitutionally stable group structure.====The paper is organized as follows. Section 2 introduces the framework of overlapping groups and the notion of constitutions. Section 3 provides an alternative criterion for guaranteeing convergence to a constitutionally stable group structure and studies the relationship between blocking power and stability. Finally, Section 4 concludes.",Paths to stability for overlapping group structures,https://www.sciencedirect.com/science/article/pii/S0304406819300461,17 April 2019,2019,Research Article,294.0
Afacan Mustafa Og̃uz,"Faculty of Arts and Social Sciences, Sabancı University, Orhanli, Tuzla, 34956, Istanbul, Turkey","Received 22 May 2018, Revised 1 February 2019, Accepted 25 March 2019, Available online 16 April 2019, Version of Record 22 April 2019.",https://doi.org/10.1016/j.jmateco.2019.03.004,Cited by (1),"We consider a task-allocation problem in which agents differ in terms of their seniority and their experience with tasks. We introduce two mechanism classes: the feasibility augmented serial dictatorship (====) and the minimally reluctant efficient priority (====). The first class is efficient, senior-optimal, and strategy-proof. However, a disadvantage of this class is that a greater number of agents can be assigned to tasks that they unwillingly perform – we call such tasks “unwillingly acceptable” – than what is actually achievable. We say that a mechanism is minimally reluctant if it always minimizes the number of agents who are matched with their unwillingly acceptable tasks. The second mechanism class is minimally reluctant, efficient, and constrained senior-optimal — senior-optimal in the class of minimally reluctant mechanisms. No minimally reluctant mechanism is strategy-proof, which implies that no ==== mechanism is strategy-proof. Each ==== mechanism has a unique equilibrium outcome that is equivalent to the truthtelling outcome of a particular ==== mechanism. Hence, in equilibrium, each ==== mechanism is efficient, senior-optimal, but not minimally reluctant. Nevertheless, no mechanism is minimally reluctant in equilibrium either.","Let us consider the problem of allocating service duties among faculty members. Each departmental (or university-wise) service duty is a task to be assigned to faculty members (agents). Faculty members may differ in terms of their seniority and their experience with tasks. More explicitly, each of them is either senior or junior, and is either experienced or inexperienced in a task. A faculty member can be experienced in a task, but inexperienced in another task. They have preferences over tasks. The department chair or the dean (planner) wants to assign a certain number of experienced and inexperienced faculty members to each task, with these numbers possibly changing across different tasks.==== ==== There are some other real-life problems having these features, such as shift allocations among employees,==== ==== and various task allocations in social and student clubs.====We introduce a new task-allocation model, mimicking the above type of problems. A matching is an assignment of tasks among agents such that each agent receives exactly one task, and each task is assigned to as many experienced and inexperienced agents as its quotas. To guarantee the existence of a matching, we impose certain suppositions on the number of agents.====A mechanism is a systematic procedure that produces a matching in any problem. We introduce two classes of mechanisms, admitting various desirable properties. We refer to the first as “====” (====). Starting with the senior agents, the mechanism first orders the agents. By following this order, one by one, each agent is assigned to his favorite task with an available slot, subject to the condition that the remaining task quotas can be filled with the later agents in the ordering. Each ==== mechanism is associated with a different agent ordering, and it is ==== ==== and ==== in the sense that its outcome is never unanimously less preferred to another matching by the seniors. Moreover, each ==== mechanism is ====.==== ====In the current context, we do not allow any agent to decline a task. However, agents can find some tasks ==== and the others ====. Once we consider this, a natural desideratum can be minimizing the number of agents assigned to their unwillingly acceptable tasks. We say that a matching is ==== if no other matching assigns fewer agents to their unwillingly acceptable tasks than the former does.====An immediate observation is that no ==== mechanism is minimally reluctant. Given this, we offer our second mechanism design, which comprises two stages. The first stage, which is reminiscent of the priority mechanisms of Roth et al. (2005), finds a set of minimally reluctant matchings. The second stage then lexicographically picks the best matching from this set by following a certain agent ordering, with the seniors considered first. This procedure defines a class of mechanisms, each associated with a different agent ordering. We refer to this as the class of “minimally reluctant efficient priority” (====) mechanisms. Each ==== mechanism is minimally reluctant, efficient, and ==== in that its outcome is never unanimously less preferred to another minimally reluctant matching by the seniors.==== ==== However, no ==== mechanism is strategy-proof. This is, nevertheless, not a problem specific to ==== because there is a general incompatibility between minimal reluctance and strategy-proofness: no minimally reluctant mechanism is strategy-proof.====Given the lack of strategy-proofness of ====, we study its equilibrium properties. In any problem, every ==== mechanism admits a unique equilibrium outcome, and it coincides with the (dominant strategy) truthtelling outcome of the ==== mechanism of which the agent ordering is the same as that of the ==== mechanism. This shows that, in equilibrium, any ==== mechanism is efficient and senior-optimal, but not minimally reluctant. However, we also obtain that no mechanism is minimally reluctant in equilibrium.",A task-allocation problem,https://www.sciencedirect.com/science/article/pii/S0304406819300394,May 2019,2019,Research Article,295.0
"Kivinen Steven,Tumennasan Norovsambuu","International College of Economics and Finance, National Research University Higher School of Economics, Moscow, Russia,Department of Economics, Dalhousie University, 6406 University Ave, Halifax, B3H 4R2, Canada","Received 6 July 2017, Revised 30 January 2019, Accepted 28 March 2019, Available online 11 April 2019, Version of Record 29 April 2019.",https://doi.org/10.1016/j.jmateco.2019.03.006,Cited by (2),"We analyze the convergence of opinions or beliefs in a general ==== with non-Bayesian agents. We provide a new sufficient condition under which opinions converge to consensus and the condition is significantly more permissive than that of Lorenz (2005). This condition, which depends on properties of the network, requires agents to incorporate others’ opinions into their own posterior sufficiently often.","Social networks are an important source of information for individuals and firms. The emergence of social media has led to an unprecedented level of information sharing among “friends”, i.e. those who are connected and communicate. Given this, should one expect people to agree in the long run? We provide a new sufficient condition under which non-Bayesian agents in a given network converge to consensus.====In our model, agents update their opinions based on the prior opinions of their friends (and potentially themselves). Though we focus on opinions, the model can accommodate any variable on a convex set, where the convex hull of initial values is compact. For instance, instead of an opinion – a subjective probability – agents may update their belief about the value of an unknown parameter or adopt a cultural norm.====Literature on non-Bayesian learning beginning with DeGroot (1974) has agents updating their beliefs to a weighted average of their friends’ beliefs. Lorenz (2005) provides a generalization of the DeGroot model by allowing the weights to depend on time and prior beliefs. The level of generality allows for many types of updating behavior, including those that exhibit optimism or pessimism (over-weighting or under-weighting), and cognitive dissonance (giving a higher weight to those with similar beliefs). He demonstrates that aperiodic and strongly connected networks reach agreement if the weight one gives to a friend’s opinion is bounded away from 0 by a positive number.==== ==== We provide a more permissive sufficient condition than that of Lorenz (2005). Roughly speaking, our result says that consensus is achieved unless some agents rely with an increasingly “faster” rate on their friends with the minimal opinion while some others on those with the maximal opinion.====DeMarzo et al. (2003) consider a time-varying social network in which the agents weigh themselves differently over time. They show that opinions converge when agents weigh other people’s opinions “often enough”. Our result is related to DeMarzo et al.’s (2003), and the two are equivalent for complete networks. In non-complete networks our condition is more restrictive. However, our condition is applicable in a wide range of environments while DeMarzo et al.’s (2003) condition is not applicable outside of their specific model.====Mueller-Frank (2013) considers a general class of time-varying updating rules that includes rules with belief-dependent weights. The main conditions for convergence to consensus are (i) updating rules must satisfy continuity and have posteriors be strictly in between the most extreme priors in one’s neighborhood and (ii) the periodwise updating functions must be of finite type. Our result does not require updating rules to be continuous or be of finite type. The assumption of continuity is especially strong in environments with endogenous network formation (Kivinen, 2017).====Several results on Bayesian updating in groups (Aumann, 1976, Geanakoplos and Polemarchakis, 1982) highlight the role of common knowledge and common priors in generating consensus. When agents communicate in a network, common knowledge of the network structure is also required (Mueller-Frank, 2014). There is a subtle difference between the models on Bayesian and non-Bayesian updating. In the former, the agents have priors regarding some parameter as well as private information. Based on the agents’ observed actions (which could involve revealing one’s posteriors), each updates one’s own prior. Here, the consensus occurs if the private information becomes “public” as time progresses. The focus is on whether agents eventually agree, and whether they learn the underlying data generating process.====The common knowledge assumption is demanding, and the agents require a powerful calculating ability to properly tease out the sources of information. In models on non-Bayesian updating the agents reveal their prior to each other, leading to an update. Private information spreads through the network but information may not be aggregated perfectly due to the lack of rationality on the agents’ part. Thus, consensus may be reached but the outcome is not necessarily the same as if the private information was pooled. Molavi et al. (2018) consider “quasi-Bayesian” learning which we consider in Section 4.====The paper is structured as follows. Next we introduce preliminary concepts and results. Section 3 contains our main results. We conclude with a discussion, which includes examples and additional results. Proofs are found in the Appendix.",Consensus in social networks: Revisited,https://www.sciencedirect.com/science/article/pii/S0304406819300412,11 April 2019,2019,Research Article,296.0
"Chen Ning,Li Mengling","Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore,Department of Economics, School of Economics and Wang Yanan Institute for Studies in Economics (WISE), Xiamen University, China","Received 1 August 2018, Revised 5 January 2019, Accepted 25 March 2019, Available online 5 April 2019, Version of Record 17 April 2019.",https://doi.org/10.1016/j.jmateco.2019.03.005,Cited by (15),"This paper studies the two-sided matching problem with multi-unit capacities and weak preferences on both sides. The simultaneous presence of these two features complicates the problem greatly, because either can make a stable matching not necessarily Pareto efficient. To ensure both fairness and efficiency, a natural solution is Pareto stability, which requires both pairwise stability and ====. We introduce a computationally efficient algorithm to construct a Pareto stable matching. This result immediately implies the existence of a Pareto stable matching in the general many-to-many matching problem with weak preferences.","Many economic situations can be modeled as ==== many-to-many matching, with two disjoint sets of agents to be matched with each other. Each agent has a multi-unit capacity and a (strict or possibly weak) preference ranking over the potential partners on the other side. Practical examples of many-to-many matching markets are pervasive, including peer-to-peer social lending (Chen and Ghosh, 2011), matching part-time jobs and workers,==== ==== assigning scientific resources to scientists (Wulf, 1993), assigning players to sports teams (Albergotti, 2010), and assigning university courses to students (Sönmez and Ünver, 2010).==== ==== Compared to the well-studied one-to-one and many-to-one matching problems, the generalization to multi-unit capacities on both sides of the market is nontrivial, because many nice prosperities of a stable matching cannot be easily extended to a many-to-many setting. One remarkable result is that a stable many-to-many matching may ==== be Pareto efficient (Roth and Sotomayor (1990), see Example 5.24).====In this paper, we study the general two-sided many-to-many matching problem under ==== preferences on ==== sides of the market. Most of the two-sided (one-to-one, many-to-one, or many-to-many) matching literature has assumed ==== preferences of agents, which helps to maintain many nice properties of a stable matching. However, in many practical matching markets, agents are not able to strictly rank their prospective partners for a variety of reasons. For instance, it may be costly or even unrealistic to have strict preferences when the size of the other side of the market is extremely large. In other cases, some alternatives might be identical, for example, the entry level posts in a firm. The widespread notion of weak preferences may even be implicit sometimes such as indifferences over slots at the same branch in the context of United States cadet-branch matching (Kominers and Sönmez, 2016) and over slots in the same school (Dur et al., 2018). Therefore, it is important to take such “details” of the market, i.e., weak preferences, into consideration when designing many-to-many matching market mechanisms.==== ==== The presence of weak preferences dramatically changes the properties of stable matchings and consequently leads to a series of negative results. A notable result receiving attention in the recent literature is that, in the presence of weak preferences, stability in a two-sided matching ==== guarantees Pareto efficiency (Sotomayor, 2011, Erdil and Ergin, 2017).====We start by examining the desirable properties in two-sided many-to-many matching with weak preferences. The simultaneous presence of the multi-unit capacities and weak preferences on both sides complicates this problem greatly, because either can result in Pareto ====efficiency of a pairwise stable matching. Pairwise stability as a well-established solution concept in two-sided matching literature captures fairness by taking preferences of both sides into account. A desirable matching outcome should also be Pareto efficient, which qualifies the overall efficiency of a matching and is a minimal requirement of efficiency in many classical problems. To ensure both fairness and efficiency in many-to-many matching with weak preferences, a natural solution concept is ====, which requires both pairwise stability and Pareto efficiency (Sotomayor, 2011).====Our main objective is to construct a systematic procedure to find a Pareto stable many-to-many matching with weak preferences. The model setup in this paper generalizes the many-to-one matching framework with weak preferences in Erdil and Ergin (2017). They propose the Efficient and Stable Matching Algorithm (ESMA), which starts from any stable matching and eliminates Pareto improving paths and cycles in a stability-preserving manner. We show that in many-to-many matching when only one side has weak preferences, the stability preserving result in Erdil and Ergin (2017) still holds (see Theorem 3.1), and hence the ESMA works. However, when both sides have weak preferences and multi-unit capacities, we have an important observation that a Pareto improvement, or equivalently, an elimination of Pareto improving path or cycle, of a stable matching may ==== preserve its stability (see Theorem 3.2). In fact, we show that even in a smaller domain with one-sided homogeneous preferences, the negative result remains. This suggests that the ESMA cannot be simply extended to the general many-to-many problems with weak preferences, nor a smaller subset of problems with one-sided homogeneous preferences. Based on the idea of Roth and Vande Vate (1990), we construct a new Pareto stable matching algorithm, which immediately implies the existence of such a matching (see Theorem 4.1). Our algorithm improves the results of Erdil and Ergin (2017) for two-sided many-to-one matching with indifferences, and is fundamentally different from their algorithm ESMA. The proposed algorithm is also computationally efficient to be applied in the real-life centralized many-to-many matching markets with weak preferences.====The remaining of this paper is organized as follows. We briefly review most related literature and highlight our main contributions in the rest of this section. Section 2 describes the many-to-many matching model with weak preferences and the solution concepts. Section 3 discusses the problems of applying previous algorithms to our model. A new Pareto stable matching algorithm is presented in Section 4. We provide some further discussions of our results in Section 5. Section 6 concludes.",Pareto stability in two-sided many-to-many matching with weak preferences,https://www.sciencedirect.com/science/article/pii/S0304406819300400,May 2019,2019,Research Article,297.0
"Dur Umut Mert,Wiseman Thomas","North Carolina State University, 2801 Founders Drive 4102 Nelson Hall, Raleigh, NC, 27695, United States,University of Texas at Austin, Department of Economics, 2225 Speedway, Austin, TX, 78712, United States","Received 23 January 2018, Revised 5 December 2018, Accepted 7 December 2018, Available online 26 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jmateco.2018.12.001,Cited by (5),"We consider the school choice problem where students who live near each other may prefer to be assigned to the same school. Even this very mild form of externality means that stable matchings may not exist, and that the student-proposing deferred acceptance mechanism may yield undesirable results — it is neither stable nor strategy-proof. We modify the school-proposing deferred acceptance mechanism to improve its performance. Our setting has important differences from both matching with couples and matching with preferences over colleagues.","In this paper, we consider the school choice problem where students who live near each other may prefer to attend the same school. In this setting, stable matchings may fail to exist, and the student-proposing deferred acceptance mechanism (Gale and Shapley, 1962, Abdulkadi̇roğlu and Sönmez, 2003) loses some of its appealing qualities. We present a modified school-proposing deferred acceptance mechanism that performs better.====Students and their families may be better off, all else equal, if neighboring children of the same age attend the same school.==== ==== In his 2012 State of the City address, Boston Mayor Thomas Menino said, ====The interpretation of this preference structure is that a student (or the student’s family) has preferences over his neighbor’s school only because he may derive benefits from attending the same school, and those benefits are not uniform across schools. For example, neighbors attending a very large school might have only a small chance of sharing teachers or homework assignments. For the same reason, a student may rank schools differently if he is attending them on his own than if he is attending with his neighbor.====We show that even this very mild form of externality has important consequences.====  Starting from the seminal paper by Abdulkadi̇roğlu and Sönmez (2003), the literature has examined different methods for assigning students to public schools that have limited capacity and that therefore rank potential students according to fixed priorities for admission.====  Desirable features of an assignment method are Pareto efficiency (with respect to students’ preferences), stability (the property that students cannot claim seats at a school from students who have higher priority), and strategy-proofness (so that students cannot gain from misrepresenting their preferences over schools). Balinski and Sönmez (1999) show that no method has all three properties because efficiency and stability are incompatible. Abdulkadi̇roğlu and Sönmez (2003) propose the student-proposing deferred acceptance (DA) mechanism of Gale and Shapley (1962) for use in the school choice problem. For the case where all students are singletons (that is, they do not have preferences over the assignments of other students), that mechanism is both stable and strategy-proof.====In the school choice problem with neighbors, however, we find that a stable matching may not exist.====  It follows that the student-proposing DA mechanism is not stable in this setting, and in fact it is not strategy-proof either. More generally, there does not exist a strategy-proof mechanism that selects a stable matching whenever it exists.====Instead, we modify the school-proposing DA mechanism to improve its stability properties, although it still fails to be strategy-proof. In particular, we show that under the modified school-proposing DA mechanism, the resulting matching is ==== in the following sense: no one student prefers to move to a school where he has priority over an already-assigned student (or where there is an unfilled seat). There may, however, exist a pair of neighbors who would benefit from making such a move together.====Technically, the feature that a student has preferences over his neighbor’s school only if they are both at the same school means that a matching mechanism should treat a pair of neighbors neither as two separate agents (as in the standard school choice problem) nor as a single agent (as in the couples problem), but as ==== agents: as a pair when their interests are aligned, and as separate agents otherwise. Specifying what priority a school assigns to the pair involves an important trade-off. In the modified school-proposing DA mechanism, we give a pair the same priority as its lower-ranked member. That rule ensures that no singleton student’s priority will be violated by a pair of neighbors. Moreover, it totally eliminates the justified envy of any “individual” student. The rule may, however, give neighbors the incentive to falsely report their preferences, since they have higher priorities as individual students. On the other hand, giving a pair the same priority as its ====-ranked member can make truthful reporting optimal, but it does not eliminate the justified envy of individual students. In fact, no strategy-proof mechanism can eliminate justified envy of individual students.",School choice with neighbors,https://www.sciencedirect.com/science/article/pii/S0304406818301356,26 March 2019,2019,Research Article,298.0
Kunimoto Takashi,"School of Economics, Singapore Management University, 90 Stamford Road, Singapore 178903, Singapore","Received 5 October 2014, Revised 14 March 2019, Accepted 15 March 2019, Available online 26 March 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.jmateco.2019.03.003,Cited by (2)," outcomes coincides with that specified by the rule. The objective of this paper is to generalize the results of mixed Bayesian implementation. By means of example, I first assess the implication of common priors in Bayesian implementation. Second, I identify a mild condition that fills the gap between the necessity and sufficiency for mixed Bayesian implementation in general environments including non-economic ones. Third, I establish some new results to unify the literature of Bayesian implementation and Nash implementation.","The theory of implementation or mechanism design attempts to identify the conditions under which a social choice rule (or welfare criterion) may be decentralized through some institution (or mechanism). In contexts in which the planner knows what agents’ preferences and/or beliefs (henceforth, I call them ==== collectively) might be, but does not know what they actually are, the theory has uncovered necessary and sufficient conditions for such decentralization.==== ====We say that a social choice rule is ==== by some mechanism if the mechanism possesses a Bayesian Nash equilibrium whose outcome is contained in that specified by the rule. We often appeal to the ====, which says that whenever partial implementation is possible, one can always duplicate the same equilibrium outcome by using the ==== equilibrium in the ==== mechanism where each agent announces his type to the planner. Thus, a necessary condition for the implementation of any welfare criterion is its ====: the best thing for each individual to do in the direct revelation mechanism is to report his true type as long as all other individuals truthfully announce their type.====Although the revelation principle has been a powerful tool in many applications, it is important to realize that the direct-revelation mechanism may possess other ==== equilibria whose outcomes are not consistent with the welfare criterion. In order to take seriously the problems resulting from the multiplicity of equilibria, some researchers have turned to the question of ====, and explored the conditions under which the ==== of Bayesian Nash equilibrium outcomes coincides with a given welfare criterion. In the case of full implementation, ==== emerges, in addition to incentive compatibility. Indeed, full implementation is the concept of implementation this paper adopts.====The main objective of this paper is to generalize the results of full Bayesian implementation established in Jackson (1991) and Serrano and Vohra (2010, henceforth, SV,). Theorem 1 of Jackson (1991) shows that under the ====, which basically says that at least two agents can never be satiated, a social choice rule satisfies incentive compatibility, Bayesian monotonicity, and closure (to be defined in Section 5) if and only if it is fully ====. Jackson’s Theorem 1 restricts attention to the setup where (1) each agent only uses pure strategies; (2) mechanisms are deterministic; and (3) the type space is finite. SV (2010) extend this result to the setup where the agents can use mixed strategies; mechanisms are stochastic; and the type space is quite general. More specifically, Theorem 1 of SV (2010) shows that in economic environments, mixed Bayesian implementation is equivalent to incentive compatibility, closure, and ==== Bayesian monotonicity, which is a strengthening of Bayesian monotonicity. The main contribution of this paper is to further generalize Theorem 1 of SV (2010) by dropping the economic condition. I consider this as a major addition to the literature because outside of economic environments, tight characterizations are generally not available, even for pure strategy equilibria. Note that Theorem 2 of Jackson (1991) proposes a sufficient condition for (pure) Bayesian implementation in “non-economic” environments. The table below depicts where the contribution of this paper lies in the literature:====The main result of this paper is to identify a mild condition under which one can fully characterize mixed Bayesian implementation in general environments including non-economic ones. More formally:====The no-worst-rule condition (NWR) is considered a version of ==== condition, which basically says that the environment is rich enough so that every type never be indifferent over the outcomes. I will later argue that NWR is a mild condition and illustrate its permissiveness by means of an example (Example 2). Another aspect of the contribution of this paper is to propose a unified treatment among different setups: I handle (i) the case of two agents (Theorem 2), (ii) the case of “single-valued” social choice rules (Theorem 3), and (iii) the case of ====, which describes the situation in which the underlying state is always commonly known among the agents. In complete information environments, I identify a condition much weaker than NWR under which one can fully characterize ==== in general environments including non-economic ones (Theorem 4).==== ====I move on to the last aspect of the generalization this paper executes. In most of applications in mechanism design and implementation theory, the researchers invoke the ==== assumption, which requires that all agents share the common belief about the state at the ex ante stage. I therefore investigate the implication of common priors in mixed Bayesian implementation. By Example 1, I confirm that if the common prior assumption is violated, Bayesian Nash equilibrium loses its predictive power quite a lot. In Section 6, building upon an example of Palfrey and Srivastava (1989b), I argue that mixed Bayesian monotonicity can be more permissive when the common prior assumption is violated than when it is satisfied.====The rest of the paper is organized as follows. Section 2 clarifies the scope of the current paper in the literature. In Section 3, I introduce the general setup for the paper. In Section 4, I introduce the concept of mixed Bayesian implementation. In Section 5, I identify the necessary conditions for implementation. In Section 6, I illustrate all the necessary conditions for implementation by means of an example. Section 7 provides a set of sufficient conditions for mixed Bayesian implementation. In Section 8, I restrict attention to complete information environments and obtain the sufficiency result for Nash implementation. Section 9 concludes. In the Appendix, I provide all the proofs omitted from the main body of the paper and discuss how one can extend this paper’s analysis to a more general setup.",Mixed Bayesian implementation in general environments,https://www.sciencedirect.com/science/article/pii/S0304406819300382,May 2019,2019,Research Article,299.0
"Baisa Brian,Burkett Justin","Amherst College, Department of Economics, USA,Georgia Institute of Technology, School of Economics, USA","Received 27 February 2018, Revised 14 February 2019, Accepted 4 March 2019, Available online 26 March 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.jmateco.2019.03.001,Cited by (3),"We study efficient auction design for a single indivisible object when bidders have interdependent values and non-quasilinear preferences. Instead of quasilinearity, we assume only that bidders have positive wealth effects. Our setting nests cases where bidders are ex ante asymmetric, face financial constraints, are risk averse, and/or face ensuing risk. We give ==== for the existence of an ex post implementable and (ex post Pareto) efficient mechanism. These conditions differ between the standard case where the auctioneer is a seller and when the auctioneer is a buyer (a procurement auction).====When the auctioneer is a seller, there is an efficient ex post implementable mechanism if there is an efficient ex post implementable mechanism in a corresponding quasilinear setting. This result extends established results on efficient ex post equilibria of English auctions with quasilinearity to our non-quasilinear setting. Yet, in the procurement setting there is no mechanism that has an efficient ex post equilibrium if the level of interdependence between bidders is sufficiently strong. This result holds even if bidder costs satisfy standard single crossing conditions that are sufficient for efficient ex post implementation in the quasilinear setting.","Efficient auction design is a central question in mechanism design. In the private value single unit quasilinear benchmark case, the English auction has an efficient dominant strategy equilibrium. More recent research gives necessary and sufficient conditions for the English auction to have an efficient ex post equilibrium when bidders have interdependent values. Thus, there are well-understood settings where the English auction’s efficient equilibrium is robust to asymmetries across bidders’ beliefs and higher order beliefs.====While these notable results on English auctions show that it is robust to asymmetries in bidder beliefs, these results require the strong assumption that bidder preferences are quasilinear. In many auction settings, bidders do not have quasilinear preferences, and violations of quasilinearity are frequently reported. For example, Maskin (2000) argues that financial market imperfections may result in liquidity-constrained bidders. Salant (1997) draws on his personal consulting experience to argue that financial constraints are a salient feature of how bidders determine their bids. In addition to access to credit, risk aversion and wealth effects are important features of auctions for larger items like houses.==== ====In this paper, we study the efficient auction design problem for a single indivisible unit when bidders have interdependent values. We remove the quasilinearity restriction on bidder preferences, and assume only that their preferences exhibit positive wealth effects. Our setting is sufficiently general to allow for asymmetric bidders who are risk averse, have financing constraints, have budgets, or face ensuing risk. Like much of the related literature on efficient auction design, we study auctions that are ex post implementable. Thus, our predictions are robust to asymmetries in bidders beliefs and higher order beliefs. Our main contribution is to provide conditions under which existing results from the literature on efficient auction design with quasilinearity can be extended to study efficient design on a more general preference domain. Interestingly, we show that the necessary and sufficient conditions for the existence of an efficient and ex post implementable auction differ depending on whether the auctioneer is a buyer or a seller.====Removing quasilinearity complicates the efficient auction design problem. With quasilinearity, an auction outcome is Pareto efficient if and only if the bidder with the highest value (or lowest cost) wins. Hence, the space of efficient allocations is independent of bidder transfers. Yet, in our non-quasilinear setting, the presence of wealth effects impliesthat a bidder’s demand for a unit following the auction depends on the amount they paid (or were paid) in the auction. With wealth effects it is possible that the bidder with the highest willingness to pay wins the auction and that there are Pareto improving trades between bidders following the auction. Thus, the space of (ex post Pareto) efficient outcomes depends on both the allocation of the object and bidder transfers. While the space of efficient outcomes is qualitatively different without quasilinearity, our first result (Theorem 1) shows that we are able to extend results describing efficient and ex post implementable auctions for bidders with quasilinear preferences to our non-quasilinear setting when the auctioneer is a seller. The theorem states that there is an auction with an efficient ex post equilibrium if there is an auction with an efficient ex post equilibrium in a corresponding quasilinear setting, in which each bidder’s valuation is equal to her willingness to pay in the non-quasilinear setting.====The existence of an efficient and ex post implementable auction in a corresponding quasilinear setting is a sufficient condition for the existence of such a mechanism in our non-quasilinear setting because positive wealth effects amplify the efficiency of the mechanism relative to the corresponding quasilinear setting. With positive wealth effects the winning bidder feels wealthier when she wins the good and pays a price that is below her willingness to pay. The increase in the winner’s perceived wealth increases her willingness to sell the good relative to her willingness to pay because we assume bidders have positive wealth effects. Thus, the winner is relatively less inclined to trade with her rivals. We use this observation to show that an auction outcome that is efficient in the corresponding quasilinear setting is also efficient in our non-quasilinear setting. A corollary of Theorem 1 is that the English auction has an efficient ex post equilibrium when the auctioneer is a seller if bidders’ willingnesses to pay satisfy the crossing conditions established by Maskin (1992), Krishna (2003), or Birulin and Izmalkov (2011).====However, the implications of Theorem 1 do not extend to procurement settings. When bidders are competing to sell a good to the auctioneer, positive wealth effects make the winning bidder more inclined to trade with her rivals because she is paid an amount in the auction that exceeds her reservation cost of supplying a unit. This makes the winning bidder feel wealthier and the increase in the winner’s wealth increases her demand for a unit relative to her losing rivals. The winning bidder is therefore more inclined to trade with her rivals following the auction. In fact, we show that there are cases where the English auction has an ex post equilibrium in which the bidder with the lowest reservation cost supplies the auctioneer with the good, but the auction outcome is inefficient. The outcome is inefficient because the auctioneer’s payment to the winner increases her demand for repurchasing good to such an extent that an ex post Pareto improving trade is created between the auction winner and one of the losers. Theorem 2 formalizes this result for the case where bidders have strictly positive wealth effects. The theorem shows that there is no procurement auction that retains the English auction’s desired incentive and efficiency properties – ex post implementability and Pareto efficiency – when the degree of interdependence in bidder preferences is sufficiently strong.",Efficient ex post implementable auctions and English auctions for bidders with non-quasilinear preferences,https://www.sciencedirect.com/science/article/pii/S0304406819300369,May 2019,2019,Research Article,300.0
"Manelli Alejandro M.,Vincent Daniel R.","Department of Economics, W.P. Carey School of Business, Arizona State University, Tempe, AZ 85287, United States,Department of Economics, University of Maryland, College Park, MD 20742, United States","Received 5 March 2019, Accepted 10 March 2019, Available online 19 March 2019, Version of Record 28 March 2019.",https://doi.org/10.1016/j.jmateco.2019.03.002,Cited by (0),"When a single-object is to be traded, ","Consider the assignment of ==== distinct and indivisible objects to ==== agents in a multi-dimensional version of the standard, symmetric, independent private-values model: Each agent observes, as private information, an ====-dimensional vector representing the agent’s monetary valuations for the various objects. Agents’ valuation for any set of objects is the sum of the valuations of each object in the set. Valuation vectors are independent and identically distributed across agents. Money enters utility functions linearly and agents are risk neutral. Within this environment, we investigate when a given Bayesian incentive compatible mechanism (BIC) has an equivalent dominant-strategy incentive compatible (DSIC) mechanism.====We employ a notion of equivalence based on payoffs, not on allocations. Two mechanisms are equivalent if each agent receives the same interim utility in both mechanisms—i.e. the agent receives the same expected payoff given the agent’s true valuation and assuming by way of equilibrium analysis that opponents report their valuations truthfully. (We introduced this notion of equivalence to trading mechanisms in Manelli and Vincent (2010). Thus, two mechanisms that implement different allocations may still be equivalent provided that they both grant the same expected payoffs to all involved.====When there is a single object (====), for ==== BIC mechanism there is a DSIC mechanism that gives each agent the same expected payoff that the agent obtained in the Bayesian mechanism Manelli and Vincent (2010). Thus, there is a priori no loss in requiring dominant-strategy incentive compatibility over Bayesian incentive compatibility. An example in Gershkov et al. (2013) shows that equivalence fails when there are multiple objects (====). The questions then arise: When are BIC mechanisms also DSIC and what, in general, is the relationship between BIC mechanisms and DSIC mechanisms when there are multiple goods to allocate?====We focus on ==== Bayesian mechanisms – i.e. mechanisms in which agent types pool into finitely many sets such that each agent within a set receives the same expected probabilities of trade and makes the same expected payment. The interim equilibrium utility of agents in finite BIC mechanisms is a piecewise linear function: the domain of each linear component corresponds to a set of pooling types. Theorem 3 demonstrates that, for any finite BIC mechanism, there is a direct mechanism that generates it which is linear over the same subsets — that is, Bayesian mechanisms which pool agents in terms of ==== outcomes can always be generated by direct mechanisms that pool agents in terms of ==== outcomes. This fact is valuable because in assessing whether a finite BIC mechanism has a DSIC equivalent we will only need to search within finite DSIC mechanisms. Theorem 4 characterizes the interim utilities from finite BIC mechanisms using the feasibility inequalities in Border (1991) and the incentive compatibility property of Rochet (1985). Theorem 5 identifies necessary conditions, implied by the feasibility inequalities, that any finite mechanism must satisfy. A DSIC mechanism that implements a candidate BIC mechanism must satisfy these conditions ==== ex post incentive compatibility. We demonstrate that some candidate BIC mechanisms cannot be implemented in dominant strategies by showing that the two criteria are inconsistent (see the examples in Sections 6 A BIC mechanism that is not DSIC, 8 A further example). In certain cases the conditions allow us to construct the ‘closest’ dominant-strategy implementable mechanism. In other cases, the conditions are strong enough to demonstrate when a candidate Bayesian mechanism ==== be implemented as a dominant-strategy incentive compatible mechanism by explicitly constructing the unique mechanism that generates it and then showing that the resulting mechanism is DSIC (Section 7).====While our results apply only to piecewise linear mechanisms, i.e. finite mechanisms, we impose no limit on the number of pieces any such mechanism possesses. In our formulation, any incentive compatible mechanism must be convex and, of course, any convex function can be arbitrarily closely approximated by a piecewise linear function.====Identifying when the BIC–DSIC equivalence holds is valuable. Dominant-strategy mechanisms have advantages over Bayesian mechanisms. For instance, one may be more confident that a rational agent will play a dominant strategy (if one is available) than that the same agent will play a Nash equilibrium strategy.==== ====While the equivalence or lack thereof between BIC and DSIC mechanisms has a long history, equivalence was defined, for much of that history, in terms of allocation: A DSIC mechanism is equivalent to a BIC mechanism if it implements the same allocation. (See, for instance, Mookherjee and Reichelstein (1992), and Williams (1999).) In the trading environment that we study, allocative equivalence means that the same probability-of-trade function – that is to say, the probability with which goods are allocated to each agent given the reports made – can be obtained by Bayesian and dominant-strategy mechanisms that only differ on their transfer functions. This is a stronger notion of equivalence than the one we use.====The equivalence between BIC and DSIC mechanisms in single-object environments is robust in various ways and fails to be robust in others. First, it holds for ====—not just the efficient mechanism or the revenue maximizing one as is the case with the first price auction and its equivalent second price auction.==== ==== Second, the equivalence holds even with heterogeneous agents and nonsymmetric mechanisms. In particular, it holds when the seller is also privately informed Manelli and Vincent (2010). Third, the equivalence has been extended to an independent private values model with finitely competing outcomes Gershkov et al. (2013) and to some instances with non-linear utilities by Kushnir and Liu (2017).====The one-dimensional equivalence fails outside the model described. Gershkov et al. (2013) illustrate this failure in various examples. One of them, with two homogeneous goods and discrete types, shows that the ==== BIC mechanism differs from the ==== DSIC mechanism. An example in Crémer and McLean (1988, Appendix A) provides an example of equivalence failure with interdependent valuations. We provide examples of the failure of equivalence when ==== that illustrate the extent of the problem.==== ====Multi-dimensional mechanism design problems are notoriously complex. Our approach focuses on finite mechanisms and combines two separate strands of the implementation literature. For the one good case, Matthews (1984) and Border (1991) characterize the functions that are the expected probability of trade for some mechanism. Maskin and Riley (1984) prove, constructively, a variation of Border’s characterization for a particular case. Border (2007) extends his own result to nonsymmetric, one-dimensional environments. These results effectively demonstrate when a candidate mechanism is ==== in the sense that it obeys the resource constraint that no more than one object be allocated.====Separately, Rochet (1985) provides necessary and sufficient conditions for incentive compatibility in the case of multi-dimensional buyer types. In brief, the condition requires the convexity of the implied interim utility function for each agent. Combining this result with an adaptation of the above feasibility results allows us to characterize BIC mechanisms in our environment. We then exploit the interaction of convexity constraints with the feasibility constraints to generate our main results concerning the relationship between BIC and DSIC mechanisms.",Dominant-strategy and Bayesian incentive compatibility in multi-object trading environments,https://www.sciencedirect.com/science/article/pii/S0304406819300370,May 2019,2019,Research Article,301.0
"Schweizer Nikolaus,Szech Nora","Department of Econometrics and OR, Tilburg University, Netherlands,Department of Economics, Karlsruhe Institute of Technology, WZB and CESifo, Germany","Received 18 September 2018, Revised 22 February 2019, Accepted 26 February 2019, Available online 8 March 2019, Version of Record 19 March 2019.",https://doi.org/10.1016/j.jmateco.2019.02.007,Cited by (8),"-regularity. The parameter ==== interpolates from Myerson regularity to the monotone hazard rate condition and beyond. We provide four equivalent definitions of the concept. These rely on a growth condition on the virtual valuations function (known as ====-strong regularity), a monotonicity condition on a generalized hazard rate, a ","Since Myerson (1981)’s seminal study of optimal auctions, his regularity condition of increasing virtual valuations has been a cornerstone of the theory of auctions and mechanism design. Yet, Myerson regularity alone is often too weak to guarantee that a model is sufficiently well-behaved for quantitative analysis, such as comparisons of welfare versus revenues, or controlling probabilities of sale in an auction. This liberality of Myerson regularity has become particularly apparent in the recent literature on algorithmic mechanism design, where it typically needs enforcement by additional assumptions.==== ==== This literature aims at turning mechanism design more scalable and more quantitative, and therefore better suited for many real world problems. By replacing the classical objective of optimality with the more modest goal of guaranteeing a good approximation of the optimal outcome, various challenging problems (asymmetry, uncertainty about distributions, complex preferences over multiple objects …) can now be handled to a degree that was previously out of reach. For example, several papers analyze successful auction design if only a sample from the bid distribution is known, thereby dropping the classical assumption of common knowledge of distributions.==== ==== Thus, these works address (Wilson, 1987)’s critique that asked for a more detail-free approach towards mechanism design.====In many applications, it is desired to estimate quantities such as the ratio of revenue to welfare for a large class of admissible distributions — ideally, for all regular distributions.==== ==== Yet, as the distributions at the boundary of Myerson regularity behave badly, such uniform estimates for all regular distributions cannot exist. In such cases, quantitative estimates have often been obtained by restricting the set of admissible distributions to those that fulfill the monotone hazard rate (MHR) condition, also known as increasing failure rate (IFR).==== ==== Quantitatively, MHR distributions are easier to handle as they have convenient properties such as log-concavity of the survival function “====”. Further, they easily compare to exponential distributions. This facilitates analysis a lot.====Moving from the Myerson regular to the monotone hazard rate case, the class of admissible distributions becomes considerably smaller. For example, the class of Myerson regular distributions contains heavy-tailed (power law) distributions. These are ruled out under the monotone hazard rate condition, under which the exponential distribution marks the boundary of the admissible heaviness of tails. Likewise, the monotone hazard rate condition rules out some local irregularities of distributions that are still admissible under Myerson regularity. Both aspects may matter in many applications. Thus, it would be good to impose less than the monotone hazard rate condition, while still being able to provide quantitative results for which Myerson regularity alone is not strong enough.====In this paper, we study an alternative strengthening of Myerson regularity. We define a distribution as ====-regular if the slope of the virtual valuations function is bounded from below by ==== for some ====. ====-regularity is thus exactly Myerson regularity. ====, i.e. ====, corresponds to the monotone hazard rate condition. Choosing ==== interpolates between the two cases. This condition has appeared before in the literature on auctions and related topics. It coincides with the ====-concavity assumption on the survival functions as in, e.g., Ewerhart (2013) and Mares and Swinkels, 2011, Mares and Swinkels, 2014 and with the concept of ====-strong regularity in Cole and Roughgarden (2014), with ==== and ====.====These authors have different perspectives on the condition which we unify in the present paper. The main contribution of this paper is twofold. First, many of the key tools in the algorithmic mechanism design literature carry over with small changes from the monotone hazard rate to the general ====-regular case.==== ==== Second, ====-regularity has deep roots in several literatures which implies that it can be exploited in various ways. ====-regularity has an alternative representation in terms of monotonicity of a generalized hazard rate, a generalized concavity condition on the survival function, and it allows for comparisons (in the convex-transform order) with an explicit class of reference distributions. These properties link ====-regularity with two rich and distinct bodies of work in applied mathematics, the literature on generalized notions of concavity and the literature on reliability and stochastic orders.==== ==== The latter, statistical implications have been applied in auction theory mainly for the monotone hazard rate case. Thus, while the concept itself is not new, we provide a more complete picture of its scope, its history and its implications in mechanism design than previous works. One of our main contributions is to emphasize that ====-regularity does not just coincide with one but with multiple existing concepts.====Combining these perspectives and the tools they imply, we first extend a series of quantitative estimates about the single bidder case from the monotone hazard rate case to the ====-regular case. These results have been used heavily in the algorithmic mechanism design literature. There, quantitative estimates for ==== are often contrasted against their breakdown for ====.==== ==== Our estimates continue to hold for any ==== with constants that (must) blow up in the limit ====. In particular, we confirm that it is, in a sense, merely the boundary of Myerson regularity that typically causes problems. We then turn to a number of results about order statistics and the ====-bidder case. We provide a lower bound on the revenue-to-welfare ratio in single-object auctions, and some additional bounds on small order statistics. As an application beyond mechanism design, we also provide sharp estimates for a family of generalized Gini indices for the measurement of inequality.",Performance bounds for optimal sales mechanisms beyond the monotone hazard rate condition,https://www.sciencedirect.com/science/article/pii/S0304406819300321,May 2019,2019,Research Article,302.0
Sandroni Alvaro,"Department of Managerial Economics and Decision Sciences, Kellogg School of Management, Northwestern University, Evanston, IL 60208, United States","Received 11 September 2018, Revised 17 February 2019, Accepted 18 February 2019, Available online 5 March 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.jmateco.2019.02.005,Cited by (0),"This paper develops a basic model of closeness where people have optimal distances they want to keep from each other. When optimal distances differ, mixed strategies are inevitable in equilibrium. This may lead to unique and completely one-sided equilibrium outcomes where some cannot obtain anything except for what is best for others.","To have good friends and a loving family is, to many, second to nothing but indispensable necessities of life. Human relations, however, are notoriously difficult and often produce distress, regardless of economic progress or wealth. Take, for example, the question of intimacy. People often have conflicting sentiments, wanting simultaneously independence and attachment, both fearing and craving those around them. If individuals could resolve their internal conflicts they would know how close they want to be from others, but then a second problem arises. Some people may prefer to be emotionally closer and others emotionally more distant. If, say, two people prefer to be at different distances then someone always wants to move. This leads to a cat-and-mouse game that does not settle deterministically.==== ====The key objective of this paper is not to explicitly model intimacy, social anxieties or any specific problem in human relations. Instead a more abstract approach is taken. In many unstable relationships, the heart of the matter is that (for specific types of payoffs), there are no deterministic profiles of attitudes that all concerned can abide to and such that no one has any incentive to deviate. That is, no pure strategy equilibrium exists. Random behavior, however, can be a strategically advantageous tactic. Some players may want to be unpredictable to force others to have predictable and controllable attitudes. This is the intuition this paper intends to formalize.====Consider a two-player game. Each player chooses an attitude (expressed as a point in the real line). Players’ have different preferences over how close they want to be from each other. One player, the attached, prefers to be closer. The other player, the remote, prefers to be further away. In these games, referred to as hedgehog’s dilemmas, there is no pure strategy Nash equilibrium. Mixed-strategy equilibria exist, but can produce completely one-sided results. One player chooses random attitudes and gets an ideal outcome. The other player chooses a deterministic attitude and is increasingly worse-off as the difference between optimal distances increase. When the ratio of optimal distances is smaller than half, this is the only equilibrium. Then, the attached chooses deterministically and cannot obtain anything except for what is best to the remote. This oppression by randomness produces an inequity that, in equilibrium, is inevitable. Hence, a hedgehog dilemma differs from classic games such as matching pennies, battle of the sexes and coordination games.====This paper is organized as follows: The model is in Section 2. Main results are in Section 3. Section 4 discusses future work. Section 5 concludes. Proofs are in the Appendix.",The hedgehog’s dilemma,https://www.sciencedirect.com/science/article/pii/S0304406819300242,May 2019,2019,Research Article,303.0
Scalzo Vincenzo,"Department of Economics and Statistics (DISES), University of Naples Federico II, via Cinthia 21, 80126 Napoli, Italy","Received 16 July 2018, Revised 26 February 2019, Accepted 27 February 2019, Available online 5 March 2019, Version of Record 15 March 2019.",https://doi.org/10.1016/j.jmateco.2019.02.008,Cited by (9),. Examples compare our result with the previous ones.,"Consider a finite set ==== of players and, for any ====, let ==== be a non-empty subset of a Hausdorff topological vector space. Denoted by ==== the set of strategy profiles and, for each ====, let ==== be a mapping from ==== to ====. The list ==== is called ====, and a ==== of ==== (==== briefly) is an element ==== so that ==== for all ==== (==== denotes the set of Nash equilibria of ====). The existence of Nash equilibria of ==== can be investigated by means of the Ky Fan minimax inequality. Given a real-valued function ==== defined on ====, the corresponding ==== (Fan, 1972) is the following problem: find ==== such that ==== for all ==== (in such a case, ==== is said to be a ====). Now, for every ==== and ==== which belong to ====, define ==== as below: ====where ==== whenever ==== and ==== otherwise. Obviously, ==== is an equilibrium of ==== if and only if it is a solution to the Ky Fan minimax inequality corresponding to ====, that is: ==== for all ====. Conditions which guarantee the existence of solutions to the Ky Fan minimax inequality allow to identify classes of games endowed with Nash equilibria. So, it is interesting to investigate if the Ky Fan minimax inequality approach leads to new sufficient conditions for the existence of Nash equilibria in ordinal games.====In this paper, we show that this is possible. Using Tian (1993, Lemma 1), we obtain the existence of solutions to the Ky Fan minimax inequality when the function ==== is ==== and ====: see Proposition 1. So, given an ordinal game ====, we relate the slight diagonal transfer continuity of the function ==== defined by (1) to a generalization of the ==== (see Reny, 2009, Nassah and Tian, 2016), that we call ====. On the other hand, the diagonal transfer quasi-concavity of ==== corresponds to a necessary condition for the existence of Nash equilibria called ==== (see Nassah and Tian, 2016 for the normal form games). Hence, we prove that the slight single deviation property and the transfer uniform quasi-concavity guarantee the existence of Nash equilibria: see Theorem. Let us note that the single deviation property (and the generalization considered in the present paper) is not sufficient to guarantee the existence of equilibria under the standard condition ====
 ==== for all ==== and all ====: see Reny (2009, Section 3.1).==== ==== Moreover, we provide examples to show that the transfer uniform quasi-concavity is not connected with condition ==== and our Theorem applies to situations which are not covered by the previous literature on the existence of Nash equilibria in ordinal games: see Shafer and Sonnenschein (1975), Yannelis and Prabhakar (1983), Wu and Shen (1996), Scalzo (2015), Carmona and Podczeck (2016), He and Yannelis (2016), Nassah and Tian (2016) and Reny (2016).====Finally, we consider normal form games ====: in this case, we associate ==== with the ordinal game ==== where, for any player ====, ====. In this setting, Nassah and Tian (2016, Theorem 6) states that Nash equilibria exist when the single deviation property and the transfer uniform quasi-concavity hold.==== ==== The result given in the present paper is a slight improvement due to the generalization of the single deviation property. Moreover, we compare our Theorem with the well known Baye et al. (1993, Theorem 1): we prove that they are independent. Furthermore, we remark that our result cannot be deduced from the previous ones given in Dasgupta and Maskin (1986), Reny (1999), Reny (2009), Bagh and Jofre (2006), Carmona (2009), Nessah (2011), Barelli and Meneghel (2013) and Prokopovych (2013).",Equilibrium existence in games: Slight single deviation property and Ky Fan minimax inequality,https://www.sciencedirect.com/science/article/pii/S0304406819300333,May 2019,2019,Research Article,304.0
"Hajargasht Gholamreza,Rao D.S. Prasada","Swinburne Business School, Swinburne University of Technology, Australia,School of Economics, University of Queensland, Australia","Received 8 December 2015, Revised 7 February 2019, Accepted 12 February 2019, Available online 28 February 2019, Version of Record 30 April 2019.",https://doi.org/10.1016/j.jmateco.2019.02.004,Cited by (3),Over the past five decades a number of multilateral index number systems have been proposed for spatial and cross-country price comparisons. These multilateral indexes are usually expressed as solutions to systems of linear or ,"Purchasing power parity (PPP) is a widely used multilateral index for comparing price levels and macroeconomic aggregates such as gross domestic product (GDP) and its components across countries. PPPs are now regularly compiled as part of the World Bank’s International Comparison Program (ICP). The most recently released findings from the ICP are for the year 2011 covering 177 countries of the world (World Bank, 2015). The PPPs from the ICP are used in assessing the size and distribution of the world economy and rankings of economies. For example, the latest ICP report indicates that the United States was the world’s largest economy in 2011 followed by China, India and Japan. Results from the ICP also indicate that there has been a significant reduction in global inequality based on PPP-converted per capita income data. ICP results are also used for calculating World Development Indicators, the Human Development Index (HDI), regional and global poverty, and for comparing health and education expenditures across countries.====The PPPs within the ICP are obtained by aggregating price data collected from countries using appropriate multilateral index formulas.==== ==== A variety of multilateral index numbers have been proposed for the purpose of PPP compilation over the last five decades including but not limited to Gini–Eltetö – Köves–Szulc (GEKS); Geary–Khamis (GK); generalized GK; Iklé (or IDB); Rao; and the Country-Product-Dummy (CPD). Rao (2013b) and Diewert (2013) describe the methods currently employed within the ICP. Economic theoretic approaches to multilateral systems have been discussed in Neary (2004), Feenstra et al. (2009), Balk (2009) and Feenstra et al. (2013) while Diewert, 1988, Diewert, 1999 and Balk, 2008, Balk, 2009 provide overviews of the axiomatic or test approach to multilateral index numbers. Hill, 2000, Hill, 2009 and Hajargasht et al. (2018) discuss spatial chaining methods based on minimum spanning trees and Rao (2009) offers a collection of papers that describe the state of the art and advances that have been made. The quest for indexes with better properties is ongoing and new indexes are being proposed e.g. see Hajargasht and Rao (2010) and Rao and Hajargasht (2016) for a new stochastic approach.====A common attribute of the multilateral index number systems used in international comparisons is that the price indexes from these methods are usually solutions to some suitably formulated systems of equations. These systems can be linear==== ==== as is the case with the GK system or nonlinear as is the case with Rao (1990) system. These systems can be meaningful only if they have solutions which are positive and unique (up to a factor of proportionality==== ====). Not surprisingly, some efforts have already been put into proving the existence of solutions to these systems [e.g. Rao (1971), Rao (1976), Khamis (1972), Balk, 1996, Balk, 2009 and Neary (2004)]. However, the existing results do not often assume the most general conditions and do not cover some indexes.====This paper contributes to the literature on existence and uniqueness of multilateral indexes in several ways. (i) It provides several theorems for existence and uniqueness of solutions to multilateral indexes in their most general forms. We use these theorems to prove viability of many indexes for some of which the results are new (e.g. for “equally weighted GK” and arithmetic index). We also provide both necessary and sufficient conditions for existence and uniqueness of IDB and Rao indexes. (ii) The paper brings together and clarifies the mathematical concepts and tools required for establishing existence and uniqueness of solutions to different types of multilateral index number systems. (iii) It introduces a new commensurability axiom for changes in the reference currency units; it is shown that this axiom leads to a class of multilateral index number systems based on generalized means of order ====. This general class encompasses most of the known systems including the commonly used systems. (iv) Another insight from our results is that a compatibility condition is often required when defining an index in the sense that the weights in the equations defining world average prices (====s) and the weights in equations defining purchasing power parities (====s) must be compatible. (v) Finally, an important contribution of the paper is to show that in general, existence and uniqueness of the indexes are guaranteed if the observed quantity matrix is connected.====The paper is organized as follows: In Section 2 we define basic notations and concepts that underpin the multilateral index number systems considered in the paper. Section 3 describes linear and nonlinear systems including several commonly used index number systems. Section 4 states and proves the main theorems on existence and uniqueness of general classes of multilateral index numbers. These general theorems are in turn used to prove the existence and uniqueness of many of the index numbers currently used in international comparisons. Appendix provides a mathematical toolkit (including various connectedness concepts, nonlinear eigenvalue theorems and their links to each other) that is used to prove the theorems stated in Section 4.","Multilateral index number systems for international price comparisons: Properties, existence and uniqueness",https://www.sciencedirect.com/science/article/pii/S0304406819300230,28 February 2019,2019,Research Article,305.0
"Murakami Hiromi,Urai Ken","School of Business Administration, Kwansei Gakuin University, Japan,Graduate School of Economics, Osaka University, Japan","Received 11 August 2017, Revised 18 January 2019, Accepted 11 February 2019, Available online 27 February 2019, Version of Record 12 April 2019.",https://doi.org/10.1016/j.jmateco.2019.02.003,Cited by (0),"In this paper, we axiomatically characterize the ==== for economies including ==== agents. A path-breaking ==== argument in Sonnenschein (1974) is generalized to include possibly satiated agents under a price mechanism with ==== or ====. That is, the price–money mechanism is characterized uniquely and efficiently representing all other allocation mechanisms satisfying several basic axioms, including the weak core and Pareto properties. To obtain our generalization results, we use a ==== in Murakami and Urai (2017).","In this paper, we axiomatically characterize the ==== for economies with possibly satiated agent preferences. A category-theoretic argument in Sonnenschein (1974) is generalized to include satiated agents under a price mechanism with ==== or ====. That is, the price–money mechanism is uniquely and efficiently characterized as representing all other allocation mechanisms satisfying several basic axioms, including the weak core and Pareto properties.====A ground-breaking work on this axiomatic characterization problem was given in Sonnenschein (1974, Propositions 1–7) for the case with standard Arrow–Debreu non-satiated economies. He uses the Debreu–Scarf core limit theorem to characterize the price mechanism. His approach is important in two respects: (i) it ==== captures ==== as a process of sending and responding to private ==== or ==== and (ii) it offers a basic framework of purely mathematical ==== for such mechanisms.==== ==== The first feature is related to the classical informational efficiency or minimality problems on allocation mechanisms like Hurwicz (1960), Mount and Reiter (1974), Osana (1978) and Jordan (1982), etc. His Propositions 3–6 are related to such arguments on the minimum dimension of information spaces. The second feature, the main interest of this paper, corresponds to his Propositions 1 and 7, and is open to a huge number of contemporary study of axiomatic characterization for resource allocation mechanisms. His Axiom S to this method precedes such later social choice axioms as replica stability in Thomson (1988) and Nagahisa (1994). By dealing with the number of agents as variables, and incorporating various topological or dimensional structures of message spaces, the method enables us to locate general classes of economies and information spaces into a unified perspective for an axiomatic characterization. His Propositions 1 and 7 were presented independently for arguments on a property called a ==== and the ====, respectively (see also Concluding Remark 5).====This paper generalizes Sonnenschein’s (1974) argument by incorporating ==== or non-negative wealth transfer as a ==== allocation. The consideration of satiated agents is important and standard to treat problems like fundamental welfare theorems (e.g., see the second welfare theorem in Debreu, 1959; Chapter 6). For the equilibrium concept, we use the ==== or ====, defined by Aumann and Drèze (1986), which is one of the most general tools for a competitive equilibrium with satiated agents.==== ==== As discussed in Kajii (1996), the concept of dividend equilibrium can be identified with an equilibrium with non-negative wealth transfers or ====, so the mechanism we axiomatically characterize can naturally be identified with the price–money mechanism.==== ====There is one difficulty, however, to generalize Sonnenschein’s argument to the characterization of the price–moneymechanism for economies including possibly satiated agents. Sonnenschein’s argument crucially depends on the ==== that represents the relation between an original economy and its replica extensions.==== ==== However, the core limit property does not work when an economy has satiated agents. Appendix A illustrates an economy with satiation in which a feasible allocation ==== is not a dividend equilibrium while the ====-fold replica allocation ==== is Pareto optimal and weak core allocation for all ====-times replica economies. The strong core also fails to characterize dividend equilibria by its equal treatment property, so both weak and strong core concepts are inadequate as is inferred from the rejective core equivalence results in limit economies like Konovalov (2005). In this paper, we apply the ==== limit (RNR core limit) result shown in Murakami and Urai (2017) to overcome the difficulty.====Note also that Sonnenschein’s Proposition 1, the dictionary theorem, treats messages as dependent merely based on each agent’s characteristics, i.e., the initial endowment and the preference. To characterize the price–money message, however, we must treat messages that depend not only on individual characteristics but also on the economy to which an agent belongs. If otherwise, monetary transfer messages would be restricted to be the same for all agents with identical individual characteristics. Therefore, to characterize price–money messages, economy-wide information like the distribution of income is necessary. This is simply why we have to use core concepts like the weak or the weak rejective core that do not have the equal treatment property.====A concept like the rejective core enables us to incorporate such an economy-wide property using a certain kind of ==== process into the deviation principle. In our proof of Murakami and Urai (2017), we introduce such a re-negotiation argument to the deviation structure of the Debreu–Scarf core limit proof for replica economies.====In summary, using such a rejective core limit argument, we generalize and reformulate Sonnenschein’s category-theoretic argument on the axiomatic characterization of the price mechanism to that of the ==== ( ==== ) ==== for economies, including possibly satiated agents. Since the dividend equilibrium concept is an equilibrium with non-negative wealth transfers, it is appropriate to treat such an allocation mechanism as depending not only on individual characteristics but also on the entire economic structure. Therefore, in this paper, Sonnenschein’s Proposition 1 is generalized to incorporate the ==== feature of messages. See the following two diagrams: ====The left commutative diagram represents Sonnenschein’s dictionary theorem and the right is for our generalized theorem. In Sonnenschein (1974), the price mechanism is characterized as ==== such that for all ====, where ==== is the class of individual characteristics, and for any function ====, there uniquely exists ====, where ==== denotes the identity mapping on ====, such that the left diagram commutes. Sonnenschein calls this the ==== of prices, which is mathematically identified with the universal mapping property with respect to the price mechanism. In our theorem (Theorem 2), the result is generalized so that the price–money mechanism is characterized as ==== such that for all ====, where ==== is the class of economies including satiated agents, and for any function ====, there uniquely exists ====, where ==== denotes the identity mapping on ====, such that the right diagram commutes. In other words, the price–money mechanism is shown to universally and uniquely implement other message mechanisms in a large category of economy-dependent messages (not on ==== but on ====) without restricting the number of agents or referring to any of the message space dimensions and/or even to their topological structures. We also obtain an isomorphism theorem (Theorem 3), arguing that an object with the above universal mapping property is unique up to isomorphism. Hence, we obtain a minimum information space having the universal mapping property.==== ====In the following, ==== is used as the set of real numbers. For finite set ====, denote by ==== the number of elements of ====. We write ==== instead of ==== to represent ====-dimensional vector space. The order relations on ====, ==== and ====, are defined respectively as ==== iff ==== for all ====, and ==== iff ==== and ====. We also define relation ==== as ==== iff ==== for all ====. By ==== and ====, we represent the sets ==== and ====, respectively.",An axiomatic characterization of the price–money message mechanism for economies with satiation,https://www.sciencedirect.com/science/article/pii/S0304406819300229,May 2019,2019,Research Article,306.0
"Bosi Gianni,Herden Gerhard","DEAMS, Università di Trieste, via dell’Università 1, 34123, Trieste, Italy,Fakultät für Mathematik, Mathematikcarrée, Universität Duisburg–Essen, Campus Essen, Thea-Leymann-Straße 9, D-45127 Essen, Germany","Received 2 August 2018, Revised 12 February 2019, Accepted 19 February 2019, Available online 26 February 2019, Version of Record 7 March 2019.",https://doi.org/10.1016/j.jmateco.2019.02.006,Cited by (8),"Let ==== be an arbitrary set. A topology ==== on ==== is said to be useful if every complete and continuous preorder on ==== is representable by a continuous real-valued order preserving function. It will be shown, in a first step, that there exists a natural one-to-one correspondence between continuous and complete preorders and complete separable systems on ====. This result allows us to present a simple characterization of useful topologies ==== on ====. According to such a characterization, a topology ==== on ==== is useful if and only if for every complete separable system ==== on ==== the topology ==== generated by ==== and by all the sets ==== is second countable. Finally, we provide a simple proof of the fact that the countable weak separability condition (====), is necessary for the usefulness of a topology.","Let ==== be an arbitrarily but fixed chosen set. From Herden, 1991, a topology ==== on ==== is said to be ==== if every ==== ==== on ==== has a continuous utility representation, i.e. can be represented by a continuous real-valued order preserving function (utility function) (see e.g. Herden, 1989a, Herden, 1989b, Herden, 1991). Continuity of ==== means that the ==== ==== induced by ==== is coarser than ==== (i.e., the sets ==== and ==== are open subsets of ==== for every ====).====Other authors call ==== the topologies satisfying the aforementioned property (see e.g. Candeal et al., 1998 and Campión et al., 2006, Campión et al., 2007, Campión et al., 2009, Campión et al., 2012). In this paper we prefer the original terminology of a useful topology, inherited from the seminal paper Herden, 1991, who first explicitly started a systematic study of this concept.====In some sense the problem of characterizing all useful topologies on ==== is the most fundamental problem in utility theory. Indeed, the classical theorems by Eilenberg, 1941 (====) and Debreu, 1954, Debreu, 1964 (====)), that only recently have been proved again by Rébillé, 2018 by very elementary methods, present sufficient conditions for a topology ==== on ==== to be useful. Other sufficient conditions, which are based upon familiar topological properties, that ensure usefulness for a topology ==== on ==== are provided in Theorem 4.3 Campión et al., 2012.====In Estévez and Hervés, 1995, it was shown that in any non-separable metric space there is a continuous complete preorder that does not admits a utility function. This result, in combination with ====, can be used to state that a metric topology ==== on ==== is useful if and only if it is second countable. We will refer to this latter result as Estévez–Hervés’ theorem (====) (see also Theorem 1 Candeal et al., 1998).====With help of the concept of a useful topology ==== on ====, the fundamental theorems above can be restated as follows.====: ==== ==== ==== ==== ====: ==== ==== ==== ==== ====: ==== ==== ==== ==== ==== ==== ====On the other hand, it is well known that second countability or separability, in general, is not necessary for ==== to being useful (cf., for instance, the Niemitzki plane that is extensively discussed in Steen and Seebach, 1970). It is important to observe that, according to Theorem 3.1 Campión et al., 2006, a very important example of a useful topology is represented by the weak topology of a Banach space.====Different characterizations of useful (or representable) topologies appear in the literature. In particular, Theorem 5.1 Campión et al., 2009, proved that a topology ==== on ==== is useful if and only if all its preorderable subtopologies are second countable, where a topology ==== on ==== is preorderable if it is the order topology of some continuous complete preorder on ====. It is remarkable that our main result (Theorem 3.1) is somewhat analogous to this characterization.====In this paper we contribute to clarify the structure of useful topologies on ==== by using the concept of a complete separable system. In particular, we prove that there exists a natural one-to-one correspondence between continuous and complete preorders and complete separable systems on ==== (cf. Proposition 3.2). This one-to-one correspondence can be applied, in particular, in order to simplify and somewhat generalize the characterization of useful topologies that has been presented in Herden, 1991, Herden and Pallack, 2000 and Campión et al., 2009. Indeed, we prove in Theorem 3.1 that a topology ==== on ==== is useful if and only if, for every complete separable system ==== on ====, the topology ==== generated by ==== and ==== is second countable. We notice that for every complete and continuous preorder ==== on ==== which is continuously representable, if we take ====, then the topology ==== is precisely the order topology ==== on ====.====Finally, by introducing the concept of a well-separated family of separable systems on ====, we provide a necessary condition for a topology to be useful. This condition, that was introduced by Herden and Pallack, 2000, is referred to as the countable weak separability condition (====). It is inspired by two well-known topological concepts: the countable chain condition (====) and the concept of a locally finite family of subsets of ==== (cf., Definition 3.1, Definition 3.2, and Proposition 3.3).",The structure of useful topologies,https://www.sciencedirect.com/science/article/pii/S0304406819300254,May 2019,2019,Research Article,307.0
Li Sung-Ko,"Department of Economics, Hong Kong Baptist University, Kowloon Tong, Hong Kong","Received 25 September 2017, Revised 4 August 2018, Accepted 14 December 2018, Available online 26 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jmateco.2018.12.003,Cited by (6),Modeling “quasiconcavity” and “variable returns to scale” simultaneously is a challenging task. This paper defines variable returns to scale vigorously and explores its relationships with ,"Some economists believe that the production process of firms usually exhibits increasing returnsto scale first and finally decreasing returns to scale. This is called ==== in this paper. This type of production functions usually follows an “S-shaped” pattern, see Duggal et al., 1999, Duggal et al., 2007 for applying parametric S-shaped production function to empirical studies. The nonparametric tests of production developed by Afriat (1972), Hanoch and Rothschild (1972), Diewert and Parkan (1983) and Varian (1984) can help to check the consistency between data and conventional assumptions. Surprisingly, existing nonparametric tests do not cover variable returns to scale.====Relevant studies on variable returns to scale are Petersen (1990), Bogetoft (1996), and Bogetoft et al. (2000). They imposed the conditions of returns to scale and convex input set or output set to construct an empirical production frontier from data. Yet in their models, increasing and decreasing returns to scale do not appear simultaneously. Kerstens and Eeckaut (1998) attempted to incorporate variable returns to scale to free disposal hull (FDH) production frontier but neither input nor output sets are convex.====The purpose of this paper is to extend the nonparametric tests for quasiconcave function introduced by Hanoch and Rothschild (1972), and Diewert and Parkan (1983) to incorporate variable returns to scale. The concept of variable returns to scale is studied vigorously. It turns out that including increasing and decreasing returns to scale simultaneously greatly complicates the nonparametric test of quasiconcavity. This complication can be solved by discovering an empirical quasiconcave, but not necessarily concave, function that exhibits both increasing and decreasing returns to scale, which is a contribution of this paper. The remainder of the paper is organized as follows: Section 2 explores the concept of variable returns to scale and its relations to elasticity of scale and S-shaped production functions. Section 3 introduces the extensions of different returns to scale. Section 4 constructs an empirical production function and shows that it satisfies the conditions to be tested. Section 5 is the main result of this paper. Section 6 concludes.",A nonparametric test of quasiconcave production function with variable returns to scale,https://www.sciencedirect.com/science/article/pii/S030440681830137X,May 2019,2019,Research Article,308.0
"Bhaskar V.,Mailath George J.","Department of Economics, University of Texas at Austin, United States,Department of Economics and PIER, University of Pennsylvania, United States,Research School of Economics, Australian National University, Australia","Received 16 November 2017, Revised 3 December 2018, Accepted 27 January 2019, Available online 19 February 2019, Version of Record 7 March 2019.",https://doi.org/10.1016/j.jmateco.2019.01.009,Cited by (10),"We study dynamic moral hazard when the principal can only commit to spot contracts. The principal and agent are ex ante symmetrically uncertain about the difficulty of the job, and update their beliefs upon observing output. Since the agent’s effort is private, he has an additional incentive to shirk when the principal induces effort: shirking results in the principal having incorrect beliefs, giving rise to future informational rents. We show that the effort-inducing contract must provide increasingly high-powered incentives as the length of the relationship increases. Thus it is never optimal to always induce effort in very long relationships.","We analyze the long-run implications of the ratchet effect, arising from the introduction of new technology, in a context where both the firm and worker are learning about its efficacy. Milgrom and Roberts (1992, pages 232–236) provide a lucid statement of the problem: When a firm installs new equipment, firms and workers must learn the appropriate work standard. It is efficient to use information to adjust the standard, but this reduces work incentives today.==== ==== The ratchet effect arises from the combination of learning, moral hazard and lack of long-term commitment by the employer.====Earlier work on the ratchet effect usually assumes ex ante differential information. The agent has private information on the nature of the job, and the principal is unable to make long-term commitments. The problem is formulated as one of dynamic mechanism design without commitment in which the principal aims to induce the agent to reveal his private information.====We differ from this literature in formulating the ratchet effect as arising from a learning problem under symmetric incomplete information and moral hazard (since worker effort is not observed by the principal). The principal and the agent are symmetrically uncertain about the difficulty of the worker’s job. We assume the principal cannot commit to long-term contracts, and has all the bargaining power when choosing optimal spot (short-term) contracts. We also assume there is no limited liability, so the agent is indifferent between accepting the principal’s optimal spot contract and taking her outside option. Furthermore, since uncertainty pertains to the nature of the job, the outside option does not depend too heavily upon what is learned. Finally, the principal cannot separately learn about job difficulty and agent behavior (in the sense that a signal that the state is good is also a signal of high effort, and conversely). Our assumption on the structure of the signals is natural, being satisfied by most parametric models.====The ratchet effect arises from the agent’s possible manipulation of the principal’s beliefs by shirking. In a pure-strategy equilibrium in which the agent exerts effort, the principal correctly anticipates the agent’s effort choices, and the beliefs of the two parties about the nature of the job agree. However, when the agent deviates and shirks, the beliefs of the two parties differ, at least temporarily. Our analysis begins with a simple observation: In a two-period world, such a deviation increases the expected continuation value of the agent. In consequence, any incentive-compatible contract inducing high effort must be sufficiently high powered to offset this deviation gain. The ratchet effect gives rise to a dynamic incentive cost (which we term the ====, or ====), since the agent must be exposed to additional risk in order to overcome the incentive problem (Proposition 1). Since the principal must compensate the agent for increased risk, his wage costs increase. This finding generalizes Milgrom and Roberts’s (1992) earlier demonstration of the need for high-powered incentives in a two-period model with linear technology and normal signals.====The bulk of our analysis concerns the behavior of dynamic-incentive costs as the time horizon ==== increases. For most of the paper, we focus on the principal’s cost-minimization problem when she induces effort in every period—this is a prerequisite for analyzing overall profit maximization. Specifically, we consider ====. These are contracts which induce high effort in every period at minimum cost. While it is intuitive that the future information rents from shirking in any period should increase with the time horizon, there is a subtlety. The dynamic-incentive cost is essentially the opportunity cost of not shirking, and little is known about the comparative statics of the optimal effort contract with respect to costs (or benefits) of shirking. Nonetheless, it turns out that the intuitive increase with the time horizon does occur if either the agent has a specific form of CRRA preferences (Proposition A.1) or if the signal distribution satisfies one additional collinearity restriction (the collinearity assumption is always satisfied with binary signals).====However, a plausible conjecture is that this effect, when present, tapers off: Since both the principal and agent learn the state of the world, there is very little uncertainty remaining towards the end of a long relationship. Our main result is that this conjecture is false. Under the collinearity restriction on the signal distribution the future information rents from shirking in any period are bounded below by a linear function of the remaining duration of the relationship (Proposition 3). The key insight is the following. Compare the problem of inducing effort in the initial period in two cases: (a) a three-period relationship, and (b) a two-period relationship. In both cases, the agent receives second-period rents by shirking in the first period. However, these rents are larger in the three-period setting than in the two-period setting, because the principal has to provide more high-powered incentives in the penultimate period of the three-period relationship, as we have already seen from the analysis of the two-period model. We provide an example showing that in the absence of this positive feedback from one period’s future information rents from shirking to earlier periods, the value from having different beliefs in all future periods is bounded (Proposition 4). We also derive a lower bound on the future information rents from shirking when the horizon is infinite and the agent discounts (Proposition C.1).==== ====Since the future information rents from shirking are bounded below by a linear function of remaining duration, ====, the expected wage costs are similarly bounded below. Consequently, it is never optimal for the principal to always induce effort when ==== is sufficiently large. While characterizing the optimal pattern of elicited effort is complex and beyond the scope of the current analysis, we do report some suggestive numerical calculations.",The curse of long horizons,https://www.sciencedirect.com/science/article/pii/S0304406819300187,May 2019,2019,Research Article,309.0
Hossain Tanjim,"Department of Management - UTM and Rotman School of Management, University of Toronto, Canada","Received 12 January 2012, Revised 31 January 2019, Accepted 7 February 2019, Available online 19 February 2019, Version of Record 8 March 2019.",https://doi.org/10.1016/j.jmateco.2019.02.002,Cited by (2),"We analyze two simple models of ==== with two-sided incomplete information where players have imperfect information about their own valuation. We suggest a model for learning by players about their own valuations during the bargaining process. Specifically, when the minimum price the buyer has to pay (or the maximum price a seller can obtain) is clear to her, it triggers a costless learning of whether her valuation is above that price. Under such learning, we show that there is an ex post efficient equilibrium in both models. Thus, a very simple model of learning about one’s own type can circumvent the Myerson–Satterthwaite theorem.","Consider a buyer and a seller negotiating the price of a used car. Suppose there is no informational asymmetry about the quality of the car and both have private valuations for it. The process of establishing a price is usually modeled as a bargaining game. In Rubinstein’s (1982) seminal paper on 2-person sequential bargaining, players have complete information about all players’ valuations. In equilibrium, trade occurs if and only if the buyer’s valuation is above the seller’s valuation. When the valuations of players are unknown to other players, however, bargaining ceases to be efficient. As a result, the probability of trade in bargaining with incomplete information is lower than that in the complete information case. Indeed, Ausubel and Deneckere (1992) show that trade does not occur at all in the continuous-time limit of the no-gap two-sided incomplete information bargaining game. In a more general setting, Myerson and Satterthwaite (1983) show that there exists no ==== incentive compatible mechanism for bargaining where valuations are drawn from a common support.==== ==== In this paper, we provide two examples of bargaining games where there exists an ex post efficient equilibrium when one or both players do not know their own valuations.====Most game theoretic models of bargaining assume that the players know their private types. However, unlike the predictions of these models, we observe a considerable amount of trade to occur even when offers and counter-offers are made frequently and gains from trade are not guaranteed. One potential reason for such discrepancy between theory and reality can be that a player’s actions do not convey as much information to the other player in real life situations as standard game theoretic models suggest. Thus, people do not need to “hide” their types as much. To address such issues in a formal setting, it is natural to look at models of bounded rationality. This paper explores situations of ==== where buyers and sellers do not know their valuations perfectly. We suggest a simple process for players to learn about their types during the bargaining process. In the games studied in this paper, there exists an equilibrium that is ex post efficient in addition to other equilibria where trade occurs with positive probability.====To illustrate this particular kind of bounded rationality, suppose the buyer and the seller do not know their own reservation values for the used car exactly. The seller has just bought a new car. She can use the old car as a second car, but is not sure how much she values having a second car. On the other hand, the buyer does not own a car currently and does not exactly know how much value having a car adds to her lifestyle. Suppose an agent can figure out her valuation exactly if she thinks about her usage of the car hard enough, but that mental exercise is costly. However, the negotiation process makes getting information about her own valuation in a particular manner effortless. Specifically, knowing her minimum possible offer in a bargaining game makes it easy for a player to learn the relation between her valuation and that price. Bargaining is not only a trading mechanism, but is also a tool for learning more about one’s own type. Although a player’s actions give away some information about her preferences, those also help her to learn more about her own preferences.====In our bargaining models, the amount the buyer has to pay (or the seller has to accept) once she has made an offer is obvious to her. That is, she knows exactly how much money is at stake. We assume that this makes it easier for the buyer to reflect on that amount and learn whether her enjoyment from the good is above that. On the other hand, comparing her valuation with a hypothetical amount has a prohibitively large cognitive cost and she does not learn about the relation between her valuation and a hypothetical price. She learns about her tastes only if it is necessary. Here individuals may learn their preferences by contemplating about their tastes as suggested by Ergin and Sarver (2010). For simplicity, we assume that contemplating the relation between her valuation and a price (provided by the bargaining process) is costless while contemplating the relation between her valuation and any other random number is infinitely costly.==== ====Not precisely knowing one’s own valuation is consistent with the observation that people often do not realize exactly how much they like an object before they have to pay for it. For example, property purchasers frequently realize that they have agreed to way too high a price after they sign the contract and have to make payments. This psychological reaction is commonly known as buyer’s remorse and is also common in purchases of other high value items such as cars, computers, and jewelry (Bell, 1967). This is believed to be caused by a sense of doubt that the purchasing decision was correct and is often considered to be a form of cognitive dissonance. In light of our paper, such behavior does not necessarily have to arise from a mistake or change in a person’s preferences.  They may arise when she receives new information about her own preferences after making an offer.====The goal of this paper is to incorporate bounded rationality in terms of imperfect information about one’s own type. We illustrate that our simple model of learning while bargaining leads to very interesting equilibria. In our first model, buyers and seller participate in a standard bargaining game where the buyer makes all the offers. The buyer does not know her valuation perfectly, only knows the distribution of the valuation. Moreover, after the buyer makes an offer, she can learn whether her valuation is equal to or above that offer without any cost. The seller knows her own valuation perfectly. We consider a case where the valuations are drawn from a discrete distribution with only three possible values. In this game, there exists an equilibrium where, for any discounting factor, trade occurs if and only if the buyer’s valuation for the object is weakly higher than the seller’s valuation. We also show that buyer’s remorse can arise in this equilibrium.====We next investigate a model where the valuations are drawn from continuous distributions and both the buyer and the seller do not know their own valuations. We design an ascending-clock version of the bargaining game where either of the players can stop the clock and make an offer. The offer price is determined by the time on the clock and the identity of the offer-maker. At any time, a player knows the relation between her valuation and the offer she would make if she made an offer at that time. If the other player makes an offer, she also learns the relation between her valuation and the offer. The game ends after one player stops the clock to make an offer. If the offer is accepted, there is trade. The game ends without trade otherwise. There is no time discounting in this game. We show that when the distributions are symmetric and concave, there exists an ex post efficient equilibrium. Thus, we provide an example of a stylized bargaining mechanism that leads to efficiency even when the distributions are continuous.====In both of these models, the amount of information a player has on her own type is limited. Hence, the amount of information that a player’s offers can reveal to the other player is also limited. As a result, the extent to which the other player can exploit the information revealed by the offers and its impact on the player’s expected payoffs are limited. In these models, while an increase in the offer or a tick of a clock worsens the terms of trade for a player, it also is beneficial for the player as it provides more personal information for her. For example, consider the case where the seller’s valuation is higher than the expected value of the buyer’s valuation according to the commonly known prior distribution of buyer’s valuation. If the buyer does not get any additional information about her true valuation, both the buyer and the seller will have zero surplus as there will be no trade. If the buyer’s true valuation happens to be above the seller’s valuation, then there will be gains from trade left on the table. Now, suppose that the buyer can get more information by making an offer to the seller or by letting the clock tick in an ascending-clock bargaining model. As this “experimentation” facilitates learning, this makes making offers beneficial to the buyer. This implies that, compared to models under full rationality, players are more willing to make offers in our model. As a result, when bargaining provides a learning environment for players about their own types, the Myerson–Satterthwaite results can be circumvented and we can design mechanisms that have ex post efficient equilibria.====Gradual learning of one’s own preference from a “posted” price in our models is similar to the learning process in Hossain (2008). In our learning process, the offer a player has made or can make (for the ascending clock) can be thought of as an anchor or reference point as in Tversky and Kahneman (1974). A player with imperfect information in our model anchors her valuation around the current price and learns whether her true valuation is above or below this anchor.==== ====The change in uncertainty in our model comes from changes in the possible trade prices unlike in the model by Yildiz (2002) where uncertainty about the value of an asset is exogenously reduced over time. Our model also has a flavor of information acquisition in private-value auctions as in Compte and Jehiel, 2004, Compte and Jehiel, 2007 and Rezende (2018). Our approach is somewhat different from that by Esponda (2008) who assumes that agents are naive who do not take the information content of other players into account. Rather, as players are not completely informed about their own preferences, their actions actually do not have as much informational content. Interestingly, this leads to the existence of a more desirable equilibrium rather than the opposite. Two recent papers have shown that Myerson–Satterthwaite results in bilateral trading can be circumvented by relaxing some assumptions. Wolitzky (2016) present conditions under which ex post bilateral trading mechanism can be designed with maxmin agents. Allowing for non quasi-linear utilities and different wealth levels, Garratt and Pycia (2016) show that efficient trade can happen if players’ utilities are not affected too much by their private information.====The next section presents a model where valuations are drawn from a discrete distribution in a standard infinite-horizon two-sided incomplete information bargaining game. In that game, the buyer knows her valuation imperfectly and makes all the offers. She gets more information about her valuation through the bargaining process. Section 3 investigates the case of two-sided imperfect information where valuations are drawn from continuous distributions and both players learn more about their valuations as the sets of possible offers they can make shrink. We illustrate the existence of an efficient mechanism with a stylized bargaining game. Finally, Section 4 concludes the paper. All proofs are in the Appendix.",Bargaining with learning,https://www.sciencedirect.com/science/article/pii/S0304406819300217,May 2019,2019,Research Article,310.0
"Coqueret Guillaume,Tavin Bertrand","EMLYON Business School, 23 Avenue Guy de Collongue, 69130 Ecully, France","Received 11 August 2017, Revised 26 December 2018, Accepted 6 February 2019, Available online 18 February 2019, Version of Record 8 March 2019.",https://doi.org/10.1016/j.jmateco.2019.02.001,Cited by (1),". We then introduce the characteristic ratio of an investment strategy, which involves the average dividend yields of the risky assets and the investors’ portfolio compositions. When equilibrium returns are uniform and when the number of assets is not smaller than the number of agents, the equilibrium with a single survivor can only be stable if the survivor has the maximal characteristic ratio. Moreover, we prove that this stability criterion holds as long as the noise in the system is sufficiently small. We confirm all our findings with a thorough empirical analysis of numerical simulations, with and without noise. All in all, our results form a theoretical rationale for portfolio strategies tilted towards firms that pay high dividend yields.","The purpose of most mathematical models is to propose an intelligible – thus simplified – version of an observable phenomenon, for example in physics, biology, finance or economics. The construction of any model is the result of an inevitable trade-off. A simplistic model will be computationally tractable, but will often fail to capture some desired properties of the phenomenon being modeled. Complicated models are usually built to overcome this limitation, but they are, by nature much harder to handle: closed-form solutions do not exist and they can only be quantitatively characterized with the help of numerical methods, such as discretizations, simulations, approximations, etc. In the extreme case when the defining assumptions are very general, and hence ambitious, the solution cannot be described beyond its existence. Thus, it is a genuine challenge to propose a framework for which intelligible relationships can be analytically deduced from realistic, non-trivial assumptions.====In this paper, we make an attempt in this direction by generalizing the economic setting of Anufriev and Bottazzi (2010) to the case when more than one risky asset can be traded. We provide comprehensive insights on the links between asset returns, dividend yields, portfolio compositions and market shares when the equilibrium is of procedural type. We also determine the connection between market selection, investment strategies and asset characteristics.====The themes we discuss are strongly related to the literature on Heterogeneous Agent Models (HAMs). While most frameworks are initially intended for multiple agents and assets, the main contributions are usually obtained for a limited number of assets and/or agents. For instance, the seminal paper by Brock and Hommes (1998) includes only one risky asset and up to four agent types.==== ==== These agent types often correspond to stylized investment strategies in which investors believe that prices either revert to their fundamental values (fundamentalists), confirm recent tendencies (trend-chasers) or oscillate in the short term (contrarians).====The purpose of the present paper is to allow for a larger palette of assets and agent demands. Some contributions in this direction include the beliefs-based CAPM of Chiarella et al., 2010, Chiarella et al., 2013, the very complete chapter of Evstigneev et al. (2009), the empirical tests of Coqueret (2017) and the demand-driven model of Koijen and Yogo (2019).====Our study is closest to that of Anufriev and Bottazzi (2010), as we propose an extension of their economic setting. Nevertheless, while they focus on the characterization and stability of equilibria, we take another route and shed some light on market selection and deterministic skeletons. Our approach also strongly relates to that of Anufriev et al. (2012), but in contrast to their results, we will show that sometimes surviving agents can have inhomogeneous demands. Also, even though we do provide insights on the relationship between asset returns, it is not the primary goal of the paper.====Rather, we choose to investigate the evolution of market shares and our interest lies in their terminal values. The topic of market selection is increasingly popular, especially in evolutionary finance, and we refer to Yan (2008), Palczewski and Schenk-Hoppé (2010) and Kogan et al. (2016) for models in continuous time and to Amir et al. (2005), Hens and Schenk-Hoppé (2005), Amir et al. (2013) and Bottazzi and Dindo (2014) for models in discrete time. Earlier seminal models of market selection, e.g., Blume and Easley (1992) and Sandroni (2000), evaluate market selection based on agents’ consumption choices and on the accuracy of their predictions. In these models, trajectories are exogenous to agent demands: they depend on states of nature.====In sum, our framework is that of Anufriev and Bottazzi (2010), but the main topic is market selection, as in Evstigneev et al. (2008). The main difference between the two models lies in the complement to risky assets. In Evstigneev et al. (2008), what is not invested on the market is consumed while in the present paper, the portion that is not invested in risky assets goes to the risk-less asset. As such, our model is purely financial in nature.==== ====Lastly, the final theme we discuss is the approximation of random systems by their deterministic skeletons. The literature provides results in this direction. In Zhu et al. (2011b), the authors investigate the relationship between the deterministic skeleton of a heterogeneous agent model and its stochastic counterpart, namely when noise is added to the dividend process. To do so they rely on results obtained in Zhu et al. (2011a). The textbook Freidlin and Wentzell (2012) details an analysis of the effect of random noise added to dynamical systems in continuous time. In Dieci et al. (2006), the authors rely on the analysis of the deterministic skeleton as well as simulations to study the evolution of the market share of different types of traders. They argue that the deterministic skeleton plays an important role in understanding the dynamics of the stochastic system. The concept of deterministic skeleton has also been used in operations research, for example in Besbes and Zeevi (2012). Finally, the deterministic approximation of a stochastic system is also addressed by Benaïm and Weibull (2003), but through the lens of game theory.====In the layout of our results, we start by considering two different cases. When aggregate demands are all non-null, we are able to characterize a condition under which an agent will survive at equilibrium: his profits are persistently higher than the wealth-weighted average profit. If agents have all the same level of leverage in risky assets (or, say, consumption rate as in Evstigneev et al. (2008)), then this strategy is to invest in the highest dividend paying asset in a proportion that is arbitrarily close to 100%. The second case is when demands violate the no short-selling restriction. It can then happen that the model becomes degenerate. But when it does not, interesting patterns emerge and we illustrate them in the case of deterministic skeletons. Unfortunately, it is then much harder to determine a dominating strategy. In fact, it is highly probable that domination in this case is also determined by initial market shares.====Our contributions can be summarized as follows. We extend the model of Anufriev and Bottazzi (2010) to the case with many assets. We discuss the configurations in asset demands which imply homogeneity or heterogeneity in equilibrium price returns. We provide and discuss the corresponding equilibrium relationships between returns, demands, market shares and dividend yields. Finally, we study market selection under deterministic skeletons. When aggregate demands are non null, a criterion that differentiates strategies is able to warrant survival upon equilibrium. Furthermore, we provide a characterization of the noise in the system under which survival holds in some almost-sure sense. When some aggregate demands are null, this criterion fails and survival will be conditioned on initial market shares.====The remainder of the paper is structured as follows. In Section 2, we present the model and we obtain the dynamics of the key variables under the condition ensuring that prices remain positive. In Section 3, we characterize the PCE of our market. To do so, we identify two situations whether there exists at least an asset with zero aggregate demand or not. In Section 4 we deal with the discrete dynamics of our market model by means of the associated Jacobian matrix. Market selection and stability of fixed-points with a single survivor are solved in Section 5 where we introduce the notion of characteristic ratio associated to a strategy. In Section 6, we provide empirical illustrations of how the market dynamics behave in the two situations, with and without zero aggregate demand assets. Moreover, in the latter two sections, we clarify the conditions under which random shocks on dividends do not affect our asymptotic results. Finally, we conclude in Section 7. All proofs are gathered in the Appendix A Proof of, Appendix B Proof of, Appendix C Proof of, Appendix D Proof of, Appendix E Proof of, Appendix F Properties of M, Appendix G Proof of, Appendix H Proof of, Appendix I Proof of, Appendix J Diagonal demands, Appendix K Evolution of market shares upon equilibrium, Appendix L Replacing the safe asset by consumption of the paper.","Procedural rationality, asset heterogeneity and market selection",https://www.sciencedirect.com/science/article/pii/S0304406819300205,May 2019,2019,Research Article,311.0
"Raurich Xavier,Seegmuller Thomas","Departament d’Economia and CREB, Universitat de Barcelona, Spain,Aix-Marseille Univ., CNRS, EHESS, Centrale Marseille, AMSE, 5 Boulevard Maurice Bourdet CS 50498 F-13205 Marseille cedex 1, France","Received 23 May 2018, Revised 25 November 2018, Accepted 24 January 2019, Available online 16 February 2019, Version of Record 8 March 2019.",https://doi.org/10.1016/j.jmateco.2019.01.007,Cited by (1),"As it is documented, households’ investment in their own education (human capital) is negatively related to the number of children individuals will have and requires some loans to be financed. We show that this contributes to explain episodes of bubbles associated to higher growth rates. This conclusion is obtained in an overlapping generations model where agents choose to invest in their own education and decide their number of children. A bubble is a liquid asset that can be used to ","Caballero et al. (2006) and Martin and Ventura (2012) show that episodes of bubbles are associated with larger growth rates. This empirical fact contradicts the results obtained in seminal contributions, where the existence of a bubble has a crowding-out effect on capital accumulation (see Tirole (1985) with exogenous growth or Grossman and Yanagawa (1993) with endogenous growth). Many recent papers have tried to conciliate the theory with the empirics, by providing some relevant explanations on the growth enhancing role of a bubble. Some of the underlying mechanisms are based on the existence of heterogeneous productive investments and bubble shocks (Martin and Ventura, 2012), the existence of financial constraints (Fahri and Tirole, 2012, Hirano and Yanagawa, 2017, Kocherlakota, 2009, Miao and Wang, 2018), the difference between liquid and illiquid assets (Raurich and Seegmuller, 2015, Raurich and Seegmuller, 2017), the existence of a bubble on a productive asset (Olivier, 2000), among others.====In this paper, we provide a new mechanism explaining that bubbles are associated to episodes of higher growth. It is based on the trade-off faced by households between investing in education (human capital) and having children. The bubble is interpreted as the liquid asset used either for loans or for saving and that modifies this trade-off. Two pieces of evidence provide support to our mechanism relating investment in education with bubbles.====First, there is a trade-off between investing in education and having children. As it is highlighted by Martinez et al. (2012),==== ==== women and men with a higher level of education have fewer children. For instance, in US during the period 2006–2010, the average number of children ever born or fathered for women aged 22–44 years is 2.5 when the woman has no high school diploma or equivalent, is 1.8 when she has a high school diploma or equivalent, is 1.5 when she was in some college, and is only 1.1 when she has a Bachelor’s degree or a higher diploma. We observe the same trend for men. Similar findings can be found in Jones and Tertilt (2008) or Preston and Hartnett (2011). This evidence clearly indicates that there exists a negative link between investment in education done at the beginning of the active life and the number of children the household will have later.====Second, any investment in education at the beginning of the active life requires some loans to finance it. This may be illustrated by the case of student loans in the US. These loans have drastically increased since the beginning of 2000s, to become larger than $1 trillion in 2013 (Avery and Turner, 2012, Dynarski, 2014). This amount is far from being negligible in comparison to other types of consumer loans, like auto loan or credit card.==== ====The previous evidence suggests that loans may facilitate investment in education, modify fertility and, finally, affect the economic growth rate. In this paper, we consider that the support of the loan is a liquid asset without fundamental value, i.e. it is a bubble when it is positively valued. As mentioned, this asset can be used to finance investment in education, like fees for instance, which will enhance growth. However, it can also be a mean to save and transfer purchasing power to finance rearing costs of having children and consumption at the retired age. In this case, the introduction of this asset may increase population and reduce economic growth.====The purpose of this paper is to analyze the effect of the bubble on growth, through its effects on human capital and fertility. To this end, we develop an overlapping generations model with three-period lived agents. A household invests in education when young and chooses the number of children when adult, facing a time cost of rearing children. She is working when young and adult, while she is engaged in home production when old. In addition, each household trades the liquid asset when young and adult to take out loans or transfer purchasing power to finance some expenses, like the rearing costs. Finally, firms produce the final good using human capital and a linear technology that makes sustained growth possible.====Our main concern is to investigate when the existence of a bubble promotes growth. In such a case, we say that the bubble is productive. We show that this happens when the time cost of rearing a child is high enough. Indeed, when the time cost per child is low, the number of children is large and the total cost of rearing children is also high. As a consequence, the bubble is used to transfer wealth from young to middle-age, implying less investment in education. The bubble has a crowding-out effect on human capital and it reduces growth. On the contrary, when the time cost of rearing a child is high, the number of children is small and the total cost of rearing children is also small. As a consequence, the bubble is used to transfer resources from adult to young households. In this case, young agents take out some loans being short-sellers of the bubble, to increase their investment in education. The bubble has a crowding-in effect on human capital, which means that it is productive. Finally, we show that the bubble can be productive even if the young households are not short-sellers of the bubble. This situation happens when the time cost of rearing a child takes intermediate values.====Our model also allows us to contribute to the debate on the link between population size and the asset price (Abel, 2001, Geanakoplos et al., 2004, Poterba, 2005). The main effect of population on asset valuation discussed in this literature is based on the number of savers with respect to dissavers. A relative larger number of savers raises the demand of asset and, therefore, its price. We associate the asset price to the bubble on the liquid asset and a larger ratio of adult over young households means that the number of buyers of this asset is larger. We observe the same link found by Geanakoplos et al. (2004) between the asset price and the demography, i.e. a positive relationship between the value of the asset and the ratio of adult over young households.====This paper is organized as follows. In the next section, we present the model. In Section 3, we analyze the economy without bubble. In Section 4, we characterize an equilibrium with a bubble. In Section 5, we show the existence of an asymptotic bubbly balanced growth path (BGP). We analyze whether a bubble may be productive in Section 6. In Section 7, we interpret our results according to the crowding-out versus crowding-in effects. Concluding remarks are provided in Section 8, while some technical details are relegated to an Appendix.",Growth and bubbles: Investing in human capital versus having children,https://www.sciencedirect.com/science/article/pii/S0304406819300163,May 2019,2019,Research Article,312.0
"Liu Liqun,Wong Kit Pong","Private Enterprise Research Center, Texas A&M University, 4231 TAMU, College Station, TX 77843, USA,Faculty of Business and Economics, University of Hong Kong, Pokfulam Road, Hong Kong","Received 12 November 2018, Revised 3 December 2018, Accepted 28 January 2019, Available online 13 February 2019, Version of Record 8 March 2019.",https://doi.org/10.1016/j.jmateco.2019.01.008,Cited by (7),"th-degree mixed risk aversion in the Arrow–Pratt tradition, which includes many competing notions of greater higher-degree (absolute) risk aversion proposed in the extant literature as special cases. Properties of greater ====th-degree mixed risk aversion are studied, a choice-based characterization is established, and several applications are presented.","While economists have long recognized that risk aversion plays a pivotal role in decision making under uncertainty (Bernoulli, 1954), measuring the intensity of risk aversion and investigating its determinants are relatively new scientific endeavors.==== ==== Consider two individuals, ==== and ====, with increasing utility functions, ==== and ====, defined over their wealth, ====, respectively. The seminal work of Arrow (1971) and Pratt (1964) in the expected utility framework provides the following three equivalent statements of ==== being more (second-degree) risk averse than ====:====The theoretical foundation of comparative second-degree risk aversion laid by Arrow (1971) and Pratt (1964) is known to be necessary and sufficient for a variety of interesting comparative statics results. For example, Arrow (1971) shows that, in a two-asset portfolio choice problem with one safe and one risky assets, individual ==== invests less in the risky asset and more in the safe asset than individual ====.====Recent experimental studies have demonstrated a salient aversion to risk increases of third and even higher degrees.====  Accompanying the discovery of higher-degree risk aversion, a natural question arises as to how to measure and compare the intensity of risk aversion beyond the second degree. In this paper, we focus on how to measure and compare the intensity of higher-degree absolute risk aversion, and use “risk aversion” as a synonym for “absolute risk aversion”.====Moving from the second to the third degree, the extant literature has thus far proposed competing notions of greater third-degree (or greater downside) risk aversion. For example, following the spirit of statement (i), Modica and Scarsini (2005) define individual ==== to be more third-degree risk averse than individual ==== if the former is always willing to pay a (weakly) larger risk premium than the latter to avoid a third-degree increase in risk in the sense of Menezes et al. (1980). Extending Ross (1981), Modica and Scarsini (2005) characterize their notion of greater third-degree risk aversion by a third-degree Ross condition on the two utility functions.====  On the other hand, following the spirit of statement (ii), Keenan and Snow (2016) combine ==== and ==== to define ==== being more third-degree risk averse than ====. They characterize their notion of greater third-degree risk aversion by consistent dislike of changes in the wealth distribution that induce third-order stochastic dominance shifts in the utility distribution. Finally, following the spirit of statement (iii), Chiu (2005) uses ====, whereas Crainich and Eeckhoudt (2008) use ====, to define ==== being more third-degree risk averse than ====.==== 
 Chiu (2005) characterizes his notion of greater third-degree risk aversion by a single-crossing property that any shifts in the wealth distribution have to satisfy.====The approach that follows the direction represented by statement (i) to generalizing greater second-degree risk aversion to higher degrees seems to be the most successful. Specifically, Denuit and Eeckhoudt (2010b), Jindapon and Neilson (2007), and Li (2009) provide a Ross-type characterizing condition on utility functions under which individual ==== is always willing to pay a (weakly) larger risk premium than individual ==== to avoid an ====th-degree increase in risk in the sense of Ekern (1980).====  We refer to this notion of greater ====th-degree risk aversion as greater ====th-degree Ross risk aversion. In contrast, the other two approaches to generalizing greater second-degree risk aversion, along the directions represented by statements (ii) and (iii) above and confined in the Arrow–Pratt tradition, have not been adequately explored beyond the third degree.====  The purpose of this paper is to develop a unified framework of greater ====th-degree mixed risk aversion, which includes Chiu (2005), Crainich and Eeckhoudt (2008), and Keenan and Snow (2016) as special cases of the third degree.====We need to introduce two concepts in order to describe what can be achieved with the general notion of greater ====th-degree mixed risk aversion. First, a utility function, ====, is said to exhibit ====th-degree mixed risk aversion if ==== for all ==== and for all ====, where ==== is the ====th derivative of ==== and ====.====  Second, generalizing the Arrow–Pratt measure of absolute risk aversion, ====, to higher degrees, the measure of ====th-degree absolute risk aversion is defined as ====. It is evident that ==== is completely general and unifies all measures found in the extant literature.====We show that our notion of greater ====th-degree mixed risk aversion ranks utility functions in a strict partial ordering, i.e., the ranking is irreflexive, anti-symmetric, and transitive, which is necessary for the concept to be useful for comparative statics analysis. Furthermore, it has some nice properties. First, ====th-degree mixed risk aversion embedded in the reference utility function, ====, is preserved when ==== is transformed into ==== that is more ====th-degree mixed risk averse than ====. Second, all measures of ====th-degree absolute risk aversion for ==== are uniformly larger than those for ====, i.e., ==== for all ==== and for all ====. We also provide a choice-based characterization for greater ====th-degree mixed risk aversion.====While our approach is completely general, two special cases that have already received some attention in the extant literature are in order. First, when ====, ==== is more ====th-degree mixed risk averse than ==== if there exists a transformation function, ====, such that ==== and ==== for all ==== and for all ====. Obviously, the notion of strong increases in downside risk aversion proposed in Keenan and Snow, 2016, Keenan and Snow, 2018 is a special case of greater ====th-degree mixed risk aversion wherein ====. Second, when ====, ==== is more ====th-degree mixed risk averse than ==== if ==== for all ====, corresponding to the greater ====th-degree Arrow–Pratt risk aversion proposed in Denuit and Eeckhoudt (2010a), Huang et al. (2017), and Jindapon and Neilson (2007), and including the notion of greater third-degree risk aversion proposed in Chiu (2005) as a special case wherein ====.====Two recent papers by Wong, 2018a, Wong, 2018b are closely related to ours. Wong (2018a) defines the ====th-degree utility premium as the pain associated with facing the passage from a more favorable risk to a less favorable risk, where the risk increase is specified by a simple increase in ====th-degree risk. He further defines the ====th-degree prudence utility premium as the increase in pain when the individual suffers a sure loss. He shows that the ====th-degree utility premium, normalized by the ====th derivative of the utility function evaluated at the initial wealth has the same ranking as that corresponds to the measure of ====th-degree Arrow–Pratt absolute risk aversion. On the other hand, the ====th-degree prudence utility premium, normalized by the ====th derivative of the utility function evaluated at the initial wealth, has the same ranking as that corresponds to the measure of ====th-degree Arrow–Pratt absolute risk aversion. Wong (2018b) applies these concepts to examine the effect of increased higher-order risk on the precautionary saving motive. He derives the necessary and sufficient condition under which saving increases in response to an increase in interest rate risk via ====th-order stochastic dominance, which is shown to describe a trade-off between a prudence effect that favors precautionary saving and a risk aversion effect that limits precautionary saving.====Our paper focuses on the measures of Arrow–Pratt absolute risk aversion of higher orders for the sake of doing comparative risk aversion analysis. This is in contrast to Wong (2018b) that focuses on changes in higher-order risk in a specific (saving/consumption) decision problem. We characterize greater ====th-degree mixed risk aversion in the Arrow–Pratt sense by the ranking of risk distributions of a reference individual that would be obeyed by all individuals who are more ====th-degree mixed risk averse than the reference individual but not vice versa. Wong (2018a) offers an alternative characterization using normalized ====th-degree utility premiums for the special case that ====, where the normalization using the ====th derivative of the utility function evaluated at the initial wealth is an arbitrarily chosen procedure that has no justification.====The paper is organized as follows. In Section 2, we define ====th-degree mixed risk aversion and ====th-order stochastic dominance, where these two concepts are shown to be closely related. In Section 3, we define our general notion of greater ====th-degree mixed risk aversion, and examine its properties. A choice-based characterization of greater ====th-degree mixed risk aversion is provided. Then, two special cases of greater ====th-degree mixed risk aversion, one for ==== and the other for ====, are analyzed in more detail. Section 4 offers a few applications. Section 5 concludes.",Greater Arrow–Pratt (Absolute) risk aversion of higher orders,https://www.sciencedirect.com/science/article/pii/S0304406819300175,May 2019,2019,Research Article,313.0
Payró Fernando,"Department of Economics, Boston University, United States","Received 17 March 2018, Revised 16 January 2019, Accepted 31 January 2019, Available online 12 February 2019, Version of Record 7 March 2019.",https://doi.org/10.1016/j.jmateco.2019.01.010,Cited by (0),"This paper provides, for finite sets of choice data, revealed preference characterizations for the additive representations considered in Dekel et al. (2001). For a particular class of data sets, it is shown that the characterizing conditions can be reformulated as ","Kreps (1979) studies preferences over menus of deterministic alternatives. He shows that a simple set of axioms characterizes a representation that can be interpreted as if the agent is uncertain about her future tastes. This taste uncertainty is summarized by a set of possible future preferences which is referred to as the subjective state space. In Kreps’ model, the subjective state space is not completely pinned down by the preference over menus. Dekel et al. (2001) (henceforth DLR) extend Kreps’ analysis to menus of lotteries. This richer domain allows them to show that, under certain assumptions, the subjective state space is “essentially unique” given the preference over menus of lotteries.====Although DLR provide axiomatic foundations for several representations that have a subjective state space component, we only consider the additive ones which have the form ====
 where ==== is a menu of lotteries, ==== is a non-empty set, ==== is an expected utility function for every ====, and ==== is a finitely additive (signed) measure over ====. As is typical of the axiomatic approach, verification of their axioms requires that the entire preference order be observable. This paper provides a corresponding revealed preference analysis assuming that only finitely many choices are observed. In particular, we assume that we are given finitely many observations, each observation consists of a ====, that is, a collection of menus of lotteries, and a ==== from each budget.====Our main result shows that a suitable adaptation of the Independence axiom for menus of lotteries to finite choice data sets characterizes the general additive representation. We also provide necessary and sufficient conditions for the cases in which ==== is a positive measure; and when the subjective state space ==== is a singleton.====In their framework, DLR show that there is a distinction between preferences that have an additive representation with an infinite state space and preferences that have a representation with a finite state space. An implication of our results is that for finite data sets there is no empirically meaningful distinction.====Our analysis builds on the revealed preference analysis for vNM utility theory carried out by Fishburn, 1975, Border, 1992 and Kim (1996). Fishburn (1975) considers lotteries over a finite abstract prize space; Border (1992) assumes the prize space is a compact subset of the real line and studies the case in which the vNM utility index is increasing. Kim (1996) generalizes Fishburn (1975) and Border (1992) by considering lotteries over an abstract compact metric space. As in Fishburn (1975), we consider an abstract finite prize space but we generalize the choice domain to menus of lotteries. Restricted to singleton menus, our characterizing conditions are equivalent to Fishburn’s condition for vNM utility theory. To the best of our knowledge this is the first paper to provide a revealed preference analysis for any model with a ==== state space component.====Finally, for the case in which each menu of lotteries is either finite or finitely generated (that is, equal to the convex hull of a finite set), we show how our results can be reformulated as a nonlinear system of inequalities and discuss the numerical methods that can be used to verify them. Hence, for this case, our results provide a test that is, in principle, implementable. See Varian, 1983, Chiappori, 1988, Diewert, 2012 and Demuynck and Seel (2018) for other instances in the revealed preference literature where characterizing conditions lead to nonlinear inequalities.====The paper proceeds as follows. Section 2 gives the general model. Section 3 states the main results. Section 4 contains a discussion of our results. Appendix A contains the proofs of our results and Appendix B describes the aforementioned systems of inequalities and applicable numerical methods.",Revealed preference and the subjective state space hypothesis,https://www.sciencedirect.com/science/article/pii/S0304406819300199,May 2019,2019,Research Article,314.0
"Raad Rodrigo,Woźny Łukasz","Department of Economics, Federal University of Minas Gerais, Antônio Carlos Avenue, 6627 Belo Horizonte, MG, Zip 31270-901, Brazil,Department of Quantitative Economics, Warsaw School of Economics, al. Niepodległości 162, 02-554 Warszawa, Poland","Received 13 March 2015, Revised 8 January 2019, Accepted 24 January 2019, Available online 12 February 2019, Version of Record 8 March 2019.",https://doi.org/10.1016/j.jmateco.2019.01.006,Cited by (0),This paper analyzes the Lucas tree model with heterogeneous agents and one asset. We show the existence of a minimal state space Lipschitz continuous recursive equilibrium using Montrucchio (1987) results. The recursive equilibrium implements a sequential equilibrium through an explicit functional equation derived from the ,"Since the work of Lucas and Prescott (1971) and Prescott and Mehra (1980), recursive equilibrium has been a key focal point of both applied and theoretical work in characterizing sequential equilibrium for dynamic general equilibrium models in such fields as macroeconomics, international trade, growth theory, industrial organization, financial economies, and monetary theory. Specifically, in general dynamic models with infinitely lived agents economists have focused on so-called minimal state space recursive equilibrium, i.e. a pair of stationary transition and policy functions that relate the endogenous variables in any two consecutive periods, defined on the natural state space. Apart from its simplicity, (minimal state space) recursive equilibrium is also widely used in applied or computational works, as powerful recursive methods provide algorithms to compute it efficiently. Results regarding equilibrium existence are necessary prerequisites for a theoretical and computational analysis, however.====Unfortunately, there are well known examples where recursive equilibria (in specific function spaces) in dynamic economies are non existent (see Santos (2002) for economies with taxes, Kubler and Schmedders (2002) for economies with incomplete asset markets or Krebs (2004) for economies with large borrowing limits). Some recent attempts that address the question of minimal state space recursive equilibrium existence and its approximation, include contributions of Datta et al. (2002) and Datta et al. (2018) for models with homogeneous agents, who propose a monotone maps method applied on the equilibrium version of the household first order conditions and prove equilibrium existence along with its comparative statics, using versions of Tarski fixed point theorem. Unfortunately, there are no known results on how to extend these techniques to models with heterogeneous agents and multiple assets. Next, Brumm et al. (2017) apply some powerful results from stochastic games literature and by adding sufficient shocks prove existence of a recursive equilibrium using operators defined on households first order conditions and applying Kakutani–Fan–Glicksberg fixed point theorem on the operator defined on the Walrasian auctioneer problem. The underlying topology is weak-star and the obtained recursive equilibrium a measurable map on the state space. The measure theoretical results together with recent contributions in stochastic games allow to prove minimal state space recursive equilibrium existence without sunspots or public coordination devices. More specifically, one of the canonical equilibrium models analyzed in the literature that significantly influenced the fields of financial economics, macroeconomics, monetary theory, optimal taxation and econometrics, was developed by Lucas Jr. (1978). However, despite the model’s wide application, typical assumptions involve a representative agent. In fact, presence of infinitely lived ==== agents can be the key to explain several peculiarities of market frictions from the perspective of models with rational expectations. Apart from mentioned Brumm et al. (2017) contribution, there are only few known results concerning recursive equilibrium existence in the Lucas three model with heterogeneous agents. These include Raad (2016), who show the existence of a possibly non-continuous recursive equilibrium with a minimal state space, however, the model assumes that agents have exogenous beliefs on portfolio transitions.==== ==== Kubler and Schmedders (2002) present an example of an infinite-horizon economy with Markovian fundamentals, where the recursive competitive equilibrium (defined on a state space of equilibrium asset holdings and exogenous shocks) does not exists. In their example, there must exist two different nodes of a tree such that along the equilibrium path the value of the equilibrium asset holdings is the same but such that there exist more than one equilibrium for both of the continuation economies. Although they claim that a slight perturbation in individual endowments will restore the existence of a weakly recursive equilibrium, we detail the set of conditions that rules out (Kubler and Schmedders, 2002) example from the model analyzed in our paper.==== ====In this paper, we take a different approach to show the existence of a minimal state space recursive equilibrium. By minimal state space, we mean the previous period asset allocation and current state of nature.==== ==== We proceed basically in five steps. First, we consider a class of transition and policy functions that are Lipschitz continuous. This allows us to obtain a sup norm compact set of candidate equilibrium functions. Second, the adopted framework and the recursive demand are constructed through a selector of the Bellman correspondence which is defined ==== using the first order conditions. This is a new approach and allow to compute equilibria with occasionally binding constraints.==== ==== Following Montrucchio (1987) results, we assume strong conditions on the primitives to ensure a Lipschitz condition of the demand is satisfied.==== ==== In order to do so, we restrict our attention to models with single asset.==== ==== Third, another problem faced in this paper is the expansion of the implied Lipschitz constants. Here we assume conditions on the primitives that assure our operator maps back to the space of Lipschitz functions with the same constant. We define upper and lower bounds of the domains so that the effective Lipschitz constants are well behaved (i.e. non-expanding). Fourth, the fixed point operator is defined using the optimization problem (defined on the candidate space of Lipschitz continuous functions) of the Walrasian auctioneer. As a result, apart from proving existence, we also establish that the constructed equilibrium is in fact Lipschitz continuous. Fifth, we use a constructive argument to explain how the sequential equilibrium can be implemented recursively by showing the consecutive relations among the endogenous variables explicitly.====Working with Lipschitz continuous functions and a sup norm, although restrictive per assumptions, allows us to avoid typical convergence problems associated with working with the set of feasible measurable functions endowed with the weak-star topology. In fact, concerning the set of measurable functions defined over uncountable domain, the Mazur lemma states that a weak-star cluster point of any subset is a pointwise cluster point of its convex hull. However, a weak-star cluster point of a typical subset may not be a pointwise cluster point of it. Importantly, this problem is present even when working in the space of randomized policies. One way to overcome this problem is to introduce some convexification devices, either via sunspots (see Duffie et al. (1994)) or external noise (see Brumm et al. (2017)) in stochastic models. Our results work for deterministic and stochastic economies and hence complement (Brumm et al., 2017) contribution. Moreover, and perhaps more importantly, working with Lipschitz continuous functions allows us to obtain a tractable and approximate space of equilibrium candidates. Although we cannot verify whether our fixed point operator is a contraction, working with Lipschitz equilibrium functions is still an important numerical advantage of our approach,==== ==== as it is easier to characterize numerically Lipschitz function as opposed to a function that is only known to be measurable. As we do not use consumers’ first order conditions, such sequential equilibrium can be computed using the dynamic programming approach and thus does not embody cumulative errors in the long run as noted by Kubler and Schmedders (2005).====Including this introduction, the paper is organized into five sections. Section 2 establishes the model. In Section 3, we define the recursive and sequential equilibrium concepts and show how they are related. Section 4 shows the existence result. We provide explicit conditions on the primitives that guarantee Lipschitz continuity of the demand correspondence on a suitable set of prices bounded away from zero and infinity. The conclusions are addressed in Section 5.",Lipschitz recursive equilibrium with a minimal state space and heterogeneous agents,https://www.sciencedirect.com/science/article/pii/S0304406819300151,May 2019,2019,Research Article,315.0
"Aguiar Victor H.,Kimya Mert","Department of Economics, University of Western Ontario, Canada,Koç University, Department of Economics, Rumeli Feneri Yolu, Sarıyer 34450, Istanbul, Turkey","Received 5 December 2017, Revised 13 September 2018, Accepted 8 January 2019, Available online 4 February 2019, Version of Record 22 February 2019.",https://doi.org/10.1016/j.jmateco.2019.01.003,Cited by (8),"We characterize Simon’s (1955) search and satisficing model with an adaptive threshold and random search (SM-AT). The ==== (DM) consistent with the SM-AT is endowed with a utility function, a random search distribution, and a deterministic but menu-dependent threshold. On any given trial, the DM searches the menu and stops whenever she finds an item with a utility level that is above the threshold. This simple choice procedure accommodates the well-known compromise and attraction effects. The SM-AT is more general than the random utility model and allows for systematic departures from regularity. Its characterization lets us differentiate adaptive satisficing behavior from random preference maximization in a (limited) standard stochastic choice data set.","We study Simon’s (1955) search and satisficing model with a deterministic but adaptive threshold (SM-AT). Under the SM-AT, on any given trial, the decision maker (DM) draws a search order using a distribution over search orders in the menu and fixes a utility threshold; using this order she visits sequentially each item in the menu and compares at each step the utility of the item to the menu-dependent threshold; she stops the search whenever she finds an item that is above the menu-dependent threshold. We observe that without any restrictions on its primitives the SM-AT has no empirical content in a standard stochastic choice data set environment.====Our main contribution is to provide a complete characterization of the SM-AT under additional constraints on the random search. We consider a Full Support restriction that requires that all search orderings are drawn with positive probability, and a Fixed Distribution property that requires that the probability of each search ordering remains stable across menus. The SM-AT under Full Support with the Fixed Distribution property on its random search ordering is characterized by a generalization of the Axiom of Stochastic Revealed Preference (ASRP) that holds only for the support of the probabilistic choice rule. In addition, we need an Acyclicity requirement on the support of the probabilistic choice rule, which is a generalization of the Strong Axiom of Revealed Preference (SARP).====We believe that allowing for an adaptive threshold is important for the following reasons: First, the experimental and theoretical search literature, have acknowledged the importance of the context in the determination of the thresholds or reservation utility. In particular, there is evidence that DMs have changing thresholds (see Caplin et al., 2011, Caplin and Dean, 2011, Santos et al., 2012). In the SM-AT the DM is allowed to adjust the heuristics that she uses, according to the context or menu, in an arbitrary but deterministic way.====Second, the SM-AT probabilistic choice rule inspired by Simon’s model is a new model of stochastic choice and is able to accommodate violations of the regularity condition. The regularity condition is a key implication of the Random Utility Model.====Third, the SM-AT generalizes the main model presented in Aguiar, Boccardi and Dean (2016). Conceptually, we complete the study of Aguiar et al. (2016) on the widely known Simon’s Satisficing model for cases where the threshold is deterministic but is no longer constant. In contrast with other works on stochastic satisficing such as Aguiar et al. (2016) and Kovach and Ulku (2017), our work provides a characterization of Simon’s (1955) search model that allows us to empirically differentiate it from random utility maximization behavior in a (limited) standard stochastic choice data. Such data is increasingly popular in the economic literature.==== ==== Previously, differentiating satisficing behavior from random utility was only possible in choice process data (Caplin and Dean, 2011, Santos et al., 2012), which is harder to collect.====Section 2 presents the choice environment. We also formally introduce the SM-AT and the restrictions that we impose in its primitives. Section 3 characterizes the SM-AT. Section 4 compares both conceptually and empirically the SM-AT with a subset of the random choice rules that in our criterion is the closest to our work. Section 5 presents an overview of the computational consideration for testing the SM-AT in practice. Finally, Section 6 concludes.",Adaptive stochastic search,https://www.sciencedirect.com/science/article/pii/S0304406819300047,March 2019,2019,Research Article,316.0
Barokas Guy,"Ruppin Academic Center, Amek Hafer, Israel","Received 16 October 2017, Revised 1 November 2018, Accepted 8 January 2019, Available online 31 January 2019, Version of Record 8 February 2019.",https://doi.org/10.1016/j.jmateco.2019.01.002,Cited by (1),"In this paper, we generalize the standard revealed preference theory in line with the idea of libertarian paternalism; roughly, ==== is said to be welfare superior to ==== (denoted ====) if ==== is chosen when ==== is available, and ==== is more normative than ==== according to a possibly endogenous normative criterion (i.e. a criterion that depends on a positive model). We find testable conditions on choice behavior that are equivalent to ==== being an order. The resulting framework is applicable to many positive models of bounded rationality, with intuitive welfare implications, while maintaining a natural libertarian principle. In addition, our framework can reconcile the often-conflicting libertarian and behavioral approaches to welfare.","The revealed preference principle, one of the most influential ideas in economics, states that preferences can be deduced from observable choices. Specifically, if we observe a decision-maker choosing a choice object ==== when another choice object ==== is available (denoted ====), then we can deduce that the decision-maker prefers ==== over ====
 (Samuelson, 1938). The philosophy underlying this principle is libertarian: it reflects the judgment that the best alternative for an individual is the one that he would choose for himself. Revealed preference yields a testable condition on choice behavior, generally known as WARP, that is necessary and sufficient for its applicability (i.e. for ==== being a complete order). However, the merit of this approach is based on the pre-assumption that the decision-maker is completely rational, while abundant evidence from decision-making research suggests that the standard rational model provides an inadequate description of actual behavior (i.e. WARP is often violated).====Considerable effort has been devoted to revealed preference analyses in the presence of bounded rationality. This literature can be categorized into two main approaches. The first approach, which we refer to as “the libertarian approach to welfare,” aims to maintain the libertarian essence of Samuelson’s revealed preference. For example, Bernheim and Rangel (2009) offered an inference rule that adheres to the strict libertarian principle==== ====: any alternative chosen from an opportunity set ==== is a weak optimum within ====. This is typically a “model-free” approach that misses valuable information on choice behavior that can guide the elicitation of preferences. Consequentially, this approach results in a coarse welfare relation, which leaves many alternatives unranked (see Rubinstein and Salant, 2012 p. 377). With the aim of making finer welfare judgments, studies adopting the second approach, the “behavioral approach to welfare,” have been assessing behavior to elicit preferences. This field of literature typically provides a more inclusive theory on the individual’s behavior than pure maximization, translates this theory to axioms on choice behavior, and conditional on those, produces the preference inference rule. Note, however, that while these testable conditions are necessary for the identification of the behavioral models, they are not sufficient for welfare evaluation. For example, Manzini and Mariotti’s (2012) and Cherepanov et al.’s (2013) models have the same positive axioms, but different welfare implications. Thus, even when one finds that certain choice data satisfy conditions imposed by a behavioral theory, one might still wish to employ some extent of libertarianism when conducting a welfare analysis.====Several authors (e.g. Masatlioglu et al., 2012, Manzini and Mariotti, 2012) have suggested that the libertarian and behavioral approaches to welfare should be complementary. However, a general framework that can reconcile the two approaches does not exist==== ==== — advising this is the goal of the present paper. More specifically, we are aiming for a framework that is libertarian in nature, would make sharp welfare judgments, and would not miss the valuable information on welfare embodied in the choice data.====For this purpose, we introduce the famous idea of libertarian paternalism (LP) (Sunstein and Thaler, 2008) to the revealed preference framework. In short, LP suggests that it would still be libertarian for a social planner to influence the framing of choice in order to induce people to make more normative decisions. Accordingly, we endow the social planner in our model with a transitive and (possibly) complete normative criterion, and offer the following inference rule: ==== is welfare superior to ==== (denoted ====) if there exists an opportunity set that includes ====, in which ==== is the most normative of all chosen alternatives.====The following example illustrates how the libertarian paternalistic relation works.====We provide a testable condition on choice behavior that is equivalent to ==== being a complete order (i.e. the equivalent of WARP in our model). This condition ensures that a social planner, who uses the LP welfare relation in every choice problem, will end up with coherent preferences.====  Furthermore, it provides the ground for a more formal discussion of the ongoing debate on whether libertarian paternalism is “libertarian enough” (see, for example, Mitchell, 2004 and Sunstein and Thaler, 2003). In particular, we show that it is only under this condition that the LP relation satisfies the natural libertarian principle: any weak optimum ==== within an opportunity set ==== must be chosen voluntarily from ==== in some situations.====Our testable condition is found to be general enough to allow the application of the LP relation to many behavioral models of bounded rationality (e.g., satisficing, dynamic inconsistency, and status-quo bias). The possibility of providing complete preference inferences while still respecting the preceding libertarian principle is often enabled owing to a certain correlation between the behavioral model and the normative criterion. Specifically, in many situations, there exists a frame under which the agent behaves rationally (or close to rationally), which induces the agent to make the most normative decisions.====While LP maintains a natural libertarian principle, at the same time it could effectively utilize the information embodied in the choice data to construct the welfare relation. This is achieved by letting the normative criteria to determined endogenously, as a function of the positive models.====  For example, in the case of the familiar ====, ==== model, we choose the normative criterion by using the individual’s instant utility function (obtained by the positive model), together with the socially desirable discount factor. In other cases, when the positive model provides clear guidance for a welfare analysis (e.g., in Masatlioglu and Ok (2005)’s status-quo bias model), we simply equalize the normative criterion to the preferences obtained from the positive theory. The LP relation can then produce the same preference inferences as in the behavioral approach to welfare, as long as the latter does not violate the aforementioned libertarian principle. Thus, our approach can help decide between several positive theories that are consistent with the data based on libertarian premises. Yet, in other situations, LP applies only the welfare consequences of a particular behavioral theory that do not deprive the agent’s liberty. In the following, we utilize Example 1 to demonstrate these two cases.====We examine the implications of the LP relation for several behavioral models finding that it often provides normative justifications for commonly used welfare criteria. For example, in the status-quo bias model, we obtain a preference relation that is synonymous with the choice when there is no status-quo; and in the ====, ==== model, our results provide justifications for the popular “long-run criterion,” the use of which is often based solely on intuition (e.g., Amador et al., 2006, Carroll et al., 2009).====Finally, we show that libertarian paternalism is more appropriate when frames are menu independent (e.g., dynamic inconsistency as opposed to temptation). In particular, borrowing the limited attention model of Salant and Rubinstein (2008), we illustrate that the possibility of applying LP negatively depends on menu effects.==== ====The rest of the paper is organized as follows. In Section 2, we introduce the general setting and several examples of behavioral models and normative criteria. Section 3 is devoted to the construction of the LP welfare relation and its characterization. In Section 4, we examine the implications of the LP relation on specific behavioral models. Section 5 relates our approach to the behavioral approach to welfare. Section 6, generalizes our characterization result and uses the limited attention model to demonstrate the role of menu effects in the applicability of libertarian paternalism. In Section 7, we conclude the paper and discuss directions for future research. Proofs appear in Appendix A.",Choice theoretic foundation for libertarian paternalism: Reconciling the behavioral and libertarian approaches to welfare,https://www.sciencedirect.com/science/article/pii/S0304406819300035,March 2019,2019,Research Article,317.0
"Thirumulanathan D.,Sundaresan Rajesh,Narahari Y.","Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, 560012, India,Department of Electrical Communication Engineering, and Robert Bosch Centre for Cyber–Physical Systems, Indian Institute of Science, Bengaluru, 560012, India,Department of Computer Science and Automation, Indian Institute of Science, Bengaluru, 560012, India","Received 29 June 2017, Revised 19 December 2018, Accepted 14 January 2019, Available online 25 January 2019, Version of Record 1 March 2019.",https://doi.org/10.1016/j.jmateco.2019.01.005,Cited by (2),"We consider the problem of designing a revenue-optimal mechanism in the two-item, single-buyer, unit-demand setting when the buyer’s valuations, ====, are uniformly distributed in an arbitrary rectangle ====. We identify five simple structures, each with at most five (possibly stochastic) menu items, and prove that the optimal mechanism has one of the five structures. We also characterize the optimal mechanism as a function of ====, and ====. When ==== is low, the optimal mechanism is a posted price mechanism with an exclusion region; when ==== is high, it is a posted price mechanism without an exclusion region. Our results are the first to show the existence of optimal mechanisms with no exclusion region, to the best of our knowledge.","This paper studies the design of revenue-optimal mechanism in the two-item, one-buyer, unit-demand setting. The solution to the problem is well known when the buyer’s value is one-dimensional (Myerson, 1981). The problem however becomes much harder when the buyer’s value is multi-dimensional. Though many partial results are available in the literature, finding the general solution remains open in the two-item setting, be it with or without the unit-demand constraint.====In this paper, we consider the problem of optimal mechanism design in the two-item one-buyer unit-demand setting, when the valuations of the buyer are uniformly distributed in arbitrary rectangles in the positive quadrant having their left-bottom corners on the line ====. Observe that this is a setting that occurs often in practice. As one example, consider a setting where two houses in a locality are sold. The seller is aware of a minimum and a maximum value for each house. Further, the buyer has a unit-demand, i.e., he can buy at most one of the houses, but submits his bids for both the houses. We consider that the buyer’s valuations are uniform in the rectangle formed by those intervals. We compute the optimal mechanism for all cases when the minimum value for both the houses is the same. Another example is one where two sports team franchises in a sports league are sold to a potential buyer. The buyer needs at most one franchise, but submits his bids for both franchises.",On optimal mechanisms in the two-item single-buyer unit-demand setting,https://www.sciencedirect.com/science/article/pii/S030440681930014X,May 2019,2019,Research Article,318.0
"Thirumulanathan D.,Sundaresan Rajesh,Narahari Y.","Department of Electrical Communication Engineering, Indian Institute of Science, Bengaluru, 560012, India,Department of Electrical Communication Engineering, and Robert Bosch Centre for Cyber–Physical Systems, Indian Institute of Science, Bengaluru, 560012, India,Department of Computer Science and Automation, Indian Institute of Science, Bengaluru, 560012, India","Received 29 June 2017, Revised 19 December 2018, Accepted 14 January 2019, Available online 25 January 2019, Version of Record 23 February 2019.",https://doi.org/10.1016/j.jmateco.2019.01.004,Cited by (0),"We consider the design of a revenue-optimal mechanism when two items are available to be sold to a single buyer whose valuation is uniformly distributed over an arbitrary rectangle ====. We identify eight simple structures, each with at most 4 (possibly stochastic) menu items, and prove that the optimal mechanism has one of these eight structures. We also characterize the optimal mechanism as a function of ====. The structures indicate that the optimal mechanism involves (a) an interplay of individual sale and a bundle sale when ==== and ==== are low, (b) a bundle sale when ==== and ==== are high, and (c) an individual sale when one of them is high and the other is low. To the best of our knowledge, our results are the first to show the existence of optimal mechanisms with no exclusion region. We further conjecture, based on promising preliminary results, that our methodology can be extended to a wider class of distributions.","This paper studies the design of a revenue optimal mechanism for selling two items. While the solution to the problem of designing an optimal mechanism for selling a single item is well-known (Myerson, 1981), optimal mechanism design for selling multiple items is a harder problem. Though there are many partial results available in the literature, finding a general solution, even in the simplest setting with two heterogeneous items and one buyer, remains open.====In this paper, we consider the problem of optimal mechanism design in the two-item one-buyer setting, when the valuations of the buyer are uniformly distributed in arbitrary rectangles in the positive quadrant. In the two-item one-buyer setting, Daskalakis et al., 2013, Daskalakis et al., 2015 and Daskalakis et al. (2017) considered the most general class of distributions till date, and gave the optimal solution when the distribution gives rise to a so-called “well-formed” canonical partition (to be described in Section 3) of the support set. Giannakopoulos and Koutsoupias (2015) provided closed form solutions when ==== and the distribution of ==== satisfies some sufficient conditions. The papers Daskalakis et al., 2013, Daskalakis et al., 2015, Daskalakis et al., 2017 and Giannakopoulos and Koutsoupias (2015) rely on a result of Rochet (1987) that transforms the search for an optimal mechanism into a search for the utility function of the buyer. This function represents the expected value of the lottery the buyer receives minus the payment to the seller. The expected payment to the seller is maximized, subject to the utility function being positive, increasing, convex, and ====-Lipschitz. The above papers then identify a dual problem, solve it, and exploit this solution to identify a primal solution. We call this approach the ==== in this paper.====The duality approach developed in these papers crucially uses the assumption that the support set ==== of the distribution is ====. We are aware of only two examples where the support sets ====, and ====, for which the exact solutions are known, are not bordered by the coordinate axes. These were considered by Pavlov (2011) and Daskalakis et al. (2017), respectively. Daskalakis et al., 2013, Daskalakis et al., 2015 and Daskalakis et al. (2017) do consider other distributions but these distributions must satisfy ==== on the boundaries of ====, where ==== is the normal to the boundary at ====. The uniform distribution on arbitrary rectangles (which we consider in this paper) has ==== in general on the left and bottom boundaries, and this requires additional nontrivial care in its handling.",Optimal mechanisms for selling two items to a single buyer having uniformly distributed valuations,https://www.sciencedirect.com/science/article/pii/S0304406819300138,May 2019,2019,Research Article,319.0
Jia Hao,"Department of Economics, Deakin University, 1 Gheringhap Street, Geelong, VIC 3220, Australia","Received 29 June 2018, Revised 15 October 2018, Accepted 8 January 2019, Available online 18 January 2019, Version of Record 31 January 2019.",https://doi.org/10.1016/j.jmateco.2019.01.001,Cited by (1),"Many economic models, especially in two-sided matching literature, involve breaking down a supermodular function into two supermodular functions according to a certain allocation rule. The conventional wisdom is to invoke the Nash bargaining solution, which boils down to an even split rule within the transferable utility framework. This paper rationalizes the use of the Nash bargaining solution in a two-sided matching model by showing that the even split rule is the ==== allocation which allows the net benefit functions to inherit supermodularity from the joint surplus function, which is necessary to ensure positive assortative matching.","Supermodularity, as a convenient and increasingly popular property, has been featured in many different areas of economics. For instance, in parametric optimization, when the feasible set has a lattice structure and the objective function is supermodular, the tools of ==== (Topkis, 1978, Milgrom and Shannon, 1994, Topkis, 1998) provide a convenient means for making predictions about the parametric dependence of the solution set and of the value function. Another example is the supermodular games (also known as games with strategic complementarities, see Topkis, 1979, Vives, 1990, Milgrom and Roberts, 1990 among others), where the strategy spaces have a lattice structure and payoff functions feature certain supermodularity properties.====Due to its generality and explanatory power, supermodular payoff functions (and their variations) have become a conventional assumption adopted in various matching models. The main characteristic of these models is that they have monotone reaction curves, reflecting a complementarity relationship between one’s own actions and potential matching partners’ actions, which makes coordination a desirable outcome. In his seminal paper, Becker (1974) shows that, in a decentralized friction-free matching market with transferable utility, if the technology exhibits strategic complementarities (i.e., supermodularity of the joint surplus function), then any core allocation, and consequently, any competitive equilibrium, features ==== (PAM for short). PAM refers to a positive correlation in sorting between the values of the traits of couples (matching of like types): the high type individual matches with individual of the same high type, and the low type agent couples with individual of the same low type. PAM is not only revealed in many social phenomena (e.g., the general practice of endogamy in the caste system of India), but also is a highly desired property in many economic settings (e.g., recruiting skilled workers for skill-intensive positions). Further studies on PAM are all based upon similar assumptions that the joint matching surplus is supermodular of some sort. For example, Shimer and Smith (2000) finds that log-supermodularity is sufficient for PAM with search frictions. Eeckhout and Kircher (2010) derives PAM in a directed search model under the condition of root-supermodularity. In a discrete search model with explicit additive search cost, Atakan (2006) finds that Becker’s supermodularity condition is sufficient — complementarities lead to PAM.====As PAM is the outcome of a non-cooperative game, despite being created jointly, the joint surplus has to be divided between matched partners according to some allocation rule. The value received and consumed by a matched individual is known as her net benefit from matching. In order to ensure the corresponding searching equilibrium features PAM, it is necessary that the net benefit function inherits supermodularity from the joint surplus. To my best knowledge, almost all the studies in PAM literature under transferable utility framework, such as (Becker, 1974), Shimer and Smith (2000), Atakan (2006), Eeckhout and Kircher (2010) and the most recent survey by Chade et al. (2017), share the same assumption that matched couples divide their joint surplus according to the Nash bargaining solution (Nash, 1950). It is well known that given transferable utilities, the Nash bargaining solution simply boils down to an even split rule, i.e., the joint matching surplus is evenly split between the matched couple.==== ==== Indeed, the even split rule has been adopted in a considerable majority of the papers in the matching literature without any particular reason other than analytical convenience. It is natural to wonder how essential this even split rule is in PAM. One way of putting the use of the even split rule on a surer footing and developing a better understanding of any advantages or limitations it might have is to study if PAM preserves if one replaces the even split rule with other allocation rules. In particular, the current paper focuses on allocation rules that are functions of the involved parties’ types. The potential candidates come from many economic areas, such as contest success functions (Skaperdas, 1996) from the contest literature and the auction solution (Krishna, 2009) from the auction literature, to name a few. However, due to the formidably large number of possible allocation rules, evaluating their effects on PAM is practically impossible. Motivated by this fact, this paper considers a general class of allocation rules that ensures supermodularity is passed from the joint surplus function to the corresponding net benefit functions.",The even split rule in positive assortative matching,https://www.sciencedirect.com/science/article/pii/S0304406819300023,March 2019,2019,Research Article,320.0
Carmona Guilherme,"University of Surrey, School of Economics, Guildford, GU2 7XH, UK","Received 15 March 2018, Revised 13 December 2018, Accepted 17 December 2018, Available online 11 January 2019, Version of Record 16 January 2019.",https://doi.org/10.1016/j.jmateco.2018.12.004,Cited by (0),"We consider the existence of limit admissible equilibria, i.e. ","The motivation of the refinement literature (see van Damme (1991) for a survey) is that the requirements of Nash equilibrium need to be strengthened to obtain a more appealing solution concept in normal-form games. Along these lines, and following Kohlberg and Mertens (1986), it would be desirable to have a solution concept whose existence is generally guaranteed and in undominated strategies.====For games with finite action spaces, Selten’s (1975) perfect equilibrium is a way of obtaining the above properties. However, in games with infinite action spaces, several difficulties arise. First, even when players’ payoff functions are continuous, it is possible that all Nash (and, therefore, all perfect) equilibria are in weakly dominated strategies (see Simon and Stinchcombe (1995, Example 2.1)). For this reason, one can only hope to obtain general existence results for limit admissible equilibria, i.e. of Nash equilibrium in which each player assigns zero probability to the interior of the set of his weakly dominated strategies.====Furthermore, when some player’s payoff function is discontinuous, existence of perfect equilibria requires conditions considerably stronger than the standard ones guaranteeing existence of Nash equilibrium (such as Reny’s (1999) better-reply security). Moreover, it is not always the case that perfect equilibria are limit admissible (see Carbonell-Nicolau (2011b)). Other solution concepts have been proposed such as Andersson et al.’s (2014) robust equilibrium to strategic uncertainty and Bich’s (2016) prudent equilibrium, but again their existence and limit admissibility have only been established under assumptions stronger than the standard ones.====In this paper we directly address the existence of limit admissible equilibria. We first show that a condition different from better-reply security is needed. Indeed, we present two examples of better-reply secure games that fail to have limit admissible equilibria: One of these examples fails to have limit admissible strategies and the other, while having limit admissible strategies, fails to have one that is also a Nash equilibrium.====We then introduce the notion of admissible security and use it to establish the existence of limit admissible equilibrium in mixed strategies.==== ==== In contrast with the complicated task of establishing the existence of perfect equilibrium (and, thus, of establishing indirectly and under some additional assumptions the existence of limit admissible equilibria), we show that the task of establishing directly that a limit admissible equilibrium exists is a relatively simple one. Indeed, besides having to assume that limit admissible strategies exist, all it takes is that better-reply security be “appropriately relativized” to the set of limit admissible strategies. In this sense, admissible security is the natural modification of better-reply security to address the existence of limit admissible equilibria.====It is important to note that admissible security is neither stronger nor weaker than better-reply security. It is stronger than assuming that the game obtained by restricting players to choose limit admissible strategies is better-reply secure; this latter condition is, as we show, not sufficient for the existence of limit admissible equilibria. An additional advantage of our approach as compared to analyzing the game restricted to limit admissible strategies is that the former does not require a complete characterization of limit admissible strategies; the general auction setting of Jackson and Swinkels (2005) provides an example where this is useful.====The paper is organized as follows. Section 2 presents our notation and general definitions. Some motivating examples, including those showing that better-reply security is not sufficient for the existence of limit admissible equilibrium are in Section 3. Our new condition, admissible security, is introduced in Section 4 and used to establish the existence of limit admissible equilibrium.====Several remarks in Section 4 complement this result, by showing that: (1) More general existence results can be obtained by relativizing Reny’s (2016) correspondence security and finite deviation property; (2) an extension of our result for pure strategies is possible in own-strategy quasi-concave games when the set of limit admissible pure strategies is convex; (3) the relativization of Reny’s (1999) payoff security and of Bagh and Jofre’s (2006) weak reciprocal upper semicontinuity together provide sufficient conditions for admissible security; (4) better-reply secure games are admissible secure provided that each non-limit admissible strategy is weakly dominated by a limit admissible strategy (a weaker version of this property suffices). In Section 4 we relate our existence result with those that follow from Simon and Stinchcombe (1995) for continuous games and from Carbonell-Nicolau (2011b) for discontinuous games; we also briefly compare our approach with that of Harris et al. (2005). Our results are applied in Section 5 to a Bertrand–Edgeworth duopoly game with strictly convex costs, modeled along the lines of Maskin (1986). In addition to being an interesting game in its own right, this game is useful to illustrate our results because it is not difficult to characterize its set of weakly dominated strategies and, once such characterization is obtained, applying our results is a relatively simple task.",On the existence of limit admissible equilibria in discontinuous games,https://www.sciencedirect.com/science/article/pii/S0304406818301381,March 2019,2019,Research Article,321.0
"Chun Youngsub,Mitra Manipushpak","Department of Economics, Seoul National University, Seoul, South Korea,Economic Research Unit, Indian Statistical Institute, Kolkata, India,Economics Division, School of Business, University of Leicester, Leicester, LE1 7RH, United Kingdom","Received 18 April 2018, Revised 13 November 2018, Accepted 17 December 2018, Available online 11 January 2019, Version of Record 20 February 2019.",https://doi.org/10.1016/j.jmateco.2018.12.005,Cited by (4),"We use the Lorenz criterion to select new allocation rules for the queueing problem. We first show that the Lorenz criterion selects a unique rule from four distinct subsets of Pareto efficient allocation rules satisfying some fairness concept. We then compare these four rules whenever possible. When we cannot compare, we use the lexicographic maximin and the lexicographic ==== criteria to make a comparison. However, a Lorenz optimal rule does not exist on the set of Pareto efficient and strategy-proof rules.","Fairness in a queueing situation requires that agents served earlier compensate those served later. How should this be done? This question was raised by Maniquet (2003) who addressed this by applying the Shapley value to an associated cooperative game.==== ==== Afterwards, important fairness notions in the economics literature have been successfully applied to the queueing problem: ==== by Chun (2006a), ==== by Chun et al. (2014), and the ==== by Chun and Yengin (2017), Maniquet (2003) and Mitra (2007).==== ==== These studies show that there are a number of Pareto efficient rules satisfying any of the above fairness concepts.====Two natural questions arise. First, from a set of Pareto efficient rules satisfying a given fairness concept, which one should we pick? Second, how do we select among rules satisfying different fairness concepts? In this paper, we use the egalitarian principle represented by the ==== or its weaker versions, the ==== (Rawlsian) criterion and the ====, to address both issues.====The Lorenz criterion is used to rank income distributions (see Dutta (2002) and Sen and Foster (1997)). It is also used in allocation problems such as the bankruptcy problem (see Chun et al. (2001) and Thomson (2012)).==== ==== The lexicographic maximin criterion is also known as the ==== (see Rawls (1972)). There is a large literature applying it to rank income vectors (see Barberá and Jackson (1988), d’Aspremont and Gevers (1977), Hammond (1976) and Sen (1970), among others). The lexicographic minimax is a sort of “reverse” of the Rawlsian criterion. It has been used in allocation problems (see, for instance, Alkan et al. (1991)).====Given ==== and ==== in ====, let ==== and ==== be rearrangements of these vectors in ascending order.==== ==== We say that ==== ==== ==== if ==== for ====. Say that ==== ==== ==== if ==== or there exists ====
 ==== such that ==== for ==== and ==== and say that ==== ==== ==== if ==== or there exists ==== such that ==== for ==== and ====.====An ==== (henceforth, rule) associates a queue and a vector of transfers to every queueing problem. Let ==== denote the utility vector resulting from the rule ==== for the queueing problem ====. The rule ==== ==== the rule ==== if for all queueing problems ====, ==== Lorenz dominates ====. Similarly, ==== ==== ==== if for all queueing problems ====, ==== lexicographically maximin (lexicographically minimax) dominates ====. Given a set of rules ====, the rule ==== is ==== if ==== Lorenz dominates ==== for all ====. Similarly, given a set of rules ====, the rule ==== is ==== if ==== leximaximin (leximinimax) dominates ==== for all ====.====On the set of Pareto efficient allocation rules, the ==== which equalizes utilities across agents, is Lorenz optimal. However, this rule does not satisfy the identical preferences lower bound, no-envy or egalitarian equivalence. Hence, we investigate the existence of Lorenz optimal rules on the sets of Pareto efficient rules satisfying each fairness concept. We show that the Lorenz optimal rules exist on each of these three sets giving us the ====, the ==== and the ====.==== ==== We then compare the four rules. While the egalitarian rule Lorenz dominates the other rules, the Lorenz no-envy rule is Lorenz dominated by the other rules, and the constrained egalitarian and the Lorenz egalitarian-equivalent rules are Lorenz non-comparable. However, we show that the Lorenz egalitarian-equivalent rule lexicographically minimax dominates the constrained egalitarian rule. These results serve as a guide when selecting among rules satisfying different fairness criteria.====Strategy-proofness requires that no agent should benefit by misreporting her waiting cost, irrespective of what other agents do. Since it is a desirable property when agents’ waiting costs are private information, it has been well studied in the queueing literature.==== ==== Our final result shows that the set of lexicographically maximin optimal rules and the set of lexicographically minimax optimal rules are both empty on the set of Pareto efficient and strategy-proof rules, and hence, there is also no Lorenz optimal rule on this set.",Egalitarianism in the queueing problem,https://www.sciencedirect.com/science/article/pii/S0304406818301393,March 2019,2019,Research Article,322.0
"Ceparano Maria Carmela,Quartieri Federico","Department of Economics and Statistics, University of Naples Federico II, Naples, Italy,Department of Economics and Management, University of Florence, Florence, Italy","Received 12 August 2018, Revised 24 November 2018, Accepted 29 December 2018, Available online 10 January 2019, Version of Record 22 January 2019.",https://doi.org/10.1016/j.jmateco.2018.12.007,Cited by (2),We introduce the notion of an antichain-convex set to extend Debreu (1954)’s version of the second welfare theorem to economies where either the aggregate production set or preference relations are not convex. We show that – possibly after some redistribution of individuals’ ,"The second welfare theorems enunciated in Debreu, 1951, Debreu, 1954 and Arrow (1951) are – more or less explicitly – proved by means of the so-called Minkowski and Hahn–Banach separation theorems. The economic thesis of their welfare theorems is that, possibly after some redistribution of individuals’ wealth, the Pareto optima of convex economies==== ==== can be spontaneously obtained as competitive equilibria of an economy with a finite set of agents where consumers choose optimal affordable consumption vectors and firms maximize own profits. The convexity enables the applications of the mentioned separation theorems but is known to be liable to objection. However, if such a condition were simply dropped then the previous thesis would not hold anymore in general.====Motivated by the need of relaxing the convexity requisites of an economy, in the seventies (Guesnerie, 1975) extended the second welfare theorem to non-convex preferences and technologies: his extension pertained the ==== for consumers’ expenditure minimization and firms’ profit maximization. In convex economies the necessary conditions are also sufficient to ensure that a Pareto optimum is the solution to such optimization problems; but the ==== is not generally guaranteed without convexity assumptions and hence a Pareto optimum of a non-convex economy need not be supportable as a valuation quasiequilibrium. The main results concerning the extension of the second welfare to non-convex economies followed the pioneering approach of Guesnerie (1975): they were devoted to finding “marginal” prices at Pareto optima which – satisfying the mentioned first-order necessary conditions – lie in suitably chosen cones of normals.==== ==== Part of the effort within this literature has been made to seek the most tight-fitting notion of a normal cone. Among the articles of this strand of the literature we mention in particular Khan and Vohra, 1987, Khan and Vohra, 1988, Bonnisseau and Cornet (1988), Khan (1999), Mordukhovich (2000), Bonnisseau (2002), Flåm and Jourani (2006), Florenzano et al. (2006), Jofré and Rivera (2006) and Habte and Mordukhovich (2011). For discerning and in-depth reviews of this literature we refer to Chapter 8 of the two-volume book by Mordukhovich, 2006a, Mordukhovich, 2006b and to Chapter 10 of Mordukhovich (2018).====It is important to observe that Debreu (1954)’s second welfare theorem does not posit the convexity of production sets but only that of their aggregate. As the finite sum of convex sets is convex, that second welfare theorem holds for convex economies: this is undisputed. On the other hand one can easily construct specific examples of economies with a convex aggregate production set where at least one firm has a non-convex production set. Therefore Debreu (1954)’s economies are not convex stricto sensu and hence the second welfare theorem stated therein holds even for some non-convex economies. However, one is left in the dark when trying to figure out which (general) conditions on firms’ production sets can guarantee the convexity of their aggregate in non-convex economies. To the best of our knowledge, the subsequent literature has not illuminated this issue which, from a mathematical viewpoint, boils down to understanding which properties – other than convexity – guarantee that the sum of a finite family of sets is convex.====The previous observation on Debreu (1954)’s assumptions is made more accurate when noting that the condition which, in fact, plays a role in the proof of Debreu (1954)’s second welfare theorem is the convexity of the Pareto improving set ==== of scarce resources.==== ==== The set ==== is the sum of the aggregate production set and a Pareto improving set of aggregate consumption vectors: thus ==== is the Minkowski sum of two Minkowski sums. Its convexity condition is well-known to be met in convex economies. But what can we say as for non-convex economies? Once again one runs into the key issue of seeking conditions ensuring the convexity of the sum of finitely many (possibly non-convex) sets.====In this paper we tackle the issue of extending the second welfare theorem to non-convex economies by applying a reformulation of the Minkowski\Hahn–Banach theorem that dispenses with convexity assumptions on sets separated by a linear continuous functional. In the same spirit of Debreu (1954), we provide sufficient conditions for the supportability of Pareto optima as valuation quasiequilibria and as valuation equilibria. But unlike Debreu (1954), we do not assume the convexity of both the aggregate production set and the preference relations of an economy. Various alternative versions of the second welfare theorem will be presented: one of them – more precisely our Theorem 7 – properly generalizes Theorem 2 of Debreu (1954) on the supportability of Pareto optima as valuation quasiequilibria in the case of an economy with locally nonsatiated preferences. Some versions – like for instance our Theorem 8 – are not stricto sensu comparable to Theorem 2 of Debreu (1954) but nevertheless explicitly display conditions on (possibly non-convex) production sets which ensure the convexity of their aggregate.====Our reformulation of the Minkowski\Hahn–Banach theorem – more precisely our Theorem 4 – relies on a notion of generalized convexity recently introduced in Ceparano and Quartieri (2017) which is now extended to arbitrary cones and to a possibly infinite-dimensional setting. Such a notion is here called ====-antichain-convexity and imposes the usual notion of convexity requisites only on the linear span of any two vectors whose differences do not belong to some fixed cone ====. To obtain the desired reformulation, we preliminarily address the problem of establishing which conditions can guarantee the convexity of the Minkowski sum of finitely many sets when some summands are not convex. One of the results of this work – more precisely our Theorem 1 – displays these conditions proving that the sum of finitely many sets is convex when each summand is ====-antichain-convex and at least one of them is ====-upward (which is a sort of general free-disposability condition).==== ==== From the pure point of view of the mathematical structure that underlies the economic results of this work, this result is perhaps our key-contribution.====The paper is tacitly organized into two parts. The first part is merely mathematical and consists of Sections 2 Fundamental mathematical notions, 3 On the convex sum of sets, 4 On the separation of sets and Appendix A. Section 2 presents the mathematical definitions of a ====-antichain-convex and of a ====-upward set and illustrates some of their general properties. Section 3 shows that the sum of finitely many ====-antichain-convex sets is convex provided one of the summands is ====-upward. Section 4 uses this last result to obtain a separation theorem which applies also to non-convex sets. Appendix A contains some mathematical facts. The second part – where the results of the first are applied – is of economic nature and consists of Sections 5 Definition of an economy, 6 First welfare theorem and an observation, 7 Second welfare theorems, 8 Discussion of some assumptions, 9 Numerical examples and final remarks, Appendix B Some economic facts, Appendix C On antichain-quasiconcavity. Section 5 recalls some classical economic notions and definitions. Section 6 contains a formulation of the first welfare theorem and an observation. Section 7 provides several second theorems of welfare for possibly non-convex economies. Section 8 discusses the hypotheses posited in the second welfare theorems. Section 9 shows some concluding corollaries and some examples of non-convex economies where the economic results of the paper apply. Appendix B proves some economic facts and Appendix C examines the representability of ====-antichain-convex preferences by means of ====-antichain-quasiconcave (utility) functions.",A second welfare theorem in a non-convex economy: The case of antichain-convexity,https://www.sciencedirect.com/science/article/pii/S0304406819300011,March 2019,2019,Research Article,323.0
"Álvarez Francisco,Rey José-Manuel","Department of Economic Analysis, Complutense University of Madrid, Campus de Somosaguas, 28223, Spain","Received 28 November 2017, Revised 9 December 2018, Accepted 11 December 2018, Available online 4 January 2019, Version of Record 14 January 2019.",https://doi.org/10.1016/j.jmateco.2018.12.002,Cited by (0),"We study existence, uniqueness and restoring dynamics of price-dispersion equilibria in a market for a homogeneous good. We assume that costs are heterogeneous across participating firms. Specifically, we rely on classical extreme ==== and some recent developments on fee-setting mechanisms to consider cost distributions that are Generalized Pareto. Our analysis provides results on the existence and uniqueness of a price-dispersion equilibrium that link the cost dispersion across firms with the search cost of consumers. If the former is large enough compared to the latter, existence and a form of uniqueness of price-dispersion equilibrium arise. In addition, we propose a natural best-response market dynamics that delivers convergence to the price-dispersion equilibrium, even if the market departs slightly from the Diamond paradox equilibrium.","The empirical literature in economics has reported over decades an intriguing finding: many – if not most – markets for a homogeneous good exhibit a significant degree of price-dispersion. Different firms charge different prices, even though all of them sell identical items. This is a pervasive phenomenon that has been documented for a large number of markets. According to Baye et al. (2006), price-dispersion is “ubiquitous and persistent”. In fact, typical price distributions for a vast number of goods have been recently shown to be symmetric with a standard deviation between 19% and 36% (Kaplan and Menzio, 2015). See Baye et al. (2006) for an exhaustive survey on this empirical literature.====A preliminary explanation for the phenomenon relies on the fact that consumers must face search costs — this literature was triggered by a seminal paper by Stigler (1961). Even if a consumer is aware that there are firms selling at low prices he still must find those firms, which is costly. The firms in the market anticipate the existence of consumer search costs and raise price over marginal cost. But why should different firms raise prices differently? The Diamond paradox concerns this question (Diamond, 1971). If consumers face search costs, the conformation in which all firms set the monopoly price and consumers search for one price quote only is an equilibrium of the market with no dispersion.====Theorists have considered different scenarios to circumvent the Diamond paradox. Reinganum (1979) provided a first way out of the Diamond’s trap by considering different costs across firms, while the consumer side is characterized by a downward sloping demand function. Other market structures with heterogeneous first have been explored, as in MacMinn (1980), which will be commented below. The case of homogeneous costs has been also considered: Burdett and Judd (1983) show that, when firms have equal costs, the presence of search costs yields (non-monopoly) equilibria that typically exhibit price-dispersion, namely different firms set different prices.====This paper aims to contribute to the theoretical literature which links price to cost dispersion. We consider a market for a homogeneous good composed of a continuum of firms with different costs and a representative consumer who faces search costs. Our articulated model provides a closed form characterization of a price-dispersion market equilibrium. Existence, uniqueness of the one-shot interaction between buyer and sellers, as well as the stability of equilibrium under an evolutionary market dynamics are analyzed. The stability analysis is particularly relevant in search theory: if price-dispersion equilibria actually coexist with the Diamond equilibrium, to what extent price-dispersion is more likely to be observed in practice? This question is clearly related with the dynamical stability of equilibria. In this paper we characterize a basin of attraction for price-dispersion equilibria and show that the Diamond equilibrium cannot appear under natural conditions.====Next we first highlight the basic features of our model regarding firms’ costs, consumer search and market dynamics. We also explain along the way the relevance of our assumptions and findings with reference to the related literature. Then we describe the different steps and main results of our analysis.====For most of our analysis we assume that unit production costs follow a Generalized Pareto (====) distribution. It turns out that assuming ==== cost distributions has significant implications, both practically and theoretically. While their use has a long tradition in economics, our motivation to consider ==== distributions is otherwise twofold. One reason comes from extreme value theory: for a large class of unit cost distributions, the conditional distribution of unit costs given that only firms with a cost below some threshold value (say, the consumer’s willingness to pay) operate at the market, is asymptotically a ==== distribution — see Balkema and De Haan (1974) and Pickands III (1975). A second argument is based on recent work on optimality of transaction fee rules: the necessary and sufficient condition to implement optimally affine fee-setting mechanisms that are observed in online markets is that sellers’ costs are drawn from a ==== distribution (see e.g. Loertscher and Niedermayer, 2012).====On the consumer side, we assume that the consumer demands one unit of the product and adopts a ==== strategy when searching for the lowest price. This is the classical approach by Stigler (1961). Other works assume an inelastic demand, e.g. MacMinn (1980), Rob (1985) or Carlson and McAfee (1983), while Reinganum (1979) considers a downward sloping demand curve. Under fixed sample size search, the consumer selects ==== the number ==== of sellers to visit and sticks to it regardless the prices she finds along the way. An alternative search strategy is sequential search, in which the consumer decides whether to stop or continue searching after observing each price. A fixed sample size search strategy is consistent with some recent empirical findings, namely De los Santos et al. (2012) and Honka and Chintagunta (2017). Some of these empirical findings, however, are also consistent with sequential search with belief updating — see Santos et al. (2017) for an empirical analysis and Janssen et al. (2017) for a theoretical ground. In general, it seems there is not conclusive empirical evidence in favor of one particular search strategy.====The basic scheme above is similar to that of MacMinn (1980). However, MacMinn (1980) assumes that unit costs obey a uniform distribution, which is embedded in the class of ==== distributions. Furthermore, the analysis in MacMinn (1980) is limited to the existence of equilibrium. The assumption that costs are ==== distributed allows us to establish the (quasi)uniqueness and stability of the market equilibrium. Such an exhaustive model analysis seems to be unusual in the theoretical literature of price-dispersion. In fact, the existence and uniqueness of price-dispersion equilibria has been largely studied. A classical reference is the paper by Burdett and Judd (1983), in which there is multiplicity of equilibria. Other references include Benabou and Gertner (1993), Rob (1985) and Stahl (1989). All of them assume homogeneous costs among firms, and sequential search strategy on the consumer side. The latter three papers assume some form of heterogeneity in search costs. Benabou and Gertner (1993) analyze a model in which there is (exogenous) inflation, though the distribution of real prices is assumed invariant over time. For the case of identical consumers and inelastic demand, they find a unique price-dispersion equilibrium. Rob (1985) characterizes necessary conditions for a price-dispersion equilibrium with continuous prices, and he also finds some closed form solutions when the search costs are Pareto distributed. Stahl (1989) assumes that consumers are either informed or uninformed, with zero and positive search costs, respectively, and characterizes a unique price-dispersion equilibrium. In the limiting case of uninformed consumers, the equilibrium that he finds collapses to the Diamond equilibrium. Carlson and McAfee (1983) characterize equilibria for a finite number of heterogeneous firms and consumers, assuming specific functional forms for both consumer’s search cost and firms’ production costs. They find a price-dispersion equilibrium for the case in which the search costs are uniformly distributed. In general, a closed form characterization of the price distribution depends on the assumptions on the exogenous variables of the model. In this regard, we show that ==== price distributions are closed under best responses if firms’ costs are ==== distributed, which is key for our extensive analysis.====Regarding the market dynamics considered in this paper, both consumer and sellers base their current decisions on past available information. We assume that, while both firms and consumers update their decisions as new information becomes available, firms have capacity to update more rapidly. So, in every period, first firms observe the previous prices and consumer’s search sample size ==== and adjust their current prices so that the equilibrium among firms is achieved; then the consumer updates the value of ==== as his best response given the price equilibrium, and the next period starts. Additional details for this dynamical setting are provided in Section 3.3. Hopkins and Seymour (2002) also study stability of price-dispersion, following a different scheme, namely they consider pricing games with a Rock–Scissors–Paper structure and analyze their stability in mixed strategies. In addition, they consider that all firms have the same unit cost, which constitutes another major difference with respect to our story. Lahkar (2011) also studies stability of price-dispersion equilibria using evolutionary game theory. The departure point for both papers is the scheme by Burdett and Judd (1983), namely equal costs across firms and use of mixed strategy equilibria.====Our analysis proceeds as follows. As a first step, we take the consumer’s search behavior as exogenous. We call the price distribution equilibrium among firms with ==== exogenous an ====. We show that, if costs are ==== distributed, a price-dispersion equilibrium does exist and it is also ==== distributed; moreover, this equilibrium is unique.====Then we consider a price dynamics within the industry as follows. Unit costs obey a ==== distribution which remains constant over time. In every period, each firm selects its price as the best response given the price distribution (and the consumer’s search size ====) in the previous period. The whole set of best responses conforms the industry price distribution in the current period, which in turn is the input distribution to determine the firms’ prices in the next period, and so on. We provide sufficient conditions for convergence of the sequence of price distributions to the industry equilibrium. Roughly, the industry equilibrium is restored as long as the mean of the initial price distribution is not too far from the mean of the unit cost distribution and, additionally, the initial price distribution is sufficiently less dispersed relative to the unit cost distribution. Since the initial price distribution must have been rationalized ==== from the unit cost distribution, the conditions above seem plausible.====The next stage of our analysis consists of determining the consumer’s ==== endogenously in a one-shot interaction: as mentioned above, we assume that the consumer decides the number ==== of sellers to visit ==== Stigler (1961), that is, by minimizing total cost defined as the search cost plus the expected minimum price. An equilibrium is now a price distribution together with a value of ====, which we call a ====. The issues of existence and uniqueness of equilibrium for the market are more involved than for the industry. A ==== distribution is characterized by three parameters: shape, location, and scale. Our analysis shows that, when costs are ==== distributed, only the shape and the ratio of scale to unitary search cost matter for the existence and uniqueness of equilibrium. Roughly, the existence and quasi-uniqueness of equilibrium occur provided that the ratio lies beyond suitable threshold values, which depend on the shape of the cost distribution. The critical threshold values for both existence and quasi-uniqueness are characterized in the paper.====Finally, we consider the issue of the restoring dynamics of the market equilibrium. Under the conditions that guarantee existence and quasi-uniqueness of the market equilibrium, we show that the off-equilibrium dynamics considered in this paper restores the price-dispersion market equilibrium for a large range of initial values of ====, with ====. Our results imply that the (quasi-unique) price-dispersion market equilibrium is asymptotically stable and, furthermore, the Diamond equilibrium cannot emerge provided that sellers’ costs are sufficiently dispersed and the consumer initially considers a sample size large enough (which is just two for uniform cost distributions).====The rest of the paper is organized as follows. In Sections 2 Price-dispersion industry equilibrium, 3 Price-dispersion market equilibrium we present the analyses of the industry equilibrium and the market equilibrium, respectively. Within each section, we first characterize the best responses of all agents, then we discuss the existence and uniqueness of equilibrium, and finally we address the issue of the stability of the equilibrium restoring dynamics. In Section 4 we conclude with some final remarks. We present our main results in the main body of the document and all proofs in Appendix A.",(Quasi) uniqueness and restoring dynamics of price-dispersion market equilibria under search cost,https://www.sciencedirect.com/science/article/pii/S0304406818301368,March 2019,2019,Research Article,324.0
"Loss Frédéric,Piaser Gwenaël","PSL, Université, Paris-Dauphine (LEDa-SDFi), Place du Maréchal de Lattre de Tassigny, 75016 Paris, France,IPAG, Business School, 184 boulevard Saint-Germain, 75 006 Paris, France","Received 10 March 2016, Revised 17 December 2018, Accepted 18 December 2018, Available online 31 December 2018, Version of Record 16 January 2019.",https://doi.org/10.1016/j.jmateco.2018.12.006,Cited by (1),"We consider a competitive insurance market in which agents can privately enter into multi-contractual insurance relationships and undertake hidden actions. We study the existence of a linear equilibrium when insurance companies have no restrictions on their pricing rules. For CARA utility functions, we show that a linear equilibrium always exists. For DARA utility functions, we provide sufficient conditions under which a linear equilibrium exists. We show that the linear equilibrium is unique, actuarially unfair, and induces partial insurance coverage. Lastly, we show that the linear equilibrium is always third-best efficient.","We consider a competitive insurance market in which agents can privately enter into contract with several insurance companies.==== ==== In this context, we study the existence and the characteristics of a linear equilibrium in prices when insurance companies do not have any restrictions on their pricing rules and agents are subject to moral hazard. Indeed, empirical evidence confirms the existence of linear price equilibria. For instance, in the UK’s annuity market,==== ====
 Finkelstein and Poterba (2002) find that pricing is linear up to a small administrative charge.==== ==== Lastly, we examine the welfare properties of the linear equilibrium.====To study non-exclusive competition in insurance contracts, we use a common-agency model (see, e.g., Bernheim and Whinston (1986)). We model insurance companies as principals offering contracts to agents who can privately choose several contracts from within the global set of offered insurance contracts, and who each exert an unobservable and costly effort that affects his probability of having an accident. We assume agents exert an effort which is selected from a continuous choice set. Moreover, following Peters (2001) and Martimort and Stole (2002), we assume insurance companies may offer menus of contracts so as to characterize all the equilibria of the game with common agency.====The model we develop is important for two reasons. First, it provides microeconomic foundations for the seminal literature on competitive non-exclusive insurance markets that assumes linear price contracts (see Pauly (1974) and Arnott and Stiglitz, 1988, Arnott and Stiglitz, 1991a, Arnott and Stiglitz, 1991b). By contrast, we allow firms to offer any type of menu of contracts (and, in particular, non-linear contracts).====Second, in the case of DARA utility functions, the model provides general sufficient conditions that guarantee the existence of a unique linear price equilibrium. Providing such general conditions is only possible because we assume the effort exerted by an agent is continuous. This assumption is a crucial one whose importance cannot be understated. Providing such general conditions would not be possible if effort were dichotomous (i.e., when effort can take only two values, “high” or “low”), as assumed in Hellwig (1983) or Bisin and Guaitoli (2004).====In the case of CARA utility functions, we show a unique linear equilibrium always exists because the agent’s willingness to pay for more insurance is increasing (at least weakly) in wealth. The existence property of the linear price equilibrium is robust because it does not rely on any assumption related to how the effort exerted by the agent affects the probability of having an accident. In the case of DARA utility functions, the agent’s willingness to pay for more insurance is not (necessarily) increasing in wealth. In this case, the existence of a linear price equilibrium relies on how the effort the agent exerts affects the probability of an accident (cf. the general sufficient conditions we provide).====Analyzing a case of strong moral hazard (i.e., a case in which the effort exerted by an agent significantly changes the probability of an accident), we show the unique linear equilibrium is such that agents are not fully insured and exert a level of effort that is strictly higher than the lowest possible level, and insurance companies have strictly positive expected profits. In the traditional Bertrand competition model, no strictly positive (expected) profit exists in equilibrium, because firms could then deviate, make aggressive offers, and take all the markets. In our model, insurance companies cannot behave this way, because they fear agents might take the deviating offer, complement it with other offers (because insurance contracts are not exclusive), and discontinuously shift to the lowest level of effort, which would make the deviating offer unprofitable. We thus show that strictly positive (expected) profits in a competitive setting, as in Hellwig (1983) or Parlour and Rajan (2001), can also exist when the effort exerted by an agent is continuous and not simply binary.==== ====Lastly, we show that the linear equilibrium is always third-best efficient, which implies a laissez-faire policy constitutes the best choice for a government when imposing the exclusivity of the insurance contracts is impossible by law.==== this article is related to other papers that have studied non-exclusive contracts in a common-agency game framework. Following an approach close to ours, Ales and Maziero (2009) study the seminal adverse-selection model of Rothschild and Stiglitz (1976) when insurance contracts are not exclusive. They show that a linear equilibrium exists in which all insurance companies offer the same menu of contracts. Rothschild (2013) also studies adverse selection in insurance markets with non-exclusive contracting, but under conditions of compulsory contracting, linear pricing, and multiple indemnity states. He shows that screening different types into different contracts is possible, despite the conditions of linear pricing and non-exclusivity.====Our article is, however, more closely related to studies of non-exclusive contracts in insurance markets under moral hazard. However, almost all of these studies consider a setting in which effort is dichotomous (a notable exception is Bertola and Koeniger, 2015). Instead, we consider a continuous effort, which allows us to generalize previous analyses.====Hellwig (1983) also examines the existence of linear price equilibria.==== ==== However, he only considers isoelastic utility functions, whereas we study CARA and DARA utility functions.==== ====Kahn and Mookherjee (1998) study a setting in which agents design their own contracts (offers are take it or leave it, each insurance company can only accept or reject an offer), make contractual decisions sequentially, and have contractual portfolios that are observable but not contractible upon. Their setting and results are totally different from ours. In particular, insurance companies always make zero profits and agents face fair insurance prices in equilibrium (even in the high-effort equilibrium) because agents are endowed with all of the bargaining power in the contractual relationship.====Bisin and Guaitoli (2004) focus on non-linear price equilibria.==== ==== They find that high effort can be obtained in a non-linear equilibrium with positive expected profit for the (active) insurance companies. A crucial condition to support this sort of non-competitive outcome in an otherwise competitive set-up is the presence of latent contracts, which are contracts offered by insurance companies but never subscribed to in equilibrium, and whose role is to provide a threat against the deviation of a single insurer to take the entire market by reducing prices. Moreover, Bisin and Guaitoli (2004) find equilibria that are based on latent contracts that pay when no accident occurs, whereas the equilibria we find are based on latent contracts that induce overinsurance. More precisely, in the case of linear equilibria, latent contracts correspond to all the linear contracts that are offered by the insurance companies but are not subscribed in equilibrium: if an insurance company deviates from the linear offers, agents buy the deviating offer, complete insurance coverage by using the linear offers of the other insurance companies until being overinsured, and then exert the lowest level of effort, making the deviating offer unprofitable.====Attar and Chassagnon (2009) consider the same setting as Bisin and Guaitoli (2004). They characterize new non-linear equilibria, which are based on ==== latent contracts (contracts that are issued at a price strictly lower than the fair one and that imply negative virtual expected profit for the issuer). They show those equilibria may fail to be third-best efficient, in contrast to Bisin and Guaitoli (2004), who find that equilibria based on positive latent contracts are third-best efficient. We also find the linear equilibrium (which is per se based on positive latent contracts) is always third-best efficient.====Attar et al. (2007) restrict the set of contracts insurance companies can offer to linear or partially linear contracts==== ====; all equilibria are linear per se. In our case, we study the existence of linear-contracts equilibria, that is, situations in which insurance companies offer linear contracts in equilibrium, even if they have no restrictions on the kind of contracts they can offer.====Bertola and Koeniger (2015) also consider a moral-hazard problem in which the level of effort is continuous. They show how insurance production costs that exceed expected claim payments can make insurance premia so actuarially unfair that the equilibrium is unique and characterized by first-order conditions. We do not assume the production of insurance services is costly. We thus complement Bertola and Koeniger’s article by analyzing the case in which the equilibrium cannot simply be characterized by first-order conditions. In other words, we provide sufficient conditions for the existence of an equilibrium when the first-order approach cannot be used, whereas Bertola and Koeniger provide sufficient conditions that guarantee the first-order approach is valid.====This paper proceeds as follows. Section 2 presents the model. Section 3 studies the linear equilibrium with nonexclusivity in the case of CARA utility functions, after which Section 4 studies DARA utility functions. Section 5 examines the welfare properties of the linear equilibrium. Section 6 presents some concluding remarks. Formal proofs are relegated to the Appendix A Proof of, Appendix B The regularity condition.",Linear price equilibria in a non-exclusive insurance market,https://www.sciencedirect.com/science/article/pii/S030440681830140X,March 2019,2019,Research Article,325.0
"Dubra Juan,Egozcue Martín,García Luis Fuentes","Universidad de Montevideo, Uruguay,Universidad de Montevideo and ANII, Uruguay,Departamento de Matemáticas, Universidade da Coruña, Spain","Received 21 May 2018, Revised 15 November 2018, Accepted 30 November 2018, Available online 17 December 2018, Version of Record 28 December 2018.",https://doi.org/10.1016/j.jmateco.2018.11.004,Cited by (1),"-shaped value function, consuming the same good in all the periods is optimal. However, if consumers can choose not to consume in some periods, then this behavior is optimal only for certain kind of piece-wise linear value functions. We also discuss briefly the case when satiation dominates habituation.",None,Optimal consumption sequences under habit formation and satiation,https://www.sciencedirect.com/science/article/pii/S0304406818301344,January 2019,2019,Research Article,326.0
"Berbeglia Gerardo,Sloan Peter,Vetta Adrian","Melbourne Business School, University of Melbourne, Australia,McGill University, Canada","Received 16 July 2017, Revised 15 November 2018, Accepted 20 November 2018, Available online 6 December 2018, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jmateco.2018.11.002,Cited by (1),"We study the uncommitted ==== monopoly problem when there are finitely many consumers, a finite horizon, and no discounting. In particular we characterize the set of strong-Markov subgame perfect equilibria that satisfy the skimming property. We show that in any such equilibrium the profits are not less than static monopoly profits; and at most the static monopoly profits plus the monopoly price. When each consumer is small relative to the market, profits are then approximately the same as those of a static monopolist which sets a single price. Finally, we extend the equilibrium characterization to games with an arbitrary discount factor.","We study a standard durable-goods monopoly game, where a seller sets a price in every period but cannot commit to its pricing strategy, while each consumer leaves the market once it has purchased one unit. It is well known that with an infinite horizon and no discounting any sharing is possible in a subgame perfect Nash equilibria (SPNE). The literature has therefore typically dealt with an infinite horizon and discounting. However, results depend crucially on how demand is modeled.====In 1972, Nobel recipient Ronald Coase made the startling conjecture that a durapolist (a monopolist in the market of a durable good) has no monopoly power at all. Specifically, a durapolist who lacks commitment power cannot sell the good above the competitive price if the time between periods approaches zero (Coase, 1972). The intuition behind the Coase conjecture is that if the monopolist charges a high price then consumers anticipate a future price reduction (as they expect the durapolist to later target lower value consumers) and therefore they prefer to wait. The durapolist, anticipating this consumer behavior, will then drop prices down to the competitive level. In essence, the argument is that a durapolist is not a monopolist at all: the firm does face stiff competition — not from other firms but, rather, from future incarnations of itself. This is known as the ====: the durapolist cannot credibly commit to charging a high price.====The Coase conjecture was first proven by Gul et al. (1986) under an infinite time horizon model with non-atomic consumers. They showed that if buyers strategies are stationary then, as period length goes to zero, the durapolist’s first price offer converges to the lowest consumer valuation or the marginal cost, whichever is higher. Ausubel and Deneckere (1989) later showed that if the stationary condition is relaxed, the durapolists profits at subgame perfect equilibria can range from Coasian profits to the static monopoly profit.==== ==== Stokey (1979) studied pricing mechanisms for durapolists that ==== commitment power in a continuous time model. She showed that durapolists can then attain the static monopoly profit by committing to a fixed price; all sales are then made at the beginning of the game. McAfee and Wiseman (2008) examined the Coase conjecture in a model where there is small cost for production capacity which can be augmented at each period. In this setting, the authors showed that the monopoly profits are equal to those that can be obtained if she could commit ex ante to a fixed capacity. Recently, Ortner (2017) studied a model where the durapolist incurs a stochastic cost.====In contrast to the results mentioned above, full extraction of economic surplus may arise if demand is instead composed of a finite number of consumers. Indeed, Bagnoli et al. (1989) proved the existence of a subgame perfect Nash equilibrium in which the durapolist extracts all the economic surplus if the demand is atomic and the time horizon is infinite. To obtain this, they considered the following pair of strategies. The durapolist strategy, dubbed ====, is to announce at each time period, a price equal to the valuation of the consumer with the highest value who has yet to buy. The strategy of each consumer, dubbed ====, is to buy the first time it induces a non-negative utility. This equilibrium refutes the Coase conjecture. Indeed, it suggests that a durapolist may have perfect price discriminatory power. Moreover, it shows there exist subgame perfect Nash equilibria where durapoly profits exceed the static monopoly profits by an unbounded factor.==== ====von der Fehr and Kühn (1995) studied the model of Bagnoli et al. (1989) with an infinite time horizon and showed that under certain conditions Pacman is the only equilibrium. However other equilibria also exist with less than full extraction. Cason and Sharma (2001) considered a different model with atomic buyers where the Pacman equilibrium of Bagnoli et al. (1989) cannot exist. Instead of assuming a durapolist with perfect information, the authors constructed a two-buyer and two-valuation model with infinite time periods in which the durapolist does not know exactly whether a consumer is of high type or of low type. They showed that in these games there exists a unique equilibrium that is Coasian. In his recent study of the durapoly problem with finitely many consumers and infinite horizon, Montez (2013) showed that there are inefficient equilibria where the time at which the market clears does not converge to zero as the length of the trading periods approaches zero.====Given the important role demand plays in the analysis discussed above, it is also relevant to understand the role played by other assumptions as well, such as the role of an infinite horizon and discounting in the analysis. Indeed there are situations where sales need to take place within a bounded period of time (for example, tickets to a running show), discounting is of second order, and a seller is able to make a sequence of offers (with potentially more eager consumers watching the show earlier). Theoretical and empirical evidence of the strong effects of deadlines have been observed in many bargaining contexts such as in contract negotiations and civil case settlements — see, for example, Cramton and Tracy (1992) and Williams (1983).====If there is a finite horizon and a continuum of consumers, a feasible action for the durapolist is to decline to sell goods until the final period and then announce the static monopoly price, obtaining the static monopoly profits discounted to the beginning of the game. Although this strategy is not an equilibrium, Güth and Ritzberger (1998) showed that when consumer valuations follow a uniform distribution, there exists a subgame perfect equilibrium, as period lengths approach zero, in which the durapolist profits converge to the static monopoly profits discounted to the beginning of the game.====It is however less well understood what happens with finite buyers and finite time horizon. Bagnoli et al. (1989) has presented some examples with two or three consumers, and showed that the Pacman equilibrium may hold as well under a finite horizon.====In this paper we provide a general treatment of profits under discrete consumers, finite horizon and no discounting for equilibria that satisfy a skimming property. An equilibrium is said to satisfy the skimming property if high-value consumers buy before (or at the same time) than lower-valued consumers. The skimming property is satisfied in many settings of the durable good monopoly problem (e.g. Güth and Ritzberger (1998), Gul et al. (1986)) as an indirect consequence of a non-increasing price path. In our setting with a finite number of consumers, however, it is possible that non-skimming equilibria may exist.====First, we prove that there always exists a skimming-property equilibrium and we are able to characterize, in Section 3, the class of all subgame perfect equilibria that satisfy the skimming property. ==== ==== ( In Section 5, we are able to extend this characterization to games with discounting.)====Our main result is then that at every such skimming equilibrium the durapoly profits are bounded by below by the static monopoly profits, and by above by the sum of the static monopoly profits and the static monopoly price. These bounds hold regardless of the number of consumers, their values, and the number of time periods. Therefore, if the size of each individual buyer is small, the durapolist will neither make significantly more, nor less, than the static monopoly profits.====The intuition behind the upper bound is as follows. Using an inductive argument, we prove that each buyer has an associated threat price: the price it can get in the end if all buyers with a valuation above her own have already purchased. The argument concludes by showing that the sum of all the consumer threat prices cannot exceed the static monopoly profit plus the static monopoly price. Moreover, we prove that our bound is tight. Indeed, we construct a (infinite) family of examples where durapoly profits approach the static monopoly profit plus the static monopoly price as the number of consumers goes to infinity.====We believe that our main result sheds light into this classical problem in at least four ways. To begin, our main theoretical result concurs with the practical experience that durapolists and static monopolists have comparable profitability (i.e. within a constant multiplicative factor). For example, following a comprehensive study on the practices of durable goods monopolies, Orbach (2004) concludes “Durapolists may collect profits higher than static monopoly profits. In fact, some of the practices durapolists employ to increase profits are not available to perishable-goods monopolists, and, therefore, monopolies over durable goods markets may be more profitable than monopolies over perishable-goods markets”. In contrast, previous theoretical works have suggested that the durapolist either has no monopoly power, perfect price discriminatory power, or a multitude of equilibria over all the range in between.====Second, the result that a durapolist can do up to an additive amount better using a threat-based strategy rather than a price-commitment strategy is actually best viewed from the opposite direction. Specifically, a durapolist can obtain almost the optimum profit (losing at most an additive amount equal to the static monopoly price) by mimicking a static monopolist via a price commitment strategy. From a practical perspective this is important because a price-commitment strategy can generally be implemented by the durapolist very easily, even with limited consumer information. Furthermore, price-commitment strategies can be popular with consumers as they are typically introduced within a money back guarantee or envy-free pricing framework. In contrast, a threat based optimization strategy is harder to implement and can antagonize consumers.====Third, the literature often highlights that the surprising and well-known result of Bagnoli et al. (1989), namely that the durapolist can extract all economic surplus, is due to the assumption of finitely many consumers. Our results show that this is not true in general — their result is also driven by the infinite time horizon. For finite time horizons, the power of a durapolist is sometimes limited (with or without discounting). For some distributions of consumer valuations, the durapoly profits are strictly less than the full extraction of economic surplus (see Sections 4 A relationship between durapoly profits and static monopoly profits, 5 The effect of discounting).==== ====Finally, the main result highlights a distinction on how the time horizon affects bargaining power. With non-atomic consumers, a finite time horizon increases the bargaining power of the durapolist. In Güth and Ritzberger (1998), a finite time-horizon increases durapolist profits from the Coasian result to the static monopoly profits. With finitely many consumers, the finiteness of the time horizon reduces the durapolist’s bargaining power for some demand distributions (either with or without discounting). In particular, for the undiscounted case, the durapolist’s profits become approximately the static monopoly profits.","The finite horizon, undiscounted, durable goods monopoly problem with finitely many consumers",https://www.sciencedirect.com/science/article/pii/S0304406818301320,May 2019,2019,Research Article,327.0
"Mendolicchio C.,Pietra T.","Institute for Employment Research - IAB, Germany,DStat, Università di Bologna, Italy","Received 6 July 2017, Revised 28 July 2018, Accepted 31 October 2018, Available online 22 November 2018, Version of Record 3 December 2018.",https://doi.org/10.1016/j.jmateco.2018.10.006,Cited by (1),"We establish that, when the number of agents is sufficiently large, but finite, there are open sets of economies with constrained Pareto inefficient equilibria and provide a simple sufficient condition for constrained inefficiency. We also show that there are open sets of economies with constrained efficient equilibria. Hence, for these economies, neither constrained efficiency, nor its lack, are generic properties. However, constrained inefficiency is a pervasive feature: for each economy with preferences satisfying a mild restriction, there are open sets of endowments such that their equilibrium allocations are constrained inefficient.","With incomplete financial markets, equilibrium allocations are typically Pareto inefficient.==== ==== The interesting question is if they satisfy weaker notions of optimality, defined taking into account the restrictions that market incompleteness imposes upon the set of feasible allocations. The canonical criterion of constrained Pareto optimality, or efficiency, was introduced by Stiglitz (1982) and Geanakoplos and Polemarchakis (1986), and further developed by Citanna et al. (1998).==== ==== The key idea is that, as a minimal efficiency requirement, an allocation should not allow for Pareto improvements attainable by rearranging portfolios and letting commodity prices adjust to restore market clearing for all the commodities. Portfolio reallocations can allow for a Pareto improvement due to the welfare effects of the induced changes in equilibrium prices.====In this, and in a companion, paper, we extend the analysis of constrained efficiency in GEI models in two directions. In Mendolicchio and Pietra (2016), we have studied the attainability of a Pareto superior allocation via an appropriate redistribution of the time-zero endowments. Here, we focus instead on the feasibility of Pareto improvements via portfolio reallocation, adopting the canonical criterion of constrained efficiency. In Geanakoplos and Polemarchakis (1986), constrained inefficiency is established for economies where the number of agents, ====, is smaller than the number of normalized commodity prices, ====. We extend their analysis to economies where the number of agents is finite, but this upper bound fails. The logic of the results in the literature implies that, no matter what the – finite – number of agents is, there are open sets of economies with constrained inefficient equilibria. Think, for instance, of replica economies: If the equilibrium is constrained inefficient with one agent per type, the same equilibrium is also constrained inefficient for each number of replicas, and for each economy sufficiently close. Therefore, there are always open sets of economies with constrained inefficient equilibria, provided that there is – in a proper sense – not that much of heterogeneity across agents.====Apart from economies which can be seen as – approximate – replicas, in the literature there are no general results on the constrained optimality properties of economies with finitely many agents, if portfolio adjustments are the only policy tool. Citanna et al. (1998) have established generic constrained inefficiency independently of the number of agents. However, they allow for both portfolio and (for at least two agents) period zero endowment reallocations. Their result is certainly important, but it exploits both the direct welfare effects of the endowment redistribution and the pecuniary externalities generated by endowment and portfolio reallocations. We think that it is interesting to consider in detail the case when the possibility of a Pareto improvement rests only upon the welfare effects of the induced price changes, which is the purpose of this paper. The previous work on this issue has made clear that, with many agents, there is no way to implement ==== conceivable Pareto improvement, since the classical (i.e., Geanakoplos and Polemarchakis (1986), and Citanna et al. (1998)) approach cannot be applied. However, this does not rule out the possibility that ==== Pareto improvements could be obtained, generically. In this paper, we show that this is not true: When the number of agents is large, but finite, there are always open sets of economies with a unique constrained efficient equilibrium. Additionally, we show that, with finitely many agents,==== ==== there are also open sets of economies with – possibly unique – constrained inefficient equilibria. They include open sets of economies far away from replica economies.====While our results establish opposite constrained optimality properties for different sets of economies, the basic logic of the argument is similar and it can be sketched more clearly focusing on the economies with a constrained inefficient equilibrium.====Our inefficiency results hold for all profiles of utility functions satisfying, at the equilibrium, a mild condition, but without any lower bound on the degree of heterogeneity. They can be summarized as follows: consider an economy with a finite, but large, number of agents. Pick any equilibrium. Fix the equilibrium prices and allocation. Consider the set of economies with the same total resources and characterized by endowment profiles such that the prices and allocation we started with are also an equilibrium given the new endowment profile. Under some technical conditions – generically satisfied at an equilibrium – , there is a relatively open neighborhood of endowments in the given set such that the equilibrium we started with is constrained inefficient. Generic regularity of equilibria guarantees that there is, actually, an open set of economies with constrained inefficient equilibria. Since, generically, the fiber associated with a no-trade equilibrium contains economies with constrained inefficient equilibria, this is a common phenomenon in GEI, independently of the number of agents.====Our technique of proof heavily exploits the special properties of the economies with a no-trade equilibrium. At these equilibria, the impact of changes in prices on the agents’ value functions is necessarily nil. By appropriately changing endowments, we can construct economies with the same equilibrium prices and allocation, but such that there are directions of price changes entailing an increase of the maximum level of utility for each agent. Under generic conditions, with many agents, each equilibrium price perturbation can be attained by adjusting appropriately – and exogenously – the agents’ portfolios. We build on this intuition to formally establish the existence of constrained inefficient equilibria for some open sets of economies.====The same basic idea is also behind the opposite – in terms of constrained efficiency – result. Starting, again, with an economy with a no-trade equilibrium, we can perturb initial endowments in other, different, directions, in such a way that, at the equilibrium of the economy so constructed, there are no price adjustments improving the maximum utility of each agent. These equilibria are, then, locally constrained efficient. To guarantee that they are so when we allow for arbitrarily large, feasible portfolio reallocations, we need to restrict the analysis to open sets of economies with the property that Pareto superior equilibria cannot be bounded away from the actual equilibria. Economies with homothetic, agent-invariant utility functions do have this property. We establish constrained optimality of equilibria for some open sets of economies which are close to this set of economies. We show that, somewhat surprisingly, each open neighborhood of an economy with identical, homothetic preferences and a no-trade equilibrium contains open sets of economies with constrained efficient equilibria, and also open sets of economies with constrained inefficient ones.====Contrary to most of the previous work on this topic, we study the optimality properties of equilibria using an approach based on the characterization of the constrained efficient allocations as solutions to a well-defined optimization problem built upon the agents’ indirect utility functions.==== ==== Using the terminology of Citanna et al. (1998), we follow an optimization approach, while both they and Geanakoplos and Polemarchakis (1986) adopted a submersion approach. We believe that it is interesting to fully and explicitly pursue our approach too, also because, for certain purposes, it is somewhat more transparent, interpretation-wise. For instance, to study constrained inefficient equilibria, we consider the open set of economies such that the first order conditions of a well-defined optimization problem are necessary for a maximum. We show that, for these economies, the FOCs can be violated at the equilibrium, so that their allocations are not constrained efficient. When the number of agents is “small” (lower than the number of non-numeraire commodities), this happens for a generic set of economies, so that we, basically, provide an alternative proof of Geanakoplos and Polemarchakis (1986). When there are finitely many agents, this happens for an open, but definitely not dense, set. We also provide a simple sufficient condition for the lack of constrained optimality of equilibria: for a generic set of economies, equilibria are constrained inefficient when the, properly discounted, present value of the vector of net trades in the numeraire commodities is strictly positive for each agent. This condition is easy to check, once an equilibrium is given. Its weakness is that it is based on both “observables” (the net trades), and “non-observables” (the normalized vectors of Lagrange multipliers that we need to discount). While it is possible that more appealing sufficient conditions could be found, they must all share this shortcoming.====To extend the analysis of constrained efficiency in GEI toeconomies with an arbitrary number of agents is important. Since we are dealing with competitive economies, any upper bound on their number is a very strong restriction. When we get rid of it, the constrained inefficiency results become weaker. However, they are still interesting for several reasons. First, we establish that constrained inefficiency, while non generic, is a pervasive phenomenon and that it may hold for any degree of heterogeneity across agents. Second, our sufficient condition for lack of constrained optimality is easy to check, once an equilibrium is given. Third, our results make transparent that the same equilibrium allocation, given preferences, may, or may not, be constrained efficient depending upon the endowment vector. Indeed, for each equilibrium, there is a polyhedron of initial endowments such that the given price and allocation is an equilibrium. The same allocation may be constrained inefficient for some initial endowments in this set, and constrained efficient for others.====The next section presents the model and establishes the, fairly standard, properties of equilibria to be exploited later on. In Section 3, we make precise the notion of constrained efficiency and prove our main results. Two parametric examples should help to clarify the constrained optimality properties of equilibria for economies with “almost” identical and homothetic preferences.",A re-examination of constrained Pareto inefficiency in economies with incomplete markets,https://www.sciencedirect.com/science/article/pii/S0304406818301241,January 2019,2019,Research Article,328.0
"Asano Takao,Yokoo Masanori","Faculty of Economics, Okayama University, Tsushimanaka 3-1-1, Kita-ku, Okayama 700-8530, Japan","Received 19 March 2018, Revised 24 September 2018, Accepted 4 November 2018, Available online 15 November 2018, Version of Record 28 November 2018.",https://doi.org/10.1016/j.jmateco.2018.11.001,Cited by (7),"We develop a simple piecewise linear overlapping generations model exhibiting endogenous business cycles, which is based on Matsuyama’s (2007) model of credit cycles. Some sort of “noise” representing information imperfection is shown to transform the Matsuyama model into a continuous, eventually expanding, piecewise linear map on the interval, which is tractable enough to investigate the dynamics in depth by using the techniques of the Frobenius–Perron operators to find observable chaos. While, according to the analysis of Asano et al. (2012), the Matsuyama model exhibits periodic cycles of arbitrarily large period, it is essentially not capable of chaotic dynamics. However, our model exhibits ergodic chaos with some robustness.","The overlapping generations (OLG, hereafter) model has been widely used as a general equilibrium model in many fields of macroeconomics. As one of the major research concerns in macroeconomic dynamics, the endogenous business cycles have also been investigated using the OLG framework, initiated by Benhabib and Day (1982) and Grandmont (1985). They find that even without external shocks, nonlinearities inherent in the underlying economic systems can cause complicated, in particular ====, dynamics.====From a technical viewpoint, however, it is not necessarily easy to detect and characterize chaotic behaviors in a given deterministic nonlinear dynamic economic model, especially when the long-run observability of irregular perpetual fluctuations is concerned, even if the model is described by a single one-dimensional difference equation. One apparent exception appears when the economic model is ==== A piecewise linear dynamic model has much to offer. Among other features, it facilitates ==== Even if piecewise linear modeling seems to be an extreme simplification, it is very beneficial in the above sense as long as it is a tolerable approximation of the real world.====Furthermore, there are empirical affinities of piecewise linear models to the ==== (TAR) models used in nonlinear time series analysis. See Tong and Lim (1980) and Tong (1983) for TAR models and Chan and Tong (1986) for the ==== models.==== ====
 That is, a theoretical piecewise linear economic model free from external shocks can be thought of as a deterministic counterpart of the TAR model. In this sense, the development of deterministic piecewise linear models in the general equilibrium framework==== ==== will give some sound microeconomic foundations for the TAR models.====In the literature of optimal growth, which is thought of as another endogenous business cycle theory in the general equilibrium framework, there are some two-sector models with Leontief technology in which optimal transition functions exhibit piecewise linearity, giving rise to ergodic chaos (that is, complex dynamics with observability in the long run). This piecewise linearity allows us to characterize complicated optimal growth paths in depth. See e.g. Nishimura et al. (1994) and Nishimura and Yano (1995) for more details.====Recent studies have investigated complex dynamics of ==== growth cycle models intensively. Piecewise smooth modeling can be regarded as an intermediate modeling between the two extremes: piecewise linear modeling and smooth modeling. See Gardini et al. (2008)==== ==== and Matsuyama et al. (2016),==== ==== who employ the relatively new theory of border-collision bifurcation to show that their macroeconomic models can exhibit complicated transition to chaotic behavior. Admittedly, the complex dynamics of the piecewise linear map of the interval has been well understood since long before at least by mathematical experts, and therefore we have seemingly little to add to the literature.==== ==== On the other hand, piecewise linear modeling is recognized to give analytical results of complex dynamics in a sharp and clear way. Taking this fact into account and regarding that there seems to be no counterpart==== ==== to the chaotic piecewise linear optimal growth model in the OLG literature, at least to the best knowledge of the authors, it must be still worth filling the gap and developing a piecewise linear OLG model exhibiting chaotic behavior. Therefore, the purpose of this paper is to explore this task.====Among recent studies related to this paper, Matsuyama (2007) proposes an OLG model with endogenous technology switch caused by financial imperfection, and shows that the model can generate several growth patterns. In Matsuyama’s (2007) model, agents face borrowing constraints due to financial imperfection, and each agent can choose to be either an entrepreneur or a lender. Furthermore, multiple investment technologies are assumed to be available. The market interest rate affects entrepreneurs’ choice of technology and the market rate varies over time depending on the level of capital. This implies that the entrepreneurs’ choice of technology changes endogenously, which gives rise to richer dynamics compared to other models in the literature on endogenous business cycles. Although the model proposed by Matsuyama (2007) is relatively simple, it leads to various phenomena, such as credit traps, credit collapses, leapfrogging, credit cycles, and growth miracles. As such, Asano et al. (2012) analyze the dynamic property of the macroeconomic model proposed by Matsuyama (2007) in depth, and show that the model can be analyzed within the framework of the neuron model studied by Hata (1982).==== ====Furthermore, Asano et al. (2012) show that the model can exhibit either periodic fluctuations or fluctuations which are ==== in some sense.==== ==== It is important to notice that chaos in Asano et al. (2012) occurs only on the set of parameter values of measure zero.==== ==== In the very sense, chaos is virtually ==== in the models of Asano et al. (2012) and, consequently, Matsuyama (2007). Once such a pathological parameter value for the occurrence of Hata’s chaos is somehow chosen, however, any initial condition leads to a complicated long-run behavior. In the latter sense, Hata’s chaos ==== observable, but such a case hardly occurs. Observability of chaos in terms of both sets of parameter values and initial conditions is important because it can be thought to capture the “recurrent but not periodic” nature of business cycles in the deterministic framework. Therefore, when we talk about the observability of chaos in the long run, we have to pay attention to both the state space and the parameter space. To avoid confusion, when we talk about observability in terms of the parameter values, we sometimes use closely related or synonymous terms such as ==== or ====.==== ====Some existing studies need mentioning from the viewpoint of analytical techniques. Ishida and Yokoo (2004) develop a macroeconomic model in which firms face a binary choice problem in investment, and show that due to piecewise linearity, the model exhibits periodic cycles. Yokoo and Ishida (2008) modify the model by introducing imperfect observability,==== ==== and provide a mechanism by which observation errors lead to chaotic fluctuations. That is, Yokoo and Ishida (2008) show that observation errors or what they call ==== can be a source of observable chaos in economic systems.====Since we hardly know the true state of the world with precision, especially when aggregate amounts are concerned, it is natural to assume that such insufficient information affects decision making, in particular, about whether an agent chooses to become an entrepreneur or not. Therefore, it is of interest to incorporate such information imperfection or imperfect observability into Matsuyama’s (2007) or equivalently Asano et al.’s (2012) framework under perfect observation to see how the dynamic patterns change.====The model proposed in this paper can be thought of as an extension of the model in Asano et al. (2012), which is a special case of Matsuyama’s (2007), along the line of Yokoo and Ishida (2008). As a result, we transform Matsuyama’s (2007) original model, together with misperception or observation errors, into a piecewise linear model, which is tractable enough to investigate the dynamics in depth by using the techniques of the Frobenius–Perron operators to find invariant measures (i.e., observable chaos) as in Yokoo and Ishida (2008).==== ==== Indeed, by specifying the set of parameters for some kind of Markov properties,==== ==== we can easily establish and characterize chaotic dynamics with observability in terms of initial conditions in more detail. Imposing Markov properties on models seems rather restrictive at first glance, however, this will be relaxed to the extent that such chaos is shown to be abundant in terms of the set of parameter values.====The organization of this paper is as follows. Based on Matsuyama (2007) and Asano et al. (2012), Section 2 provides a benchmark model, in which the productivity of agents is perfectly observable. Section 3 provides the main model of this paper, in which the productivity of agents is imperfectly observable. Section 4 considers further specifications of our model discussed in Section 3. Section 5 analyzes chaotic dynamics in detail. Section 6 gives concluding remarks. Some derivations are relegated to the Appendix.",Chaotic dynamics of a piecewise linear model of credit cycles,https://www.sciencedirect.com/science/article/pii/S030440681830123X,January 2019,2019,Research Article,329.0
Gollier Christian,"Toulouse School of Economics, University of Toulouse-Capitole, France","Received 21 June 2018, Accepted 21 October 2018, Available online 15 November 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jmateco.2018.10.003,Cited by (2),"Suppose that the decision-maker is uncertain about the variance of the payoff of a gamble, and that this uncertainty comes from not knowing the number of zero-mean i.i.d. risks attached to the gamble. In this context, we show that any n-th degree increase in this variance risk reduces expected utility if and only if the sign of the 2n-th derivative of the utility function ==== is ====. Moreover, increasing the statistical concordance between the mean payoff of the gamble and the n-th degree riskiness of its variance reduces expected utility if and only if the sign of the (2n+1)-th derivative of ==== is ","How does the uncertainty affecting the variance of an asset’s return affect its value to investors? More generally, how does a shift in the distribution of this variance influence expected utility? In this paper, we build a theory of stochastic dominance on variance that is based on the seminal work of Eeckhoudt and Schlesinger (2006) who raised the following more specific question: Does one prefer to bear a zero-mean risk for sure, or two independent draws of this risk with probability ====? They showed that shifting from the first risk context to the second one is an example of fourth degree risk increase as defined by Ekern (1980), in the sense that it is perceived as undesirable by any von Neumann–Morgenstern individual with a negative fourth derivative of the utility function, a condition coined as “temperance” in decision theory (Kimball, 1993, Gollier and Pratt, 1996). We generalize this result by showing that any Rothschild–Stiglitz increase in risk in the number of zero-mean risks attached to the gamble reduces expected utility if and only if ==== is negative. In other words, in this context of additive i.i.d. risks, temperant people dislike increasing variance risk.====The role of putting risk on risk has emerged as an important research object over the last decade or so. For example, Weitzman (2007) has shown that the uncertainty surrounding the variance of the growth rate of consumption has a first-order impact on welfare and asset prices. Using a Bayesian approach, he assumed an inverted gamma posterior distribution for the variance of the growth rate and consumption, which implies a Student-==== distribution for log consumption. This yields an unbounded risk premium at equilibrium, under constant relative risk aversion (CRRA). This is an extreme illustration of our result, since constant relative aversion implies temperance, and the Student-==== has fatter tails, a necessary condition for a 4th degree risk increase. This is also related to the literature on long-run risk pioneered by Bansal and Yaron (2004), in which the variance of the growth rate of consumption is subject to persistent stochastic shocks. In the discounted expected utility model, this positively affects the systematic long-term risk premium under temperance, as shown in Gollier (2017).====In the spirit of Eeckhoudt and Schlesinger (2006), Eeckhoudt et al. (2009), Crainich et al. (2013), Ebert (2013) and Deck and Schlesinger (2014), we systematize the risk apportionment approach by considering other classes of changes in distribution of variance. Following Ekern (1980), we say that a random variable ==== undergoes a ====th degree increase in risk if and only if it reduces the expectation of ==== for all real-valued functions ==== such that ==== is negative.==== ==== Cases ==== and ==== correspond respectively to first-degree stochastic dominance and Rothschild–Stiglitz increases in risk. A ====th degree risk increase in ==== does not affect its ==== first moments. It raises its ====th moment if ==== is even, and it reduces it when ==== is odd. We show that a ====th degree increase in the variance risk generates a 2nth degree increase in consumption risk if ==== is even. The opposite result holds when ==== is an odd number. For example, an increase in downside (i.e., third degree) risk in variance yields a sixth degree reduction in consumption risk, which increases expected utility if the sixth derivative of the utility function is negative, as in the CRRA case.====An interesting feature of this property comes from the possibility to use it recursively. For example, suppose that the uncertainty affecting the variance ==== of a gamble is measured by the variance of ====, and that this object is itself uncertain. Performing a second degree risk increase on the variance of ==== generates a fourth degree increase in the risk affecting ====, and thereby an eighth degree increase in consumption risk. This “vol-of-vol” type of model exists in the asset pricing literature. For example, the standard long-run risk model assumes a stochastic volatility that is governed by an autoregressive process of degree 1. This generates a time-invariant term structure of variance premia, which is counterfactual. To solve this problem, Bollerslev et al. (2009), Tauchen (2011) and Drechsler and Yaron (2011) introduced some uncertainty on the variance of the persistent shocks to the volatility to generate a time-varying variance premium as observed on financial markets. Our result indicates that on top of these time variations of the equilibrium price of risk, this new ingredient generates the additional property to raise the expected risk premium if and only if the eighth derivative of the utility function of the representative agent is negative, which is the case under constant relative risk aversion.====Observe that all these findings provide a new characterization of the 2nth derivatives of the utility function, leaving odd derivatives aside. Eeckhoudt and Schlesinger (2006) explored a road to characterize odd derivatives by combining zero-mean risks with sure losses. For example, they showed that shifting a zero-mean risk from a low income state to an equally likely larger income state raises expected utility when the third derivative of the utility function is positive, i.e., when the decision-maker is prudent. To generalize this result, we use the concept of increasing concordance between two random variables, as introduced in economics by Epstein and Tanny (1980) and Tchen (1980). This concept is stronger than the linear concept of increasing correlation, and it preserves the marginal distributions of the two random variables. In this paper, we show that increasing the concordance between the background income and the number of zero-mean risks of the gamble increases expected utility if and only if the individual is prudent. In other words, it generates a third degree reduction in the consumption risk. This is linked to the result by Tinang (2017) who introduced a negative correlation between the shock on the trend and the shock on the volatility of consumption growth. From our analysis, the third degree risk increase that it generates should raise the risk premium, because CRRA individuals are prudent.==== ====We generalize this finding by showing that increasing concordance between background income and the ====th degree riskiness of the variance of the gamble yields a ====th degree change in consumption risk. Therefore, we obtain a complete characterization of all even and odd derivatives of the utility function by considering various changes in the distribution of the variance of the lottery under consideration and in its correlation with the background income.====The paper is structured as follows. In Section 2, we characterize the impact of increasing the ====th degree riskiness of the variance of consumption on expected utility. This impact is univocally linked to the sign of the successive even derivatives of the utility function. We characterize the impact of increasing the statistical relationship between the mean and the ====th degree riskiness of the variance of consumption in Section 3. This impact is univocally linked to the sign of the successive odd derivatives of the utility function. Section 4 is devoted to the implications of these results for asset pricing in a simple two-date Lucas tree economy. In the last section, we warn the reader that our results cannot easily be extended to a multiplicative framework for the variance risk.",Variance stochastic orders,https://www.sciencedirect.com/science/article/pii/S0304406818301204,January 2019,2019,Research Article,330.0
"Iraola Miguel A.,Sepúlveda Fabián,Torres-Martínez Juan Pablo","Department of Economics, University of Miami, United States,Banco Santander, Chile,Department of Economics, Faculty of Economics and Business, University of Chile, Chile","Received 6 December 2017, Revised 11 June 2018, Accepted 31 October 2018, Available online 15 November 2018, Version of Record 4 December 2018.",https://doi.org/10.1016/j.jmateco.2018.10.007,Cited by (1),"In this paper we address equilibrium existence in economies with default, long-term collateralized debt, and financial market segmentation. We first prove equilibrium existence in the finite-horizon case of our model, by adapting techniques recently applied in two-period economies with segmented financial markets. We then show that a competitive equilibrium exists for infinite-horizon economies when credit markets are composed of finite-lived contracts or infinite-lived contracts that can be refinanced over time. Finally, we show that if credit markets include infinite-lived contracts that cannot be refinanced, an equilibrium exists when per-period utility functions are either additive in at least one commodity or satisfy a substitution condition, which holds for unbounded per-period utility functions. We illustrate our contributions by providing examples of economies that conform with the assumptions of each of our equilibrium existence results.","This paper studies the existence of equilibrium in economies with default, long-term collateralized debt, and financial market segmentation. The recent evolution of the applied economics literature emphasizes the relevance of financial market segmentation and endogenous default as essential ingredients to understand diverse asset pricing puzzles and the identification of optimal macroprudencial and monetary policies.==== ==== At the same time, a recent strand of theoretical literature provides foundations to better understand general equilibrium models with endogenous default and collateral requirements (see, e.g., Araujo et al. (2012), Gottardi and Kubler (2015), Araujo et al. (2015) and Brumm et al. (2015)). However, despite the ample attention devoted in the applied economics literature, there is no theoretical analysis identifying sufficient conditions to guarantee equilibrium existence in infinite-horizon economies with long-term collateralized debt contracts, endogenous default, and segmented financial markets. The aim of our paper is to close this gap and provide a solid ground for the further development of applied models in this area.====In general equilibrium models with non-recourse collateralized debt, although the scarcity of collateral avoids Ponzi schemes (see Araujo et al., 2002, Araujo et al., 2011 and Kubler and Schmedders (2003)), the incorporation of financial segmentation generates methodological challenges to prove equilibrium existence. Indeed, commodity and asset prices cannot be normalized without affecting the continuity of individual optimal actions. To overcome this difficulty, we single out endogenous upper bounds for asset prices adapting to our theoretical framework recent approaches modeling financial segmentation in two-period economies (see Seghir and Torres-Martinez (2011) and Cea-Echenique and Torres-Martínez, 2016, Cea-Echenique and Torres-Martínez, 2018).====We consider a model including commodities that may be perfectly durable or depreciable, and may transform into other commodities through time. Financial markets are composed of non-recourse collateralized loans that are packaged into pass-through securities delivering endogenous borrowers’ payments. Credit contracts are characterized by general functions specifying physical collateral requirements, coupon payments, and prepayment costs. Our model encompasses a rich set of collateralized debt contract designs. For instance, we may consider long-term adjustable rate mortgages (ARMs), fixed rate mortgages (FRMs), shared appreciation mortgages (SAMs), and general prepayment costs that may include penalties. Our general theoretical framework seems to be of special interest for the incorporation of mortgage-backed security markets in full fledged general equilibrium models (cf. Gabaix et al. (2007)).====We extend Iraola and Torres-Martínez (2014) three-period model in two directions. First, we consider an arbitrary time horizon. Second, we consider not only segmented access to borrowing markets but also segmented access to lending markets. Financial market segmentation is exogenous in our model.==== ==== It is a reduced form that may be the result of frictions like asymmetric information, institutional constraints, and behavioral frictions that may induce agents to specialize in trading a subset of the available securities (cf. Merton (1987), Grossman and Miller (1988), and Shleifer and Vishny (1997)).====Our model represents a significant deviation from the often standard models with one-period collateralized debt, unsegmented financial markets, and time-invariant financial contract sets. In these models, the optimal default decision is homogeneous across agents and given by ==== agents pay the minimum between the debt value and the collateral value. Our theoretical framework, however, allows to consider a rich set of long-term credit contracts and more realistic prepayment and default decisions. As in Iraola and Torres-Martínez (2014), optimal payment strategies – coupon payment, prepayment, and default – may be non-homogeneous across agents. Additionally, underwaterborrowers – households with negative equity – may decide to meet their financial obligations.==== ==== These results are in accordance with a burgeoning empirical literature seeking to identify the main determinants of prepayment and default risks (cf. Deng et al. (2003) and Gerardi et al. (2018)). In our model, the lack of liquidity may force individuals to honor their commitments even though a prepayment cost specified as the outstanding debt balance may exceed the collateral value. However, note that this result does not create opportunities to implement Ponzi schemes because the collateral value at the issuing date always exceeds the loan value. This is a consequence of the non-negative return of buying the collateral with debt and the monotonicity of preferences on collateral consumption (cf. Araujo et al., 2002, Araujo et al., 2011 and Kubler and Schmedders (2003)).====As we capture financial segmentation by means of heterogeneous access to financial market instruments, ==== do not hold in our model (cf. Aouani and Cornet, 2009, Aouani and Cornet, 2011 and Cornet and Gopalan (2010)). As mentioned before, given that any joint normalization of commodity and asset prices may induce discontinuities on individual optimal actions, to prove equilibrium existence we need to identify endogenous upper bounds for asset prices. To do so, we follow two procedures: we may appeal to the fact that debts are non-recourse or make use of some characteristics of preferences.====This paper provides five equilibrium existence results. First, as an intermediate step, we prove equilibrium existence in the finite-horizon version of our model (see Proposition 1). Our strategy to bound the price of a pass-through security is to find a consumption plan that ==== its payouts (cf. Cea-Echenique and Torres-Martínez, 2016, Cea-Echenique and Torres-Martínez, 2018). It is worth noting that although borrowers’ optimal payment strategies may be heterogeneous, it may be the case that all issuers of a debt contract decide to prepay or default at a given state. Hence, since debts are non-recourse, the tightest upper bound that we are able to find for a market feasible security payout is the depreciated collateral value. This implies that the no-arbitrage price of a pass-through security can be bounded by the cost of a bundle of commodities that is proportional to both the collateral requirement of the underlying debt and the (finite) number of periods until maturity. Our second equilibrium existence result adapts our previous super-replication argument to an infinite-horizon economy with finite-lived credit contracts (see Theorem 1). As a corollary, we recover the main result in Araujo et al. (2002). Our third equilibrium existence result considers infinite-lived credit contracts that can be refinanced (see Theorem 2). In particular, it is assumed that credit contractions are not permanent. Hence, since agents can refinance their debt contracts, a portfolio of infinite-lived promises can be shown to be equivalent to a sequence of finite-lived ones. Thus, every competitive equilibrium of the original economy with infinite-lived credit contracts is equivalent to an equilibrium of an abstract economy where only finite-lived credit contracts are available. Therefore, the proof of equilibrium existence in this case goes along the lines of the proof of our second result above. As a corollary of this third result, we recover the main equilibrium existence result in Araujo et al. (2011).====Our fourth equilibrium existence result considers infinite-lived credit contracts under permanent credit contractions (see Theorem 3). In this framework, our previous super-replication technique cannot be implemented. Indeed, as discussed above, the consumption plan that super-replicates the deliveries of a pass-through security need to be proportional to the longest period that the underlying credit contract might remain open (taking as given alternative credit opportunities). However, such a consumption plan may not exist. Hence, to find upper bounds for asset prices we adapt an assumption on preferences from Seghir and Torres-Martinez (2011). We assume that for any pass-through security there is a potential lender satisfying a ==== on preferences. It essentially requires an increment in current consumption to be able to compensate losses on past and future consumption. This property guarantees that pass-through security prices are bounded. Otherwise, some agents would be able to implement financial strategies allowing the consumption of plans providing unfeasible utility levels. Our substitution assumption is rather mild as it holds for any time-separable utility function with unbounded per-period utilities. It differs from ==== requirements imposed in infinite-horizon models without collateralized debt (cf. Magill and Quinzii, 1994, Magill and Quinzii, 1996 and Hernández and Santos (1996)).====Our last equilibrium existence result assumes that preferences can be represented by utility functions that are separable node-by-node in at least one commodity (see Theorem 4). As in Moreno-García and Torres-Martínez (2012), we show that agents’ optimal actions in any economy satisfying this additivity condition are the same as those in an economy where our previous substitution condition holds. Therefore, the existence of equilibrium in this case directly follows from our fourth result above.====To illustrate our contributions, we provide examples ofeconomies that conform with the assumptions of each of our equilibrium existence results.====The rest of the paper proceeds as follows: Section 2 sets out the model, notation and equilibrium definition, Section 3 establishes equilibrium existence for the finite-horizon version of our model, Section 4 establishes our main results of equilibrium existence in infinite-horizon economies, and Section 5 provides some concluding remarks. All the proofs are left to the Appendix A Proof of, Appendix B Equilibrium existence in infinite-horizoneconomies.",Financial segmentation and collateralized debt in infinite-horizon economies,https://www.sciencedirect.com/science/article/pii/S0304406818301253,January 2019,2019,Research Article,331.0
Li Jian,"School of Economics, Shanghai University of Finance and Economics, China","Received 25 October 2017, Revised 10 October 2018, Accepted 12 October 2018, Available online 8 November 2018, Version of Record 29 November 2018.",https://doi.org/10.1016/j.jmateco.2018.10.002,Cited by (13),"This paper studies the impact of ambiguity in the classic K-armed bandit problem. I show that two classic results are robust to ambiguity in the multiple-priors model: (i) In the one-armed bandit, the optimal plan is a switching strategy characterized by a multiple-priors Gittins–Jones index. (ii) The seminal Gittins–Jones theorem is generalized to the multiple-priors K-armed bandit case. Introducing ambiguity has two implications. First, in the K-armed bandit case, the incentive to experiment with an arm decreases in its own perceived ambiguity and increases in other arms’ ambiguity, differing from the comparative statics on risk. This suggests that ambiguity might explain the widely observed underexperimentation in new technologies and consumer products. Second, I characterize an upper bound for the multiple-priors Gittins–Jones index, as the lower envelope of the classic single-prior Gittins–Jones index for every prior lying in the multiple-priors set. I show with a ==== that this upper bound can be strict, and I identify sufficient conditions under which this upper bound is exact.","The K-armed bandit problem (Gittins and Jones, 1979) has incited interest in economic applications (Bergemann and Valimaki, 2008), for it captures an intriguing trade-off between information acquisition (experimenting to find the option that pays the most) and exploitation (choosing the option that pays the most according to current knowledge). It has been used to model economic problems such as job search, consumer behavior and market pricing, research and development, adoption of new technology, and collective experimentation (Strulovici, 2010).====The classic bandit model assumes that the decision maker (DM) has a unique prior belief about the payoff distributions. However, the model is often used to study the optimal experimentation with a novel option about which the DM has little information. Thus, the classic unique-prior assumption can be too strong in many cases. For example, workers searching for a new job might not know the exact distribution of the matching quality (Nishimura and Ozaki, 2004); or farmers considering for a new technology might not know the distribution of its productivity (Bryan, 2014).====Motivated by this concern, this paper incorporates ambiguity – where the DM has little information and, thus, cannot form a unique prior about the payoff distributions – into an otherwise classic K-armed bandit problem. I allow the DM to have multiple Bayesian priors about the likelihood distribution of each arm, following a model by Marinacci (2002).==== ==== In every period, exactly one arm is chosen, and an observation is generated for this arm. The DM updates her beliefs about this arm prior-by-prior based on this observation while maintaining beliefs about other arms (arms are independent). The DM chooses a strategy that determines which arm to choose in each period, depending on her initial beliefs and past histories. I assume that the DM evaluates the random payoff stream resulting from a strategy backward recursively, applying the maxmin expected utility (MEU) criterion (Gilboa and Schmeidler, 1989) for the sum of instantaneous utility and discounted next-period continuation utility period-by-period at all histories. This ensures that dynamic programming techniques can be applied.====The first set of results characterizes the optimal strategies of a ====-armed multiple-priors bandit problem. As a result of dynamic consistency, the problem is well-behaved. Theorem 1 shows that in a one-armed bandit problem in which there are only an unknown arm and a safe arm, the optimal strategy is also a switching time strategy characterized by a generalized multiple-priors (MP) Gittins–Jones index. Theorem 2 shows that the seminal Gittins–Jones index theorem (Gittins and Jones, 1979) is robust to the MP preferences. This result highlights that it is the independence (of arms) assumption rather than the expected utility assumption that is driving the Gittins–Jones index theorem.====The first economic insight is that the comparative statics on ambiguity often differ from those on risk in the K-armed bandit problem. For the same DM – hence fixing her risk and ambiguity attitudes – increasing ambiguity in her beliefs about a bandit arm reduces her incentive to experiment with it. Intuitively, if an arm is perceived to be more ambiguous, then, all else equal, the maxmin EU induced by a strategy that chooses the arm more often will decrease more than that induced by another strategy that chooses this arm less often. Hence, the MP Gittins–Jones index of this arm, a measurement of the DM’s incentive to experiment with it, decreases in its degree of ambiguity. In contrast, increasing the riskiness of a bandit may typically raise the option value of experimenting with it. This comparison provides qualitatively different behavioral implications of risk and ambiguity that are testable in data. It also suggests that ambiguity can provide an explanation for the widely observed underexperimentation in new technology and consumer products. For instance, Meyer and Shi (1995) document a one-armed bandit experiment on airline choices and report a modal bias towards undersampling of the unknown airline. Anderson (2012) cites earlier empirical papers on search and provides further experimental evidence that the ambiguity-averse agents have a lower Gittins–Jones index and undervalue information from experimentation.====The second behavioral insight is a potential gap between the multiple-priors Gittins–Jones index and the lower envelope of the classic single-prior Gittins–Jones index for every prior lying in the multiple-priors set. I show by example that the latter can be a strict upper bound of the former, and I identify conditions under which this upper bound is exact.====A broader implication of this bandit model is a simple justification for why ambiguity might not be learned away in the long run: if the information acquisition decision is endogenous and the DM has a safe outside option, then she might stop learning at some finite time and remain ambiguous about the unknown arm.",The K-armed bandit problem with multiple priors,https://www.sciencedirect.com/science/article/pii/S0304406818301198,January 2019,2019,Research Article,332.0
Miyagishima Kaname,"Aoyama Gakuin University, Japan","Received 14 March 2018, Revised 27 October 2018, Accepted 31 October 2018, Available online 8 November 2018, Version of Record 28 January 2019.",https://doi.org/10.1016/j.jmateco.2018.10.005,Cited by (3),"In a simple model where agents have ordinal and interpersonally noncomparable subjective expected utility preferences over uncertain future incomes, we analyze the implications of equity, efficiency, separability, and social rationality. Our efficiency conditions are fairly weak, because there are criticisms on the standard ex ante Pareto principle in the literature.Our social welfare criteria from the axioms satisfy ex ante equity, but violate ====, often referred to as ”the minimal criterion” of rationality under uncertainty.","Although welfare economics has provided various social criteria, there is still disagreement concerning which should be used to evaluate social situations under risk and uncertainty.==== ==== A major reason for this disagreement is tension between equity, efficiency, and social rationality. In this paper, we study implications of these principles in a simple model where agents’ future monetary payoffs (called ====) are uncertain, and preferences are represented by ordinal and interpersonally noncomparable subjective expected utility functions, following the fair social welfare function approach (Fleurbaey and Maniquet, 2011).====We first study social criteria satisfying weak equity and efficiency requirements as well as axioms of separability and invariance of preference changes. In particular, we consider fairly weak efficiency axioms because, as discussed in Section 6, it has been often argued in the literature that the standard ex ante Pareto principle is not compelling. Despite these criticisms, this principle is considered important in many fields of economics. We examine what kind of efficiency principle can be obtained from our axioms.====One of our efficiency conditions is inspired by ==== (Fleurbaey, 2010, Fleurbaey and Zuber, 2017). Another efficiency axiom is ====, which states that increases in all agents’ incomes in every state should imply a social improvement. We consider two equity axioms. The first one, ====, requires that for a pair of agents with the same preference, if one agent has more income in each state than the other, then the inequality should be reduced by a transfer. Another one, ====, insists that any transfer should be socially accepted. The former equity is weaker than the latter. We also introduce separability requiring that agents who are indifferent on a social judgment should not affect the judgment (Fleming, 1952), and an independence of risk preferences whenever riskless allocations are compared (Chambers and Echenique, 2012).====A main contribution of this paper is as follows. We show that the separability, the efficiency principles above, and ==== together imply ==== (Lemma 1), and thus are incompatible with ==== as shown by Fleurbaey and Trannoy (2003). This result is striking because it uncovers a tension between equity, efficiency, and separability in our environment. Lemma 1 would also provide a normative justification to use ==== if the separability is accepted. Using this result, we derive a maximin social criterion based on certainty equivalents (Theorem 1).====By the result above, if ==== is considered more compelling than ====, the separability axiom should be weakened. Then, our next step is to consider a combination of weaker separability termed ==== (Fleurbaey and Maniquet, 2011, Axiom 5.3), ====, and two efficiency principles. ==== requires that an irrelevant agent should not affect social judgements if the agent is unambiguously better off than some other agent. This separability respects information on the worse-off agents which would be important for egalitarian social evaluations. Two other Pareto conditions are introduced. The first is ====, which requires that agents should not be forced to take risks if they do not wish to. The second is ====, which states that if each agent can move from a riskless situation to a risky situation and this risk-taking behavior is supported by all agents, the risk-taking is also socially supported. Using these axioms and the independence axiom, we derive another form of maximin criterion satisfying ==== (Theorem 3). This is another main contribution of this paper.====We also show some impossibilities under ====, often referred to as the “minimal” rationality under uncertainty. Theorem 5 shows that an efficiency axiom weaker than ==== proposed by Gayer et al. (2014) is not compatible with a weak ex post equity axiom, ====, under ====. Theorem 6 implies that a large class of social criteria, including expected equally distributed equivalent criteria (Fleurbaey and Zuber, 2017), does not satisfy ==== under ====. Then, we argue that our criteria satisfy ex ante equity but violate ====.====The remainder of the paper is organized as follows. In Section 2, we present the model. Section 3 analyzes the implications of the first set of axioms including the separability, ====, and ====. In Section 4, we consider social criteria satisfying ==== and ====. In Section 5, we discuss ==== and ex ante equity. In Section 6, the related literature is introduced. Section 7 offers concluding remarks. All proofs and the independence of axioms are shown in the Appendix.",Fair criteria for social decisions under uncertainty,https://www.sciencedirect.com/science/article/pii/S0304406818301228,January 2019,2019,Research Article,333.0
