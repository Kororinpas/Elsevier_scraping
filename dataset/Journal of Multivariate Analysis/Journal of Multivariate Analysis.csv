name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Liang Chunhui,Ma Wenqing,Xing Yanchun","KLAS and School of Mathematics and Statistics, Northeast Normal University, China,School of Statistics, Jilin University of Finance and Economics, China","Received 20 February 2021, Revised 21 May 2023, Accepted 22 May 2023, Available online 2 June 2023, Version of Record 10 June 2023.",https://doi.org/10.1016/j.jmva.2023.105203,Cited by (0),"Multivariate longitudinal studies often involve two or more outcomes of interest measured repeatedly across time for each subject. A main challenge in the analysis of such data is the complex correlation structure. Appropriate modeling of the ==== can provide more efficient parameter estimators. In this paper, multivariate ==== are built for the working correlation matrix of the generalized estimating equations (GEE). A new procedure is proposed to estimate the parameters while ensuring the positive ","The generalized estimating equations (GEE) [17] method is widely used to analyze traditional univariate longitudinal data. The method avoids making assumptions about the joint distribution for the correlated response vector by introducing a working correlation matrix into the estimating equations. The GEE method can provide a consistent estimator and an estimate of variance. However, Crowder [10] has shown that the moment estimators of these nuisance parameters do not exist in some cases, which could cause the singularity problem for the estimated working correlation matrix. Wang and Carey [27] noted that the efficiency of the regression parameter estimators would be seriously affected when the correlation structure is incorrectly specified. Extensive studies have been conducted to improve the efficiency of the regression parameter estimators [5], [7], [22], [33]. What these methods have in common is that the working correlation matrices need to be prespecified; however, the true correlation structure is unknown in practice. The authors in [16] proposed a hybrid GEE method by combining multiple GEEs with different linearly independent working correlation matrices that performs better than the GEE method using a single working correlation structure. However, Xu et al. [32] noted that the hybrid GEE method could be unstable when some of the estimated correlation matrices are nearly singular. Therefore, they proposed a mix-GEE method that represents the working correlation matrix as a combination of a finite number of correlation matrices. Compared to the estimator based on the hybrid GEE method, the mix-GEE estimator is more stable since the mix-GEE method can guarantee the positive definiteness of the estimated correlation matrix under mild conditions. However, this method is suitable only for traditional univariate longitudinal data.====Compared to univariate longitudinal data, with multivariate longitudinal data, each subject has two or more response variables measured repeatedly across time, where the repeated measurements from the same subject and different response variables are likely to be correlated. The complex correlation structure poses a considerable challenge to research with multivariate longitudinal data [8]. We can investigate such data by analyzing one outcome at all times. However, ignoring the correlation among multiple response variables may result in inefficient estimates of the regression parameters. In some cases, the scientific question of interest concerns multiple outcomes as a whole [14]. A comprehensive review of the constant approaches proposed in the statistical literature was provided in [3], [26]. A major work of these authors was to model multiple response variables jointly, for which mixed effects models have been widely applied [11], [12]. In these models, a normal distribution is assumed for the random effects, which is a strict assumption in practice [1], [28], [31]. Meanwhile, computationally intensive and unstable issues may arise since the number of parameters increases with an increase in the sample size or the dimensions of the response variables [8]. Asar and ====lk [2] applied the GEE method based on multivariate marginal models to analyze multiple response variables belonging to the same distribution family. However, the asymptotic properties, such as consistency and efficiency of estimation, have not been studied. Cho [8] applied the quadratic inference function (QIF) method in multivariate marginal models to analyze multivariate longitudinal data. This approach yields a consistent and efficient estimator by incorporating some information about the correlation structure. However, to improve the efficiency of parameter estimation, this method requires more basis matrices to approximate the correlation structure accurately. The results can be unstable or infeasible due to the high-dimensional estimating equations, which could cause the singularity problem for the estimated weighting matrix [9].====In this paper, multivariate finite mixture models for the working correlation matrix in generalized estimating equations (M-mix-GEE) are proposed for multivariate longitudinal data. Meanwhile, the ECM algorithm [14] is applied to estimate these correlation parameters via the maximum pseudo-likelihood. Our proposed approach takes into account the association among multiple response variables based on the serial correlation at different times within the same subject, and it can estimate the regression and correlation parameters simultaneously. Furthermore, the algorithm can ensure the positive definiteness of the estimated correlation matrix under mild conditions. Importantly, according to the estimates of these mixture proportion parameters, the true correlation structure can be identified flexibly. In addition, under some regularity conditions, we show that the proposed method theoretically yields a consistent and asymptotically efficient estimator.====The rest of this paper is organized as follows. Section 2 introduces multivariate finite mixture models and the corresponding estimation procedure. Section 3 investigates the numerical performance of the M-mix-GEE estimators via simulation studies. Section 4 presents an application of our proposed approach to real multivariate longitudinal data. Finally, some concluding remarks are given in Section 5. The proofs and technical derivations are provided in the Appendix.",Multivariate mix-GEE models for longitudinal data with multiple outcomes,https://www.sciencedirect.com/science/article/pii/S0047259X23000490,2 June 2023,2023,Research Article,0.0
"Long Zhanting,Li Zeng,Lin Ruitao,Qiu Jiaxin","Southern University of Science and Technology, China,The University of Texas MD Anderson Cancer Center, United States of America,The University of Hong Kong, Hong Kong, China","Received 10 March 2022, Revised 25 May 2023, Accepted 26 May 2023, Available online 1 June 2023, Version of Record 6 June 2023.",https://doi.org/10.1016/j.jmva.2023.105205,Cited by (0)," sample auto-correlation matrix ====, and derive the limit of its largest singular value. All the asymptotic results are derived under the high-dimensional asymptotic regime where the data dimension and sample size go to infinity proportionally. Under mild assumptions, we show that the LSD of ==== is the same as that of the lag-==== converges almost surely to the right end point of the support of its LSD. Based on these results, we further propose two estimators of total number of factors with lag-==== sample auto-correlation matrices in a factor model. Our theoretical results are fully supported by numerical experiments as well.","Consider a sequence of ====-dimensional stationary random vectors ==== that has a factor structure and can be represented as  ====where ==== is a sequence of ====-dimensional latent factor vectors, and ==== is a sequence of unobservable stochastic error vectors of independent and identically distributed (i.i.d.) components with zero mean and unit variance, independent with ====. Determining the number of factors ==== is a core problem for the factor model, and it possesses many challenges in the high-dimensional setting. Bai and Ng [2] first proposed a consistent estimator for static factor models. Hallin and Liška [14] developed an information criterion for dynamic factor models. Lam and Yao [18] studied the factor model for high-dimensional time series based on lagged auto-covariance matrices. Fan et al. [12] proposed an estimator based on sample correlation matrices to overcome the issue of the heterogeneous scales of the observed variables. In this paper, we study the lagged sample auto-correlation matrix for two reasons. On one hand, we believe that compared with the sample covariance matrix alone, the auto-correlation matrices of different lags may contain more information on ====. Our ultimate goal is to investigate whether or not borrowing information from the auto-correlation matrices of different lags would make the final inference on the unknown number of factors more accurate or efficient. On the other hand, as with Fan et al. [12], the lag-==== auto-correlation matrix overcomes the disadvantage of the heterogeneity among different components by self-normalization.====Mathematically, given the sequence of random vectors ====, we denote the population covariance matrix, the lag-==== (with ==== being a fixed positive integer) auto-covariance, and auto-correlation matrices of ==== as ====, ==== and ====, respectively. Similarly, the population auto-covariance or auto-correlation matrices can be defined for sequences ==== and ==== by way of analogy. For example, ==== is the lag-==== auto-covariance of ====. Let the superscript “====” denote the transpose of a vector or matrix. It is known that the lag-==== auto-correlation matrix  ====exactly has ==== non-null singular values. As a result, based on the i.i.d observed data sample ====, the number of factors ==== can be naturally estimated via the singular values of sample version of the lag-==== auto-correlation matrix  ====Note that, the lag-==== sample auto-covariance matrix is given by  ==== where for a sequence ====, or ====, ==== and by convention ==== for ====. Since ==== is of rank ====, the lag-==== sample auto-covariance matrix of ====, ====, can be treated as a finite rank perturbation of the lag-==== sample auto-covariance matrix of ====, ====, which is of rank ====. Consequently, under certain circumstances, the lag-==== sample auto-correlation matrix of ====, ====, is also a finite rank perturbation of the lag-==== sample auto-correlation matrix of ====, ====, where  ====Hence ==== follows the spike model pattern which is well studied in the random matrix theory (RMT), see, Johnstone [17], Baik and Silverstein [9], Bai and Yao [4] and Benaych-Georges and Nadakuditi [10]. In fact, based on these observations, we proposed two estimators of total number of factors using sample auto-correlation matrices in the application section. Simulation experiments show that both estimators have satisfactory numerical performances.====In order to estimate total number of factors ====, a clear picture is needed for the asymptotic behavior of the singular values of ====, which are effected by the finite rank matrix and ====. As a result, studying the sample auto-correlation matrix of ====, ====, takes the first step to identify the number of factors in factor analysis. In this paper, we study the limiting singular value distribution and the limit of the largest singular value of ==== under the high-dimensional setting where the dimension ==== and sample size ==== are assumed to be of the same order.====Because the eigenvalues of certain large random matrices play a critical role in many multivariate statistical analyses, limiting spectral properties of various matrix models has been widely studied using the RMT. In this paper, we use the tools of RMT to study the limiting spectral properties of the lag-==== sample auto-correlation matrix. There is rich literature on LSD and extreme eigenvalues of large-dimensional matrices. As a pioneering work, Wigner [27], [28] discovered LSD for a large dimensional Wigner matrix and the limiting distribution is known as the semicircle law.  Marčenko and Pastur [21] found that the empirical spectral distribution of sample covariance matrix converges to the Marčenko–Pastur law under mild conditions. Considering the product of random matrices, Yin and Krishnaiah [32], and Yin [29] investigated the LSD of ====, where ==== is sample covariance matrix and ==== is a positive definite matrix. Bai et al. [1] exhibited the existence of LSD of ==== where ==== is an arbitrary Hermitian matrix, and also investigated the LSD of ==== where ==== is a Wigner matrix. Yin et al. [30] and Bai et al. [7] showed the existence of the LSD of multivariate ====-matrix. Bai et al. [8], Wachter [25] and Silverstein [22] derived the explicit form of the LSD of multivariate ====-matrix. The form of ====, where ==== is a Hermitian matrix, ==== is diagonal, and ==== contains independent columns, has been studied by Silverstein and Bai [23]. Bose and Mitra [11] derived the LSD of a circulant matrix. The limiting distributions of eigenvalues of sample correlation matrices were discovered by Jiang [16]. For a high-dimensional time series structure, Li et al. [19] investigated the limiting singular value distribution of sample auto-covariance matrices. Most results are derived via the tools of the Stieltjes transform and moment method.====As for the limiting behavior of extreme eigenvalues, the first known result was established by Geman [13], who showed that the largest eigenvalue of a sample covariance matrix convergences to a limit almost surely under a growth condition on all the moments. Yin et al. [31] improved this result under the existence of the fourth moment. For the Wigner matrix, Bai and Yin [5] found the sufficient and necessary conditions for the almost sure convergence of the largest eigenvalue. Jiang [16] showed the largest eigenvalue of a sample correlation almost surely convergences to the right edge of its LSD support. Vu [24] derived the upper bound for the spectral norm of symmetric random matrices with independent entries. Wang and Yao [26] established the convergence of the largest singular value of a sample auto-covariance matrix based on graph theory.====The results derived in this paper heavily rely on the pioneer work of Jiang [16] and Li et al. [19]. In particular, Jiang [16] showed that LSD for the sample correlation matrix ==== is the same as that for the sample covariance matrix ==== and also established the convergence of the largest eigenvalue of ====. Indeed, inspired by Jiang [16], we try to relate the asymptotic results of singular values of ==== to ==== for fixed ====. Since ==== is not symmetric, we equivalently investigate the limiting behavior of eigenvalues of ====. We show that LSD for ==== is the same as LSD for ==== in Li et al. [19], mimicking the case of ==== and ==== as shown in Jiang [16]. Additionally, we also prove that the largest eigenvalue of ==== converges almost surely to the right edge of its LSD support.====The rest of the paper is organized as follows. Section 2 introduces the main theoretical results in this paper, including LSD and limit of the largest singular value of ====. The detailed proofs of the theorems and lemmas are given in Section 3. Section 4 describes the application of estimating total number of factors based on our theoretical results. Simulation experiments are carried out to check the performance of the proposed estimators.",On singular values of large dimensional lag-,https://www.sciencedirect.com/science/article/pii/S0047259X23000519,1 June 2023,2023,Research Article,1.0
"Zhang Yilin,Zhu Liping","Center of Applied Statistics and Institute of Statistics and Big Data, Renmin University of China, Beijing, China","Received 6 May 2022, Revised 24 May 2023, Accepted 26 May 2023, Available online 1 June 2023, Version of Record 9 June 2023.",https://doi.org/10.1016/j.jmva.2023.105204,Cited by (0)," into ==== slices, each of size ====. The entire procedure has the complexity of ====, which is prohibitive if ==== is extremely large. To alleviate computational complexity, we implement this slicing procedure together with a block-wise estimation, which divides the whole sample into ==== blocks, each of size ====. This block-wise and slicing estimation has the complexity of ====, which reduces the computational complexity substantially if ==== and ==== are relatively small. The resultant estimation is asymptotically normal and has the convergence rate of ==== is relatively small, indicating that the block-wise implementation does not result in power loss in independence tests. We demonstrate the computational efficiencies and theoretical properties of this block-wise and slicing estimation through simulations and an application to psychological datasets.","Testing for statistical independence and measuring the degree of nonlinear dependence between random vectors are closely relevant and yet different issues. Both are fundamental problems in machine learning and statistics communities. Let ==== and ==== be two random vectors. It is generally anticipated that, ==== and ==== are independent if and only if a metric, when it is designed to measure the degree of nonlinear dependence between ==== and ====, is equal to zero exactly at the population level. Suppose a random sample of size ====, denoted by ====, is available. We argue that, in the big data era, an ideal metric should possess at least the following two properties at the sample level:====Numerous efforts have been devoted to testing for statistical independence and measuring the degree of nonlinear dependence. In the univariate case, Pearson correlation is the first and perhaps the most popular metric to measure the degree of linear dependence. Extensions include Spearman [31]’s ==== and Kendall [17]’s ====. Both can detect monotone dependence even in the presence of outlying observations. Sample estimates of these metrics are asymptotically normal and computationally efficient. However, at the population level these metrics can be zero even when the two random variables are statistically dependent. Another line of extensions include [1], [3], [8], [13], [19]. Estimating these metrics is usually computationally efficient. Some estimates are even asymptotically distribution-free. However, their asymptotic null distributions are generally not normal, and most are weighted summations of finite or infinite number of chi-square distributions. This typically limits their usefulness in practice because most practitioners, particularly those who are not experts in statistics, are much more familiar with normal distribution than other distributions. Chatterjee [4] proposed a rank-based independence test, which is computationally efficient and asymptotically normal. However, Zhang et al. [37] stated that the power performance of this rank test has much room to improve. In addition, it is unknown how to generalize the applications of such rank tests to the multivariate case.====Many metrics are designed to test independence and measure association between multivariate random vectors. For instance, distance correlation [33] measures the discrepancies between the joint and the product of marginal characteristic functions. Zhu et al. [40] and Kim et al. [18] propose to measure the discrepancies between the joint and the product of the marginal distributional functions. These metrics can be unified under the same framework [34]. Their asymptotic null distributions are weighted summations of infinite number of chi-squares, and yet the weights depend upon the parent distributions of both random vectors. Shen et al. [27] suggest to approximate the asymptotic null distributions with a single chi-square distribution. Gretton et al. [10], [11] and Yin and Yuan [36] introduce several metrics in the reproducing kernel Hilbert space. Their asymptotic behaviors are all dependent upon the parent distributions. In addition, implementing the above independence tests involves the complexity at least of order ====, which is computationally very inefficient, sometimes even prohibitive, in the big data era. Deb and Sen [7] and Shi et al. [28] proposed an asymptotically distribution-free independence test, borrowing the strength of optimal transportation to ensure that the coordinates of transformed random vectors are independent of each other. Implementing this test however involves the complexity of ====.====In this article, we introduce projection divergence in the reproducing kernel Hilbert space, to test for independence and measure nonlinear association between a univariate random variable ==== and a multivariate random vector ====. At the population level, projection divergence is zero if and only if ==== and ==== are independent. At the sample level, we introduce a block-wise and slicing procedure to estimate projection divergence. The classic slicing procedure [21] divides the whole random sample of size ==== into ==== slices, each of size ====, according to the realizations of the univariate random variable. This procedure has the complexity of ====. To alleviate computational complexity, we implement this slicing procedure together with a block-wise estimation, which randomly divides the whole sample into ==== blocks, each of size ====. This block-wise and slicing procedure has the complexity of order ====, which is suboptimal yet generally acceptable if both ==== and ==== are relatively small. More importantly, the resulting estimation is asymptotically normal. A consistent estimate of the asymptotic variance can be easily constructed. Our theoretical investigations indicate that the block-wise estimation does not necessarily inflate the asymptotic variance. As a consequence, implementing our independence test does not result in power loss.====We demonstrate the desirable properties of our proposal through a toy example. The data generating process is given in Study 1(a), Section 3, with the sample size ====. We fix ==== and vary ====. The block-wise and slicing estimation with ==== reduces to its classic version. The empirical densities of our normalized statistics with different ==== values are displayed in Fig. 1(a), all are close to the standard normal density. Fig. 1(b) presents the elapsed time (in seconds) to implement our proposed independence test. We fix ==== and vary ====. It can be seen that, when ==== is fixed, the smaller ==== is, the less the elapsed time it takes. Implementing our proposal with the block-wise and slicing procedure takes less one second if ====, and more than 6 min if ====. In this sense, the block-wise and slicing estimation gains substantial computational efficiency.====This paper is organized as follows. In Section 2, we introduce projection divergence in the reproducing kernel Hilbert space. We propose a block-wise and slicing procedure to estimate the kernel projection divergence, and investigate its asymptotic behaviors systematically. In Section 3, we illustrate the estimation efficiency, asymptotic normality and power performance of our proposal on the finite-sample basis. This paper is concluded with brief discussions in Section 4.","Projection divergence in the reproducing kernel Hilbert space: Asymptotic normality, block-wise and slicing estimation, and computational efficiency",https://www.sciencedirect.com/science/article/pii/S0047259X23000507,1 June 2023,2023,Research Article,2.0
Sundararajan Raanju R.,"Department of Statistical Science, Southern Methodist University, Dallas, TX, USA","Received 9 January 2022, Revised 18 May 2023, Accepted 18 May 2023, Available online 30 May 2023, Version of Record 9 June 2023.",https://doi.org/10.1016/j.jmva.2023.105202,Cited by (0),A frequency domain factor model method for multivariate ,"Time series data from several applications appear often in multivariate form which makes the statistical analysis challenging from a computational and theoretical standpoint. Vector autoregressive moving average models (Lütkepohl [23]), also known as VARMA, are commonly used to model dependence in multivariate stationary time series due to its straightforward implementation, interpretation and its ability to provide predictions. VARMA models involve several challenges related to estimation, identifiability and lag order mis-specification. Dimension reduction hence becomes a very relevant and important problem in the analysis of multivariate time series.====Among dimension reduction techniques in the time domain, factor models and independent component analysis (ICA) aim at linearly transforming the components of the observed multivariate time series into a few useful independent/orthogonal components or factors; see Matteson and Tsay [25] and Motta and Ombao [27] for examples. Pena and Box [33] and Lam and Yao [21] are two examples of factor model methods where the observed multivariate series is assumed to be linearly generated by a lower dimensional latent factor series that is allowed to be multivariate stationary. An alternative approach involves the restriction of parameters in parametric multivariate time series models. Davis et al. [15] propose a method wherein the significant VAR model coefficients are detected using a test of partial spectral coherence between the components. Principal component analysis (PCA) for multivariate time series is another dimension reduction approach attempted in Stock and Watson [37] and Chang et al. [10]. The typical objective in PCA is to find contemporaneous linear transforms of the observed series resulting in several orthogonal univariate subseries. The work in Chang et al. [10], however, aims at linear transforms of the observed series into multivariate stationary subseries where serial and contemporaneous dependence is allowed between the components of the subseries. In analyzing neuroimaging data such as fMRI and its associated problems such as estimating functional connectivity, time domain methods are unable to perform frequency wise analysis to understand dependence between brain regions at different frequencies.====Among dimension reduction techniques in the frequency domain, the classical dynamic PCA for stationary time series from Brillinger [7] expresses the observed multivariate series as a two-sided moving average of an uncorrelated vector process. This vector process is called the principal component series and has a diagonal spectral matrix with no spectral coherence between any two components. Other PCA type methods, for stationary and nonstationary time series, have been attempted in Ombao et al. [29] and  Ombao and Ho [28] wherein the objective is to find contemporaneous linear transforms of the observed series resulting in several orthogonal univariate subseries. In the limited factor modeling literature in the frequency domain, Stoffer [38] and Macaro and Prado [24] are two examples that decompose the observed multivariate time series as generated by a few independent/orthogonal univariate factor series. The former estimates the factor loadings using an eigenanalysis on the spectral matrices and the latter adopts a Bayesian nonparametric approach. In Forni et al. [17], the observed series is assumed to be linearly generated by lagged values of a latent factor series (referred to as common shocks) with orthogonal components. The noise term in their setup is allowed to be cross-sectionally dependent, and the latent process is recovered using an eigendecomposition of the spectral matrices. The methods in Stoffer [38] and Forni et al. [17] do not include formal tests or any methodological techniques to estimate the number of factors in a factor model setup that involves eigenvalues and eigenvectors of the spectral matrices. Hallin and Liška [19] consider the same setup as in Forni et al. [17], but construct an information criterion metric involving an integrated form of the eigenvalue functions of the spectral density matrix. This metric is utilized for inferring the number of factors, under the assumption of diverging eigenvalues. Li et al. [22] is a recent work that concerns estimation of high-dimensional spectral matrices of locally stationary time series. After assuming an approximate rank factorization of the time-varying spectral matrix, their estimation of the square roots of the spectral matrices at different frequencies is carried out using a Bayesian approach.====In this paper, we propose a new frequency domain factor model method for multivariate stationary time series. The observed ====-variate series ==== is assumed to be linearly generated by an ====-variate series ====, where ====, via sums of orthogonal frequency components. More specifically, each frequency component of ==== is assumed to be linearly generated by the corresponding frequency component of the series ==== using a real-valued frequency-specific loadings matrix. These factor loadings matrices are then estimated using an eigenanalysis on symmetric nonnegative definite matrices involving the real and imaginary parts of the spectral matrices of ====. Consistency results on the estimation of eigenvalues, eigenvectors, factor loadings matrices and the factor dimension are provided. The advantages of the proposed method are (a). Unlike many time domain and all frequency domain methods on factor models, the factor series ==== need not have components that are orthogonal/independent/uncorrelated. This factor series, in the proposed setup, is assumed to be multivariate stationary and this is a less restrictive assumption. Further, allowing for dependent factors is a more realistic assumption in the analysis of various biomedical time series; see Cardoso [9] and Lahat et al. [20] for related discussions on such applications.; (b). The estimation of the factor loadings matrices involves an eigendecomposition of symmetric nonnegative matrices and is computationally less expensive than some aforementioned literature involving Bayesian techniques and numerical optimization; (c). A nonparametric bootstrap test for determining the dimension ==== of the factor series, also referred to as factor dimension, is proposed wherein the test statistic is based on eigenvalues of the symmetric nonnegative definite matrices involving the real and imaginary parts of the spectral matrices; (d). The method enables frequency-specific factor analysis and we utilize this unique feature to understand dependence (functional connectivity) in resting-state networks of fMRI time series at different frequencies.====The rest of the paper is organized in the following manner. In Section 2, the factor model assumption using frequency components is presented followed by a detailed description of the proposed method to estimate the factor loadings matrices and the factor dimension. The consistency results concerning the eigenvalues, eigenvectors and the factor dimension are in Section 2.2. The performance of the proposed method in finite sample situations is discussed in Section 3 using a few simulation examples. An application of the proposed work in analyzing fMRI time series data from autism individuals is presented in Section 4. Here, differences in fMRI data from autism and healthy individuals are investigated using a frequency-specific factor analysis. The concluding remarks are given in Section 5.",Factor modeling of multivariate time series: A frequency components approach,https://www.sciencedirect.com/science/article/pii/S0047259X23000489,30 May 2023,2023,Research Article,3.0
"Tezuka Taiki,Kuroki Manabu","Department of Mathematics, Physics, Electrical Engineering and Computer Science, Yokohama National University, 79-1 Tokiwadai, Hodogaya-ku, Yokohama 240-8501, Japan","Received 4 May 2022, Revised 4 May 2023, Accepted 4 May 2023, Available online 12 May 2023, Version of Record 30 May 2023.",https://doi.org/10.1016/j.jmva.2023.105201,Cited by (0),This paper assumes a context in which cause–effect relationships between random variables can be represented by a Gaussian linear ,None,An unbiased estimator of the causal effect on the variance based on the back-door criterion in Gaussian linear structural equation models,https://www.sciencedirect.com/science/article/pii/S0047259X23000477,12 May 2023,2023,Research Article,4.0
Müller Dennis,"Institute of Mathematics, University of Rostock, Ulmenstrasse 69, D-18051 Rostock, Germany","Received 10 May 2022, Revised 28 April 2023, Accepted 28 April 2023, Available online 8 May 2023, Version of Record 20 May 2023.",https://doi.org/10.1016/j.jmva.2023.105200,Cited by (0), and use covering methods to define a mode estimator and deduce strong consistency and rate ====.,"It is common that datasets emerging from observations in the fields of finance, chemometrics (especially spectrometrics), biometrics, econometrics or medicine consist of a collection of functions. Thus, there has been an extensive study of statistical models that are suitable for a sample of functions and the developed techniques have been applied to a broad spectrum of scientific fields. We can refer to [28] for a thorough overview of the applications of functional methods to datasets from various scientific branches between the years 1995 and 2010. It is our aim to define and estimate the mode of a functional random variable.====In finite-dimensional settings, the mode is a popular notion of centrality in classification tasks because of its usefulness in depicting groups. It is also less sensitive to outliers than the mean. It is our goal in this work to extend the concept of a mode to infinite-dimensional spaces and estimate the mode of a functional random variable (or of its probability distribution, which is an equivalent task).====In textbook literature, the mode is usually defined for a Lebesgue-continuous distribution whose density attains some maximum and is set equal to the respective maximum point (e.g., [13], [14], [19], [20], [22], [32]). An important distinction between the existing definitions lies in the requirement (or omission) of smoothness constraints at and/or around the mode. For instance, whereas continuity at the mode is a typical requirement (e.g., [20]), Meister [18] and Milbrodt [19] give definitions omitting any smoothness constraint at the mode.====Since there is no analogue of the Lebesgue-measure on an infinite-dimensional (Banach) space, it is usually difficult to describe probability measures defined on such spaces by the means of density functions. We will solve that issue by defining the mode in a way that only relies on the small ball probability functions of centre points in the space. We will prove that our approach is consistent with the common, density-based definition. The uniqueness of the mode will be a direct implication of the statement of its definition.====The history of mode estimation in a univariate setting goes back to Parzen [21], who introduces the kernel density estimator (KDE). Therein, conditions are established which guarantee the uniqueness of the (global) maximum point of the KDE, which is then declared the mode estimator. Parzen continues to prove both consistency and asymptotic normality. The properties of that kernel mode estimator (KME) have been thoroughly investigated and extended to a multivariate setting in the last decades (e.g., [1], [6], [7], [11], [15], [25], [26], [30]). In fact, analogues of the KME in an infinite-dimensional setting have already been studied (e.g., by Ferraty and Vieu [8] and Dabo-Niang et al. [4]), although minimax optimality has yet to be established. The authors assume that the distribution of the functional random variable ====, which is supposed to take values in a separable, infinite-dimensional semi-metric space, can be described by the means of a density function ==== with respect to some abstract measure ====. Consequently, using kernel techniques, they define a functional version of the KME and prove its consistency for the estimation of the mode, which itself is set equal to the unique maximum point of the abstract density. The constraints imposed in these works include the uniform continuity of ==== as well as the regularity of the small ball probability functions. The notion of density for a square-integrable, compactly supported random function ==== has been taken up by Delaigle and Hall [5].====To our best knowledge as of today, there exist no results on minimax optimality of the mode estimation problem on an infinite-dimensional space. Our approach will combine the concepts of small ball probability functions and covering numbers. We will assume that the mode is contained within some set ==== for which finite ====-covers exist, which means there is a finite collection of balls with radii smaller than or equal to ==== such that ==== is covered by the union of these balls. We will define two different mode estimators and analyse their asymptotic behaviour under additional requirements, e.g., bounds on small ball probabilities. In either scenario, the mode estimator will be set equal to the centre point of some ball for which the amount of data points that are located within it is maximised. Rates of convergence are established for one of these two mode estimators under constraints for the small ball probability functions of the mode itself and of points in a neighbourhood of it. In particular, in order to derive the lower bound of the minimax risk, we will restrict our considerations to the setting in which ==== is equal to a Sobolev class of functions.====Our results fit in the frameworks for functional variable statistics given by Goia et al. [10] and Aneiros et al. [2]. The methods developed in this work can be further applied to handle unsupervised classification problems. Our approach to mode estimation relies on detecting sets (e.g., balls centred around data points) with large probability mass, which can be used to cluster a sample of random curves or detect outliers far from the mode. A notion of a mode is proposed by Cuevas [3], which goes back to Dabo-Niang et al. [4]. Therein, the ====-mode is defined using kernel techniques. While neither of our two mode estimators relies on kernel methods, they also involve a bandwidth parameter, namely the radius ==== of balls. The minimax results presented in this work hold under asymptotic restrictions for the radius. It is of great interest for future research to develop error-minimising bandwidth selection procedures. For the presentation of our results we will rely on using aspects of small ball probability theory, which is a tool regularly used in nonparametric functional data analysis.====A definition of the mode is given in Section 2.1 and two estimators will be introduced in Section 2.2, whose asymptotic properties are studied in Section 3.====In the following, let ==== denote a Polish metric space and set ==== equal to the set containing all probability distributions on ====, where ==== is the Borel ====-algebra of ====. For every ==== and ==== we set ====, where ==== is the closed ball around ==== with radius ====. We set ====.====Additionally, for every ==== and non-empty ==== shall denote the ====-covering number of ====, which is equal to the minimum cardinality of a set ==== such that ====.",Minimax estimation of the mode of functional data,https://www.sciencedirect.com/science/article/pii/S0047259X23000465,8 May 2023,2023,Research Article,5.0
"Dörr Christopher,Schlather Martin","Universität Mannheim, Institut für Mathematik, 68131 Mannheim, Germany","Received 10 August 2022, Revised 28 April 2023, Accepted 28 April 2023, Available online 5 May 2023, Version of Record 7 June 2023.",https://doi.org/10.1016/j.jmva.2023.105199,Cited by (0),"So far, the pseudo cross-variogram is primarily used as a tool for the structural analysis of multivariate random fields. Mainly applying recent theoretical results on the pseudo cross-variogram, we use it as a cornerstone in the construction of valid ","Multivariate data are of ever-increasing importance in today’s world. They usually show dependencies between the variables and therefore contain additional information to exploit; so joint instead of separate modeling is needed to use them to full advantage. A popular approach in that regard is to use multivariate random fields, and to describe the dependence structure via a cross-covariance function. This way, cross-covariance functions are applied to problems in various areas, which include atmospheric science [3], [22], [38], meteorology [8], [16], [24], [30], oceanography [39], or geology [50], for instance.====Constructing valid cross-covariance functions is a challenging task. Several approaches have been proposed, which include latent dimensions [3], the linear model of coregionalization [32], convolution methods [48], [67], deformations [61], [69], multivariate adaptation [4], [30], [50], [53], mixtures [8], [58], and a conditional approach [16]. The benefits and drawbacks of the resulting models are sufficiently well-known. For instance, all referenced models stemming from the multivariate adaptation approach are symmetric, which can lead to inferior predictions [39], [68], [70].====Pseudo cross-variograms are useful quantities for the structural analysis of multivariate random fields. Apart from their usage in multivariate geostatistics, they also appear naturally in extreme value theory in the context of multivariate Brown–Resnick processes [25]. Only recently, theoretical results on pseudo cross-variograms have been established [19], which bridge between pseudo cross-variograms and matrix-valued correlation functions through a matrix-valued version of Schoenberg’s theorem [64]. This intimate connection adds another dimension to the range of applications of pseudo cross-variograms, that is, the construction of valid covariance models for multivariate random fields. In this regard, pseudo cross-variograms have already been used in [1], [19], [54] to propose several extensions of Gneiting’s popular univariate space–time covariance model [29], thereby meeting one of the requests of [14] for flexible space–time cross-covariance models.====Our aim here is to further highlight the potential of pseudo cross-variograms for the construction of (asymmetric) cross-covariance models. To this end, we present several selected extensions of univariate constructions found in the literature which can be transferred to the multivariate case via pseudo cross-variograms. We also illustrate that pseudo cross-variograms can be used to further generalize, in some sense, parsimonious cross-covariances.====In Section 2, we briefly provide the necessary background on pseudo cross-variograms and cross-covariance functions. In Section 3, we present a general construction principle for conditionally negative definite kernels and connect it with existing construction principles for pseudo cross-variograms. In the remaining sections, we present several matrix-valued covariance models, starting with mixture proposals in Section 4, followed by non-stationary models, and models involving derivatives in Sections 5 Non-stationary spatial models, 6 Derivative related results. Eventually, we present a particular class of infinitely divisible positive definite matrix-valued models in Section 7.",Covariance models for multivariate random fields resulting from pseudo cross-variograms,https://www.sciencedirect.com/science/article/pii/S0047259X23000453,5 May 2023,2023,Research Article,6.0
"Bhagwat Pankaj,Marchand Éric","Université de Sherbrooke, Département de mathématiques, Sherbrooke Qc, Canada, J1K 2R1","Received 30 September 2022, Revised 18 April 2023, Accepted 18 April 2023, Available online 26 April 2023, Version of Record 6 May 2023.",https://doi.org/10.1016/j.jmva.2023.105190,Cited by (0),"This paper addresses the problem of an efficient predictive density estimation for the density ==== of ==== based on ==== for ====. The chosen criteria are integrated ==== loss given by ====. For absolutely continuous and strictly decreasing ====, we establish the inevitability of scale expansion improvements ==== over the plug-in density ====, for a subset of values ====. The finding is universal with respect to ====, and ====, and extended to loss functions ==== with strictly increasing ====. The finding is also extended to include scale expansion improvements of more general plug-in densities ====, when the parameter space ====. Numerical analyses illustrative of the dominance findings are presented and commented upon. As a complement, we demonstrate that the unimodal assumption on ==== is necessary with a detailed analysis of cases where the distribution of ==== is uniformly distributed on a ball centered about ====. In such cases, we provide a univariate (====) example where the best equivariant estimator is a plug-in estimator, and we obtain cases (for ====) where the plug-in density ==== is optimal among all ====.","We consider the problem of obtaining an efficient predictive density estimator ====, ====, of the density ==== of ==== based on spherically symmetric distributed ====. In this set-up, the densities are Lebesgue on ====, ==== and ==== are known but not necessarily equal, and ==== and ==== are independently distributed (which we assume throughout). The observable ==== may be a summary statistic arising from a sample. We evaluate the efficiency of the predictive density of ==== with integrated ==== loss and risk ==== Spherically symmetric models are prominent in statistical theory and practice and inference for such models have a long history, including shrinkage estimation techniques (e.g., Fourdrinier et al. [9]). Our set-up includes the normal case with ====as well as scale mixtures of normals with ==== and ==== random, and including multivariate Cauchy, Student, Laplace, Logistic distributions, and many others.",Predictive density estimators with integrated ,https://www.sciencedirect.com/science/article/pii/S0047259X23000362,26 April 2023,2023,Research Article,7.0
"Kim Rakheon,Pourahmadi Mohsen,Garcia Tanya P.","Department of Statistical Science, Baylor University, Waco, TX 76798, USA,Department of Statistics, Texas A&M University, College Station, TX 77843, USA,Department of Biostatistics, Gillings School of Global Public Health, UNC Chapel Hill, Chapel Hill, NC 27516, USA","Received 11 April 2022, Revised 29 March 2023, Accepted 29 March 2023, Available online 6 April 2023, Version of Record 17 April 2023.",https://doi.org/10.1016/j.jmva.2023.105186,Cited by (0)," or deal with the ==== of the covariance estimator. Focusing on the classical setting when the number of Gaussian variables is fixed and the sample size increases, we construct a positive definite and asymptotically efficient estimator by the iterative conditional fitting algorithm (Chaudhuri et al., 2007) when the location of the zero entries is known. If the location of the zero entries is unknown, we further construct a positive definite thresholding estimator by combining the iterative conditional fitting algorithm with thresholding. We prove our thresholding estimator is asymptotically efficient with ==== tending to one. In simulation studies, we show our estimator more closely matches the true covariance and more correctly identifies the non-zero entries than competing estimators. We apply our estimator to a neuroimaging study of Huntington disease to detect non-zero correlations among brain regional volumes. Such correlations are timely for ongoing treatment studies to inform how different brain regions are likely to be affected by these treatments.","In multivariate data analysis, like with neuroimaging data, the strength of relationships between variables is often identified with a sparse covariance matrix where zero entries mean no linear relationship and non-zero entries indicate the relationship strength [3]. Estimating constrained covariance matrices, like those with some zero entries, variance components models and factor model structures, is difficult mostly due to the notorious positive definiteness requirement. Despite substantial progress on estimation of a covariance matrix with zero entries, constructing a positive definite estimator whose non-zero entries are consistent and asymptotically efficient remains a challenge. Positive definiteness is an essential requirement to produce a valid covariance matrix estimator and has been addressed by convex optimization for soft thresholding [29], [37] or non-convex optimization for ==== penalized likelihood estimator [6]. However, non-zero entries of these estimators are biased, and hence fail to be asymptotically efficient. Without an asymptotically efficient estimator, our confidence in correctly estimating the non-zero entries is substantially reduced. Focusing on the classical setting when the number of variables is fixed and the sample size increases, we overcome these challenges by constructing a positive definite and asymptotically efficient estimator under Gaussian assumption.====We observe that when the location of the zero entries in a covariance matrix is known, then the estimation problem falls in the framework of linear covariance models in Anderson [2] where the maximum likelihood estimator is asymptotically efficient only when the iterative algorithm starts from a consistent estimator. Moreover, the algorithm is not guaranteed to converge. Fortunately, Chaudhuri et al. [8]’s iterative conditional fitting algorithm converges to a positive definite solution of the likelihood equation, but asymptotic efficiency of such estimator is unknown due to multiple local maxima of the normal likelihood function [8], [39].====Our first contribution in this paper follows from combining the advantages of asymptotic efficiency of Anderson [2] and the convergence property of Chaudhuri et al. [8] when the location of the zero entries is known. Specifically, we prove that the iterative conditional fitting algorithm will produce a positive definite and asymptotically efficient covariance estimator when the algorithm starts from a consistent estimator. In contrast to Anderson [2], we suggest an explicit way to construct the starting consistent estimator by applying one iteration of the iterative conditional fitting algorithm to the sample covariance matrix.====Our second contribution deals with the more common situation where the location of the zero entries is unknown. Our proposed estimator determines location of the non-zero entries by thresholding the sample covariance matrix in the first stage and then applies the iterative conditional fitting algorithm to refit the non-zero entries in the second stage. Two-stage approaches which involve thresholding have been widely studied. For example, Van de Geer et al. [15] discussed least squares refitting for the thresholded LASSO regression. In time series, Davis et al. [10] proposed two-stage methods that involved thresholding the t-statistics of the coefficients for vector autoregression. In Gaussian graphical models, Wang and Allen [34] provided comprehensive review of such two-stage thresholding approaches and proposed thresholded graphical LASSO estimator for improved rate of convergence. In this paper, our focus is on covariance matrix and we propose to combine thresholding with refitting by maximum likelihood estimation. We prove that the second stage after thresholding assures the estimator to be always positive definite and asymptotically efficient with probability tending to one.====Certainly, performance of thresholding estimators depends on the selection of the threshold parameter [4]. An advantage of our thresholding estimator is that we can now appeal to the Akaike Information Criterion (====) and Bayesian Information Criterion (====) because we assume normality of the data. Our approach not only allows us to select the threshold parameter but also answers the following question posed in the introduction of Li and Zou [19]: what is the analogue of ==== or ==== for the covariance matrix estimation? To the best of our knowledge, we are the first to use ==== and ==== to select the threshold for covariance matrix estimation with thresholding. We discuss theoretical properties of ==== and ==== to support our choice of the threshold parameter.====The outline of the paper is as follows. Section 2 proposes a positive definite and asymptotically efficient estimator of a covariance matrix with zero entries. Since such an estimator is available only when the location of zero entries in a covariance matrix is known, we also discuss the implication of unknown zero entries in the matrix. Section 3 extends the discussion further by assuming that the location of the zero entries is unknown and proposes a new thresholding estimator which is always positive definite and asymptotically efficient with probability tending to one. In Section 4, we conduct simulation studies to compare the proposed estimators in Sections 2 MLE of covariance matrices with zero entries, 3 A positive-definite thresholding estimator with efficiency with existing estimators. In Section 5, we apply our new thresholding estimator to Huntington disease study to identify the relationships among different brain regions.",Positive-definite thresholding estimators of covariance matrices with zeros,https://www.sciencedirect.com/science/article/pii/S0047259X23000325,6 April 2023,2023,Research Article,8.0
"Wu Chengxin,Ling Nengxiang,Vieu Philippe,Liang Wenjuan","School of Mathematics, Hefei University of Technology, Hefei 230009, China,Institut de Mathématiques, Université Paul Sabatier, Toulouse, France,School of Mathematics and Statistics, Huangshan University, Huangshan, 245041, China","Received 16 April 2022, Revised 31 March 2023, Accepted 31 March 2023, Available online 5 April 2023, Version of Record 21 April 2023.",https://doi.org/10.1016/j.jmva.2023.105189,Cited by (0)," in the model, we also give a variable selection procedure by the method of adaptive LASSO penalty and establish the oracle property of the proposed weighted penalized estimators simultaneously. Finally, some simulation studies and a real data analysis are carried out to show the performances of the proposed methods.","Over the past two decades, with the development of modern technology of data collection and storage, data are being recorded in the form of random curves and surfaces or images, which is called functional data. Functional data analysis (FDA) has become an important field of statistical research. For an introduction to this field, one can refer to the monographs by [1], [4], [5], [12], [16], [17], [31], [34], [47]. For the latest contributions in FDA and its related topics, one can refer to the bibliographical reviews by [2], [3], [9], [13], [27], [30], [38], among others.====Quantile regression (QR) has emerged as a considerable statistical methodology for data analysis since the seminal work of Koenker and Bassett [23]. This technique is an alternative to mean regression, and has many desirable properties such as it is more efficient than mean regression when the data follows a heavy-tailed distribution, and is often used to describe the entire conditional distribution of the response. While in the case of FDA, some scholars had focused on the study of QR model. For example, Cardot et al. [7] introduced penalized quantile regression when the covariates are functional. Kato [21] obtained the convergence rates of the estimators under suitable norms and showed that the convergence rate of the estimator for the unknown slope function was optimal in a minimax sense. Lu et al. [28] and Tang and Cheng [36] studied respectively the convergence rates of the quantile estimation for the partially functional linear model, and established the asymptotic performances of the finite-dimensional parameter respectively. Yu et al. [45] investigated the quantile estimation of the partially functional linear model for neuroimaging data analysis. Recently, Du et al. [10] developed the methods of estimation and variable selection for a partially functional linear regression model based on the composite quantiles. Ma et al. [29] studied the quantile estimation of a partially functional linear model with multiple functional predictors and ultrahigh-dimensional scalar covariates. Zhu et al. [49] proposed a novel estimator of the extreme conditional quantiles for the partial functional linear regression model with heavy-tailed distributions, and also established the asymptotic properties of the estimators.====It should be noted that all the contributions involved above are in the case of the samples being observed completely. However, in many practical works such as market surveys, medical studies, reliability test and so on, the response may not be completely observed due to the censorship. Compared with the complete data, there are few studies on the functional quantile regression model when the responses are censored. For this content, we can only cite a few articles. For example, Chaouch and Khardani [8] studied the conditional quantile estimation of a randomly censored scalar response variable given a functional random covariate when considering stationary ergodic data. Jiang et al. [19] investigated the method of martingale-based estimation for the censored partially linear quantile regression model. On the other hand, although the censoring indicators are usually assumed to be observable, sometimes they may be also observed incompletely in practice. In recent years, some authors have done a little work on quantile regression when some censoring indicators are missing. For example, Shen and Liang [33] considered the data with the responses being right-censored, while the censoring indicator is missing at random (MAR) in the partially linear varying-coefficient quantile regression model. Zou et al. [51] studied quantile regression and variable selection for the partially linear single-index model with missing censoring indicators. Wang et al. [40] considered the weighted composite quantile estimation of the linear model when the responses are right-censored and the censoring indicators are MAR. However, as far as we know, there have been no studies on FDA that contain the responses censoring and missing censoring indicators. In addition, variable selection is an effective method that can remove the insignificant variables in the large amount of data collection in modeling, and can also improve the efficiency of model estimation. For example, a large number of penalty methods had been developed and used in the selection of important variables such as LASSO (see Tibshirani [37]), SCAD (see Fan and Li [11]), adaptive LASSO (see Zou [50]), and so on. Recently, the variable selection method has also been used in some partially functional quantile regressions with complete data, one can refer to Yao et al. [44], Du et al. [10], and Ma et al. [29], etc.====Inspired by the results above, in this paper, we aim to study the partially functional linear quantile regression model and variable selection when the response variables are right-censored and the censoring indicators are MAR. To the best of our knowledge, there is no literature on this subject. Our contributions include the following aspects. Firstly, we adopt the functional principal component analysis method to reduce the dimensionality of the functional part of the model. Then we propose weighted quantile regression estimators by combining calibration; imputation and inverse probability weighting methods to get the estimator of the unknown parameter and function. The convergence rate of the estimators for the slope function and the asymptotic distribution of the estimators for the finite scalar parameters are also established respectively. Furthermore, we study the variable selection of the model by using the adaptive LASSO penalty approach. Finally, a simulation study and a real data analysis are conducted to evaluate the performance of the proposed methods.====The rest of the paper is organized as follows. In Section 2, we present the partial functional linear quantile regression model with the censoring indicators being MAR, and then develop the three weighted estimation methods for the model. Meanwhile we use the adaptive LASSO penalty to select the scalar covariates in the model, and establish the oracle property of the proposed weighted penalized estimators simultaneously. In Section 3, we give some assumptions and the main results of this paper. The simulation studies and a real data analysis are carried out in Section 4. Some concluding comments are given in Section 5. The proofs of the main results are provided in Appendix A. Finally, the supplementary data for the numerical studies are given in Appendix B.",Partially functional linear quantile regression model and variable selection with censoring indicators MAR,https://www.sciencedirect.com/science/article/pii/S0047259X23000350,5 April 2023,2023,Research Article,9.0
Feldman Michael J.,"Department of Statistics, Sequoia Hall, Stanford University, Stanford CA 94305, United States of America","Received 10 January 2022, Revised 28 March 2023, Accepted 29 March 2023, Available online 3 April 2023, Version of Record 12 April 2023.",https://doi.org/10.1016/j.jmva.2023.105187,Cited by (0),"The behavior of the leading singular values and vectors of noisy low-rank matrices is fundamental to many statistical and scientific problems. Theoretical understanding currently derives from asymptotic analysis under one of two regimes: ====, with a fixed number of rows, large number of columns or vice versa; and ====, with large numbers of rows and columns, proportional to one another. This paper is concerned with the ==== regime, where the matrix is either “tall and narrow” or “short and wide”: we study sequences of matrices of size ==== or ==== as ====. This regime has important “big data” applications.==== corresponding to the longer of the two matrix dimensions are asymptotically uncorrelated with the noise-free signal.","The low-rank signal-plus-noise model is a simple statistical model of data with latent low-rank structure. Data ==== is the sum of a low-rank matrix and white noise: ====where ==== are signal strengths, ==== and ==== are the left and right signal vectors, and the noise matrix ==== contains independent and identically distributed (i.i.d.) entries with mean zero and variance one. The noise matrix is normalized so that rows are asymptotically unit norm.",Spiked singular values and vectors under extreme aspect ratios,https://www.sciencedirect.com/science/article/pii/S0047259X23000337,3 April 2023,2023,Research Article,10.0
Saâdaoui Foued,"Department of Statistics, Faculty of Sciences, King Abdulaziz University, P.O BOX 80203, Jeddah 21589, Saudi Arabia,Laboratoire d’Algèbre, Théorie de Nombres et Analyse Non-linéaire, Faculté des Sciences, Monastir 5019, Tunisia,University of Sousse, Institut des Hautes Etudes Commerciales (IHEC), Sousse 4054, Tunisia","Received 30 August 2021, Revised 27 March 2023, Accepted 29 March 2023, Available online 31 March 2023, Version of Record 3 April 2023.",https://doi.org/10.1016/j.jmva.2023.105188,Cited by (0)," experiments at the end of the paper to demonstrate the applicability and interest of these convergence acceleration schemes, whether applied to the EM algorithm or one of its variants.","The EM algorithm [7] is one of the most popular and useful algorithms for parameter estimation in statistics. This numerical method is used for finding maximum likelihood estimates (MLE) in incomplete data problems. The EM algorithm is commonly formulated as a nonlinear multivariate problem which is iteratively solved by looking for a local optimum on the likelihood function. Considering it as a fixed point problem, the objective is to find some solution ==== (the parameters set) such that ====where ==== is a mapping from ==== to ====, assumed nonlinear. It is also supposed that ==== is monotonously decreasing and satisfies a Lipschitz condition, i.e., ====
 ====where ====. The EM is generally solved as a Picard method, i.e., going from a starting point ====, iterations are defined as: ====The sequence ==== converges to a fixed point ==== of ==== under suitable assumptions, but the convergence can be seriously slow, or even worse can diverge.====Vector extrapolation methods were already successfully applied to improve the EM convergence [5], [8], [12], [23], [25], [33]. These procedures allow us arriving to a compromise between speed and implementation easiness, in the sense that they are based upon the strong relationship that connects sequence transformations and fixed point methods. Extrapolated versions of the EM algorithm do not require the computation of auxiliary quantities such as incomplete or complete data log-likelihood functions or their respective gradients since they necessitate only EM updating solely. They require negligible additional effort to that of the standard EM algorithm, and they converge linearly, just like EM, but at a faster rate, where the gains can be substantial, especially in problems where the EM is very slow due to a large fraction of missing information. In the literature, several other approaches have been applied for the acceleration of the EM algorithm, such as [1], [10], [14], [15], [17], [19], but in fact a large part was criticized for its complexity, while another was not generalized enough to be applied to various probability densities. Let us also mention here that another advantage of the extrapolation methods is that they are also applicable to these improved versions of the EM algorithm [32].====Recently, quadratically extrapolated iterative schemes have appeared to be handy extensions that can complement conventional methods and provide improved strategies for solving both linear and nonlinear systems [9], [24], [26], [27], [29], [31]. As their classical primitive versions, these recent methods are also completely general in the sense that they can accelerate any linearly convergent fixed point iteration and consequently any variant of the EM algorithm (e.g., expectation conditional maximization (ECM), expectation–conditional maximization either (ECME) [16], parameter expansion (PX-EM) [17], stochastic EM (SEM) [19], etc.), which represent the remarkable contribution in comparison with existing techniques. Despite their effectiveness, most of the extrapolation methods used in practice belong to the family of polynomial methods, which are known to suffer from numerical instabilities such as stagnation and break-down. To overcome these issues, this paper proposes a new randomly relaxed scheme that is compared to a set of state-of-the-art methods. The principle of the strategy is to introduce a random perturbation into the entries of the difference vectors of the step-lengths of the schemes, while maintaining convergence speeding-up conditions. This procedure enables the proposed scheme to overcome stagnation points of the algorithm in a dynamic way, somewhat similar to stochastic optimization algorithms. Furthermore, the proposed scheme can be used in a squared mode, similar to how polynomial methods are used in SQUAREM, providing an alternative family of iterative quadratic methods. In summary, the proposed randomly relaxed scheme offers a promising alternative to the traditional polynomial methods, addressing some of the numerical instabilities that can arise in practice.====This paper is organized as follows. Section 2 provides a review of maximum likelihood estimation via the EM algorithm and the associated convergence process. Section 3 introduces the proposed extrapolation principle and demonstrates how it can be used to solve the EM fixed point problem. Section 4 presents the randomized extrapolation and its variants, along with their convergence properties. Finally, in the penultimate section, we analyze the proposed procedures and their limitations using several numerical illustrations. By following this organization, the paper provides a clear and comprehensive overview of the proposed extrapolation methods and their potential applications to accelerate the convergence of the EM algorithm and related techniques.",Randomized extrapolation for accelerating EM-type fixed-point algorithms,https://www.sciencedirect.com/science/article/pii/S0047259X23000349,31 March 2023,2023,Research Article,11.0
"Romanov Elad,Kur Gil,Nadler Boaz","Department of Statistics, Stanford University, United States of America,Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, United States of America,Faculty of Mathematics and Computer Science, Weizmann Institute of Science, Israel","Received 21 June 2022, Revised 23 March 2023, Accepted 24 March 2023, Available online 28 March 2023, Version of Record 10 April 2023.",https://doi.org/10.1016/j.jmva.2023.105184,Cited by (1),". In this work, we study the non-asymptotic behavior of these estimators, for data sampled from a distribution that satisfies one of the following properties: (1) independent sub-Gaussian entries, up to a ====; (2) log-concave distributions; (3) distributions satisfying a convex concentration property. Our main contribution is the derivation of tight non-asymptotic concentration bounds of these M-estimators around a suitably scaled version of the data ====. Prior to our work, non-asymptotic bounds were derived only for Elliptical and Gaussian distributions. Our proof uses a variety of tools from non asymptotic random matrix theory and high dimensional geometry. Finally, we illustrate the utility of our results on two examples of practical interest: sparse covariance and sparse precision matrix estimation.","Let ==== be ==== i.i.d. samples from a ====-dimensional random variable ====. The ==== covariance matrix ==== of ==== is a central quantity of interest in multiple applications [5], [57]. In the classical regime with ====, if the random variable ==== is not heavily tailed and there are no outliers, the empirical covariance matrix yields a relatively accurate estimator for ====.====To deal with heavy tails and potential outliers, several robust estimators were proposed and studied theoretically. Two popular procedures, applicable when ====, include Maronna’s and Tyler’s M-estimators [50], [72]. Regularized variants, applicable also when ====, were also proposed and studied [1], [22], [59], [61]. These estimators have found use in multiple applications, ranging from signal processing and radar detection to finance, see for example [24], [59], [61]. We remark that in addition to the above, many other robust covariance estimators have been proposed and analyzed, see for example  [19], [21], [28], [30], [37], [39], [51], [54], [55], [76] and references therein.====In this work we study the properties of Tyler’s and Maronna’s M-estimators under several families of multivariate distributions. Our analysis is non-asymptotic and generalizes previous results, which were either asymptotic or limited to elliptical distributions. Before presenting our results, we first briefly describe these estimators and related prior work. For simplicity, we describe the estimators assuming ==== has zero mean, and discuss how to relax this assumption later on.",Tyler’s and Maronna’s M-estimators: Non-asymptotic concentration results,https://www.sciencedirect.com/science/article/pii/S0047259X23000301,28 March 2023,2023,Research Article,12.0
Li Jun,"Department of Mathematical Sciences, Kent State University, Kent, OH 44242, USA","Received 22 April 2022, Revised 22 March 2023, Accepted 24 March 2023, Available online 28 March 2023, Version of Record 1 April 2023.",https://doi.org/10.1016/j.jmva.2023.105183,Cited by (0),"When sample sizes are small, it becomes challenging for an asymptotic test requiring diverging sample sizes to maintain an accurate Type I error rate. In this paper, we consider one-sample, two-sample and ANOVA tests for mean vectors when data are high-dimensional but sample sizes are very small. We establish asymptotic ====-distributions of the proposed ","Testing for population means is a classical problem in statistics, which has a wide range of applications in clinical trails, case-control and financial studies. In the univariate case, the Student’s ====-test can be applied to test whether a population mean equals a claimed value when the sample mean follows a normal distribution, sample variance follows a ==== distribution and the sample mean and sample variance are independent. In the traditional multivariate setting, Hotelling’s ==== test [9] can be applied to test a population mean vector when dimension ==== is fixed and dimension ==== and sample size ==== satisfy the relation ====. The Student’s ====-test and Hotelling’s ==== test are exact tests for finite sample sizes if data are normally distributed. If data are non-normally distributed, they are asymptotic tests as relatively large sample sizes are required to approximate sample means by a normal distribution.====With the development of high-throughput technologies, high-dimensional data characterized by the “large ==== and small ====” situation have been widely observed in functional magnetic resonance imaging (fMRI), microarray, next-generation sequencing (RNA-Seq), and genome-wide association (GWA) studies. When ====, the Hotelling’s ==== test becomes infeasible due to singularity of the sample covariance matrix. Even when ==== is close to ====, the Hotelling’s ==== test loses its power as revealed by [2]. Many approaches have been proposed to modify the Hotelling’s ==== test for high-dimensional data. Some were constructed to discard or stabilize the inverse of sample covariance matrix. Examples include [2], [5], [6], [7], [11], [16], [20]. Some were proposed to reduce the noise contributed by non-signal bearing components for sparse signal detection. Examples include the maximum type test proposed in [3] and the thresholding tests in [4], [22]. Some were developed to utilize advantages of both maximum type and sum-of-squares type tests to achieve better power against both sparse and dense alternatives. Examples include [8], [21]. Others were proposed to project the classical Hotelling’s ==== statistic to a low-dimensional space. Examples include [13], [17], [19]. Except the aforementioned methods, there are many other contributions on testing high-dimensional means. We refer the readers to [10] for a recent review.====Most proposed tests for high-dimensional means are asymptotic procedures, in the sense that they require both dimensionality and sample sizes to diverge to infinity even though dimensionality can be much larger than sample sizes. In many biological and financial studies, high dimensional data with very small sample sizes often occur due to ethical and cost reasons. In hypothesis testing, a primary requirement for a proposed test is to maintain its accurate type I error rate. As demonstrated by the simulation studies in Section 4, when sample sizes are very small and the null hypothesis is true, tests established by requiring diverging sample sizes tend to reject the null hypothesis with a probability higher or lower than a preselected nominal significance level. They thus cannot control type I error rate accurately.====To propose a robust test which maintains type I error rate accurately especially for small sample sizes, we establish the asymptotic normality of a one-sample ====-statistic standardized by its standard deviation only requiring data dimensionality to diverge to infinity but sample sizes to be fixed. In practice the standard deviation of the ====-statistic is unknown and cannot be consistently estimated when sample size is small. By analogy with the univariate Student’s ====-statistic, we propose an estimator for the variance of the ====-statistic, which is not needed to be consistent but shown to be asymptotically ==== distributed and independent to the ====-statistic under the null hypothesis. The result enables us to establish the asymptotic ====-distribution of the ====-statistic standardized by the sample standard deviation. The test can be applied to high-dimensional data with any finite sample size no less than ====, and maintains accurate Type I error rate. Moreover, it is nonparametric and can be applied to normally distributed or heavy-tailed data. We further extend the test to the two-sample and ANOVA testing problems. It is worth mentioning that the current work is not the only one to propose a finite sample ====-test for high-dimensional testing problems. In [18], the authors developed a modified distance correlation statistic for the problem of testing the independence of high-dimensional random vectors. As the dimension diverges, the test statistic was shown to converge to a ====-distribution for any sample size greater than ==== and an approximate standard normal when the sample size is greater than ====.====The rest of the paper is organized as follows. Section 2 introduces the one-sample ====-statistic and establishes its asymptotic ====-distribution. Extension to the two-sample and ANOVA problems is provided in Section 3. Simulation and case studies are presented in Sections 4 Simulation studies, 5 Application to fMRI dataset. Section 6 concludes the paper with discussion. Technical proofs of theorems are relegated to the Appendix.",Finite sample t-tests for high-dimensional means,https://www.sciencedirect.com/science/article/pii/S0047259X23000295,28 March 2023,2023,Research Article,13.0
"Bagkavos Dimitrios,Patil Prakash N.,Wood Andrew T.A.","Department of Mathematics, University of Ioannina, 45110, Greece,Department of Mathematics and Statistics, Mississippi State University, USA,Research School of Finance, Actuarial Studies and Statistics, Australian National University, Canberra, ACT 2601, Australia","Received 11 August 2022, Revised 19 March 2023, Accepted 19 March 2023, Available online 27 March 2023, Version of Record 6 April 2023.",https://doi.org/10.1016/j.jmva.2023.105182,Cited by (0)," size function approximation yields cut-off points suitable for finite sample implementations of the test. An extensive simulation study under Pitman and Kullback–Leibler alternatives compares the new test to well-established tests in the literature and demonstrates the strong and competitive performance of the former in the majority of the examples considered. Finally, the practical usefulness of the new test is demonstrated in the analysis of a real dataset involving stock market returns.","In this article we propose a novel goodness-of-fit test of a continuous multivariate parametric model. The main idea underlying the construction of the relevant test statistic is to aggregate local discrepancies between the fitted parametric density and a nonparametric empirical density estimator. The test statistic is a quadratic form in these local discrepancies with weights determined by a kernel function; see the discussion in Section 2.2 leading up to formula (7). Under the null hypothesis, that the unknown density belongs to a specified parametric model, a suitably scaled version of the test statistic is asymptotically normal; but under any fixed alternative, that the density does not belong to the parametric model, the scaled version of the test statistic goes to ====; see Theorem 1. The test has excellent power properties both in theory (as seen in the study of various types of local alternatives; see Corollary 1) and in extensive numerical studies (see Appendix A in supplementary material). Other developments include the extension to ====-sample problems (Corollary 2) and a bootstrap version of the test (Corollary 3), in which resampling is used to derive an empirical cut-off.====A further contribution is a new type of bandwidth selector. Using results of Hall and Heyde [13] and Hall [12] we have derived a Berry–Esseen type bound on the distribution function of leading term of the test statistic under the null hypothesis. This motivates a bandwidth selector which, under the null hypothesis, minimizes a measure of the rate of convergence of the distribution of the test statistic to normality.====Numerical examples under Pitman and Kullback–Leibler alternatives indicate that, in almost all cases studied, the proposed test is more powerful than the Kolmogorov–Smirnov, Cramer–von Mises and a variety of modern density goodness-of-fit tests based on the likelihood ratio principle and the phi-divergence metric. Simulations in the two-sample setting indicate that the proposed test is competitive in comparison with the two-sample version of the Cramer–von Mises test and more powerful than a likelihood ratio based test.====The test statistic proposed here may be viewed as a discrete approximation of the ====-based bias-corrected test statistics explored in Fan [4], [5], [6], Gouriéroux and Tenreiro [11], Li and Racine [17]. However, in contrast to the ==== approach, the bandwidth parameter here is only used to determine the spread of the local Integrated Square Error (ISE) approximation and does not affect the size of the local neighborhood in which the density is estimated. As a result, the approach suggested here does not impose a trade-off between optimal probability density function (pdf) estimation and estimation of the distance from the null. Consequently its power improvement does not conflict with the test statistic’s rate of convergence to normality, in contrast to, e.g., the test of Bickel and Rosenblatt [2]. Also, similarly to the bias corrected tests of Fan [4] and Li and Racine [17], the proposed test is more powerful than the Kolmogorov–Smirnov test for the sharp peak alternatives introduced in Rosenblatt [19]; see also Ghosh and Huang [9] and Fan [4].====For a thorough overview of goodness-of-fit tests in the closely related nonparametric regression setting see González-Manteiga and Crujeiras [10]. Related work also includes Chen et al. [3], Hallin et al. [14], Khmaladze [16], Jiménez-Gamero et al. [15] and the interesting and technically impressive paper of Gao and Gijbels [7] which uses state of the art Edgeworth expansions to obtain refined approximations of the size and power of their testing procedure and select an optimal bandwidth which maximizes power subject to bounds on the size being satisfied. However, it should be noted that – as also emphasized in Gao and Gijbels [7], p. 1585 – the focus in their paper is on local alternatives and in fact their method breaks down in the case of a fixed alternative. To see why this is so, observe that for a fixed alternative, ==== defined in their formula (22) remains bounded away from ==== as ==== and hence the bandwidth ==== in their formula (34) goes to ==== at a rate that is too fast, e.g., when ==== in their formula (22), ==== is proportional to ====, and consequently the test in the fixed alternative case typically fails to give a sensible result because ==== in their formula (11) goes to ==== in probability under very mild conditions, with similar problems occurring in the bootstrap version of their test. Our goal in this paper is different: we propose a testing procedure that is consistent against all fixed alternatives as well as for a wide class of local alternatives. Additional related work includes the interesting articles of Aït-Sahalia [1] and Gao and King [8] who consider testing parametric stochastic differential equation models driven by Brownian motion against nonparametric alternatives. In these two papers the parameter vector in the null model is estimated by minimizing an ==== distance from the marginal parametric density to a nonparametric estimate of the density. The basis for the nonparametric estimation of the marginal density of the process is the assumption of stationarity. In the approach proposed in this paper, which focuses on the case of independent and identically distributed observations, it is assumed that an optimal or near-optimal method (e.g., maximum likelihood or a method closely related) is used for estimating the parameter vector ==== under the null hypothesis. In particular, our estimate of ==== does not depend on a nonparametric estimator of the density.====Precise formulation and justification of the proposed test statistic together with analytic quantification of its theoretical properties is provided in Section 2. In particular, the proposed test statistic is introduced in Section 2.2; Section 2.3 contains the assumptions upon which this research is based; Section 2.4 contains the theoretical results of the research, including the extension of the test in the ====-sample setting; the operational characteristics of the test, including bandwidth selection and cut-off point estimation are discussed in Section 2.5; and a bootstrap approach for approximating the cut-off point of the test in provided in Section 2.6. Detailed proofs of all results established in this article are contained in Section 3 while auxiliary technical results are provided in Section 4. Numerical examples which demonstrate the excellent performance of the proposed methodology in relation to existing methods, and a data analysis which illustrates the benefits arising from using the proposed test in real world problems are contained in the Supplementary Material in Appendix A.====Finally, a few words on notation: for sequences ==== and ==== of positive numbers, ==== means ==== and ====. For sequences of real numbers ==== and ==== we write ==== if ==== has a limit and this limit is ====. For sequences of positive random variables ==== and ====, ==== means ==== and ====.",Nonparametric goodness-of-fit testing for a continuous multivariate parametric model,https://www.sciencedirect.com/science/article/pii/S0047259X23000283,27 March 2023,2023,Research Article,14.0
"Green Brittany,Lian Heng,Yu Yan,Zu Tianhai","Department of Information Systems, Analytics, and Operations, University of Louisville, Louisville, KY, USA,Department of Mathematics, City University of Hong Kong, Hong Kong,Department of Operations, Business Analytics, & Information Systems, University of Cincinnati, Cincinnati, OH, USA,Department of Management Science and Statistics, University of Texas at San Antonio, San Antonio, TX, USA","Received 8 January 2022, Revised 3 March 2023, Accepted 5 March 2023, Available online 23 March 2023, Version of Record 10 April 2023.",https://doi.org/10.1016/j.jmva.2023.105175,Cited by (0),"In many biomedical and health studies, ","Numerous large scale public health research studies are longitudinal, where participants have repeated measurements taken over time. To analyze such kind of multivariate data that exhibit clear correlation among within-subject responses, we need to incorporate the correlation structure rather than assuming a simple multivariate regression model under independence.====One main goal of analyzing these types of longitudinal studies is to identify the genetic and phenotype factors related to a disease to provide insight into more effective treatment and disease prevention strategies. For example, researchers have discovered risk factors linked to various disease mechanisms (e.g., [23]) using the Framingham data, an ongoing large scale multi-generational health study [6]. Within these types of large-scale longitudinal studies, while the true correlation among participants is usually difficult to uncover, incorporating within participant dependence can lead to more efficient estimation. Moreover, the disease of interest measured over time is sometimes a correlated discrete measure, such as diabetes status. This non-normal correlated response creates a challenge in specifying the joint likelihood for longitudinal data.====In addition to the longitudinal nature of large scale public health studies, more recently, the genotype of participants is also collected. These genetic factors are very high dimensional as demonstrated in the Framingham data which collects a complex array of genetic data, including tens of thousands of single nucleotide polymorphisms (SNPs) from each participant. Notably, previous research has linked some genetic factors to disease. For instance, genomic studies have helped identify mechanisms of hypertension and diabetes [39]. In these very high dimensional settings, variable selection is imperative to identify the important risk factors, since usually only a few covariates relate to the response and including non-important variables lessens estimation efficiency and impedes inference. In addition, not only do these types of studies have high dimensional genetic data, recent research has found that genetic data interacts with phenotype variables. For example, [31] show that the genetic effects of hypertension are altered under phenotype factors such as BMI.====A motivating example of this paper to exemplify this complexity focuses on identifying factors related to diabetes status, a correlated discrete response, from the offspring cohort of the longitudinal Framingham data. Diabetes affects millions of people worldwide, and identifying genetic and phenotype factors that relate to diabetes can help inform preventative measures and further uncover biological measures of diabetes [10]. In particular, one research question of interest is to determine which SNPs in high dimensions and phenotype factors relate to diabetes status among participants over time. While a traditional approach to this problem employs a linear model, due to the complexity of gene expression and interactions with phenotype factors, inflexible models with parametric assumptions may not account for the potential nonlinearity and synergy between genetic and phenotype data in high dimensional longitudinal data. To demonstrate, Fig. 1 shows a clear nonlinear relationship under our proposed model between diabetes and the combination of SNPs and phenotype factors in the Framingham data.====To balance interpretability and flexibility for accurate estimation for this high dimensional set of risk factors, we consider a flexible semiparametric approach for repeated observations. Specifically, we adopt generalized partially linear single-index models (GPLSIM) [4] for longitudinal data in ultra-high dimensions. Generalized partially linear single-index models achieve dimension reduction by reducing the high-dimensional predictors to a univariate index within a flexible function. Moreover, single-index models can capture some interactions among covariates as opposed to additive models. This is advantageous since SNPs and phenotypic risk factors do not relate to disease in isolation: the compound impact of the genotype–phenotype interaction has been shown to outperform the impact of using the conventional risk factors in isolation (e.g., [10], [31]).====Moreover, diabetes status, the outcome of interest measured during multiple waves of the Framingham data, is a correlated discrete response. This poses a challenge as the full joint likelihood can be intractable for correlated discrete data. To tackle this challenge, we employ the penalized quadratic inference function (QIF) to account for within-subject correlation, perform model selection, and seek efficient estimation for diverging and potentially ultra-high dimensional longitudinal data. Previous research employing penalized generalized estimating equations (GEE) for diverging and ultra-high dimensional longitudinal data for linear and semiparametric models includes [38] and [12]. However, generalized estimating equations are known to be less efficient and overfit the model compared to the quadratic inference function approach when the working correlation matrix is misspecified (e.g., [5], [27]). In addition, the quadratic inference function has applicability in various model setups and shows promising results (e.g., [26], [36], [37]).====As a result of the multiple benefits of the quadratic inference function, a handful of works have proposed research employing partially linear single-index models using the quadratic inference function for longitudinal data (e.g., [2] and [18]). However, these works assume the dimension of both the single-index and partially linear covariates is fixed. Also in fixed finite dimensions, [21] and [19] incorporated variable selection for the partially linear single-index model employing the quadratic inference function for continuous longitudinal responses with the identity link function. In these studies, the real-data applications considered a fixed low-to-moderate dimensional setup: the application in [21] included a total of 11 covariates, and the application in [19] included 13 covariates. In contrast, the Framingham data analyzed using our approach have 878 participants but involves over 500,000 SNP covariates and phenotype variables even within the nonparametric portion.====As opposed to the previous works, our approach allows the number of covariates in both the nonparametric and linear components of the generalized partially linear single-index model to diverge and even in ultra-high dimensions. We also allow the number of important covariates to diverge. This is especially pertinent for our motivating example, since there are more than 500,000 genetic SNP variables. In particular, allowing flexible modeling and determining the sparse set of important covariates can lead to more accurate estimation. However, as a result of incorporating diverging covariates, we encounter added challenges in both computation and theory when we allow ultra-high dimensional data within the nonlinear, unknown, flexible function with potentially diverging support of the single index. We establish asymptotic theory for ultra-dimensional covariates for model selection and estimation including the oracle property, which is much more challenging to establish than fixed-dimensional theory. In addition, while previous approaches exist for estimating the coefficients of the generalized partially linear single-index model, efficient estimation becomes even more difficult when introducing ultra-high dimensional correlated data. This is because of the potentially high dimensional covariates in a nonlinear unknown function estimated nonparametrically together with the non-convex smoothly clipped absolute deviation (SCAD) penalty function as in [7], all within a longitudinal framework. Therefore, to select the sparse set of important covariates and estimate the corresponding coefficients, we provide a computationally efficient iterative algorithm. This approach implements strategic approximations to reduce the computational burden and increase the effectiveness of the algorithm, even for discrete correlated responses.",Semiparametric penalized quadratic inference functions for longitudinal data in ultra-high dimensions,https://www.sciencedirect.com/science/article/pii/S0047259X23000210,23 March 2023,2023,Research Article,15.0
"Zhang Rongmao,Chan Ngai Hang,Chi Changxiong","Zhejiang University City College , Hangzhou, China,Zhejiang University, Hangzhou, China,City University of Hong Kong, Hong Kong","Received 4 March 2022, Revised 15 March 2023, Accepted 15 March 2023, Available online 18 March 2023, Version of Record 5 April 2023.",https://doi.org/10.1016/j.jmva.2023.105180,Cited by (0)," hypothesized model to test for the goodness-of-fit of spatial trends. By virtue of the ====-dependence approximation of a stationary random field, it is shown that under certain ","Trend-surface models, i.e., a model that can be decomposed into one large scale variation (trend or mean structure) and one small scale variation (mean-zero error process), have been extensively studied in geostatistics, see Cressie [7]. Examples of trend-surface models appear in image processing, pattern recognition, climate change and disease outbreak detection, among others fields, see for example Jun and Stein [26] for air pollution, Sherwood [36] for climate change, Neill and Lingwall [33] and Neill [32] for the spatial pattern and sudden events detection, see also Chapter 4 of Cressie [7]. Misspecified models will often result in inefficient inference and inaccurate predictions. It is therefore important to test for the specific form of spatial trend functions.====An important example along this direction is to test for the presence of a signal in functional magnetic resonance imaging (fMRI, see Fox and Raichle [14]), which can be summoned via the following representation: ====where ==== is a partition of ==== and ====. The main concern therein is to test whether there is any signal at a given block ====. That is, to test ==== versus ==== for some ====, see for example, Chan, Zhang and Yau [4].====As pointed out by Hansen [20], Chen and Hong [5] and among others, in many spatial phenomena, a constant or an abrupt change in the trend as prescribed in model (1) might be inadequate. It is more reasonable to allow the structure to change smoothly over the entire domain, i.e., consider the situation ====, where ====. When ====, then ==== reduces to the constant mean case in model (1). Note that this setting is consistent with the change point analysis in time series. We have to similarly rescale the changing domain of the trend within the region ====, so that when more data is available, the boundary of changing region also grows with ====. Restricting to the domain ==== also reflects the smooth transition of ====, see Chen and Hong [5] for more discussions on this point.====Motivated by these considerations, consider the following trend-surface model: ====where ==== is an unknown measurable function and the noise ==== is a mean-zero spatial process with expression ==== for some measurable function ==== and i.i.d spatial random variables ====. A typical example of ==== is the spatial linear process ==== for certain real numbers ====, see Lahiri and Robinson [28].====One of the main objectives of this paper is to test whether the spatial trend ==== follows a specific parametric form ====where ==== is a known function and ==== is an unknown parameter vector. For example, an extensively used model in geostatistics is ====, where ====s are known functions, see Section 4 Chapter 3 of Cressie [7].====When ====, problem (3) has attracted considerable attention in the literature. For the independent noise case, Härdle and Mammen [21] proposed a ====-distance test; Dette [8] constructed a test based on the deviation of variance estimators; Aerts, Claeskens, and Hart [1] developed a test based on orthogonal series estimators; Fan and Huang [11] considered an adaptive Neyman test; Fan, Zhang and Zhang [13] proposed a generalized likelihood ratio test, and Van Keilegom, Manteiga and Sellero [37] considered a test based on the difference between the empirical distributions of the residuals. Under the dependence setting, one can refer to Wu and Zhao [39], Zhang and Wu [40], Horváth and Rice [23], Chen and Wu [6], among others. However, little is known for the case ====, especially when the data are spatially correlated.====To address this problem, we propose a kernel smoothing device to estimate the trend function and adopt a global integrated squared error (GISE) statistics between the nonparametric smoothing surface and the hypothesized function to test for goodness-of-fit. GISE was studied in Bickel and Rosenblatt [2] and Hall [16] for the independent situation and by Biedermann and Dette [3], Pawlak and Stadtmüller [34] and Zhang and Wu [40] for dependent data when ====. Under certain regularity conditions, an asymptotic theory for GISE under the spatial dependent setting is established in this paper. This result will shed light on spatial model checking and opens a new avenue to model spatial trends.====Throughout the paper, we assume ==== for simplicity and set ==== as the sample size, ==== be the vector ====, ====, and ==== denotes the collection of functions having up to ====th-order derivatives on ====. The paper is organized as follows. In Section 2, we introduce the nonparametric estimation of the trend ==== and the integrated squared error measure. Section 3 considers two practical issues related to nonparametric estimation: bandwidth selection and variance bootstrapping. Simulation studies and real data application are given in Section 4. Section 5 concludes and Section 6 presents the proofs.",Nonparametric testing for the specification of spatial trend functions,https://www.sciencedirect.com/science/article/pii/S0047259X2300026X,18 March 2023,2023,Research Article,16.0
"Soloveychik Ilya,Tarokh Vahid","Department of Statistics, The Hebrew University of Jerusalem, Israel,Department of ECE, Duke University, United States of America","Received 5 September 2022, Revised 12 March 2023, Accepted 13 March 2023, Available online 16 March 2023, Version of Record 25 March 2023.",https://doi.org/10.1016/j.jmva.2023.105178,Cited by (0),We consider the problem of model selection in Gaussian Markov fields in the sample deficient scenario. The benchmark information-theoretic results in the case of ,None,Region selection in Markov random fields: Gaussian case,https://www.sciencedirect.com/science/article/pii/S0047259X23000246,16 March 2023,2023,Research Article,17.0
"Wei Zheng,Wang Li,Liao Shu-Min,Kim Daeyoung","Department of Mathematics and Statistics, Texas A&M University-Corpus Christi, Corpus Christi, TX 78412, USA,Applied Scientist II, Microsoft, 14820 36th St, Redmond, WA 98052, USA,Department of Mathematics and Statistics, Amherst College, Amherst, MA 01002, USA,Department of Mathematics and Statistics, University of Massachusetts-Amherst, Amherst, MA 01003-9305, USA","Received 15 March 2022, Revised 14 March 2023, Accepted 14 March 2023, Available online 16 March 2023, Version of Record 24 March 2023.",https://doi.org/10.1016/j.jmva.2023.105179,Cited by (0),"In this paper, we propose a new data-driven method to explore complex regression ","Multivariate categorical data with ordinal response variables is very common in various fields of science such as social, behavioral, health, and biomedical sciences and it is typically summarized into a multi-dimensional contingency table. A major goal of statistical analysis of such multivariate data is to uncover, model and understand regression dependence between an ordinal response variable and a set of independent categorical variables. To this end, two types of methods have been developed [2], [21], [30], [38], [50], [53]. The first one is a model-based method that requires an explicit specification of the underlying regression dependence structure and is useful for the formal modeling such as explanatory modeling or predictive modeling. The second one is a non-model-based method for descriptive/exploratory analysis which does not need parametric specification of the dependence structure in the data.====The model-based method includes (but not limited to) cumulative link models [4], [18], [41], continuation ratio logit model [36], adjacent-categories logit model [26], [48], stereotype model [5], latent variable models, and association models [25], [27]. The non-model-based method includes Cochran–Mantel–Haenszel methods [37], [40], ordinal odds ratios [15], [25], [26], [41], rank-based methods including Kendall’s tau [32], Goodman and Kruskal’s gamma [28], Spearman’s rank-based correlation [2], and their extensions (for example, Kendall’s conditional tau-b [1], [33], Goodman and Kruskal’s conditional gamma [10], conditional Spearman’s rank correlation using probability-scale residuals [39]), and copula-based measures such as checkerboard copula-based concordance/rank correlation measures [12], [43] and the checkerboard copula regression association measure [54].====An essential step for proper analysis of multivariate categorical data with an ordinal response variable is to explore various aspects of regression dependence structures between an ordinal response variable and a set of categorical independent variables in a model-free manner. The information obtained from this step would be informative in gaining insight into the complex regression dependences for descriptive modeling, formulating patterns of regression associations or identifying potential independent variables that may provide relevant and non-redundant information on an ordinal response variable [14], [47], [52].====However, it is not clear how to use the non-model-based approaches listed above in a holistic and understandable way. This is because each of them was developed to measure only one of regression associations: either global association between an ordinal response variable and all available independent variables, or bivariate marginal associations, or bivariate conditional associations conditional on the remaining independent variables. It would be desirable to develop a systematic method to accomplish two goals: (1) measuring all aforementioned associations (i.e., global/marginal/conditional associations) in an integrated manner, and (2) quantifying the contributions of any subset of independent variables of interest (via marginal/conditional associations) to the measured global regression association in an interpretable manner.====To achieve these goals, we propose a new data-driven and model-free approach to the analysis of a multidimensional contingency table with an ordinal response variable and categorical (ordinal or nominal) independent variables. The main idea of the proposed approach is based on the decomposition of the overall Checkerboard Copula Regression Association Measure (CCRAM) [54] in a holistic and interpretable fashion. The overall CCRAM was developed to quantify the global regression association for the data using the checkerboard copula regression of an ordinal response variable on all available categorical independent variables. However, as claimed by Kendall and Stuart [34], the regression dependence resulting from the joint behavior of the variables is too complex to be understood in a single coefficient, and additional steps are necessary to distill meaningful information from such a global dependence. Thus, we first define the marginal and conditional CCRAMs between an ordinal response variable and a subset of independent categorical variables of interest, one unconditional and the other conditional on other independent variables. More specifically, the conditional CCRAM is designed to quantify the conditional contribution of additionally considered categorical independent variables given other independent variables are taken into account, unlike the marginal CCRAM. We then propose a sequential decomposition method for the overall CCRAM and show that the overall CCRAM can be partitioned into a sum of a marginal CCRAM and a sequence of conditional CCRAMs in a hierarchical manner which considers the order of the categorical independent variables of interest. An advantage of the proposed decomposition is that it enables us to quantify the marginal and conditional contributions of any subset of categorical independent variables of interest to the overall regression association (measured by the overall CCRAM).====The rest of the article is organized as follows. Section 2 gives a brief review on the overall CCRAM [54] for the global regression association between an ordinal response variable and all categorical (nominal/ordinal) independent variables in a multi-dimensional contingency table. In Section 3 we define the marginal and conditional CCRAMs between an ordinal response variable and a subset of independent categorical variables of interest, one unconditional and the other conditional on other independent variables, and investigate their theoretical properties. Then we propose a sequential decomposition approach of the overall CCRAM in a hierarchical manner, taking into account the order of the categorical independent variables of interest, and investigate its properties under different types of independence for a multi-way contingency table. Note that the proposed marginal/conditional CCRAMs and decomposition method are also applicable for the contingency tables with nominal independent variables. Section 4 discusses the estimation of the marginal and conditional CCRAMs and the decomposition method proposed in Section 3. Section 5 conducts data analysis to illustrate the practical utility of the proposed decomposition method with two real data examples, one from a randomized controlled trial and the other from a longitudinal epidemiological study. Section 6 closes the paper with a discussion on simulation results and future work.",On the exploration of regression dependence structures in multidimensional contingency tables with ordinal response variables,https://www.sciencedirect.com/science/article/pii/S0047259X23000258,16 March 2023,2023,Research Article,18.0
"Mondal Anjana,Sattler Paavo,Kumar Somesh","Department of Mathematics, Indian Institute of Technology Kharagpur, Kharagpur 721302, India,Department of Statistics, TU Dortmund University, 44227 Dortmund, Germany","Received 10 August 2022, Revised 11 March 2023, Accepted 11 March 2023, Available online 16 March 2023, Version of Record 20 March 2023.",https://doi.org/10.1016/j.jmva.2023.105177,Cited by (0),"In this paper, we consider tests for homogeneity of row effects against ordered alternatives in a randomized block design without interaction effects. Error variances in all cells are assumed to be heterogeneous and data are unbalanced. The likelihood ratio test (LRT) and two heuristic tests named Min-T and Max-T are proposed. Numerical procedures to find solutions of likelihood equations under null hypothesis space and full parameter space are developed. The existence and uniqueness of the solutions and the convergence of the algorithm are established. Parametric bootstrap (PB) is used to evaluate critical points and to estimate sizes and powers of these tests. Asymptotic distributions of the heuristic test statistics are derived and tests are proposed based on these also. Asymptotic accuracy of bootstrap procedures is established in all cases. Using Max-T test we have derived simultaneous confidence intervals for successive pair-wise differences of row-effects. An extensive simulation study shows that all the proposed PB tests maintain a preassigned type I error 0.05. Power comparison shows that LRT achieves more power values than all the other tests. However, Min-T and Max-T tests are easy to implement and take considerably less time as these do not need the evaluation of maximum likelihood estimators (MLEs). The robustness of all tests is also studied and it is seen that LRT is not robust for some skewed distributions. However, heuristic tests are robust in all cases. This is also one advantage of using Min-T and Max-T tests.","In a traditional two-way ANOVA with factors ==== and ====, testing equality of effects of various levels of these factors against their natural alternatives is considered under the assumption of homogeneous error variances. However, in many practical situations, this assumption fails to hold and applying usual ====-tests leads to erroneous conclusions. Zhang [36], Xu et al. [34], [35] have investigated effects on size and power values of classical ====-tests (CFT) in two-way ANOVA when error variances are not homogeneous. Their simulation studies showed that these standard tests are not robust for heterogeneous variances, see also Pauly et al. [22], Bathke et al. [6], Sattler and Pauly [25] and Noguchi et al. [21] for similar observations in related settings. Also in classical ANOVA the standard alternative hypothesis is at least one inequality in treatment effects. However, sometimes experimental set up yields that effects of treatments may follow a natural ordering. In dose response studies, usually doses of drugs are administered to experimental animals (mice, guinea-pigs, monkeys etc.) in such a way that each successive batch receives increased amount of drug content. This may lead to increase in cure rate, survival times or decrease in mortality etc. (see for example, Haque et al. [15], Greenland and Longnecker [14]). Similarly in industrial designs problems, engineering changes are made in equipment or system so that at each stage the efficiency of the set up is expected to increase (for example, strength of material, fuel efficiency, speed of vehicle etc.). Therefore, when this is prior information about an ordering among mean effects of different levels of factors, it is more useful to test the hypothesis of equality of mean effects against ordered alternatives.====Consider the general two-way ANOVA model with two factors ==== and ==== without interaction effect (also called additive model) described as ====Here ==== and ==== denote the number of levels of factor ==== and ==== respectively and ==== is number of observations in ====-th cell. Further, ==== denotes the general effect, ==== is the effect of ====-th level of factor ====, ==== is the effect of ====-th level of factor ==== and independent errors ====. It is assumed that ==== are completely unknown, that is, they may be homogeneous or heterogeneous.====In this paper, we consider the problem of testing the equality of effects of factor ==== against their ordered alternatives. Testing against ordered alternatives of the effects of factor ==== is similar to ====. Also, cell sizes are taken to be unequal in general.====The testing procedures with interaction effect are quite different. We have addressed this problem in a separate paper.====As mentioned before, the classical ====-tests are not robust when error variances are heteroscedastic. For natural alternatives in one-way and two-way ANOVA models, Bishop and Dudewicz [8] proposed one-stage and two-stage procedures. These procedures were extended by Bishop and Dudewicz [9] to a ====-way ANOVA model. Fujikoshi [13] considered a two-way ANOVA model with homogeneous variances but unequal sample sizes in each cell. He proposed tests under three types of constraints on main and interaction effects.====Ananda and Weerahandi [2] pointed out that in two-way ANOVA models, CFT is not robust for unbalanced data and heteroscedastic error variances. They proposed an extension of the CFT to a generalized ====-test (GFT) for testing homogeneity of effects of factors ==== and ==== as well as no interaction effect. A simulation study was carried out to compute size and power values of the GFT. We note that their test is quite conservative for all therein considered sample sizes. Further, Bao and Ananda [3] carried out a more elaborate simulation study for size and power values of the CFT and GFT for the hypothesis of no interaction effect. It is shown that when heteroscedasticity is present under normal populations the GFT is quite conservative, but the CFT is sometimes liberal. When size values are adjusted to the same level, the GFT has higher power than CFT. Under non-normality (gamma), both CFT and GFT are shown to be robust.====For testing hypothesis of homogeneity of main effects and no interaction effects, Wang and Akritas [30] proposed a modification of the CFT when the number of levels of row and/or column factor becomes large. In such cases CFT does not work well. They derived the asymptotic distribution of their test under null and alternative hypotheses. Simulations are used to compute size and power values of the test. Xu et al. [35] proposed PB tests for testing equal main effects and no interaction effects in a two-way ANOVA model with unequal sample sizes and error variances. Through simulations, they have shown that these tests have better size and power performance compared to GFT. Xu et al. [34] consider a two-way ANOVA model without interaction and with heteroscedasticity. For testing hypothesis of equal main factor effects they proposed a PB test and showed this to be better than GFT of Ananada and Weerahandi [2] using simulations. Zhang [36] proposed an appropriate degrees of freedom test for heteroscedastic ANOVA and showed that it performs better than CFT in terms of size and power values. In a one-way layout with unknown and unequal variances, Hasler and Hothorn [17] proposed a simultaneous test for testing multiple contrasts. To find critical values they approximated the joint distribution of test statistics using some heuristic techniques.====We note that all above cited studies are about testing of homogeneity of main effects and two-way ANOVA with or without interaction effects against natural alternatives of at least one inequality among the main effects. Testing against ordered alternatives in a one-way ANOVA model was first studied by Bartholomew [5]. When error variances are known and unequal or unknown but equal, Bartholomew [5] proposed LRT. Shorack [28] extended results of Bartholomew [5] to two-way ANOVA model without interaction effects and equal error variances. The sample sizes were also taken to be equal. For testing against simple ordered alternatives of mean responses of normally distributed populations with homogeneous but unknown variance, Williams [32] proposed a test based on the difference of MLEs of highest and lowest mean responses. When treatment replications are different, Williams [33] reformulated this test. He also gave a two-sided test statistic when the particular order is not specified. Williams tests [32], [33] were extended to a general linear model by Bretz [10]. Critical values of this test can be evaluated through numerical integration. Hasler and Hothorn [18] developed a Williams-type approach for testing for multiple contrasts in randomized dose–response studies with correlated multiple primary endpoints. They have approximated the critical points using a multivariate ====-distribution for the test statistics. There are some studies in two-way ANOVA without interaction under a non-parametric set up for testing against ordered alternatives. Distribution-free and rank based tests have been studied by Mansouri [20] and Callegari and Akritas [11]. Davidov et al. [12] developed estimation and testing procedures in this problem under a semi-parametric set up.====Existing studies on testing against ordered alternatives are with homogeneous variances and in non-parametric setups. There are no studies in a parametric setup with unknown and heterogeneous variances. For the two-way ANOVA model (1), we consider the problem of testing the equality of row effects against their ordered alternatives when errors follow normal distributions with mean zero and unequal variances. Here we have addressed this problem assuming that there are no interaction effects. The problem when interactions are present has been studied separately in another work. We consider general case of unbalanced sample sizes.====Precisely speaking we are interested in the following hypothesis testing problem: ==== with at least one strict inequality. In order to have ==== estimable, we need additional constraints. In our case, we assume the following constraints: ====For the testing problem, we develop LRT and two simultaneous tests, called Min-T and Max-T tests. The Min-T test is particularly useful when the alternative hypothesis has strict ordering. Rejection by Max-T test would imply ==== for at least one ====. This does not imply much about the alternative ====. Rejection by Min-T implies ==== for all ====. On the other hand, the Max-T test can be used to evaluate simultaneous confidence intervals for successive pair-wise differences ====. Based on the asymptotic null distributions of the Min-T and Max-T test statistics, we propose two more tests, named AMin-T and AMax-T. For finding critical values of the tests, we use the classical PB procedure. As LRT is developed by taking the complete information of parameters, LRT performs better than the other tests. However, one difficulty of using LRT is that it needs the evaluation of maximum likelihood estimators under the null and alternative hypotheses. This requires implementation of computationally intensive algorithms. Hence, along with LRT, we propose two heuristic tests which are very easy to implement. We have developed an ‘R’ package for using the tests on real data sets and shared it on open source platform ‘Github’.====This paper is organized in the following way. In Section 1, we present the LRT for the testing problem ==== against ====. We have developed an iterative procedure for finding maximum likelihood estimators (MLEs) of the parameters under null space in Section 2.1. Section 2.2 presents the procedure for finding MLEs under full parameter space. The convergence of the iterative procedure is established here. The corresponding parametric bootstrap test is developed in Section 2.3. Asymptotic accuracy for the parametric bootstrap procedure used in LRT is established in Section 2.4. In Section 3, details about the parametric bootstrap testing procedures for using Min-T and Max-T are presented. In Section 3.1, the asymptotic distribution of the vector ====, used in Min-T and Max-T tests, is derived under the null hypothesis ====. An algorithm for finding critical points of the asymptotic tests AMax-T and AMin-T is also given. Asymptotic accuracy of the bootstrap, used in Min-T, Max-T is proved in Section 3.2. By inverting Max-T test statistic, simultaneous confidence interval for successive pair-wise differences ====, ==== is derived in Section 3.3. Results of an extensive simulation study on size and power calculations for all the proposed tests are presented in Section 4. Robustness of all the test procedures is investigated in Section 5. Finally, all the proposed test procedures are illustrated with the help of a real data example in Section 6. Proofs of some theorems are presented in Appendix A.",Testing against ordered alternatives in a two-way model without interaction under heteroscedasticity,https://www.sciencedirect.com/science/article/pii/S0047259X23000234,16 March 2023,2023,Research Article,19.0
"Fraiman Ricardo,Moreno Leonardo,Ransford Thomas","Centro de Matemática, Facultad de Ciencias, Universidad de la República, Uruguay,Instituto de Estadística, Departamento de Métodos Cuantitativos, FCEA, Universidad de la República, Uruguay,Département de Mathématiques et de Statistique, Université Laval, Québec City (Québec), Canada G1V 0A6","Received 12 September 2022, Revised 8 March 2023, Accepted 8 March 2023, Available online 13 March 2023, Version of Record 15 March 2023.",https://doi.org/10.1016/j.jmva.2023.105176,Cited by (0),"According to a well-known theorem of Cramér and Wold, if ==== and ==== whose projections ==== onto each line ==== in ==== satisfy ====, then ====. Our main result is that, if ==== and ====, it suffices merely to check that ==== for a certain set of ==== lines ====. Moreover ==== and ====. We use our results to derive a statistical test for equality of elliptical distributions, and carry out a small simulation study of the test, comparing it with other tests from the literature. We also give an application to learning (binary classification), again illustrated with a small simulation.",None,A Cramér–Wold theorem for elliptical distributions,https://www.sciencedirect.com/science/article/pii/S0047259X23000222,13 March 2023,2023,Research Article,20.0
"Valeriano Katherine A.L.,Galarza Christian E.,Matos Larissa A.,Lachos Victor H.","Departamento de Estatística, Universidade Estadual de Campinas, Campinas, Brazil,Facultad de Ciencias Naturales y Matemáticas, Escuela Superior Politécnica del Litoral, ESPOL, Vía Perimetral Km. 30.5, Guayaquil, Ecuador,Department of Statistics, University of Connecticut, Storrs, CT 06269, USA","Received 7 June 2022, Revised 1 March 2023, Accepted 1 March 2023, Available online 4 March 2023, Version of Record 11 March 2023.",https://doi.org/10.1016/j.jmva.2023.105174,Cited by (0),"Skew-====, skew-====, and extended skew-","The study of models in which the variable of interest is subject to certain threshold values below or above which the measurements are not quantifiable has been a common topic of interest in the statistical literature in recent years. For example, in environmental research, the concentration levels of the dissolved trace metals in freshwater streams in Virginia are subject to multiple limits of detection values [see, for instance,  15]. In AIDS research, the quantification of viral load measurements is typically assessed according to certain upper and lower detection limits. As a result, the viral load responses are either left or right-censored depending on the diagnostic assays used [31]. This kind of data can be modeled using censored regression (CR) models, or the Tobit model, and has become quite common in the literature with a wide range of applications in biology, biometrics, genetics, medicine, finance, and marketing, among many others.====CR models usually use the normal distribution for mathematical convenience for continuous data. However, it is well known that the normal distribution (N-CR) is sensitive to outliers. Moreover, the use of N-CR may be unsuitable for a set of data containing observations with heavy tails or asymmetric behavior. It can unduly affect the fit of the CR model. This inconsistency in the N-CR model led to the development of less sensitive estimators to the assumption of normality. Several authors have studied CR models involving response variables with heavier tails than the normal distribution in recent years. For instance, [22] have studied CR models based on the univariate Student’s-==== distribution (T-CR). In a multivariate setting, [13] [16], [24] advocated the use of the multivariate Student’s-==== distribution in the context of CR models, where a simple and efficient EM-type algorithm for iteratively computing maximum likelihood (ML) estimates of the parameters was presented. They demonstrated the robustness aspects of the T-CR model against outliers through extensive simulations using the Expectation–Maximization (EM) algorithm, which is based on the first two moments of the multivariate truncated Student’s-==== distribution. More recently, [15] proposed the multivariate skew-normal (SN) distribution to analyze censored or missing data (SN-CR), and a fully likelihood-based approach is carried out, including the implementation of an EM-type algorithm for ML estimation. However, neither the T-CR model nor the SN-CR model is appropriate when data simultaneously present skewness and heavy-tailed behavior.====In this paper, we attempt to overcome these limitations in the aforementioned CR models by proposing a more adapted and robust CR model that can simultaneously deal with the issues of censored and/or missing data, skewness, and heavy-tailed and atypical data. Our contribution extends the recent works of [13], [15] since they considered only the Student’s-==== and the SN distribution, respectively, which are particular cases of the skew-==== (ST) family of distributions, including the popular normal one. We show that the E-step reduces to computing the first two moments of a truncated multivariate Student’s-====, skew-====, and extended skew-==== distributions, which are implemented in the ==== [12] and ==== [30] ==== packages. The likelihood function is easily computed as a by-product of the E-step and is used for monitoring convergence and model selection. Furthermore, we consider a general information-based method for obtaining the asymptotic covariance matrix of the ML estimates.====The paper is organized as follows. Section 2 introduces some notations and outlines the main results of the ST and truncated skew-==== (TST) distributions. In Section 3, the ST censored regression model (ST-CR) and related likelihood-based inference are presented, including the implementation of an EM-type algorithm called the Expectation/Conditional Maximization Either (ECME) algorithm [20] for obtaining ML estimates of the parameters. Section 4 presents some simulation studies to illustrate the performance of the proposed method. Section 5 discusses two real data applications in environmental and astronomical research. Finally, Section 6 concludes with some discussion and possible directions for future research.",Likelihood-based inference for the multivariate skew-,https://www.sciencedirect.com/science/article/pii/S0047259X23000209,4 March 2023,2023,Research Article,21.0
"Cook R. Dennis,Forzani Liliana,Liu Lan","School of Statistics, University of Minnesota, USA,Facultad de Ingeniería Química and CONICET, Universidad Nacional del Litoral, Argentina","Received 26 July 2022, Revised 30 January 2023, Accepted 1 February 2023, Available online 19 February 2023, Version of Record 26 April 2023.",https://doi.org/10.1016/j.jmva.2023.105163,Cited by (0), to compress the response and predictor vectors was judged to be the best for prediction and parameter estimation.,None,Partial least squares for simultaneous reduction of response and predictor vectors in regression,https://www.sciencedirect.com/science/article/pii/S0047259X2300009X,19 February 2023,2023,Research Article,22.0
Cavicchioli Maddalena,"Department of Economics “Marco Biagi”, University of Modena and Reggio E., Viale Berengario 51, 41121 Modena, Italy","Received 21 September 2022, Revised 14 February 2023, Accepted 14 February 2023, Available online 19 February 2023, Version of Record 21 February 2023.",https://doi.org/10.1016/j.jmva.2023.105164,Cited by (0),We consider multivariate Markov switching first-order ,"Economic time series prediction deals with the task of modelling the underlying data generation process using past observations and using the model to extrapolate the time series into the future. Of course, real-world systems are seldom linear, and it has been shown empirically that many financial and economic series typically exhibit changes in the dynamics of the conditional distribution.====One popular approach to modelling changes in regimes is the class of Markov switching (MS) models since the seminal paper of [20]. Results concerning stationarity, existence of moments, autocovariance structure, geometric ergodicity, statistical inference and asymptotic theory for Markov switching vector autoregressive moving-average (MS VARMA) models have been derived by several authors (see, e.g., [2], [4], [9], [15], [16], [17], [29], [38], [39], [40], [42], [43]). Estimation, consistency, testing for linearity and model selection of MS VARMA models have been discussed by [6], [7], [8], [14], [28], [44]. Spectral density matrix of such models is studied in [5]. Matrix expressions for higher order moments and asymptotic Fisher information matrix of MS VARMA models are provided in [10], [11], respectively.====The vast literature generated by the [20] paper typically assumes that the regressors are exogenous with respect to all realizations of the disturbance term. Successively, [25], [26] introduces and studies univariate MS regression models with endogenous explanatory variables. Such models (and some generalizations of them) are shown to be powerful tools for modelling various financial and economic time series. Estimation of MS regressions typically relies on the assumption that the latent state variable controlling regime change is exogenous. [27] relax this assumption, propose a univariate regression of endogenous Markov regime-switching, and show that inference via maximum likelihood estimation (MLE) is possible with relatively minor modifications to existing recursive filters. Further results on univariate multi-state endogeneous MS models with applications in macroeconomics and finance can be found in [24]. These authors provide an iterative filter that generates the model likelihood function and estimated regime probabilities. The parameterization of the model also allows for a simple test of the null hypothesis of exogenous switching. Asymptotic theory and spectral analysis for multivariate GARCH-type models in various specification (including Markov regime changes) have been developed by [12], [13], [19].====It is known that the ML estimation of a MS regression model based on the Hamilton filter [20], [21] is not valid in the presence of endogeneous explanatory variables. However, [25] describes an appropriate transformation of the model that permits to employ the Hamilton filter. Within this framework, quasi ML estimation methods for univariate endogenous MS regression models have been presented in [25], [26] based on EM algorithm.====In this paper, we consider multivariate MS first-order autoregression models with endogenous explanatory variables, and concentrate on some questions that were not treated by the cited authors. Specifically, the present paper contributes to the existing literature in threefolds.====First, we propose an algorithm of type EM to estimate the model parameters (in the case of Gaussian disturbances), which is alternative to those employed by [25], [26] in the univariate regression case. The Kim algorithm is performed by using numerical iterations that approximate the derivative by the change in the likelihood function for small changes in the parameter vector. This may not be especially efficient because such techniques typically require several numerical evaluations. Using analytical gradients, [18] showed that the number of calculations can be greatly reduced, and this in turn considerably speeds up ML algorithm with no loss of accuracy. By matrix differentiation, we derive a recursive algorithm based on simple matrix expressions of ML estimates. This allows us to solve the maximization equations without the use of numerical procedure. The obtained recursion formulas, written as concisely as possible at the vector–matrix level, improve the computational performance because they are readily programmable. So the first interesting contribution of the paper is the calculation of both expectation and maximization steps in a modified EM algorithm. The link between them is the computation of filtered and smoothed regime probabilities, obtained from the Hamilton filter (see [22], Section 22). Further, we provide a mathematical machinery based on matrix differential calculus, and computational methods, which are very different to those employed in the univariate case and are not a direct extension of the theory of univariate regression models.====Second, the asymptotic properties and consistency of the ML estimator are investigated. A further advantage of the proposed approach is that an expression in closed form for the asymptotic covariance matrix of the ML estimator can be explicitly provided by using matrix differential calculus. To the best of our knowledge, such an expression is completely new and cannot be easily derived from the univariate case. The matrix formulas we obtain are potentially useful for statistical inference of the considered models, and provide a significant contribution to the existing literature on the topic.====Third, we derive explicit matrix formulas for the Wald statistic and the likelihood ratio (LR) statistic to test for endogeneity. This fills a gap in the literature on the multivariate context, and completes the discussions presented in [25], [26] for the special case of univariate regressions. The proposed LR statistic has a rather simple form as it reduces to the use of the estimated unrestricted and restricted covariance matrices of the regression disturbances and the smoothed regime probabilities.====The plan of the rest of the paper is as follows. In Section 2 we introduce the model, describe the main assumptions, and provide a vectorial representation of it, which is used to derive the main results. Then we describe the estimation procedure in matrix form for multivariate MS first-order autoregression models with endogenous explanatory variables, based on explicit matrix formulas for the ML estimators of model parameters. In Section 3 we prove consistency of such estimators and their asymptotic properties. More precisely, we provide an explicit matrix expression in closed form for the asymptotic covariance of the ML estimator. A procedure to test for endogeneity based on the Wald statistic and the LR statistic completes this section. Section 4 is devoted to discuss a numerical application. Section 5 gives some concluding remarks. A final Appendix collects all technical proofs of the results. For matrix differential calculus we refer to [33], [34], [36], [37].",Statistical analysis of Markov switching vector autoregression models with endogenous explanatory variables,https://www.sciencedirect.com/science/article/pii/S0047259X23000106,19 February 2023,2023,Research Article,23.0
"Lee Yoonseok,Sul Donggyu","Department of Economics and Center for Policy Research, Syracuse University, Syracuse, NY 13244, United States,Department of Economics, University of Texas at Dallas, Richardson, TX 75080, United States","Received 16 February 2022, Revised 13 February 2023, Accepted 14 February 2023, Available online 17 February 2023, Version of Record 26 February 2023.",https://doi.org/10.1016/j.jmva.2023.105165,Cited by (0),We study the depth-weighted ,"The depth of multivariate data is commonly used to measure how close a given observation vector is toward the center of the underlying joint distribution, and hence it leads to a center-outward ordering of each observation. Examples of data depth measures include the half-space depth [33], the simplicial depth [22], the projection depth [23], [35], [38], and Mahalanobis depth [25], to name a few. As a robust location estimator of multivariate observations, [5], [22], [24], [31] consider using the depth to construct weighted means, which enjoy good efficiency and robustness properties. The asymptotic behavior of the general form of the depth-weighted ====-type location estimators is studied in [37].====These studies presume that we observe the true multivariate data of interest and estimate its depth-weighted mean. In this paper, we instead assume that we cannot observe the true data of interest but the observations measured with errors. The main interest of this paper is to use such noisy observations to estimate the depth-weighted mean of the latent variables (i.e., the data without noise) and study its limiting properties. To this end, we consider a drifting asymptotic framework to ensure a meaningful bias–variance trade-off in the limit, where the noise vanishes at a certain rate as the sample size increases though it presents for any fixed sample size. Under this framework, we extend the asymptotic results of [37] to the depth-weighted ====-type location estimator of noisy data. We show that, though such a local deviation or noise in general yields non-zero bias in the limit expressions of the empirical distribution and its linear statistical functions, it is not the case for the depth-weighted ====-type location estimator when the noise vanishes at (or faster than) the square-root of the sample size. This reveals the robustness property of the depth-weighted mean estimator in the view of local misspecification of the underlying distribution.====A motivating example of observations measured with noise that satisfies the drifting asymptotics is a collection of consistent estimators. More precisely, for any parameter vector of each individual agent in a heterogeneous longitudinal model, such as the heterogeneous treatment effect, we can regard its consistent estimator as the observation with vanishing noise, whereas the true parameter vector value is the latent observation without noise. In this context, as an application, we consider model averaging in heterogeneous longitudinal data regression models and develop the depth-weighted mean-group (DWMG) estimator of a vector of random coefficients. Under a certain rate condition between the magnitude of the noise and the sample size, it estimates multivariate average effects in heterogeneous longitudinal data models consistently and also robustly toward outlying individuals or erroneous reports. In this regard, it extends the common average estimators in heterogeneous panel data regression models such as [13], [29], [32]. It can be also seen as a robust difference-in-difference estimator in a highly heterogeneous environment or an average estimator of heterogeneous (multivariate) treatment effects. When the partial effect is indeed homogeneous, furthermore, the depth-weighted mean-group estimator is shown to be consistent to the true effect. The simulation results show that this new estimator can achieve a good balance between robustness and efficiency.====The rest of the paper is organized as follows. In Section 2, we study the empirical distribution of multivariate noisy data and establish an asymptotic representation of the depth-weighted mean estimator of the noisy observations. We also discuss statistical inference of the depth-weighted mean. In Section 3, we apply the new estimator in the context of longitudinal data regression and develop the depth-weighted mean-group estimator. In Section 4, we present the finite sample efficiency of the depth-weighted mean-group estimator in simulations and examine relative purchasing power parity as an empirical illustration. We conclude in Section 5. We collect all the proofs and computational details in Appendix.",Depth-weighted means of noisy data: An application to estimating the average effect in heterogeneous panels,https://www.sciencedirect.com/science/article/pii/S0047259X23000118,17 February 2023,2023,Research Article,24.0
"Qiu Tao,Xu Wangli,Zhu Lixing","Center for Statistics and Data Science, Beijing Normal University, Zhuhai, 519087, China,Center for Applied Statistics and School of Statistics, Renmin University of China, Beijing 100872, China","Received 1 July 2022, Revised 20 January 2023, Accepted 20 January 2023, Available online 2 February 2023, Version of Record 3 February 2023.",https://doi.org/10.1016/j.jmva.2023.105160,Cited by (0),Testing for independence between two random vectors is a fundamental problem in ,"Let ==== and ==== be two random vectors with distribution functions ==== and ====, respectively, and ==== and ==== are two random samples drawn independently from ==== and ====, respectively. Consider the hypothesis testing problem: ====where the alternative hypothesis relates to measuring nonlinear dependence. As well known, the testing for independence between two random vectors is a fundamental problem in statistics [5], [17].====Nonlinear dependence is associated with a wide variety of dependence measures. In the univariate case of ====, the Spearman’s ==== [20] and Kendall’s ==== [13] are perhaps two of the most classical metrics to measure the degree of nonlinear dependence between two univariate random variables. In the multivariate case of ====, among all the nonlinear dependency measures, the distance covariance and Hilbert–Schmidt independence criterion have received great attention in statistics communities and the machine learning field. Specifically, [24] introduces the distance covariance (dCov, hereafter) to quantify the difference between the joint characteristic functions of the two random vectors and their marginals. There are many follow-up studies and generalizations in the low-dimensional context; see for example [3], [11], [15], [21], [23], [27]. Gretton et al. [8] propose the Hilbert–Schmidt independence criterion (HSIC, hereafter) to quantify the covariance between functions of the random variables in reproducing kernel Hilbert spaces. This methodology has also been extended in various ways; see for example [1], [6], [7], [8], [10], [19]. In addition, [18] shows that HSIC can be seen as a generalization of dCov by kernelizing the Euclidean distance and further establishing the equivalence between the HSIC and dCov via the correspondence between positive definite kernels and semi-metrics of the negative type.====Both dCov and HSIC are nonnegative and equal to zero if and only if ==== in (1) holds and require weak distributional assumptions on the pair of random vectors. Further, the complexities of calculating dCov and HSIC are of order ====. Therefore, these two measures have been used to construct computationally simple tests. However, the behaviors of these tests are focused on fixed ==== and ==== in the literature. As the distance covariance and kernel-based dependence metrics are based on the usual Euclidean distance [18], this distance usually causes the curse of dimensionality in high dimensions. Chakraborty and Zhang [2] point out that the distance correlation-based ====-test proposed by [22] for independence between two high dimensional random vectors has trivial power when the two random vectors are nonlinearly dependent but component-wise uncorrelated. In addition, [4], [28] show that the sample dCov and HSIC can only detect componentwise linear dependency, but fail to detect non-linear dependency in the regime of fast-growing dimensionality ====. To overcome this problem, [28] suggests a test of independence by aggregating the componentwise sample marginal statistics, and [2] proposes independence tests using the sum of group-wise squared sample (generalized) dCov or HSIC. These tests mainly quantify only marginal or group-wise non-linear dependence and cannot capture the dependence structure among the components. In general, these tests are not consistent with general alternatives.====This paper introduces two nonparametric dependence tests, which generalize the dCov and HSIC tests in terms of random subspace. We randomly select two subspaces consisting of components of the vectors, respectively. Let ====, and ====, where ====’s and ====’s are respectively drawn independently from Bernoulli distribution ==== and ==== for ==== and ====. Denote by ==== and ==== the joint distribution functions of ==== and ====, respectively. Specifically, we propose mapping high dimensional vectors ==== and ==== to ==== and ====, where ==== denotes the Hadamard product. Then, we test whether ==== and ==== are independent through comparing the differences of the dependence of ==== and ==== by the following generalized dCov and HSIC ==== where ==== and ==== are respectively defined in (4), (7). We integrate over ==== and ==== such that all possible subspaces are taken into account to avoid potential power loss caused by lousy subspaces. We introduce (2), (3) to quantify the difference of the dependence between ==== and ====, which has the following advantages.====1. ==== and ==== are nonnegative and equals zero if and only if ==== in (1) holds true. Therefore, our proposed test based on (2) or (3) is consistent and has nontrivial power against general alternatives.====2. The proposed tests inherit the properties of dCov and HSIC in the low-dimensional setting as they detect the dependence between any pair of two low-dimensional sub-vectors from ==== and ====. Thus, they can detect non-linear dependence between two high-dimensional random vectors.====3. The asymptotic theory is established under a general multivariate model with certain moment conditions with dimension-agnostic properties. In addition, the asymptotic null distributions of our proposed tests are standard normal, without resorting re-sampling procedure to approximate critical values.====The rest of this paper is organized as follows. Section 2 reviews the definition of dCov and HSIC gives the explicit forms of the test statistics. Section 3 investigates asymptotic behaviors of the proposed tests. Section 4 includes some simulation studies to evidence the power performances of our proposed tests in the comparisons with several existing tests. Finally, Section 5 contains a brief discussion. All technical details are relegated to Appendix.",Independence tests with random subspace of two random vectors in high dimension,https://www.sciencedirect.com/science/article/pii/S0047259X23000064,2 February 2023,2023,Research Article,25.0
Miyazaki Izuru,"Toyota Central R&D Labs. Inc., Japan","Received 8 August 2022, Revised 20 January 2023, Accepted 22 January 2023, Available online 1 February 2023, Version of Record 1 February 2023.",https://doi.org/10.1016/j.jmva.2023.105161,Cited by (0),"In high-dimensional data analysis, we often encounter partly sparse and dense signals or parameters. Considering an ====-penalization with different ====s for each sub-vector of the signals, we formularize an optimal solution for ==== or 2 in a linear regression model to well represent such signals or parameters. We also provide an algorithm to derive it. Furthermore, we provide the consistency result of the variable selection in this optimal solution under a fixed design. Simulation study and real-data analysis illustrate its improved variable selection performance relative to the conventional methods.","In high-dimensional data analysis, signal recovery and parameter estimation are challenging tasks [10], [13], [14], [15], [19], [20], [24], [41]. ====-based penalization is the representative method for these tasks, and various derivatives have been proposed for their use [27]. ====-based penalization (e.g., lasso [33] and soft-thresholding [11], [12]) are widely used, and its statistical properties, including the consistency of its prediction error [35], parameter estimation [8], [26], [35], and variable selection [25], [37], [42], have been studied extensively. This method produces sparse solutions in which many estimated parameters are exactly zero; therefore, the method is suitable for sparse signal recovery [4], [7], [8], [33], [37].====Another commonly used method is ====-based penalization (e.g., ridge estimation [21]). Unlike ====-based penalization, ridge estimation produces a dense solution in which all parameter estimates are nonzero; therefore, it is suitable for dense signal recovery [6], [16]. For theoretical properties, see [22].====Contrary to expectations, there are cases in which signals and parameters are only partly sparse and dense, so they cannot be well-approximated by a single sparse or dense vector. Partial (but sometimes large amounts of) prior knowledge is valid case. For examples, the support of a medical image sequence in dynamic MRI undergoes small variations with support changes, thus the previous estimate can be used as prior knowledge in dynamic reconstruction (e.g., [28]). Furthermore, in epidemiologic studies, for example, we have environments in which some explanatory variables are known to have nonzero parameters in advance from other studies, such as experimental studies, finding other explanatory variables that also have nonzero parameters is critical. Additionally, if sufficient prior knowledge is obtained from other studies, these parameters would be dense. Thus, the assignment or position of the dense parameter or component of the design matrix can be assumed to be known for the estimation of parameters. This setting is assumed throughout this paper.====Considering the properties of ====- and ====-based penalization for linear regression modeling, the following estimator may be suitable for partly sparse and dense signal recovery: ====where ==== is an outcome variable, ==== is a design matrix whose columns each contain explanatory variables, ==== are parameters, ==== and ==== are penalty constants greater than zero, and ==== represents an ====-norm. This is a slight generalization of the formulation of [16], [39] so that the ====-norm can change for each parameter’s sub-vector. This method can apply different penalty functions for each parameter component based on its sparsity or density; therefore, we call this penalization a “talio” of parameters. In this context, the model in which ==== is either one or two (talio-====) is notable: ====where ==== and ==== are the dense components of design matrix ==== and its corresponding parameters, respectively. Here, ==== and ==== are the sparse components of the design matrix and its corresponding parameters, respectively, and ==== and ==== are some positive constants. In this paper, we focus only on this estimator. If we define the ordinary least squares (OLS) estimator as ====, the optimal solution for the simplest case of talio-==== with ==== can be represented as shown in Fig. 1. This optimal solution provides a nonzero estimate for ====. On the contrary, it can provide an estimate that is exactly zero for ==== (red ellipses in the figure).====There are some related works considering the cases in which signals and parameters are not well-approximated by a single sparse or dense vector. Elastic net [44] and lava [9] also combine the ====- and ====-norm penalties for parameters; however, they are both calculated for the entire parameter vector instead of the sub-vectors, as opposed to talio-====. The trimmed lasso method [3], [40] considers different ====-penalty constants for each parameter. Still, it does not impose penalties on the largest ==== parameters within all parameters, in which ==== is a constant. In contrast, it imposes an ==== penalty on the remaining parameters with the same penalty constants. The ==== package of the lasso implementation [17] provides a method in which pre-specified parameters are non-penalized, while the other parameters are ====-penalized. Adaptive lasso [43] imposes different penalty constants for each parameter that is inverse proportional to the absolute value of the OLS of the parameter. The spike-and-slab lasso [29] is a Bayesian variable selection method that considers different penalties for each parameter with mixtures of Laplacian distributions. The concept of talio-==== appears simple, but thus far, its statistical properties and computation algorithms have not been investigated.====The remainder of this paper is structured as follows. In Section 2, we formularize the talio-==== estimator in a linear regression model and derive an algorithm to compute the estimates of these parameters when data are obtained. We also characterize this estimator and its relationship with other models (e.g., non-penalizing and precondition methods). In Section 3, we perform a theoretical analysis of this estimator. In particular, we evaluate the sufficient condition for the consistency of variable selection from the sparse component under a fixed design. In Section 4, we provide simulation experiments to evaluate the performance of this estimator. We show the suitability of this method for the case in which we have partial prior knowledge of nonzero parameter values in addition to the sparsity of the parameter vector. In Section 5, we apply this estimator to real data to evaluate the practical performance. Section 6 presents our conclusions.",Recovery of partly sparse and dense signals,https://www.sciencedirect.com/science/article/pii/S0047259X23000076,1 February 2023,2023,Research Article,26.0
"Hyodo Masashi,Nishiyama Takahiro,Pavlenko Tatjana","Faculty of Economics, Kanagawa University, 3-27-1 Rokkakubashi, Kanagawa-ku, Yokohama-shi, Kanagawa, Japan,Department of Business Administration, Senshu University, 2-1-1, Higashimita, Tama-ku, Kawasaki-shi, Kanagawa 214-8580, Japan,Department of Statistics, Uppsala University, SE-751, 05, Uppsala, Sweden","Received 23 February 2022, Revised 26 January 2023, Accepted 27 January 2023, Available online 30 January 2023, Version of Record 30 January 2023.",https://doi.org/10.1016/j.jmva.2023.105162,Cited by (0),"We revisit the well-known Behrens–Fisher problem in an original and challenging high-dimensional framework, and propose a testing procedure which accommodates a low-dimensional ","Let ==== be iid ====-dimensional random vectors collected from the ====th subject in the ====th population with mean vector ==== and covariance matrix ====, where ==== denotes the distribution function for ====th population, ====, ==== and ==== denotes the set ==== for ====. Specifically, we design the test procedure for testing ====in the setting where ==== may be large and even ====, ==== may be non-normal and ==== may be unequal which, along with ==== also allowed to be unequal.====As the classical methods of mean comparisons for low-dimensional data, of which the best-known is Hotelling’s ==== test, do not work when ==== and need to be modified, a number of useful two-sample tests have been proposed for high-dimensional settings. The construction of many such tests has been motivated by the work of Bai and Saranadasa [3], who proposed to substitute an identity matrix ==== for the pooled sample covariance matrix in Hotelling’s ==== statistic under the assumption of a common covariance matrix ====. Chen and Qin [4] extended the ====-norm-based construction and established asymptotic properties of the proposed test under much weaker conditions, in particular by relaxing the homoscedasticity assumption of Bai and Saranadasa [3]. Note that their tests are guaranteed under the ====–type condition (sphericity condition) which stated as ====see e.g., assumption (3.6) in Chen and Qin [4]. Here, ==== is the trace operator of a matrix. The sphericity assumption is crucial for establishing the asymptotic normality of the test statistic proposed by Chen and Qin [4]; see Theorem 1 of their paper. However, besides of being difficult to verify in practice, sphericity assumption can be easily violated in covariance models where the eigenvalues of ==== are dominated by few top ones.====Therefore, it is necessary to establish a test method for the situation where the sphericality condition is not satisfied. Recently, some proposals have been put forth in the literature with this motivations in mind. There are two different approaches to this effort. One is the test under the covariance structure called the strongly spiked structure, and the other is test under the low-dimensional factor model. Under these two settings, the sphericity conditions (2) are not satisfied, and the essential difference between the two settings is the structure of the noise term. The test for (1) under strongly spiked structure has been proposed by Ishii [5], Aoshima and Yata [2], and Ishii et al. [6]. On the other hands, Ma et al. [7] proposed a test for (1) under the low-dimensional factor model with homoscedasticity assumption, i.e., an assumption of common covariance matrix ====. However, this is a very strong assumption which is hard to practically verify in ==== settings.====As mentioned above, while the tests under the strongly spiked structure are well developed, the tests under the factor model are still evolving. Therefore, in this research, we are interested in developing a high-dimensional test for difference of mean vectors, by relaxing the standard latent factor model assumptions, e.g., large-sample setting ====, normality and homoscedasticity. In other words, we aim to improve the Ma et al.’s method under the low-dimensional factor model.====A factor model is a convenient structural assumption on the covariance matrix which is popular in a wide spectrum of modern applied fields like genetics, microbiome and metagenomic, fMRI, economics and finance, or more generally in high-dimensional data, where the dependence of measurements can be attributed to a relatively small number of latent factors. A factor model assumes that for each ====, the observable vector ==== is decomposable into a latent factor and an idiosyncratic (noise) component as follows: ====where ==== is a deterministic intercept vector, ==== is a ====-dimensional latent (unobservable) factor vector and ==== is a ====-dimensional error (noise) vector which is uncorrelated with the latent factor. In what follows, we assume that ==== is a fixed number. Further, ==== denotes the loading matrix where for each ====, ==== is a non-random vector, and ==== is a non-random ==== diagonal matrix whose elements are ====. For the latent vector ==== and error vector ====, we further assume that ==== are iid with ====, ==== and ====, and ==== are iid with ====, ==== and ==== for ====, ====, ==== and ====. The structural assumptions of the model (3) imply that ====where ==== and ==== denotes the space of real, symmetric, positive definite, ==== matrices.====We do not assume that ====; our test statistics, along with their limit properties are studied under heteroscedasticity, i.e., solves a general, two-sample Behrens–Fisher problem for the latent factor model (3). Our testing procedure can accommodate the class of highly spiked high-dimensional covariance models of ====’s where the few leading eigenvalues may be extremely large. Furthermore, this relaxation is of crucial importance for the asymptotic theory of the proposed tests. Due to the restrictive framework of the normal asymptotic theory, we shift focus to more flexible approximation types, specifically, to a chi-square mixture-type of asymptotic approximation, which leads to totally different testing procedures compared to those of Bai and Saranadasa [3] or Chen and Qin [4]. Our asymptotic results are valid with no specific distributional on the data. Numerical studies demonstrate that the chi-square mixture asymptotic approximation allows for better size control under a variety of practical scenarios of the factor model in high dimensions.====The rest of this paper is organized as follows. Section 2 lays out a high-dimensional asymptotic framework, presents the new test statistics along with their limiting properties, and provides the data-driven test procedures. Discussion and concluding remarks are provided in Section 3. All proofs and auxiliary technical results are delegated to Appendix. Supplemental material provides evaluation of a finite sample performance of the proposed tests where the simulation study is followed by the real-data applications.",A Behrens–Fisher problem for general factor models in high dimensions,https://www.sciencedirect.com/science/article/pii/S0047259X23000088,30 January 2023,2023,Research Article,27.0
"Guo Wenxing,Balakrishnan Narayanaswamy,He Mu","Department of Mathematical Sciences, University of Essex, Colchester, United Kingdom,Department of Mathematics and Statistics, McMaster University, Hamilton, Canada,Department of Foundational Mathematics, Xi’an Jiaotong-Liverpool University, Suzhou, China","Received 1 April 2020, Revised 10 January 2023, Accepted 10 January 2023, Available online 14 January 2023, Version of Record 19 January 2023.",https://doi.org/10.1016/j.jmva.2023.105159,Cited by (0),Envelope models were first proposed by Cook et al. (2010) as a method to reduce estimative and predictive variations in ==== and oracle property in high-dimensional data. We carry out some ==== studies and also analyze two datasets to demonstrate that the proposed envelope-based sparse reduced-rank regression method displays good variable selection and prediction performance.,"In this work, we consider the following multivariate linear regression model: ====where ==== denotes a multivariate response vector, ==== denotes a non-stochastic vector of predictors, ==== is an error vector having mean 0, covariance matrix ==== and is independent of ====, and ==== is the regression coefficient matrix in which we are primarily interested in. If ==== is a vector of random quantities during sampling, then the model is conditional on the observed values of ====. Let ==== and ==== denote ==== and ====, respectively. Without loss of generality, let us assume that the data are centered, so that the intercept can be excluded from the regression model. Then, model (1) can be re-expressed as ====where ==== is ====.",Envelope-based sparse reduced-rank regression for multivariate linear model,https://www.sciencedirect.com/science/article/pii/S0047259X23000052,14 January 2023,2023,Research Article,28.0
"Chen Kun,Chan Ngai Hang,Yau Chun Yip,Hu Jie","Southwestern University of Finance and Economics, China,City University of Hong Kong, Hong Kong,The Chinese University of Hong Kong, Hong Kong,Xiamen University, China","Received 30 September 2022, Revised 6 January 2023, Accepted 6 January 2023, Available online 14 January 2023, Version of Record 17 January 2023.",https://doi.org/10.1016/j.jmva.2023.105156,Cited by (0)," of the proposed estimator are derived under mild assumptions without assuming Gaussianity. In addition, a computationally efficient method is developed to optimize the penalized likelihood function. Simulation results and real data examples are also provided to illustrate the finite sample performances of the methodology.","Recently, there has been increasing interest in analyzing spatial data in various fields, such as environmental science, economics and sociology [7], [19]. Inference for spatial data can be categorized into two approaches, i.e., the spatial domain approach and the frequency domain approach. Traditional spatial domain approach often relies on the Gaussian likelihood method, which requires evaluating the determinant or the inverse of the covariance matrix. However, when the dataset is huge, the method involves intensive computational burden and is difficult to implement [11], [14], [22]. To overcome this problem, in recent years, the frequency domain approach [4], [9], [15], [26] has attracted increasing interest, which is based on (local) Whittle approximation of Gaussian likelihood [25]. The Whittle likelihood function only involves periodogram ordinates and spectral density functions, which are easy to calculate by the fast Fourier transform algorithm.====The main goal in frequency domain analysis is to estimate the spectral density function. In a time series context, nonparametric estimation methods for spectral density have been studied in [5], [17], [23]. In the spatial context, [18], [20] investigated nonparametric kernel estimators of spectral densities for regularly spaced random fields. The methods of [18], [20] rely on kernel bandwidths to control the smoothness of the spectral density estimates. In addition, they only consider the Gaussian processes, and do not address the difficult problem of optimal bandwidth selection. Compared to their methods, our method is a penalized spectral-likelihood method, where the penalty parameter controls the smoothness, and is applicable to both Gaussian and non-Gaussian processes. Since the penalty involves derivatives of log-spectral density functions (log-SDFs), our estimates can be ensured to be differentiable. Moreover, the difficult problem of optimal smoothness penalty parameter is addressed. Recently, [3] proposed the penalized Whittle likelihood for estimating and clustering two-dimensional SDFs. However, their method is not directly comparable to our method for the following reasons. First, our method can be applied to more than two-dimensional data and SDFs. Second, apart from the smoothness penalty, they consider a spatial dependence penalty to spatially-correlated subregions for clustering the SDFs. In contrast, we do not consider spatial clustering, and thus the spatial dependence penalty is irrelevant in our case. Third, they assume that the SDFs can be represented by a linear combination of a set of ==== linear independent common basis functions, which can be further spanned by ====
 (====) basis functions. The determination of the number ==== and ==== and the choice of basis functions is difficult and lacks theoretical justification. In contrast, our method is fully non-parametric, requiring only the smoothness of SDFs, and is applicable to spatial data on a ====-dimensional grid with ====. Furthermore, the neighboring data-sites in the methods of [3], [18], [20] are assumed to be separated by a minimum distance, which may not be satisfied in mining and some geostatistical applications. However, we consider the mixed increasing domain (MID) asymptotics, which allows arbitrary small neighboring data-sites.====The penalized likelihood approach has been widely used in nonparametric function estimation (see [24]), which can be regarded as a natural extension of the parametric likelihood estimation. In this paper, by extending the work of [17] for time series, we develop a penalized Whittle likelihood estimation procedure for nonparametric spectral density estimation for regularly spaced spatial data. In particular, our method can be applied to non-Gaussian spatial processes, including linear and Markov random fields, by allowing the data-sites to fill in the sampling sites increasingly densely. The estimator unifies several nonparametric and semi-parametric estimation methods into a coherent framework. For example, the raw periodogram is an un-penalized estimator of the spectral density; and the smoothed log-periodogram [23] is the first order approximation of the penalized Whittle likelihood estimator.====To establish consistency, a key condition is the asymptotic independence of the spatial periodogram, which may be affected by the asymptotic regimes and shape of sampling regions [2], [7], [21]. To achieve asymptotic independence of spatial periodograms, the sampling regions are formed with shapes of hyper-cubes or hyper-rectangles. Furthermore, we adopt two types of asymptotic regimes, i.e., pure increasing domain (PID) asymptotics and mixed increasing domain (MID) asymptotics, which enables asymptotic independence of spatial periodograms, and thus guarantees the consistency of the penalized Whittle likelihood estimator. A key difficult problem of nonparametric function estimation is the choice of optimal smoothing parameter. In our method, the penalty parameter controls the smoothness of the density estimates. Under both PID and MID asymptotics, we propose an objective data-driven method to determine the penalty parameter, and develop a computationally efficient algorithm to optimize the penalized likelihood function.====The remainder of this paper proceeds as follows. Section 2 reviews results on spatial sampling designs and spatial periodograms for regularly spaced data. In Section 3, we propose the penalized Whittle likelihood under both PID and MID asymptotics. In Section 4, we state the regularity conditions and the main result, i.e., the consistency of the penalized Whittle estimators and the associated convergence rates. Simulation studies are given in Section 5. All proofs are provided in Appendix A. The Supplementary Material provides a real data analysis.====Throughout the paper, the following notations are adopted: ==== denotes a term (a random variable) which is bounded (in probability); ==== denotes a term (a random variable) converging to zero (in probability); the variable driving the asymptotics is denoted by ====. for ====, let ==== denote the ====-norm of ====; ==== is the ====-dimensional lattice; and ==== denotes the positive line of ====.",Penalized Whittle likelihood for spatial data,https://www.sciencedirect.com/science/article/pii/S0047259X23000027,14 January 2023,2023,Research Article,29.0
"Yang Shuquan,Ling Nengxiang","School of Mathematics, Hefei University of Technology, Hefei 230009, China","Received 23 March 2022, Revised 6 January 2023, Accepted 6 January 2023, Available online 10 January 2023, Version of Record 13 January 2023.",https://doi.org/10.1016/j.jmva.2023.105155,Cited by (0),Semiparametric factor structures are ubiquitous in panel data analysis. Conventional methods for estimating the ,"Driven by wide applications in many areas of social sciences, such as econometrics, finance, and education, panel data analysis has been one of the most compelling topics in statistical research. In the era of big data, massive information acquisition has facilitated the collection of high-dimensional data with complex structures. When both cross-section ==== and time dimension ==== are large, classical pure panel data models are no longer suitable to capture dynamic relationships among subjects as some dependency structures may be characterized by a few latent factors. To deal with this problem, a large number of researchers have considered large-scale panel data models with factor structures, see [1], [3], [4], [5], [21], [26], [29] for details. For example, Pesaran [26] proposed the common correlated effects (CCE) estimators of the individual-specific coefficients that allow for multiple factor error structure with ==== and ==== going to infinity. He exerted ordinary least squares (OLS) on an auxiliary regression, where the observed regressors are augmented by the cross-sectional averages of the dependent and independent variables. Bai [3] studied the large panel data models with multiple interactive effects. He alleviated the restriction of the slope parameter in [26] by imposing additivity and further discussed the identification, consistency, and the limiting distribution of the interactive-effects estimators. Ando and Bai [1] improved Bai’s model by adding a group-specific pervasive factor structure and presented an information criterion for choosing the number of group-specific factors, the number of groups, and relevant regressors under large cross-section and long time series. Su et al. [29] developed the classifier-Lasso method to classify and estimate linear panel data models with unknown homogeneous groups and heterogeneity across groups. Their method determined the penalty in terms of a mixed additive-multiplicative penalty form in order to produce a joint shrinkage process, thereby delivering simultaneous variable selection and estimation in a single step. While methods for panel data models with traditional factor structures are well-established, there are very few studies that develop methods for panel data models with semiparametric factor structures. Semiparametric factor models are commonly used in finance. For instance, factor loadings are known to depend upon the subject-specific observations which represent a series of time-invariant characteristics including individual stocks’ size, momentum, and values. To incorporate the information that arises from these characteristics, [7], [8] modeled explicitly the loading matrices as the functions of characteristics. Ma et al. [22] considered a quantile regression version of [7] under the same semiparametric factor model and proposed an alternate optimization algorithm to estimate the factor returns and the characteristic-beta functions without any moment conditions. Furthermore, [13] extended their model so that components in the factor loadings are not completely explained by characteristics and proposed a projected principal component analysis (projected-PCA) method, which begins with projecting the observations onto the sieve space spanned by the basis functions of covariates and improves the PCA method, to estimate the unknown factors and loading functions. Recently, [39] introduced the latent semiparametric factors into panel data models to simultaneously account for the subject-specific heteroscedasticity and the serial correlations. A two-stage projection-based estimator (TOPE) for both the modulating and dependence components of the model was derived from a generalized least squares (GLS) type approach. However, the least-squares-based estimation often suffers from the shortcoming of lack of robustness in the presence of heavy-tailed data. It is well known that financial returns exhibit heavy tails (see [9]). Several works have provided evidence that the effect of heavy-tailedness is non-negligible for estimating the factor space and loadings (see [17], [18], [20], [36], [38]). To the best of our knowledge, [17] for the first time proposed a method in the robust PCA literature that can consistently estimate the factors without imposing moment constraints on the idiosyncratic errors. He et al. [20] conducted robust quantile factor analysis, and identified the common and idiosyncratic components for each variable without any moment constraint on the idiosyncratic errors. This motivates us to consider a robust estimation tool for panel data models with semiparametric factor structures.====In this paper, we pursue the robust estimators of regression coefficients by replacing the least squares (LS) regression with the Huber regression that was initiated in Peter Huber’s seminal work [19]. Some recent studies (e.g., [2], [11], [12], [14], [23], [31], [35], [40]) have tackled issues arising from heavy-tailedness by revisiting Huber’s wisdom. Sun et al. [31] achieved nonasymptotic deviation bounds for adaptive Huber regression in the low and high dimensional regime. Fan et al. [11] and Zhou et al. [40] investigated Factor-Adjusted Robust Multiple Testing (FarmTest) procedures for large-scale simultaneous inference with dependent and heavy-tailed data. More recently, [35] proposed a data-driven procedure to robustly estimate the large-scale panel data with an interactive effects model and learned the homogeneity structure. Different from all these works above, our main focus is on the panel data with the semiparametric factor model, where both factors and idiosyncratic errors are allowed to be heavy-tailed and serially-correlated. Following the ideas of [35], [39], we propose two types of Huber estimators of the regression coefficients and establish the corresponding non-asymptotic properties with only finite moment conditions. As an illustration, Fig. 1 depicts the boxplots of the mean squared error (MSE) for estimating the regression coefficients by the Huber estimator (HE) method, the ====-regularized Huber estimator (RHE) method, and the TOPE method proposed by [39] when both factors and idiosyncratic errors are from the light-tailed or heavy-tailed distribution. It can be clearly seen that our method leads to much smaller dispersions and biases regardless of the distribution’s tail. More detailed simulation designs can be found in Section 4. We unveil the proposed robust procedure in two steps. In the first step, we recover the factors and loading functions by performing the projected-PCA method in [13] to the refined response variables. With the estimated factors and loadings, the original model can be transformed into a heteroscedastic regression model. We further estimate the regression coefficients by adopting the Huber loss. In the second step, we consider ====-statistic based estimators for the covariance matrix of factors and idiosyncratic errors. Combining with the loading matrices from the first stage, the covariance matrix of heteroscedastic errors will be obtained. Then, we employ an iterative optimization procedure to solve the Huber loss minimization problem, starting from some preliminary estimators of the regression coefficients. Our simulation studies show that the iteration of the Huber estimators is terminated within a finite number of alternating steps. In addition, we are also concerned with the ====-regularized Huber estimators when the number of parameters exceeds the sample size. Recent studies by [25] revealed that the iteratively reweighted ====-regularization is particularly useful for high-dimensional adaptive Huber regression to achieve oracle statistical property and good computational efficiency at the same time. Motivated by such an iterative procedure, we address the ====-regularized Huber loss minimization problem by the alternating direction method of multipliers (ADMM), which originates from [6], in the sense of treating it as an iteratively reweighted GLS problem. Numerically, the proposed robust estimators are proved to be able to improve the interpretability as well as prediction accuracy in various cases even with the heavy-tailed factors and idiosyncratic errors.====The remainder of this paper is organized as follows. Section 2 reviews the panel data models with semiparametric factor structures and the Huber regression. Estimators of the factors, loadings, and regression coefficients are also provided. In Section 3, we introduce some setup assumptions and carry out the non-asymptotic analysis including the convergence rate for our estimators. Simulation studies and an application to a real dataset are then presented in Section 4. We conclude the article in Section 5, followed by some extra numerical results and proofs of the main theorems given in the Appendix.====We adopt the following notations throughout the paper. For any vector ==== and ==== is the ==== norm. For a ==== matrix ====, we write ====, ==== and ====. When ==== is symmetric, we have ====, where ==== are the eigenvalues of ====. Further, we use ==== and ==== to denote the maximum and minimum eigenvalues of ====, respectively. The constants ==== in different lines can be nonidentical.",Robust projected principal component analysis for large-dimensional semiparametric factor modeling,https://www.sciencedirect.com/science/article/pii/S0047259X23000015,10 January 2023,2023,Research Article,30.0
"Baringhaus Ludwig,Gaigall Daniel","Institute of Actuarial and Financial Mathematics, Leibniz Universität Hannover, Postfach 6009, 30060 Hannover, Germany,FH Aachen - University of Applied Sciences, Heinrich-Mußmann Straße 1, 52428 Jülich, Germany","Received 9 April 2022, Revised 28 December 2022, Accepted 28 December 2022, Available online 30 December 2022, Version of Record 5 January 2023.",https://doi.org/10.1016/j.jmva.2022.105154,Cited by (0),", we consider testing the hypothesis that the distribution of ==== as the number of raindays, and ==== as the number of losses, and ==== as total loss expenditure during a certain time period. The compound Poisson exponential model is characterized in the way that a specific transform associated with the distribution of ==== satisfies a certain differential equation. Mimicking the function part of this equation by substituting the empirical counterparts of the transform we obtain an expression the weighted integral of the square of which is used as test statistic. We deal with two variants of the latter, one of which being invariant under scale transformations of the ","Let ==== be a bivariate random vector, where ==== is non-negative and ==== takes values in ====, the set of natural numbers including zero. In the case that ==== has the same distribution as the sum ====, where the random variables ==== are independent, the distribution of ==== belongs to the Poisson distribution family, and the random variables ==== have the same exponential distribution, we speak of the compound Poisson exponential model for the distribution of ====. Application of the compound Poisson exponential model takes place in stochastic hydrology, see [14], [15]. Here, ==== and ==== model the total rainfall amount and the number of raindays during a certain period, where ==== can be seen as the single (e.g., daily) rainfall amounts in the time period. The compound Poisson exponential model is related to (a special case of) the collective risk model, where the total risk of a collective is modeled by a sum of individual risks, and the number of summands is also random. In actuarial science, ==== and ==== typically describe the total loss expenditure and the number of losses of an insurer during a certain time period, where ==== can be seen as the individual losses in the time period. In this context, application takes place in risk management of general insurance, especially for modeling large losses, see [3]. Applications of the collective risk model in practice are also given for modeling operational risks, see [1]. The works [4], [7], [11] discuss methods for verifying the distributional assumption for ==== which arises with the compound Poisson exponential model, that is the so-called compound Poisson exponential distribution, a special case of the compound Poisson distribution; compare with [7]. In applications, the data typically arise as observations of pairs ====, that are independent copies of ====. For instance, we have with ==== the total rainfall amount ==== and the number of raindays ==== in year ==== in stochastic hydrology, or the loss expenditure ==== and the number of losses ==== in month ==== in actuarial science. In particular, observations for single rainfall amounts or individual losses within the respective years are often not available. This motivates treating the goodness-of-fit problem that the joint distribution of ==== belongs to the class of distributions arising with the compound Poisson exponential model on the basis (of observations) of ====. It is important to point out once again, that we do not assume in advance that ==== has the same distribution as ====, with ==== as a sequence of independent and identically distributed positive random variables that is independent of ====. In fact, we even do not assume that ==== has the same distribution as ====, where ==== are independent, ==== is a sequence of positive independent and identically distributed random variables and ==== is some non-negative integer valued random variable; see Remark 1. In Section 2 we propose a new test that takes advantage of the fact that the compound Poisson exponential model can be characterized in the way that a new specific transform associated with the distribution of ==== satisfies a certain differential equation. Mimicking the function part of this differential equation by substituting the empirical counterparts of the transform, and replacing unknown parameters by their maximum likelihood estimators obtained in the case that the compound Poisson exponential model applies, we obtain an expression the weighted integral of the square of which is used as test statistic. We present two variants of the latter; one of which is invariant with respect to scale transformations of the ====-part by fixed positive constants. Handleable alternative representations of the test statistics useful for practical purposes are available. There are also simple expressions of the maximum likelihood estimators, that, in fact, are the method of moments estimators. The limit null distributions of the test statistics are derived. They turn out to be the distributions of weighted integrals of squared zero mean Gaussian processes. The covariance functions of these processes are explicitly given. As these distributions depend on unknown parameters, we suggest a parametric bootstrap procedure to get critical values. Section 3 deals with the necessary theoretical details to see that the bootstrap procedure works. As a main tool, a central limit theorem for triangular schemes of Hilbert space valued random variables is used. The consistency of the testing procedures is proved in Section 4. The result of a simulation study on the power performance of the tests is presented in Section 5. Real data examples dealing with the question that the compound Poisson exponential model applies to certain rainfall data or losses of a motor third-party liability insurance company is discussed in Section 6. A multivariate extension coming with a random vector ==== is considered in Section 7.2. The ====, ====, may be the possibly dependent total loss expenditures and the numbers of losses associated with the ==== risks held by an insurer during a certain time period in actuarial science, and it may be of interest testing simultaneously the hypothesis of independence of the ====, ====, and the hypotheses that the compound Poisson exponential model applies to ====, ====.",A goodness-of-fit test for the compound Poisson exponential model,https://www.sciencedirect.com/science/article/pii/S0047259X22001452,30 December 2022,2022,Research Article,31.0
Chu Ba,"Department of Economics, Carleton University, B-857 Loeb Building, 1125 Colonel By Drive, Ottawa, ON K1S 5B6, Canada","Received 20 April 2021, Revised 15 December 2022, Accepted 17 December 2022, Available online 28 December 2022, Version of Record 9 January 2023.",https://doi.org/10.1016/j.jmva.2022.105151,Cited by (0),We contribute to recent research on ,"Distance correlation, first proposed by Feuerverger [15] (for univariate random variables), Bakirov et al. [3] and Székely et al. [46] (for multivariate random variables) as a new measure of dependence between two vectors of random variables, has now become standard in econometrics and statistics. Unlike the standard correlation coefficient, the distance correlation takes value of zero only if two random vectors are independent. The empirical distance correlation is based on the sample covariance between the centered Euclidean distances of pairs of data points from each random vector, thus it has a compact representation that is very easy to calculate. There has been an increasing number of applications of distance correlation in many fields. Interested readers may refer to Edelmann et al. [13] and references therein for an updated literature review on this topic. Work on distance correlation for time series is quite recent. Distance correlation has been extended to measure and test for (nonlinear) serial dependence (see Zhou [52], Davis et al. [10], Fokianos and Pitsillou [16]).====This paper contributes to the growing literature of distance correlation by introducing a new pivotal distance-based test of independence between two weakly dependent, stationary multivariate time series. We employ a generalized spectral density approach similar to Hong [26] to construct our Portmanteau-type test statistic. This statistic can then be written as a summation of kernel-weighted empirical cross-covariances (at a very large number of leads and lags) between double-centered ====-powered Euclidean distances of the realizations of each multivariate process at two different points in time (====, the empirical double-centered distance cross-covariances as defined by (4) in the main text). The population versions of these centered distance cross-covariances equal zero only in the independence case. We show that the proposed test statistic is asymptotically normal under the null hypothesis of independence (thus, it is pivotal), and it is consistent under fixed alternative hypotheses of dependence. We also propose a new wild bootstrap procedure that can generally improve the finite-sample performance of our pivotal test statistic in many cases. An important implication of bootstrapping a pivotal test statistic is that many bootstrap procedures can provide approximations that are more accurate than first-order asymptotic approximations under certain regularity conditions (see, e.g., MacKinnon [33]).====The derivation of our test statistic is based on an integral formula (cf. Lemma 1) arising from the theory of distance correlation (see, e.g., Székely et al. [46], Székely and Rizzo [43], Dueck et al. [12]), which effectively leads to a parsimonious form for the proposed test statistic. Therefore, our proposed test statistic is closely related to the squared empirical distance cross-covariance for randomly sampled data, which has been studied in, ====, Székely et al. [46] and Székely and Rizzo [43], [44]. Thus, this statistic can effectively be viewed as an extension of the empirical distance cross-covariance for independent data to time series data by combining the approaches of Hong [25] and Székely et al. [46].====Existing tests for independence between two multivariate time series are based on the residuals of parametric data generating processes (DGPs): when two time series are generated by parametric processes initialized at two fixed or independent random points, independence between these two series is equivalent to that between their respective innovation processes. Therefore, a practical statistic for testing independence should be based on some measure of dependence between two series of residuals at multiple leads and lags. Tests for non-correlation between two multivariate time series are particularly extensive, including (among others): El Himdi and Roy [14] (which extends Haugh’s [1976] test to multivariate ARMA processes); Bouhaddioui and Roy [5] (which extends Hong’s [1996] test to infinite-order vector autoregressive processes); Hallin and Saidi [20] (which extends Koch and Yang’s [1986] approach for testing non-correlation between two multivariate ARMA processes); Tchahou and Duchesne [47] (which tests for causality in variance between two multivariate time series by checking for the absence of cross-correlations between the squared residuals in these models, effectively extending the procedure developed by Cheung and Ng [8]); Robbins and Fisher’s [2015] test based on block Toeplitz matrices containing autocorrelations and cross-correlations of the residuals from two multivariate ARMA processes. A recent test for independence between two multivariate time series is proposed by Wang et al. [49]; this procedure is based on Box–Pierce-type statistics that employ the Hilbert–Schmidt independence criterion (HSIC) introduced by Gretton et al. [19] to measure dependence between two sequences of residuals. Wang et al.’s [2021] test statistics are not pivotal, and a residual-based bootstrap procedure is then employed to approximate the critical values of the statistics. Note that both the HSIC and distance covariance use some distance kernel in their definitions. However, the kernel used in the distance correlation method is not smooth and thus makes it more challenging to study (see Sejdinovic et al. [42] for a comparative analysis of the HSIC and distance covariance).====There are three important features that distinguish our tests from all the other tests: First, the other tests are residual-based tests, thus they can have significant size/power distortions if the DGPs used to calculate residuals are misspecified. Meanwhile, our test is not a residual-based test, and we show that it can be robustified by using a machine learning method (e.g., Random Forest) to estimate the centering terms in the double-centered ====-powered Euclidean distances. Second, our test is much faster to implement than the bootstrap HSIC-based test (i.e., the execution time is 4.15 min for our test robustified with Random Forest versus 110.815 min for the HSIC-based test applied to a sample of over 250 observations on an Intel®Xeon®E5-2699v4 processor-based workstation). Third, in the presence of model misspecification, our test performs well under all hypotheses considered in the simulation study while other tests may not.====The layout of this paper is as follows. Section 2 provides a new pivotal test statistic and its wild bootstrap version to test the null hypothesis of independence between two weakly dependent, strictly stationary multivariate time series. Asymptotic properties of the proposed test statistic and its required regularity conditions are discussed in Section 3. A Monte Carlo simulation study provided in Section 4 shows that (1) the proposed procedure has a very good finite-sample performance and (2) it can perform better than other approaches, especially when the DGPs are misspecified. Section 5 applies our test to verify the relationship between stock returns and bond returns. Section 6 concludes this paper. To keep the paper concise, we shall collect results of technical flavor, lengthy mathematical derivations, examples, and further Monte Carlo simulations as supplemental materials (SM).",A distance-based test of independence between two multivariate time series,https://www.sciencedirect.com/science/article/pii/S0047259X22001427,28 December 2022,2022,Research Article,32.0
"Karling Maicon J.,Lopes Sílvia R.C.,de Souza Roberto M.","Mathematics and Statistics Institute, Federal University of Rio Grande do Sul, 91500-900 Porto Alegre, RS, Brazil,Dean’s Office of Research and Graduate Studies, Federal Technology University of Paraná, 80230-000 Curitiba, PR, Brazil","Received 11 May 2022, Revised 20 December 2022, Accepted 21 December 2022, Available online 26 December 2022, Version of Record 31 December 2022.",https://doi.org/10.1016/j.jmva.2022.105153,Cited by (1),"Measuring the dependency between lagged random vectors in time series is extremely important to detect appropriate models for fitting real data. Vector autoregressive processes (VAR), in particular, are among the most popular multivariate time series that allow modeling schemes for two or more random variables. However, if the series of innovations process does not have finite second-order moments, the classical analysis that uses the empirical auto-covariance function is not adequate as its theoretical counterpart is not defined. After perceiving the lack of a suitable multivariate version of the codifference function, we propose an adaptation of this tool to measure interdependence in multivariate ====-stable processes. We establish such a function based on notions presented in the existing literature. We also show some of its properties and give an estimator based on the empirical characteristic function. Subsequently, we evaluate another measure of dependence with different properties but similar usage for comparison reasons. These two measures are computed for VAR(1) processes with ====-stable innovations, proving to be useful in the identification of associated patterns related to them. Finally, we demonstrate how these measures can be applied with real data sets by analyzing the monthly number of patients with community-acquired pneumonia and the registered values of PM10 in the city of São Paulo, Brazil, from January 2008 to December 2021.","Since the first studies regarding univariate ====-stable distributions, its theory has gained the attention of diverse practitioners due to its applicability and practical relationship with real data sets. For instance, in the modeling of economics, biological, and physical systems (see, e.g., Achar et al. [3], Achar and Lopes [2], Crato et al. [14], Medino et al. [34], and references therein). Likewise, multivariate distributions, in general, are being more studied. Since the introduction of multi-task processors, statistical calculations have become more feasible. In particular, the computational analysis and multivariate design with ====-stable random vectors have been displaying exceptional features in various problems. Two decades ago, research involving ====-stable distributions on the univariate and multivariate cases was limited mainly by their lack of closed probability density functions and finite second moments. Nowadays this has become more tangible.====Vector autoregressive processes of order ==== (VAR(====), or, more shortly, VAR), in like manner, have appeared under various circumstances to model multivariate data sets. They are commonly defined as the stochastic processes ==== in ==== which solve the recurrence relation, for ====, ====where ==== are ==== real-valued matrices and ==== is the sequence of innovations, i.e., ====-variate random vectors with independent and identically distributed (i.i.d.) components. In the literature (see, e.g., Brockwell and Davis [8]), it is possible to find a wide range of its applications. Frequently, the innovations’ distribution is assumed to have finite second moments. For instance, concerning Bayesian methods, Ni and Sun [37] examined the effect of using competing priors and estimates for VAR models under various assumptions, including ====-Student and Gaussian innovations. Another example can be found in the practical study of Raffee et al. [54], where the authors’ analysis suggests that the VAR(1) is the most appropriate model for forecasting ==== concentrations in four monitoring stations in Malaysia.====Nonetheless, in the presence of impulsive volatility or heavy tails, finite second moments distributions might not capture all the information present in the sample under consideration. In that event, the class of ====-stable distributions has proved itself reliable to model this kind of phenomenon (see, e.g., Nolan [39] and Samorodnitsky and Taqqu [60]).====The present work aims to study the particular case of the VAR(====) stationary processes with ====-stable innovations, which, as it is well known, do not have finite second moments when ====. In Brockwell and Mitchell [9] and Peiris and Thavaneswaran [51], the authors gave suitable conditions to ensure the existence and uniqueness of valid solutions for (1), under the hypothesis of symmetric ====-stable (S====S) innovations. Moreover, they provided the background for the prediction problem. We use some of their results to study two different dependence measures, detect the order of raw data, and see if it attends to the specific conditions of a VAR(1) model.====In summary, this work is organized as follows. In Section 2 we recall the definitions and some properties of multivariate ====-stable distributions. Besides, we introduce a novel adaptation to estimate the discrete spectral measure of bivariate ====-stable distributions, proposed by Nolan et al. [42], in combination with the Bayesian approach presented in the author’s precedent work, Karling et al. [26]. Afterward, we dedicate Section 3 to a study of alternative dependence measures to the auto-covariance function. More specifically, we propose a measure of dependence for multivariate ====-stable random vectors, calling it the generalized multivariate codifference function (gmcf), an extended version of the generalized codifference function for univariate ====-stable distributions, presented in Kokoszka and Taqqu [29]. An alternative measure of dependence, namely, the spectral covariance, introduced in Paulauskas [48], is also investigated, and a novel estimation procedure is proposed for the VAR(1) process case. Our primary goal is to give a criterion that allows one to identify whether, or not, a VAR(====) process has order ==== if the innovations are multivariate ====-stable. In Section 4 we specify the concept of VAR(1) processes with ====-stable innovations used throughout this work and discuss criteria presented in the literature for the existence of stationary solutions. Furthermore, we present a simulation study to evaluate the asymptotics of the codifference marginals when bivariate VAR(1) processes with S====S innovations are considered. A similar study is conducted when the empirical spectral covariance is under analysis. To demonstrate how the gmcf and the spectral covariance can be used with real data sets, in Section 5 we analyze the monthly number of patients with community-acquired pneumonia and the registered values of PM10 in the city of São Paulo, Brazil, from January 2008 to December 2021. Finally, Section 6 concludes the manuscript.",Multivariate ,https://www.sciencedirect.com/science/article/pii/S0047259X22001440,26 December 2022,2022,Research Article,33.0
Ogasawara Haruhiko,"Otaru University of Commerce, 3-5-21, Midori, Otaru 047-8501, Japan","Received 5 September 2022, Revised 20 December 2022, Accepted 20 December 2022, Available online 23 December 2022, Version of Record 27 December 2022.",https://doi.org/10.1016/j.jmva.2022.105152,Cited by (0),"The ==== of the sample variance-ratios and ==== with possible truncation under multivariate elliptical symmetry is unchanged irrespective of distinct elliptical distributions. A general condition for transformed sample variances and covariances to have an unchanged pdf under elliptical symmetry is given. Examples satisfying this condition are shown for the intraclass correlation, coefficient alpha and principal component analysis.","The distribution of the sample correlation coefficient denoted by ==== under bivariate normality was given by Fisher [9, p. 516] using a differential expression with the bivariate Wishart density. Currently, there are many expressions of the distribution (see [17, Chapter 32, Section 2]). It has been known that the distribution of the sample correlation matrix under null multivariate normality or uncorrelated normality is robust under elliptical symmetry, i.e., the elliptically contoured distribution [19, Theorem 5.1.3]. Fisher [10] discussed the joint distribution of the sample correlation matrix under non-null normality especially under the trivariate case. Anderson and Fang [3, Theorem 4], and Ali and Joarder [1] showed that the distribution of the sample correlation matrix under non-null normality also holds under elliptical symmetry. Joarder and Ali [16, Theorem 3.1] gave the probability density function (pdf) of the sample correlation matrix under non-null normality or elliptical symmetry.====When a single Wishart observation based on ==== independent normal vectors is truncated due to, e.g., the high or low values of the associated sample variances, the distribution of ==== was derived by Ogasawara [25], which does not hold under elliptical symmetry since the distribution of the sample variances depends on the fourth cumulants of the associated ==== random vectors. However, Ogasawara [25] also showed that the null distribution of the sample correlation matrix under normality with truncated associated sample-variances is equal to that without truncation and consequently holds under null elliptical symmetry.====Recently, Joarder [15, Theorem 5.1] showed that the distribution of the sample variance-ratio is the same over the distributions under non-null bivariate elliptical symmetry. Note that there are two sample variances for a single correlation coefficient while we have a single variance-ratio or its reciprocal. Consequently, it is convenient to deal with the variance-ratio rather than two sample variances for truncation of Wishart observations.====Omar, Joarder and Riaz [27, Section 7] gave applications of the variance-ratio in quality control. Note that the variance-ratio in their applications is seen as a risk index found in industry. That is, products with the high value of the ratio in some lots are excluded. This type of process monitoring with a variance-ratio being a risk index can also be found in other fields. For instance, in health sciences, it is known that patients with high values of the variation of the blood pressure in a circadian rhythm need medical care or treatment even when their average values are normal whereas recovering patients with the normal averages and variances are excluded from medical care. It is found that the roles of truncation and inclusion in the examples of industry and medicine are reversed.====The first purpose of this paper is to derive the distribution of ==== under bivariate elliptical symmetry when the sample variance-ratio is truncated in various ways. The second purpose is to have the joint distribution of the sample variance-ratios and correlation matrix under multivariate elliptical symmetry in the untruncated case. The third purpose is to obtain a general condition for transformed sample variances and covariances to have an unchanged pdf under elliptical symmetry.====The remainder of this paper is organized as follows. In Section 2, the result of the first purpose is given using the weighted negative binomial expansion with some conditions for the pdf’s under truncation being unchanged from the untruncated case. General formulas for moments are also given. In Section 3, numerical illustrations of the moments of the distributions including simulations with the truncated variance-ratio are shown. Based on the results in Sections 2 The density of the sample correlation coefficient with the truncated variance-ratio under elliptical symmetry, 3 Numerical illustrations of the moments of the sample correlation coefficient with the truncated variance-ratio, a conjecture for the asymptotic moments for the truncated moments is presented in Section 4. Section 5 gives the results for the second and third purposes, where as examples of the functions of variance-ratios or correlations, the distributions of the sample intraclass correlation  [5] and coefficient alpha [21]; and the asymptotic covariance of two sample correlations under elliptical symmetry are shown. An example of principal component analysis satisfying a general condition for an unchanged pdf under elliptical symmetry is also given. Section 6 concludes with some associated issues. Proofs of a lemma, theorems and a corollary when necessary are given in the appendix. In the supplement to this paper [26], the expressions of the pdf’s of the untruncated correlation coefficients are shown when increasing the number of variables successively.",The density of the sample correlations under elliptical symmetry with or without the truncated variance-ratio,https://www.sciencedirect.com/science/article/pii/S0047259X22001439,23 December 2022,2022,Research Article,34.0
"Zhang Wei,Gao Wei,Ng Hon Keung Tony","School of Mathematics and Statistics & KLAS, Northeast Normal University, Changchun, Jilin 130024, China,Department of Mathematical Sciences, Bentley University, Waltham, MA 02452, USA","Received 14 January 2022, Revised 8 December 2022, Accepted 10 December 2022, Available online 20 December 2022, Version of Record 26 December 2022.",https://doi.org/10.1016/j.jmva.2022.105144,Cited by (0),"In this paper, we propose a new class of independence measures based on the maximum mean discrepancy (MMD) in Reproducing Kernel ","Testing the independence of two random vectors is a classic and important statistical problem that has numerous significant applications such as clustering [45], dimension reduction [5], [10], [31], feature screening [23]. There are many dependence measures and statistical independence tests for different kinds of variables. For example, the Pearson correlation measures the linear dependence between two univariate continuous random variables. The Spearman’s correlation measures the dependence under the assumption of monotonic correlation [36]. The Wilks’ lambda is a likelihood ratio statistic based on the normal theory that measures the differences between the means of different groups. However, the Wilks’ lambda may not be valid when the sample size is smaller than the dimension of the data or when the normality assumption is violated [49]. Some nonparametric competitors of the Wilks’ lambda presented in [14], [19], [46], [47] can be used to test the independence of two random vectors when these random vectors follow elliptically symmetric distributions, but, once again, these tests may not be applicable when the normality or ellipticity distributional assumptions are violated or when the sample size is smaller than the dimension of the data. Therefore, it is desired to develop independent tests for high-dimensional data that follows an arbitrary distribution.====For high-dimensional data, different kernel-based methods and the distance covariance (DCOV) methods have been proposed. Kernel-based methods are derived from the machine learning literature which have many statistical applications such as classification [9], clustering [4], regression modeling [13], [34], [42], and two-sample testing [15]. Different kernel-based methods have been used for detecting independence for high-dimensional problems since they do not require strict conditions on the relationship between the dimension of the data and the sample size, and they do not require parametric assumptions for the two random vectors. Among those kernel-based methods, the kernel canonical correlation is based on canonical correlations in a reproducing kernel Hilbert space (RKHS) [1] which was proposed by Bach and Jordan [2]. Bach and Jordan [2] used a regularized correlation operator derived from the covariance and cross-covariance operators as a test statistic for the test of independence. By using the largest singular value of the cross-covariance operator, a criterion for dependence measurement called constrained covariance (COCO) based on RKHS is proposed by Gretton et al. [18]. An independence measure called Hilbert–Schmidt independence criterion (HSIC) was constructed by utilizing the sum of the squared singular values of the cross-covariance operator in RKHS [16]. The HISC has been extended for testing conditional independence [11], [12], [52], for detecting independence under large-scale data sets [27], [51], for dimension reduction [11], and for testing joint independence [26]. A class of independence measures based on the characteristic kernel was proposed by Chen et al. [5]. A special case of the HSIC is the DCOV which is based on the discrepancy between the joint characteristic function and the product of the two marginal characteristic functions [29], [44]. Recently, Ke and Yin [21] proposed the expected conditional characteristic function-based measures independence criterion (ECCFIC). Due to the “curse of dimensionality” in the estimation process [22], [28], the test of independence proposed by Ke and Yin [21] may not perform well for high-dimensional data. Therefore, we aim to propose an approach that can avoid this issue. In general, the proposed approach shares the advantages of some existing RKHS-based measures that they do not require strong assumptions on the data structure, including the underlying probability distribution and the dimensionality of the data. Recently, the maximum mean discrepancy (MMD) has been studied in [15], [35], [38], [39]. The MMD is a special case of an integral probability metric (IPM) [24]. The IPMs have been applied in different aspects such as proving the central limit theorems using Stein’s method [3] and proving the empirical process theory [48]. Most of the applications require MMD to be a metric on a space of all Borel probability measures ====. By selecting the set of functions ====, different distance measures on ==== can be obtained. For example, the total variation distance [32], the Wasserstein distance [8], the Dudley metric [32], and the Kolmogorov distance [32] are MMD with difference choices of ====.====In this article, we combine the ideas of MMD and RKHS to construct a novel class of independence measures for two random vectors. We obtain an independence measure based on the discrepancy between the joint distribution and the product of the two marginal distributions. Specifically, we obtain a new class of independence measures by creating a class of metrics on ====. While HSIC is based on a specific metric on ====, the approach proposed in this manuscript can be based on different metrics on ==== and provide more flexibility. Since HSIC is a particular case of the proposed method, the proposed method will be at least as good as the HSIC and probably will provide better performance in some situations. One of the advantages of the proposed measures is that they do not require strict assumptions about the data structure and they can identify nonlinear the relation between two random vectors. We show that the asymptotic properties of the proposed empirical independence measures are not related to the dimension of the data. Therefore, even when the dimension of the data (====) exceeds the sample size (====) (i.e., ====), the proposed measures are applicable and they work reasonably well compared to the other measures based on RKHS.====The rest of the paper is organized as follows. In Section 2, we propose a new class of independence measures based on the MMD, and compare it with those existing RKHS-based methods. In Section 3, we develop the corresponding empirical version (i.e., estimators) of the proposed independence measures by considering two special cases under normal and Student ====-distributions. The asymptotic properties of these estimators are also discussed in Section 3. In Section 4, Monte Carlo simulation studies are used to evaluate the performance of the proposed empirical measures for testing the independence of two random vectors and compare with several existing procedures under different settings. In Section 5, we summarize the article by providing some concluding remarks. The proofs of the theoretical results are provided in the Appendix.",Multivariate tests of independence based on a new class of measures of independence in Reproducing Kernel Hilbert Space,https://www.sciencedirect.com/science/article/pii/S0047259X2200135X,20 December 2022,2022,Research Article,35.0
"Soukarieh Inass,Bouzebda Salim","Université de technologie de Compiègne, LMAC (Laboratory of Applied Mathematics of Compiègne), CS 60 319 - 60 203 Compiègne Cedex, France","Received 15 February 2022, Revised 8 December 2022, Accepted 8 December 2022, Available online 10 December 2022, Version of Record 14 December 2022.",https://doi.org/10.1016/j.jmva.2022.105143,Cited by (5),"In this paper, we investigate the uniform limit theory for a ====-statistic of increasing degree, also called an infinite-degree ====-statistic. Infinite-degree ====-statistics (IDUS) (or infinite-order ====-statistics (IOUS)) are useful tool for constructing simultaneous prediction intervals that quantify the uncertainty of several methods such as subbagging and random forests. The stochastic process based on collections of ====-statistics is referred to as a ====-process, and if the ====-statistic is of infinite-degree, we have an infinite-degree ====-process. The results obtained in this paper are, to our knowledge, the first known results on the infinite-degree ====-process in the Markovian setting.","Let ==== be an homogeneous Markov chain, defined on a measurable space ====, where ==== is a separable ====-algebra. The infinite or increasing degree ====-statistics can be defined as: ====where ==== is the set of all ====-tuples of different integers between ==== and ====. Common examples of estimators for a finite degree are the empirical variance, Gini’s mean difference, or Kendall’s rank correlation coefficient, while a classical test based on a ====-statistic is Wilcoxon’s signed rank test for the hypothesis of the location at zero (see, e.g., [90], Example 12.4). [45], [54], [66], who provided (amongst others) the first asymptotic results for the case that the underlying random variables are independent and identically distributed. ====-statistics of infinite-order (IOUS). Frees uses the term infinite order U-statistics (IOUS), and Heilig and Nolan used the term infinite degree U-statistics (IDUS) in keeping with Hoeffding’s original use of the terms degree and order, so the terms IOUS and IDUS indicate the same object. have attracted renewed interest in the recent statistics and machine learning literature in relation to uncertainty quantification for Breiman’s bagging [25] and random forests [26]. In such applications, the tree-based prediction rules can be thought of as ====-statistics with deterministic and random kernels, respectively, and their order corresponds to the sub-sample size of the training data [64]. Statistically, the subsample size ==== used to build each tree needs to increase with the total sample size ==== to produce reliable predictions. As a leading example, we mention the construction of simultaneous prediction intervals for a version of random forests discussed in [64], [88]. We mention that several classes of statistics share commonalities with IDUS, including infinite-degree ====-statistics, partial-sum ====-processes [there are two perspectives on U-processes: (1) they are infinite-dimensional versions of U-statistics (with one kernel); (2) they are stochastic processes that are nonlinear generalizations of empirical processes] symmetric statistics, and elementary symmetric polynomials. The computational intractability of the IDUS is a significant constraint when the sample size and/or order are large. Extensive literature have treated the theory of ====-statistics of fixed order, for instance, see among many others [3], [19], [20], [21], [22], [23], [24], [54], [60], [71], [85], [89]. However, the ====-statistics of infinite degree have been studied for the first time by Frees [41], he called it the infinite order ====-statistics. In his work, he aimed to show if ==== is still an unbiased estimator, even more, a desirable estimator of a parameter of interest, say ====, and what conditions are needed to be satisfied for the asymptotic convergences. Besides, he presented many examples to illustrate the usefulness of the IDUS. The authors of [12], [18], [76] consider statistics based on resampling procedures. Among the procedures summarized in [12], the ==== out of ==== bootstrap is an infinite-degree ====-statistic and the ==== choose ==== bootstrap is an infinite-degree ====-statistic. Historically speaking, [58] generalized the study of [41] by representing the weak convergence of infinite ====-process in the degenerate case. Precisely, he used a technique from his previous work [57]. Kohatsu-Higa in [82] discussed the projection’s conditions of the asymptotic behavior of ====-statistics from the finite degree to the infinite degree. Kohatsu-Higa [58] revisited the results related to those of [37] for symmetric statistics. Heilig and Nolan [51] treated the subject of infinite degree, as well as many previous works for both, like [49], [50]. On the other hand, Shieh in [87] has proposed an analogy between infinite-degree ====-statistics and the IDUS, which may have more efficient calculation techniques than ====-statistics, even though they are not typically unbiased. A class of deterministic weighted ====-statistics was proposed in [74], called quasi ====-statistics. The kernels of this ====-statistics are degenerate of degree two, and then they generalized their investigation to the kernels of degree ==== in [75] and to an infinite order degenerate kernels in [73]. The last cited references’ major concern is the limiting laws’ characterization. But the problem in both cases, finite and infinite order, resides in the fact that the limiting distributions of the ====-processes, or their functionals, are rather complicated, which does not permit explicit computation in practice. To overcome that difficulty, many authors suggested and studied solutions like [2], [46], [59] by investigating what is called bootstrapped ====-statistics. Then using the bootstrap approach, we can simulate the limiting distribution of ====-statistics. Those last references used Efron’s bootstrap developed in [38], which is an important non-parametric technique to solve the problem of complex limiting distribution. Nevertheless, as it is well known, Efron’s technique is assigned to the independent and identically distributed random variable. As a result, different types of processes emerge in response to an expanded number of statistical criteria while adhering to the core principles of the i.i.d. (independent and identically distributed) bootstrap. [40] gives for the first time an extension of Efron’s technique to the dependent framework, followed by [16], [27], [80] who offer different non-parametric and semi-parametric methods or models of bootstrap concerning the dependent data. Over and above, as the Markov chain model is an important tool for statisticians, engineers, data scientists, and econometricians, the investigation of bootstrap in this field is relevant. This investigation can be found in many papers like in [5], [6], [29]. In the dependent setup, the most results of ====-statistics are proved under the mixing conditions; see the work of [14], [15], [30], [31], [32], [55]. However, in the stationary setup, it can be referred to [55], [91]. Note that [88] have investigated an incomplete version of the infinite order ====-statistics with a random kernel. They derived sub-sampling procedures for making inferences on the parameter of interest with a class of high-dimensional random kernels of diverging orders. In our approach, we will use the same strategy as our previous work [24], i.e., no mixing conditions are involved. We are based on the strong property of Markov chains, which allows us to divide the chain into i.i.d. blocks, based on [11]. In addition to the previously mentioned work, [11], the authors in [35], [39], [47] remain within the framework of Markov chain ====-statistics while showing other assumptions and properties concerning this estimator. The primary purpose of the present paper is to generalize the work of ====-statistics and ====-process for the case where the degree ==== converges to infinity or, in other words, increases with the size of the sample in the framework of Markov chains. We generalize the work of [51] to the Harris recurrent Markov chain by using the renewal properties of this chain. In addition, we study the limit theorems of the ====-statistic estimators in a class of functions that keep some properties to find the desired results. We also consider the bootstrapped version of the ====-process, which is of its own interest. The context of the present paper has, to the best of our knowledge, not been considered so far in the literature, which is substantially more complicated than the independence framework. Recall that we use the abbreviations IDUS or IOUS for increasing (or infinite) degree/order ====-statistics.====This paper is organized as follows. In Section 2, we summarize notation, definitions, and properties related to the Markov chain and ====-statistics with a kernel of many arbitrary arguments belonging to a class of functions. Section 3 reviews ====-statistics with infinite degree, including the Hoeffding decomposition. In Section 4, we apply the empirical process technique to the first-order projection for the ====-statistics based on the Harris Markov chain. The core of this paper is the asymptotic theory of IDUS represented in Section 5 and the asymptotic theory of bootstrapped IDUS in Section 6. Then, Section 7 collects some examples and potential applications. Some concluding remarks and possible future developments are relegated to Section 8. Finally, to avoid interrupting the flow of the presentation, all mathematical proofs are presented in Section 9.",Renewal type bootstrap for increasing degree ,https://www.sciencedirect.com/science/article/pii/S0047259X22001348,10 December 2022,2022,Research Article,36.0
"Chen Xiao,Feng Zhenghui,Peng Heng","Department of Mathematics, Hong Kong Baptist University, Hong Kong,School of Science, Harbin Institute of Technology, Shenzhen, 518055, China","Received 11 April 2022, Revised 30 November 2022, Accepted 1 December 2022, Available online 8 December 2022, Version of Record 21 December 2022.",https://doi.org/10.1016/j.jmva.2022.105140,Cited by (0)," is a promising statistical model in investigating the heterogeneity of population. For multivariate non-Gaussian density estimation and approximation, in this paper, we consider to use multivariate exponential power mixture models. We propose the penalized-likelihood method with a generalized ",None,Estimation and order selection for multivariate exponential power mixture models,https://www.sciencedirect.com/science/article/pii/S0047259X22001312,8 December 2022,2022,Research Article,37.0
"Liebscher Eckhard,Okhrin Ostap","University of Applied Sciences Merseburg, Department of Engineering and Natural Sciences, Merseburg, Germany,Chair of Econometrics and Statistics esp. Transportation, Technische Universität Dresden, D-01062 Dresden, Germany","Received 23 March 2022, Revised 2 December 2022, Accepted 3 December 2022, Available online 7 December 2022, Version of Record 14 December 2022.",https://doi.org/10.1016/j.jmva.2022.105142,Cited by (0),. An extensive simulation study has supported the theory.,"For many decades, a multivariate normal distribution was a running horse in various empirical applications due to its simplicity and support from the central limit theorem. Nevertheless, although compelling, this is just one distribution that does not always meet practitioners’ needs. A larger family, to which also belong the multivariate normal one, is the elliptical distribution, see [1], [3], [10], [11]. The whole multivariate elliptical distribution has thus much more flexibility, as it is driven not only by the covariance (or scaling) matrix and the location but also by the density generator function ==== which dictates the overall shape of the distribution. Consider a specific example from economics, e.g., portfolio allocation under specific risk bounds. It requires a good knowledge of the joint distribution of the involved risk factors. For multivariate normal distributions, the mean–variance optimal portfolio is a simple function of just the risk factors’ covariance. For huge portfolios, the dimensionality in relation to the available sample size becomes a quest for the precision of the tail distribution, and it is where the elliptical distribution becomes handy. Thus, one needs to treat the density generator function: estimate or fix its parametric form. We again come down to one specific distribution with the fixed parametric form. Thus, we aim to estimate the generator function nonparametrically, making the complete estimation procedure semiparametric because there is still an assumption of ellipticity and the scale and location parameters need to be estimated. For several important works that provide a nonparametric estimation of the density generator, we refer to [7], [17]. Both papers consider only fixed dimension, where [17] uses the transformation of the data before applying the nonparametric estimator. Unfortunately, theorems from [17] do not completely cover the cases when the dimension increases. To the best of our knowledge, no research considers the case where dimensionality tends to infinity with the sample size, even being smaller than the latter. This paper derives a semiparametric estimator of the multivariate elliptical density and provides an in-depth analysis of its asymptotic properties. As an important product, we derived the convergence rates of the sample covariance matrix in the high-dimensional setup and under the Frobenius norm. Our work can be extended to other estimators of the covariance matrix, as one based on the approximate factor models by [8], [9], using shrinkage estimation by [4], [16], those based on the distance-based classifiers as in [2] or with noise reduction with geometric representations as [23], mainly for the cases where the dimension is larger than the sample size, but we keep it for further research.====The paper is structured as follows. The following section defines spherical and elliptical distributions and the estimation procedure. Section 3 provides the main theorems of the convergence of the generator function and the complete multivariate elliptical density. Section 4 shows almost sure convergence of the sample covariance and Tyler’s M-estimator with the corresponding convergence rates. Section 5 delivers an extensive simulation study that supports the theory. Section 6 contains all the proofs of the theorems.",Semiparametric estimation of the high-dimensional elliptical distribution,https://www.sciencedirect.com/science/article/pii/S0047259X22001336,7 December 2022,2022,Research Article,38.0
"Giesen Joachim,Kahlmeyer Paul,Laue Sören,Mitterreiter Matthias,Nussbaum Frank,Staudt Christoph","Friedrich-Schiller-Universität, Jena, Germany,Data Assessment Solutions GmbH, Hannover, Germany,DLR Institut für Datenwissenschaften, Jena, Germany","Received 3 November 2021, Revised 30 November 2022, Accepted 1 December 2022, Available online 6 December 2022, Version of Record 16 December 2022.",https://doi.org/10.1016/j.jmva.2022.105141,Cited by (0),"In recent years, there has been significant progress in unsupervised ====. Classical statistical and machine learning techniques can be used on these representations for downstream tasks and applications. Here, we consider an unsupervised downstream task, namely, topic modeling with mixed membership models. Prototypical mixed membership models like the latent Dirichlet allocation (LDA) topic model use only simple discrete observed features. However, state-of-the-art representations of images, text, and other modalities are given by continuous feature vectors. Therefore, we study mixed membership Gaussians that operate on continuous feature vectors. We prove that the parameters of this model can be learned efficiently and effectively by Pearson’s method of moments and corroborate our theoretical findings on synthetic data. Experiments on standard labeled image data sets show that mixed membership Gaussians on state-of-the-art image representations yield topics that are semantically meaningful, coherent, and cover the labels that have not been used during training well.","Topic models are characterized by latent class variables that represent the different topics. By assigning topics to data points, topic models can be used for (self-)labeling data. This assignment of labels to feature vectors is typically addressed as a clustering problem, see [5] and references therein. One popular choice in this context is ====-means clustering [5], [11], which is closely related to Gaussian mixture models. These methods assign only a single label to a given feature vector. However, real-world data is often better modeled by admixture models, where each observed feature vector is itself a mixture of topics.====Traditional admixture topic models like latent Dirichlet allocation (LDA) [8] are concerned with the text modality, where text documents are represented by discrete feature vectors (word count vectors). Here, we focus on LDA-like admixture topic models that use continuous feature vectors. These feature vectors can, for instance, be derived from self-supervised representation learning [22], [34]. This approach allows representing most types of data effectively using continuous representations, in particular, also text data [32].====In our admixture model, each observation is a mixture of Gaussians and, as in LDA, the mixing proportions are drawn from a Dirichlet distribution. We call the resulting model Gaussian mixed membership model, or short mixed membership Gaussian. Mixed membership Gaussians differ from classical Gaussian mixture models, where the mixing proportions are fixed and each individual observation is from a single Gaussian component. Thus, the mixture in Gaussian mixture models is, in contrast to mixed membership Gaussians, only over several observations.====Learning the parameters of a mixed membership Gaussian is not straightforward since, like in other latent-class models, the likelihood function is non-concave. Spectral methods are a viable alternative to the likelihood approach for estimating parameters of latent-class models [4], [33]. Many of the spectral approaches are provably efficient and effective, that is, they have polynomial running times and come with statistical consistency guarantees. One such approach is Pearson’s method of moments [28] that in many cases boils down to computing low-rank matrix and low-rank orthogonal tensor decompositions [3], [21].====In this work, we derive the method-of-moments system of equations for mixed membership Gaussians and show that it is solvable by the low-rank decomposition techniques presented in [3]. The resulting algorithm has been presented in our recent conference article [15]. Here, we specialize the algorithm to mixed membership Gaussians and significantly extend [15] by proving algorithmic as well as statistical guarantees for the solution. Such a combination of guarantees is not known for the maximum likelihood approach. We validate our theoretical results with experiments on synthetic data. Finally, we demonstrate that mixed membership Gaussians provide high-quality topics on state-of-the-art image features that we compute by the Siamese network that was introduced in [12] for unsupervised representation learning.",Mixed membership Gaussians,https://www.sciencedirect.com/science/article/pii/S0047259X22001324,6 December 2022,2022,Research Article,39.0
"Bai Yansong,Zhang Yong,Liu Congmin","School of Mathematics, Jilin University, Changchun 130012, China","Received 30 April 2022, Revised 26 November 2022, Accepted 27 November 2022, Available online 30 November 2022, Version of Record 11 December 2022.",https://doi.org/10.1016/j.jmva.2022.105139,Cited by (0),"Consider a multivariate linear regression model where the sample size is ==== and the dimensions of the predictors and the responses are ==== and ==== are fixed, the limiting distribution of the LRT is a ==== distribution. However, in the high-dimensional setting, the ==== approximation to the LRT may be invalid. In this paper, based on He et al. (2021), we give the moderate deviation principle (MDP) results for the LRT in a high dimensional setting, where the dimension parameters ==== are allowed to increase with the sample size ====. The performance of the numerical simulation confirms our results.","In statistics, the study of LRT statistics has become mainstream. For example, [18] applied the LRT of a multi-sample mixture model to genetic imprinting. A generalized quasi-LRT with applications to opioid agonist treatment was proposed by [23]. The authors of [11] used the LRT they proposed to analyzed the breast cancer dataset. Apart from this, we found that applications of the LRT include but are not limited to testing parameter hypotheses, assessing the fit of statistical models, and constructing confidence intervals/regions for the parameters of interest. The main purpose of this paper is to test the structure of the regression coefficient matrix under the multivariate linear regression model using the LRT statistic.====Suppose we have ==== observations of ====-dimensional responses ==== and ====-dimensional predictors ====, for ====. Let ==== be the ==== responses matrix, and ==== be the ==== design matrix. Assume a multivariate linear regression model ====, where ==== is a ==== matrix of unknown regression parameters, and ==== is an ==== matrix of regression errors, where ==== is independently sampled from distribution ====.====Under the multivariate linear regression model, we are interested in testing the null hypothesis ====where ==== is an ==== matrix with rank ====, and ==== is an all-zero matrix. (1) is often referred to as the general linear hypothesis in multivariate analyses and is widely used in multivariate analysis of variance (see, e.g., Section 10.2 in [19]). The testing formulation incorporates many hypotheses of interest with various choices for matrix ====. Now, we will give some examples for the test (1).====It is very popular to test (1) by using LRT. When ====, ==== is positive definite, and ==== has rank ====, then the LRT statistic is ====where ====Here ==== and ==== are the residual sum of squares and the regression sum of squares matrices, respectively, and ==== is the least squares estimator. It is worth noting that in the case ====, the LRT may be difficult to define. The reason is that in this case, ==== is a singular matrix, this excludes the LRT from many ==== or ==== high-dimensional applications. Studies on ==== or ==== include but are not limited to [6], [12], [21], [28]. In this paper, our study is conducted with ====. Assume that the dimension parameters ==== are considered to be fixed, then under ==== in (1), a classical asymptotic result shows that ====where ==== denotes the convergence in distribution, see [1], [19].====However, with the advent of the information age, the scale of data we are exposed to has gradually increased, and multivariate linear regressions are gradually applied to data with high-dimensional characteristics, such as financial data, genetic engineering data and multimedia data all have this feature. Relevant examples can be found in [16]. Unfortunately, in the high-dimensional setting (allowing the dimension parameters ==== to increase as ==== increases), the limiting distribution of ==== is no longer a ==== distribution under the null hypothesis. The failure of the ==== approximations of the LRT distribution in high dimensions has been studied in various model settings. For instance, in the setting of a high-dimensional multivariate linear model, [24] derived an asymptotic expansion of the LRT as the sample size ==== and the dimension ==== tend to infinity. In the work of [27], they gave asymptotic expansion formulas for three test statistics (the LRT statistics, Lawley–Hotelling’s generalized ==== statistics and Barlett–Pillai’s test statistics) when the dimension is large. The authors of [26] derived asymptotic distribution and asymptotic expansion of the Wilks’ lambda statistic in the high-dimensional setting. From the above works of literature, we found that the leading term of the expansion of the LRT is normally distributed. Moreover, a Berry–Esseen bound for a high-dimensional asymptotic approximation of the distribution of the LRT was given in [25]. [3] explained why the ==== approximation fails in high-dimensional data and proposed a modified LRT.====In the other model settings, two LRTs for covariance matrices in the high-dimensional case were discussed in [2], they proved the poor performance of the ==== approximation and thus gave a modified normal limiting distribution. For random samples from a Gaussian random vector, where the sample size is ==== and the dimension is ====. [15] considered the classical LRTs for sample means and covariance matrices when both ==== and ==== are infinity and ====, and found that the LRTs converge in distribution to normal distributions with well-defined means and variances. [13] generalized the results of [15], and their conditions need only be satisfied, for some ====, ==== tends to infinity and ====. The work of [7] extends the preceding results. They proved the central limit theorems (CLT) of two LRTs under the null hypothesis and the alternative hypothesis when the dimension ==== tends to infinity. For a ====-dimensional normal random vector with sample size ====, [20] studied the limiting distribution for the LRT for testing the independence of its grouped components. They proved that the LRT converges weakly to a normal distribution as ==== and presented an adjusted test statistic that has a ==== limit in general. The authors of [10] considered ====-independent random samples from a ====-variate normal distribution. They studied the asymptotic distribution of the LRT for testing for the equality of ==== covariance matrices and derived the CLT for the LRT when either ==== or ==== tends to infinity. The above literature demonstrated that the asymptotic distribution of the LRT satisfying the ==== distribution does not hold as the dimension parameters ==== increase. Based on this, when the dimension parameters ==== grow with ====, under ==== in (1), [11] derived a corrected normal limiting distribution for the LRT statistic by the Gamma function.====From [17], we know that the performance of the test statistic ==== can be measured by a local measure, i.e., for any ====, we have ====So it is natural to think about whether the following equation holds, ====where ==== is a sequence of positive numbers satisfying ==== as ====. We refer to (7) as moderate deviation estimation or MDP in general, as introduced by [8]. Our paper mainly focuses on the MDP results for the LRT to hold when the dimension parameters ==== (which must be smaller than ==== in LRT) are large but not necessarily of the same order as ====. Recent MDP works for the LRT include but is not limited to [5], [14], [22]. The common feature of this literature is that they consider the MDP for the LRT for the mean vector and covariance matrix of high-dimensional normal distributions. We found that the tests they proposed include testing covariance matrices proportional to the identity matrix, testing equality of several covariance matrices, testing specified values for mean vector and covariance matrix, etc. However, in the high-dimensional setting, there is no MDP result for the LRT of the regression parameters matrix of the multivariate linear model. Thus, our paper addresses a gap in this area which is theoretically interesting. Furthermore, we know from (7) that we focus on the decay rate of the probability ==== as ====, which can help us to control the type I error of the hypothesis test (1). According to [9], we know that if the MDP in (7) holds, then we can use the LRT statistic to construct the rejection region, and then the probability related to type I error tends to zero at an exponential rate. Thus, we can reduce the experimental cost by giving the minimum sample size based on the given type I error. Moreover, the numerical results show that the empirical size of the LRT using the rejection region obtained from the MDP results performs better compared to the rejection region based on the usual asymptotic distribution. It means that the MDP result is valuable in applications.====The paper is organized as follows. In Section 2, we introduce our main results with some remarks. In Section 3, we will study our conclusions by numerical simulations. Some technical tools and proofs are postponed to Appendix.",Moderate deviation principle for likelihood ratio test in multivariate linear regression model,https://www.sciencedirect.com/science/article/pii/S0047259X22001300,30 November 2022,2022,Research Article,40.0
Yuasa Ryota,"Institute of Statistical Mathematics, 10-3 Midori-cho, Tachikawa, Tokyo, 190-8562, Japan,Faculty of Economics, University of Tokyo, 7-3-1, Hongo, Bunkyo-ku, Tokyo 113-0033, Japan","Received 1 February 2022, Revised 25 November 2022, Accepted 25 November 2022, Available online 29 November 2022, Version of Record 8 December 2022.",https://doi.org/10.1016/j.jmva.2022.105138,Cited by (0),In the estimation of the mean matrix in a ,"The problem of estimating the mean vector in the multivariate normal distribution has been studied in a decision-theoretic framework since the inadmissibility of the maximum likelihood estimator was established by [7]. This problem has been extended to the estimation of the mean matrix since [1], who provided the empirical Bayes estimator of the mean matrix and showed the minimaxity. For the good account of this problem, see [2], [14].====To explain the problem more specifically, let ==== be ==== random matrix, where row vectors are independent and ====th row vector of ==== follows multivariate normal distribution with mean ==== and ==== positive definite covariance matrix ====. Let ====, then we denote the model ==== and for known covariance ==== consider the problem of estimating ====, where estimator ==== is evaluated with respect to the quadratic loss function ====. The Efron–Morris estimator of ==== is the matricial shrinkage procedure given by ====and the minimaxity was shown by [1]. We need the condition of ==== for the minimaxity. On the other hand, the James–Stein estimator is the scalar shrinkage procedure given by ====and the minimaxity was shown in the case of ==== by [3]. Let ==== be positive eigenvalues of ==== for ====. When ====’s are close each other, the scalar shrinkage is appropriate, because it is reasonable to shrink all the components in ==== with the same function ====. When ====’s are diversified, on the other hand, it is not good to shrink all the components in ==== with the same function, and the matricial shrinkage provides a suitable risk gain. This suggests to consider a combination of the two shrinkage estimators. For example, the convex combination ==== remains minimax for any constant ==== with ====.====In this paper, we consider the weighted shrinkage estimators ====, where ==== is a function of ====. The problem is how to choose ====. Since a constant weight ignores information in data on which estimator is appropriate, we want to derive a random weight which takes the advantages of the James–Stein and Efron–Morris estimators. For deriving the weight function ====, we suggest the two methods. One is a method based on Stein’s unbiased estimation of risk. That is, we derive an unbiased estimator of the risk function of ==== with random weight ==== and find an optimal weight ==== which minimizes the unbiased estimator except the term of the partial derivative of ====. The resulting weight is an increasing function of the ratio of the arithmetic mean to the harmonic mean, given by ====The other is the method based on the empirical Bayes approach. The weight function is provided by the function of the Bayes factor, namely the ratio of the marginal densities. The resulting weight is an increasing function of the arithmetic mean to the geometric mean, given by ====It is interesting to note that ==== and ==== are statistics for testing the sphericity of a covariance matrix and that ==== is the likelihood ratio test statistic. When ====’s are more diversified, ==== and ==== take larger values, which implies that both weights ==== @are closer to one. Then the weighted shrinkage estimators comes closer to the Efron–Morris estimator. When ====’s are close each other, ==== and ==== are close to one, and the weighted shrinkage estimators approach the James–Stein estimator. These observations imply that the suggested weights perform suitably to choose the correct estimators.====An idea of weighted shrinkage estimators has studied in [12] and they considered weighted shrinkage estimator of modified Stein estimator and James–Stein estimator. The weight function given in [12] is a simple function such that ==== where ==== is a constant. Also, in Remark 4.2 in [12], they mentioned the weighted estimator of James–Stein estimator and Efron–Morris estimator and weighted function using ====. However, they have not given any theoretical and numerical results for the weighted estimator of James–Stein estimator and Efron–Morris estimator. We consider two natural derivations of weighted function and give theoretical and numerical results. By these natural derivations, our proposed weight is a little complicated compared to ====. The difficulty arises from the fact that the shrinkage functions do not increase in ====. Thus, we need to evaluate all the terms in the unbiased estimators of the risk functions directly and derive conditions for their minimaxity. We also consider the dominance properties in the case of unknown ====.====The paper is organized as follows: In Section 2, we treat the case of the known covariance matrix and provide the weight function based on Stein’s unbiased estimation of risk and derive the conditions for the minimaxity. In Section 3, we discuss the Bayesian approach. The weight function is derived as the Bayes factor based on the empirical Bayes method, and the resulting empirical Bayes estimator is shown to be minimax. The extension to the case of unknown covariance matrix is studied in Section 4. Some numerical investigations based on simulation experiments are given in Section 5. The paper is concluded with some remarks in Section 6.",Weighted shrinkage estimators of normal mean matrices and dominance properties,https://www.sciencedirect.com/science/article/pii/S0047259X22001294,29 November 2022,2022,Research Article,41.0
"Sheng Tianhong,Li Bing,Solea Eftychia","Department of Statistics, The Pennsylvania State University, 326 Thomas Building, University Park, PA 16802, USA,CREST, ENSAI, Campus de Ker-Lann, rue Blaise Pascal, BP 37203, 35172 Bruz cedex, France","Received 18 October 2020, Revised 11 November 2022, Accepted 12 November 2022, Available online 21 November 2022, Version of Record 11 December 2022.",https://doi.org/10.1016/j.jmva.2022.105129,Cited by (0),We introduce a skewed Gaussian ,"Graphical models are a class of statistical models defined by conditional independence between variables which can be represented by graphs. Among graphical models, the Gaussian graphical model has been one of the most popular in many applied fields, including network analysis [40], image processing [31] and statistical machine learning [22]. Although the Gaussian assumption brings great simplicity to the graphical model, it is severely restrictive for many applied problems. As a result, estimation based on the Gaussian graphical model may be biased. This motivates us to consider a model which assumes that the variables follow a multivariate skewed Gaussian distribution. The skewed Gaussian distribution extends the Gaussian distribution by introducing a parameter that characterizes the direction of skewness and a location parameter. Skewed Gaussian distributions give our model more flexibility while retaining similar mathematical simplicity of the Gaussian distribution.====Let ==== be a ====-dimensional random vector. Let ==== be an undirected graph, where ==== represents the set of ==== nodes and ==== the set of edges describing the relationship among ====. The edge between nodes ==== and ==== is absent if and only if ==== and ==== are conditional independent given all other components of ====. In symbols, ====
 where ==== represents vector ==== with two elements ==== and ==== removed.====Under the Gaussian graphical model, conditional independence is determined solely by the zero entries of the precision matrix. Specifically, if ==== follows a multivariate Gaussian distribution with covariance matrix ====, and if ==== is the precision matrix with ==== as its ====th entry, then ====
 Thus, under the Gaussian distribution assumption, the estimation of ==== is equivalent to identifying the zero entries in the precision matrix ====. There have been many recent developments regarding the efficient estimation of Gaussian graphical model. Penalized maximum likelihood estimation with ====-penalty is considered in [42]. A neighborhood selection scheme using lasso [33] is proposed in [30]. A block interior point procedure is used to maximize the ====-penalized log-likelihood in [6]. A fast blockwise coordinate descent algorithm called graphical lasso is developed for solving the ====-penalized log-likelihood maximization problem efficiently in [14]. A constrained ====-minimization method for inverse covariance matrix estimation is introduced in [10]. Besides ====-penalty, sparsity can also be introduced by other means. For example, the hard thresholding technique is employed in [7], [8]; a Dantzig selector [11] type modification is used to replace the lasso selection by in [41].====Since the Gaussian assumption is restrictive in many applications, various adaptations have been taken to extend the Gaussian graphical model. One of the solutions is to assume a Gaussian copula model, which allows data to be non-Gaussian but requires it to be multivariate Gaussian after marginal monotone transformation, see [19], [27], [28], [37]. Other works on non-Gaussian graphical models include [1], [13], [25], [26], [34], [38]. Although the skewed Gaussian distribution has been proposed by [2] more than three decades ago and its advantages have been well documented [3], [5], [20], this flexible framework has never been adopted in statistical graphical models. Since the skewed Gaussian distribution enjoys several statistical properties parallel to the Gaussian distribution, it is a convenient joint distribution to relax the Gaussian assumption in the Gaussian graphical model. Like the Gaussian graphical models, in the skewed Gaussian graphical models conditional independence is also formulated by a sparse parameter space. This reduces the difficult task of characterizing conditional independence to parametric sparse estimation. Compared with the Gaussian copula graphical model, the skewed Gaussian graphical model allows skewness to occur along an arbitrary direction in the ====-space, whereas the former only allows skewness to occur in the coordinates of ====. Even with this flexibility, the skewed Gaussian graphical model is still a parametric model, which is more parsimonious than the semiparametric copula graphical model.====The idea of our method is outlined as follows. Compared to a Gaussian graphical model, in a skewed Gaussian graphical model there is an additional shape parameter when evaluating conditional independence. Nevertheless, there is still a simple relation between the sparse parameter space and conditional independence: in this case conditional independence is determined by the sparseness in both the precision matrix and the shape parameter. We introduce a penalized likelihood to induce sparsity in both parameters and design its structure in such a way that the precision matrix and the shape parameter are separated, which allows us to efficiently perform the penalized optimization by the alternating direction method of multipliers [17].",On skewed Gaussian graphical models,https://www.sciencedirect.com/science/article/pii/S0047259X22001208,21 November 2022,2022,Research Article,42.0
Fang Jianglin,"School of Computational Science and Electronics, Hunan Institute of Engineering, Xiangtan 411104, Hunan, China","Received 19 April 2022, Revised 12 November 2022, Accepted 12 November 2022, Available online 17 November 2022, Version of Record 28 November 2022.",https://doi.org/10.1016/j.jmva.2022.105128,Cited by (0),"Estimation and variable selection in partially linear models for massive data has been discussed by several authors. However, there does not seem to exist an established procedure for other semiparametric models, such as the semiparametric varying-coefficient linear model, the single index regression model, the partially linear errors-in-variables model, etc. In this paper, we propose a general procedure for variable selection in high-dimensional general semiparametric models by penalized semiparametric estimating equations. Under some ","In the past two decades, revolutions in information technologies have produced many kinds of massive data. It is very challenging to extract useful features from a massive dataset since many statistics are difficult to compute by traditional algorithms or statistical packages when the massive dataset is too large to be stored in primary memory. In recent years, there have been active researches on developing methods to analyze massive data. Lin and Xi [15] developed an aggregated algorithm for estimating equation estimations in massive datasets using a divide-and-conquer strategy; Chen and Xie [2] considered a divide-and-conquer approach for generalized linear models where both the sample size ==== and the number of covariates ==== are large; Song and Liang [18] proposed a split-and-merge Bayesian variable selection method for ultrahigh dimensional linear regression models; Kleiner et al. [1] proposed the bag of little bootstraps approach for massive data; Liang et al. [14] developed a resampling-based method for the big geostatistical data; Schifano et al. [17] presented an online updating strategy for big data arising from online analytical processing; etc.====Variable selection is an interesting problem in statistical analysis. Variable selection for high dimensional regression is often treated with penalized likelihood procedures. The popular penalized likelihood procedures include the LASSO (Tibshirani [20]), the smoothly clipped absolute deviation (SCAD) (Fan and Li [4]), among others. Variable selection procedures for high-dimensional setting, where the dimension ==== of the observations increases with the sample size ====, are given by Fan and Peng [5], Tang and Leng [19], etc. The penalized procedure has been extended to generalized estimating equations through the penalized generalized estimating equations. Fu [8] proposed a generalization of the bridge penalty to generalized estimating equations. Johnson et al. [10] proposed a procedure for variable selection in semiparametric regression models by penalizing appropriate estimating functions. Wang [24] proposed the SCAD-penalized generalized estimating equations procedure in which the number of parameters is allowed to diverge.====In this paper, motivated by the variable selection procedure for estimating equations in Johnson et al. [10], we firstly extend the penalized procedure to general semiparametric models based on penalized semiparametric estimating functions with the case of the data dimension ==== diverging. Johnson et al. [10] proposed a variable selection procedure for estimating equations. However, the combining estimating equation and the penalty function method by Johnson et al. [10] could not be used directly to semi-parametric estimating equations. For example, we consider the partially linear model ====, the semi-parametric estimating functions can be taken by ====. Since the corresponding semi-parametric estimating equations involve the unknown function ==== and ====, thus we need to estimate the function ==== and ==== in advance. We propose a general procedure for variable selection in high-dimensional semiparametric models by penalized semiparametric estimating equations, including partially linear models, semiparametric varying-coefficient linear models, partially linear errors-in-variables models, etc.====Semiparametric models are a useful compromise between parametric and nonparametric models to mitigate the curse of dimensionality but still allow reasonable flexibility to specify functional form; see, e.g., Engle et al. [3]. For massive data setting, Zhao, Cheng and Liu [25] considered a partially linear framework for modelling massive heterogeneous data, and proposed to extract the common feature across all sub-populations while exploring heterogeneity of each sub-population. Lian, Zhao and Lv [13] studied divide-and-conquer methodology for high-dimensional partially linear models, focusing on the estimation and asymptotic distribution of the nonparametric function. Wang, Lian and Liang [21] proposed an additive partially linear framework for modelling massive heterogeneous data. Wang, Zhang and Lian [22] considered divide-and-conquer strategy for partially linear additive models with a high dimensional linear part.====Statistical modeling for massive data has attracted a lot of recent research. However, as far as we are aware, semiparametric inference for massive data, such as the single index regression model, the semiparametric varying-coefficient linear model, the partially linear errors-in-variables model, etc, still remains untouched. Motivated by the divide-and-conquer approach for generalized linear models with massive data in Chen and Xie [2], we consider a split-and-conquer variable selection procedure for high-dimensional general semiparametric models with massive data, focusing on selection consistency and oracle property of the proposed procedure. The main contributions of this paper are:====1. We propose a general procedure for variable selection in high-dimensional semiparametric models by penalized semiparametric estimating equations, including partially linear models, single index regression models, semiparametric varying-coefficient linear models, partially linear errors-in-variables models, etc. Under some regularity conditions, the oracle property is established, which the number of parameters is allowed to diverge.====2. For extraordinarily large data, we propose a split-and-conquer variable selection procedure for general semiparametric models based on penalized semiparametirc estimating functions. Under some regularity conditions, we establish the oracle property of the aggregated estimator ==== when the number of subsets does not grow too fast. What is more, the split-and-conquer procedure enjoys the oracle property as the penalized estimator by using all the dataset, and can substantially reduce computing time and computer memory requirements. The main differences between the second main contribution in this paper and theirs we compare with are: (1) Zhao, Cheng and Liu [25], Lian, Zhao and Lv [13] and Wang, Lian and Liang [21] considered partially linear models with massive heterogeneous data, while we consider general semiparametric models with massive data by using the semiparametric estimating equations, including partially linear models, semiparametric varying-coefficient linear models, partially linear errors-in-variables models, single index regression models, etc. (2) Lian, Zhao and Lv [13], Wang, Lian and Liang [21] and Wang, Zhang and Lian [22] proposed to employ the polynomial splines to approximate the nonparametric functions, while we propose to estimate ==== by the kernel estimators.====The remainder of this paper is organized as follows. In Section 2, we propose a split-and-conquer variable selection procedure for high-dimensional general semiparametric models with massive data, and establish selection consistency and the oracle property of the proposed method. In Section 3, we propose a computational algorithm to solve the penalized semiparametric estimating equations. Simulation results are reported in Section 4, and one real data example is presented in Section 5. Finally, the technical proofs of main results are stated in the Appendix.",A split-and-conquer variable selection approach for high-dimensional general semiparametric models with massive data,https://www.sciencedirect.com/science/article/pii/S0047259X22001191,17 November 2022,2022,Research Article,43.0
"Sheng Haiyang,Yu Guan","Department of Biostatistics, University at Buffalo, Buffalo, NY, United States,Department of Biostatistics, University of Pittsburgh, Pittsburgh, PA, United States","Received 10 April 2022, Revised 28 October 2022, Accepted 29 October 2022, Available online 10 November 2022, Version of Record 22 November 2022.",https://doi.org/10.1016/j.jmva.2022.105126,Cited by (0)," shift setting. In the posterior drift or the more general setting where both covariate shift and posterior drift exist, the excess risk of TNN depends on the maximum posterior discrepancy between the distribution of the supplementary training samples and the distribution of interest. Both our simulation studies and an application to the land use/land cover mapping problem in geography demonstrate that TNN outperforms other existing methods. It can serve as an effective tool for transfer learning.","Classification is an important supervised learning problem. It has a wide range of applications including disease diagnosis ====, ====, handwriting recognition ====, ==== and spam detection ====, ==== nearest neighbors (====NN) classifier is conceptually the simplest and perhaps the most popular one. Given a training data set ==== drawn from some distribution ====, ====NN predicts the label ==== of a test sample ==== by ====, where ==== is the number of nearest neighbors, ==== is the class label of the ====th nearest neighbor of ====, and ==== is the indicator function that takes value 1 if ==== and 0 otherwise. Although ====NN has many appealing properties ====, ====, one defect of ====NN is that it uses the same weight on the class labels of all ==== nearest neighbors of ====.====Intuitively, it is better to assign larger weights to neighbors closer to ==== since they are more similar to ==== and thus can be more informative. Extensive studies on weighted nearest neighbors (WNN) classifiers have shown the advantage of assigning flexible weights to different neighbors ====, ====, ====, ====, ====, ====, ====. In particular, Samworth et al. ==== proposed the optimal weighted nearest neighbor (OWNN) classifier which uses the asymptotical optimal weights. It puts decreasing weights on the class labels of the successive more distant neighbors. Similar to ====NN, OWNN uses the same number of nearest neighbors for all test samples. For a given number of nearest neighbors, weights used in OWNN only depend on the dimension ==== and the rank of the distance.  Anava and Levy ==== introduced a more flexible ====NN classifier in which both the number of nearest neighbors and the weights are determined for different test data points adaptively. For the test data point ====NN can perform better than ====NN, the theoretical guarantee of ====NN has not been established.====Despite the significant progress in WNN, most existing WNN classifiers are designed for traditional supervised learning problems where we assume all training samples ==== and the test sample ==== are independent and identically distributed. However, in many real applications, it could be difficult or expensive to obtain training samples that are drawn from the distribution of interest. One typical example is the land use/land cover (LULC) mapping problem in geography ====. The LULC mapping problem can be formulated as a classification problem where the class labels are LULC classes (e.g., farm, forest, grass) and the features are normalized difference vegetation index (NDVI) values of Landsat satellite imagery. In order to learn a good non-parametric classifier, we often need a significant amount of accurately labeled training samples. Unfortunately, such a training data set for LULC classification can be time-consuming and/or expensive to obtain, particularly if ground surveys are needed to collect the class labels ====. Therefore, geographic information on LULC provided by geography amateurs rather than professionals, which is a relatively new source of freely-available crowdsourced information, is often used as a supplementary or even alternative source of training data ====. Although the volunteered geographic information is less accurate than the geographic information from professionals due to many geography amateurs’ lack of formal training ====, it provides the cheapest (and sometimes the only) source of geographic information ====. It is important and challenging to incorporate both the accurately labeled data from professionals and the unreliable labeled data from geography amateurs to learn a good classifier for the LULC mapping.====, which we call ====-data hereafter, transfer learning also makes use of training data from a different but related distribution, denoted by ====, which we call ====-data later. The distribution ==== is called the source distribution since it is where we want to borrow information from. Oftentimes, ====-data are much easier and cheaper to collect than ====-data, and thus we have ====. Transfer learning has drawn more and more attention in recent years and has been successfully applied to various fields such as speech recognition ====, ====, ==== and image classification ====, ====. Two major structural frameworks of transfer learning are ==== and ==== ====. Consider the pair ====, the ==== framework assumes that the conditional distributions of ==== given ==== are the same for the distributions ==== and ====, i.e., ====, whereas the marginal distributions of ==== differ, i.e., ====. Covariate shift can happen when there exists a sample selection bias with respect to the collection of training data and test data ====. Existing works on covariate shift ====, ====, ====, ====, ====, ==== framework assumes that the marginal distributions are the same for the pair ==== but the conditional distributions of ==== given ==== differ, namely, ==== but ====. One example of posterior drift is the LULC mapping problem introduced above. The feature distribution of the accurately labeled data from professionals should be the same as that of the unreliable labeled data from geography amateurs since the images are collected from the same population. However, the conditional distributions of the assigned class label given an image differ between the accurately labeled data and the unreliable labeled data. Some recent works on the posterior drift model include ====, ====. In particular, under the posterior drift model, Cai and Wei ==== proposed a two-sample weighted ====-data to the classification problem under the target distribution ====. Although this two-sample weighted ====NN classifier can be rate optimal in some cases, it uses equal weights on the class labels of nearest neighbors from the same distribution. It is possible to develop a better transfer learning WNN classifier that uses flexible weights on nearest neighbors by considering both the difference between the source and target distributions, and the distances between the nearest neighbors and the test data point.====In this paper, we propose a novel Transfer learning weighted Nearest Neighbors (TNN) classifier. As an adaptive WNN classifier, TNN determines the weights on the class labels of training samples for a test data point ==== adaptively by minimizing an upper bound on the conditional expectation of the estimation error of the regression function at ====. To accommodate the difference between the source and target distributions, TNN adds a non-negative offset to the distance between the test data point ==== and each training sample in the ====-data, which tends to constrain the excessive influence of ====, we introduce notations and the classification problem of interest. Our motivation and the proposed TNN classifier are shown in Section ====. The theoretical properties of TNN are studied in Section ====. We demonstrate the performance of TNN by simulation studies and a real application in Sections ====, ====, respectively. Some conclusions are given in Section ====. All proofs are shown in the ====. Some additional numerical results are shown in the supplementary materials file.==== The derivation of ==== is now completed.",TNN: A transfer learning classifier based on weighted nearest neighbors,https://www.sciencedirect.com/science/article/pii/S0047259X22001178,10 November 2022,2022,Research Article,44.0
"Li Weiming,Zhu Junpeng","School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai 200433, China","Received 6 April 2022, Revised 2 November 2022, Accepted 2 November 2022, Available online 9 November 2022, Version of Record 16 November 2022.",https://doi.org/10.1016/j.jmva.2022.105127,Cited by (0), grows proportionally to the sample size ,"Spiked population model originally introduced in ==== of a population ==== is a finite-rank perturbation of a base matrix. For the simplest case where the base matrix is an identity and the perturbation matrix is nonnegative, the eigenvalues of ==== can be grouped into two separated classes ====Here the top ==== eigenvalues ==== are called spiked eigenvalues of ====. These spikes often carry a wealth of information about the dependence among the components of ====. However, in high-dimensional situations where the dimension ==== is non-negligible with respect to the sample size ====, it is well known that the eigenvalues of ==== deviate from their population ones in a subtle manner. See ====, ====, ====, etc.====Let us consider the widely used independent components (IC) model ==== for the population ====, admitting the following stochastic representation ====where ==== denotes the population mean and ==== is a vector of independent and standardized random variables. Let ==== be ==== independent and identically distributed (i.i.d.) observations from this population. The sample covariance matrix is ====the eigenvalues of which are denote as ====, arranged in descending order and referred to as sample eigenvalues. These eigenvalues have been well studied in the so-called Marčenko–Pastur (MP) asymptotic regime ==== where ====In particular, under the spiked population model ==== with ==== pair-wise different, the ====-th (====) largest eigenvalue ==== of ==== converges to ==== with a Gaussian fluctuation if ==== is larger than the critical value ====, otherwise to ==== the right edge point of the MP law with a Tracy–Widom fluctuation, see ====, ====, ====, ====. Some extensions of these results can be found in ====, ====, ====, ====, ====.====Most recently, ==== considered a mean mixture (MM) model when investigating the problem of high-dimensional clustering. An interesting phenomenon is that a sample covariance matrix from such mixture inherently carries some spiked eigenvalues. To fully understand such phenomenon, let us consider an MM ==== of ==== subpopulations ==== with mean vectors ==== and a common covariance matrix ====, that is, ====where ==== denote the ==== subpopulations and ==== is a random vector similarly defined as in the IC model ====. Let ==== be ==== observations from this mixture and denote by ==== the (unknown) sizes of samples from each of the subpopulations. Then, given these sizes, the expectation of the sample covariance matrix is ====Clearly, this covariance matrix ==== is a sum of the base matrix ==== and a finite rank perturbation matrix, which forms a spiked population model and the spikes are generated by the differences among the ==== subpopulation means. The main results in ==== imply that, in the MP asymptotic regime ====, spiked eigenvalues of ==== from the MM model converge almost surely to some limits in the same way as that already established under the IC model ==== with covariance matrix ====.====This paper takes a step further to investigate the second order limits of these spiked sample eigenvalues. A Gaussian mean mixture (GMM) population is considered in our study, i.e., the MM model ====, the main difference of the GMM lies in that the observations are heterogeneous in conditional mean, given the group information. Hence our main task here is to quantify the effect of such heterogeneity on the fluctuation of spiked sample eigenvalues.====, the mean differences ==== and the common covariance matrix ==== of subpopulations. In particular, the inner products between the means ==== play important roles in the fluctuation of the spikes. As these determining factors are not functions of ====, our CLT is essentially different from the case under the IC model as illustrated in ====, ====.====The rest of the paper is organized as follows. Section ==== details our model and assumptions. Section ==== presents our new CLT for spiked sample eigenvalues. Technical proofs are presented in Section ====.",CLT for spiked eigenvalues of a sample covariance matrix from high-dimensional Gaussian mean mixtures,https://www.sciencedirect.com/science/article/pii/S0047259X2200118X,9 November 2022,2022,Research Article,45.0
"Loubaton Philippe,Rosuel Alexis,Vallet Pascal","Laboratoire d’Informatique Gaspard Monge (CNRS, Univ. Gustave-Eiffel), 5 Bd. Descartes 77454 Marne-la-Vallée, France,Laboratoire IMS (CNRS, Univ. Bordeaux, Bordeaux INP), 351, Cours de la Libération 33405 Talence, France","Received 10 July 2021, Revised 21 October 2022, Accepted 21 October 2022, Available online 8 November 2022, Version of Record 23 November 2022.",https://doi.org/10.1016/j.jmva.2022.105124,Cited by (1),-variate complex Gaussian time series with mutually independent components when the dimension ==== and the number of samples ==== both converge to infinity. If ==== and ==== components of the observed time series. Numerical simulations support our results.,None,On the asymptotic distribution of the maximum sample spectral coherence of Gaussian time series in the high dimensional regime,https://www.sciencedirect.com/science/article/pii/S0047259X22001154,8 November 2022,2022,Research Article,46.0
"Jeon Jeong Min,Van Keilegom Ingrid","ORSTAT, KU Leuven, Belgium","Received 27 January 2022, Revised 21 October 2022, Accepted 21 October 2022, Available online 2 November 2022, Version of Record 14 November 2022.",https://doi.org/10.1016/j.jmva.2022.105125,Cited by (0),"In this paper, we study density estimation for mixed Euclidean and non-Euclidean variables that are subject to measurement errors. This problem is largely unexplored in ====. We develop a new deconvolution density estimator and derive its finite-sample properties. We also derive its ====, which has not been well used in statistics. We provide full practical details on the implementation of the estimator as well as several simulation studies and real data analysis.","Analyzing data contaminated by measurement errors has been a challenging topic during the recent decades. Direct applications of existing methods designed for error-free data to contaminated data usually cause severe biases. To deal with this issue, several approaches have been proposed. The most popular approach is the deconvolution approach studied in ====, ====, ====, ====, ====, ====, ====, ====Recently, non-Euclidean data such as functional data and manifold-valued data are emerging in many areas due to advanced technologies. We also refer to ====, ==== for a recent review on non-Euclidean data analysis. Among non-Euclidean data, analyzing manifold-valued data is particularly challenging since they have no vector space structure. For general manifold-valued data or for particular manifold-valued data such as circular data, spherical data and matrix-valued data having a special structure, many density estimation methods have been proposed in the absence of measurement error (e.g., ====, ====, ====, ====, ====).====However, manifold-valued data sometimes contain measurement errors as Euclidean data do. For example, wind directions are not easy to measure exactly due to the fast speed of wind. Also, periodic time variables such as the time of a daily event and the date of an yearly event are prone to contain measurement errors unless one observes them all the time. In addition, the directions of animal movements can also contain measurement errors as illustrated in ====. Those are examples of circular data. Another area in which contaminated manifold-valued data arise is astronomy. For example, measuring the exact positions of sunspots or the exact relative directions of astronomical objects to the earth is not easy since they move very fast from far away. Those are examples of spherical data. For the same reason, the orbits of comets or asteroids, which can be transformed to ====-valued data ====, can also contain measurement errors, where ==== is the space of ==== are subject to measurement errors as demonstrated in ====In spite of the importance of analyzing contaminated manifold-valued data, not many works have been conducted for this challenging topic. For instance, ==== for the circle, ==== for the sphere, ==== for the two-dimensional Euclidean motion group, ==== for the six-dimensional Euclidean motion group. The aforementioned works only considered the rates of convergence of their estimators. Recently, Jeon et al. ==== also studied regression analysis on compact and connected Lie groups. They consider the case where the predictor is completely observed while the real-valued response is incompletely observed.====In many datasets, there are both Euclidean and manifold-valued variables since both types of variables are abundant. However, density estimation for such mixed variables is not much studied even for the error-free case (e.g., ====, ====, ====, ====). Besides, the case of contaminated mixed variables is only considered ====, ==== to the best of our knowledge. However, ==== only covers ====-valued data and ==== only covers ====-valued data. Hence, they only cover some Euclidean motion groups. One may ignore the second element of ==== and apply the method of ==== to deal with ====-valued data. However, this kind of approach to deal with lower-dimensional Euclidean variables is not valid due to the special structures of Euclidean motion groups, see the Supplementary Material S.1 for details. However, there are some mixed contaminated variables that do not take values in the above Euclidean motion groups. For example, the Spanish air pollution data that we analyze in Section ==== contain an air pollutant level as an one-dimensional Euclidean variable and a wind direction as a circular variable, which are prone to contain measurement errors as described in ====. The authors in ==== estimated the joint densities of the air pollutant level and wind direction for different years to check whether the air pollutants produced by a power plant located in a certain direction of a weather station are reduced after some measures. However, there has been no method that can analyze such data properly to the best of our knowledge.====In this paper, we consider a data domain of the form ==== to analyze general contaminated data, where ==== and ==== is an arbitrary compact and connected Lie group. We note that ==== includes not only ==== for ==== allows us to cover multivariate circular variables, ====-valued variables, three-dimensional hyperspherical variables and so on. However, ====, which covers many types of data, like e.g., linear-circular data on the cylinder. Our deconvolution density estimation is based on the Euclidean deconvolution kernel technique and the deconvolution technique in ====, ==== for ====, which are not adopted in ====, ====. Also, our measurement error structure is different from the one in ====, ====. Hence, the problem setting, methodology and theory in this paper are completely different from those in ====, ==== even when ==== or ====. Our approach even allows for the case where some or all of the Euclidean and non-Euclidean variables are error-free. In this paper, we do not assume that our target density is the product of a marginal density on ==== and a marginal density on ====This paper is organized as follows. In Section ====, we briefly introduce Fourier analysis on topological groups. The problem setting and our methodology are given in Section ====. We provide numerical studies in Section ==== and all technical proofs in Section ====. Ancillary results including practical details, additional theoretical properties and an additional simulation study can be found in the Supplementary Material.====The following is the Supplementary material related to this article. ",Density estimation for mixed Euclidean and non-Euclidean data in the presence of measurement error,https://www.sciencedirect.com/science/article/pii/S0047259X22001166,2 November 2022,2022,Research Article,47.0
"Xu Yangchang,Xia Ningning","School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai 200433, China","Received 30 March 2022, Revised 9 October 2022, Accepted 9 October 2022, Available online 25 October 2022, Version of Record 5 November 2022.",https://doi.org/10.1016/j.jmva.2022.105119,Cited by (0), of observations and the sample size ,The following is the Supplementary material related to this article. ,On the eigenvectors of large-dimensional sample spatial sign covariance matrices,https://www.sciencedirect.com/science/article/pii/S0047259X22001105,25 October 2022,2022,Research Article,48.0
Kulik Rafał,"Department of Statistics, Colorado State University, Fort Collins, CO 80523-1877, United States,University of Ottawa, Canada","Received 20 February 2022, Revised 13 October 2022, Accepted 13 October 2022, Available online 23 October 2022, Version of Record 5 November 2022.",https://doi.org/10.1016/j.jmva.2022.105123,Cited by (1),"Principal Components Analysis is a widely used approach of ==== is in the context of infinite variance multivariate or functional data. We derive suitable large sample theory. In particular, we specify normalizing sequences and conditions for suitably defined consistency. We study multivariate models in which explicit limits can be derived. These examples show that definitions, results and intuition developed for multivariate and functional data with finite variance need not apply in the setting we consider.",", Ferraty and Vieu ====, Ramsay et al. ====, Shi and Choi ====, Horváth and Kokoszka ====, Hsing and Eubank ==== and Kokoszka and Reimherr ====. Many review papers focusing on specific aspects or applications of FDA are available, including many published by this journal, e.g., Goia and Vieu ====, Aneiros et al. ==== and Aneiros et al. ====.====As an outgrowth of its well-known multivariate counterpart, Functional Principal Component Analysis (FPCA) has been an important tool of FDA since the early days of the field and remains so. To explain the contribution of this paper, we begin by presenting the basics of the FPCA. Denote by ==== with the inner product ====. If ==== is a Polish space (complete and separable metric space), then ====Statistical software, see e.g., Ramsay et al. ==== that satisfy ====and the sample scores ====. The ==== are the sample Functional Principal Components (FPCs). Clearly, no assumptions are needed to compute the sample quantities ====. To establish their convergence to population quantities, one must impose assumptions on the observations. If the ==== are independent with the same distribution as ====, and ====, the products in the definition of ==== converges with standard rate to the (population) covariance operator ====A more subtle question is if the ==== and the ==== converge to the ==== and ==== defined by ====. This problem was solved by Dauxois et al. ==== whose results are reported e.g. in Bosq ==== and Horváth and Kokoszka ==== in greater generality. In particular, it is known that ==== and ====If one drops the assumption ====, the standard Hilbert space CLT cannot be applied to the operator ====. Even in the case of partial sums of scalar observations, to obtain convergence to a nondegenerate limit, one must assume that the observations have regularly varying tails. A basically complete theory is given in Gnedenko and Kolmogorov ====, which is summarized on a few pages in Section 2.2 of Embrechts et al. ====. In the case of functional observations, it must be assumed that the ==== are regularly varying in ====. Postponing the definitions and details to Section ==== satisfies ====, then ====, so the covariance operator ==== given by ====. One can still conclude that ====, ==== and ====If ====, one cannot define the covariance operator ====, because then ==== and eigenvalues ====. However, one can always compute the sample covariance operator ==== and the FPCs ====. Some questions are then: Does ==== and do the ==== converge to any limits? Since ==== and the ====that is commonly used in FDA for dimension reduction or feature extraction. We want to understand the large sample behavior of the FPCs ==== and their scores ====.====, who studied the polar decomposition of a regularly varying multivariate time series, Meinguet and Segers ==== extended their results in two aspects: regular variation of the time series treated as a single random element in a sequence space and the polar decomposition in star-shaped metric spaces. It may be hoped that such general results combined with the results of this paper will motivate the development of useful statistical models for regularly varying functional data.====The remainder of the paper is organized as follows. In Section ====, we conveniently organize known results and prove corollaries that are needed in subsequent sections. Section ==== considers general, infinitely dimensional functional data. In Section ====, we focus on multivariate models in which asymptotic quantities can be computed explicitly. The results of Section ==== emphasize important differences between the cases of finite- and infinite variance multivariate or functional observations. Section ==== contains infinitely dimensional results that extend some results of Section ====. Longer and more technical proofs are collected in Sections ====, ====. Online material contains a data example illustrating our theory.====The following is the Supplementary material related to this article. ",Principal component analysis of infinite variance functional data,https://www.sciencedirect.com/science/article/pii/S0047259X22001142,23 October 2022,2022,Research Article,49.0
"Goegebeur Yuri,Guillou Armelle,Ho Nguyen Khanh Le,Qin Jing","Department of Mathematics and Computer Science, University of Southern Denmark, Campusvej 55, 5230 Odense M, Denmark,Institut Recherche Mathématique Avancée, UMR 7501, Université de Strasbourg et CNRS, 7 rue René Descartes, 67084 Strasbourg cedex, France","Received 23 December 2021, Revised 11 October 2022, Accepted 12 October 2022, Available online 21 October 2022, Version of Record 4 November 2022.",https://doi.org/10.1016/j.jmva.2022.105121,Cited by (0),"Several risk measures have been proposed in the literature, among them the marginal mean excess, defined as ==== provided ====, where ==== denotes a pair of risk factors, ====, ==== and ====. In this paper we consider a generalization of this measure, where the random variables of main interest ====, and where the ","In many scientific disciplines, quantifying risks related to extreme events is of crucial importance. For instance, insurance companies are once in a while faced with extreme claims which can jeopardize the solvency of a portfolio, and hence accurate modeling of the upper tail of the claim size distribution assumes a central place in their risk management. Other examples of disciplines include environmental science (storms, rainfall), geology (severe earthquakes), hydrology (flooding) and telecommunication (network load). The quantification of the risk of a risk factor ==== with distribution function ==== is done by so-called risk measures. In the univariate context, the most commonly used risk measures are the Value-at-Risk (VaR), defined as ====, where ====, i.e., ====see, e.g., Jorion ==== for a review, and the conditional tail expectation (CTE), given by ====provided ====. During recent years, the CTE became a popular alternative to VaR since, compared to VaR it is more conservative and it is also a coherent risk measure. We refer to, e.g., Artzner et al. ====, Cai and Tan ==== and Brazaukas et al. ====. In practice, risk is often related to several risk factors, and hence the above discussed risk measures need to be adjusted for this multivariate context. For a pair of risk factors ====, with ====, the CTE can be generalized to the marginal expected shortfall (MES), defined as ====where ====. This measure was introduced by Acharya et al. ==== quantile is extreme, i.e., when ====. See also Di Bernardino and Prieur ====, and Das and Fasen-Hartmann ====, ==== for related analyses of the MES in a multivariate extreme value setting. By replacing ==== in ==== by an excess over a high quantile of its distribution one obtains the marginal mean excess (MME): ====provided ====, and where ==== and ==== is the quantile function of ====. Das and Fasen-Hartmann ====, ==== study a slightly different version of MME under multivariate regular variation, introduced an estimator for it based on extreme value arguments, and established its consistency.====, ==== studied the estimation of extreme conditional quantiles, while the CTE was extended to the regression case in El Methni et al. ====, ====. Goegebeur et al. ====, ==== considered the estimation of the MES in presence of random covariates.====In this paper we consider a generalization of the marginal mean excess, where the random variables of main interest ==== are observed together with a random covariate ====, and where the ==== excess is also power transformed. We assume that the covariate ==== has a density function ====, with support ====, and we denote by ==== the conditional distribution function of ==== given ==== and by ==== the associated conditional tail quantile function, i.e., ====, ====. In particular, we introduce the conditional marginal excess moment (CMEM), defined as ====where ====, provided ====, and where ==== is a reference position such that ====, the interior of the support of ====, assumed to be non-empty. The motivation for introducing this power ==== is that, in an insurance context for instance, with ==== denoting the claim size, ==== can be viewed as the payment by the reinsurer. Thus, different values of ==== allow us to compute, among others, the expectation or the variance of this payment. We are interested in the situation where ==== is small, i.e., ====, with ==== denoting the sample size. To obtain this extrapolation, we will work in two steps, where in a first step we study an intermediate case, which allows for constructing an empirical estimator for ====. In a second step this intermediate estimator will be extrapolated outside the data range by a Weissman-type construction.====The remainder of the paper is organized as follows. In Section ==== we consider the CMEM in the intermediate case. We introduce a locally weighted average as estimator for the CMEM and derive its limiting distribution under suitable conditions. This intermediate estimator is then extrapolated outside the data range in Section ==== we evaluate the finite sample performance with a simulation experiment, while in Section ==== we illustrate the method on a vehicle insurance customer dataset. Some auxiliary results and their proofs are given in Section ====. Section ==== contains the proofs of the main results.",Nonparametric estimation of conditional marginal excess moments,https://www.sciencedirect.com/science/article/pii/S0047259X22001129,21 October 2022,2022,Research Article,50.0
Dörnemann Nina,"Fakultät für Mathematik, Ruhr-Universität Bochum, 44801 Bochum, Deutschland","Received 10 March 2022, Revised 12 October 2022, Accepted 12 October 2022, Available online 20 October 2022, Version of Record 27 October 2022.",https://doi.org/10.1016/j.jmva.2022.105122,Cited by (1),We investigate the ,", ====, ====, is developed under the paradigm that the dimension is negligible compared to the sample size and breaks down seriously if this assumption is violated. Such problems have spurred the development of new analysis tools, that work for dimensions of the same order as and even larger than the sample size. The literature on these topics is so large, that we can only cite a few illustrative examples, related to the present work: The works ====, ====, ====. In the work ====, Hu et al. concentrate on tests for the equality of high-dimensional mean vectors, while ==== take a broader perspective on high-dimensional testing by investigating a class of ====-statistics.==== establishing CLTs for the corresponding log-likelihood ratio tests, including the two main testing problems investigated in this work. The authors of ==== tried to relax the assumptions on the parameters, while other authors extended these results in various directions. For example, ====, ====, ====, ====, ====, ====, ====. We add to this line of literature by dropping the restrictive distributional assumption of normality. In particular, we find that the CLTs for the log-likelihood of two specific testing problems remain still valid when only assuming moments of order ==== for some ====. Besides the theoretical importance of our findings, these results ensure more robust statistical guarantees for practitioners, as the validity of the normal assumption is not a priori clear for high-dimensional data sets.==== where we consider the problem of testing whether the covariance matrix of a ====-dimensional random vector admits a block-diagonal structure with ==== of this work.==== near singularity, while ====, ====, in which the authors proved Girko’s logarithmic law for a general random matrix with independent entries and brought his “method of perpendiculars” ==== to a mathematically rigorous level. Via our representation, we are in the position to decompose the test statistic into three parts: we will prove that the dominating linear term satisfies a central limit theorem for martingale difference schemes, while the quadratic term converges to constant and the remainders are asymptotically negligible. Heuristically, this decomposition can be motivated by Taylor’s expansion ====, though one needs more delicate arguments in order to justify this step mathematically correct.====This work is structured as follows. In Section ====, we present a CLT for the log-likelihood ratio test of a block-diagonal covariance matrix under the null hypothesis. Here, the number of blocks may increase together with the dimension of the data and sample size while we do not assume that the data is generated by a normal distribution. As a corollary, the distribution of a test for a diagonal covariance matrix is derived. In Section ====, ==== are proven in Section ====. We illustrate our findings with a simulation study, including a comparison to other criteria, and a real data analysis in Section ====.",Likelihood ratio tests under model misspecification in high dimensions,https://www.sciencedirect.com/science/article/pii/S0047259X22001130,20 October 2022,2022,Research Article,51.0
"Bianchi Pascal,Elgui Kevin,Portier François","Télécom Paris, Institut Polytechnique de Paris, France,CREST, ENSAI, France","Received 3 August 2021, Revised 10 October 2022, Accepted 10 October 2022, Available online 17 October 2022, Version of Record 29 October 2022.",https://doi.org/10.1016/j.jmva.2022.105120,Cited by (0),The test statistic proposed in this paper is an explicit Cramér–von Mises transformation of a certain weighted partial ,"Let ==== be a triple of continuous random variables. We say that ==== and ==== are conditionally independent given ==== if ====: ====This property is denoted by ==== and roughly speaking, it means that for a given value of ====, the knowledge of ==== does not provide any further information on ====, ====; see also ==== for a study specific to cellular networks. Moreover the concept of conditional independence lies at the core of sufficient dimension reduction methods ==== and is useful to conduct variable selection in regression ====. Finally, conditional independence is relevant in many application fields such as economy ==== or psychometry ====.====The approach taken in this paper is related to the well-studied problem of (unconditional) independence testing, in which, inspired by ====, rank-based statistics have received an increasing interest ====, ====, ====, ====, ==== and we refer to ====, ====, ====, ====.====The conditional copula of ==== and ==== given ==== is defined in the same way as the copula of ==== and ==== but uses the conditional distribution of ==== and ==== given ==== in place of the joint distribution of ==== and ====. Compared to the copula, the conditional copula captures the conditional dependency between random variables and is thus useful to build conditional dependency measures ====, ====. Therefore, as in the case of independence testing, the conditional copula appears to be a relevant tool for building a statistical test of conditional independence. This has been pointed out as an “interesting open issue” in ====.====In this work, a new statistical test procedure, called the weighted partial copula test is investigated to assess conditional independence. The proposed approach follows from using the weighted partial copula, an integral transform allowing a simple characterization of conditional independence. Given estimators of the conditional marginals of ==== and ==== given ====, the empirical weighted partial copula is introduced to estimate the weighted partial copula and the test statistic results from an easy-to-compute Cramér–von Mises transformation. The use of the weighted partial copula is motivated by the conditional moment restrictions literature (see ==== and the reference therein) in which integrated criteria, similar to the weighted partial copula, have been frequently used. Those criteria are interesting because even when they involve local estimates converging at a slower rate than ====, their convergence rates are in many cases in ====.====Inspired by the independence testing literature ====, ====, the weak convergence of the test statistic is obtained.====Nonparametric testing for conditional independence between continuous variables has received an increasing interest the past few years ====. Some of the existing approaches are based on comparing the (estimated) conditional distributions involved in the definition of conditional independence. The probability distributions can be compared using their conditional characteristic functions as in Su and White, Wang et al. ====, ====, their conditional densities as proposed in ====, or their conditional copulas as studied in ====. Extending the Hilbert–Schmidt independence criterion proposed in Gretton et al., Zhang et al. ====, ==== defines a kernel-based conditional independence test (KCI-test) by estimating the cross-covariance operator (see also ====). A surge of recent research ====, ====, ==== has focused on testing conditional independence using permutation-based tests. The work of ==== had led to many conditional independence tests depending on the availability of an approximation to the distribution of ====, ====.====While it is impossible to claim the superiority of our approach compared to the existing methods, we may emphasize several notable advantages:====The outline is as follows. In Section ====, we introduce the weighted partial copula test and provide implementation details regarding the bootstrap procedure. In Section ====, we state the main theoretical results. In Section ====.====Under the stipulated assumption, we also have that ",Conditional independence testing via weighted partial copulas,https://www.sciencedirect.com/science/article/pii/S0047259X22001117,17 October 2022,2022,Research Article,52.0
"Baillien Jonas,Gijbels Irène,Verhasselt Anneleen","KU Leuven, Department of Mathematics, Celestijnenlaan 200B, Box 2400, B-3001 Leuven (Heverlee), Belgium,Center for Statistics, Data Science Institute, Hasselt University, Agoralaan-building D, B-3590 Diepenbeek, Belgium","Received 29 April 2022, Revised 6 October 2022, Accepted 6 October 2022, Available online 15 October 2022, Version of Record 28 October 2022.",https://doi.org/10.1016/j.jmva.2022.105118,Cited by (0),"In a univariate setting there is a near unanimous agreement on the notion of skewness. Nevertheless, many more skewness measures, or also called asymmetry measures (or indices) exist, each with their benefits. Extending the concept of skewness or asymmetry to a multivariate setting is a much harder problem. Attempts have been made, but the unanimity of the univariate setting is no longer present. Most asymmetry indices are scalar or vector based measures, but this can lead to a loss of information concerning asymmetry. To this end, we propose a novel functional asymmetry index which is based on the natural idea of reflective symmetry around the mode. The proposed index is also extended to the multivariate setting and a summarizing scalar (or vector based index in multivariate setting) is derived from it.","Quantifying asymmetry (or skewness) is a well known problem. As early as the late 19th century, great mathematicians as Pearson came up with nowadays widely known measures to capture asymmetry. Multivariate extensions came later with Mardia’s skewness ====Denote with ==== a univariate random variable taking values in ====, with density function ====, cumulative distribution function ====. The earliest measures of skewness for a random variable ==== were based on measures of location for ====. These mainly consisted in comparing the mean (====), median (====) and mode (====). It has been agreed upon by several authors (see e.g., ====) that a measure of asymmetry ====Properties (P1) to (P3) together can be expressed as ====, for any ====. The reason these properties are separated is from a historical point of view. For the earliest notions of skewness, it was already a base criterion that a measure of skewness should be unaffected by scaling or location shifts. Also the measure equaling zero under symmetry was unanimously agreed upon. So what then with negative scaling? The most logical line of thought starting from Properties (P1) and (P2) would be Property (P3) as a reflection of the distribution makes a left skewed distribution right skewed and vice versa, without changing anything else. So the sign of the skewness changes, but not the magnitude. Property (P4) saw life in ==== and is used to put an ordering based on a skewness measure, i.e., when the condition holds for ==== and ====, then ==== always. Over the course of time, many other conditions have been stated, depending on the skewness measure. A nice review is given in ====. The condition stated in Property (P4) is among the strongest, but is arguably not a true necessity as other ad hoc ordering schemes can be devised. Examples of such can be found in ====, ====. In essence, this property can thus be replaced with any other condition which, if it holds for a class of distributions, puts an ordering on the entire class based on its skewness.====One could argue that for an easier interpretation, ====The remainder of the paper is structured as follows. In Section ==== a small review on existing univariate and multivariate asymmetry measures is given. In Section ==== we propose a new measure of asymmetry as well as a multivariate extension of it. Section ==== calculates the newly proposed asymmetry measure for several examples. Section ==== we compare, for some illustrative examples, our measure with the classical measure of asymmetry, and finally in Section ==== we provide some concluding remarks. The paper comes with some Supplementary Material, containing additional information in the form of tables and figures to accompany Sections ====, ====.====The former two conditions are (easily) met. The third condition is (i) of this theorem. Applying Theorem 4.1.1 of ==== concludes parts (ii) and (iii). Part (iv) then automatically follows from (ii) and (iii), completing the proof. □",A new distance based measure of asymmetry,https://www.sciencedirect.com/science/article/pii/S0047259X22001099,15 October 2022,2022,Research Article,53.0
Brück Florian,"Technical University Munich, Chair of Mathematical Finance, Parkring 11, 85748 Garching, Germany","Received 3 January 2022, Revised 4 October 2022, Accepted 5 October 2022, Available online 10 October 2022, Version of Record 28 October 2022.",https://doi.org/10.1016/j.jmva.2022.105117,Cited by (1),An algorithm for the unbiased simulation of continuous max-(resp. min-) infinitely divisible stochastic processes is developed. The algorithm only requires the simulation of finite Poisson random measures on the ,"This paper provides an exact simulation algorithm for real-valued continuous stochastic processes ==== with the property that for every given ==== there exist independent and identically distributed (iid) stochastic processes ==== such that ====. Recently, max-id processes have attracted attention in the modeling of extreme events, see e.g. ====, ====, ====Under the assumption that ==== and ==== are continuous, ====, ==== show that ==== can be represented as the pointwise maximum of a (usually infinite) Poisson random measure (PRM) ====The intensity measure ==== of the PRM ==== is also called the exponent measure of ==== and it uniquely characterizes its distribution. The initial motivation for our simulation algorithm for ==== stems from ====, who have provided an exact simulation algorithm for continuous max-stable processes. In this paper, we generalize the ideas of ==== to a simulation algorithm for continuous max-id processes. The key ingredient of their and our simulation algorithm is the PRM representation of ==== in ==== and its associated exponent measure. Basically, both simulation algorithms can be deduced from results of ====, ==== about the conditional distribution of a specific decomposition of the PRM ====. This specific decomposition of the PRM ==== allows to simulate only those functions which are relevant to determine the values of ==== at certain locations ==== and to approximate the whole sample path of ==== via the pointwise maximum over those finitely many functions. The mechanism of our simulation algorithm can be summarized as follows.====Motivated by the recent results of ====, we apply the proposed simulation algorithm for continuous max-id processes to the simulation of exchangeable sequences of random variables ==== with the property that for every ==== there exist iid sequences of random variables ====
 ====Such sequences are known as minimum-infinitely divisible (min-id) sequences and are as well characterized by a so-called exponent measure ====. It is obvious that ==== is a sequence of exchangeable random variables with stochastic representation ====, therefore simply being a particular example of a general continuous max-id process with index set ====. According to de Finetti’s seminal theorem every exchangeable sequence of random variables admits the (unique) stochastic representation ====where ==== denotes a (unique in law) non-negative and non-decreasing (nnnd) stochastic process with càdlàg paths. ==== show that when ==== has the stochastic representation ==== satisfies the property that for every given ==== there exist iid stochastic processes ==== such that ====Such processes are called infinitely divisible (id) and were extensively investigated in ====. In analogy to the Lévy–Khintchine triplet of id random vectors on ====.====In theory, the stochastic representation ==== immediately suggests a simulation algorithm for ==== as the first passage times of the id process ==== over iid Exponential barriers. In practice, however, even the approximate simulation of the associated id process ==== is usually a challenging task. For instance, when the ====-dimensional marginal distributions of ====, then ====, i.e. ====, ====, ====. Thus, the lack of the ability to simulate general id processes ==== limits the practical use of the stochastic representation ====, even though one may be able to analytically characterize the law of the id process ====.====To overcome this challenge, we exploit the stochastic representation of ==== in terms of maxima over points of a Poisson random measure, which can be derived from ==== and the Lévy measure and drift of the associated id process ====. More specifically, ==== shows that the exponent measure of ==== can be uniquely characterized as a (possibly infinite) mixture of iid sequences in terms of the Lévy measure and drift of the associated id process ====. This will allow us to construct an exact simulation algorithm for ==== via ====, while essentially simulating a finite number of conditionally iid sequences.====The rather general theoretical results about the simulation of exchangeable min-id sequences are then used to derive an exact simulation algorithm for the class of exchangeable Sato-frailty sequences, which have been fully characterized analytically in ====. Exchangeable Sato-frailty sequences can be characterized as the class of exchangeable min-id sequences associated to self-similar additive processes, i.e. they are associated to stochastically continuous càdlàg processes with independent increments which have the additional property that there exists some ==== such that for all ==== we have ====, see e.g. ==== for more details on self-similar additive processes. Even though analytical expressions of their multivariate marginal distributions are available, the simulation of such sequences has so far only been feasible for small sample sizes or some particular cases, which is due to the fact that the simulation of the associated self-similar additive process is generally complicated. We characterize the exponent measure of an exchangeable Sato-frailty sequence in terms of the Lévy measure of the associated self-similar additive process and illustrate that our simulation algorithm essentially boils down to the simulation of two-dimensional random vectors.====In a recent article ==== have independently developed a simulation algorithm for continuous max-id processes on compact non-empty real domains ==== under the additional assumption of continuous marginal distributions. Their algorithm follows similar ideas as ==== translated to the max-id case. However, both of these algorithms require the computation of certain conditional distributions of the (infinite) exponent measure, which is usually a challenging task. Moreover, our framework is more general than that of ==== as index sets and non-continuous marginal distributions. This level of generality is necessary for our purposes, since we put special emphasis on simulation algorithms for exchangeable max-id sequences which have locally compact (but not compact) index sets and possibly non-continuous marginal distributions.====The remainder of the paper is organized as follows. Section ==== summarizes the theoretical background on continuous max-id processes. Section ==== introduces the exact simulation algorithm for continuous max-id processes and characterizes the complexity of the algorithm. In Section ==== we illustrate how our simulation algorithm for continuous max-id processes can be used to simulate exchangeable max-id sequences and we derive a particular exact simulation algorithm for exchangeable Sato-frailty sequences in Section ====. Section ==== provides a short example of how our simulation algorithm for exchangeable Sato-frailty sequences could be used in practice. ====.====This section is devoted to the exact simulation of a max-id random vector ====. Since ==== can be viewed as a continuous max-id process on ==== Algorithm 1 is, in principle, applicable to every max-id random vector. However, the exponent measure of a max-id random vector is often more easily described by exploiting the specific geometric structure of ====. For example, a common representation of an exponent measure of a max-id random vector is the scale mixture of a probability distribution on the non-negative unit sphere of some norm on ====. Two famous representatives of this class of exponent measures are the exponent measures of max-stable random vectors with unit Fréchet margins ==== and random vectors with reciprocal Archimedean copula ====, ====, see ====. In both cases, a simulation of ==== via Algorithm 1 would require to deviate from the natural description of the exponent measure to simulate a PRM with intensity ====. Thus, there is a need to adapt Algorithm 1 to exploit the natural structure of many exponent measures of max-id random vectors. Again, to simplify the theoretical developments, we can w.l.o.g. assume that ====.====Our goal is to generalize the algorithms of ====, ====, ==== to max-id random vectors. Similar to Algorithm 1 we will only simulate those atoms of the PRM ==== with intensity ==== which may be relevant to determine ====. We start by dividing ==== into disjoint “slices” ==== of finite ====-measure. Then, assuming that we can simulate finite PRMs ==== with intensities ====, we iteratively simulate the ==== until a stopping criterion is reached. To obtain a valid stopping criterion we need to assume that the slices ==== eventually approach ====, which is mathematically described as eventually residing in an open ball around ====. This will force the algorithm to stop after finitely many steps, since atoms of the PRM ==== in a neighborhood of ==== eventually cannot contribute to the maximum of the already simulated points.====Under these conditions on ==== we can propose the following algorithm for the exact simulation of max-id random vectors with exponent measure ====. ====The use of Algorithm 2 is illustrated by the following example in which we provide an exact simulation algorithm for a large family of max-id distributions. As a byproduct, the simulation algorithm for max-stable random vectors ==== and the simulation algorithm for random vectors with reciprocal Archimedean copula ==== are unified in a common simulation scheme.",Exact simulation of continuous max-id processes with applications to exchangeable max-id sequences,https://www.sciencedirect.com/science/article/pii/S0047259X22001087,10 October 2022,2022,Research Article,54.0
"Jiang Feiyu,Wang Runmin","Department of Statistics and Data Science, School of Management, Fudan University, China,Department of Statistics, Texas A&M University, United States of America,Department of Statistics, University of Illinois at Urbana Champaign, United States of America","Received 11 May 2022, Revised 21 September 2022, Accepted 22 September 2022, Available online 30 September 2022, Version of Record 17 October 2022.",https://doi.org/10.1016/j.jmva.2022.105114,Cited by (1),"This paper proposes a new test for a change point in the mean of high-dimensional data based on the spatial sign and self-normalization. The test is easy to implement with no tuning parameters, robust to heavy-tailedness and theoretically justified with both fixed-==== and sequential asymptotics under both null and alternatives, where ==== is the sample size. We demonstrate that the fixed-==== asymptotics provide a better approximation to the finite sample distribution and thus should be preferred in both testing and testing-based estimation. To estimate the number and locations when multiple change-points are present, we propose to combine the ====-value under the fixed-==== asymptotics with the seeded binary segmentation (SBS) algorithm. Through numerical experiments, we show that the spatial sign based procedures are robust with respect to the heavy-tailedness and strong coordinate-wise dependence, whereas their non-robust counterparts proposed in Wang et al. (2022)==== appear to under-perform. A real data example is also provided to illustrate the robustness and broad applicability of the proposed test and its corresponding estimation algorithm.",", ====, ====, ==== for recent reviews. With the development of modern data collection techniques, high-dimensional data has become more common in the foregoing areas, and the associated data analysis has also triggered the advancement of inference methods for change-points in high-dimensional data, see, e.g. ====, ====, ====, ====, ====, ====, ====, ====, ====. Among the proposed tests and estimation methods, most of them require quite strong moment conditions (e.g., Gaussian or sub-Gaussian assumption, or sixth moment assumption) and some of them also require weak component-wise dependence assumption. There are only a few exceptions, such as ====, the interest in the dense alternative can be well motivated by real data and is often the type of alternative the practitioners want to detect. For example, copy number variations in cancer cells are commonly manifested as change-points occurring at the same positions across many related data sequences corresponding to cancer samples and biologically related individuals; see ====.====In this article, we propose a new test for a change point in the mean of high-dimensional data that works for a broad class of data generating processes. In particular, our test targets the dense alternative, is robust to heavy-tailedness, and can accommodate both weak and strong coordinate-wise dependence. Our test is built on two recent advances in high-dimensional testing: spatial sign based two sample test developed in ==== and U-statistics based change-point test developed in ==== for a book-length review. However, it was until recently that ====, ==== discovered that spatial sign could also help relax the restrictive moment conditions in high dimensional testing problems. ==== advanced the high-dimensional two sample U-statistic pioneered by ==== to the change-point setting by adopting the self-normalization (SN) ====, ====. Their test targets dense alternative, but requires sixth moment assumption and only allows for weak coordinate-wise dependence.====Building on these two recent advances, we shall propose a spatial signed SN-based test for a change point in the mean of high-dimensional data. Our contribution to the literature is threefold. Firstly, we derive the limiting null distribution of our test statistic under the so-called fixed-==== asymptotics, where the sample size ==== is fixed and dimension ==== grows to infinity. We discovered that the fixed-==== asymptotics provide a better approximation to the finite sample distribution when the sample size is small or moderate. We also let ==== grow to infinity after we derive ==== mixing, and strong coordinate-wise dependence under the framework of “randomly scaled ====-mixing sequence” (RSRM) in ====. The process convergence associated with spatial signed U-process we develop in this paper further facilitates the application of our test under sequential asymptotics where ====, in addition to ====, also goes to infinity. In particular, we have developed novel theory to establish the process convergence result under the RSRM framework. In general, this requires to show the finite dimensional convergence and asymptotic equicontinuity (tightness). For the tightness, we derive a bound for the eighth moment of the increment of the sample path based on a conditional argument under the sequential asymptotics, which is new to the literature. Using this new technique, we provide the unconditional limiting null distribution of the test statistic for the fixed-==== and growing-==== case. This is stronger than the results in ==== which is a conditional limiting null distribution. Thirdly, we extend our test to estimate multiple changes by combining the ====-value based on the fixed-==== asymptotics and the seeded binary segmentation (SBS) ====. The use of fixed-==== asymptotics is especially recommended due to the fact that in these popular generic segmentation algorithms such as WBS ==== and SBS, test statistics over many intervals of small/moderate lengths are calculated and the sequential asymptotics is not accurate in approximating the finite sample distribution, as compared to its fixed-==== counterpart. The superiority and robustness of our estimation algorithm is corroborated in a small simulation study.====The rest of the paper is organized as follows. In Section ====, we define the spatial signed SN test. Section ====. Section ==== contains a real data example and Section ==== concludes. All proofs with auxiliary lemmas are given in the appendix. Additional simulation results are provided in the online supplementary material. Throughout the paper, we denote ==== as the weak convergence for stochastic processes. The notations ==== and ==== are used to represent vectors of dimension ==== whose entries are all ones and zeros, respectively. For ====, denote ==== and ====. For a vector ====, ====, ==== if there exists ==== such that ==== for ====, and let ==== if ==== and ====. Let ==== denote the spatial sign of a vector ====.====In what follows, let ==== denote the ====th coordinate of a vector ====.",Robust inference for change points in high dimension,https://www.sciencedirect.com/science/article/pii/S0047259X22001051,30 September 2022,2022,Research Article,55.0
"Kalogridis Ioannis,Van Aelst Stefan","Department of Mathematics, KU Leuven (University of Leuven), Celestijnenlaan 200B, 3001 Leuven, Belgium","Received 26 August 2021, Revised 12 September 2022, Accepted 12 September 2022, Available online 20 September 2022, Version of Record 21 December 2022.",https://doi.org/10.1016/j.jmva.2022.105104,Cited by (1),. These results can be generalized to higher dimensions under similar assumptions. The finite-sample performance of the proposed family of estimators is investigated by a Monte-Carlo study which shows that these estimators reach high efficiency while offering protection against outliers. The proposed estimators compare favourably to existing robust as well as non-robust approaches. The good performance of our method is also illustrated on a complex real dataset.,"In recent years, technological innovations and improved storage capabilities have led practitioners to observe and record increasingly complex high-dimensional data. Among others, data that are characterized by an underlying functional structure have attracted considerable research interest, following works such as [33], [34], [35]. Particular interest has been devoted to the functional linear model, relating a scalar response ==== to a random function ====, which is viewed as an element of ==== with sample paths in ====, through the model ====Here, ==== is the intercept, ==== is a square integrable coefficient (weight) function defined on a compact interval ==== of a Euclidean space, ==== is an unknown scale parameter and ==== is a random error, that is assumed to be independent of ====. Typically, ==== is also assumed to possess finite second moments, but this assumption is not needed for the theoretical results in this paper.====The vast domain of applications of the model, ranging from meteorology [35] and chemometrics [11] to diffusion tensor imaging tractography [16], has spurred the development of numerous novel estimation methods. Since estimating the coefficient function ==== is an infinite dimensional problem, regularization through dimension reduction or penalization is crucial for the success of these methods. Regressing on the scores of the leading functional principal components [4] is the oldest and perhaps to this day the most popular method of estimation. However, although consistent [17], functional principal component regression may fail to yield smooth estimates of the coefficient function, even in moderately large samples. This fact has motivated proposals that explicitly impose smoothness of the estimated coefficient function. Cardot et al. [5] proposed estimation through a penalized spline expansion while functional extensions of smoothing splines have been proposed and studied by Crambes et al. [7] and Yuan and Cai [50]. A hybrid approach between principal component and penalized spline regression has been developed by Reiss and Ogden [38] and Goldsmith et al. [15], who combine these methods in order to attain greater flexibility. Variable selection ideas have also been adapted to the functional regression setting. James et al. [19] proposed imposing sparsity on higher order derivatives of a high dimensional basis expansion of ==== in order to produce more interpretable estimates. Expressing the coefficient function in the wavelet domain, Zhao et al. [51] proposed an ==== regularization scheme in order to select the most relevant resolutions and ensure stable and accurate estimates of a wide variety of coefficient functions. For more details on existing estimation methods as well as informative comparisons, one may consult the comprehensive review papers of Morris [29] and Reiss et al. [37].====Since all of the above methods rely on generalized least-squares type estimators, a drawback in their use is that the presence of outliers can have a serious effect on the resulting estimates. To address this lack of robustness, more robust estimation procedures have been introduced. Maronna and Yohai [28] proposed a robust version of the smoothing spline estimator of Crambes et al. [7] but did not study theoretical properties of their method. Shin and Lee [43] have extended the work of Yuan and Cai [50] by considering more outlier-resistant loss functions and showed that under regularity conditions their M-type smoothing spline estimator attains the same rates of convergence as its least-squares counterpart. Similarly, Qingguo [32] generalized the work of Hall and Horowitz [17] to functional principal component regression with a general convex loss function. More recently, Boente et al. [2] proposed a family of sieves estimators based on bounded loss functions and B-spline expansions and investigated rates of convergence with respect to the prediction error.====In general, sieves estimators based on either functional principal components or B-splines and smoothing spline estimators can be considered to be situated on the two ends of a spectrum. Unpenalized sieves estimators are easy to implement, yet frequently result in either undersmoothed or oversmoothed estimates of the regression function. This undesirable feature results from the discrete nature of their smoothing parameter, which in this case is the dimension of the basis. On the other hand, smoothing spline estimators, while capable of yielding estimates with the right amount of smoothness, can be unwieldy due to their high dimension. In particular, the requirement to have as many basis functions as the sample size leads to computationally challenging estimators that are prone to instabilities due to the often complex nature of functional data. In the nonparametric regression framework, the case for lower-rank representations on the grounds of simplicity has already been made by Wahba [45]. For functional regression, an even stronger case can be made due to the lack of banded matrices that enable fast computational algorithms for smoothing splines in this setting.====As a compromise between these two types of estimators, this paper introduces and studies a family of lower-rank penalized estimators based on the principle of MM-estimation, as described by Yohai [48]. The proposed class of estimators exhibit a high degree of robustness against both vertical outliers and leverage points, while also maintaining high efficiency under Gaussian errors. In our opinion, this class of estimators fills an important void in the literature by providing a family of flexible and resistant estimators that is also computationally feasible. Our framework does not only include the popular B-spline basis combined with a quadratic roughness penalty, but also many other basis systems combined with a wealth of possible penalties. Examples include the Fourier basis with the harmonic acceleration penalty introduced by Ramsay and Silverman [35] and the wavelet basis with bounded variation or Besov penalties [13, Chapter 10]. It should be noted that the theory for functional linear regression developed herein cannot be deduced from earlier results in the field of nonparametric regression with robust penalized spline estimators, such as Kalogridis and Van Aelst [21], [22], due to the more complex nature of the functional linear regression model in (1), which involves an infinite-dimensional predictor rather than a one-dimensional scalar.====The remainder of the paper is organized as follows. Section 2 introduces the proposed family of penalized estimators and discusses some popular choices of basis systems and penalties in more detail. In Section 3 we study asymptotic properties of these estimators. We show that under mild regularity conditions the estimators achieve a high rate of convergence with respect to the commonly considered prediction error. Our regularity conditions do not require the existence of any moments of the error term, allowing in effect for very heavy-tailed error distributions. Our analysis also uncovers a useful error decomposition pointing to the roles of the variance as well as the twin biases stemming from modelling and regularization. Sections 4 A Monte-Carlo study, 5 Real data example: archaeological glass vessels illustrate the competitive finite-sample performance of the proposed estimator in a Monte Carlo study and in real data. Section 6 contains a final discussion while all proofs are collected in the Appendix.",Robust penalized estimators for functional linear regression,https://www.sciencedirect.com/science/article/pii/S0047259X22000951,20 September 2022,2022,Research Article,56.0
"Bousebata Meryem,Enjolras Geoffroy,Girard Stéphane","Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France,Univ. Grenoble Alpes, CERAG, 38000 Grenoble, France","Received 28 June 2021, Revised 2 September 2022, Accepted 2 September 2022, Available online 14 September 2022, Version of Record 21 December 2022.",https://doi.org/10.1016/j.jmva.2022.105101,Cited by (2),"We propose a new approach, called Extreme-PLS, for dimension reduction in conditional extreme values settings. The objective is to find ==== of ==== that best explain the extreme values of the response variable in a non-linear inverse regression model. The ==== of the Extreme-PLS estimator is established in the single-index framework and under mild assumptions. The performance of the method is assessed on simulated data. A statistical analysis of French farm income data, considering extreme cereal yields, is provided as an illustration.","One of the main goals of statistical analysis is to seek a relationship between a response variable ==== and a ====-dimensional vector ==== of covariates starting from a ====-sample. A common way to describe the possible link is to use the regression function ====. However, in many situations, the entire conditional distribution of ==== given ==== may be of interest, rather than the only central part. This has led to the development of conditional quantiles, or regression quantiles, as an alternative to the conditional mean. Quantile regression was introduced by [42] in a parametric framework. Since then, non-parametric regression methods have been considered in the literature. Among others, [5] studied kernel and nearest neighbour estimators of conditional quantiles, while [36] focused on spline methods.====A complementary way to investigate the relationship between ==== and ==== is to focus on conditional extremes. The goal is then to describe how tail characteristics such as extreme quantiles of ==== may depend on the explanatory vector ====. One motivating example in agricultural risk management is the study of the influence of meteorological parameters (temperature, humidity, …), agricultural inputs (pesticides, fertilizers, …) and risk management tools (insurance premiums, subsidies, …) on low values of crop yields (see Section 6). Other motivations can be found in finance [49], climatology [41], hydrology [27] and environmental sciences [54], to name a few. In these applications, the estimation of extreme conditional quantiles is a crucial issue that has been studied from various points of view. A first approach is based on a parametric modelling, see for instance [11] who deals with extreme quantiles in the linear regression model and derives their asymptotic behaviour under several error distributions. Other parametric models are proposed in [10], [22], [54] using extreme-value techniques to model exceedances above a high threshold. A second line of work relies on non-parametric approaches that can be split into three main categories: fixed design, random design, and functional covariates. Fixed design methods aim at estimating conditional extreme quantiles depending on a non-random covariate, see [27] for a nearest neighbours technique. In the random design setting, one can cite [20], [31] who studied the estimation of extreme quantiles under a conditional heavy-tail model, later extended in [19] to conditional distributions belonging to any maximum domain of attraction. Finally, see [28] for the functional covariate situation. Semi-parametric approaches have also been considered for trend modelling in extreme values. Local polynomial fitting of extreme-value models is investigated in [2], [21], a non-parametric estimation of the temporal trend is combined with parametric models for extreme values in [33], and a location-dispersion regression model for heavy-tailed distributions is introduced in [1].====In a high dimensional context, ====, when the dimension ==== of ==== is large, the above-mentioned estimation methods may suffer from the so-called “curse of dimensionality”. This phenomenon results in an exploding variance of the estimators, thus impeding the inference in practice. A number of statistical approaches to dimension reduction have been introduced to circumvent this issue and to exhibit the most relevant directions in the high-dimensional covariate space. One of the most popular ones is Partial least squares (PLS), introduced by [60], which combines characteristics of Principal component analysis (PCA) for dimension reduction and multiple regression, see also [47] for an interpretation as a Krylov space method. The development of PLS has been initiated within the chemometrics field, see the reference book [48]. Since then, PLS has also received attention in the statistical literature. For example, [37] discusses the statistical properties of the PLS procedure under a factor analysis model, while [25] provides a comparison between PLS and Principal component regression (PCR) from various perspectives. See also [16] for a connection between PLS approach and envelopes and [13] for a sparse version of PLS. The basic idea of PLS is to seek directions, ====, linear combinations of ==== coordinates having both high variance and high correlation with ====, unlike the PCR method which only takes into account high variance components [25], [56]. Sliced inverse regression (SIR) [45] is an alternative method that takes advantage of the simplicity of the inverse regression view of ==== against ====. It aims at replacing ==== by its projection onto a subspace of smaller dimension without loss of regression information. Many extensions of SIR have been proposed [29] such as Partial inverse regression handling the ==== situation [46], Kernel sliced inverse regression allowing the estimation of a nonlinear subspace [61], Student sliced inverse regression dealing with heavy-tailed errors [12], Sliced inverse regression for multivariate response [18], among others. Single-index models provide additional practical tools to overcome the curse of dimensionality by modelling the non-linear relationship between ==== and ==== through an unknown link function and a single linear combination of the covariates referred to as the index, see [40, Chapter 2]. As such, they provide a reasonable compromise between non-parametric and parametric approaches. Among the numerous works dedicated to the estimation of the index and the link function, the most popular ones are the average derivative estimation method in the context of kernel smoothing [34], [52], and the M-estimation technique based on spline regression [59], [64]. One can also mention [43], [62] who considered single-index models to estimate conditional quantiles. In [50], it is proved that PLS provides a consistent estimator of the direction when ==== given ==== follows a single-index model and when ==== with a fixed dimension ====, under the additional assumption of independence between noise and covariates. From the practical point of view, PLS can perform better than SIR even though the link function is non-linear. This result is extended in [13] to the multiple-index situation and when both ==== and ==== with ====. It is also shown that, in contrast, the PLS estimator is no more consistent in the case where ====, and a sparse version of PLS is introduced to avoid this vexing effect. More recently, [15] tempered this conclusion by exhibiting some designs under which single-index PLS is consistent in the linear regression situation, when both ==== and ====, regardless of the alignment between ==== and ====.====Finally, the curse of dimensionality may also be tackled using shrinkage methods which aim at reducing the complexity of the inference by variable selection. As an example, Lasso method [57] penalizes regression coefficients similarly to Ridge regression [39] but replacing the ==== penalty by the ==== counterpart. Some extensions include Fused lasso [58] and Elastic net [66] to deal with the case where ==== is larger than ====. Many other shrinkage and variable selection methods are discussed in [35, Chapter 3].====Dimension reduction dedicated to conditional extremes is limited in the literature, and only a few recent works have been devoted to it. One can mention [26] where a dimension reduction framework adapted to conditional tail distributions is developed assuming that, when ==== is large, ==== and ==== are independent conditionally on a linear combination of the covariates. Another approach [17], [23] consists in adapting the (unsupervised) dimension reduction method PCA to the extreme setting. In [63], a semi-parametric approach is introduced for the estimation of extreme conditional quantiles based on a tail single-index model. The authors propose to estimate the dimension reduction direction ==== using local linear quantile regression. The method is developed under the tail single-index model and a conditional mean linearity assumption, which is satisfied, for instance, when ==== is elliptically distributed (the method is described in further detail in Section 5).====We introduce a new approach, referred to as extreme-PLS (EPLS), for dimension reduction in an extreme conditional setting. The underlying idea is to look for linear combinations of covariates that best explain the extreme values of ====. More precisely, we first propose a single-index approach to find a direction ==== maximizing the covariance between ==== and ==== given ==== exceeds a high threshold ====. An iterative procedure is then exhibited to adapt the method to the multiple-index situation. In practice, ==== allows to quantify the effect of the covariates on the extreme values of ==== in a simple and interpretable way. Plotting ==== against the projection ==== provides a visual interpretation of conditional extremes. Moreover, working on the pair ==== should yield improved results for most estimators dealing with conditional extreme values thanks to the dimension reduction achieved in the projection step. From the theoretical point of view, the asymptotic properties of the EPLS estimator are established under an inverse single-index model and a heavy-tail assumption, without recourse to linearity as in [63] nor independence assumptions as in [26]. It appears on simulated data that the EPLS estimator provides promising results in high-dimensional settings and outperforms the estimator proposed in [63] in a wide range of situations.====The paper is organized as follows. In Section 2, the EPLS approach is introduced in the framework of a single-index model and heavy-tailed distributions. Some preliminary properties are stated to justify the above heuristics from a theoretical point of view. The associated estimator is exhibited in Section 3 and its asymptotic distribution is established under mild assumptions. This approach is extended to the multiple-index setting in Section 4. The method’s performance is investigated through a simulation study in Section 5. EPLS approach is then applied in Section 6 to assess the influence of various parameters on cereal yields collected on French farms. A small discussion is provided in Section 7 and proofs are postponed to the Appendix. A Supplementary material is also provided to complete the simulation study. Data and R code are available at ====.",Extreme partial least-squares,https://www.sciencedirect.com/science/article/pii/S0047259X22000926,14 September 2022,2022,Research Article,57.0
"Moreva Olga,Schlather Martin","Mercedes-Benz AG, 050-B420, 71059 Sindelfingen, Germany,Institute of Mathematics, University of Mannheim, 68159 Mannheim, Germany","Received 16 April 2020, Revised 25 August 2022, Accepted 27 August 2022, Available online 9 September 2022, Version of Record 21 December 2022.",https://doi.org/10.1016/j.jmva.2022.105099,Cited by (1), and ,"Multivariate data measured in space arise in a variety of disciplines including soil science, ecology, mining, geology and meteorology. Air temperature and pressure in a certain geographical region or the content of two metals in a geological deposit are examples of spatial processes with two components. Spatial dependence within and between the components is exploited in particular when the component of interest is not exhaustively sampled, whereas the measurement of other components can be easily carried out, e.g. in soil sciences [5], [22]. An appropriate multivariate spatial covariance model gives more sensible results for spatial interpolation than univariate models, see for example [9]. In environmental and climate sciences it is important to model spatial meteorological data jointly in order to reflect spatial dependence within and between components adequately (see the discussions in [7], [12], [13]); otherwise the obtained results might be unsound.====Spatial data are assumed to stem from a multivariate Gaussian random field ====, ====, ====, which is uniquely characterized by its mean and its covariance function. For simplicity, we assume that the random field has zero mean. A covariance function ==== of a multivariate field is a matrix-valued function, whose diagonal elements are the marginal covariance functions and the off-diagonal elements are the cross-covariance functions. A covariance function ==== is called stationary if for any ==== and ==== it holds: ====The matrix ==== is stationary and isotropic if additionally ==== whenever ====, i.e., the marginal and cross-covariance functions depend only on the distance between the locations. Hereinafter we write ==== instead of ==== with  ====, whenever ==== is stationary and isotropic.====We recall that a covariance function must be positive definite, i.e. it guarantees that the variance of an arbitrary linear combination of observations of any involved components ====, ====, taken at arbitrary spatial locations is nonnegative. That is, for any ====, ====, and ==== it must hold: ====A comprehensive overview of covariance functions for multivariate geostatistics is found in [14], [43]. Among these models is the linear model of coregionalization [23], [46]. Although it is widely used by practitioners, it lacks flexibility; its limitations are discussed in [18]. Models with compact support are introduced in [10], [11], [41], [44]. The properties of multivariate random fields in the frequency domain are studied in [30]. A conditional approach for constructing multivariate models is developed in [9]. In this paper we restrict our attention to stationary and isotropic bivariate (====) models, whose components stem from the same family, i.e., to models of the form ====where ==== is the variance of the field ====, ==== is a continuous univariate stationary and isotropic correlation function, which depends on a scale parameter ====, ====, and another optional parameter ==== with ==== (e.g., smoothness, long range behaviour). Necessarily, ====. Note that isotropy implies ====. For instance, the multivariate Matérn model [2], [18] is a member of this class with ====where ==== is a scale parameter, ==== is a smoothness parameter and ==== is a modified Bessel function of the second kind.====The class given by (1) also can be seen as a generalization of the class of separable models introduced by [33], where a multivariate covariance function factorizes into a product of a covariance matrix ==== and a univariate correlation function ====, i.e. ====That is, a separable model assumes that all components share the same spatial correlation structure and differ only in their variances. In particular, the scale parameter is the same for both marginal and cross-covariance functions. The class (1) is more flexible allowing each field to have distinct smoothness, scale, and variance parameters and admitting flexible cross-correlation between the fields. Given a univariate correlation function ====, our goal is to find the parameter sets for which the function ==== in (1) is a covariance function. Clearly, if the components are uncorrelated, i.e., ====, then ==== is always a bivariate covariance function. Thus, we are interested in ====. Furthermore, if ==== then ==== is also sufficient.====It is worth pointing out that not all univariate models can be generalized to non-trivial multivariate models in a direct way. For example, the univariate spherical model, ====, ====, is widely used in geostatistics, but its bivariate generalization ====with ====, ====, ====, is a valid covariance model in ==== if and only if ==== or ====. This follows from the multivariate version of Schoenberg’s theorem [45], [48] and the fact that the spectral density of the spherical covariance function is a pseudo periodic function with an infinite number of zeros, see Appendix A for details. Of course, any convolutional approach for the cross-covariance function including the marginal covariance functions as factors is a promising candidate for a non-trivial model. Examples are given by [11], where the cross-covariance function stays constant for ==== below a certain threshold, and the delay effect in [46].====Genton and Kleiber [14] pose the question, how to characterize a parameter set of the valid multivariate powered exponential (or stable) model. In Section 2 we give a partial answer to this question, providing sufficient conditions for the positive definiteness of the bivariate model based on Pólya type conditions. In a similar way we can also formulate sufficient conditions for the positive definiteness of the bivariate generalized Cauchy model. The models are flexible, intuitive and easily interpretable: in both models three parameters characterize the smoothness of the covariance functions of process components and the cross-covariance functions. Further three parameters model the long-range behaviour in the bivariate generalized Cauchy model. The smoothness parameters of the marginal covariance functions in both models are restricted to values in ====, similarly to the application of Pólya criterion in the corresponding univariate models. We focus on a Euclidean space, ====, ====.",Bivariate covariance functions of Pólya type,https://www.sciencedirect.com/science/article/pii/S0047259X22000902,9 September 2022,2022,Research Article,58.0
"Saulo Helton,Vila Roberto,Cordeiro Shayane S.,Leiva Víctor","Department of Statistics, Universidade de Brasília, Brasília, Brazil,School of Industrial Engineering, Pontificia Universidad Católica de Valparaíso, Valparaíso, Chile","Received 3 July 2022, Revised 21 August 2022, Accepted 21 August 2022, Available online 28 August 2022, Version of Record 1 December 2022.",https://doi.org/10.1016/j.jmva.2022.105097,Cited by (5),"A sample selection bias problem arises when a variable of interest or response is correlated with a latent variable. This problem is presented when the response variable has part of its ==== distribution, which has appealing statistical properties. In this article, we introduce an extention of the Heckman sample selection model to the wide class of ==== model, as a special member of the family of symmetric Heckman models, is analyzed. ==== are performed to assess the statistical behavior of the estimation method. Two real data sets are analyzed to illustrate our results.",". The author proposed a sample selection model by jointly describing the response and latent variables.====). Some studies on Heckman models were conducted by Nelson ====, Paarsch ====, Manning et al. ====, Stolzenberg and Relles ====, and Leung and Yu ====. These studies suggested that the Heckman model can reduce or eliminate selection bias when the assumptions hold. However, deviation from the normality/Gaussianity assumption may distort the information obtained from this model.====The Gaussianity assumption of the Heckman-normal model (Heckman ====) was relaxed to more flexible models using the Student-====
 (Marchenko and Genton ====, Ding ====, Lachos et al. ====), skew-normal (Ogundimu and Hutton ====), and Birnbaum–Saunders (Bastos and Barreto-Souza ====) distributions.====The Heckman-normal model states that the dispersion and correlation (sample selection bias parameter) are constant, which might not be adequate in diverse empirical situations. To the best of our knowledge, the bivariate Heckman sample selection model has not been extended to the wide class of ====. In addition, modeling of varying dispersion and correlation is also of interest in the bivariate symmetric Heckman distribution.====In this context, the objective of the present investigation is to propose bivariate Heckman sample selection models based on symmetric or elliptically contoured distributions (Fang et al. ====, and of the Heckman-Student-==== model presented by Marchenko and Genton ====, which was also analyzed by Lachos et al. ====. We show that the proposed bivariate symmetric Heckman models outperform the classical Heckman-normal, generalized Heckman-normal, and Heckman-Student-====The rest of the present article proceeds as follows. In Section ====, we introduce the bivariate symmetric Heckman models, including some of their mathematical properties. Section ==== provides the related conditional distributions. In Section ====, the parameters of Heckman models are estimated with the ML method. In Section ====, we analyze the generalized Heckman-Student-==== model, which is a member of the family of bivariate symmetric Heckman models. Section ==== presents a Monte Carlo simulation study for evaluating the statistical performance of the associated ML estimators. In Section ====, the new bivariate models are applied to two real data sets to show their usefulness. Finally, in Section ====, we state some concluding remarks.",Bivariate symmetric Heckman models and their characterization,https://www.sciencedirect.com/science/article/pii/S0047259X22000896,28 August 2022,2022,Research Article,59.0
"Aubin Jean-Baptiste,Bongiorno Enea G.,Goia Aldo","INSA-Lyon, France,Dipartimento degli Studi per l’Economia e l’Impresa and AI@UPO, Università del Piemonte Orientale, Italy,Gruppo Nazionale per l’Analisi Matematica, la Probabilità e le loro Applicazioni, Istituto Nazionale di Alta Matematica, Italy","Received 19 February 2021, Revised 8 July 2021, Accepted 20 October 2021, Available online 3 November 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104891,Cited by (0),"This work proposes an analysis of the correction term appearing in a Small-Ball ==== ==== are provided. Finally, in the context of reconstructing a sample of curves by truncated Karhunen–Loève expansion, a local approach to select the dimensionality is illustrated through numerical and real data examples.","In the last two decades, functional statistic has consolidated, reaching increasingly deeper levels of maturity along different directions. As evidence of this, there are several monographs [17], [21], [22], [24], [30], surveys [12], [27] and special issues [1], [2], [15], [19] that especially in recent years have enriched the literature on this topic. Nevertheless, in the frenzied rush to explore the enormous potential of functional statistics in the applications, different aspects has been left underdeveloped. Among these, an overlooked tool is the small–ball probability (SmBP) that, for a random element ==== of a metric space ====, is ==== as ==== where ==== is the ball centered at ==== with radius ====. The main difficulty, which has limited the exploitation of SmBP in applications, consists in transforming a purely theoretical concept into a useful operative instrument. In fact, until recently, the use of the SmBP has been limited to the evaluation of the convergence rates of the estimators emerging from functional regression problems (see [17], [28] and references therein) and, only in the last decade, few works have given to the SmBP an active role in applications as well. The efforts made so far are closely related to the possibility of factorizing the SmBP, for example assuming that, for any ==== and as ====, ====with the identifiability constraint ====. The convenience of the above factorization resides in the fact that it isolates the way the SmBP depends upon ==== and ==== (the space and volumetric variables respectively) and allows to interpret ==== and ==== as a surrogate of the probability density of ==== (that cannot be straightforwardly defined in an infinite dimensional space) and a volumetric term respectively. This interpretation of the factorization (1) strongly resembles the multivariate case and has led to define a concept of mode (see [18]), to introduce a surrogate-density (see [16]) and to evaluate the intrinsic complexity or dimension of the underlying process ==== (see [8], [9], [10]).====Although the factorization (1) appears very attractive from both a theoretical and a practical point of view, it is known that it holds true only for some processes (see [25], [26] and references therein) and there is no global theory that assesses its validity for a general process ====. To compensate for this lack, some alternatives have been proposed (see [6], [13]).====In this paper, a three terms factorization proposed in [6] is considered: given ====, a separable Hilbert space, as ====, ====where ==== is the pdf of the first ==== principal components, ==== is the volume of the ====-dimensional ball of radius ==== and ====, depending on ====, ==== and ==== denotes a suitable correction factor. While ==== and ==== maintain the same meaning of ==== and ==== respectively in (1), making possible the use of ==== in density based techniques (e.g., [5], [23] and references therein), the factor ==== has never been analyzed or used for statistical purposes.====The aim of this work is to fill this gap deepening the knowledge of ==== to understand what role it plays in factorizing the SmBP, how it can modify the assessments made on the process and/or on its SmBP and, finally, how it can be exploited in the applications in identifying a local dimension to parsimoniously represent ====. At the first glance, looking at the factorization (2), ==== provides a compensation for the use of the finite dimensional factorization ====; in particular, the closer the correction factor and zero are, the worse the factorization ==== is in approximating the SmBP. This implies that, instead of using the sole ==== in the density based statistical applications, one should employ its adjusted version ====. Going deeper, this work studies the behavior of ==== varying ==== by means of some bounds. For some processes, it is proven that ==== reaches its maximum over all the points in the space ==== generated by the first ==== eigenfunctions of the covariance operator of ====. These latter results allow to interpret ==== as a local measure of the quality of the representation of ==== as an element of ====. As a consequence, in this paper the correction factor is used to customize the dimension used in the truncated Karhunen–Loève representation of each curve of a given sample defining a novel local reduction dimensionality principle that can be seen as an alternative to the well known fraction of explained variance which leads to a unique dimension for all the curves. Further, the average behavior of ==== is also studied, leading in a natural way to characterize finite dimensional processes.====To make ==== usable in practice, a kernel–type nonparametric estimate is provided exploiting the fact that, by its own definition, ==== is a conditioned mean. For this estimator, it has been proved that the rate of convergence in quadratic mean is the optimal one. As a by-product, similar results are derived for the surrogate density ==== and its adjusted version ====. By using the proposed nonparametric estimate of the correction factor, a local reduction dimensionality algorithm is proposed, implemented and its performances are analyzed by means of a Monte Carlo simulation study. Finally, the new technique is applied to two real datasets.====The outline of the paper goes as follows. Section 2 formally introduces factorization (2). Section 3 discusses various aspects of the correction factor proposing some interpretations, properties and potential uses. Section 4 studies nonparametric estimators for ====, ==== and ====. Section 5 introduces the local reduction dimensionality algorithm which is tested by means of simulations. An application to a neuronal dataset is illustrated in Section 6. An other application to a well-known dataset and some technical results can be found in the Supplementary material.",The correction term in a small-ball probability factorization for random curves,https://www.sciencedirect.com/science/article/pii/S0047259X2100169X,3 November 2021,2021,Research Article,64.0
"Horváth Lajos,Rice Gregory,Zhao Yuqian","Department of Mathematics, University of Utah, Salt Lake City, UT 84112–0090, USA,Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, Canada,Essex Business School, University of Essex, Colchester, CO4 3SQ, United Kingdom","Received 3 January 2021, Revised 21 April 2021, Accepted 20 October 2021, Available online 2 November 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104877,Cited by (5), in this context generally improves performance over existing methods. These new statistics are demonstrated in an application to detecting changes in the volatility of high resolution intraday asset price curves derived from oil futures prices.,"Functional data analysis has emerged as a vibrant area of research in statistics over the past several decades, owing to the multitude of data now collected, often at a high resolution, over a continuum. Such data can be viewed as discrete observations from functional data objects taking values in a function space. In a number of examples of interest, functional data objects are obtained sequentially as functional time series. We refer the reader to [22], [29], [39], [43] for textbook length treatments of functional data analysis, and [19] for a survey of modern research topics. Seminal work on functional time series analysis is summarized in [17], and further reviewed in [38], [44].====In the setting of functional time series, one often encounters series of curves exhibiting nonstationarity that appears as “shocks” or structural changes in the data generating mechanism. A simple model for such data is a change point model in which various features of the series are allowed to change at unknown points over the observation period; see [32] for a survey of change point analysis. Inference for change points in the level or mean function with functional data has been avidly studied recently. [5], [6], [14], [23] consider mean change point analysis of serially independent functional data, and many of these methods were extended to cover potential serial dependence in [4], [10], [46]. [12] develops a test for changes in the mean of a functional time series evolving according to heteroscedastic functional factor model.====Many change points appear though as changes to the underlying variance or covariance structure, rather than level shifts in the mean. Change point analysis for the variance and covariance matrices of scalar or vector valued observations enjoys an enormous literature going back to at least [33] in the scalar case, and more recent references for change point analysis of the covariance matrix include [8], [13], [24], [49], [50]. The problem of conducting change point analysis for the covariance function or operator describing the second order behaviour of functional data has been comparatively less explored. [34] is apparently the first to consider the change point detection problem for the covariance operator of independent functional data, and their approach is based on an initial dimension reduction step using functional principal component analysis. [48] generalized these detection methods as well as several other dimension reduction based approaches to allow for general forms of serial dependence. [11], [20] consider change point inference under similar weak dependence conditions for the spectrum and eigenfunctions, respectively, of covariance operators. Evaluating potential changes in the covariance operator is of particular interest in functional data analysis, since the covariance operator is central to a wide range of dimension reduction approaches for functional data, as described in the prior surveys [1], [26].====Most closely related to the present paper are [47], who develop change point detection methods for the covariance operator based on norms of a suitably constructed functional cumulative sum (CUSUM) process under general weak dependence conditions. The recent preprint [35] considers change point detection as well as estimation using similar norm-based techniques and conditions to [47], with applications to functional data objects derived from rat brain studies. A notable feature of each of these procedures is that they are based on norms of the standard CUSUM process. As is well known in general with change point analysis, detection and estimation can be improved when change points are located away from the middle of the sample by applying weights to the standard CUSUM process in order to make the “variance” of the process more comparable at each potential change point; see e.g. [31] for a detailed discussion. Although it is conceptually simple to apply such weights, the application in this context encounters some technical challenges in that suitable weighted approximations for the CUSUM process of random elements in general infinite dimensional Hilbert space are not available.====In this paper, we develop and study change point detection and estimation procedures for the covariance operator based on the norms of weighted functional CUSUM processes. In the absence of a change point we establish the asymptotic distribution of a change point detector based on integrating such processes across the partial sample parameter under general weak dependence conditions similar to those considered in [47], [48], and we further derive consistency and local asymptotic results for this detector in the presence of a change in the covariance function. Additionally, we show that the natural change point estimator based on such processes is rate optimal for estimating an existing change point, and further is asymptotically distributed as the argument maximum of a Gaussian process under a local asymptotic framework. In place of suitable weighted approximations, we establish Hájek–Rényi style inequalities for the norms of partial sample estimates of the covariance function, which underpin these results. We study the detector and change point estimator in a small simulation study to detect changes in the covariance of functional autoregressive and generalized conditionally heteroscedastic processes, which demonstrate that the use of the weighted CUSUM statistics in this context generally improves performance over existing methods. These new statistics are demonstrated in an application to detecting changes in the volatility of high resolution intraday asset price curves derived from oil futures prices.====The rest of the paper is organized as follows. In Section 2 we introduce the change point model and assumptions, define the weighted CUSUM change point detector, and detail its asymptotic behaviour. Section 3 details some specific examples, functional linear processes, in which changes in the covariance function arise and can be quantified based on changes in the model parameters. We define and present the asymptotic properties of the change point estimator in Section 4. The results of the Monte Carlo simulation study and analysis of intraday asset price curves are contained in Sections 5 Simulation results, 6 Detecting covariance changes in Crude oil intra-day return curves, respectively. All technical details and some concluding remarks follow these sections.",Change point analysis of covariance functions: A weighted cumulative sum approach,https://www.sciencedirect.com/science/article/pii/S0047259X2100155X,2 November 2021,2021,Research Article,65.0
"Lee Sangyeol,Meintanis Simos G.,Pretorius Charl","Department of Statistics, Seoul National University, Seoul, Republic of Korea,Department of Economics, National and Kapodistrian University of Athens, Athens, Greece,Pure and Applied Analytics, North-West University, Potchefstroom, South Africa,Department of Probability and Mathematical Statistics, Charles University, Prague, Czech Republic","Received 20 February 2021, Revised 30 September 2021, Accepted 20 October 2021, Available online 2 November 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104892,Cited by (0),We consider model-free monitoring procedures for strict ,"The notion of stationarity plays a very important role in statistical modeling. In its weakest form of first or second order stationarity it implies that the mean or second moment, respectively, are time invariant; see for instance  Xiao and Lima [87], Dwivedi and Subba Rao [28], and Jentsch and Subba Rao [56]. A more general related notion is that of ====th order (weak) stationarity which requires that all joint product moments of order (up to) ==== are time invariant. Most studies of stationarity are restricted to some form of weak stationarity, which of course is the most suitable concept for linear time series. On the other hand, the property of strict stationarity states that not only moments, but the entire probabilistic structure of a given series is time invariant. This property is of great relevance with non-linear time series in which low order moments are not sufficient for the dynamics of the series, not to mention the case of heavy-tailed time series lacking higher moments [2], [65]. Another divide is between parametric and non-parametric tests for stationarity, with the first class containing the majority of procedures in earlier literature. There is also the methodological approach that categorizes methods which operate either in the time or in the frequency domain. As the existing literature is vast, we provide below only a selected set of references which is by no means exhaustive.====In econometrics the majority of earlier tests for weak stationarity are either tests for stationarity against unit root, or alternatively, tests of a unit root against stationarity, with the KPSS and the Dickey–Fuller tests being the by far most popular ones and having enjoyed a number of generalizations; see for instance Gil-Alana [36], Giraitis et al. [37], and Müller [72]. In this connection L. Horváth and co-workers have put forward such generalizations for testing strict stationarity, with functional data [47], for monitoring stationarity against certain “mild” alternatives [48], as well as extensions of CUSUM-based methods tailored for detection in the correlation structure of multivariate volatility models [12]. On the other hand Dette and Wu [26] and Dette et al. [27], investigate the issue of break detection in a given aspect of the model, which allows for potential non-stationarity of other (nuisance-type) aspects of the same model under the null hypothesis. For a general review of available change point methods we refer to Horváth and Rice [49].====When it comes to testing for strict stationarity in parametric time series there exist many tests. These tests typically reduce to testing for a given portion of the parameter space and may often readily be extended to monitoring procedures. We indicatively mention here the works for ARMA, GARCH, and DAR models by Bai [9], Francq and Zakoïan [34] and Guo et al. [41], respectively, while Na et al. [73] suggest monitoring methods for the autocovariance function within a more general class of linear processes. On the other hand testing methods for strict stationarity are scarce when performed within a purely nonparametric framework. In particular Kapetanios [58] suggests a smoothing technique for testing strict stationarity of the marginal distribution of an arbitrary time series, while Busetti and Harvey [21] and Lima and Neri [64] test for constancy (of a discretized version) of the marginal quantile process.====The interest in testing for stationarity rests on the fact that modeling, predictions and other inferential procedures are invalid if this assumption is violated. However, although strict stationarity is widely assumed in the literature, it is not truly a realistic assumption when one observes a given time series over a long period of time. This is even more so with continuously observed data such as functional time series whereby it is expected that institutional changes cause structural breaks in the stochastic properties of certain variables, particularly in the macroeconomic and financial world. Nevertheless a sizeable corpus of the existing statistical methodology for functional observations and most importantly prediction methodology, is tailored to stationarity; see for instance Antoniadis et al. [4], Aue et al. [6], Bosq and Blanke [16], and Panaretos and Tavakoli [74]. Probably motivated by the serious concern for the existence of structural breaks in continuously observed curves, and apart from [47], testing procedures for stationarity have been developed by Aue et al. [7], Bandyopadhyay and Rao [11], and Bandyopadhyay et al. [10], for spatial or spatio-temporal data. For a wider review of recent developments on functional data analysis the reader is referred to the editorials Goia and Vieu [38], Aneiros et al. [3], and González-Manteiga and Vieu [39] and the papers in these three special issues, and the volumes Bosq [15], Ramsay and Silverman [80], Ferraty and Vieu [33], Ferraty [32], Horváth and Kokoszka [46], and Kokoszka and Reimherr [61].====In view of the preceding discussion, monitoring the stationarity of a stochastic process seems to be of an even greater importance than testing. Motivated by this fact, in this paper we propose a sequential procedure for strict stationarity. Our approach is in the spirit of Hong et al. [45] that use the characteristic function (CF) as the main tool for testing strict stationarity. Note in this connection that several stochastic properties, including stationarity, may conveniently be expressed by means of either the distribution function (DF) or the CF, and that both these functions may be estimated purely non-parametrically without the use of smoothing techniques. However, and while the DF has also been used for detecting structural change in distribution, see for instance Kojadinovic and Verdier [60], a non-parametric estimate of the vectorial CF is easier to obtain than in the case of the joint DF, and moreover this estimate, unlike its DF counterpart, is continuous in its argument. These features offer specific theoretical and computational simplifications which are particularly important when dealing with vectors of observations, and are clearly reflected in the competitiveness of the resulting methods over classical ones; see for instance Pinske [77], Su and White [83], Hlávka et al. [43], and Matteson and James [68]. We refer the interested reader to Meintanis [69] for a general review of testing procedures based on the CF.====On the other hand, passing from multivariate to Fourier methods for functional observations requires to work with the characteristic functional which is the natural extension of the notion of the CF from finite dimension to, say, a Hilbert space setting. While such methods have already been realized in different testing contexts (see for instance Lyons [66], Jiang et al. [57], Hlávka et al. [42], and Meintanis et al. [71]), in this work we will restrict treatment to finite dimension and postpone the study of a monitoring procedure for functional observations to future work.====The remainder of the paper is as follows. In Section 2 we introduce the basic idea behind the proposed procedures. Section 3 presents the corresponding detector statistics and in Section 4 we study the large sample behavior of the new methods. As the limit null distribution of our detector is complicated, we propose in Section 5 a resampling procedure in order to actually carry out the suggested monitoring method. The results of a Monte Carlo study for the finite-sample properties of the methods are presented in Section 6. Some real-world empirical applications to financial market data are presented and discussed in Section 7. Finally, we end in Section 8 with conclusions and discussion. The proofs of the lemmas and theorems in Section 4 are all provided in the Appendix, while some extra numerical results are included in the accompanying Supplement.",Monitoring procedures for strict stationarity based on the multivariate characteristic function,https://www.sciencedirect.com/science/article/pii/S0047259X21001706,2 November 2021,2021,Research Article,66.0
Fujikoshi Yasunori,"Department of Mathematics, Hiroshima University, Higashi-Hiroshima, Japan","Received 18 October 2021, Accepted 18 October 2021, Available online 2 November 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104860,Cited by (3)," models. The KOO methods considered are mainly based on general information criteria, but we take up also KOO methods based on some other selection methods. Some references are given for high-dimensional consistencies in some other multivariate models.","It is important to consider selection of variables in multivariate analysis. One of the approaches is to first consider variable selection models and then apply model selection criteria such as ====, ====, ====. The AIC and BIC are to find the model which minimizes ====where ==== is the maximum likelihood, ==== is the penalty term, and ==== is the number of unknown parameters. For ==== and ====, ==== is defined by ==== and ====, respectively, where ==== denotes the sample size. ==== is defined by using the mean squared error instead of ====.====In the selection of ==== variables ====, we identify ==== with the index set ====, and denote ==== for subset ==== by ====. Then, the model selection based on ==== chooses the model ====Here the minimum is usually taken over all subsets. It has been pointed out that there are computational problems for ==== methods, for like ====, ==== and ==== methods, since we need to compute ==== statistics for the selection of ==== variables. To avoid this computational problem, Nishii et al. [11] proposed a method which is essentially due to Zhao et al. [26]. The method, which was named the knock-one-out (KOO) method by Bai et al. [1], determines “selection” or “no selection” for each variable by comparing the model afters removing a variable and the full model. More precisely, the KOO method chooses the model or the set of variables given by ====where ==== is the set obtained by removing element ==== from the set ====.====In a large-sample setting, Nishii et al. [11] studied strong consistency of ==== and ==== in discriminant analysis, canonical correlation analysis and multivariate calibration analysis. It is well known that ==== is consistent, but ==== is not consistent. On the other hand, from some recent work in multivariate regression models by Fujikoshi et al. [7], Yanagihara et al. [25], and Bai et al. [1], it is known that if the ==== selection rule is consistent so is the ==== selection rule, but not vice versa. There are considerably many results on the KOO methods besides these high-dimensional consistency results, but the purpose of this paper is to review only the recent of the latter. Though we consider mainly the KOO methods based on general information criteria, we consider also the ones based on some other selection methods. Some results in multivariate regression models have been studied for strong consistency under nonnormality by Bai et al. [1]. However, since their results are under revision, in the present paper we discuss only results on weak consistency under normality.====The remainder of the present paper is organized as follows. In Section 2, we present KOO methods based on a general information criterion in a multivariate regression model, as well as KOO methods based on some other criteria. In Section 3, we present KOO methods based on a general information criterion for selection of variables in discriminant analysis. The methods are discussed in two-group and multiple group cases separately. In Section 4, we briefly discuss selection of variables in some other models.",High-dimensional consistencies of KOO methods in multivariate regression model and discriminant analysis,https://www.sciencedirect.com/science/article/pii/S0047259X2100138X,2 November 2021,2021,Research Article,67.0
"Chen Di-Rong,Cheng Kun,Liu Chao","School of Mathematical Sciences, Beihang University, Beijing 100191, PR China,Department of Statistics and Data Science, The Southern University of Science and Technology, Shenzhen 518055, PR China","Received 26 February 2021, Revised 23 August 2021, Accepted 20 October 2021, Available online 1 November 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104895,Cited by (1)," based on discretely observed data is important in functional data analysis. In this paper, we propose a framelet block thresholding method for the case of sparsely observed functional data. The procedure is easily implemented and the resultant estimators are represented as explicit ====-spline expressions. For sparsely observed functional data, we establish, under some mild conditions but without knowing the smoothness parameter, convergence rates of mean integrated squared errors for mean and covariance estimators respectively. In particular, the mean estimator attains ==== optimal rate. The simulated and real data examples are provided to offer empirical support of the theoretical properties. Compared to the existing methods, the proposed method outperforms in adapting automatically to local variations.","Functional data analysis (FDA) refers to the statistical analysis of data samples consisting of random functions or surfaces. We refer to the monographs [4], [20], [26], [27], [31], [35], [38]. Recent developments in FDA are illustrated in the articles [1], [13], [14], [22], [34], [36], [39].====In the context of FDA, the estimation of mean and covariance functions plays a prominent role, and various methods have been developed to address a variety of practical issues involved in this topic. Typically, we have ==== random functions ====, and each function is observed at discrete ==== time points. In [40], the authors constructed the local linear smoothers (LLSs) with equal weight per observation (OBS) and obtained a sub-optimal uniform convergence rate. This was improved in [33] by using a different weighing scheme, i.e., equal weight per subject (SUBJ). In [37], a linear wavelet method was introduced to estimate mean and covariance functions of longitudinal data. Among others in [6], the authors proposed a regularized method under reproducing kernel Hilbert space framework to estimate covariance function. Moreover, a smoothing spline estimator was constructed in [7] for mean estimation, and the minimax bounds of estimators in ==== norm were established for types of functional data. Recently, a comprehensive work by [42] investigated LLSs with general weighing schemes and provided a unified result.====In these studies, the sparsity in the observed grids has received substantial attention, where all ==== are bounded away from infinity or follow a fixed distribution. The sparsity effects appear under many guises in FDA and have broad meanings [1], [2], including sparsity in the functional space [41], sparsity in the model [3] and sparsity in the grids [42], in which the third one is specific to the functional setting. In this paper, we consider the sparsity in the grids, and the goal is to estimate the mean and covariance functions based on the sparse and irregular data, which arises frequently in longitudinal studies [16]. Note that only limited information is given by the sparse observations from per subject in this case. To conduct inference effectively, observations from all subjects are pooled to borrow information from each other, so that the pooled time points are sufficiently dense and nonparametric methods can be further applied.====While various methods have been proposed to estimate mean and covariance functions in FDA, most studies focus on the estimation where the smoothness parameters are assumed to be known, that is, the estimators typically depend on the smoothness parameters. It was mentioned in [37] that the author would present in next papers about the adaptive results for mean and covariance estimations of bounded processes and Gaussian processes by applying wavelet shrinkage. However, we have not been able to aware of any such paper of the author’s. In addition, it is notable that the boundedness or Gaussian condition on random functions is somewhat restrictive. Hence this makes adaptation essential for the nonparametric estimation of mean and covariance functions under some mild conditions, especially when the smoothness parameters are unknown in practice. In the present paper, we construct adaptive estimators which automatically adjust to the smoothness properties of the underlying mean and covariance functions, and only some standard conditions stated in Section 3 are required. The main technical tools in the construction of estimators are multi-resolution analysis of wavelet frame and block thresholding rules.====Wavelet methods have demonstrated considerable success in many areas of finitely dimensional data analysis and processing, including nonparametric regression and density estimation. They possess the spatial adaptation property in the sense that they can adapt readily to different degrees of smoothness and can neatly capture short and sharp aberrations of signals. This spatial adaptation property is shared by the local polynomial method with data-driven variable bandwidth, see [19]. Standard wavelet approach thresholds the empirical coefficients term by term, and the estimator is within a logarithmic factor of the optimal convergence rate over a wide range of Besov classes, as shown in [17], [18]. Furthermore, a block thresholding rule was considered in [23], [24] for wavelet estimation which thresholds empirical wavelet coefficients in groups rather than individually. The estimator attains the exact minimax rate of convergence without the logarithmic penalty over a range of perturbed Hölder classes. Besides, [9] proposed a data-driven block thresholding procedure for wavelet regression. There is a great deal of literature about wavelet-based approach to FDA. However, most of the studies focus on densely observed functional data, see, e.g., [10], [21], [32], [43].====In this paper we propose estimators of mean and covariance functions with framelet block thresholding method. Due to the desirable features of redundancy and flexibility over orthogonal wavelets, tight framelets are useful in applications, see [25]. While closed analytic representations of wavelets and their generators are often not available, spline tight framelets with short supports and high approximation orders are systematically constructed in [15]. Such framelets are more appropriate for sparse and irregular data since the empirical framelet coefficients are easily computed. Hence, the procedure is easily implemented and the resultant estimators are represented as explicit ====-spline expressions. To our knowledge, the wavelet block thresholding method has not been used to estimate mean and covariance functions of functional data in the literature.====The most related work to ours is [37], in which a linear wavelet procedure was presented to estimate mean and covariance functions of longitudinal data with the prior knowledge of smoothness parameters. While the convergence rates are the same to ours, the method in the current paper is different from the linear wavelet method. In aid of block thresholding rules, the constructed estimators are highly adaptive, that is, they do not depend on the regularity parameters of target functions and achieve optimality results simultaneously over a collection of parameter spaces. Similar advantages are also illustrated in classical nonparametric estimation, see, e.g., [5], [23], [24]. Recently, [8] introduced a block thresholding approach to functional principal components in functional linear regression. Although our estimators are easily implementable, a more profound tool, the concentration inequality for suprema of empirical processes, is needed to derive the asymptotic properties, and the analysis is much more involved.====Although inspired by wavelet block thresholding for finitely dimensional data, the problems arisen in FDA pose essential difficulties. To be specific, the high correlation of data from the same subject, which is usually not an issue in classical nonparametric regression, and the unboundedness of data disturbs the control of deviation in blocks, making the problems more complicated due to the functional nature of observations. With the help of the compactly supported properties of framelets, we solve these technical problems by truncating the observations into bounded interval. More details will be provided in next sections. In this regard, it indicates that the compactly supported framelet is a competitive candidate in FDA.====In this paper, we obtain, under some mild conditions, the orders of convergence rates ==== in ==== sense for sparse functional data, where ==== are the probably unknown smoothness parameters of mean and covariance functions, respectively. The numerical performances of the proposed estimators are investigated by simulations and a real data analysis, and the comparison with the existing methods is also illustrated in Section 4. The results show the proposed estimators are competitive with the others and demonstrate the advantage of ours in estimating sharp variation, because we have removed the over-smoothing feature of the more conventional approach. It is also observed in the examples that only about 0.2% empirical framelet coefficients retain with block thresholding rules. This implies potentially significant computational savings and scalability to data of large dimensions/sizes.====To summarize, the main contributions of this work are three-fold. (i) We develop a new technique, framelet block thresholding method, to handle the sparse and irregularly sampled functional data. The new method provides a class of effective and easily implementable estimators for mean and covariance functions. (ii) It is proved that the constructed adaptive estimators attain the optimal nonparametric rates for sparse functional data. (iii) In contrast to the existing methods, the proposed estimators adapt to local features of signal by applying tight framelets, which is verified via numerical studies. Besides, the new method is less restrictive to smoothness assumption for the ==== convergence rate, i.e., only an ==== smoothness of target functions rather than the ==== smoothness as in [42] is needed.====The remainder of this paper is organized as follows. In Section 2, the model and framelet block thresholding method are introduced. Section 3 discusses asymptotic properties of the estimators and provides some remarks about the estimation procedures. We provide the finite sample performances through simulation studies and analyze a longitudinal CD4 cell data in Section 4. Section 5 concludes the paper. More theoretical results and technical proofs are relegated to Section 6 and the Appendix.",Framelet block thresholding estimator for sparse functional data,https://www.sciencedirect.com/science/article/pii/S0047259X21001731,1 November 2021,2021,Research Article,68.0
"Castrillón-Candás Julio E.,Kon Mark","Department of Mathematics and Statistics, Boston University, 111 Cummington Mall, Boston, 02215, MA, USA","Received 19 February 2021, Revised 9 June 2021, Accepted 21 October 2021, Available online 1 November 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104885,Cited by (2)," and on a spherical domain ====. However, the method is flexible, allowing the detection of orthogonal signals on general topologies, including spatio-temporal domains.","In this paper an orthogonal and new direction in anomaly detection is framed in the context of functional analysis and tensor product representations such as the multivariate Karhunen–Loève (KL) [23], [24], [38], [46] expansion, on complex topologies. This method is very different from the previous approaches — KL expansions are an important method for representing stochastic processes and random fields, forming optimal tensor product representations. Due to the generality of this approach, a large class of processes and fields over complex geometrical domains can be represented with high accuracy. In particular the approach is not limited to time series problems, and can be applied to detect anomalies both in the temporal and spatial domains. Detection is achieved by constructing nested subspaces adapted to eigenspaces of truncated KL expansions.====There are significant connections between this work and traditional and more recent Functional Data Analysis (FDA) techniques. In particular, the major underlying commonality involves the application of Functional Principal Components (FPC), also known as the KL expansion, to data representations [27], [32]. FPC is considered fundamental to many FDA methods. We refer the reader to the detailed surveys of FDA techniques and applications in [2], [21]. However, to our knowledge the univariate form of FPC has been its primary focus. In the book of Horvath and Kokoszka  [27] KL expansions are significantly introduced, though only on the unit interval. In a more recent book [32], Kokoszka and Reimherr formulate this in a more general Hilbert space context, though in applications it appears that only the univariate case (in time series) is used, and more complex topologies are not discussed. In general it appears that many publications restrict their discussion of the FPC to the case of (univariate) time series. In a recent publication for example,  [20] the authors develop a data reduction technique for high dimensional functional time series forecasting by expanding each entry of a data vector with a univariate FPC. In fact, this example can benefit from applying the vector functional form of the KL expansion, [24], [38], [46] which can produce an optimized vector function representation.====In [27], [32] the authors present detailed applications that include functional linear models and functional autoregressive models, among others; change (point) detection using autoregressive models is also described in detail. These approaches serve to track changes in the autoregressive kernel with respect to principal components of the observations. The accuracies of these approaches will be subject to the appropriate choices of the models. Moreover, to our knowledge most of these applications are again limited to scalars or univariate functional vectors with respect to time. In contrast to current FDA/statistical approaches, we do not use functional models per say but construct function spaces from the data and quantify hidden phenomena in terms of projections onto these spaces. Due to its foundation in functional analysis of tensor product expansions, our extended approach has many useful properties that are well suited for detection of hidden phenomena on complex domains, in particular:====This work lies at the intersection of the fields of probability, functional analysis, statistics and computational applied mathematics.====The topic of anomaly and change detection has been studied in many different statistical contexts and has received much consideration. The scan statistic approach [3], [4], [14], [22], [33], [41], [42], [51] has become popular in syndromic surveillance, and in signal and image processing, among other applications. Another approach to anomaly detection is based on Principal Component Analysis (PCA) and has been used for the detection of anomalies in network traffic and online social networks [35], [50]. The PCA approach has many similarities to the method developed in this paper. However, detection is has typically been restricted to global anomalies, without localization criteria. Change point detection is a widely studied statistical topic in the context of time series, and has received much attention particularly in relation to break points (see the literature review in  [8], [29]). There are many approaches to these problems, including a posteriori change point analysis [7], [17], [30]. Other directions concentrate on parameter changes [25], [34], [40], [44]. More recently, an avenue based on tracking changes in a linear model was proposed in [15] and extended in [6], [19], [26], [31], [45], [52]. This direction has been recently expanded to problems in ====
 [18] and combined with ideas involving self-normalization [47], [48], [54]. However, applications are restricted to ==== time series.====In Section 2 the mathematical background is discussed. In particular, the KL expansion of a stochastic process is defined. In Section 3 the theory of anomaly detection via application of nested function spaces is developed and applied to stochastic process and random fields. We show how to build at estimator of the size of the anomaly given multiple realizations of the data. In addition, a small section is added to show the application of multilevel nested subspaces to the change point detection problem.====In Section 4 an algorithm for the construction of these spaces is shown in detail. This method allows the construction of multilevel bases on very general simplicial complex domains. An example application of this method to Spherical Fractional Brownian Motion (SFBM) is shown in Section 5.",Anomaly detection: A functional analysis perspective,https://www.sciencedirect.com/science/article/pii/S0047259X21001639,1 November 2021,2021,Research Article,69.0
"Leucht Anne,Paparoditis Efstathios,Rademacher Daniel","University of Bamberg, Research Group of Statistics and Business Mathematics, Feldkirchenstraße 21, D-96052 Bamberg, Germany,University of Cyprus, Department of Mathematics and Statistics, P.O. Box 20537, CY-1678, Nicosia, Cyprus,Technische Universität Braunschweig, Institut für Mathematische Stochastik, Universitätsplatz 2, D-38106 Braunschweig, Germany","Received 12 September 2020, Revised 23 June 2021, Accepted 20 October 2021, Available online 1 November 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104889,Cited by (0),The problem of comparing the entire second order structure of two functional processes is considered and a ,"Functional data analysis is a branch of statistics that in recent years has grown considerably and has created great research interest in the scientific community, especially in connection with the increasing number of situations in which theoretical and applied scientists have to deal with data of a continuous nature (i.e., curves, images, surfaces, etc.). For various works and references in different branches of functional data analysis, we refer to the recent special issues of Goia and Vieu [1] and Aneiros et al. [2]. See also the monograph by Horváth & Kokoszka [17] which discusses inference problems in a variety of setting concerning independent as well as dependent functional data.====In our work, we focus on dependent functional data and, in particular, on functional time series analysis. Functional time series occurs in many applications such as daily curves of financial transactions, daily images of geophysical and environmental data and daily curves of temperature measurements. Such curves or images are viewed as functions in appropriate spaces since an observed intensity is available at each point on a line segment, a portion of a plane or a volume. Moreover, and most importantly, such functional time series exhibit temporal dependence and ignoring this dependence may result in misleading conclusions and not appropriate inferential procedures.====Comparing characteristics of two or more groups of functional data forms an important problem of statistical inference with a variety of applications. For instance, comparing the mean functions between independent groups of independent and identically distributed (i.i.d.) functional data has attracted considerable interest in the literature, see, e.g., Benko et al. [4], Zhang et al. [36], Horváth and Kokoszka [17] (Chapter 5), Horváth et al. [18] and Paparoditis and Sapatinas [25]. In contrast to comparing mean functions, the problem of comparing the entire second order structure of two independent functional time series has been much less investigated. Notice that for i.i.d. functional data this problem simplifies to the problem of testing the equality of (the lag zero) covariance operators, see, e.g., Panaretos et al. [21], Fremdt et al. [14], Pigoli et al. [26] and Paparoditis and Sapatinas [25]. The same problem of testing the equality of the (lag-zero) covariance operators of two sets of independent functional time series has also been investigated by Zhang and Shao [37] and by Pilvakis et al. [28].====However, the comparison of the entire second order structure of independent functional time series, is a much more involved problem due to the temporal dependence between the random elements considered. In describing the second order structure of functional time series, the spectral density operator, introduced in the functional set-up by Panaretos and Tavakoli [22], is a very useful tool since it summarizes in a nice way the entire autocovariance structure of the underlying functional time series; see also Hörmann et al. [16]. It is, therefore, very appealing to develop a spectral approach for testing equality of the entire second order structure of two functional time series. Tavakoli and Panaretos [33] proposed an approach based on projections on finite dimensional spaces of the differences of the estimated spectral density operators of the two functional time series. Projection-based tests have the advantage to lead to manageable limiting distributions. However, such tests have no power for alternatives which are orthogonal to the projection space considered. Furthermore, the number of projections appears as an additional tuning parameter which has to be chosen by the user. Finally, simulations in the much simpler i.i.d. set-up suggest that the quality of the large sample Gaussian approximations of the corresponding test, is affected by the number of projections used; see Paparoditis and Sapatinas [25]. In this paper we focus on tests which evaluate the differences between the entire, infinite dimensional, structure of the two spectral density operators compared. For this, the Hilbert–Schmidt norm of the differences between the estimated spectral density operators, evaluated over all frequencies, is used as the basic building block of the test statistic considered.====The contribution of this paper is twofold. First, we focus on testing the equality of the entire second order structure between two independent functional processes by evaluating for each frequency, the Hilbert–Schmidt norm between the (estimated) spectral density operators of the functional process at hand. Integrating these differences over all possible frequencies, leads to a global, ====-type, measure of deviation which is used to test the null hypothesis of interest. We show that under the assumption of a linear Hilbertian processes, the limiting distribution of an appropriately centered version of such a test statistic under the null, is Gaussian. This Gaussian distribution does not depend on characteristics of the underlying functional processes beyond those of second order. Second, and because of the slow convergence of the distribution of the considered ====-type test statistic under the null against the derived limiting Gaussian distribution, we develop a novel frequency domain bootstrap procedure to estimate this distribution. The frequency domain bootstrap method works under minimal conditions on the underlying functional process and its range of applicability is not restricted to the particular class of processes considered and which is used to derive the limiting distribution of our test. We prove under very general conditions, that the bootstrap procedure correctly approximates the distribution of the proposed test statistic under the null. Furthermore, consistency of the bootstrap-based test under the alternative is established. Our theoretical deviations are accompanied by a simulation study which shows a very good behavior of the bootstrap procedure in approximating the distribution of interest and the good size and power performance of the test based on bootstrap critical values. Notice that the frequency domain bootstrap method proposed in this paper, can potentially be used to improve the performance of other tests too, like for instance, the projections based test of Tavakoli and Panaretos [33].====Developing bootstrap procedures for functional time series has attracted considerable interest in the literature. Politis and Romano [29] established weak convergence results for the stationary bootstrap, Dehling et al. [8] for the (non-overlapping) block bootstrap in a testing context, Raña et al. [30] applied a stationary bootstrap to functional time series, Ferraty and Vieu [11] a residual-based bootstrap and Franke and Nyarige [13] established consistency of a model-based bootstrap for functional autoregressions. Pilavakis et al. [27] derived theoretical results for the moving block bootstrap and for the tapered block bootstrap, Shang [32] applied a maximum entropy bootstrap and Paparoditis [24] introduced a sieve bootstrap for functional time series. In contrast to the aforementioned contributions, the bootstrap procedure proposed in this paper acts solely in the frequency domain and generates replicates of the periodogram kernels stemming from functional processes that satisfy the null hypothesis of interest.====A test related to ours and proposed after the first preprint of this paper has been appeared (see Leucht et al. [20]), is that of van Delft and Dette [34], which deals with testing a different set of hypotheses, so-called relevant hypotheses, about the second order dynamics of two functional processes. Important differences between the two procedures appear which will be discussed in more detail later on. However, we stress here the fact that the test statistic proposed in this paper is not a special case of the test statistic used in the aforementioned paper and, consequently, the limiting distribution of our test statistic is different and not covered by the asymptotic results derived in that paper. See Remark et al. for more details.====The remainder of the paper is organized as follows. Section 2 contains the main assumptions on the underlying functional linear processes and states the hypothesis testing problem under study. Section 3 is devoted to the suggested test statistic and its asymptotic behavior while Section 4 presents the frequency domain bootstrap procedure proposed to estimate the distribution of the test statistic under the null. Asymptotic validity of the bootstrap procedure is established and consistency of the corresponding test under the alternative also is proved. Section 5 contains numerical simulations and an application to a bivariate meteorological functional time series while Appendix concludes our findings. Auxiliary results containing some new results on frequency domain properties of linear Hilbertian processes as well as proofs of the main results are deferred to the Appendix and to the Supplementary Material.",Testing equality of spectral density operators for functional processes,https://www.sciencedirect.com/science/article/pii/S0047259X21001676,1 November 2021,2021,Research Article,70.0
"Boukhiar Souad,Mourid Tahar","Laboratoire de Statistiques et Modélisations Aléatoires, Université Abou Bekr Belkaid, Tlemcen 13000, Algeria","Received 4 September 2020, Revised 10 September 2021, Accepted 20 October 2021, Available online 1 November 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104884,Cited by (0), (up to a ==== factor). Then we propose an estimator of the variance of random operators and show its convergence. These results extend and improve those of Mas in the framework of functional AR Processes with deterministic coefficients. Simulated and real data examples are used to illustrate the performance of these predictors and showing competitive results.,"An important early contribution to the theory of functional time series is [9], where a theoretical treatment of linear processes in Banach and Hilbert spaces is developed. the Functional Autoregressive (FAR) processes are mathematically and statistically quite flexible and well established, they are used in practice for modeling and prediction of continuous-time random experiments. Examples range from predictions of electricity consumption [14], road traffic [9], electrocardiogram [6], economics and finance [27], [31], [33]. These generalizations lead to a growing demand for developed statistical methods which are carried out through the Functional Data Analysis (FDA) [2] . Several publications in FDA include the work of [42], [43] which covers many practical aspects of FDA. Other authors like [9], [21], [28], [38] came to enrich this field, see also [15], [24], [41] for recent overviews and surveys.====In this paper, we are interested by the class of FAR processes with random coefficients where the autoregressive equations are ruled by a sequence of random operators. However, the fixed coefficient ==== in the deterministic model ====, may change with time or others in certain cases. For example, if ==== in this model indicates the number of offspring species under a small isolated area in time ====, and ==== denotes the admitted of newly species in time ====, then ==== stands for the number of species in time ====, but this model does not show the influence from the external factors (temperature, humidity, etc.) which may affect ==== very much. Therefore, it would be more reasonable to express the fixed coefficient ==== as random variables ====. The class of FAR processes with random coefficients is used as a tool for handling possible nonlinear features of real life data, this includes nonlinear models, switching models, threshold models and doubly stochastic model developed in [26], [40], [44]. [25] proposed a model with a random operator taking two values according a Bernoulli law, [16] generalized such models for cases involving several regimes, he described an application using the national demand for electricity in France where the temperature information is an exogenous covariate to express the sensitivity of the demand to meteorological conditions. This relationship is not linear, and it depends on the hours, days, and months of the cold season.====Our main goal in this work is the estimation of the means of random operators using the class of resolvent estimators that has been first investigated by [35] in the framework of FAR processes with deterministic coefficients. The existence of resolvent estimators and estimation are deeply related to the well-known linear ill-posed inverse problem and perturbation theory [19], [32]. We investigate the asymptotic properties of resolvent estimators, firstly, we establish exponential bounds, we apply large deviations for Hilbert martingale differences [9] and results on empirical covariance operator [1] to deduce exponential inequalities and parametric convergence rate ==== up to a ==== factor depending on a regularizing parameter, the almost sure convergence with parametric rate improve the results given by [11], [35] for FAR processes with deterministic coefficients. We provide a limit law for the resolvent estimators deeply rests on a suitable decomposition of Hilbert space valued martingale differences and martingale approximation. Similar results on resolvent predictors are presented. We also, study an estimator of the variance of the random operators. We provide its convergence in probability with a parametric rate. This result may serves for discriminating between deterministic and random coefficients FAR processes. The interest of this study covers a wide scope of problems, including for instance the prediction of time continuous processes [9], [21], [43], testing procedures for classification curves [29] and functional principal components analysis (FPCA) which plays a major role in regression analysis [20], for nonparametric modeling we can refer to [18], [34], on a semi-parametric functional regression setting we can see [23]. Our results will thus be useful in this area because an estimation of distribution law of the random coefficients is needed. It is worth noting that in statistical inference, one important application of previous results is the construction of confidence sets for prediction for moderate sample sizes, however, there were other conditions that needed to be met.====The structure of the paper is as follows. Section 2 is devoted to the presentation of our estimators, the results are presented in Section 3. In Section 4, we conduct a simulation study showing the performance of the considered predictors. Finally, proofs are gathered in Section 5.",Resolvent estimators for functional autoregressive processes with random coefficients,https://www.sciencedirect.com/science/article/pii/S0047259X21001627,1 November 2021,2021,Research Article,71.0
"Zhong Rou,Liu Shishi,Li Haocheng,Zhang Jingxiao","Center for Applied Statistics, School of Statistics, Renmin University of China, China,Department of Mathematics and Statistics, University of Calgary, Canada","Received 17 February 2021, Revised 22 September 2021, Accepted 20 October 2021, Available online 30 October 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104864,Cited by (5),"Functional principal component analysis is essential in functional data analysis, but the inference will become unconvincing when non-Gaussian characteristics occur (e.g., heavy tail and skewness). The focus of this manuscript is to develop a robust functional principal component analysis methodology to deal with non-Gaussian ====, where sparsity and irregularity along with non-negligible measurement errors must be considered. We introduce a Kendall’s ==== function to handle the non-Gaussian issues. Moreover, the estimation algorithm is studied and the ==== is discussed. Our method is validated by a simulation study and it is applied to analyze a real world dataset.","Functional data analysis has become increasingly important in various areas. We recommend Aneiros et al., Goia and Vieu [1], [14] and Wang et al. [39] for an overview in this field. Moreover, there are also many representative monographs focused on functional data, see, Ferraty and Vieu, Horvat́h and Kokoszka, Hsing and Eubank [11], [20], [21] and Ramsay and Silverman [37].====In this manuscript, we develop a new functional data analysis method for longitudinal data. Longitudinal data are typically measured sparsely and irregularly over time with non-ignorable measurement errors. The connection between longitudinal data analysis and functional data analysis was discussed in Marron et al. [33]. Further, many studies considered longitudinal and functional data simultaneously. Yao et al. [44] proposed a functional principal component analysis (FPCA) technique for longitudinal and functional data through conditional expectation. Hall et al. [16] proposed a latent Gaussian process model for longitudinal observations. Jiang and Wang [23] added covariate information into the FPCA process. Hall et al. [15] and Li and Hsing [28] discussed the statistical properties for functional and longitudinal data analysis. Li et al. [30] proposed a covariance-based FPCA method for multivariate longitudinal data using penalized tensor-product splines. Other studies can be referred to [7], [8], [9], [19], [22], [24], [29], [47].====Among the aforementioned methodology in functional data analysis, FPCA is the key technique due to the infinite dimensional feature of functional data. However, as pointed out by Kraus and Panaretos [26] and Zhong et al. [46], functional data inference may be implausible when the data deviate from Gaussian assumption. Therefore, robust estimators for principal components have been explored. Locantore et al. [32] introduced a spherical principal component analysis method, and Gervini [13] further developed the spherical principal components based on a newly defined functional median. Asymptotic properties were discussed by Boente et al. [4]. Other robust methods can be referred to Bali et al. [3] and Boente and Salibian-Barrera [5]. Although Gaussian assumption is not required in these studies, distributional constraints still exist, such as symmetric or elliptical assumptions. In addition, these methods often ignore the sparse data and measurement error issues. To the best of our knowledge, only Hall et al. [16] and Gertheiss et al. [12] discussed non-Gaussian principal component estimation for longitudinal data. Hall et al. [16] proposed a latent Gaussian model, where a known link function is required. Therefore, implicit constraints on distribution are also imposed. Similarly, Gertheiss et al. [12] implemented the FPCA through a generalized additive mixed model and proposed computational algorithms in both frequentist and Bayesian approaches. However, the model is specific for exponential-family outcomes.====The goal of this paper is to develop robust estimators for functional principal components, which accommodate to longitudinal data. We emphasize that the robustness in this manuscript requires our method to be: (1) insensitive to the deviation of Gaussian assumption for responses; (2) free of any distributional constraints on responses. For this purpose, we propose a Kendall functional principal component analysis (KFPCA) approach. To be specific, a novel Kendall’s ==== function is defined, which is inspired by the Kendall’s ==== correlation coefficient in [25] and the spatial sign covariance function in [4], [13]. The Kendall’s ==== function is further shown to have the same eigenspace as the population covariance function, while the assumption of symmetric distribution is not necessary. For the model estimation, local linear estimate for Kendall’s ==== function is introduced to facilitate the estimation of the functional principal components. The robustness of our method can be achieved as the rank information is utilized. As we will show in our simulation study, the proposed method performs well even under heavy-tailed or skewed distribution for sparse and severely sparse settings. Moreover, the asymptotic consistency properties of our estimators are studied in this manuscript.====The contributions of this paper are summarized as follows. We propose to use the Kendall’s ==== function into FPCA. Compared to the commonly used covariance function, the Kendall’s ==== function is less likely to be affected by the non-Gaussian settings. In addition, we introduce an estimation algorithm for the Kendall’s ==== function to analyze longitudinal data. We also prove the asymptotic consistency properties for the estimators of the Kendall’s ==== function and principal components. Finally, we develop an ==== package ==== to implement our proposed method. The package can be downloaded at ====.====The rest of the article is organized as follows. In Section 2, we present the KFPCA approach along with the introduction of the proposed Kendall’s ==== function. In Section 3, the estimation procedure is illustrated. The theoretical results are studied in Section 4. We present a simulation study to evaluate the performance of the proposed KFPCA in Section 5. Our proposed method is applied to analyze a real world dataset in Section 6. We conclude this manuscript with discussions in Section 7.",Robust functional principal component analysis for non-Gaussian longitudinal data,https://www.sciencedirect.com/science/article/pii/S0047259X21001421,30 October 2021,2021,Research Article,72.0
"Boudou Alain,Viguier-Pla Sylvie","Equipe de Stat. et Proba., Institut de Mathématiques, UMR5219, Université Paul Sabatier, 118 Route de Narbonne, F-31062 Toulouse Cedex 9, France,Université de Perpignan via Domitia, LAMPS, 52 av. Paul Alduy, 66860 Perpignan Cedex 9, France","Received 13 February 2021, Revised 31 August 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104875,Cited by (5),"In this paper, we give a definition of a cyclostationary function, which specifies and extends usual definitions of cyclostationary processes. We transform such a cyclostationary function into a series. The property of ","Cyclostationary processes, also refered as periodically correlated random processes, have been explored since the 1960s and before (Voychishin and Dragan [14], is an English translation of articles first published in 1957 and 1960). It has been firstly mathematically treated by Gladyshev [9], and largely developed by Hurd [10], as a modelling of phenomena which are periodically correlated, that is as processes for which some statistics present a periodicity. A renewed interest of mathematical aspects of this field can be observed, as in Bouleux et al. [6], where we find a characterization of these processes using dilation matrices. Periodicity occurs in various phenomena, due for example to modulation in signal theory, rotation in mechanics, revolution of planets or pulsation of stars for astronomy, seasonality in economics, or sanguine pulse for medicine. This notion has already been largely employed in applications. Let us cite some examples such as telecommunications (Gardner [8]), mechanic transmission (Randall et al. [11]), radioastronomy (Weber and Faye [15]), locomotion (Zakaria [16]), or medical studies (Roussel [12]). A collection of illustrations can be found in Antoni [1].====Many authors address cyclostationary signals on a temporal level, indexed by ==== or ====. In that case, the shape of the process is easy to visualize and to model. In the present paper, we give a definition of cyclostationarity for random functions (r.f.’s) indexed by ====, and we propose a way for processing the Principal Components Analysis (PCA) of such r.f.’s. Indeed, we can imagine phenomena varying on both space and time, as, for example, a flow in fluid mechanics. This kind of cyclostationarity gives tools for modelling such phenomena, and many other types.====Very often, when considering a process, it is centered, so the scalar product becomes a covariance. We do not make this hypothesis, because it is not necessary for the mathematical development, and this simplifies the writing. Nevertheless, this hypothesis can be done, if it makes more sense from a statistical point of view.====Section 2 is devoted to mathematical recalls. We work in the complex field, to be able to use Fourier transform, as we address signals in the frequency domain.====The stochastic integral is defined as an isometry. The spectral measure is exposed as a mapping defined on a set which is a ====field, what is natural as it is a measure, taking values in a set of projectors, that is idempotent mappings. We then introduce the association between a unitary operator and a spectral measure. Let us expose hereafter a particular case for which the notions are easy to write. We consider a ====Hilbert space ====, which can be a space of random variables, where mappings operate. If ==== is a finite family of projectors from ==== into ==== such that ====, then ==== is a unitary operator for which the associated spectral measure is ====, where ==== is the Dirac measure concentrated on ====, element from ====. If ==== is an element from ====, we note that ==== (we further denote ====) is a stationary series, because ====, for which the associated random measure is ====, what means that ====.====We end the section by the recall of the PCA in the frequency domain. We say that a ====dimensional series ==== is ====stationary when ==== (or more generally, ====, where ==== stands for the tensor product), for any pair ==== of elements from ====. The first ==== steps (====) of the PCA in the frequency domain of this series may be presented as the search of a ====dimensional filter ====
 (==== being an operator from ==== into ====) which summarizes ====. In order to measure the quality of the resulting summary, we transform the ====dimensional series ==== into a ====dimensional series thanks to a second filtering operation, ====
 (==== is an operator from ==== into ====, and then ==== is an operator from ==== into ====). This last series is a reconstitution of the data. It is stationarily correlated with ====: ====, and then ====. The filters are chosen such that this last quantity is as small as possible.====In Sections 3 to 6, we develop mathematical tools which we are going to use in the studies of cyclostationary functions. We define the conjugate spectral measure. We associate a spectral measure with a family of stationary series. We study the ampliation, operation which consists, from an operator of ==== (mapping from ==== into ====), to define an operator of ==== (space of square-integrable mappings from ==== into ====). All these mathematical tools will be necessary for Section 7, where we study the cyclostationarity. Section 8 is devoted to a simulated example. We end by an exploration of a particular case of cyclostationary function, which is decomposed into the product of two independant random variables, one of them is stationary, and the other one is periodic.====Let us expose shortly the case of cyclostationary series.====The set of the random variables ==== is a ====cyclostationary series when ====, for any ==== from ====. So we can easily verify that ==== is a family of stationary series pairwise stationarily correlated. The subset ==== is such that with any ==== from ==== we can associate an element, and only one, from ====, ==== such that ====
 (==== is the integer part of ====).====If we consider the ====dimensional random vector ====, we can easily verify that ==== is a ====stationary series. So we can define the PCA in the frequency domain of ====. The first ==== steps give a ====stationary series, ====, and a ====stationary series, ====. Each of the random vectors ==== is ====dimensional: ====. We can then verify that ====, where ====, is a ====cyclostationary series.====We show that ====So we can summarize a cyclostationary series by a ====stationary series, ====. The quantity (1) is a measure of the quality of this summary. As for the cyclostationary series ====, it enables the reconstruction of the data.====Let us now present a particular case of the foregoing study. This case is similar to the previous one, but concerns the functions instead of series. Let us consider a family of integrable square module random variables ==== such that ====, for any pair ==== of reals.====From the process ====, we build a stationary series ====, each element ==== is a random vector, more precisely, a random variable taking values in the ====Hilbert space ==== (which substitutes to ====), which is defined from the family of random variables ==== (this family substitutes to the family ==== in the previous example).====The series ==== is such that ==== and we can proceed to the PCA in the frequency domain.",Principal components analysis and cyclostationarity,https://www.sciencedirect.com/science/article/pii/S0047259X21001536,29 October 2021,2021,Research Article,73.0
"Fang Kuangnan,Chen Yuanxing,Ma Shuangge,Zhang Qingzhao","Department of Statistics and Data Science, School of Economics, Xiamen University, China,Department of Biostatistics, Yale University, United States of America,MOE Key Laboratory of Econometrics, Department of Statistics and Data Science, School of Economics, Wang Yanan Institute for Studies in Economics, and Fujian Key Lab of Statistics, Xiamen University, China","Received 4 November 2020, Revised 12 June 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104874,Cited by (4),"In biomedical data analysis, clustering is commonly conducted. Biclustering analysis conducts clustering in both the sample and ","In biomedical data analysis, clustering has been routinely conducted. The clustering of samples can assist better understanding sample heterogeneity, and the clustering of covariates can identify those that behave similarly across samples and then, for example, improve our understanding of covariate functionalities. Clustering can also serve as the basis of other analysis, for example, regression. Biclustering analysis has also been developed, identifying clustering structures in both sample and covariate dimensions. It includes sample- and covariate-clustering as special cases and, in a sense, can be more comprehensive. For generic reviews of techniques, theories, and applications of clustering, we refer to [19], [46].====This study has been partly motivated by the analysis of gene expression data, for which sample- and covariate-clustering as well as biclustering have been extensively conducted [21], [45]. Most gene expression studies generate “snapshot” values. Unlike some types of omics measurements, gene expression values can be time-dependent, and the temporal trends of gene expressions can have important biological implications [16]. Accordingly, time-course gene expression studies have been conducted, generating multiple measurements at different time points for each gene of each sample. In the analysis of time-course gene expression data, besides simple statistics, functional data analysis (FDA) techniques, have been adopted and shown as powerful [12].====FDA deals with data samples that consist of curves or other infinite-dimensional data objects. Over the last two decades, we have witnessed significant developments in its theory, method, computation, and application. For systematic reviews, we refer to [2], [15], [23], [40]. In FDA, clustering analysis has been of particular interest. A popular approach projects functional data into a finite-dimensional space and then applies existing clustering methods. For example, Abraham et al. [1] conduct B-spline expansions, and clusters the estimated coefficients using a k-means algorithm. Peng and Müller [30] develop a distance for sparse functional data, and apply a k-means algorithm to functional principle component analysis (PCA) scores. Other approaches, such as Bayesian [37], subspace [3], [9], [10], and model-based [18], [20], have also been developed. We refer to [17], [40] for surveys on functional data clustering. Most works in this area, however, have focused on either sample- or covariate-clustering.====For biclustering analysis (of gene expression and other types of data), in this article, we take the “natural next step” and consider the scenario where for each covariate of each sample, a function or its realizations at discrete time points are available. We note that, although this study has been partly motivated by gene expression data and some of the discussions are focused on such data, the considered data scenario and proposed technique can have applications far beyond such data. For example, in biomedical studies, many biomarkers measured in blood tests vary across time, and their values can be obtained from medical records. In financial studies, many measures of a company, for example size and stock price, vary across time. As such, our investigation can have broad applications.====There is a vast literature on biclustering analysis with scalar measurements. Directly applying such techniques to the present problem will involve either treating functional measurements as scalars and then computing distances (between covariates and samples) – which may be ineffective by not sufficiently accounting for the functional nature of data, or first estimating functionals and then computing distances between the estimates – which may also encounter challenges when a large number of functionals need to be jointly estimated. Our literature review suggests that there are also a handful recent biclustering methods designed for functional (especially including longitudinal) data. For example, Slimen et al. [35] propose a biclustering method for multivariate functional data based on the Gaussian latent block model (LBM) using the first functional PCA scores. Bouveyron et al. [4] develop an extension of the Gaussian LBM by modeling the whole set of functional PCA scores. In another work [28], a biclustering method with a plaid model is extended to three-dimensional data arrays, of which multivariate longitudinal data is a special case.====For the biclustering analysis of functionals, in this article, we develop a penalized fusion based approach. More specifically, a nonparametric model is assumed for each covariate of each sample, allowing for sufficient flexibility in modeling. A doubly penalization technique is adopted, which includes a smoothness penalty to regulate nonparametric estimation. The most significant advancement is the second, fusion penalty, which “transforms” clustering in both sample and covariate dimensions to a penalized estimation problem. Statistical and numerical investigations are conducted, providing the proposed approach a solid ground. This study may complement and advance from the existing ones in multiple aspects. Compared to direct applications of biclustering methods for scalars (that either directly compute distances without functional estimation or estimate functionals separately), the proposed approach can more effectively accommodate the functional nature of data or generate more effective estimation. This is because it “combines” clustering and estimation, and as such, estimation only needs to be conducted for clusters as opposed to individual covariates, potentially leading to a smaller number of parameters and hence more effective estimation. Compared to some of the existing biclustering methods for functionals, such as [4], [35], the proposed approach has a much easier way of determining the number of clusters. In addition, unlike [4], [35], it does not make stringent distributional assumptions (for example, normality). Meanwhile, rigorous theoretical investigations are conducted beyond methodological developments, granting the proposed approach a stronger statistical basis. It also advances from the clustering of functional covariate effects (assuming homogeneous samples) by simultaneously examining sample heterogeneity, thus being more comprehensive. Additionally, this study may also advance and enrich the penalized fusion technique. Clustering via penalized fusion has been pioneered in [8] and other studies. Compared to alternative clustering techniques, it is more recent and has notable statistical and numerical advantages [44]. Compared to the existing penalized fusion based clustering, this study differs by conducting biclustering and by having unknown parameters generated from the basis expansion of functionals. Last but not least, this study also provides a practically useful and new way of analyzing time-course gene expression data (and other data with similar characteristics).====The remainder of this article is organized as follows: Section 2 introduces the new biclustering approach via penalized fusion and develops an effective computational algorithm. Statistical properties are established to provide our method a strong theoretical support. Simulation studies and the analysis of two time-course expression data are conducted in Sections 3 Simulation, 4 Applications, respectively. Section 5 concludes with a brief discussion. The proofs of the main results are presented in Appendix A.",Biclustering analysis of functionals via penalized fusion,https://www.sciencedirect.com/science/article/pii/S0047259X21001524,29 October 2021,2021,Research Article,74.0
"Xiong Wei,Wang Dehui,Deng Dianliang,Wang Xinyang,Zhang Wanying","School of Mathematics, Jilin University, Changchun 130012, PR China,Department of Mathematics and Statistics, University of Regina, SK S4S 0A2, Canada,School of Economics, Liaoning University, Shenyang 110036, PR China,School of Mathematics and Systematic Sciences, Shenyang Normal University, Shenyang 110034, PR China","Received 5 June 2021, Revised 20 October 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 12 November 2021.",https://doi.org/10.1016/j.jmva.2021.104867,Cited by (0),"Multiply robust estimation with missing data is considered as an important field in ====, which incorporates information by weighting multiply candidate models and loosens the requirement of the model specification. Nevertheless, in high-dimensional cases one more flexible hypothesis is the “true structure” beyond the correct model. In this paper, we study the ==== estimation for high-order autoregressive processes with a lagged-dependent binary ","Explanatory variables mixed time series model has received considerable attention because of the flexible description of the dynamic linear relationship. Assume that ==== be a sequence of independent identically distributed (i.i.d.) random variables with mean zero and variance ====
 ====, the ====-order autoregressive processes with a binary explanatory variable are defined as ====
 ====, ==== and ==== are coefficients that we are interested to estimate. ==== is a one-dimensional binary explanatory variable which satisfies ====where ==== has a parametric form with the parameter ====, ====. Model (1) with such ==== can better characterize a number of realistic problems. For instance, clinicians need to follow up the indicator for a patient during a period of time, which may be affected by the status of a complication of his/her time. In this case, ==== denotes the outcome and ==== stands for the actual incidence of the individual at time ====. Condition (2) effectively explains the potential relevance between ==== and ====, that is, the complication is influenced by the state of the patient in earlier time. Some of economic time series with the type of binary covariate could be provided with a gratifying relevance as well. The following legend simply depicts this relationship with ====: ====Let ==== denote the vector of all parameters in the model. There are a great deal of literatures to estimate ==== in terms of full data. For example, Crowder [2] made the inference of the consistency and asymptotic normality of the conditional least-squares (CLS) estimator. Zhao and Wang [26] investigated the procedure and propensity of the coefficient via empirical likelihood (EL). Yang and Wang [23] studied the related Bayesian inference based on independently and identically distributed explanatory variables.====In practice, it is common to consider high-order processes in the procedure of modeling. Variable selection is fundamental for shrinking to 0 for some insignificant orders to improve the overall prediction accuracy, for instance, Huang and Yang [10] advanced a lag-selection method for non-linear additive autoregressive models and Wang et al. [19] proposed the modified LASSO for the linear regression with autoregressive errors (REGAR). Zhang et al. [25] made the inference for the penalized conditional least squares estimation in generalized integer-valued autoregressive (GINAR) processes. Wang et al. [21] applied the penalized conditional maximum likelihood estimation for the Poisson autoregression (PAR) model, and Kwon et al. [11] studied the selection criterion of tuning parameter with adaptive LASSO (ALASSO) penalized function in autoregressive (AR) processes.====Missing data is frequently encountered in empirical studies. Ignoring drop-out information often destroys the representativeness of the sample and leads to a biased conclusion. Let ==== be the finite sample from (1), ==== is always available and ==== is subject to missingness with a response indicator ==== satisfying that ==== if ==== is observed and ==== otherwise. The response mechanism is denoted by ====with an unknown parameter vector ====. When ====, the mechanism is called missing at random (MAR) (refer Little and Rubin [13]), and (3) can be rewritten as ====. One most general estimation for MAR data, weighting the complete case through the inverse of the selection probabilities, is named as the inverse probability weighted (IPW) method (Rosenbaum and Rubin [16]). Beyond IPW estimation, Robins et al. [15] proposed a double robust estimation for ==== by solving ====where ====, ==== and ==== is a ====-field of events generated by ====. In the independent and identically distributed (i.i.d.) case, Chan [1], Han [7], Duan and Wang [4] separately generalized (4) to a multiply robust (MR) estimator if either ==== or ==== has a correct specification in their candidate model sets. Li et al. [12] discussed the relevance between double robustness and multiple robustness, and proposed a model mixing method for multiply candidate models to improve the effect of the double robust estimation.====However, there are a few difficulties to determine the exact model for ==== and ==== through little of implicit information, especially in high-order processes. Suppose that ==== and ==== are the nonnested multiply candidate model sets for ==== and ==== with corresponding parameter vectors ==== and ====, universally we can formulate the model with a large number of lag-dependent variables to attenuate modeling biases as the “true structure” (see in Definition 1) model. To deal with the problem, we consider a variable selection approach with determined penalties. One can prove that the estimators of the “true structure” models obey the asymptotic theory of White [22], also concur with the oracle properties. Furthermore, we study the parametric estimation for (1), (2) with missing ====. Under MAR mechanism, we propose a penalized multiply robust estimation (PMREE) with a sparse ====. The proposed method is applicable for the significant lags in strictly stationary autoregression models that outperform the general multiply robust estimation. To improve the accuracy of the result, we develop a modified tuning parameter selection criterion to recover the information of the missing part. We briefly point out that our selection criterion possesses the consistency.====The rest of the paper is organized as follows. We discuss the penalized estimation for candidate models, which is ====-consistent with the extremum of the Kullback–Leibler Information Criterion (KLIC) in Section 2.1. In Section 2.2 we propose the PMREE for ==== in virtue of the empirical probability mass and LS weights. The multiply robust property and the oracle properties for PMREE estimator are derived in Sections 3.1 Multiple robustness, 3.2 Oracle properties respectively. The selection criterion for the tuning parameter is modified in the multiply robust framework of Section 3.3. Simulation studies are procedured in Section 4.1 to evidence the validity, and we cite an instance in Section 4.2 by fitting the monthly U.S. Industrial Production Index time series data to report the usability of the result. Conclusion remarks are presented in Section 5. Related assumptions and technical proofs are shown in the Appendix, and all tables and figures of numerical studies are attached in the Supplementary Material.",Penalized multiply robust estimation in high-order autoregressive processes with missing explanatory variables,https://www.sciencedirect.com/science/article/pii/S0047259X21001457,29 October 2021,2021,Research Article,75.0
"Wang Bingling,Li Yingxing,Härdle Wolfgang Karl","Humboldt-Universität zu Berlin, IRTG 1792, Dorotheenstr.1, 10117 Berlin, Germany,Wang Yanan Institute for Studies in Economics, and Fujian Provincial Key Laboratory of Statistical Science, Xiamen University, 422 Siming S Rd, 361005 Fujian, China,Humboldt-Universität zu Berlin, Blockchain Research Center, Unter den Linden 6, 10099 Berlin, Germany,Sim Kee Boon Institute for Financial Economics, Singapore Management University, 50 Stamford Road, Singapore, 178899, Singapore,Department of Information Science and Finance, National Yang Ming Chiao Tung University, Management Building 1, 1001 University Road, Hsinchu, 30010, Taiwan, ROC,Department of Mathematics and Physics, Charles University, Ke Karlovu 3, 12116 Prague, Czech Republic","Received 1 March 2021, Revised 29 September 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104869,Cited by (1),"-means clustering is one of the most widely-used partitioning algorithm in cluster analysis due to its simplicity and computational efficiency, but it may not provide ideal clustering results when applying to data with non-spherically shaped clusters. By considering the asymmetrically weighted loss, we propose the ==== -variance. We provide algorithms based on two schemes: the fixed ==== clustering, and the adaptive ==== clustering. Validated by simulation results, our method has enhanced performance on data with asymmetric shaped clusters or clusters with a complicated structure. Applications of our method show that the fixed ==== clustering can bring some flexibility on segmentation with a decent accuracy, while the adaptive ==== clustering may yield better performance. All calculation can be redone via quantlet.com.","Clustering is a useful technique to discover and identify homogeneous groups of data points in a given sample. As an unsupervised learning algorithm, it aims to extract information on the underlying characteristics via dividing the data into groups that maximize common information. Obviously the information about homogeneity of groups is key in such a sample dividing mechanism. Among the simplest choice is the ====-means clustering method described by [6], [22], which adopt the Euclidean distance as neighborhood measure, thus leading to spheres as silhouettes and means as centers of clusters. Indeed, while keeping a balance between group size and information gain, ====-means is the most widely used partitioning algorithm due to its simplicity, efficiency in computing and easiness of interpretation.====Surprisingly, ====-means clustering also works well for high-dimensional or functional objects. Successful applications include signal processing, image identification, customer segmentation. For a nice overview of recent trends in high-dimensional or functional data analysis, readers could refer to [2], [5]. However, there might exist various types of heterogeneity among sub-samples, it is interesting to explore new clustering approach especially for high dimensional or functional objects.====The principle of a partitioning clustering algorithm is to assign data points to the nearest cluster by optimizing some objective function. The objective function of ====-means is the sum of within-group variance, and thus the correspondence cluster centers are the mean of each cluster. Minimizing the objective function is equivalent to maximizing the log-likelihood function with independent Gaussian density. Although ====-means clustering is often viewed as a “distribution free” algorithm, it is actually partitioning using equal sized spherical contour lines which can be considered as assuming independent identically distributed (i.i.d.) Gaussian clusters. Therefore, ====-means approach works better for cluster in the symmetric distribution than the skewed ones.====On the other hand, when applied on skewed or asymmetric distributed data whose characteristics may not be fully captured by the first two moments, new methods are required for non-spherical cluster. A nice summary of model-based clustering methods is presented in [3]. To account for within-cluster skewness, [7] introduces the ====-quantile clustering algorithm based on the asymmetric absolute discrepancy and they linked their approach to a fixed partition model of generalized asymmetric Laplace distributions. This quantile discrepancy based density relies on both the quantile level ==== and some additional scale/penalty parameter ====. However, ==== and ==== are assumed the same across different clusters to reduce the computation complexity. An analogous work on quantile based clustering is proposed in [28], where they developed a model-based iterative algorithm to identify subgroups with heterogeneous slopes. In particular, they consider clustering across multiple quantiles to capture the full picture of heterogeneity. For that accordance, how to specify the appropriate quantile level vector ==== could be a problem for large dimensional data.====This motivates us to consider a novel method, ====-expectile clustering. This method is based on a similar idea as ====-means but with an expectile cluster center and aims at minimizing the so-called ====-variance, which is a weighted quadratic loss to take into account asymmetry. Besides being simple and fast, our algorithm can be applied on wider range of data compared with ====-means. In particular, we consider two schemes, either with a pre-specify ==== level or an adaptive ==== that may vary across different dimensions or clusters, which accommodates either a fixed cluster shape or a data-driven cluster shape to capture heterogeneity.====To better understand the basic ideas of ====-expectile clustering, we recall some basic knowledge about tail events. Quantile regression [11] and expectile regression [16] have been suggested for displaying the whole picture of the conditional distribution of response variable on covariates, especially for data not sufficing the condition of homoscedasticity or conditional symmetry. For a random variable ==== drawn from distribution ====, a location model of ====-th tail event measure with ==== could be defined as: ====With an assumption on the ====-th quantile or expectile of ==== being zero, ==== is by definition the ====-th quantile or expectile of ==== accordingly. An estimator of the location model of quantiles and expectiles can be naturally formed: ====where the loss function ==== is defined as: ====with ==== and ==== respectively.====Although the concept of expectiles is natural analogues of quantiles, expectiles enjoy the computation efficiency over quantiles [19]. In finance, the expectile might be preferred as a favorable risk measures due to its desirable properties such as coherence and elicitability [12], [29]. Recently, the use of expectiles attracts more and more attention, such as the nonparametric expectile regression by [21], [27], the principle expectile analysis by [23]. Our proposed ====-expectile clustering allows us to take into account tail characteristics and asymmetry when identifying homogeneous groups of data, while simulation studies and applications justify its excellent performance.====The rest of the paper is organized as follows. In Section 2, we will briefly review the classical ====-means algorithm, and then propose our ====-expectile clustering in two schemes. In Section 3, we present the simulation study that includes data from different distribution and compare the performance of ====-expectiles clustering with other methods. Section 4 applies our method to real crypto currency market analysis and image segmentation. Codes of all the functions, applications and data are uploaded to ====.",-expectiles clustering,https://www.sciencedirect.com/science/article/pii/S0047259X21001470,29 October 2021,2021,Research Article,76.0
"Meintanis Simos G.,Hušková Marie,Hlávka Zdeněk","Department of Economics, National and Kapodistrian University of Athens, Athens, Greece,Unit for Pure and Applied Analytics, North–West University, Potchefstroom, South Africa,Charles University, Faculty of Mathematics and Physics, Department of Statistics, Sokolovská 83, 18675 Prague, Czech Republic","Received 3 February 2021, Revised 28 June 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104873,Cited by (1), for multiple time series is also considered.,"Testing independence of random vectors in finite dimension is one of the classical problems in non-parametric statistics. In fact the subject has recently enjoyed a renewed interest, and we refer the reader to the articles by Roy et al. [50], Herwartz and Maxand [22], and Josse and Holmes [34] for up-to-date reviews of the existing literature. When it comes to functional objects however, statistical methods designed for finite dimension need to be appropriately modified in order to meet the challenges brought forward by the infinite dimension of these objects. The range of methods that have recently been developed and are tailored to functional observations may be appreciated by referring to the special issues Goia and Vieu [17],  Aneiros et al. [1], and González Manteiga and Vieu [18], the review articles by Wang et al. [55] and  Cuevas [8], and the volumes by Ramsay and Silverman [48], Ferraty and Vieu [13], Ferraty [12], Horváth and Kokoszka [28], and Kokoszka and Reimherr [38]. Coming back to independence testing, but for functional observations, there exist only a handful of contributions primarily based on covariance operators [16], [27], [30], while the “Fourier” approach is also becoming popular with the recent contributions by Lyons [45], Hlávka et al. [25], and Krzysko and Smaga [40].====The aforementioned Fourier approach utilizes the property that a given pair of random vectors is independent if and only if the joint characteristic function (CF) is identical to the product of corresponding marginal CFs. This property has propagated the Fourier approach to the problem of testing independence in the past. In finite dimension Fourier methods date back to Csörgő and Hall [7], Csörgő [6], Feuerverger [14], and Kankainen [35]. Since then they have enjoyed increasing popularity particularly so with the introduction of the novel notion of distance covariance, and beyond. Indicatively we refer to the contributions by Kankainen and Ushakov [36], Hong [26], Gretton et al. [19], Székely et al. [54], Meintanis and Iliopoulos [46], Hlávka et al. [24], Fan et al. [11], Davis et al. [9], Chen et al. [5], Chakraborty and Zhang [4], Shen et al. [53], and Ke and Yin [37], that propose tests of independence in varying settings and different levels of generality but always with the CF being the underlying notion.====Here we follow the lines of reasoning in Horváth and Rice [30] and Hlávka et al. [25] and base our test procedure on functionals of empirical CFs. The outline of the paper is as follows: In Section 2 the null hypothesis and the test criterion are formulated and various related aspects are discussed. In Section 3 we investigate the asymptotic behavior of the test statistic under the null hypothesis as well as under alternatives. In Section 4 the finite-sample properties of a resampling version of the new test are studied by simulation. In Section 5 we present a real-data application, as well as a first attempt to extend the method for tests of independence of several time curves. Testing independence of multiple time curves continues into Section 6 where a generalization of the test procedure is also discussed. The paper concludes in Section 7. The Appendix contains detailed proofs of limit results.",Fourier-type tests of mutual independence between functional time series,https://www.sciencedirect.com/science/article/pii/S0047259X21001512,29 October 2021,2021,Research Article,77.0
"Hlávka Zdeněk,Hlubinka Daniel,Koňasová Kateřina","Department of Probability and Mathematical Statistics, Charles University, Sokolovská 83, 186 75, Praha, Czechia","Received 9 February 2021, Revised 4 October 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104878,Cited by (2),"Functional two-sample tests based on empirical characteristic functionals are studied. We consider a test statistic of Cramér–von Mises type with integration over a preselected family of probability measures, say ====, leading to a computationally feasible and powerful test statistic. Small sample properties of the resulting two- and ==== gives very good power in detecting shift and scale alternatives.","Functional data analysis (FDA) gradually became one of the most vivid fields in statistical research and application, see, e.g., [19], [22], [25], [29]. FDA applications were propelled mainly by new computational possibilities of collecting and storing observations measured on a very dense time (or space) domain. These new data sets motivated also related theoretical research, often based on generalizations of classical multivariate methods. The research is still in progress; many interesting mathematical problems need to be addressed to overcome some of the difficulties encountered in this field.====The recent development of the FDA includes almost all classical statistical techniques known for finite-dimensional random vectors with particular attention to the classification of functional data, functional regression and time series analysis, and estimations of the mean function and covariance operator. Different nonparametric, semiparametric, and parametric methods already exist to analyze functional data. Especially, kernel methods, random projections and principal components analysis represent well-developed tools for infinite-dimensional data. Lately, the data depth approach has become popular in the FDA context, with many appealing properties and promising performance.====It is out of the scope of this paper to list all the contributions made recently in the field of FDA but the reader may see the discussion in [1], [9] for more details.====In FDA, characteristic functionals often lead to a natural formulation of the problem of interest [17], [18]; a functional two-sample test based on empirical characteristic functions has been investigated in [21]. The characteristic functionals are also the central point of this contribution.====In this paper, we address a general two-sample test for functional observations, i.e., for random elements in a Banach space ==== of real functions on a compact interval. Without loss of generality, the functions are assumed to be defined on the unit interval ====.====Assuming two independent functional random samples, say ====, for ====, the problem of testing the null hypothesis of ==== ====, i.e., ====and a similar hypothesis of ==== ==== has already been extensively investigated, see [5], [12], [13], [27], [40]. Although the test of equal means or covariance operators may detect the difference in the distributions, the power of the test may be quite low for more general alternatives.====Instead of comparing only the mean functions or covariance operators, we are interested in testing a general null hypothesis of ====. Moreover, rather than testing the equality of the distribution functions of the functional random variables, see [15], we consider the hypothesis ====where ==== denotes the ==== (CF) of the ====th sample.====The proposed test statistic is of Cramér–von Mises type with the distance of the ==== (ECFs), ====, where ==== is the ECF of the ====th sample and ==== denotes some probability measure on the dual space to ====. The choice of the measure ==== is investigated in Section 4, and we show that it plays a crucial role in the performance of the test. Choosing the measure ==== as the distribution of a centered Gaussian process on ==== functions with compact support simplifies the evaluation of the test statistics substantially since the integral w.r.t. the measure ==== is replaced by Lebesgue integral over ====. We consider Gaussian measures characterized by covariance operators that may be represented by a positive semi-definite function ====.====Technically, the proposed two-sample test statistic is a degenerated V-statistic and may be computed straightforwardly for the discretely observed functional data for any choice of the Gaussian measure ====. We show that the choice of the covariance operator, i.e., the function ====, is crucial for the power of the test. We also give some practical recommendations for location and scale shifts.====The paper is organized as follows. In Section 2, we review the characteristic functionals, in particular, the spaces of continuous and square-integrable functions are discussed. In Section 3, we describe the proposed Cramér–von Mises type ECF-based test statistic with a Gaussian measure ====. Computational aspects of the two-sample test statistic for discretely observed functional data are discussed in Section 4, and Section 5 contains results of a small simulation study. Some simple extensions to contrasts and ====-sample comparisons are discussed in Section 6.",Functional ANOVA based on empirical characteristic functionals,https://www.sciencedirect.com/science/article/pii/S0047259X21001561,29 October 2021,2021,Research Article,78.0
"Novo Silvia,Vieu Philippe","MODES, Departamento de Matemáticas, CITIC, Universidade da Coruña, Spain,ITMATI, Spain,Institut de Mathématiques, Université Paul Sabatier, France","Received 23 September 2021, Revised 20 October 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104871,Cited by (7),"Despite of various similar features, Functional Data Analysis and High-Dimensional Data Analysis are two major fields in ==== that grew up recently almost independently one from each other. The aim of this paper is to propose a survey on methodological advances for variable selection in functional regression, which is typically a question for which both functional and multivariate ideas are crossing. More than a simple survey, this paper aims to promote even more new links between both areas.","Nowadays, Functional Data Analysis (FDA) is among the main fields in Statistics. The rich production is confirmed in various surveys (see, for instance, Goia and Vieu [35], Aneiros et al. [4]). In the beginning, the presence of functional data in applications was rare. However, with the development of modern technology most applied sciences have to treat datasets containing one, or more, functional object. For the same reasons one has to treat High-(but finite) Dimensional Data, and High-Dimensional Statistics (HDS) grew up at the same time with FDA. The main common feature of both fields is that they take part of the recent infatuation for Big Data Analysis. At the beginning, both fields developed in the statistical community in rather independent ways but the benefits that one could get by crossing ideas from both fields have been highlighted in the last decade as well in the HDS community (see Ahmed [1], Sangalli [63], Vieu [67]) as in the FDA community (see Aneiros et al. [5], Bongiorno et al. [12]). Following its 50 years long tradition of publishing top level innovative methodological advances on multidimensional data analysis, the Journal of Multivariate Analysis has played a leading role in the last decade for bridging gaps between FDA and HDS. This is, for instance, attested by various special issues aiming to promote methodological advances by linking both fields (see Goia and Vieu [35], Aneiros et al. [4], [7]). This paper aims to celebrate this 50th birthday by proposing a review on variable selection methods within a functional framework which is a topic where both fields are crossing in a natural way.====When dealing with a regression problem with one (or more) functional predictor, sometimes with some non-functional multivariate predictor, one has many concerns. First of all one has to take into account the fact that we are dealing with infinite-dimensionality of functional objects (see, for instance, Cuevas [16]) and to keep in mind the necessity of building models balancing flexibility, dimension reduction properties and interpretability (Vieu [67]). Secondly, one also has to worry about the quantity of information to be included into the model: this concerns the number of predictors as well as the number of discretizations that one has at hand for each functional predictor. In a pragmatic way, as it is the case in HDS with non-functional high-dimensional predictors, one would like to determine a smaller subset of variables that exhibits the strongest effects on the response (see Hastie et al. [37]). In the last decade, there has been a rather large production on sparse modelling and variable selection techniques in functional setting, and this article is aiming to review the state of art on this topic.====Our paper is organized as follows. Because most of the variable selection procedures in functional setting were extended from finite-dimensional regression, we start in Section 2 with a selected review on the techniques employed in HDS with main attention on penalized methods. In the exposition we will present the procedures by splitting them according to the nature of the model: linear, grouped and additive regression. Of course, Section 2 is not supposed to be an exhaustive review of the very wide set of contributions existing in a multivariate setting, but only a presentation of those contributions which have been adapted for FDA. In Section 3 we will go through the functional setting. The rich production and the variability in types of models, variables included and tools, led us to make distinction between four types of methodologies. Sections 3.1 Selection of scalar covariates, 3.2 Selection of scalar covariates with functional origin, 3.3 Selection of functional covariates are dedicated to scalar response models which are most often studied in the literature: firstly we will study selection of scalar variables in models which contain some functional predictor, secondly we will deal with the selection of scalar variables derived from the discretization of a functional object, and thirdly with the selection of functional objects. Finally, Section 3.4 concerns the regression models with functional response. To conclude the paper, in Section 4 we will present some ideas about how variable selection could behave in functional regression in the next following years.====To make simpler the exposition of all the methodologies, we will assume without loss of generality that the involved variables (functional or not) are centred to have zero mean. In the same way, we will not show the assumptions (neither on the random errors nor the covariates) used in the different methodologies (note that such assumptions could change from one methodology to other one).",Variable selection in functional regression models: A review,https://www.sciencedirect.com/science/article/pii/S0047259X21001494,29 October 2021,2021,Research Article,79.0
Belli Edoardo,"MOX - Modeling and Scientific Computing, Department of Mathematics, Politecnico di Milano, Italy","Received 30 November 2020, Revised 12 July 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104882,Cited by (0),With a focus on linear models with smooth functional ,"Smooth and highly collinear data generally arises in applications where high frequency acquisition devices are employed, like chemometrics, spectroscopy and electrical engineering. A natural way of modeling these data-generating processes is by means of functional data analysis (FDA), where each covariate can be seen as a smooth function ==== that has been evaluated at sequential timesteps, often with missing values and different spacing between the observations. FDA has been an active research field in the last decade, with contributions that encompass a wide range of statistical problems, including theoretical aspects [2], [17], [27]. In dealing with classification and regression problems with functional covariates, the main approaches can be summarized in three classes of methods: linear, nonparametric, and semiparametric. Regarding the first one, which will be the main focus of this work and will be discussed more in depth later, the main advantage lies in the interpretability of the results, while being limited by the inherent lack of flexibility. Nonparametric approaches [22] are instead employed when nonlinear relationships are involved, with the main drawback of having a high sensitivity to dimension. The k-nearest-neighbor (kNN) kernel has proven to be successful as it is location adaptive and easier to tune [12], [25], [37], while the problem of dimensionality reduction has been tackled by means of stepwise algorithms [21] and multistage additive models based on a predetermined grouping structure [4]. Semiparametric methods try to balance the trade-off between the two previous approaches, as in the functional partial linear model [41] and the functional single-index model [1], [50], the latter being further studied in conjunction with additive models [20] and domain partitioning techniques [26]. On the assumption that the functional covariates possess some specific local variation, another interesting line of research is the one that pertains the so called impact points, where the objective is to find the points of the domain that are most influential in predicting the response, which is akin to a variable selection problem. Starting from a single impact point [43], [47], these methods have been expanded to multiple points in a nonparametric [4], [53], partial linear [39], [42] and linear setting [6]. In real world applications, it is often the case that curves are collected together with other scalar quantities, and therefore this has led to the development of hybrid approaches that combine multivariate linear concepts with functional ones, including linear [55], [66], nonparametric [5], [44] with multiple functional covariates [3] and semiparametric [51], [59]. As previously mentioned, in this work we will only consider the penalized functional linear model, but our proposed method can be implemented in many of the settings that we have referenced, by directly replacing the standard functional linear model with our estimator, given that the fitting algorithms are often iterative and do not depend on how the functional linear model is computed. For instance, our method could be used with a single-index model. Other computational and modeling refinements are of course possible, but that would be beyond the current scope. Without loss of generality, we will focus on scalar-valued functions defined on ====, but this approach can be extended to deal with vector valued functions defined on multidimensional domains. The two main aspects to be considered are the estimation of the underlying functional covariates from the raw data, and the estimation of the predictive model itself. Regarding the first one, we will assume that the functions have already been recovered and that we have a set of discrete evaluations over the same dense equispaced ====-dimensional grid over ====. Let ==== be the dataset with random i.i.d. functions ==== and responses ====, we focus on the scalar on function linear model: ====where ==== is the intercept, ==== is the coefficient function and ==== are random i.i.d. errors. In fact, the penalties that we analyze can also be employed in a generalized linear model (GLM) framework, and we will study classification problems with functional logistic regression as well. Regarding the aspect of estimating the predictive model, the coefficient function ==== is expressed as a basis expansion, with some form of regularization as an identifiability constraint, given that the theoretical functional linear model is ill-posed. A parsimonious approach is to restrict the number of basis functions, by either fixing a known suitable basis like a Fourier basis, or by considering only the first ==== eigenfunctions of the covariance operator obtained from fPCA [13], which for any given ==== explains most of the variation of the input functions in the ==== sense. On the opposite side of the spectrum, another approach is instead to employ a rich enough basis while at the same time including some form of penalization, typically an ==== penalty on ==== or its derivatives in order to impose smoothness [14], [15], [16], [62], but ====-based penalties have also been used [38], [46]. Note that the restricted basis and the penalization approaches are not mutually exclusive, and hybrid techniques have been proposed as well [36], [40], [45]. In our setting we choose to adopt the penalization approach, by using the following simple grid basis with ==== dense and equispaced knots placed on the ==== evaluations corresponding to the evaluation grid of the estimated input functions: ====which is a common solution that enables us to use any multivariate method for the numerical estimation, allowing for a proper comparison between different approaches, as the initial FDA preprocessing is shared between all the tested methods. Moreover, while all functional datasets can be seen as a discrete dataset of function evaluations on a grid, this view does not take into account the high collinearity between adjacent variables, and it is interesting to take a look at the empirical behavior of methods that were not designed for this task, but that are highly successful in other settings. The main objective of this work is to propose an adaptive penalization approach that is able to fit smooth and/or sparse coefficient functions [35], ideally being able to recover the regions of the domain in which the covariates have no effect on the response, while at the same time allowing for a smooth behavior if needed. Given the abundance of ==== applications with different requirements, it is no surprise that the literature on variable selection in linear models has experienced a significant growth in both the statistical and machine learning communities. What appears to be the most successful framework is based on the well known penalized least squares formulation (in the multivariate notation), and in particular the bridge estimator [23]: ====where ====, and ==== that controls the strength of the penalization (we omit the intercept). It is known that for ==== this yields a non-convex optimization problem, where in particular for ==== the bridge reduces to best subset selection [24]. Besides the computational issues, subset selection methods are also known to be unstable [11], and for this reason we will focus only on penalty-based approaches, although we are aware of the different stepwise algorithms. Moreover, given that the penalties are not scale-invariant, we will assume that the input data has been standardized. When ==== the problem is instead convex but we pay the price of unwanted shrinkage of the coefficients, which introduces further bias, even if we include only the correct variables in the model. For ==== in particular we obtain the lasso [57], which is a convex relaxation of best subset selection, while ==== corresponds to ridge regression [34]. Regarding our specific setting, which deals with high dimensional and highly collinear data, it is not clear which approach to adopt, as the lasso may exclude important variables from the model and produce nonsmooth coefficient functions, the ridge may yield both nonsparse and nonsmooth ones, while the usual FDA roughness penalty may be too smooth and fail to recover any sharp change in the support of the coefficient function ====. A possible solution is to impose hybrid penalties, like in the case of the elastic net [68] or the smooth lasso [33]. Our proposed approach is instead exclusively based on the nonzero centered ==== penalty [7], [54], [56], [60], [61], and it is also inspired by the adaptive ridge estimator and other reweighted bias reduction techniques, as we will discuss in Section 2. Section 3 describes our method in detail, the applications are shown in Section 4, with concluding remarks in Section 5.",Smoothly adaptively centered ridge estimator,https://www.sciencedirect.com/science/article/pii/S0047259X21001603,29 October 2021,2021,Research Article,80.0
"Helander Sami,Laketa Petra,Ilmonen Pauliina,Nagy Stanislav,Van Bever Germain,Viitasaari Lauri","Aalto University School of Science, Finland,Charles University, Faculty of Mathematics and Physics, Czech Republic,University of Namur, Department of Mathematics and Namur Institute for Complex Systems (Naxys), Belgium,Aalto University School of Business, Finland","Received 28 February 2021, Revised 26 June 2021, Accepted 20 October 2021, Available online 29 October 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104880,Cited by (0),This paper develops a new integrated ball (pseudo)metric which provides an intermediary between a chosen starting (pseudo)metric ==== and the ==== distance in general function spaces. Selecting ,"Measuring the shape similarity or dissimilarity of functions has been a concurrent problem in many fields of research. In functional data analysis, the shape features often comprise the key modes of variance in the functional observations and comparison problems such as classification are common. This can be seen, for instance, in the functional depth literature, where recent advances have begun emphasizing shape features as the focal point of analysis. See, for example Sguera et al. [35], Claeskens et al. [14], Harris et al. [22] and Nagy et al. [31], [32] for recent approaches to shape-sensitive functional depths.====Similarly, attention has been devoted to developing metrics that are more sensitive to variations in shape. The use of such metrics has become commonplace in shape and pattern matching applications in machine learning and computer vision. See Alt and Godau [2], Huttenlocher et al. [23], Rucklidge [33], Yi and Camps [40], Veltkamp and Hagedoorn [39], Alt et al. [1], Brakatsoulas et al. [10], Aronov et al. [5], De Carvalho et al. [17], Jiang et al. [24], Timmermans et al. [36] and Timmermans and von Sachs [37] for a wide variety of applications for such shape-sensitive metrics. Popular approaches include considering Hausdorff or Fréchet distances between the graphs of the functions. However, as both of these distances are based on a global supremum, they do not consider the local likeness of the graphs. Therefore, in the literature some attention has been devoted to developing averaged (i.e. integrated) versions of these distances.====Baddeley [6] developed an integrated alternative to the Hausdorff metric by substituting the supremum for an integral in an expression equivalent to the usual definition. A more direct approach was taken by Marron and Tsybakov [30] who proposed an error criterion for nonparametric curve estimation, where the features of the curves are matched “vertically” in value and “horizontally” in timing. Schütze et al. [34] proposed an averaged version of the Hausdorff distance by replacing the sup–inf in its definition with an ==== mean. Vargas and Bogoya [38] and Bogoya et al. [8] further developed the notion to an integrated counterpart.====The integrated Fréchet distance was studied in Buchin [11] who considered several approaches for both a summed and an averaged Fréchet distance and analyzed their properties. Efrat et al. [18] developed a notion of dynamic time warping that can be seen as an integrated version of the Fréchet distance. Buchin et al. [12] considered the partial curve matching problem where the similarity of two curves was measured by the largest fraction of their lengths that could be matched within a certain Fréchet distance threshold.====Many of the approaches discussed above, however, do not satisfy the key properties of a metric. See for example Marron and Tsybakov [30], Maheshwari et al. [28], and the extensive analysis provided by Buchin [11] for more details. Specifically, in the case of integrated Fréchet distances, the triangle inequality fails for approaches considered previously.====Use of pseudometrics (also called semimetrics) in FDA has been shown to be of interest. See, for example, Geenens [20], and the review papers Aneiros et al. [3], [4], Cuevas [15] and Goia and Vieu [21]. In this paper, we generalize any pseudometric ==== on a function space to a family of integrated ball pseudometrics ====, obtained through local integration of ====. The locality parameter ==== allows to control the shape sensitivity of the pseudometric by balancing between ==== (for ====) and the ==== distance (for ====). Furthermore, we show that ==== is a pseudometric for any ====. Convergence of suitable discretizations ensures that the integrated ball pseudometric can be computed in practice. When the integrated distance ==== is a metric, then, under mild technical assumptions, the integrated version is a metric as well.====The theoretical results are derived for general (pseudo)metrics ====. In a second part of this paper, we apply the integrated ball construction to the Hausdorff and Fréchet distances, ==== and ====, and provide their locally integrated counterparts, ==== and ====. We show important properties of the integrated ball Hausdorff and Fréchet distances and study their behavior in a variety of simulated outlier detection settings. The appeal of local distances in the functional context is illustrated in Fig. 1, where examining local distances ==== and ====, ====, ====, highlights shape-outlying curves. In this example, the compactness of the domain ==== of the functional data allows to restrict ==== to ====.==== Fig. 1 illustrates two models, drawn in gray in the left panels, with some contaminating observations drawn in color. On the top row, the outlying observations differ from the base model in phase, whereas on the bottom, three of the outliers have different timing of the sharp feature and one lacks it entirely. In the middle and right panels, the average ==== and ==== distances between the observations and each outlier are drawn in corresponding color, as a function of ====, with the between-observation average distances drawn in light gray.====In the first model, global Hausdorff and Fréchet distances (corresponding to ====) fail to properly separate the outliers from one another and from the rest of the observations. In the second model, the ==== distance (for ====) does not properly separate outlying curves and fares poorly in detecting them among the observations of the base model. Examining the curves ==== and ==== shows a different behavior for the outlying curves and allows proper separation on a wide range of values of the locality parameter ====.====The rest of the paper is organized as follows: in Section 2 we introduce a general functional framework, under which we present the main contribution of the paper, the integrated ball (pseudo)metric ====, and analyze its properties. We show that, under non-restrictive assumptions, ==== is indeed a proper (pseudo)metric with many desirable properties. Furthermore, we provide an important convergence result for ==== for discrete approximations of the functions, that ensures the feasibility of computations of the (pseudo)metric in practice. In Section 3, we formally introduce our two leading examples, the Hausdorff and Fréchet distances ==== and ====, and define their locally integrated counterparts, ==== and ====. We show that ==== and ==== satisfy our main assumptions and therefore ==== and ==== exhibit the properties stated in Section 2. In Section 4 we apply the integrated ball Hausdorff and Fréchet distances in simulated outlier detection settings, and study their behavior as a function of the locality parameter ====. A discussion closes the paper.",Integrated shape-sensitive functional metrics,https://www.sciencedirect.com/science/article/pii/S0047259X21001585,29 October 2021,2021,Research Article,81.0
"Mohammadi Raziyeh,Kazemi Iraj","Department of Mathematical Sciences, Isfahan University of Technology, Iran,Department of Statistics, Faculty of Mathematics and Statistics, University of Isfahan, Iran","Received 10 February 2021, Revised 6 October 2021, Accepted 6 October 2021, Available online 26 October 2021, Version of Record 6 November 2021.",https://doi.org/10.1016/j.jmva.2021.104856,Cited by (0),", the estimate of variance–covariance components can be merely accessible. We also present a spline mixed model to account for the ==== effect. To highlight the usefulness of our methodology, we conducted a simulation study and analyzed a data set collected on type 2 ==== with microalbuminuria over a 6-year prospective cohort study. Findings show that our proposed robust model leads to convincing conclusions in empirical studies.","Longitudinal studies arise routinely in various areas of science, such as biomedical and biology. The data structure involves the same group of subjects taken repeatedly over a time sequence. A linear mixed-effects (LME) model is a primary tool for deliberating longitudinal features into the data analysis process. Denote ==== the response vector for the ====th subject, ====. The familiar LME model is given by ====where the ==== and ==== are the design matrices corresponding to the ====-dimensional fixed-effects vector ==== and ====-dimensional random-effects vector ====, respectively, and ==== is the residual vector to account for the within-subject variations.====The success of the standard model (1) for estimation and prediction purposes depends on convincing strict assumptions though it may fail to hold in empirical applications. Specifically, in the presence of outliers, the normality assumption of residuals is suspicious. Operative techniques to accommodate outliers in longitudinal studies rely on making the mixed-effects model sufficiently robust such that the ongoing model can down weight outliers with reducing their influence [5], [13], [22], [32]. Demidenko [4] extended a parametric regression model for cross-section data based on the Huber function to allow all parameters to be a part of the estimation process. The proposed Huber distribution is mainly helpful in minimizing outlying data points but is no longer helpful for asymmetric situations. So far, no serious attempt has been untaken to improve robustness with the skewed pattern. An appealing extension of (1) to deal with these complications is to initiate new versions of the underlying distribution.====Such appealing motivated us to construct a skew-Huber family of multivariate distributions aimed at the robustifying model (1). To clarify it, let ==== be a random vector whose components are independently and identically (====) Huber distributed. To construct the skew multivariate form, we use the transformation ====, where ==== is a location parameter, and ==== is a real lower triangular matrix with positive diagonal elements, followed by extending familiar methods to create skew-elliptical distributions [12], [23]. A restriction of traditional Huber’s approach relates to presuming constant the tuning parameter. From a robustness point of view, the desire is a smaller value of this parameter, whereas finding a suitable estimate to balance robustness and efficiency is still challenging. [29] suggested choosing the parameter based on a data-driven approach or making it data-dependent. Operationalizing the scheme in multivariate settings is more motivating. Depending on occurring outliers separately at each particular subject or data point at a specific time, they can have a varying impact on inference. Thus, it seems realistic to allow the tuning vector to be subject-specific or time-varying in the model specification of (1).====Some benefits of using the proposed robust model include (i) identifying outlying points that are inconsistent with the normal and robustifying model under outliers, (ii) assigning appropriate weights to data points that deviate from the normal, (iii) providing joint estimating the tuning parameter with variance–covariance components and fixed effects using an easy algorithm, (iv) facilitating the computational process to implement them in freely available software packages, (v) allowing for computing the model selection measures and other quantities of interest, and (vi) capturing main features of observed data in comparison with some competing models, such as the multivariate skew-normal and skew-t [1], [7], and accordingly leading to convincing conclusions in empirical studies.====To show the performance of our model, we analyze a set of data collected on type-2 diabetic patients with microalbuminuria based on a 6-year prospective cohort study in Isfahan’s Endocrine and Metabolism Research Center (IEMRC) in Iran. The study aimed to investigate the effect of some risk factors, such as the duration of diabetes, on microalbuminuria. Primary data analysis showed that the mean profile plots of microalbuminuria were nonlinear in diabetes duration. This event can influence the pattern of change in diabetes. Thus, imposing a flexible, functional form of the mean structure to the duration, assuming a linear scheme on other covariates, is essential to improving fitness. In a semiparametric setting, it is worth examining the application of penalized splines for the duration trends over time. We investigated the usefulness of our proposed model. We found that it better fitted to the data than standard normal, student’s t, and skew-elliptical distributions, including skew-normal and skew-t models.====The plan of this paper is given as follows. In Section 2, we introduce the multivariate skew-Huber distribution and find its main properties. In Section 3, we present our robust LME model together with some familiar submodels. Section 4 discusses the parametric estimation process, including a reparameterization of the covariance matrix via the modified Cholesky decomposition technique. In Section 5, we present some model selection criteria. In Section 6, we conduct a simulation study to investigate the robustness of our proposed model compared to comparative models that deal with outliers. In Section 7, we analyze a longitudinal data set on type 2 diabetic patients to highlight the value of our proposed model in empirical studies. Section 8 includes discussions and concluding remarks.",A robust linear mixed-effects model for longitudinal data using an innovative multivariate skew-Huber distribution,https://www.sciencedirect.com/science/article/pii/S0047259X21001342,26 October 2021,2021,Research Article,82.0
"Nasri Bouchra R.,Rémillard Bruno N.,Bahraoui Tarik","Centre de Recherche En Santé Publique and Département de Médecine Sociale Et Préventive, École de Santé Publique De l’Université de Montréal 101, Avenue du Parc, Montréal (Québec) H3N 1X9, Canada,CRM, GERAD and Department of Decision Sciences, HEC Montréal 3000, Chemin de la Côte-Sainte-Catherine, Montréal (Québec), Canada H3T 2A7,Département de Mathématiques, Université de Sherbrooke 2500, Boul. de l’Université , Sherbrooke (Québec), CanadaJ1K 2R1","Received 8 February 2021, Revised 12 October 2021, Accepted 13 October 2021, Available online 23 October 2021, Version of Record 30 October 2021.",https://doi.org/10.1016/j.jmva.2021.104857,Cited by (2)," (Nasri and Rémillard, 2021) includes all the methodologies proposed in this article.","Inference methods for detecting change-points in the distribution of independent univariate observations focused first on detecting a change in the mean [24]. After that, researchers looked at detecting changes in the location parameters [34] and distribution functions [8], [9]. [3], [19] then considered change-points in the parameters of the conditional mean and conditional variance of univariate time series. The multivariate case for independent random vectors was studied by [26] for change-point in the copula, [18] for the joint distribution function and the copula. Their work was then extended by [7] who considered strongly mixing time series in addition to independent random vectors for change-point detection in the associated copula; they also proposed a new sequential empirical process specially designed for copula problems. [33] also worked with stationary ====-mixing sequences to detect changes in their dependence structure using conditional correlation instead of copulas. However, when dealing with time series, instead of assuming strong mixing conditions, one can also consider change-points in the distribution of the innovations; see, e.g., [2], [15] in the univariate ARMA case. For example, in the AR(1) model ====, one might want to test the null hypothesis of no change-point in the distribution of the innovations ====, with ==== being the unknown common distribution function under the null hypothesis of no change-point. In this case, under smoothness conditions on the density ==== of ==== and assumptions on the convergence of the estimation of the parameters, using the residuals ====, one gets [15] that ==== converges to ====, where ====, ==== converges to ====, ==== converges in law to ====, and ==== converges in law to ====. In [2], ==== since it was assumed that the mean ==== were known, yielding ====. This simple example shows that when using residuals, the limiting sequential empirical process ==== depends on the estimation error of unknown parameters. However, since ==== is unknown, it must be replaced by the empirical estimator ====, based on residuals, and tests of change-point are functionals of the sequential empirical process ====converging to ====, which no longer depends on the estimated parameters.====In this paper, we are interested in change-point tests for non-observable independent random vectors ==== associated with a multivariate time series ====. For example, one might consider generalized error models [10], i.e., for each fixed ====, ==== are iid with a continuous distributions function ====, where ==== is a ====-measurable mapping and ==== is a sigma-algebra for which ==== is measurable. These models include stochastic volatility models, ARMA models, and also regime-switching models. In the latter case, one can take ====, so ==== is the uniform distribution function over ====. Note that even if the univariate series ==== are iid for each fixed ====, the joint distribution function of ==== might depend on ==== [21]. However, under the null hypothesis of no change-point, the ==== are iid with continuous distribution function ====, margins ====, and unique associated copula ====. More precisely, we want to test ====where ==== are continuous distribution functions on ====.====Our first aim is to show that under realistic assumptions of the behavior of the sequence empirical process constructed from the pseudo-observations ==== with components ====, ====, the asymptotic distribution of sequential empirical processes used for detecting change-points do not depend on the estimated parameters, i.e., the limiting distribution is the same as if we were able to observe the ====s. Furthermore, we show that the new sequential processes developed by [7] for testing change-point in the associated copula can also be used with pseudo-observations, without any change in the inference methods. Since tests statistics based on these processes were shown to be quite powerful, our results are important from a practical point-of-view. In [7], the authors did not consider pseudo-observations but considered strong mixing sequences, without modeling the serial dependence. Their results are very important but we believe that modeling the dynamics of the time series is very convenient. Next, performing change-point tests for multivariate data is a bit different from the univariate case where in general the limiting distribution of the test statistics based on the sequential empirical process is distribution free. In our context, the test statistics are functionals of sequential empirical processes of pseudo-observations ====, since the ====, and in general, the limiting distribution of these statistics under the null hypothesis depends on the unknown joint distribution function ==== or its unknown associated copula ====, making it impossible to construct tables or to compute ====-values. Our second aim is to circumvent this problem, by showing that multipliers bootstrap and traditional bootstrap are valid even for pseudo-observations. Note that [7] also used multipliers, as proposed in [28], [30], for dealing with unknown copula functions. Under strongly mixing assumptions, they also proposed to use dependent multipliers. Since we work with serially independent generalized errors here, we only need independent multipliers or traditional bootstrap. Finally, our third aim is to find out which test statistics perform better and which bootstrapping methods are faster. To this end, in addition to numerical experiments, we also look at the asymptotic distribution of sequential processes under contiguous alternatives.====The paper is organized as follows. In Section 2, the change-point problem using pseudo-observations is presented and the main convergence results are stated. The bootstrapping methods as well as their validity are investigated in Section 3. Numerical experiments to compare the finite sample performance of the proposed tests are presented in Section 4, while in Section 5, we consider the limiting behavior of the sequential empirical processes of pseudo-observations under contiguous alternatives. These results explain why in the copula setting, statistics of the new sequential empirical process proposed by [7] perform so well. Finally, in Section 6, we revisit examples considered in [1], [7].",Change-point problems for multivariate time series using pseudo-observations,https://www.sciencedirect.com/science/article/pii/S0047259X21001354,23 October 2021,2021,Research Article,83.0
"Harrar Solomon W.,Kong Xiaoli","Dr. Bing Zhang Department of Statistics, University of Kentucky, Lexington, KY 40536, USA,Department of Mathematics, Wayne State University, Detroit, MI 48202, USA","Received 1 October 2021, Revised 5 October 2021, Accepted 5 October 2021, Available online 22 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104855,Cited by (4),"In this paper, we give the most current account of methods for comparison of populations or treatment groups with high-dimensional data. We conveniently group the methods into three categories based on the hypothesis of interest and the model assumptions they make. We offer some perspectives on the connections and distinctions among the tests and discuss the ramifications of the model assumptions for practical applications. Among other things, we discuss the interpretation of the hypotheses and results of the appropriate tests and how this distinguishes the methods in terms of what data type they are suitable for. Further, we provide a discussion of computational complexity and a list of available R-packages implementations and their limitations. Finally, we illustrate the numerical performances of the various tests in a simulation study.","High-dimensional data have been a subject of theoretical and applied investigations in the last few decades sparked by advances in technology that allowed a large number of observations to be collected from each analysis unit (subject). Examples include data from genomic studies, biological studies, financial studies, satellite imaging, modern diagnostic and intervention modalities, etc. To analyze these data, in particular in the context of group or treatment comparison, the asymptotic theory requires both the sample sizes and dimensions to diverge. In this paper, we focus mainly on reviewing some of the recent tests for group or treatment comparison in terms of mean vectors, location parameters or meaningful functionals of the distribution function, also known as the nonparametric relative effects, for high-dimensional populations.====Inference for comparison of high-dimensional populations, especially in the context of treatment effect comparison, considered in the literature can be categorized into three groups depending on the quantities used to formulate the hypothesis: (a) mean-based; (b) location-based; (c) distribution-function- or relative-effects-based. Most of the methods in all the three categories are, broadly speaking, nonparametric in the sense that they do not assume a specific distributional form for the populations. The mean-based approaches assume that the mean vectors and, mostly, high-order moments of the populations exist and, therefore, we refer to them as parametric methods. On the other hand, the location-based methods generally assume that the populations are centered at some location parameters by which comparison of the populations is made. These methods also assume a general family of symmetric distributions for the populations, e.g., elliptically contoured distributions, and, therefore, we refer to them as semiparametric methods. A fully nonparametric approach does not require a parameter in the model by which the hypothesis is formulated. Rather, a comparison of the populations is made in terms of distribution functions or some meaningful functionals of them. We refer to these methods as nonparametric methods.====To elaborate on the distinction between these three categories, we introduce some notation which are also used in the remainder of the paper. We suppose that there are in total ==== mutually independent populations. Let ==== be a ====-variate random sample from the ====th population, for ====. We denote the total sample size by ====. For brevity, we will omit the one-sample case (====) and describe the two-sample (population) situations (====) in detail. The extension to the multi-sample (population) situation generally follows along similar lines.====Suppose the mean of ==== exists and is denoted by ====. To compare groups or treatment effects, the parametric methods state the hypothesis on the mean vectors as ====However, one cannot always assume the mean vectors exist. Semiparametric methods generally assume ==== and ====, where ==== and ==== are symmetric distributions. The hypothesis is formulated as ====Here, ==== and ==== are not necessarily the mean vectors and, strictly speaking, the means may not even exist. For this reason, tests in this category are based on spatial signs or spatial ranks.====For each ====, let ==== be the marginal distribution of ====. Here, we are using the normalized version ofthe distribution function, which is defined by ====where ==== and ==== are the right- and left-continuous versions of the distribution function, respectively. Using the normalized distribution function allows us to treat discrete and continuous cases in a unified manner [e.g., Akritas et al., 1997]. One may formulate the nonparametric hypothesis in terms of the marginal distributions as ====A stronger but rather restrictive form of this hypothesis is one that states equality of the joint distribution functions.====Earlier works for nonparametric tests focus mainly on hypothesis formulated in terms of distribution functions [13], [14], [38], [52], [60], [63], [69], [70]. A limitation of such formulation is that the results of the tests are difficult to interpret when the null hypothesis is rejected and the tests are not consistent against any fixed alternative [12].====A more interpretable and meaningful nonparametric hypothesis is one based on the so-called nonparametric relative effects [11], [15], [55], which are the multifactor or multivariate analogs of Wilcoxon–Mann–Whitney effects. For the ====th group and ====th variable, define the nonparametric relative effects ==== by ====where ==== is the average of all marginal distribution functions. Let ==== be a random variable that is independent of all ====. Then ====Therefore, ==== indicates the tendency of observations on the ====th variable from group ==== to be larger (smaller) than observations from the average distribution according as ====. More importantly, ==== is interpreted as on the ==== variable individuals in the first group tend to have smaller values compared to those in the second group. In view of this, one may consider the testing problem ====where ====. The quantity ==== does not involve any parameter nor require the existence of any moment. Besides these obvious advantages, the hypothesis in terms of the relative effects does not impose equality of marginal nor joint distributions under the null hypothesis. The significance of this versatility is that the treatment groups could be different but in ways that are not interesting to the researcher. Note that the hypotheses of equality of all marginal distributions or joint distributions between the two treatment groups imply ==== for ====. Therefore, this testing problem is the nonparametric analog of the Behrens–Fishers problem for multivariate data [15].====The approaches in the three categories are designed for different hypotheses and it may not be practical to compute and conduct all of them in a given situation. The interpretation of the hypothesis and the test results may be different. For example, the mean-based methods can at best be applied when we are willing to assume that the populations have first and second moments. The location-based semiparametric methods generally assume symmetric distributions and their parameters may not be suitable for comparison when this assumption is known to be violated, for example when the data is measured on an ordinal scale. The nonparametric method based on relative effects is the most general one and is appropriate for continuous, count, binary or ordered categorical data. However, the power of these methods may not be as large when the assumptions of the parametric or semiparametric methods are plausible.====Mean-based tests for high-dimensional situation have been widely studied over the past twenty five years. In a review paper, Hu and Bai [42] outlined high-dimensional tests for mean vectors and covariance matrices. For testing (1), Hu and Bai [42] discussed Hotelling ==== test [4], [41], non-exact tests [24], [25], asymptotic normal tests without the normality assumption [6], [22], scale-invariant test [65], Euclidean norm or Mahalanobis distance tests [17], MANOVA and multiple comparisons [18], [43], [67], regularized ==== test [21], random projection tests [53], [71], etc. Some of these high-dimensional methods assume multivariate normality [24], [25], [27], [32], [44], [50], [62], [65], [66], [80], while others assume existence of higher-order moments and pseudo-independence in the sense that higher-order mixed moments can be factored into the product of the corresponding univariate moments [1], [5], [6], [22], [58], [64], [67], [79], [83]. A few others require a different form of weaker dependence but they are still parametric methods [17], [18], [20], [28], [31], [34], [47], [54], [77], [85]. The spatial sign and spatial rank tests are multivariate extensions of the sign and rank-sum tests which serve as robust alternatives to the mean-based tests for heavy-tailed data [see e.g., [57], [56]]. The classical ones are not applicable when the data dimension exceeds the sample size because the covariance matrices computed from the sample are singular. In high-dimensional cases, the spatial signs were used for constructing one-sample nonparametric test in Wang et al. [75] assuming elliptically contoured distribution. It can be viewed as a semiparametric extension of Chen and Qin [22]. Two-sample spatial sign- and spatial-rank tests were proposed in Chakraborty and Chaudhuri [19] under weaker dependence conditions. Scale invariant two-sample signed-rank and spatial rank tests for locations were investigated in Feng et al. [30], [29]. Not much progress has been made in the nonparametric category. To date, the only available works are Wang and Akritas [73], Ghosh and Biswas [33] and Kong and Harrar [46].====The aim of this paper is to provide the most current account of the literature for high-dimensional tests in three categories and offer some perspectives on the connections and distinctions among the tests. It is also our aim to provide a nearly complete list of available R-packages implementation of these tests and shed some light on their computational efficiencies and limitations. Furthermore, we illustrate numerical performances by comparing most of the reviewed tests in the same simulation. Due to space limitations, we cannot do an exhaustive review of all the developments in this thread of research, although some of them are excellent and interesting for the field of high-dimensional data analysis. We focus on a review of the works for the most recent five years and we refer readers to Hu and Bai [42] for a fairly complete account of the earlier methods. The remainder of the paper is organized as follows. In Section 2, several parametric approaches are reviewed and discussed. Semiparametric and nonparametric approaches are reviewed and discussed in Sections 3 Semiparametric tests, 4 Nonparametric tests , respectively. R-package implementations and their computational efficiencies are discussed in Section 5. A simulation study is given in Section 6 and our conclusions are summarized in Section 7.","Recent developments in high-dimensional inference for multivariate data: Parametric, semiparametric and nonparametric approaches",https://www.sciencedirect.com/science/article/pii/S0047259X21001330,22 October 2021,2021,Research Article,84.0
"Liu Shuangzhe,Leiva Víctor,Zhuang Dan,Ma Tiefeng,Figueroa-Zúñiga Jorge I.","Faculty of Science and Technology, University of Canberra, Australia,School of Industrial Engineering, Pontificia Universidad Católica de Valparaíso, Chile,School of Mathematics and Statistics, Fujian Normal University, China,School of Statistics, Southwestern University of Finance and Economics, China,Department of Statistics, Universidad de Concepción, Chile","Received 30 September 2021, Accepted 30 September 2021, Available online 20 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104849,Cited by (7),", psychometrics, and ====: namely in efficiency comparisons, sensitivity analysis, and local influence diagnostics.",None,Matrix differential calculus with applications in the multivariate linear model and its diagnostics,https://www.sciencedirect.com/science/article/pii/S0047259X21001275,20 October 2021,2021,Research Article,85.0
Bhattacharya Ritwik,"Department of Mathematics and Statistics, McMaster University, Hamilton ON L8S 4K1, Canada,Department of Industrial Engineering, School of Engineering and Sciences, Tecnológico de Monterrey, Querétaro 76130, Mexico","Received 4 October 2021, Accepted 5 October 2021, Available online 19 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104854,Cited by (3),"In this article, the joint best ==== future order statistics. Both scale and location-scale family of distributions are considered as the ==== for the underlying random variables.","The issue of prediction of future unobserved failure times has been of great interest in reliability life-testing experiments. The problem can be mathematically formulated as follows. Consider a continuous (lifetime) distribution with probability density function ====where ==== and ==== are the location and scale parameters, respectively. Suppose the first ==== order statistics (that is, a Type-II right censored sample) ====out of a sample of size ==== from (1), are observed. We are then interested in predicting the unobserved future order statistics ====, based on the ==== observed order statistics.====To begin with, let us focus on the importance of the prediction problem. The prediction of future order statistics can be viewed as follows: consider a ====-component parallel system. Based on the information on the first ==== components failure time, one may wish to predict the system failure time, which is the ====th ordered failure time. Similarly, the ====th ordered failure time is the lifetime of a ====-out-of-==== system, and one may then wish to predict it. [8] showed the application of prediction of order statistics for detecting outliers. For an extensive review of prediction problems in ordered statistics, one may refer to [13]. A common alternative to point prediction is the interval prediction. However, it needs approximation techniques for most models except in few cases like exponential. An extensive review of interval prediction problems can be found in [15]. It may be noted that while most of the works concentrate on point or interval prediction, the joint prediction case has not been discussed in the literature. By joint prediction, we refer to the simultaneous prediction of more than two order statistics. This is the problem that forms the prime motivation for this paper.====The prediction problems are closely related to the estimation problems. Let us restrict our discussion to linear prediction and estimation case. [1] first developed weighted generalized least squares method for linear models. The formal construction of best linear unbiased estimate (BLUE) based on ordered data was first introduced by [14]. A list of references on the works on linear estimates can be found in the books by [2], [4]. In the setup of generalized regression model, [11] first derived the explicit expression of the marginal best linear unbiased predictor (BLUP) of unobserved quantity. This idea was extended by [12] for the case of ordered data. It is important to mention that the above mentioned BLUEs (==== and ====) and BLUP (====) possess several interesting properties. Interested readers may refer to [5], [10] and [6], [7] for pertinent details. Recently, [3] derived explicit expressions for the joint BLUPs of two future order statistics by minimizing the determinant of the variance–covariance matrix of the predictors, and also showed the non-existence of joint BLUPs when three or more order statistics are considered for the prediction. In this article, we first derive explicit expressions of joint BLUPs obtained by minimizing the determinant of the mean squared predictive error matrix. Thence, we establish the property that the BLUPs are also determinant-efficient predictors. More generally, we then establish the complete mean squared predictive error matrix dominance property of these joint BLUPs, similar in principle to the complete covariance matrix dominance property of BLUEs established by [5], [7]. Finally, we extend these results to the general case of simultaneous best linear unbiased prediction of any ==== future order statistics. All these developments are made when scale and location-scale family of distributions are both considered as the parent distribution for the underlying variables.====The rest of this paper is organized as follows. We provide a brief background on the known results on linear estimation and prediction problems first in Section 2. Simultaneous prediction of two future order statistics is then developed in Section 3. Equivalence of joint BLUPs and marginal BLUPs has also been shown here. Section 4 demonstrates the complete mean squared predictive error matrix dominance property of these joint BLUPs. The simultaneous prediction of any ==== future order statistics, along with one data-set example, are discussed in Section 5. BLUP in scale-family of distributions is briefly discussed in Section 6. Finally, some concluding remarks are made in Section 7.",On simultaneous best linear unbiased prediction of future order statistics and associated properties,https://www.sciencedirect.com/science/article/pii/S0047259X21001329,19 October 2021,2021,Research Article,86.0
"Girard Stéphane,Lorenzo Hadrien,Saracco Jérôme","Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France,Univ. Bordeaux, CNRS, Bordeaux INP, IMB, UMR 5251, F-33400, Talence, France,Inria, IMB, UMR 5251, F-33400, Talence, France","Received 1 October 2021, Revised 5 October 2021, Accepted 5 October 2021, Available online 19 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104852,Cited by (5), of the ==== and variable selection.,"Let us consider a regression setting where the goal is to estimate the link between a univariate response variable ==== and a covariate ====. When the dimension ==== of the covariate is one or two, a simple two-dimensional or three-dimensional plot can reveal the relationships between ==== and ====, and thus be useful in determining the regression strategy to be used. Such an approach is not feasible when ==== is large. A possibility to overcome dimensionality problems arising in the regression context is to make the assumption that the response variable does not depend on the whole predictor space but only on a projection of ==== onto a subspace of smaller dimension. Such a dimensionality reduction leads to the concept of sufficient dimension reduction and to the notion of dimension-reduction subspace (DRS) [22]. A subspace ==== is a DRS if ==== is independent of ==== given the orthogonal projection of ==== onto ====. In other words, all the information carried by the covariate ==== on ==== can be compressed in its projection onto ====. It is then of particular interest to estimate a DRS since, once it is identified, the initial regression problem can be solved equivalently using the low-dimensional projection of ==== onto the subspace.====Among methods providing an estimation of the dimension-reduction subspace, Sliced Inverse Regression (SIR), introduced 30 years ago in [50], is one of the most popular with 33====500 entries on Google Scholar. The basic principles of SIR are recalled in Section 2. Since it is not possible to discuss all extensions and applications of SIR, we focus on three main hot topics. In Section 3, some extensions of SIR to a multidimensional response variable ==== are presented. Section 4 discusses the adaptation of SIR to a (very) high-dimensional covariate ==== through the use of regularization methods. Finally, Section 5 is dedicated to variable selection techniques adapted to the SIR context. The paper is concluded with a short discussion.",Advanced topics in Sliced Inverse Regression,https://www.sciencedirect.com/science/article/pii/S0047259X21001305,19 October 2021,2021,Research Article,87.0
Azzalini Adelchi,"Dipartimento di Scienze Statistiche, Università degli Studi di Padova, Italy","Received 15 September 2021, Revised 4 October 2021, Accepted 4 October 2021, Available online 13 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104851,Cited by (10),"In the last two decades or so, much work has been dedicated to the portion of distribution theory stemming from the skew-normal distribution and its ramification. This contribution presents an outline of the theme, without attempting a detailed review, which would be unfeasible, given the amount of available material. The aim is to present a panoramic view of the theme, leaving out the fine details, with rather more emphasis on the evolution of the underlying ideas and on the breath of the overall developments, as for range of specific directions considered.",None,An overview on the progeny of the skew-normal family— A personal perspective,https://www.sciencedirect.com/science/article/pii/S0047259X21001299,13 October 2021,2021,Research Article,88.0
"Lee Sharon X.,McLachlan Geoffrey J.","School of Mathematics and Physics, University of Queensland, St Lucia, 4072, Australia","Received 30 September 2021, Revised 5 October 2021, Accepted 5 October 2021, Available online 13 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104853,Cited by (3),"The literature on non-normal model-based clustering has continued to grow in recent years. The non-normal models often take the form of a mixture of component densities that offer a high degree of flexibility in distributional shapes. They handle skewness in different ways, most typically by introducing latent ‘skewing’ variable(s), while some other consider marginal transformations of the original variable(s). We provide a selective overview of the main types of skew distributions used in the area, based on their characterization of skewness, and discuss different skew shapes they can produce. For brevity, we focus on the more commonly-used families of distributions.","The extensive growth in the literature on multivariate flexible distributions in the past few decades has provided a plethora of choices for modelling the distribution of non-normal data. Quite a number of these have been adopted in the area of mixture modelling and applied to practical problems in various related fields (see, for example, Riggi and Ingrassia [70], Abanto-Valle et al. [1], Contreras-Reyes and Arellano-Valle [19], Pyne et al. [69], Lee and McLachlan [39], Schaarschmidt et al. [72], Asparouhov and Muthén [6]).====As can be appreciated from the voluminous amount of proposals, there are many different ways to construct skew distributions and these can have subtle implications on the types of skewness shape that can be produced. From the classical (multivariate) skew normal and skew ====-distributions [8], [10] obtained by the perturbation approach, to copula approaches [27], [35], [73], [79], multiple-scaled approach [21], [82], and transformation approaches [29], [46], they are motivated by different theoretical and/or practical reasons and may be suitable for various applied problems. For recent accounts on flexible and/or skew distributions, see the books by Genton [24], Azzalini and Capitanio [9], and Dávila et al. [20], and also the papers by Azzalini [7], Jones [28], Ley [45], Babić et al. [11], and Adcock and Azzalini [3].====A particular group of skew distributions that have received much attention are those arising from perturbation or modulation of symmetry. They are also known as skew-symmetric distributions. These include the more commonly used skew normal and skew ====-distributions, as well as other members of the wider family of skew-elliptical distributions. The density for each of these distributions takes the form of the product of a symmetric density and a ‘skewing’ function, the latter typically taken to be the corresponding cumulative distribution function (cdf) of the symmetric density. The characterization of skewness is perhaps easier to understand by considering their generating mechanisms or stochastic representations. Consider, for example, a ====-dimensional random vector ==== following the skew normal distribution which admits a convolution-type representation given by ====where ==== and ==== are independent. In the above, ==== is a ====-dimensional vector of location parameters, ==== is a ==== positive definite matrix of scale parameters, and ==== is a ==== matrix of skewness parameters. We let ==== to denote the ====-dimensional normal distribution, ==== to denote the ====-dimensional identity matrix, and ==== to denote the zero vector of appropriate dimension. Also, we let ==== to denote the vector obtained by taking the absolute value of the elements of ====. In this case, it follows that ==== has a (====-dimensional) half-normal distribution. It can be observed from (1) that skewness is introduced via the latent variable ==== (or more specifically, ====) and is regulated by ====. By imposing different constraints on ==== and ====, we obtain different versions of the skew normal distribution with different skewness properties. For example, the (classical) Azzalini’s skew normal distribution [10] takes ====. Thus ==== becomes a ==== vector ==== and ==== becomes a scalar half-normal random variable ====, implying skewness is concentrated along a single direction in the feature space [60].====Another flexible family of parametric distributions recently studied in the context of mixture modelling is the generalized hyperbolic (GH) distribution. Originally defined as a mean–variance mixture of normal distributions, the GH distribution has a convolution-type stochastic representation similar to (1) but with the latent ‘skewing’ variable appearing also in the last term. It should be noted that, similar to the classical skew normal distribution, skewness is characterized by a scalar latent variable and thus it has similar implications on the type of skewness it can handle. This will be discussed later in the paper. Members of this family of distributions include the well-known normal inverse Gaussian distribution [16], [30], the shifted asymmetric Laplace distribution [22], and (a different variant of) the skew ====-distribution [63].====Some other approaches for dealing with non-normal data have also been considered recently. The multiple-scaled framework allows densities to have (independent) marginal tailweights by incorporating multidimensional weights in the eigen-decomposition of the scale matrix. This framework was initially demonstrated on the ====-distribution [21] and later applied to a GH distribution in Wraith and Forbes [82], the latter effectively allows for multiple independent latent skewing variable. Other works have considered the transformation approach, that is, marginally transforming the data to bring them closer to normality, then fitting mixtures of symmetric distributions before back-transforming the results into the original feature space. However, similar to the multiple-scaled approach, employing marginal transformation is essentially assuming skewness is characterized along each feature axis and so independent of each other. Proposals along this track include fitting ====-mixture models with the Box–Cox transformation [52] and more recently normal mixture models with the so-called Manly transformation [86]. Another approach that is perhaps more commonly used in the finance and economics literature is the approach using copula. It provides a convenient mechanism for constructing multivariate distributions and attempts were made in a number of papers to incorporate this into model-based clustering [27], [35], [79].====With so many different proposals for dealing with non-homogeneous and skewed data, it is sometimes difficult to choose an appropriate model for the data. The subtle differences between the type of skewness shape that different proposals can handle are also not well understood. This paper provides an overview of recent developments in this area, with an emphasis on the ways in which skewness is characterized in this models. Examples are provided to illustrate the various distributional shapes they can produce. It should be stressed that this paper is not intended to be a comprehensive survey of all relevant model-based methods. The literature in this area has grown so vast that we will focus only on the more commonly used approaches.====To establish notation, a ====-component mixture model has density given by ====where ====, are the mixing proportions, ==== denotes the density of the ====th component of the mixture model, and ==== contains the parameters of ====. The mixing proportions are non-negative and sum to one. The discussion in this paper focuses on the choices for ====, particularly those that can accommodate asymmetric or skew distributional shapes.====The remainder of this paper is organized as follows. For ease of presentation, we discuss first in Section 2 Azzalini-type (and related) distributions where skewness characterized by a single latent random variable. This includes, for example, the classical skew normal and skew ====-distributions, as well as the generalized hyperbolic family of distributions. In Section 3, we examine skew-elliptical distributions where skewness is characterized by multiple latent skewing variables. This includes the skew-elliptical family of distributions put forward by Sahu et al. [71] and the canonical fundamental distributions. In the next few sections, we address also alternative approaches for dealing with skew data in mixture modelling, including transformation approaches (Section 4), the multiple-scaled framework (Section 5.1), the copula approach (Section 5.2), and the split distribution approach (Section 5.3). Finally, concluding remarks are given in Section 6.",An overview of skew distributions in model-based clustering,https://www.sciencedirect.com/science/article/pii/S0047259X21001317,13 October 2021,2021,Research Article,89.0
"Song Jian,Yao Jianfeng,Yuan Wangjun","Research Center for Mathematics and Interdisciplinary Sciences, Shandong University, China,Department of Statistics and Actuarial Science, The University of Hong Kong, Hong Kong,Department of Mathematics and Statistics, University of Ottawa, Canada","Received 20 September 2021, Revised 23 September 2021, Accepted 23 September 2021, Available online 12 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104847,Cited by (0),"Since the introduction of Dyson’s Brownian motion in early 1960s, there have been a lot of developments in the investigation of stochastic processes on the space of ","Stochastic processes with values in the space of symmetric matrices have been attracting the attention for some years. Their introduction is commonly attributed to the celebrated work [16] by F. J. Dyson. By that time, Gaussian matrix ensembles were well known; the distribution of their eigenvalues has a density function of the form ====where ==== are parameters, and ==== is a normalization constant. The distribution (1) also appears in the Coulomb gas model: it is the probability distribution of the positions of ==== point charges which are free to move on the real line ==== under the forces derived from the potential energy ==== and in a state of thermodynamic equilibrium at a temperature ====
 (==== is the Boltzmann constant). Note that Eq. (1) is static and does not describe the evolution of the position of the point charges before reaching the equilibrium. Dyson brought in the Brownian motion to get a time-dependent model that describes the evolution of the positions from an initial distribution ====. The Brownian motion, also called the time-dependent Coulomb gas has a simple structure, and the joint density function ==== of the positions of the ==== point charges at time ==== is fully characterized as a solution to the Smoluchowski equation ====where ==== is a constant and ====is an external electric force. In particular, ==== tends to the Coulomb gas distribution ==== in (1) as ====.====This extension of Coulomb gas from the static equilibrium state to a dynamical version also applies to the associated Gaussian matrix ensembles. More precisely, Dyson introduced a stochastic process with values in the space of symmetric matrices, the eigenvalues of which coincide with the dynamical Coulomb gas model. Amazingly, this process is extremely simple: its elements are independent Ornstein–Uhlenbeck processes on the underlying field! (The underlying field is ==== for ====, ==== for ====, and the quaternion field for ====).====This deep connection between stochastic processes with values in the space of symmetric (Hermitian) matrices and the induced dynamical system of its eigenvalues, had been however quite ignored during a while afterwards. It was revived in the papers [6], [51] where the idea of Dyson was extended to the space of positive-definite matrices (ellipsoids). In the subsequent three decades, the study of these stochastic processes and their associated eigenvalue processes has been developed in much depth. Particularly, the symmetric (Hermitian) matrix-valued processes have covered Brownian motion, Ornstein–Uhlenbeck process and fractional Brownian motion.====Instead of considering the ==== particles (eigenvalues) with ==== fixed, the limits of the empirical measures of particles when ==== tends to infinity (high-dimensional limits) were studied in several models. In particular, the high-dimensional limit of the empirical measures of the Dyson’s Brownian motion is the famous Wigner’s semicircle law, which provides a dynamical version of Wigner’s Theorem for GOE and GUE (see, e.g., [1]). In this aspect, the study of large particle systems is closely related to the random matrix theory. Moreover, the equation satisfied by the limits of the empirical measures of the Dyson’s Brownian motion is the so-called McKean–Vlasov equation, which appears in the study of propagation of chaos for large systems of interacting particles (see [3], [30], [63]).====Another motivation for studying high-dimensional limits of the empirical measures of eigenvalues arises from free probability theory. By [4], the free additive Brownian motion can be viewed as the high-dimensional limit of a matrix Brownian motion with appropriate scaling. Moreover, [4], [5] developed the stochastic calculus for free Brownian motion. Besides, the non-commutative fractional Brownian motion was introduced in [52].====There is also a deep connection between matrix-valued stochastic processes and multivariate statistical analysis. Here are a few applications of these processes in recent statistical literature:====This survey reviews a selection of results from the last three decades. In Section 2, we provide a study of Dyson’s Brownian motion with full details. This includes a modern derivation of the process using Itô calculus. A limit for the processes of empirical eigenvalue measures is derived when the number of eigenvalues, or electric charges, tends to infinity. Besides, a limiting Gaussian process is derived in order to characterize the fluctuation of the empirical eigenvalue measures around their limit. In Section 3, we discuss two specific classes of stochastic processes with values in the space of positive-definite matrices, that is, Brownian motions of ellipsoids and Wishart processes. In Section 4, a more general form of stochastic processes on the space of Hermitian matrices is studied, and a link is also made with some familiar systems of interacting particles. The following Sections 5 Matrix-valued stochastic processes driven by fractional Brownian motion, 6 Matrix-valued stochastic processes driven by Brownian sheet concern extensions of Dyson’s Brownian motion in two different directions. The first extension replaces the Brownian motions in the matrix by fractional Brownian motions, and the second one by Brownian sheets. Finally in Section 7, we conclude with a discussion on open problems related to the results introduced in the preceding sections.",Recent advances on eigenvalues of matrix-valued stochastic processes,https://www.sciencedirect.com/science/article/pii/S0047259X21001251,12 October 2021,2021,Research Article,90.0
"Ditzhaus Marc,Smaga Łukasz","Faculty of Statistics, TU Dortmund University, 44221 Dortmund, Germany,Faculty of Mathematics and Computer Science, Adam Mickiewicz University, Uniwersytetu Poznańskiego 4, 61-614 Poznań, Poland","Received 11 August 2020, Revised 3 June 2021, Accepted 27 August 2021, Available online 7 October 2021, Version of Record 26 October 2021.",https://doi.org/10.1016/j.jmva.2021.104848,Cited by (3),"New inference methods for the multivariate coefficient of variation and its reciprocal, the standardized mean, are presented. While there are various testing procedures for both parameters in the ====, it is less known how to do inference in the multivariate setting appropriately. There are some existing procedures but they rely on restrictive assumptions on the underlying distributions. We tackle this problem by applying Wald-type statistics in the context of general, potentially heteroscedastic factorial designs. In addition to the ","A widely used unit-free measure of dispersion is the coefficient of variation (CV), which is the ratio of the standard deviation and the population mean. It is a popular tool to judge, e.g., the repeatability of measurements in clinical trials [20], the risk in the financial world [21] or in psychology [49], and the quantitative variability in genetics [50]. Moreover, it serves as a reliability tool in control charts [1], [11], [36]. The reciprocal of the CV, the standardized mean, is a quantity of its own interest, which can be motivated by one-way analysis of variance problems when the observations are standardized with the sample standard deviation before statistical analysis.====Various inference methods are suggested to compare two or several groups in terms of CV, or equivalently of standardized means. To get an overview, we refer to [2], [39]. In various fields, e.g. in biomedicine or psychology [6], [10], [24], [29], [33], the one-way layout is too narrow and factorial designs are needed to discuss main effects of different factors, e.g. gender, measurement, site, but also interaction effects between them: ‘====’ [32]. Consequently, the question arises: can we extend the existing methods to general factorial designs?====But first, let us come to the multivariate setting. When more than one feature is of interest, comparisons based on marginal CVs are misleading due to potentially different decisions for the single features, as pointed out by Van Valen [47] in the biology field, and does not account for correlations between the features. The solution is to use a summarizing measure for all features, e.g. the multivariate coefficient of variation (MCV). However, a drawback in this direction is that the extension is not unique and there is no default choice up until now. For example, Reyment [43], Van Valen [47], Voinov and Nikulin [48] and Albert and Zhang [4] suggest to define the MCV by ====respectively. Here ==== denotes the nonzero mean vector of a ====-dimensional random variable and ==== is corresponding covariance matrix. All these definitions reduces to the CV in the univariate ==== case. The differences of them are discussed in great detail by Albert and Zhang [4]. A further problem of the MCV is the lack of generally applicable inference methods. To the best of our knowledge, there is only a proposal by Aerts and Haesbroeck [2] for testing the equality of several MCVs following the definition of Voinov and Nikulin [48]. But their methods rely on the specific assumption of the underlying distribution and the convergence speed of their test statistic is rather slow leading to an inconsistent type-1 error control for small sample sizes; the latter is demonstrated in our simulation study.====To address all problems raised in the last two paragraphs simultaneously, we suggest Wald-type statistics leading to generally applicable testing procedures====We tackle the last aim by following a permutation strategy. It is well-known that permuting exchangeable data (e.g. the distributions in all groups coincide) leads to finitely exact tests. A prominent idea for multivariate data is to run a multivariate permutation test through the nonparametric combination methodology [5], [7], [40], [44]. An extension of this approach, so-called synchronized permutation tests, can even be applied in one-way and two-way ANOVA settings [7], [25]. However, it is less known that permutation tests can also be applied beyond the too narrow exchangeability assumption, e.g. for heteroscedastic designs. The finite exactness cannot be preserved beyond the exchangeable situation but permuted studentized statistics were shown to be still asymptotically exact for various non-exchangeable two-sample scenarios [16], [27], [35], [37]. Recently, the success story of this idea has been continued in the framework of one-way layouts [12], [13] and even general factorial designs [14], [15], [17], [19], [23], [26], [38], [45]. For the latter, Wald-type statistics, as proposed here, are favorable choices for such appropriately studentized statistics. In the univariate one-way layout, our proposal coincides with the permutation test of Pauly and Smaga [39].====The remainder of this paper is organized as follows. In Section 2, we introduce the general factorial design set-up and formulate the statistical hypotheses in terms of MCVs and standardized means. Moreover, consistent estimators of both are presented. These estimators are used to build the Wald-type statistics in Section 3, which are shown to be asymptotically exact under the null hypotheses and consistent under general alternatives. Their permutation counterparts are considered in Section 4 and the tests’ asymptotic properties are transferred to them. An exhaustive simulation study is presented in Sections 5 Simulation study, 6 Illustrative real data examples, respectively. Section 7 concludes the paper and discusses further research possibilities. All proofs are given in Appendix A. Additional details and results of the simulation study and a real data example for a univariate two-way layout are presented in the supplementary materials.",Permutation test for the multivariate coefficient of variation in factorial designs,https://www.sciencedirect.com/science/article/pii/S0047259X21001263,7 October 2021,2021,Research Article,91.0
"Basu Ayanendranath,Chakraborty Soumya,Ghosh Abhik,Pardo Leandro","Indian Statistical Institute, Kolkata 700108, India,Department of Statistics and O.R. Complutense University of Madrid, 28040 Madrid, Spain","Received 23 September 2021, Accepted 23 September 2021, Available online 6 October 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104846,Cited by (2), is one of the fundamental paradigms of ,"The systemic use of hypothesis testing, which possibly originated with Pearson’s path breaking 1900 paper [49], is one of the most important components of statistical inference. Uniformly most powerful tests, which are the optimal statistical procedures in a hypothesis testing problem, rarely exist for complex real situations. In practice the statistician has the choice of using several classical tests. These include the LR test, the Wald test and the Rao (score) test. The LR test, the Wald test and the Rao test are widely used in the statistical literature, and there are some similarities in their asymptotic properties. Up to the first order of approximation, all these test statistics have the same asymptotic distribution both under the null hypothesis and under a sequence of Pitman alternatives, i.e., a sequence of local alternatives that shrink to the null hypothesis at a convergence rate of ====. Peers [50] demonstrated that none of these tests is uniformly superior to the other two when the power function is considered. Chandra and Joshi [18], on the other hand, claimed that for large sample sizes the Rao test is more powerful. See [20], [21], [38] for some other interesting comparisons of these three classes of tests. However, all these three classes are based on the MLE of the unknown parameter over the unconstrained parameter space and/or under the constraints of the null hypothesis. Even when the MLE is not directly involved, as in the case of Rao tests for the simple null, the tests are based on the non-robust likelihood equations. It is well-known that the MLE has poor robustness properties, and this can lead to a substantial degradation in the performance of these three classes of tests under data contamination and model misspecification. This non-robustness is a major hindrance and hypothesis testing procedures which preserve the model optimality (substantially, if not fully) of the classical tests but provide a considerably enhanced stability in small neighbourhoods of the true model can represent extremely useful alternatives to the classical tests.====The LR test was formalized by Neyman and Pearson [46], [47]. The asymptotic distribution of the LR test was developed by Wilks [62]. Several refinements and extensions of the LR test have been tried in the literature over the years. Many of them are based on the minimum distance idea, and some of them have the robustness of the procedure as the main concern. Those which are specially relevant to the present work include [5], [6], [8], [37], [43], [44], [54], [56], [57], [58]. A distance based weighted likelihood approach which can lead to tests of hypothesis in the same spirit includes [1], [40].====After the emergence of the LR test, Wald [60] introduced another test procedure which is commonly referred to as the Wald test. As opposed to the LR test (and the Rao test described later), the Wald test is much easier to use when the parameter estimation is simple in the unrestricted model compared to the restricted one. Like the LR test, the Wald test is also a very popular statistical tool. However, see [30], [32], [35], [59] for some issues related to the Wald test.====Rao proposed the so called Rao test (or the score test) as an alternative to the LR test and the Wald test in 1948 [52]. Aitchison and Silvey [2] and Silvey [55] represented the Rao test in terms of the Lagrange multiplier test. Several authors have considered different interpretations of the Rao test; see, e.g., [14], [15], [16], [19], [21], [26], [27], [28], [29], [34], [36], [39], [45], [61]. Rao [53] provides a useful review. The version of the Rao test for the simple null hypothesis only uses the null parameter and so no parameter estimation is needed in this case; however the Rao test makes direct use the likelihood score function associated with the MLE, and therefore inherits all its non-robust properties even when the MLE is not actually used.====Our aim in the present work is to describe a development of robust analogues of the three types of tests based on a divergence which provides a generalization of the likelihood (and, by extension, the associated tests provide generalizations of the classical tests). In particular we will aim to achieve this through the density power divergence (DPD) of Basu et al. [9] and the corresponding minimum DPD estimators (MDPDEs). We will demonstrate that our DPD based test statistics behave comparably to the classical statistics under pure data, but provide markedly improved stability compared to the latter under data contamination and model misspecification, making them practically useful statistical tools.====Together with the empirical demonstration of their performance, we will extensively explore the theoretical robustness properties of the proposed tests on the basis of their influence functions. It will be observed that the tests give sturdy inference and produce stable levels and powers (under fixed and contiguous alternatives). See [31] for a general description of the concept of the influence function of a test statistic as well as level and power influence functions. A useful review of the influence function and the study of the robustness of test statistics is given in [41]. Recently, [3], [22], [24], [25], [51] have also presented important new results along this direction in connection with the DPD.====See [13], [48] for a general discussion of the robustness of statistical inference based on divergence measures. The articles which lend the review materials considered in this paper include, among others, [7], [10], [11], [12], [22], [23], [24], [25].====The rest of the paper is organized as follows. Section 2 gives a description of the density power divergence and the corresponding MDPDE for both unrestricted and restricted cases. Sections 3 and 4 described the robust DPD based generalizations of the three classical tests for the simple null and the composite null hypothesis respectively. Section 5 presents the robustness comparisons of the three classes of tests based on the influence function. Examples are provided in Section 6 for the multivariate normal model whereas Section 7 provides empirical comparisons of the finite-sample performances of the classes of DPD-based tests. Finally the paper ends with some concluding remarks in Section 8.",Robust density power divergence based tests in multivariate analysis: A comparative overview of different approaches,https://www.sciencedirect.com/science/article/pii/S0047259X2100124X,6 October 2021,2021,Research Article,92.0
"Park Hoyoung,Baek Seungchul,Park Junyong","Department of Statistics, Seoul National University, Republic of Korea,Department of Mathematics and Statistics, University of Maryland Baltimore County, MD, USA","Received 5 August 2021, Revised 18 September 2021, Accepted 18 September 2021, Available online 29 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104836,Cited by (1)," is larger than the number of observations ====
 ","We consider a binary classification problem where there are ==== different populations, and each is assumed to follow ==== dimensional multivariate normal distribution with a vector ==== and a common ==== covariance matrix ====. Throughout the paper, we denote boldface letters for vectors and matrices. Suppose that we have ==== observations ==== where ==== represents predictor vector, and ==== denotes the group label. For notational simplicity, we define ====, and ==== and assume that ==== for ==== and ====, ====. Then the optimal classifier minimizing the misclassification error rate, called the Bayes rule, is ====We classify a new observation ==== to group ==== when ==== and ==== otherwise. The Fisher’s linear discriminant analysis (LDA) estimates the unknown parameters ====, ==== and ==== by the sample mean vectors ====, pooled sample covariance matrix ==== and sample proportions ====, ====. Although it has a linear form derived under the normality, it has successfully obtained high accuracy on many applications to real data due to some excellent properties such as robustness to model assumptions [12]. However, the performance of LDA is not guaranteed in high dimensional data when the dimension ==== is greater than the sample size ====
 (====). The linear rule (1) needs estimators of mean vectors and covariance matrix, which are challenging problems in high dimensions. In case of ====, sample mean vector and sample covariance matrix are commonly used in (1), however, such estimators have some drawback in high dimension, for example, the accuracy of sample mean vector is very poor compared to shrinkage estimators of mean vector and the sample covariance matrix has singularity. To the best of our knowledge, there have been three categories of methods to overcome these problems. However, each of them has weakness under some situations, so we propose a new method which does not belong to the three categories.====The first one is based on using ==== diag====, where ==== is the sample covariance matrix, which ignores correlations among all variables. Based on this, there are different types of classifiers depending on estimators of mean vectors. For example, the “independent rule” (IR) and “Naive Bayes” (NB) rule use MLE of mean vectors,  [21] proposed the “nearest shrunken centroid” (NSC) method which removes non-informative components by setting a soft threshold. [5] suggested “features annealed independence rules” (FAIR) which selects feature component by hard threshold. Furthermore, “nonparametric empirical Bayes” (NPEB) estimation and “nonparametric maximum likelihood estimation” (NPMLE) are used to estimate mean vectors in [10] and [2], respectively. All these methods are not seriously affected by the dimension ==== from the benefit of ignoring correlations. However, the truncation-based methods (NSC, PLDA and FAIR) and the NPEB have merit under some specific structures such as a small number of nonzero components (sparse case) and many nonzero components (dense case) in mean vectors, respectively, while the NPMLE is more robust to both sparse and dense structures in mean vectors. Furthermore, all of them may have poor performance under some moderate or vigorous dependency among variables.====The second way is based on regularizing the sample covariance matrix to resolve the singularity problem, however, sample mean vectors are used without further modification such as the shrinkage idea on the estimation of mean vectors. In other words, this approach puts more emphasis on the estimation of covariance matrix than mean vectors. “Regularized data piling” (RDP) in [18] paid attention to obtaining the inverse of the ridge type estimator of the covariance matrix, however sample mean vectors are plugged into the Bayes rule (1). On the other hand, instead of inverting estimated covariance matrix, [16] directly estimates the precision matrix ==== as a regularized version of an estimator which is plugged into (1) with sample mean vectors. These methods with regularized covariance or precision matrix reflect dependence among variables. However, they have some limitations in high dimensional classification compared to the methods in the first category since sample mean vectors are used which are known as poor estimators of high dimensional mean vectors compared to shrinkage estimators.====The third way is to estimate ==== in the Bayes rule (1). Witten and Tibshirani [22] proposed ==== constraint estimator ====, called “penalized linear discriminant analysis” (PLDA) and  [11] used the ridge type estimator of ==== and applied the soft threshold to ==== where ==== is the centroid of each group ====. In addition, there are many analogous methods based on estimation of ====, see [1], [6] for more studies.====Each of these methods has some drawbacks, for example, the sensitivity to the structure of mean vectors or ignorance of dependence resulting in estimating too simplified covariance matrices. One primary motivation of our proposed method is that we want to reflect a more flexible structure of mean vectors and covariance matrix through the recent development of estimation procedures. Regarding estimation of mean vectors, estimators based on thresholding are designed for sparse mean vector, while the NPEB is efficient in estimating dense mean vectors. On the other hand, many studies have shown that mean estimation based on the NPMLE is more robust to the structure of mean vectors [7], [13], [14], [20]. However, the NPMLE is developed to estimate the mean vector with independent components that does not hold in our settings. Therefore, we first consider a covariance (precision) matrix estimation method to decorrelate the data. Rather than using the ridge type covariance matrix, which is a linear shrinkage estimator, we refer to a more recently developed idea, for example, nonparametric estimation of the precision matrix in [17]. As a result, we improve the LDA by combining the nonparametric estimation of the precision matrix and mean vector after decorrelation step. Throughout this paper, we call this classifier the nonparametric LDA (NPLDA). Our NPLDA handles a variety of structures of mean vectors and dependence among all variables which leads to significant improvement of existing linear classification rules.====This paper is organized as follows. In Section 2, we describe our proposed methods. We present theoretical results that support our methodology in Section 3. We provide numerical studies and real data examples in Sections 4 Simulation studies, 5 Data examples, respectively. Concluding remarks will be presented in Section 6. Finally, technical proofs are relegated to the Appendix.",High-dimensional linear discriminant analysis using nonparametric methods,https://www.sciencedirect.com/science/article/pii/S0047259X21001147,29 September 2021,2021,Research Article,93.0
"Zhang Ruoyang,Ghosh Malay","Department of Statistics, University of Florida, Gainesville, FL 32611, United States","Received 14 January 2020, Revised 14 September 2021, Accepted 15 September 2021, Available online 24 September 2021, Version of Record 5 October 2021.",https://doi.org/10.1016/j.jmva.2021.104835,Cited by (1)," and ==== are nearly optimal. Our results hold when number of features p grows much faster than the sample size n, which is of great interest in modern data analysis. We show that a class of readily implementable scale mixture of normal priors satisfies the conditions of the main theorem.","Parameter estimation, variable selection and prediction in high dimensional regression models have received significant attention in these days, particularly when the number of regressors ==== is much larger than the number of observations ====. Examples abound — brain imaging, microarray experiments, satellite data analysis, just to name a few. In many of these examples, one key issue is to address sparsity of effective regression parameters in the midst of a multitude of inactive ones. For example, there are only a few significant genes associated with Type I diabetes along with million others of no direct impact for such a disease.====In a frequentist framework, the most commonly used approach for inducing sparsity is by imposing regularization penalty on the parameters of interest. The most popular ones are ==== (lasso) penalty or a combination of ==== and ==== penalties (elastic net). The ==== and ==== regularization can naturally be extended to multivariate case where sparsity in the coefficient matrix is desired. Rothman et al. [28] used ==== penalties on each entry of the coefficient matrix as well as on each off-diagonal element of the covariance matrix. Wilms and Croux [35] considered a model that puts an ==== penalty on the rows of coefficient matrix to shrink the entire row to zero, and an ==== penalty on the off-diagonal elements of the inverse error covariance matrix. Li et al. [16] proposed a multivariate sparse group lasso imposing ==== penalty on the rows of the regression matrix and in addition an ==== penalty on individual coefficient of the regression matrix to perform sparse estimation and variable selection both at the between and within group levels.====In a Bayesian setting, spike-and-slab priors, originally introduced by Mitchell and Beauchamp [18] have become very popular for handling sparsity. Spike-and-slab priors are mixture densities with positive mass at zero to force some parameters to be zero, and a continuous density to model the nonzero coefficients. These priors have been used in a variety of contexts. For example, for Bayesian Group Lasso, Xu and Ghosh [36] used these priors for both variable selection and estimation. This work was extended by Liquet et al. [17] to the multivariate case. More recently, Ročková and George [27] introduced spike-and-slab lasso for variable selection and estimation. Deshpande et al. [12] extended it to multivariate case by putting spike-and-slab prior on each entry of the coefficient matrix as well as on each off-diagonal element of the precision matrix.====Spike-and-slab priors face severe computational challenges, when ====, the number of regressors, is very large. This is because that one needs to search over ==== possible models. An alternative to spike-and-slab priors are global–local shrinkage priors. These priors approximate the spike-and-slab priors well and are usually much easier to implement because they are continuous. And they also have broad applications, ranging from normal mean estimation [23] to subspace shrinkage [30]. In addition to the practical benefits, some global–local shrinkage priors, e.g., Dirichlet–Laplace priors [7], have been proved to have optimal posterior concentration in high-dimensional setting. Like spike-and-slab priors, global–local shrinkage priors also put significant probability around zero, but retain heavy enough tails so that the true signals are very unlikely to be missed.====For regression with multivariate responses, Bai and Ghosh [4] considered the case when the number of regressors can grow at a sub-exponential rate when compared to the sample size. They established posterior consistency of their prior and showed that the insignificant regression coefficients converge to zero at an exponential rate. Song and Liang [31] provided some general posterior contraction rates in the context of variable selection and estimation in univariate regression models with unknown variance.====Our paper is a follow-up of the works by Bai and Ghosh [4] and Song and Liang [31]. In particular, unlike the former, we do not need to assume a known covariance matrix in the original regression model to establish exponential convergence rate of tail probabilities. We propose a set of general conditions on continuous prior for achieving nearly-optimal posterior contraction rate for both coefficient matrix and covariance matrix. This extends the work of Song and Liang [31] to the multivariate case. Also, we have demonstrated that these regulatory conditions are satisfied by a general class of global–local shrinkage priors. Our technical results borrowed tools developed by Song and Liang [31], but handling multivariate data presented some new challenges in proving the results.====Ning and Ghosal [20] also addressed the issue of variable selection with unknown covariance matrix and established posterior consistency result similar to ours. But their results are based on spike-and-slab priors instead of global–local shrinkage priors and utilized different techniques from ours.====This paper is organized as follows. In Section 2, we establish general conditions on priors for achieving nearly-optimal posterior contraction rate for both coefficient matrix and covariance matrix. In Section 3, a class of global–local shrinkage prior that satisfies these general conditions is proposed. In Section 4, finite sample performance of the proposed model is evaluated through numerical experiments. Some final remarks are made in Section 5. Most of the technical theorems and lemmas are relegated to Appendix A.",Ultra high-dimensional multivariate posterior contraction rate under shrinkage priors,https://www.sciencedirect.com/science/article/pii/S0047259X21001135,24 September 2021,2021,Research Article,94.0
Kakizawa Yoshihide,"Faculty of Economics, Hokkaido University, Nishi 7, Kita 9, Kita-ku, Sapporo 060–0809, Japan","Received 1 February 2021, Revised 13 September 2021, Accepted 13 September 2021, Available online 22 September 2021, Version of Record 2 October 2021.",https://doi.org/10.1016/j.jmva.2021.104834,Cited by (3),"The Birnbaum–Saunders distribution has been generalized in various ways, for parametric or nonparametric statistical inference. In this paper, as a remedy for the boundary bias problem of nonparametric density estimation, a family of deformed multivariate elliptical-based non-central Birnbaum–Saunders ==== is introduced, and its asymptotic mean integrated squared error is discussed. The simulation results reveal that a novel log-elliptical density estimator has a good performance in small sample size.","Nonparametric density estimation is one of the fundamental topics (see, e.g., [40]). From this subject, we are mainly concerned with the estimation of the density ==== that has the support ====, where ==== and ==== is the dimension of the data. A starting point for estimating ====, ====, nonparametrically is to select a certain kernel ====, ====, such that, given ==== observations ==== with nonnegative components, where ====, the kernel-type estimator ==== is, at least, an asymptotically unbiased estimator of ====, ====, i.e., ==== for any ==== when the smoothing parameter ==== tends to ====. Several different types of such delta-sequences were studied by, e.g., [39]. Note that the commonly used standard kernel density estimator (KDE), introduced by [34], has, generally, ==== bias, hence, inconsistent, at or near the boundary of ====, because the location-scale symmetric kernel creates a mass outside ====. Some boundary correction methods, for the ====-variate case, are found in, e.g., [24], according to Jones [22]. Instead, we focus on a direct application of a kernel with support ====, as a remedy of avoiding the boundary bias problem.====Such an asymmetric kernel seemed to be natural and fairly simple. Unexpectedly, to the best of our knowledge, Silverman [38, page 28] first mentioned, in the univariate case, possible ideas of choosing gamma or log-normal (LN) kernel. The specific proposals of boundary-bias-free density estimators using beta or gamma kernel were discussed by [6], [7] for the univariate data supported on ==== or ====. Since then, this kind of density estimation has received renewed attention. Some univariate kernels with support ==== are gamma kernels [5], [7], [17], Birnbaum–Saunders (BS) and LN kernels [21], inverse Gaussian/reciprocal inverse Gaussian (IG/RIG) kernels [36], inverse gamma kernels [26], [29], generalized BS (GBS) kernels [33], [35], a generalized IG kernel [17], a weighted LN kernel [15], a family of generalized gamma kernels [18] (see also [14]), and a beta prime kernel [9]. Note that Igarashi and Kakizawa [17] treated the IG, RIG, and BS kernels in a unified way, to construct a mixture of IG and RIG kernels, which is referred to as a MIG kernel. The BS kernel was then viewed as an equally weighted mixture of the IG and RIG kernels. Kakizawa [23], [25] further considered not only a family of deformed symmetrical-based MIG kernels, but also a family of deformed skew BS-type kernels, including a subfamily of logarithmic-type kernels. The analysts can now choose what they like, among many options available for the univariate kernel with support ====.====For a multivariate data supported on ==== or ====, a product-type density estimation [4], [11], [13], [19], [42] is straightforward, using the univariate kernels as mentioned above. On the other hand, there is, however, little work on non-product-type, except for a bivariate beta-Sarmanov kernel [28] and a multivariate weighted LN kernel [16]; both kernels allow the correlation structure. As a continued work of [23], [24], [25], we aim at conducting a systematic study of non-product-type kernels with support ====, in the light of a recent progress of the BS-type distribution theory. For comprehensive reviews on the multivariate BS-type, we refer the readers to [1], [2].====The basic idea behind the construction of the BS-type KDE is illustrated for the univariate setting. Recall that the classical BS density [3], with two parameters ====, is given by ====where ==== and ====. For a review of this BS model, we refer the readers to the monograph [32]. Replacing ==== by ====, ====, where ==== is a nonnegative function on ==== and ==== is a constant given by ====, the GBS density ====was introduced by [8]. In this paper, we call it “symmetrical-based (central) BS” (rather than “GBS”) in order to make its meaning more clear, since the distribution having the density ====, ====, associated with a density generator ====, is symmetric about ==== (such a symmetrical distribution is denoted by ====). Furthermore, it is possible to replace an increasing function ==== by its ====-analogue (see, e.g., [20], [41]) ====or create a non-central BS [12]. This leads to a family of ====-deformed symmetrical-based non-central BS densities, with additional parameters ====; ====where ====. Without loss of generality, it suffices to consider ==== only, noting that ====. In order to estimate the univariate density with support ====, Kakizawa [23], [25] suggested, for the design point ====
 (====), the parameterization ====where ==== and ====. He constructed a family of ====-deformed symmetrical-based non-central BS KDEs, as follows: ====where ==== is the observed data supported on ====. It turns out that some asymptotics of the estimator ====, ====, is independent of the choice of ====
 (====), no matter what the feasible density generator ==== is. This is the reason why the mean integrated squared error of the (re-formulated) LN KDE (====) is asymptotically the same as that of the (re-formulated) BS KDE ====); see [15], [17].====It is worth noting that, except for the non-central extension (====), the varying-parameterization (2) was different from the parameterization ====, originally suggested by Jin and Kawczak [21]. In principle, there may be infinitely many varying-parameterizations; however, their construction [21], that was succeeded by [33], [35], should not be recommended from not only theoretical but also practical sides, because their kernel at ==== degenerates ==== almost everywhere, i.e., the resulting estimator yields ====, which is obviously not suitable unless ====. In other words, even if the support of the kernel under consideration matches the support of the density to be estimated, its “bad” parameterization causes the boundary problem. Some original papers cited above were still not satisfactory in this sense. Thus, some estimators were re-formulated to be boundary-bias-free, regardless of ==== or ====; see [15], [17], [23], [25], [26].====The contribution of this paper is that, extending these ideas to the multivariate case, we study in detail a family of ====-deformed multivariate elliptical-based non-central BS KDEs. Such a KDE is general, including the multivariate LN/BS KDEs [16], [24] as special cases. Note that any multivariate elliptical density, except for the multivariate normal density, is not product-type, since, for a subfamily of elliptical densities with diagonal scatter matrix, all components are mutually independent iff the normality holds (e.g., [10, page 106]). Although, in this paper, we focus on an application to nonparametric density estimation, we hope that a ====-deformed multivariate elliptical-based non-central BS distribution itself has an impact upon the BS distribution theory, together with related (parametric) inferential issues.====The rest of the paper is organized as follows. Section 2 introduces multivariate elliptical-based BS-type kernels. Section 3 provides asymptotic properties of the proposed nonparametric density estimator. Section 4 contains the simulation results for the bivariate case. A real data analysis is done in Section 5. Section 6 concludes this paper.====Throughout this paper, we use the symbols ====
 (==== vector of zeros), ====
 (==== vector of ones), and ====
 (==== identity matrix), and denote by ==== the Euclidean norm on ====. Also, for notational simplicity, instead of ====, we write ====, where ====.",Multivariate elliptical-based Birnbaum–Saunders kernel density estimation for nonnegative data,https://www.sciencedirect.com/science/article/pii/S0047259X21001123,22 September 2021,2021,Research Article,95.0
"Liu Bin,Zhang Xinsheng,Liu Yufeng","School of Management, Fudan University, Shanghai, 200433, China,Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, USA,Department of Genetics, Department of Biostatistics, University of North Carolina at Chapel Hill, USA,Carolina Center for Genome Sciences, Lineberger Comprehensive Cancer Center, University of North Carolina at Chapel Hill, USA","Received 16 August 2021, Revised 7 September 2021, Accepted 8 September 2021, Available online 22 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104833,Cited by (6),"Change point analysis aims to detect structural changes in a data sequence. It has always been an active research area since it was introduced in the 1950s. In modern statistical applications, however, high-throughput data with increasing dimensions are ubiquitous in fields ranging from economics, finance to genetics and engineering. For those problems, the earlier works are typically no longer applicable. As a result, the problem of testing a change point for high dimensional data sequences has been an important yet challenging task. In this paper, we first focus on models for at most one change point, and review recent state-of-art techniques for change point testing of high dimensional mean vectors and compare their theoretical properties. Based on that, we provide a survey of some extensions to general high dimensional parameters beyond mean vectors as well as strategies for testing multiple change points in high dimensions. Finally, we discuss some open problems for possible future research directions.","Change point analysis has a long history since the seminal work of [50], [51]. Since then, it has become an active research area in various scientific fields including finance, genetics, climatology, engineering, and astronomy. Generally speaking, suppose we have a sequence of ordered observations such as a time series. Change point analysis aims to answer the following two questions: [i] whether there is a change for the parameter of the underlying data distribution during the observations; [ii] If a change is detected, where is the position of the change point? The above two questions are referred to as the change point testing and estimation problems, which are two indispensable pillars in change point analysis. In this paper, we mainly focus on the former question. Classical methods for change point testing assume that the data dimension is fixed. In the last few decades, a rich literature has been developed in addressing different specific problems under various model settings. See the book in [11], [22] for a summary of the classical methods and a recent review paper in [32] for some extensions.====With the rapid development of data collection and storage capacity, high dimensional data are ubiquitous, where the data dimension can be tens of thousands and are typically much larger than the sample size. In this case, the data generating mechanism can be complicated and heterogeneity often exists. Hence, for high dimensional data analysis, heterogeneity detection or change point detection is an important issue. While there are many earlier works obtained with good theoretical results, in high dimensions, the classical methods are often no longer applicable. As a result, it is desirable to design new methods suitable for modern statistical applications. Driven by this demand, rapid developments have been made in the literature over the last 5–10 years for change point analysis. In this paper, focusing on change point testing, we first review recent developments on single change point detection of high dimensional mean vectors. We believe this can reflect the distinctive challenges for high dimensional change point analysis. In addition, it can serve as a foundation for developing new methodologies for some other complex change point problems. With the concept of high dimensional efficiency, we compare different methods in terms of size and power, and show their optimality in terms of the minimax optimality separation rate. The latter one is more technically involved in high dimensions and usually exhibits a phase transition according to the change point alternative patterns. Beyond mean vectors, in the second part of this paper, we review some recent extensions to other high dimensional parameters such as variance, covariance matrices, or non-parametric testing of distributional changes. Lastly, based on the existing literature, we point out several possible research directions for more complex model settings and problems.====Note that there are two related problems in high dimensional change point analysis. The first one is change point estimation [17], [49], [56]. For this problem, it is usually assumed there are ==== change points that exist in the model and the goal is to simultaneously estimate both the number and locations of the change points. This problem is also called data segmentation. Although change point testing and estimation are related, they are fundamentally different. For example, the former is concerned with proposing tests for controlling the type I and type II errors, while the latter one mainly focuses on developing algorithms for estimation consistency of numbers and locations. The second type of problem is called online or sequential change point detection [48], where data are observed sequentially. The goal is to detect a change point as soon as possible while controlling the false alarms. This is very different from our considered offline setting that we have observed all historical data at once. Although the above two problems are interesting and actively studied to date, it is not possible for us to review all related works in this paper. Hence, in this review, we only focus our attention on the problem of offline change point testing.====Throughout this paper, for ====, define its ====-norm as ==== for ====. For ====, define ====. For any set ====, denote its cardinality by ====. For two real numbered sequences ==== and ====, we set ==== if there exists a constant ==== such that ==== for a sufficiently large ====; ==== if ==== as ====; ==== if there exist constants ==== and ==== such that ==== for a sufficiently large ====. For a sequence of random variables (r.v.s) ====, we set ==== (or ====) if ==== converges to ==== in probability (or in distribution) as ====. We also denote ==== if ====. For a positive number ====, we use ==== to denote the largest integer less than or equal to ====.====The rest of this paper is organized as follows. Section 2 introduces the formulations of high dimensional change point inference and its distinctive challenges from the low dimensional problems. Sections 3–4 review recent methods for change point inference of high dimensional mean vectors. Section 5 discussed their theoretical properties. Sections 6 High dimensional change point inference for general parameters, 7 Extensions to multiple change points inference provide some extensions to high dimensional change point inference for general parameters as well as techniques for testing multiple change points. We conclude this paper in Section 8.",High dimensional change point inference: Recent developments and extensions,https://www.sciencedirect.com/science/article/pii/S0047259X21001111,22 September 2021,2021,Research Article,96.0
"Ouimet Frédéric,Tolosana-Delgado Raimon","California Institute of Technology, Pasadena, CA 91125, USA,McGill University, Montreal, QC H3A 2K6, Canada,Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resources Technology, 09599 Freiberg, Saxony, Germany","Received 17 March 2021, Revised 14 August 2021, Accepted 5 September 2021, Available online 17 September 2021, Version of Record 29 September 2021.",https://doi.org/10.1016/j.jmva.2021.104832,Cited by (9),"We study theoretically, for the first time, the ==== estimator introduced by Aitchison and Lauder (1985) for the estimation of multivariate densities supported on the ====-dimensional simplex. The simplex is an important case as it is the natural domain of compositional data and has been neglected in the literature on asymmetric kernels. The Dirichlet ==== for the mean squared error and the mean integrated squared error, we prove its ","Kernel smoothing or kernel density estimation is a well-known methodology to characterize (and visualize) the probability density function of a random variable or random vector in a nonparametric way. It can be considered as a bin-free alternative to histograms, and is particularly useful in multivariate cases with a low to moderate number of dimensions, where the accuracy of histograms dramatically deteriorates with the number of variables due to the curse of dimensionality. Apart from visualization purposes, density estimation can be used for nonparametric alternatives to regression and classification (both supervised and unsupervised). One of the most intuitive usages, for instance, is to construct conditional density plots (in ==== command “====”), to represent how the conditional probabilities of a categorical variable depends on quantitative covariables. This is true in particular for compositional data, but methods of density estimation on the simplex that address the well-known spill-over problem of traditional kernel estimators are very scarce in the literature, and theoretical results specific to the simplex are almost nonexistent. To remedy this situation, our main goal in this paper is to revisit the Dirichlet kernel estimator on the simplex introduced by Aitchison and Lauder [3] and study its asymptotic properties in details. A case study on minerals processing presented in Section 5 will show an explicit and elaborate use of conditional density plots using Dirichlet kernel estimators.====Nevertheless, our main contribution in this paper remains theoretical. We will find asymptotic expressions for the pointwise bias, the pointwise variance, the mean squared error (MSE) and the mean integrated squared error (MISE). These results generalize the ones for the Beta kernel in [34] (====). The optimal bandwidth parameters ====, with respect to MSE and MISE, are also written explicitly. In practice, this can be used to implement a plug-in selection method for the bandwidth parameter. The asymptotic normality follows from a straightforward verification of the Lindeberg condition for double arrays, although it is completely new even for Beta kernel estimators. We also obtain the asymptotics of the mean integrated absolute error (MIAE) and the uniform strong consistency, which generalize the results from Bouezmarni and Rolin [16]. To be more precise, the proof of the ==== asymptotics follows the same strategy but the proof of the uniform strong consistency is completely different and represents our biggest contribution (we combine estimates on the difference of Dirichlet densities with different parameters together with a novel chaining argument). Our rates of convergence for the MSE and MISE are optimal, as they coincide (assuming the identification ====) with the rates of convergence for the MSE and MISE of traditional multivariate kernel estimators, studied for example in [148]. In contrast to other methods of boundary bias reduction (such as the reflection method or boundary kernels (see, e.g., [156])), this property is built-in for Dirichlet kernel estimators, which makes them one of the easiest to use in the class of estimators that are asymptotically unbiased near (and on) the boundary. Dirichlet kernel estimators are also non-negative everywhere on their domain, which is definitely not the case of many estimators corrected for boundary bias. This is another reason for their desirability. Bandwidth selection methods and their consistency will be investigated thoroughly in upcoming work.====Here is the outline of the paper. In Section 2, we present an overview of the literature of asymmetric kernels. In Section 3, we define Dirichlet kernel estimators, we state some of their basic properties, and we show that Dirichlet kernels are continuous examples in the broader class of multivariate associated kernels introduced by Kokonendji and Somé [111], [112]. In Section 4, our main results are stated, which consists of the asymptotic behavior of the pointwise bias and variance (including points near the boundary of the simplex), the mean squared error, the integrated mean squared error, the mean integrated absolute error, the uniform consistency and the asymptotic normality. All the proofs are gathered in Section 6. In Section 5, the case study on minerals processing is presented.",Asymptotic properties of Dirichlet kernel density estimators,https://www.sciencedirect.com/science/article/pii/S0047259X2100110X,17 September 2021,2021,Research Article,97.0
"Castilla Elena,Zografos Konstantinos","Instituto de Matematica Interdisciplinar and Department of Statistics and O.R., Complutense University of Madrid, Spain,Department of Mathematics, University of Ioannina, Greece","Received 11 August 2021, Revised 1 September 2021, Accepted 1 September 2021, Available online 16 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104831,Cited by (2),"In this paper, we develop a new procedure for estimating the parameters of a model by combining Zhang’s (2019) recent Gaussian estimator and the minimum density power divergence estimators of Basu et al. (1998). The proposed estimator is called the Minimum Density Power Divergence Gaussian Estimator (MDPDGE). The consistency and ==== of the MDPDGE are proved. The MDPDGE is applied to some classical ==== and it is also investigated for the family of elliptically contoured distributions. A ==== illustrates the robustness of the proposed estimator.","The recent paper by Zhang [34] proposes a method to estimate the parameters of a model when the implementation of maximum likelihood estimation is difficult or impossible. The proposed estimators are characterized by nice properties, such as consistency and asymptotic normality. To formulate the problem, let ==== be independent and identically distributed replications of the ====-variate random vector ==== which is governed by the probability density or mass function ====, where ==== is a parameter vector, ==== is a measurable subset of ====, and ==== is the dimension of ====. In this setting, the maximum likelihood estimator (MLE) of the unknown parameter ==== is defined as ====where ==== is the log-likelihood of ====, which is defined on the basis of a set of observations ====, with ==== a realization of ====, ====. The MLE, ====, obeys nice properties, like consistency and asymptotic normality, subject to some mild conditions. In fact, it is well known that the MLE is a BAN (best asymptotically normal) estimator. Although the maximum likelihood estimation method is simple and it leads to efficient estimators of the unknown parameters in the large sample case, it cannot be applied so easily in practice when the initial model ====, which describes the data, is not so tractable or it contains intractable normalizing constants which include the unknown parameters. Moreover, maximum likelihood estimation is highly affected by the presence of outliers in the data. To overcome these shortcomings, several methods have been proposed in the literature. We mention the composite likelihood method and the distance-based estimation methods, among many others.====The main task in the approach proposed by Zhang [34] is to construct a likelihood function, like ==== in (1), which can be easily managed. In this direction, such an estimation function may be that of the multivariate normal distribution. This is the main idea in the paper by Zhang [34], which proposes that if the computation of ==== is difficult but the computation of ==== and ==== is not so arduous, then we can put them into the log-likelihood function of the multivariate normal distribution, which provides, in this way, an estimation function. This motivates Zhang’s Gaussian estimation approach.====To formulate this approach suppose, as above, that ==== are realizations of the independent and identically distributed replications ==== of the random vector ==== which is described by a probability density function or a probability mass function ====, where ====, ====. Moreover, suppose that the mean vector ==== and variance–covariance matrix ==== of ==== are known or they can be easily obtained in an explicit form. In this frame, Zhang [34] proposes that instead of using the true but intractable log-likelihood function ====, in the estimation approach described in (1), to use the Gaussian estimation function of ==== under ====, which is defined as ====In this setting, the Gaussian estimator of ==== is then defined by ====If ==== is an interior point of ====, then it also satisfies ====where for each ====, the ====th component of ==== is ====Based on the above exposition and on Definition 1 in Zhang ([34], p. 236) we formulate the following, quite similar definition:====At a first glance, the replacement of the initial and valid, even intractable, model ==== by the normal model, in Zhang’s [34] method, looks a little bit arbitrary. However, this apparent arbitrariness is removed when taking into account the maximum entropy principle, introduced by Jaynes [23] (cf. also, Zografos [35], [36] and the references appeared therein, among many others). Indeed, Zhang’s [34] method is developed on the basis of two assumptions: First, that the model which describes the existing data is difficult to be managed and second, that the mean and variance of the model are available in an explicit form. On the other hand, it is well known, by means of the maximum entropy principle, that the normal model is the most suitable model to describe and fit the available data, among all the other candidate distributions with given mean and variance (cf., for example, Kagan et al. [25], Theorem 13.2.2, p. 410 or Cover and Thomas, [14], p. 413). Hence, the use by Zhang [34] of the normal model and the subsequent Gaussian estimation function (2) is not so arbitrary as it seems, and it can be considered as a direct consequence and a nice application of the maximum entropy principle to the area of maximum likelihood point estimation.====On the other hand, minimum distance estimation methods occupy a significant part of the existing literature in estimation theory as they are characterized by good efficiency and robustness properties. These methods play an important and decisive role in the area of multivariate analysis because parameter estimation procedures are directly connected with classic topics like plug-in classification rules in discriminant analysis. Minimum distance estimation techniques are based on a distance measure between the empirical model and the unknown, in practice, parametric model that ideally describes the available data. In this setting, minimum distance estimators are the result of a minimization, with respect to the unknown parameters, of the distance measure between the empirical and the theoretical model that is adopted to fit the existing data. Minimum density power divergence (DPD) estimation method, introduced in the pioneer paper by Basu et al. [5] and exhaustively studied in the monograph by Basu et al. [7], occupies a prominent position among the existing minimum distance estimation methods due to its balance between efficiency and robustness. Basu et al. [6] presented a class of generalized Wald-type test statistics based on DPD in case of independently and identically distributed observations, while Basu et al. [4] extended these tests to independently but non identically distributed data. To date, several studies have confirmed the effectiveness of the use of DPD and DPD-based Wald-type tests in different areas, such as regression models (Ghosh and Basu, [17]; Castilla et al. [9], [10]), survival analysis (Ghosh et al. [18]) or even meteorology (Hazra and Ghosh, [22]). In recent years, DPD has been also applied in the prominent area of high-dimensional data; see Ghosh and Majumdar [20] or Ghosh et al. [19].====This paper provides elaborated new estimators which are developed on the common ground of Zhang’s [34] recent general Gaussian estimation method and the minimum DPD estimation method of Basu et al. [5]. In particular, in Section 2 both approaches are combined to introduce the minimum DPD Gaussian estimation (MDPDGE). The consistency and asymptotic normality of the so called estimators are studied in the subsequent Section 3. In Section 4, the behaviour of MDPDGE, in the univariate case, is illustrated with some elementary examples. Section 5 concentrates on the elliptic family of multivariate distributions. It initially reviews estimation of the parameters of the model by the classic maximum likelihood method. In the sequel, Zhang’s [34] type Gaussian estimators of the parameters of the elliptic family are obtained in an explicit form and they are applied to particular members of the elliptic family along with the respective MDPDGE. Finally, in Section 6 some numerical studies are developed to illustrate the behaviour of the proposed estimators in different models and to investigate the robustness of the MDPDGE. Some concluding remarks and possible future research lines are discussed in Section 7. The paper is integrated with some technical details which are presented in the final section.",On distance-type Gaussian estimation,https://www.sciencedirect.com/science/article/pii/S0047259X21001093,16 September 2021,2021,Research Article,98.0
Farebrother Richard W.,"11 Castle Road, Bayston Hill, Shrewsbury, United Kingdom","Received 29 June 2021, Revised 16 August 2021, Accepted 16 August 2021, Available online 11 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104814,Cited by (2),"Principal Components Analysis was developed by Harold Hotelling (1895–1973) in 1933 and ==== in 1936.  In this article we trace some of the stages leading up to the development of these procedures, chiefly in the hands of Francis Galton (1822–1911) and Karl Pearson (1857–1936) paying particular attention to the two-variable case developed independently by Julius Ludwig Weisbach (1806–1871) in 1840 and Robert Jackson Adcock (1826–1895) in 1877-78.","As will be well-known to readers of this journal, Principal Components Analysis was developed by Harold Hotelling (1895–1973) [28] in 1933 and Canonical Correlations Analysis by the same author [29] in 1936. In this article we trace some of the stages leading up to the development of these procedures, chiefly in the hands of Francis Galton (1822–1911) and Karl Pearson (1857–1936). In Section 2 we introduce the ‘Lunar Society of Birmingham’ associated with members of the Darwin and Galton families and two related family groups; in Section 3 we outline the contents of Galton’s ====; in Section 4 we discuss his association with the subject area of eugenics; in Section 5 we describe Galton’s discovery of the bivariate normal distribution and in Section 6 his discovery of the concept of correlation; in Section 7 we comment briefly on Galton’s use of the median and the probable error rather than the arithmetic mean and the standard error; in Section 8 we outline his study of anthropometry; in Section 9 we outline Pearson’s derivation of the orthogonal least squares procedure and in Section 10 we specialise Pearson’s result to the two-variable case; in Section 11 we outline the contributions of Adcock and Weisbach and in Section 12 we explain how R. J. Adcock was correctly identified in 2019; in Section 13 we note some well-known applications of the method of orthogonal least squares; and, finally, in Section 14 we introduce a parametrisation of an orthonormal matrix which offers the possibility of directly deriving the eigenvectors of a symmetric matrix without needing to determine its eigenvalues.",Notes on the prehistory of principal components analysis,https://www.sciencedirect.com/science/article/pii/S0047259X21000920,11 September 2021,2021,Research Article,99.0
"Klimova Anna,Rudas Tamás","National Center for Tumor Diseases (NCT), Partner Site Dresden, Dresden, Germany,Department of Statistics, Eötvös Loránd University, Budapest, Hungary,Institute for Medical Informatics and Biometry, Technical University, Dresden, Germany","Received 5 February 2020, Revised 19 July 2021, Accepted 19 July 2021, Available online 9 September 2021, Version of Record 29 September 2021.",https://doi.org/10.1016/j.jmva.2021.104808,Cited by (0),Multivariate sample spaces may be incomplete ==== and ,"In multivariate statistical analysis the joint sample space of the variables is usually the Cartesian product of the ranges of the individual variables, which may be Euclidean spaces or discrete (finite or countable) sets. There are, however, situations where the joint sample space has a different structure — most importantly in the case of the analysis of register data. Registers contain records which list the features that characterize particular events. For example, in a registry of congenital malformations, an event is when a child under a specific age threshold is diagnosed with a congenital malformation and the features are the abnormalities observed. Or, in a register of road traffic violations, the records describe the rules violated by the driver. The registers contain no records where none of the features is present. If there are ==== features, the sample space is not the Cartesian product CP = ====, rather the incomplete product IP = ====.====While the combination of no feature present is not part of a register, such cases may exist and even their number may be known. For example, there are newborns without congenital abnormalities, their number is known, and the register data may be completed so that the sample space will be the full Cartesian product CP. On the other hand, an instance of driving or parking without a traffic rule being violated hardly makes any sense, and even if some definition was agreed on, the number of those instances could not be determined. Thus, the sample space is inherently incomplete in the case of a traffic violation register.====When the sample space is inherently incomplete, statistical modeling has to reflect upon this fact, in particular that in so-called big data analysis, register data are often used (see, e.g., [34]), and the specific properties implied by having observed data in an IP are usually overlooked. In the discrete case, the often applied multiplicative (or log-linear) models would associate a positive probability with the non-existing category combination, and, therefore, are not appropriate. This paper introduces multiplicative models on the IP and discusses many of their properties.====When there are two features, the sample space has the structure shown in Table 1. Obviously, the model of independence defined by the assumption that the odds ratio (OR) is equal to 1 does not apply.====The sample space depicted in Table 1 also occurs in experimental design, when the combination of no features (no treatments) is not observed. For example, [18] observed traps filled with sugarcane of fish or both as bait and recorded the number of swimming crabs entering the traps. To test the hypothesis of the independence of the effects of the two types of baits, one would need the number of those swimming crabs entering a trap with no bait. Without having included this experimental condition, one is left wondering whether or not crabs could be caught in a trap with no bait. If the answer is yes, then the model of independence of the effects of the two bait types is relevant, but cannot be tested based on the available data. If the answer is no, the model of independence cannot be applied, as it would associate a positive probability with the experimental condition of no bait. On the other hand, the models developed in this paper do apply in such situations, as well.====Section 2 discusses whether any of the properties of independence which is the simplest multiplicative model on the CP may be retained for the case of the IP, and gives a negative answer. Further, the concept of the Aitchison–Silvey ratio ====is introduced. The models to be developed later in the paper are defined by restrictions on ASRs and their generalizations.==== Section 3 defines conditional and higher order ASRs, parallel to the usual definitions for ORs and also shows that they are the canonical parameters in an exponential family representation of all distributions on the IP. Section 4 considers mixed parameterizations of this exponential family on the IP using lower order marginal distributions and higher order conditional ASRs. Also in this setup, a variation independence property, up to a constant multiplier, holds between the two components.====When there are three binary features, ====, ====, ====, the IP contains 7 cells and its structure is shown in Table 2. If conditioned on ====, there is a conditional odds ratio ==== measuring the strength of association between ==== and ====, when ==== is present. But as there is no ==== cell, when conditioned on ====, no COR exists. The existing modeling approaches in such situations include using context specific models, see [17], [29]. Such a model would assume the independence of ==== and ==== in the context when ====, but would make no restriction when ====.====Another approach to analyzing IPs is applying quasi models [1], [15]. Such a model is usually defined by allowing exclusive multiplicative parameters in the cells where the model of interest is not assumed to hold, i.e., when ====. This implies for log-linear models that the MLE reproduces those cell frequencies, and model misfit, if any, concentrates on the cells which are being modeled, i.e., when ====. However, allowing exclusive parameters in the cells not of interest, does not imply that the MLE reproduces their observed frequencies in the more general model class of relational models, if the exponential family does not contain the overall effect [22], [24], [26].====Section 5 extends the definition of quasi models, as the collection of conditional distributions on the IP, when the model holds on the CP. This, however, does not affect the fact that a quasi model leaves nearly half of the sample space unmodeled and nearly half of the data do not enter the analysis based on the quasi model. The proposed hierarchical class of AS (HAS) models restricts, in addition to the CORs, also the conditional ASRs, similarly to a context-specific model. For example, in the case of the sample space in Table 2, a conditional independence HAS model would assume that ==== and also that conditional Aitchison–Silvey ratio, ====is equal to ====. These properties may imply that the second order AS ratio (see Section 3), ====is also equal to ====.====The development of HAS models in Section 5 is parallel to the usual treatment of hierarchical log-linear (HLL) models, (see, e.g., [32]) and comparisons are made to quasi log-linear (QLL) models. It is shown, among others, that QLL models are obtained from HAS models by allowing an overall effect and vice versa. [24] discussed in detail the effect of adding or removing the overall effect to/from an exponential family and some of the results presented in that paper are generalized here. While the treatment is parallel to that of HLL models, there are many noticeable differences, as HAS models are shown to be relational models without the overall effect. The general relational model class was defined (see [26]) for arbitrary discrete sample spaces, which may even not have a product structure, at all, and the models restrict the values of arbitrary generalized odds ratios, the special subclass of HAS models applies to sample spaces with the IP structure, and the generalized odds ratios restricted are a hierarchical system of CASRs, making HAS models parallel to but fundamentally different from HLLs. Being a relational model has wide ranging consequences for the model properties and maximum likelihood estimation, discussed later in the paper. The HAS variants of the conditional independence and no-highest-order interaction models are defined and studied in detail.====Section 6 looks at the relationship between HLL, QLL and HAS models using the approach of algebraic geometry, to complete the result of Section 5. Statistical models are characterized not only as varieties but also as their vanishing ideals, as the collection of all properties which may be used to characterize the models. It turns out that the relationship between the model classes discussed in the paper may be described by well established transformations studied in algebraic geometry. The main results obtained here are that the QLL is the elimination ideal of the HLL on the IP, and the HLL on the CP is the extension ideal of the QLL, HAS on the IP is obtained from the HLL through dehomogenization, while the transformation in the opposite direction is the so-called projective closure. This creates a direct link between HLL and HAS models, while in statistical terms, the link only existed though the QLL models.====The most important consequence of HAS models being relational models without the overall effect is that MLEs reproduce the marginals of the interactions allowed in the model only up to a constant multiplier. Algorithmic issues are discussed in Section 7. An existing ==== function [21] may also be used to obtain MLEs under HAS models.",Hierarchical Aitchison–Silvey models for incomplete binary sample spaces,https://www.sciencedirect.com/science/article/pii/S0047259X21000865,9 September 2021,2021,Research Article,100.0
"Yang Jun,He Ping,Fang Kai-Tai","Division of Science and Technology, BNU-HKBU United International College, Zhuhai, China,The Key Lab of Random Complex Structures and Data Analysis, The Chinese Academy of Sciences, Beijing, China","Received 13 August 2021, Revised 28 August 2021, Accepted 28 August 2021, Available online 9 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104829,Cited by (1),Discrete approximations of statistical continuous distributions have been widely requested in various fields. Using random samples generated by Monte Carlo (MC) method to infer the population has been dominant in ,"Discrete approximations of statistical continuous distribution have been widely used in various situations. For example, we often concern various characteristics of the continuous distribution being studied, however, sometimes integration techniques may fail to provide analytic solutions due to the complicated distribution function, especially for multivariate continuous distributions. Let ==== be a ====-variate random vector following a continuous distribution ====. Monte Carlo (MC) method provides ways to generate a random sample ==== from the population, and its empirical distribution is given by ====where ==== is the indicator function of event ====. Under some regular assumption, the weak law of large numbers implies that ==== converges to ==== in probability as ====. Hence, ==== can be considered as an estimate of the population distribution ====, and the characteristics of ==== can be estimated by the random sample. The MC method has a lot of advantages in estimation and performing statistical tests. However, it sometimes has poor performance. For example, the convergence rate for a numerical integration by the MC method is ==== independent of dimension ====, which is slow. Therefore, some alternative approaches including number-theoretic methods or Quasi-Monte Carlo (QMC) methods [32] are proposed. The convergence rate for a numerical integration by the QMC methods can be reached to ====, which is faster than that of the MC method if ==== is not large, and a set of points of size ==== generated by the QMC methods is called quasi-random ====-numbers (or QMC ====-numbers) with respect to ==== (see Definition 2 of the paper). Another way to generate a set of points which can represent ==== well is the Mean Square Error (MSE) methods, which provides a set of MSE representative points of ==== by minimizing the MSE (see Definition 3 of the paper).====The bootstrap method proposed by [8] is one of the resampling techniques. The idea of bootstrap is sampling from the empirical distribution based on a random sample, which can be regarded as an approximation of the distribution ====. Hence, a natural question is whether sampling from a better discrete approximation of ==== can provide more accurate estimations? Fang et al. [14] first employed resampling techniques to the approximations of the univariate normal distribution constructed by Quasi-Monte Carlo methods (QMC) and MSE representative points, and the results indicated a significant improvement in estimation of the mean, variance, skewness and kurtosis. In this paper, one of the purposes is to compare the performance of resampling techniques based on discrete approximations in statistical inference for the elliptically contoured distribution and the skew-normal distribution, where the discrete approximations are constructed by MC, QMC and MSE methods, respectively.====Discrete approximation of a probability distribution typically is determined by a set of representative points and associated probabilities. Suppose a discrete approximation of ==== with ==== points is denoted as ====. There are various ways to measure the closeness between ==== and ====, such as consistency, ====-distance and the mean square error (MSE) which will be introduced in (18), etc. Here, ====-distance between ==== and ==== is defined as ====where ====. The ====-distance also known as ====-discrepancy, ====-distance and ====-distance are commonly used to measure the closeness between ==== and ====. Therefore, finding an optimal discrete approximation can be considered as a problem of selecting a given number of points that retain as much information of the population as possible under some appropriate criterion, and such a set of points are also called representative points (RPs). In other words, the set of RPs of ==== is chosen by minimizing the bias between ==== and ====. In the following discussion, we mainly focus on the RPs obtained by minimizing ====-discrepancy and MSE.====Number-theoretic methods or Quasi-Monte Carlo (QMC) methods [32] are proposed as the alternative approaches to the MC method. A set of points generated by the QMC methods can be regarded as RPs of the continuous distribution ==== (QMC-RPs for short) which has greater uniformity than a set of random points generated by the MC method (MC-RPs for short). It is known that the QMC-RPs are constructed under the criterion ====-discrepancy which is often used to measure the closeness of the set of points, so QMC-RPs have better performance than MC-RPs. Using the QMC methods, the discrete approximation of the continuous distribution ==== is constructed by a set of ==== QMC-RPs as the support points and the corresponding probability at each point ====. The QMC methods have been widely applied in various areas of statistics [12], [13], [33], including tests for multi-normality, multivariate goodness-of-fit, Markov chain Monte Carlo (MCMC), permutation tests, and robust regression model fitting, etc., and they are also utilized to solve financial problems [18], [22], [36], [48] and partial differential equations [20], [21].====Another important criterion for representativeness is MSE. Cox [5] discussed the problem of classifying the observations of a univariate continuous distribution into ==== groups and assigning the individuals in the ====th group with an associated value ====, where the MSE is proposed to measure the “representativeness” of the selected points to the continuous distribution. In engineering, the problem of finding RPs is described as a problem of optimal quantization. The optimal quantization is originally applied to optimize signal transmission by appropriate discretization procedures [34]. Max [31] discussed the problem of minimizing the distortion of a signal by a quantizer with a fixed number of output levels. The distortion is defined as the expected value of squared error between the input and output of the quantizer, so the quantizer with ==== output levels having a minimized distortion of a signal is exactly the same as a set of RPs that minimizing the MSE (MSE-RPs for short). For more applications of MSE-RPs (or the optimal vector quantization) in the field of signal and image processing and information theory, one can see in [17], [19]. In spite of this, Fang and He [10] proposed a computational procedure for selecting RPs with minimized MSE to develop clothing standards. Flury [15], [16] studied the MSE-RPs of elliptically contoured distribution with the motivation of determining optimal sizes and shapes of protection masks for the Swiss Army. Flury and his co-authors used the term “principal points” for MSE-RPs due to the relationship between MSE-RPs and principal components (see [15], [42]). In addition, the MSE-RPs can be utilized in problems for numerical computation of conditional expectations, stochastic differential equations and stochastic partial differential equations, which are mostly motivated by problems arising in finance (e.g., option pricing) [4], [9], [34], [35], and they are also applied for classification to distinguish drug responders from placebo responders in psychiatric studies [44], [45]. Fang et al. [14] suggested to apply the MSE-RPs in statistical inference by resampling technique. Recently, Lemaire et al. [27] proposed a new variance reduction technique for Monte Carlo estimators with one dimensional MSE-RPs. In the following discussion, the discrete approximation distributions of ==== constructed with MC-RPs, QMC-RPs and MSE-RPs are denoted as ====, ====, and ====, respectively.====Although the three types of discrete approximations, ====, ==== and ==== have been widely applied to various fields, to our knowledge, there are few studies comparing their performance in statistical inference, especially for multivariate distributions. Therefore, it is meaningful to compare the performance of these discrete approximation methods in statistical inference for multivariate distributions, and the research results can provide guidance for the choice of discrete methods in practical problems. In addition, Fang et al. [14] proposed to resample from these three kinds of discrete approximations of univariate normal distribution using the idea of bootstrap, and results showed that ==== has superior performance in the estimation accuracy among the three kinds of approximations. In this paper, we will focus on the comparison of the estimation accuracy of these three discrete approximations for some multivariate distributions, including the elliptically contoured distributions and the skew-normal distribution.====Algorithms which can generate the QMC-RPs and MSE-RPs accurately and effectively are very important in this study. Fang and Wang [12] provided an efficient algorithm to compute the QMC-RPs of spherical distributions. For generating a set of MSE-RPs, ====-means algorithm is important and widely used. For a random sample of ==== and a given ====, Pollard [37] showed that the set of optimal sample cluster means converge to the optimal population cluster means almost surely under some regularity conditions when the sample size goes to infinity. Hence, the cluster means of ====-means algorithm are self-consistent points for the empirical distribution of ==== [40], which gives the nonparametric estimators of MSE-RPs of the underlying distribution based on the self-consistency of MSE-RPs [41]. However, the training set generated from the underlying distribution ==== and the set of initial points selected are also crucial for the ====-means algorithm. From this point of view, the estimation of MSE-RPs can be improved with a better training set and initial points. Fang and Wang [12] provided an effective algorithm by means of the QMC methods, which is the NTLBG algorithm, and they showed the details for manipulation of the method. In this paper, besides the NTLBG algorithm, we also make an attempt to improve the accuracy of MSE-RPs by employing the ====-means++ seeding method for selecting initial points [2] instead of the QMC methods, as it can be more competitive than the QMC methods in some cases with a QMC training set.====Another topic of interest is that MSE-RPs relate to principal components. Several studies have attempted to determine the geometric pattern of MSE-RPs for the elliptically contoured distributions (see [15], [16], [24], [39]). The simplest pattern formed by MSE-RPs of the elliptically contoured distributions is a line, and this line is spanned by the first principal component based on the principal subspace theorem. Tarpey [39] provided a lower bound ==== of the value ==== in a covariance matrix of the form ==== such that the MSE-RPs of a bivariate normal distribution with this covariance matrix form a line if ====, where ==== is related to the number of MSE-RPs ====. Inspired by this numerical result, we consider the covariance matrix with another special structure ====. For a bivariate normal distribution with ==== there also exists a lower bound ==== for ==== such that all the MSE-RPs locate on the direction of the first principal component if ====. For the other subclasses of the elliptically contoured distribution of dimension ==== such as the Kotz type distribution, we have the similar findings with the bivariate normal distribution, and the numerical results for the Kotz type distribution are also presented. Due to the relationship between MSE-RPs of the elliptically contoured distribution and principal components, the contribution of the first principal component ==== will also be discussed. For given ====, we observe that the covariance matrices ==== and ==== have the same value of ====, and the ==== MSE-RPs of the elliptically contoured distribution with these two covariance matrices are located in the direction of the first principal component. This may provide a potential application of MSE-RPs for principal component analysis, and the discussion is in Section 5. In addition, we will also discuss the relationship between the magnitude of MSE and contribution of the first principal component.====The rest of this paper is organized as follows. In Section 2, the elliptically contoured distributions and the algorithms for generating different types of RPs will be reviewed. The applications of discrete approximations ==== and ==== in two geometric probability problems will be considered in Section 3. Estimation of the mean vector and covariance matrix for the elliptically contoured distribution by resampling from ====, ==== and ==== will be investigated and numerical results will be shown in Section 4. We will discuss the relationship between the MSE-RPs and principal component analysis in Section 5. Finally, the conclusions and further study will be considered in Section 6.",Three kinds of discrete approximations of statistical multivariate distributions and their applications,https://www.sciencedirect.com/science/article/pii/S0047259X2100107X,9 September 2021,2021,Research Article,101.0
"Hu Jianhua,Liu Xiaoqian,Liu Xu,Xia Ningning","School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai 200433, China,School of Economics and Finance, Shanghai International Studies University, Shanghai 200083, China","Received 14 August 2021, Accepted 15 August 2021, Available online 6 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104821,Cited by (0),Multivariate linear regression analysis is an important technique for modeling the predictive relationships of multiple related response variables on a set of common ,"A classical multivariate linear regression model is the model which we observe ====where ==== is the ==== observation matrix of ==== response variables taken on ==== individuals, ==== is the ==== predictor matrix for ==== predictor variables, and ==== is the unknown ==== regression coefficient matrix. Usually it is assumed that observations on individuals are independent, so that the rows ==== of the random error matrix ==== are independent and identically distributed (i.i.d.), following a multivariate distribution with mean vector ==== and a common covariance matrix ====. The common covariance matrix ==== is usually assumed to be positive definite.====In the classical multivariate regression analysis, all response variables in the model (1) are included in the analysis, see, e.g., [1], [3], [14], [28]. However, there are many applications where some response variables are almost invariant with respect to changes of the predictor variables and have very small regression coefficients, for example, in deciphering the associations between gene expression and copy number alteration [27], [29], [34], [36], in micro-array bi-clustering analysis [8], [24], among others. This implies that only a few of all response variables are necessary for the problems. Consequently, response variables need to be selected. Deleting unnecessary response variables can lead to more interpretable results and also improve estimation efficiency. This topic has attracted the attention of many researchers, see, e.g., Chen et al. [9], Hu et al. [19], Liu et al. [25], Shi et al. [32] and Su et al. [35].====There is a close connection between response variable selection and multiple testing problems. Finner and Giani [16] established a general duality between multiple testing and selection. Selection problems are considered to introduce selection indicators which can be used to define a related multiple hypotheses testing problem. Finner and Strassburger [17] investigated the construction of multiple test procedures and selection procedures based on a partition of the parameter space or the called partitioning principle. In particular, the multiple testing procedures with the linear step-up structure [5], [18] can be used to select response variables within a multivariate linear regression frame.====In this article, we try to give a selective review of response variable selection and estimation under a multivariate linear regression setting concerning methodological developments, theoretical properties and computational algorithms, and also highlight two issues for future research.====The outline of this article is as follows. Section 2 introduces response variable selection methods including theoretical properties and computational algorithms. Section 3 addresses the finite sample performance of selective methods in the sense of the recall rate or true positive rate, true negative rate, model size, precision rate, F-measure and their standard deviations. Section 4 discusses some issues that require further investigations.",Some aspects of response variable selection and estimation in multivariate linear regression,https://www.sciencedirect.com/science/article/pii/S0047259X21000993,6 September 2021,2021,Research Article,102.0
Guinness Joseph,"Cornell University, Ithaca, NY, USA","Received 10 April 2020, Revised 12 August 2021, Accepted 12 August 2021, Available online 6 September 2021, Version of Record 28 September 2021.",https://doi.org/10.1016/j.jmva.2021.104823,Cited by (3)," into a linear model of coregionalization plus a residual process. The methods are applied to two storm datasets, one of which is from Hurricane Florence, which struck the southeastern United States in September 2018. The application demonstrates how fitted models from different datasets can be compared, and how the methods are computationally feasible on datasets with more than 200,000 total observations.","There are several exciting scientific campaigns to produce and observe multivariate data that vary over space and time. For example, large climate centers such as the National Center for Atmospheric Research (NCAR) produce high resolution simulations of the Earth system. These models include dozens of variables evolving in concert over space and time. The National Aeronautics and Space Administration (NASA) and the National Oceanic and Atmospheric Administration (NOAA) have deployed numerous satellites that collect observations of the Earth surface and atmosphere. NASA and NOAA recently launched a pair of satellites in its ongoing geostationary operational environmental satellite (GOES) program, the GOES-16 and GOES-17 spacecraft, that sit in geostationary orbit, continually monitoring light reflected by the Earth surface and atmosphere in 16 separate wavelength bands. In turn, the raw data are processed to produce dozens of physically relevant variables, such as atmospheric water vapor content and land surface temperature.====A statistical framework for analyzing these data should, at a minimum, include (1) sufficiently flexible statistical models capable of capturing complex multivariate dependencies, and (2) computationally efficient tools for estimating the models from data. [10] provide a thorough review of existing modeling and estimation frameworks. [18] conducted a theoretical analysis of a number of multivariate spatial models from the literature and concluded that many of them impose oversimplistic restrictions on the coherence between pairs of variables. For example, separable and kernel convolution multivariate spatial models [22] have constant coherence. In the linear model of coregionalization (LMC) [3], when component processes have spectral densities that decay with different rates, the coherence always converges to a non-zero constant as the frequency increases.====It is desirable to have models that do not impose such restrictions, for example, to have a model class that allows the coherence to decay to zero as frequency increases. [18] showed that the multivariate Matérn models [2], [12] do possess such flexibility. However, likelihood-based estimation of parameters in multivariate Matérn models is not computationally feasible for the massive datasets mentioned in our opening paragraph. Computational issues come from two sources. The first is that the data size prohibits formation and factoring of the covariance matrix, necessary operations for evaluating the likelihood function. There exist promising approximations that could in principle apply to the multivariate case, such as Vecchia’s likelihood approximation [8], [13], [17], [26] and hierarchical matrix approximations [1], [21], [25], but these approximations are untested for multivariate models, and it is not yet clear how to optimally implement them, and how their performance will compare to univariate cases. Such work would be a welcome development to the literature. However, a second and perhaps more serious computational problem, even for fast approximate methods, is that multivariate spatial models contain many more parameters, usually on the order of the square of the number of components. The large number of parameters poses a serious issue for optimization of the (approximate) likelihood function, which typically requires many iterations until convergence over a large parameter space. We demonstrate this problem with a simple example in Section 4.====Nonparametric spectral methods offer the potential of addressing both the modeling and computational demands for multivariate spatial and spatial–temporal data. In this framework, a discrete Fourier transform (DFT) is applied to each multivariate component, and then the periodogram vectors are smoothed over frequency to generate nonparametric estimates of the cross spectral density matrices. This approach is reasonably flexible from a modeling standpoint, since the only assumption is that the cross spectral density matrices vary smoothly with frequency. For example, the class of stationary spatial–temporal models includes non-separable models and asymmetric spatial–temporal models [11]. The approach is also computationally inexpensive, since we can evaluate the DFTs quickly with FFT algorithms. However, nonparametric spectral methods have limited applicability since they typically apply only to complete, gridded data on rectangular domains. Moreover, even if we have such data, edge effects can introduce severe biases in the spatial and especially the spatial–temporal cases [16]. Tapering [7] and differencing [20] have the potential to reduce edge effects, but differencing is not possible when there are missing values, and [14] showed that tapering can be ineffective under certain missingness scenarios and suboptimal compared to imputation-based methods.====This paper provides methods that addresses edge effects and extend the applicability of nonparametric spectral methods to incomplete gridded multivariate spatial–temporal data. This is achieved by leveraging the simple, yet powerful technique of periodic imputation introduced for the univariate case in [14]. We also introduce a method for decomposing the estimated cross spectral density function into an LMC cross spectral density function plus a residual cross spectral density function, which can be used as tool for exploring the spatial–temporal variation in the data. We apply the methods to compare the multivariate spatial–temporal covariances of data from an ordinary storm and from Hurricane Florence, demonstrating that these methods can computationally feasibly estimate multivariate spatial–temporal models with more than 200,000 observations. We also show how the estimates can be interpreted to distinguish among models.",Nonparametric spectral methods for multivariate spatial and spatial–temporal data,https://www.sciencedirect.com/science/article/pii/S0047259X21001019,6 September 2021,2021,Research Article,103.0
"Kang Kai,Song Xinyuan","Department of Statistics, The Chinese University of Hong Kong, Hong Kong","Received 25 February 2021, Revised 26 August 2021, Accepted 27 August 2021, Available online 5 September 2021, Version of Record 28 September 2021.",https://doi.org/10.1016/j.jmva.2021.104827,Cited by (1), of parameter estimators. The method is evaluated through simulation studies and applied to a dataset about Alzheimer’s disease.,"The investigation of the relationship between a time-to-event outcome and the features of multiple longitudinal profiles is often of scientific interest in clinical trials. A popular approach for this type of problem is to simultaneously model longitudinal and survival processes to borrow strength from each component in the model-building procedure. Literature on the joint modeling approach for (multivariate) longitudinal and survival data is rich [7], [10], [19], [20], [23], [24], [26]. Despite the rapid development of the joint model for longitudinal and survival data, nearly all of them assume a ‘one-to-one’ relationship, that is, one observed longitudinal measurement corresponds to one actual time-dependent covariate. For instance, [7] and [20] modeled the actual CD4 counts using observed CD4 measurements through a measurement error model. [15] characterized the underlying changes of body mass index and systolic blood pressure using their corresponding longitudinal measurements, respectively.====In certain circumstances, a time-dependent risk factor cannot be summarized by a single longitudinal observation with measurement error and it instead should be characterized by multiple observed longitudinal variables from different angles. Such a ‘many-to-one’ relationship is common in medical, behavioral, and social sciences, wherein several relevant observed variables are combined to evaluate one latent trait that truly exists but cannot be directly observed. Specifically, in the ADNI study, learning score, short delay recall (SDR), long delay recall (LDR), and recognition are used together to measure subject’s memory; and Trail Making Test Part A (TMTA) and Trail Making Test Part B (TMTB) are combined to evaluate subject’s executive function [18]. Although these observed variables are highly correlated and reflect the same neuropsychological trait, their specific characteristics vary and do not replace each other. For example, TMTA is a test of psychomotor processing speed and visual scanning, whereas TMTB further assesses the attentional set-shifting. Naively choosing one of them as the representative of executive function in the conventional joint model leads to information loss and potential misleading results. However, if one simultaneously incorporates TMTB and TMTA in a traditional joint model for multivariate longitudinal and survival data [15], [19], the high correlation between TMTA and TMTB causes multicollinearity problem and results in misleading results (Section 6). Furthermore, interpreting the effect of each surrogate of time-dependent risk factors on the hazard of interest is tedious and incomprehensible. In particular, the conventional joint model can only reveal how time-varying learning score, SDR, LDR, or recognition influence the hazard of AD but cannot conclude the overall effect of memory on AD. Therefore, a new joint modeling approach that enables such ‘many-to-one’ relationship must be developed for addressing all the aforementioned issues.====Factor analysis [1], [12], [13], [14] is a commonly used approach for exploring the correlations among multivariate observed and latent variables. Specifically, these methods have been introduced into survival analysis to investigate the relationships between medical traits and the endpoints of interest. [17] used confirmatory factor analysis (CFA) model to group latent risk factors from multiple observed variables and proposed an additive hazard model to explore the effects of latent risk factors on the hazard of chronic kidney disease. [8] adopted the corrected estimating equation approach to analyze an additive mean residual life model with latent factors summarized from the CFA model. However, their studies restricted the effects of latent factors on the hazard of interest to be time-independent. [11] and [16] considered joint models to analyze multivariate longitudinal biomarkers and survival data. However, their models assumed that subject-specific random effects were normally distributed. Moreover, they conducted full Bayesian analyses and did not investigated the asymptotic properties of parameter estimates.====We develop a new joint model for multivariate longitudinal and survival data with latent variables. The proposed model consists of two parts. The first part is a dynamic CFA model for depicting the dependence between time-varying latent factors and multivariate longitudinal observations. The ‘many-to-one’ relationship is well established via a factor loading matrix, and the obtained lower-dimensional latent factors can be regarded as a compressed summary of the multivariate observed variables. Meanwhile, we construct a mixed effects structure for each time-dependent latent factor to characterize its dynamic trajectory. The second part is a PH model for linking time-dependent latent factors and time-independent observed covariates to the hazard of interest. By introducing the factor analytic technique, the proposed model enables us to incorporate the information of all available observed indicators rather than subjectively select one of the surrogates to characterize a latent trait, thereby avoiding information loss and a tedious selection of the best surrogate. Moreover, the proposed model elicits attractive interpretation. Taking ADNI analysis as an example, the proposed model provides a clear and comprehensive answer to the central question in clinical trials, i.e. how neuropsychological traits, such as memory or executive function, influence the hazard of AD? Furthermore, in spite of comprehensive information incorporated, the proposed model is relatively simple due to the dimension reduction of risk factors through the CFA model. Such a reduction condenses the information of highly correlated surrogates, thereby avoiding the multicollinearity problem suffered by conventional regression analyses.====To analyze joint models for longitudinal and survival data, most of existing methods used likelihood-based approaches [7], [20], [28], wherein the random effects are treated as missing data, and the parameter estimates are obtained by maximizing the expectation of the complete-data log-likelihood. However, these methods relied on the normality assumption of random effects and are computationally intensive. When applying such likelihood-based approaches to the proposed joint model with latent variables, the computational inefficiency become severer due to the incorporation of the CFA model. Moreover, the normality assumption of random effects and latent factors may be too restrictive to reflect reality in practice. To address the aforementioned issues, we develop a two-stage method to analyze the proposed model. At the first stage, we use an asymptotically distribution-free generalized least square (ADF-GLS) method to conduct estimation for the CFA model. At the second stage, we adopt a conditional score method [22], [26] to analyze the PH model, given the estimates of the factor scores obtained from the CFA model in the first stage. By allowing the random effects and latent factors to be arbitrarily distributed, the proposed model greatly enhances its flexibility and capacity. The asymptotic properties of the parameter estimators are established.====The remainder of the paper is organized as follows: Section 2 introduces the proposed joint model. The model identifiability issue is discussed as well. Section 3 presents a hybrid estimation procedure that comprises an ADF-GLS approach and a conditional score method for parameter estimation. Section 4 establishes the asymptotic properties of the proposed estimators. Section 5 illustrates the utility of our method in the application of ADNI study. Section 6 concludes the paper. Appendix provides the proofs of theoretical results. Simulations are provided in Online Supplementary Material.",Consistent estimation of a joint model for multivariate longitudinal and survival data with latent variables,https://www.sciencedirect.com/science/article/pii/S0047259X21001056,5 September 2021,2021,Research Article,104.0
"Bai Zhidong,Silverstein Jack W.","KLASMOE & School of Mathematics and Statistics, Northeast Normal University, Changchun, China,Department of Mathematics, North Carolina State University, Raleigh, NC, USA","Received 26 August 2021, Accepted 26 August 2021, Available online 5 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104828,Cited by (0),". They recount their individual associations with him, mainly involving the behavior of eigenvalues of random matrices, and outline two areas of applied work he performed with one of the authors.","Paruchuri R. (P.R.) Krishnaiah was considered a great leader in the field of statistics, especially multivariate analysis. He devoted his academic career in the pursuit of knowledge and was a major influence in this area of statistics. He founded the famous ==== at the Department of Mathematics and Statistics, University of Pittsburgh in 1982 and in 1971 created the prestigious ==== as the founding editor in chief. Based on his wide knowledge in statistics and familiarity of the literature, his activities were spread throughout the world. He has contributed a great deal, including the area involving limit theorems on the eigenvalues of random matrices as the dimensions increase. The authors of this paper became directly associated with Professor Krishnaiah’s work on large dimensional random matrices. They pay tribute to this wonderful man by first recalling their fond remembrance of him, and then outlining significant contributions he gave to image reconstruction and to general information criteria (GIC) in model selection (note: “ZB” will refer to the first author, “JS” to the second author).",A tribute to P.R. Krishnaiah,https://www.sciencedirect.com/science/article/pii/S0047259X21001068,5 September 2021,2021,Research Article,105.0
"Düker Marie-Christine,Pipiras Vladas,Sundararajan Raanju","Faculty of Mathematics, Ruhr-Universität Bochum, Universitätsstr. 150, IB, 44780 Bochum, Germany,Department of Statistics and Operations Research, University of North Carolina, CB# 3260, Hanes Hall, Chapel Hill, NC 27599-3260, USA,Department of Statistical Science, Southern Methodist University, Dallas, TX 75205, USA","Received 1 July 2020, Revised 24 August 2021, Accepted 24 August 2021, Available online 4 September 2021, Version of Record 28 September 2021.",https://doi.org/10.1016/j.jmva.2021.104825,Cited by (0),"In a varying means model, the temporary evolution of a ====-vector system is determined by ","The topic and the results of this work can be viewed from several interesting angles. We shall first describe the problem in more technical terms and then discuss its connections to other lines of investigation. We are interested here in a statistical model of the form ====Here, ==== is thought as time, the observations ==== are ====-vectors, ==== is a ====-vector deterministic function with component functions ==== and ==== are ====-vector i.i.d. error terms with ====. We shall further assume that the covariance matrix of the error terms ==== may vary with time, and also treat the simpler special case when it does not, that is, ====, separately — the reader may have this case in mind for the rest of this section. The case when ==== are dependent in time will also be discussed but the focus below will be on the case when ==== are independent across time. We think of (1) as modeling varying means across ==== dimensions and shall refer to (1) as a varying means or VM model. The mean vector function ==== is nonparametric and will be assumed to be piecewise continuous. The focus here is on the “fixed ====, large ====” asymptotics.====The question we ask here for the VM model is whether there are (linearly independent) linear combinations of the components of ==== that are stationary across time ==== at the mean level. That is, we look for a ==== matrix ==== with linearly independent columns (which are not identically zero) such that ==== for a constant ==== vector ==== or, at the model level, ====The dimension ==== indicates how many non-constant deterministic functions drive the system (1). We are interested here in inference about ==== (and hence ====), and that of the corresponding subspace.====To make inference about ==== and ====, we relate (2) to a problem involving matrix nullity (the number of zero eigenvalues) and rank. The matrix in question is defined based on the following observation. Under mild assumptions on ==== (see Section 3), the relation (2) is equivalent to ====where ====The matrix ==== is positive semidefinite. Then, according to (3), ====where ==== and ==== denote the nullity and the rank of the matrix ====. Inference about ==== is then that about the nullity or the rank of the matrix ====. Similarly, the cotrending subspace ==== is spanned by the eigenvectors associated with the zero eigenvalues of ====.====A number of tests are available for the rank of a matrix, given an asymptotically normal estimator ==== of ====, especially in the econometrics literature (Cragg and Donald [8], Gill and Lewbel [14], Kleibergen and Paap [21], Robin and Smith [29]), and slightly less so in the statistics literature (Anderson [1], Eaton and Tyler [12], Camba-Mendez and Kapetanios [6]). Furthermore, there are technical reasons for ==== to be nondefinite, when ==== is positive semidefinite itself, as it is the case here (Donald et al. [11] and Section 3). Nondefiniteness refers to matrices which are neither positive semi-definite nor negative semi-definite. Much of the technical contribution of this work consists of introducing such an estimator for ==== in (4), proving its asymptotic normality result and then applying the available matrix rank tests. The considered estimator turns out to have a surprisingly simple form. We shall also discuss what can be said about the convergence of the sample eigenvectors corresponding to the cotrending subspace ====. Another less technical contribution is to relate the considered problem to a number of other lines of work, as outlined next and investigated in greater depth below.====The problem described above is related to stationary subspace analysis (SSA), which was one motivating starting point. In SSA, one similarly seeks linear combinations of vector observations collected over time that are stationary, possibly not just at the mean but also the covariance level. The SSA was introduced by von Bünau et al. [5], and studied further by Blythe et al. [3], Sundararajan and Pourahmadi [33]. See also [7] in the context of locally stationary processes (Dahlhaus [9], Dahlhaus and Polonik [10], Ombao et al. [25]). In the work somewhat parallel to this (Sundararajan et al. [32]), we use similarly matrix constructs and their eigenstructure to study the SSA at the covariance level, supposing the mean is zero, though the overall approach turns out to be much more involved than the one presented here.====This work, probably unsurprisingly to the reader, also has connections to principal component analysis (PCA). Two aspects of this connection should be highlighted here and kept in mind. Unlike in the standard PCA, to estimate ==== in (4), we shall not work with the sample covariance matrix of the data but rather effectively with the autocovariance matrix at lag ====. Using such covariance matrices in PCA though is not completely new; see, e.g., Lam and Yao [22]. Furthermore, from the PCA perspective, this work provides a new framework where the number of principal components can be tested for in a theoretically justified approach.====Lastly, we shall also draw connections to cointegration. Cointegration is a, if not the, approach of choice in modern time series analysis that also seeks linear combination of nonstationary time series that are stationary. Nonstationarity though is understood in the form of random walks, whereas the formulation (1) takes the view of deterministic trends. Similar time series realizations are nevertheless expected to be captured by either formulation. In our real data applications, we shall also contrast our approach to cointegration. The term “cotrending” used in this work is inspired by “cointegrating”. While “integrated” refers to random walks, “trended” alludes here to deterministic trends.====The rest of the paper is organized as follows. In Section 2, we introduce an estimator of ==== in (4) and state its asymptotic normality results, whose proofs can be found in Appendix A. Complementary results can be found in Appendix B which got moved to the Supplementary material. The application of some of the available matrix rank tests is discussed in Section 3. Section 4 concerns inference of the cotrending subspace ====. In Section 5, we discuss possible extensions of the underlying model and give an alternative approach to estimate the cotrending dimension. Section 6 details connections to PCA and cointegration. A simulation study and applications are considered in Sections 7 Simulation study, 8 Applications. Section 9 concludes.",Cotrending: Testing for common deterministic trends in varying means model,https://www.sciencedirect.com/science/article/pii/S0047259X21001032,4 September 2021,2021,Research Article,106.0
"Bodnar Taras,Parolya Nestor","Unit of Statistics, School of Business, Örebro University, Fakultetsgatan 1, SE-70182 Örebro, Sweden,Department of Mathematics, Stockholm University, Roslagsvägen 101, SE-10691, Stockholm, Sweden,Department of Applied Mathematics, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands","Received 10 August 2021, Revised 25 August 2021, Accepted 25 August 2021, Available online 4 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104826,Cited by (9),"Recently, the shrinkage approach has increased its popularity in theoretical and applied statistics, especially, when ==== of a given portfolio based on the shrinkage approach is presented as well.","High-dimensional inference procedures play an important role in many fields of science, like in economics, finance, environmetrics, physics, signal processing, etc., when a statistical model is needed to be fitted to real data. For instance, high-dimensional optimal portfolios are well motivated by the rapid development of technology, which provides investors opportunities to construct a portfolio consisting of a large number of assets traded simultaneously across the world. Moreover, the availability of high-frequency financial data provides a considerable amount of information which can be used in the construction of optimal portfolios.====It is remarkable that the application of the traditional sample estimators is not recommendable in the high-dimensional setting. Although the sample estimators work well when the process dimension is fixed and is significantly smaller than the number of observations, it does not longer hold when the two quantities are comparable. The former case is often used in statistics and it is called the standard asymptotic regime (see, [63]). Under this asymptotic regime the traditional sample estimators, like the maximum likelihood estimator or method of moments estimator, are usually consistent under some regularity conditions. However, it does not longer hold true when the process dimension is comparable to or even larger than the sample size. Here, we are in the situation when both the number of assets and the sample size can tend to infinity. This double asymptotic regime has an interpretation when the ratio between the process dimension and the sample size, also known as the concentration ratio, tends to a finite value as the sample size tends to infinity. This asymptotic regime is known as a high-dimensional asymptotics or “Kolmogorov” asymptotics (see, e.g., [33]). Under the high-dimensional asymptotics the sample estimators behave very unpredictable and they are far from the optimal ones. In general, the greater the concentration ratio the worse are the sample estimators. This well-known problem in statistics is called ”the curse of dimensionality”.====Recently, new estimators came in play which are biased but can significantly reduce the mean square error in comparison to the traditional estimators. These estimators are known as shrinkage estimators and were introduced in the seminal paper of Stein (see, [81]). A shrinkage estimator is usually defined as a linear combination of the corresponding sample estimator and a known target. The corresponding coefficients in the linear combination are often called shrinkage intensities. It is a very challenging task to find consistent estimators for the shrinkage intensities.====The first shrinkage estimator was developed for the mean vector of a multivariate normal distribution with identity covariance matrix [58], [81] and was extended to the case of an arbitrary covariance matrix in [5], [7], [8], [41], [47], [50], [67], [82]. These results were obtained in the standard asymptotic regime, while a high-dimensional version of the James–Stein type estimator was proposed by [38]. Recently, an optimal shrinkage estimator obtained by minimizing the expected quadratic loss function was derived in [85], while a shrinkage estimator of the mean vector shrunk towards an arbitrary target vector was introduced in [21].====The situation becomes more challenging when the covariance matrix is estimated and, especially, when one needs to infer its inverse, the precision matrix. There are some significant improvements when the covariance matrix has a special structure, e.g. sparse, low rank etc. (see, [35], [36], [75]). The results for the covariance matrix that possesses a factor structure were derived in [42], [43], [44]. In these cases the covariance matrix can consistently be estimated even for high-dimensional data. However, when no information about a specific structure of the covariance matrix is available, the shrinkage estimator seems to be the most favorable approach in the high-dimensional setting (cf., [19], [65], [66]. The shrinkage estimators for the precision matrix were derived in [20], [62], [84], among others.====In order to handle the curse of dimensionality in the case of the high-dimensional asymptotic regime the results from random matrix theory are usually used. Random matrix theory is a very fast growing branch of probability theory with many applications in statistics and finance. It studies the behavior of the eigenvalues of random matrices under the double asymptotic regime (see, e.g., [2], [3], [4], [13], [23], [28], [49], [68], [76], [78]). It is discovered that appropriately transformed random matrix at infinity has a nonrandom behavior and showed how to find the limiting density of its eigenvalues. In particular, Silverstein and Bai [78] proved under very general conditions that the Stieltjes transform of the sample covariance matrix tends almost surely to a nonrandom function which satisfies some equation. This equation was first derived by [68] who showed how the real covariance matrix and its sample estimator are connected at infinity, while a general form of this equation was given in [76]. Finally, using the results of random matrix theory statistical tests on the structure of the covariance matrix were suggested by [14], [37], [46], [52], [86], [88].====Improved estimators of the model parameters constructed by employing random matrix theory, especially shrinkage-based estimators, are widely used in many fields of science, like in signal processing and finance (see, [34], [40], [44], [45], [55], [83], [87]). For instance, an improved calibration of the high-dimensional precision matrix was suggested in [87], while the applications of random matrix theory to signal processing and portfolio theory was discussed in [45]. Furthermore, several authors showed that the shrinkage estimators applied to portfolio weights indeed lead to better results (see, e.g., [22], [25], [48], [51], [65]. In particular, the shrinkage estimator for the covariance matrix was applied to construct an improved estimator of the weights of the global minimum variance portfolio by [65], while the multivariate shrinkage estimator obtained by shrinking the portfolio weights directly was suggested in [51]. The same idea was also used by [48] who constructed a feasible shrinkage estimator for the global minimum variance portfolio which dominates the traditional sample estimator. More recently, the shrinkage estimators based on an arbitrary target vector of portfolio weights were derived by [25] and [22] in the case of the global minimum variance portfolio and mean–variance portfolio, respectively. Finally, statistical test theory on the optimality of portfolio weights was developed in [17], [18] that is based on the shrinkage approach, while sequential procedures derived on the weights of optimal portfolios were established in [9], [10].====The rest of the paper is organized as follows. In Section 2 we present the shrinkage estimator for the high-dimensional mean vector, covariance matrix and precision matrix. Recent results of the application of the shrinkage approach in finance is discussed in Section 3. Discussion of the results is provided in Section 4.",Recent advances in shrinkage-based high-dimensional inference,https://www.sciencedirect.com/science/article/pii/S0047259X21001044,4 September 2021,2021,Research Article,107.0
"Fan Jinlin,Zhang Yaowu,Zhu Liping","Center for Applied Statistics, Institute of Statistics and Big Data, Renmin University of China, Beijing 100872, China,Research Institute for Interdisciplinary Sciences, School of Information Management and Engineering, Shanghai University of Finance and Economics, Shanghai 200433, China","Received 14 August 2021, Revised 20 August 2021, Accepted 20 August 2021, Available online 3 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104818,Cited by (0)," of the modified distance correlation thoroughly. In addition, we demonstrate its finite-sample performance through extensive simulations and a real-world application.","Measuring nonlinear dependence and testing statistical independence are fundamental problems in statistics. Let ==== and ==== be two random vectors of primary interest. For example, ==== stands for the systolic and diastolic pressures, and ==== stands for the degree of exposures to air pollutants. It is of scientific importance to investigate whether the blood pressures are relevant to the exposures to pollutants. However, in such scientific studies, both ==== and ==== are measured with non-ignorable random errors. Instead of observing ==== and ==== directly, we observe the surrogate random vectors, ==== and ====, which are related to ==== and ====, respectively, through ====where ====, ==== are non-random vectors, ====, ==== are full-rank matrices, and ====, ==== are unobservable random errors. In some real-world applications, ==== and ==== can be identity matrices. We assume ====, ====, and ==== are mutually independent, and allow ==== and ==== to be dependent. The measurement error model (1) is widely assumed in literature. See, for example, [7] and [12].====Extensive studies have been conducted to investigate how ==== dependents on ==== in the regression context, where the covariates are measured with linear errors. Ignoring measurement errors will induce non-ignorable bias in parameter estimation, which would lead misleading results in practice. [3] proposed an efficient estimation for linear measurement error model in which ==== depends on ==== linearly. [23] and [32] considered partially linear model and generalized linear model, respectively. Interested readers may refer to [7] for a comprehensive review. Recent advances include [1], [20], [24], [26] and [34]. In these recent studies, the covariate vectors are allowed to be high dimensional.====The measurement error models have also been considered in the context of sufficient dimension reduction [11], which falls into the framework of semi-parametric regression. [6] observed that, the central subspace of ==== given the unobservable covariate vector ====, recovered through ordinary least squares and sliced inverse regression [19], is the same as that of ==== given a particular linear transformation of ====. In their context this surprising phenomenon is referred to as the invariance property, which was later found to be generally applicable to many other sufficient dimension reduction methods [21]. Similar observation has also been made by [10] in the context of the score test.====Measuring nonlinear dependence between ==== and ==== in the absence of measurement errors has been extensively studied in literature. See, for example, [2], [4], [8], [14], [16], [30], [33], [35] and [36]. Interested readers may refer to [31] for a comprehensive review. However, measuring nonlinear dependence between ==== and ==== in the presence of measurement errors is rarely touched in the literature. To the best of our knowledge, [9] is perhaps the first attempt to measure nonlinear dependence of ==== and ==== through the distance correlation between ==== and ==== [33]. In their context, both ==== and ==== are restricted to be univariate, and the random errors ==== and ==== are assumed to be normal with zero means and known variances. These requirements indeed limit the usefulness of [9]’s proposal in real-world problems. In addition, the metric proposed by [9] cannot attain one even when ==== and ==== are perfectly linearly dependent. This is because the variabilities of ==== and ==== are always larger than those of ==== and ==== when ==== and ==== in Model (1), due to the presence of measurement errors.====In this paper, we propose to test statistical independence and measure nonlinear dependence between ==== and ====, the random vectors of primary interest, through distance correlation between the surrogate random vectors ==== and ====. We allow for non-normal measurement errors and multivariate random vectors. The distance correlation consists of two components: distance covariance and distance variance. Throughout we use ==== to stand for the classic distance covariance between ==== and ====, and ==== to stand for the distance variance of ==== [33]. To test statistical independence, it suffices to use distance covariance. In particular, to test statistical independence between ==== and ====, we suggest to modify the weight functions in the classic distance covariance such that the modified distance covariance between ==== and ==== is exactly the same as ====. We refer to this property as an invariance law. To measure the degree of nonlinear dependence, it is required to quantify the variabilities of ==== and ====, which are usually smaller than those of ==== and ====. An immediate issue arises: The classic distance correlation between ==== and ==== cannot attain one even when ==== and ==== are exactly linearly dependent. In other words, the presence of measurement errors may substantially weaken the degree of nonlinear dependence. We use a toy example to demonstrate this phenomenon. Suppose for now that in Model (1), both ==== and ==== are univariate and ====. In addition, ==== and ==== follow bivariate standard normal distribution with correlation coefficient ====. The squared distance correlation between ==== and ====, averaged over 1000 replications and denoted as ====, is displayed in Fig. 1. We also report ==== with dashed line in the same figure for the purposes of comparison. It can be clearly seen that, when the correlation coefficient between ==== and ====, denoted as ====, approaches 1 or -1, ==== is very near to 1. However, ==== is smaller than 0.2 throughout. This indicates that, directly using ==== to quantify the degree of nonlinear dependence between ==== and ==== is substantially biased. To address this issue, we suggest to modify the distance variances slightly, by assuming the repeated measurements, ==== and ====, which are indeed the respective independent copies of ==== and ==== when ==== and ==== are fixed, are available. We suggest to use ==== and ====, to replace ==== and ==== in distance correlation, which leads to a new metric, denoted as ==== in Fig. 1. It can be clearly seen that, the modified squared distance correlation, ====, is indeed very close to ====, across all ==== values.====This paper is organized as follows. In Section 2, we introduce an invariance law to test independence in the presence of measurement errors. At the sample level, we propose to estimate the modified distance correlation with ====-statistic theory and the distance variances with repeated measurements. The asymptotic properties of these estimates are also studied. In Section 3 we illustrate the finite-sample performance of our proposal through extensive simulations and an application. We conclude this paper with brief discussions in Section 4.",Independence tests in the presence of measurement errors: An invariance law,https://www.sciencedirect.com/science/article/pii/S0047259X21000968,3 September 2021,2021,Research Article,108.0
Jolliffe Ian,"Isle of Wight, UK","Received 19 August 2021, Accepted 19 August 2021, Available online 3 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104820,Cited by (19),"Principal component analysis (PCA) is one of the most widely used multivariate techniques. A little more than 50 years ago I first encountered PCA and it has played an important role in my career and beyond, for many years since. I have been persuaded that an account of my 50-year journey through time with PCA would be a suitable topic for inclusion in the Jubilee Issue of JMVA and this is the result.","Principal component analysis (PCA) is one of the most widely used techniques in multivariate analysis, so it seems appropriate for the Jubilee Issue of JMVA to include an account of its developments over the past 50 years. I have been persuaded to write such an account even though I have never published in JMVA. The theoretical developments which are the mainstream of JMVA hold comparatively little interest for me. Rather I am a retired statistician who liked to get his hands dirty analysing and interpreting data or, as John Tukey memorably put it, ‘playing in other people’s backyards’. However, I also liked to be aware of at least the basic theory underlying any techniques I used, so my book on PCA, Jolliffe [37], [41], – probably the main reason I was invited to contribute to this Issue – includes a mixture of methodology, data, and theory. PCA was historically not a very frequently discussed topic in JMVA. As far as I can ascertain, it was more than 10 years before JMVA first included an article with the phrase “principal component analysis” in its title (Ruymgaart [68]). It is possible that because, at heart, PCA is a simple technique, much of its basic theory had already been published before JMVA was launched. There are, however, various subtleties and modifications to PCA, which are still being explored and expanded, and there is strong evidence of increasing recent interest within JMVA, with 16 articles whose title contains one of the phrases “principal component analysis”, “principal components” or “PCA”, published in the 5 years 2016–2020. This compares with only 15 in the first 34 years of JMVA’s existence. There will, of course, be other articles that involve PCA but have none of these phrases in their title, but there is no unique simple way to search for them. I believe that focussing on the easy-to-trace PCA articles gives a reasonable representation of the relative rarity of PCA-based papers in early issues of JMVA as compared to a significant expansion of interest in recent years. From a practical point of view PCA has also never been more widely used. The various techniques that come under headings such as big data, machine learning, artificial intelligence, etc. often need a dimension-reducing first step, a role that can be played by PCA. The next Section of this paper will define PCA and give a brief description of its history prior to the first issue of JMVA. Subsequent Sections will cover the 50 years of JMVA chronologically, concentrating on developments related to PCA, both in my own research and more widely. The final Section will consist of some closing remarks.",A 50-year personal journey through time with principal component analysis,https://www.sciencedirect.com/science/article/pii/S0047259X21000981,3 September 2021,2021,Research Article,109.0
"Kuriki Satoshi,Takemura Akimichi,Taylor Jonathan E.","The Institute of Statistical Mathematics, 10-3 Midoricho, Tachikawa, Tokyo 190-8562, Japan,The Center for Data Science Education and Research, Shiga University, 1-1-1 Banba, Hikone, Shiga 522-8522, Japan,Department of Statistics, Sequoia Hall, 390 Jane Stanford Way, Stanford University, Stanford, CA 94305-4020, USA","Received 12 August 2021, Accepted 13 August 2021, Available online 3 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104819,Cited by (1),The tube method or the volume-of-tube method approximates the tail probability of the maximum of a smooth ==== when the variance is not constant.,None,The volume-of-tube method for Gaussian random fields with inhomogeneous variance,https://www.sciencedirect.com/science/article/pii/S0047259X2100097X,3 September 2021,2021,Research Article,110.0
Cook R. Dennis,"School of Statistics, University of Minnesota, 313 Ford Hall, 224 Church St SE, Minneapolis, MN 55455, United States of America","Received 11 August 2021, Revised 24 August 2021, Accepted 24 August 2021, Available online 3 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104812,Cited by (6),"We describe how many dimension reduction strategies are connected conceptually and philosophically, paving the way for a unified approach to multivariate dimension reduction in ","Viewed broadly, dimension reduction – reducing the size of a data set while preserving relevant information – is a huge area. It is one of the foundational concerns of statistics, it touches on all of the applied sciences and it is an active research area today, as it has been for over a century. Classical multivariate books by Anderson [5], Morrison [51], Gnanadesikan [40] and Seber [61], and reviews like that by Schervish [60] all emphasize dimension reduction methods, particularly principal components and factor analysis, as major tools of multivariate analysis. New dimension reduction methods seem to spring from the sciences daily. Burges [15] provided an overview of dimension reduction from the computer science and machine learning viewpoints. Cavalli-Sforza et al. [16] used principal components to produce acclaimed continental maps summarizing human genetic variation. In an influential article, Bro and Smilde [14] described the use of principal components in chemometrics. Principal components have been called “eigengenes” when used in the analysis of microarray data [4]. Our emphasis is on dimension reduction from a statistical perspective with the general objective of giving a unifying philosophical and historical overview of dimension reduction that leads naturally to contemporary methodology.====Although they are not normally linked, Fisher’s ideas on sufficiency and Hotelling’s rationale for principal components in many ways can be seen as the birth of a substantial slice of contemporary dimension reduction ideas and methods. One goal of this article is to describe that foundation, how it has evolved and how it influenced contemporary methodology. This article is not intended as a comprehensive literature review or as a guide to methodology. We target underlying motifs and how they form a conceptual foundation for many methods.",A slice of multivariate dimension reduction,https://www.sciencedirect.com/science/article/pii/S0047259X21000907,3 September 2021,2021,Research Article,111.0
"Kurita Eri,Seo Takashi","Department of Applied Mathematics, Tokyo University of Science, Tokyo, Japan","Received 19 August 2021, Revised 24 August 2021, Accepted 25 August 2021, Available online 3 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104824,Cited by (1)," for certain parameters. Finally, we present a numerical example to illustrate the proposed procedure.","In a multivariate analysis, the assessment of whether multidimensional data follow a multivariate normal distribution is an important task, and many test procedures have been proposed and discussed for multivariate normality (MVN) tests (for related studies, see Farrell et al. [3], Hanusz et al. [4], Henze and Zirkler [7], Kollo [12], Rao et al. [15], Thode [18], and Zhou and Shao [22]). Among the test statistics used in MVN tests are those based on multivariate skewness or multivariate kurtosis, which are defined by Koziol [13], Mardia [14], and Srivastava [17], and their distributions are given for a large sample. In addition, Henze [6] discussed the asymptotic distribution of Mardia’s kurtosis test statistic under nonnormality. An MVN test using a normalizing transformation for Mardia’s multivariate kurtosis was recently given by Enomoto et al. [2]. Moreover, Yamada et al. [21] discussed a test for an MVN with two-step monotone missing data. By considering a case in which the data contain missing values, this study extends the sample measure of multivariate kurtosis defined by Mardia [14]. In this paper, we define a sample measure of multivariate kurtosis when the data have a two-step monotone pattern of missing observations, where the multivariate kurtosis defined here is different from that by Yamada et al. [21]. We also consider an MVN test under the assumption of a two-step monotone missing data. In particular, we focus on an MVN test statistic using multivariate kurtosis. For two-step monotone missing data, the maximum likelihood estimators (MLEs) of the mean vector and covariance matrix are given (see Kanda and Fujikoshi [9]). Tests of the mean vectors or covariance matrix using these MLEs were discussed by Hao and Krishnamoorthy [5], Tsukada [19], and Yagi et al. [20]. The sample measure of multivariate kurtosis discussed in this paper is also based on MLEs developed by Kanda and Fujikoshi [9]. By decomposing the multivariate kurtosis, the sample analogue of multivariate kurtosis with two-step monotone missing data can be defined, and asymptotic results of the expectation and variance are given using a perturbation method. For references partially related to the perturbation method described in this paper, see Kawasaki and Seo [10] and Kawasaki et al. [11]. The rest of this paper is organized as follows. Section 2 introduces two-step monotone missing data and provides a definition of the sample measure of multivariate kurtosis using such data. In Section 3, we derive the expectation and variance of the sample measure of multivariate kurtosis defined in Section 2, where the result is partly an approximation through an asymptotic expansion. In Section 4, some simulation results for two-step monotone missing data are presented to investigate the accuracy of the normal approximation of the test statistics proposed in this paper. We also provide a numerical example to illustrate the method in Section 4. Finally, we give some concluding remarks in Section 5.",Multivariate normality test based on kurtosis with two-step monotone missing data,https://www.sciencedirect.com/science/article/pii/S0047259X21001020,3 September 2021,2021,Research Article,112.0
"Huang Yuan,Li Changcheng,Li Runze,Yang Songshan","Department of Biostatistics, Yale School of Public Health, New Haven, CT 06510, USA,Department of Statistics, The Pennsylvania State University at University Park, PA 16802, USA,Institute of Statistics and Big Data, Renmin University of China, Beijing 100872, China","Received 1 August 2021, Revised 17 August 2021, Accepted 17 August 2021, Available online 3 September 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104813,Cited by (2),"Testing high-dimensional means has many applications in scientific research. For instance, it is of great interest to test whether there is a difference of gene expressions between control and treatment groups in genetic studies. This can be formulated as a two-sample mean testing problem. However, the Hotelling ","With the rapid development of modern data collection and processing technologies, a vast amount of data with large dimensional features become increasingly popular and have been involved in many scientific areas such as biology, medicine, finance, and social science, calling for an advancement of classical methods to handle the high dimensionality. In recent years, considerable attention has been devoted to variable selection and feature screening ([17] and references therein). Statistical inference for high-dimensional means has been a very active research topic in the literature because of its important applications, such as those in genetic studies. For instance, many biological processes involve regulation of multiple genes, and such research suffers from low power for detecting important genetic markers and poor reproducibility if it focuses on the analysis of individual genes [47]. In other cases, genes are often analyzed in their functional groups to reduce the complexity of analysis [25]. Accordingly, the analysis of gene sets/pathways, which are groups of genes sharing common biological functions, chromosomal locations, or regulations, has become increasingly important in modern biological research. In many important applications, the problem of evaluating whether a group of genes are differentially expressed from another group can be formulated as a problem of testing two-sample means.====Hotelling’s ==== test [23] perhaps is the most well-known test on means in the multivariate analysis when the sample is from multivariate normal distributions. To implement the Hotelling ==== test, the sample size ==== should be greater than the dimension ==== of data. Motivated by real-world applications, Dempster [13], [14] proposed tests for a two-sample normal mean problem when ====. Läuter [27] proposed exact ==== and ==== tests for normal mean problems based on left-spherical distribution theory [12] to improve the power of the Hotelling ==== when ====. See more detailed discussions in Section 2. Bai and Saranadasa [2] employed random matrix theory to prove that the power of ==== test can be adversely affected even with ====. Since the seminal work [2], testing hypotheses on high-dimensional means has become a very active topic.====This paper aims to provide a selective overview of research on testing high-dimensional mean problem. We will focus on the two-sample mean problem. Since the Hotelling ==== test involves inverse of a sample covariance matrix and is not well defined when the inverse does not exist, Bai and Saranadasa [2] proposed a test statistic based on the ====-distance between the sample mean and the population mean, and has inspired many follow-up works including, but not limited to, [8], [10], [40], [41], [48], [54]. We review these methods in Section 3. Multiple comparison has been used to construct tests for high-dimensional means by considering tests for means of individual variables. This leads to ====-type tests, which have been shown to be more powerful than the ====-distance based tests in the presence of a few large sparse signals [4], [52]. We review works on this topic in Section 4. Since the ====-distance based tests may be more powerful than the ====-type tests in the presence of dense signals, i.e., many small signals. These tests cannot dominate each other. Adaptive tests are ====-distance based tests with ==== being selected by data-driven methods. In other words, the adaptive tests essentially aim to achieve high power against various kinds of alternatives by adapting test statistics based on ====-values calculated from statistics of different orders [22], [51]. These tests are reviewed in Section 5. The ====-distance based tests, ====-type tests, and the adaptive tests do not take into account the correlation among variables. To utilize the correlation information in testing the high-dimensional means, researchers have considered projecting the high-dimensional samples to a low-dimensional space and then applying the classical Hotelling ==== test on the projected data. Lopes et al. [34] constructed a random projection test, followed by Thulin [43] and Srivastava et al. [42] with permutation-based computation methods to handle multiple projections. Huang [24] derived the theoretical optimal direction with which the projection test possesses the best power under alternatives, and further proposed a sample-splitting strategy to construct an exact ==== test. Li and Li [31] and Liu et al. [33] further studied how to implement the projection test using the optimal projection direction in practice. Section 6 provides a comprehensive review of these projection tests. We provide a numerical simulation comparison among these tests for the high-dimensional two-sample mean problem in Section 7, followed by discussions in Section 8.",An overview of tests on high-dimensional means,https://www.sciencedirect.com/science/article/pii/S0047259X21000919,3 September 2021,2021,Research Article,113.0
"Zhang Jin-Ting,Zhou Bu,Guo Jia","Department of Statistics and Data Science, National University of Singapore, Singapore,School of Statistics and Mathematics, Zhejiang Gongshang University, China,School of Management, Zhejiang University of Technology, China","Received 3 October 2020, Revised 19 August 2021, Accepted 20 August 2021, Available online 2 September 2021, Version of Record 29 September 2021.",https://doi.org/10.1016/j.jmva.2021.104816,Cited by (0),"A general linear hypothesis testing (GLHT) problem in heteroscedastic one-way ==== for high-dimensional data is considered and a normal reference ====, the proposed test statistic under the null hypothesis and a chi-square type mixture have the same normal or non-normal limiting distributions. It is then suggested to approximate the test’s null distribution using the distribution of the chi-square type mixture, which can be further approximated by the Welch–Satterthwaite chi-square-approximation with approximation parameters consistently estimated. Several simulation studies and a real data application are presented to demonstrate the good performance of the proposed test.","As data collecting technology evolves, high-dimensional data become increasingly prevalent in the recent decades. The main feature of high-dimensional data is that the data dimension may be comparable to or even much larger than the sample size. A motivating example for our study is the soft tissue tumors DNA microarray described in Witten and Tibshirani [37]. Following these two authors, we focus on five types of soft tissue tumors with group sample sizes at least ==== in the dataset, including the synovial sarcoma, myxoid liposarcoma, dedifferentiated liposarcoma, myxofibrosarcoma, and malignant fibrous histiocytoma, with each observation having 22,283 gene expression levels. Of interest is to test whether the five groups of soft tissue tumors have the same mean gene expression levels. Since in high-dimensional settings, it is often difficult to figure out if the five types of soft tissue tumors have the same covariance matrix, this motivates the following heteroscedastic one-way MANOVA (multivariate analysis of variance) testing problem for high-dimensional data.====Suppose we have ==== independent ====-dimensional i.i.d. samples of sample size ====, respectively: ====We allow the dimension ==== to be much larger than the total sample size ==== and we do not make any assumption on the equality of the ==== covariance matrices ====. The goal is to test whether the ==== mean vectors are equal: ====The main challenge of high-dimensional data analysis is that the sample covariance matrices are often singular or near singular, so classical testing procedures involving the inverse of sample covariance matrices, such as the well-known Hotelling ==== test, are not well defined or no longer powerful. Many authors have proposed alternatives to surmount this challenge in the past decades. When ====, the problem (2) reduces to the high-dimensional two-sample problem. Under the assumption that the two samples have equal covariance matrices, Dempster [14], [15] firstly proposed some non-exact tests using F-distribution approximation, and Bai and Saranadasa [4] proposed a non-exact test by removing the sample covariance matrix from the Hotelling ==== test. Since then, a number of different tests have been proposed, such as Srivastava and Du [28], Chen and Qin [12], Aoshima and Yata [1], [3], Park and Ayyala [25], Feng et al. [16], Cai et al. [8], Wang et al. [34], Chen et al. [11], and Ma et al. [23], among others. In the above literature, many of them required strong assumptions on the underlying covariance matrices except Katayama and Kano [22] who proposed a weighted two-sample test with a proper choice of the weight matrix so that the asymptotic null distribution of the proposed test is asymptotically normal without any assumptions on the population covariance matrices, Ishii et al. [21] who applied a data transformation technique (Aoshima and Yata [3]) to transform a strongly spiked eigenvalue model into a non-strongly spiked eigenvalue model, and Zhang et al. [42] who proposed a simple and adaptive ====-norm based test with the Welch–Satterthwaite (W–S) ====-approximation. For a general ====, when the ==== samples have a common covariance matrix, Schott [27] constructed a MANOVA test by extending Bai and Saranadasa’s [4] two-sample test, while Cai and Xia [9] proposed a linear transformation based test. When the ==== samples have different covariance matrices, Zhang and Xu [43] proposed an approximate solution based on Bennett’s [5] transformation, Yamada and Himeno [38] and Hu et al. [19] extended Chen and Qin’s [12] U-statistic based two-sample test to this ====-sample problem, Aoshima and Yata [2] studied the asymptotic normality of the test statistic of a ====-sample testing problem under some mild conditions, Hyodo et al. [20] considered a simultaneous confidence interval estimation problem for paired mean vectors, Chen et al. [10] proposed a powerful test for sparse and faint mean differences, and Watanabe et al. [35] studied the two-way MANOVA problem.====In this paper, we are interested in testing the following heteroscedastic general linear hypothesis testing (GLHT) problem: ====where ==== is a ==== matrix consisting of the ==== mean vectors and ==== is a ==== known coefficient matrix with full row rank ====. Actually, the heteroscedastic GLHT problem (3) is very general. By setting ==== to be any ==== contrast matrix, i.e., any ==== matrix with linearly independent rows and zero row sums, the GLHT problem (3) reduces to the one-way MANOVA problem (2). What is more, various post hoc and contrast tests can be written in the form of (3). For example, let ==== denote a unit vector of length ==== with the ====th entry being ==== and others ====, by setting ====, one can test if ====. This special kind of linear hypotheses for mean vectors with simple scalar coefficients was studied in Nishiyama et al. [24].====To the best of our knowledge, only a few articles have been devoted to test the important GLHT problem (3). In the context of multivariate linear regression models, Fujikoshi et al. [17] and Srivastava and Fujikoshi [29] considered testing (3) and studied Dempster’s test, Yamada and Srivastava [39] proposed a scale-invariant test, and Srivastava and Kubokawa [30] extended the works of Srivastava and Fujikoshi [29] and Yamada and Srivastava [39] to non-normal data. Recently, in the context of one-way MANOVA, Zhang et al. [41] considered this GLHT problem with a common covariance matrix and proposed an ====-norm based test and Zhou et al. [46] proposed a test based on U-statistics of the ==== samples (1) without assuming a common covariance matrix. In all these articles, strong assumptions are imposed on the underlying covariance matrices of the ==== samples (1) to ensure the null distributions of the associated tests are asymptotically normal when both the sample sizes and the dimension tend to infinity, except that Zhang et al. [41] who suggested to use Welch–Satterthwaite’s method for an adaptive null distribution approximation. However, similar to the tests proposed by Fujikoshi et al. [17], Srivastava and Fujikoshi [29], and Yamada and Srivastava [39], the test proposed by Zhang et al. [41] relies on the common covariance matrix assumption. When the covariance matrices of different samples are different, applications of these tests may lead to misleading results.====From previous discussion we see that tests proposed in existing literature are useful only for some underlying covariance matrices. To overcome this difficulty, in this paper, we propose and study an ====-norm based test statistic for the GLHT problem (3), extending the works of Zhang et al. [41] and Zhang et al. [42] so that the resulting test is useful for any underlying covariance matrices. We show that under some regularity conditions and the null hypothesis, the proposed test statistic and a chi-square type mixture have the same normal and non-normal limiting distributions. Therefore, it is justified to approximate the null distribution of the proposed test statistic using that of the chi-square type mixture. It turns out that the chi-square mixture is obtained from the proposed test statistic when the ==== samples are normally distributed and hence it is natural to term the associated ====-norm based test as a normal reference ====-norm based test. To approximate the distribution of the chi-square type mixture, the well-known Welch–Satterthwaite ====-approximation is applied with the approximation parameters consistently estimated from the data. The asymptotic powers of the proposed test is also established under some local alternatives. In Section 3, we demonstrate via simulations that in terms of size control, our new test outperforms the tests proposed by Zhang and Xu [43], Yamada and Himeno [38], Srivastava and Fujikoshi [29], Srivastava and Kubokawa [30], and Zhou et al. [46] for moderately or highly correlated data. In particular, we note that the tests proposed by Srivastava and Fujikoshi [29], Srivastava and Kubokawa [30], and Zhang et al. [41] cannot work properly for heteroscedastic GLHT problems.====It may be worthwhile to compare our test (denoted as ====) with the tests proposed by Zhang et al. [41], Zhang et al. [42] and Watanabe et al. [35] (denoted as ====, ==== and ====, respectively) in more details. First of all, the four tests are concerned with different testing problems. Zhang et al. [41] considered a GLHT problem under a one-way homogeneous MANOVA model, Zhang et al. [42] considered a simple two-sample problem, and Watanabe et al. [35] considered hypothesis testing problems under a two-way heteroscedastic MANOVA model, while in this paper, we consider a GLHT problem under a one-way heteroscedastic MANOVA model. For ==== and ====, the covariance homogeneity assumption is made but it is not the case for ==== and ====. The covariance homogeneity assumption is known to be very strong for high-dimensional multi-sample data and it is often difficult to verify. Existing tests for checking the covariance homogeneity assumption for high-dimensional data (see, e.g., Chen et al. [13], Srivastava and Yanagihara [31]) may require some additional unverifiable technical assumptions. Therefore, the problems considered by ==== and ==== are much more general than those considered by ==== and ==== and hence they have a much wider range of applications than ==== and ====. Secondly, the four tests consider two different data models. ==== considers a data model (defined by Condition (A1) of Watanabe et al. [35]) which allows heavy-tailed data while ==== and ==== consider a factor model (defined by Conditions C1 and C2 in the next section) which excludes heavy-tailed data, e.g., multivariate t distributed data with a small degrees of freedom. This means that the data model conditions of ==== is weaker than those of ==== and ====. Nevertheless, as ====-norm based tests, it is expected that the powers of all the four tests will be compromised for heavy-tailed data because for the one-sample mean vector testing problem, the power of an ====-norm based test can be significantly improved for heavy-tailed data after a spatial sign rank transformation; see Wang et al. [33] and Zhou et al. [45] for some details. Thirdly, the four tests establish different theoretical results and under different regularity assumptions. Zhang et al. [41] derived the chi-square type mixture expression of ==== for normal data only and such a result has not been established for non-normal data although the normal limiting distribution of ==== is derived with the data normality assumption dropped. Zhang et al. [42] showed that ==== under the null hypothesis and a chi-square type mixture have the same normal and non-normal limiting distributions under some mild conditions. The density approximation error bound of the W–S ====-approximation to a chi-square type mixture with nonnegative coefficients is also established and it is shown that the W–S ====-approximation is preferred to the normal approximation theoretically. Watanabe et al. [35], on the other hand, established the asymptotic normality of ==== under some regularity conditions which allow heavy-tailed data. In this paper, following Zhang et al. [42], we also show that ==== under the null hypothesis and a chi-square type mixture have the same normal and non-normal limiting distributions under some mild regularity conditions. Nevertheless, the regularity assumptions made for ==== and ==== involve the common covariance matrix ==== only while the regularity assumptions made for ==== and ==== involve all the group sample sizes and the group covariance matrices and hence are more complicated. In addition, the chi-square type mixture for ==== and ==== just depends on the eigenvalues of ==== while the chi-square type mixture for ==== depends on all the group sample sizes and the group covariance matrices in a complicated way. This makes the derivation of the asymptotic results of ==== more involved and hence more challenging than the derivation of the asymptotic results of ==== and ====; see the proofs of Theorem 1, Theorem 2, Theorem 3, Theorem 4 for more details. For example, in the proof of Theorem 3, we need to find a proper upper bound of ==== (defined in the next section), which depends on ==== and ==== in a complicated way. Fourthly, the four tests approximate their null distributions using either the normal approximation or the W–S ====-approximation. In fact, ==== approximates its null distribution by the normal approximation while all ==== and ==== employ the W–S ====-approximation to approximate the null distribution of the respective test statistic. In this sense, ====, and ==== work under a wider range of assumptions than ====. In fact, ==== cannot control its type-I error when the data are moderately or highly correlated. Besides, the implementation of the W–S ====-approximation for ====, however, is much more involved and also much more challenging than that for ==== and ====. In the implementation of the W–S ====-approximation for ==== and ====, the approximation parameters ==== and ==== depend on ==== and ==== only and their estimators depend on the ratio-consistent estimators of these three quantities only. However, in the implementation of the W–S ====-approximation for ====, the estimators of the approximation parameters ==== and ==== will depend on all the group sample sizes and all the ratio-consistent estimators of ==== and ==== and ==== in a complicated way. See Section 2.3 for details. Thus, it is also more challenging to prove the ratio-consistency of the associated estimators of ==== and ==== for ==== than for ==== and ====. Last but not least, the four tests use two different methods to estimate the approximation parameters. our test ==== estimates the approximation parameters based on the ratio-consistent estimators of ==== and ==== given in Eq. (27) only, ==== estimates the approximation parameters based on the unbiased estimators of ==== and ==== developed by Himeno and Yamada [18] only while ==== and ==== consider both the above two sets of estimators. The ratio-consistent estimators (27) are biased for non-normal data but they are simpler in form and much more computationally efficient than the unbiased estimators of Himeno and Yamada [18]. In addition, the simulation studies in Zhang et al. [41] and Zhang et al. [42] indicate that the performances of ==== and ==== with the above two sets of estimators are generally comparable under their simulation settings. Some additional simulation results presented in the Supplement Material also show that the performance of ==== with either of the above two sets of estimators are generally comparable except when the data are heavy-tailed and less correlated.====The rest of the paper is organized as follows. Our test statistic for the GLHT problem (3) is constructed in Section 2.1. In Section 2.2, we investigate some properties of our test and study its null distribution approximation. In Section 2.3, we consider the estimation of parameters for the null distribution approximation. In Sections 2.4 Asymptotic power of the proposed test, 2.5 Heteroscedastic one-way MANOVA, we respectively study the asymptotic power of our test and give a brief discussion about the heteroscedastic one-way MANOVA. Four simulation studies are presented in Section 3 with some additional simulation results presented in the Supplementary Material. An application to the soft tissue tumors data is presented in Section 4. Technical proofs of the main theorems are outlined in Appendix A.",Linear hypothesis testing in high-dimensional heteroscedastic one-way MANOVA: A normal reference ,https://www.sciencedirect.com/science/article/pii/S0047259X21000944,2 September 2021,2021,Research Article,114.0
Letac Gérard,"Institut de Mathématiques de Toulouse, 118 route de Narbonne, 31062 Toulouse, France","Received 10 August 2021, Accepted 10 August 2021, Available online 26 August 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104811,Cited by (1),Consider a measure ==== on ==== with variance function ====A dual measure ==== satisfies ==== Such a dual measure does not always exist. One important property is ==== leading to the notion of duality among exponential families (or rather among the extended notion of T exponential families ==== obtained by considering all translations of a given exponential family ====).,"One can be surprised by the explicit formulas that one gets from large deviations in one dimension. Consider iid random variables ==== and the empirical mean ====. Then the Cramér theorem [6] says that the limit ====does exist for ====. For instance, the symmetric Bernoulli case ==== leads, for ====, to ====For the bilateral exponential distribution ==== we get, for ====, ====What strange formulas! Other ones can be found in [15], Problem 410. The present paper is going to interpret in certain cases the function ==== as the Laplace transform of a certain dual measure ==== which can be deduced explicitly from the distribution ==== of ====. Since the Cramér theorem can be seen as a result about one dimensional exponential families, we develop the idea in this framework, using the large box of examples obtained from the theory of variance functions of exponential families initiated by the article of Carl Morris [20] in 1982. An even simpler example of duality is provided by the Tweedie families (Barlev and Enis [2], Jørgensen [9] and Tweedie [21]): the variance function ==== with ==== has dual ==== with ==== for suitable pairs ==== (see (8)). For instance, the Inverse Gaussian family ==== has dual ====.====The Poisson distribution, with the one of the simplest variance functions ====, leads to the study of exponential family with variance function ==== generated up to a translation by the unsymmetric stable law with Laplace transform ====, also called Landau distribution. This gives tools for describing duals of other familiar exponential families. The cases of the normal and gamma families are very simple, being self dual, but other familiar cases like the negative binomial, the Bernoulli distribution and the cubic families are tougher. Finally, we consider another family, which is the dilogarithm family with variance function ==== as well as the one with variance function ====. Like the normal and gamma families, they have the remarkable property to be self dual.====The definition of dual measures makes sense also in ==== while the probabilistic interpretation in terms of large deviations is lost. However we consider several cases in ====: the multinomial distribution, the Wishart ones and other quadratic families as classified by Casalis [4].====We proceed as follows: the notion of duality leads us unfortunately to change a bit the tradition about the exponential families. Indeed we will use ==== instead of ==== in order to obtain later more readable formulas. This is explained in Section 2, together with the description of the classical objects attached to an exponential family.====In the preceding lines we have been vague about duality. Section 3 gives proper definitions, explaining what we call ==== dual measure ==== of ==== and showing that some measures have no dual. We explain also what a T exponential family ==== is. It is nothing but an exponential family ==== plus all its translations. Indeed, talking about the dual ==== of an exponential family ==== does not exactly make sense, while the dual ==== of ==== does. Section 3 gives also the link with large deviations.====Section 4 concentrates on the ==== when the variance function ==== is ==== and some parent distributions. It also give details on what we call Lévy measures of types 0, 1 and 2. Of course, large parts of this material are well known from probabilists and statisticians: exponential families, variance functions, Lévy measures, Landau distribution. It was necessary to expose them again for commodity of reading. This section contains crucial calculations for the sequel in Proposition 7.====Section 5 applies the results of Section 4 to the description of the duals of the Morris and the cubic families, with the surprising fact that they exist all with the only exception of the hyperbolic family with variance function ====.====Section 6 describes the self dual dilogarithm distribution ==== on the set ==== of integers defined by ====which, for ====, generates the exponential family with variance function ====. Since the consideration of this exponential family and of a set of parent distributions is not done in the literature, we develop some of their properties, somewhat deviating from the study of duality. For instance, if ==== is the standard Gaussian distribution, then the variance of the exponential family generated by the convolution ==== is ====.====Section 7 considers the ==== case: The multinomial distribution has a very explicit dual expressed in terms of the Landau distribution. The Wishart distribution is self dual as the one dimensional gamma distribution. We prove some negative results, like the fact that the multivariate negative binomial law has no dual.====Section 8 discusses open problems.",Duality for real and multivariate exponential families,https://www.sciencedirect.com/science/article/pii/S0047259X21000890,26 August 2021,2021,Research Article,115.0
"Battey H.S.,Cox D.R.","Department of Mathematics, Imperial College London, 180 Queen’s Gate, London W3 9AH, UK,Nuffield College, University of Oxford, Oxford, OX1 1NF, UK","Received 17 June 2021, Revised 24 June 2021, Accepted 24 June 2021, Available online 20 August 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104810,Cited by (0),A broad review is given of some areas of ,"In many contexts it is appropriate in the various phases of an investigation to consider data as arranged in separate study units on each of which observations are available, possibly of several different kinds. Much of the extensive older literature concentrates implicitly largely on essentially linear relations, for example involving one or more multivariate Gaussian distributions and thus often characterized by correlation or covariance matrices or by functions of those. The issues involved may be divided broadly into internal and external ones, for example into studies of correlation contrasted with studies of regression.====The useful broad distinction, to be interpreted flexibly, between internal and external analyses is that in the former, but not the latter, component variables are all on an equal footing, it being required to find suitable summary variables that in some sense encapsulate most of the variation present, whereas in external formulations there are two or indeed several or many types of variable with interest focused on conditional dependences taken in appropriate sequence. Such chains of dependency aim at a deeper understanding of the data-generating process often implicitly or explicitly evolving in time.====By multivariate analysis, multiple outcomes are usually understood and we focus on that here. For methods of analysis largely emanating from the normal distribution and extending to distributions with ellipsoidal contours of equal density, we defer to excellent accounts such as [3] or [27]. Instead we focus on less standard aspects. We start by outlining some possibilities where any underlying distributions are far from multivariate normal form. In particular linear combinations of components have the potential to be unsatisfactory. Our emphasis here is on binary component variables. A second class of non-standard multivariate analysis problems concerns time or spatial series. In some special contexts, for example when data defined over ever-extending spatial or temporal ranges are involved, effective variances may decay more slowly than inversely as sample size, as found initially in agricultural trials [15], and turbulence [17], [21] and in hydrological studies of the River Nile [19]. The properties and implications of processes with such long-range dependence are reviewed in Section 4.",Some aspects of non-standard multivariate analysis,https://www.sciencedirect.com/science/article/pii/S0047259X21000889,20 August 2021,2021,Research Article,116.0
"Arnold Barry C.,Sarabia José María","Department of Statistics, University of California, Riverside, USA,Dept. de Métodos Cuantitativos, CUNEF Universidad, Spain","Received 16 July 2021, Revised 27 July 2021, Accepted 27 July 2021, Available online 19 August 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104801,Cited by (3),In a ,"The study of the multivariate distributions (MD) and their uses and applications in applied and theoretical statistics and data science, is one of the most active research fields in mathematical statistics. Some relevant references about MD are [25], [34], [37].====In the last years, some models of MDs unknown until now have been developed. An example of these new models are the classes of MDs based on the skew normal distribution and its generalizations, proposed by Azzalini and authors (see [1], [23], [24]).====In this context, MD models based on conditional specification have received special attention. For an MD, it is sometimes easier to visualize conditional distributions of experimental variables rather than the joint distributions. The new classes of distributions based on conditional specification have interesting and unexpected properties, such as multimodality or non-linear regressions. The most representative case is the bivariate distribution with normal conditionals, which will be discussed in Section 4. Another advantage of these models based on conditional specification is that they can be easily extended to other kinds of distribution families, such as the ====-parameter exponential family.====In this paper we summarize some of the main aspects of models with conditional specification. In addition, we highlight the most relevant aspects of these models and establish some challenges that we will face in the coming years. Some previous information about topics related with conditional specification can be found in [7], [8], [12], [13]. The contents of this paper are the following. We begin with the problem of compatibility in conditional densities (Section 2). In Section 3 we study the construction of distributions with conditionals in prescribed families. We study some classical model and some new models based on CS in Section 4 and in Section 5 we consider multivariate extensions. New alternative approaches to CS are presented in Section 6 and some applications are included in Section 7. In Section 8 we discuss some strategies of estimation. Finally, in Section 9 we present some challenges in conditional specification.","Conditional specification of statistical models: Classical models, new developments and challenges",https://www.sciencedirect.com/science/article/pii/S0047259X21000798,19 August 2021,2021,Research Article,117.0
"Fokianos Konstantinos,Fried Roland,Kharin Yuriy,Voloshko Valeriy","Department of Mathematics & Statistics, University of Cyprus, PO BOX 20537, 1678 Nicosia, Cyprus,Department of Statistics, TU Dortmund, 44221 Dortmund, Germany,Research Institute for Applied Problems of Mathematics and Informatics, Belarusian State University, Minsk, Republic of Belarus","Received 27 June 2021, Revised 31 July 2021, Accepted 31 July 2021, Available online 18 August 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104805,Cited by (3),"This work gives an overview of statistical analysis for some models for multivariate discrete-valued (MDV) time series. We present observation-driven models and models based on higher-order ====. Several extensions are highlighted including non-stationarity, network ====, conditional non-linear ====, robust estimation, random fields and spatio-temporal models.","Multivariate time series are sequentially taken observations in time for multiple units. For instance, consider rainfall precipitation in different locations measured every hour or the price of several stocks observed over time. The main investigated problems in this context are modeling, inference and prediction. It is rather common to analyze such data by so-called Vector Autoregressive (VAR) models. They are easily understood by a wide audience, can be fitted by simple statistical procedures and they provide predictions; see the excellent textbooks by Lütkepohl [97] and Tsay [125] for an introduction to VAR models. Though VAR models are well understood from theoretical and methodological points of view and are quite useful for the analysis of continuous-valued data, they become inappropriate when dealing with multivariate time series with integer-valued components (daily number of patient admissions to a hospital, number of transactions of some stocks, absence or presence of a daily characteristic). This contribution aims to give an overview of statistical analysis of multivariate discrete-valued (MDV) time series.====Consider, for instance, the case of multivariate count time series. This is an active research topic as such data can be observed in several applications; see [112] for a medical application, [114] for a financial application and more recently [118] for a marketing application and [96] for an environmental study. A review of current statistical methods for multivariate count time series is given by [43]. In summary, three main approaches are presented: integer autoregressive (INAR) models (see [93], [113], [114], among others), parameter-driven models (see [57], [67], [68], [129], among others) and observation-driven models. This class of models is the topic discussed in Section 2.====Section 3 gives an overview of multivariate discrete-valued time series with emphasis on high-order Markov chains and their properties, construction of parsimonious models, and methods and algorithms for their fitting. Such data occur when the observation space ==== is a discrete (finite or countable) subset of the ====-dimensional Euclidean space and the corresponding models are increasingly more important in applications. For example, genetic data (modeling and analysis of genetic time series with ====
 ==== 4), economics (number of transactions), sociology (modeling of social behavior), medicine (diagnostics in personalized medicine) and information protection (analysis of binary sequences with ====
 ==== 2). Finally, Section 4 lists several possible research directions based on personal research interests.",Statistical analysis of multivariate discrete-valued time series,https://www.sciencedirect.com/science/article/pii/S0047259X2100083X,18 August 2021,2021,Research Article,118.0
"Li Yehua,Qiu Yumou,Xu Yuhang","University of California - Riverside, Riverside, CA 92521, USA,Iowa State University, Ames, IA 50011, USA,Bowling Green State University, Bowling Green, OH 43403, USA","Received 1 August 2021, Revised 6 August 2021, Accepted 7 August 2021, Available online 18 August 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jmva.2021.104806,Cited by (4),"Functional data analysis (FDA), which is a branch of ====. We review some fundamental concepts of FDA, their origins and connections from multivariate analysis, and some of its recent developments, including multi-level functional data analysis, high-dimensional functional regression, and dependent functional data analysis. We also discuss the impact of these new methodology developments on genetics, plant science, wearable device data analysis, image data analysis, and ====. Two real data examples are provided to motivate our discussions.","In celebrating the 50 year anniversary of ==== (JMVA), we provide a review on recent developments of functional data analysis (FDA), an important research area within the scope of the journal. Data considered in FDA include curves and images, which are commonly viewed as infinite-dimensional random vectors in a functional space [47]. Nonparametric smoothing techniques are also routinely used in FDA to recover functions from discrete and noisy measurements. JMVA has a long history of publishing cutting-edge statistical theories and methods in the aforementioned areas. An early JMVA paper by  [23] studied asymptotic properties of principal component analysis for a vector of random functions, including almost sure consistency and asymptotic normality, which paved the road for functional principal component analysis (FPCA), the most widely used technique in FDA. JMVA is also known for publishing high-quality research papers on nonparametric smoothing, such as early work on M-smoothing by [42].====While statistical theory on infinite-dimensional random variables and nonparametric smoothing methods are the pillars for modern FDA, other commonly used statistical methods at the center stage of JMVA have also been extended to functional data, including dimension reduction, penalization, canonical correlation analysis, clustering, classification, and regression. FDA has become such an active research area on JMVA that a keyword search for “functional data” on the journal website results in over 800 papers, including those collected in a recent special issue on functional data analysis in 2019. Some recent JMVA publications on FDA cover topics such as, statistical inference on the mean and covariance functions [13], [106], nonlinear principal component analysis [97], canonical correlation analysis [44], [95], [128], functional linear models [15], [58], functional quantile regression [119] and high-dimensional functional regression [75], just to name a few.====For comprehensive accounts of FDA theory and methods, we refer the readers to the classic textbook by Ramsay and Silverman [87], recent monographs by [46], [52] and review papers by [78], [107]. Hsing and Eubank [47] provided a comprehensive introduction to the theoretical foundation of FDA, especially from the perspective of operator theory. In this article, we review the fundamental theory and methods in FDA from our own relatively narrow expertise in this broad research area, including recent contributions from JMVA. We also introduce a few emerging new areas of FDA which we believe will be research active in the near future and have increasingly greater impacts on many scientific areas. We hope this paper can serve as a tutorial for researchers and domain field practitioners.====To facilitate the discussion, we first introduce two motivating data examples.====Fig. 1(a) shows the growth of 1208 maize plants based on a data set collected by the TERRA-REF program. Plant heights were extracted from raw images from 302 maize genotypes during a period of 85 days. The data are trait trajectories and have a multi-level structure among the observations, with plants nested in genotypes. The experiment also has an irregular sampling design, where on average each plant only has observations on approximately half of the days during the 85-day period. Studies of trait processes and their associations with genetic variations are of great interest to plant scientists [74], [115], and those findings are important to understand the underlying biological mechanism of plant growth and produce better lines in plant breeding. Fig. 1(b) shows the growth of plants from three selected genotypes. Notice that the growth patterns are obviously different across genotypes: plants with genotype G1 grow faster than plants with genotype G3; the growth of plants with genotype G2 has more variation than the ones with genotype G3. These growth curves can be modeled as functional data with a plant-genotype nested hierarchical structure.====We focus on New York–Newark–Jersey City Metropolitan Statistical Area defined by the U.S. Office of Management and Budget, including a population of 20.2 million people by the 2015 U.S. Census. There are 278 neighborhoods identified in the New York metropolitan area, the locations of which are illustrated in Fig. 2(a) obtained by Google Maps API. The definition of the neighborhoods can be found at ====. The data consist of monthly records of median home price-to-rent ratios for each neighborhood from October 2010 to September 2017. The longitudinal trajectories of price-rent ratio curves are shown in Fig. 2(b), which we model as functional data. A similar data set was analyzed by [123].====The rest of the paper is organized as follows. In Section 2, we introduce the foundations of FDA and describe its connection to multivariate analysis. In Sections 3 to 5, we summarize the state-of-the-art methods and theory for multi-level functional data analysis, functional regression with high-dimensional covariates, and dependent functional data, respectively. Some concluding remarks are provided in Section 6.","From multivariate to functional data analysis: Fundamentals, recent developments, and emerging areas",https://www.sciencedirect.com/science/article/pii/S0047259X21000841,18 August 2021,2021,Research Article,119.0
"Horová Ivana,Hušková Marie,Vieu Philippe","Grupo de investigación MODES, Departamento de Matemáticas,CITIC e ITMATI, Universidade da Coruña, A Coruña, Spain,Department of Mathematics and Statistics, Masaryk University, Brno, Czech Republic,Department of Statistics, Charles University, Prague, Czech Republic,Institut de Mathématiques de Toulouse, Université Paul-Sabatier, Toulouse, France","Available online 10 November 2021, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jmva.2021.104908,Cited by (8),None,None,Special Issue on Functional Data Analysis and related fields,https://www.sciencedirect.com/science/article/pii/S0047259X21001809,10 November 2021,2021,Research Article,127.0
"Tsukuda Koji,Matsuura Shun","Faculty of Mathematics, Kyushu University, 744 Motooka, Nishi-ku, Fukuoka-shi, Fukuoka 819-0395, Japan,Faculty of Science and Technology, Keio University, 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Kanagawa 223-8522, Japan","Received 21 July 2020, Revised 24 August 2021, Accepted 24 August 2021, Available online 3 September 2021, Version of Record 17 September 2021.",https://doi.org/10.1016/j.jmva.2021.104822,Cited by (0),This paper describes the derivation of a new property of the ,"This paper investigates the asymptotic normality of the trace of products of four independent Wishart matrices in a high-dimensional setting, and proposes a statistical test procedure for the common principal components (CPC) hypothesis on two covariance matrices. Here, the CPC hypothesis, first considered by Flury [8] in 1984, means several covariance matrices can be simultaneously diagonalized.====A classical setting in multivariate analysis is that population distributions are normal and that the number of observed variables is much less than the number of individuals in a sample. In 1928, John Wishart derived the probability density function of sample covariance matrices calculated from a sample of size ==== from the ====-dimensional normal population (====). It determines the so-called Wishart distribution, the distribution of a scatter matrix ==== calculated from i.i.d. (independent and identically distributed) ====-dimensional zero-mean normal vectors ====, where ==== and ==== denotes the transpose. A number of studies have investigated its asymptotic properties under the traditional multivariate analysis setting: ==== with fixed ====. As the observed variables have increased with the development of information technology, however, multivariate statistical methods have been developed to deal with this situation. In particular, when few variables are observed, the likelihood ratio test is quite useful for testing hypotheses about population covariance matrices. On the other hand, when more variables are observed than the number of individuals in the samples (the so-called high-dimensional setting), the likelihood ratio test is unavailable in many cases because scatter matrices are not of full-rank. In one-sample testing problems such as “the population covariance matrix is an identity matrix”, “the covariance population matrix is spherical”, and “the covariance matrix is diagonal”, alternative test procedures that cleverly use the trace of some functions of a scatter matrix have been proposed. Such procedures are considered to be effective in high-dimensional settings; see, e.g., Chen et al. [6], Hyodo et al. [16], [17], Ishii et al. [19], Srivastava [24], Srivastava et al. [26], Yamada et al. [30], and Yata et al. [31].====In two-sample testing problems for covariance matrices, hypotheses such as “two covariance population matrices are identical”, “two population covariance matrices are proportional”, and “two population covariance matrices can be simultaneously diagonalized” (the CPC hypothesis) have been considered. These three hypotheses are typical in two-sample problems in multivariate analysis. Indeed, they correspond to Flury’s hierarchical model, where the likelihood ratio is used for testing and model selection; see, e.g., Flury [10] and Phillips and Arnold [22]. As the likelihood ratio test for testing the above three hypotheses is unavailable in a high-dimensional setting, alternative test procedures have been proposed for the former two hypotheses (equality and proportionality); see, e.g., Aoshima and Yata [1], Ishii et al. [18], Li and Chen [20], Liu et al. [21], Schott [23], Srivastava and Yanagihara [25], Srivastava et al. [26], Tsukuda and Matsuura [28] and Xu et al. [29]. In contrast, for testing the CPC hypothesis, Flury [8] and subsequent studies, such as those of Boente et al. [4], Boik [5] and Hallin et al. [13], [14] have not considered high-dimensional settings. Therefore, in this paper, we propose a test procedure for the CPC hypothesis in a high-dimensional setting. Approaches based on a statistic that use the trace of some functions of scatter matrices are considered to be promising for two-sample high-dimensional problems; see, e.g., Aoshima and Yata [1], Schott [23], Srivastava and Yanagihara [25], Srivastava et al. [26], and Sugiyama et al. [27]. Hence, we take a similar approach with the trace operator. High-dimensional data have been collected in various fields such as evolutionary biology; see, e.g., Collyer et al. [7]. In evolutionary biology, CPC analysis is widespread (Houle et al. [15]), and testing the CPC hypothesis is crucial for conducting CPC analysis. Thus, this paper aims to contribute to making CPC analysis more widely available.====The remainder of this paper is organized as follows. In Section 2, we present the main result with an outline of its proof. An application of the limit theorem for testing the CPC hypothesis is described in Section 3. Concluding remarks are provided in Section 4.",Limit theorem associated with Wishart matrices with application to hypothesis testing for common principal components,https://www.sciencedirect.com/science/article/pii/S0047259X21001007,3 September 2021,2021,Research Article,129.0
"Samadi S. Yaser,Billard Lynne","School of Mathematical and Statistical Sciences, Southern Illinois University, Carbondale, IL, United States of America,Department of Statistics, University of Georgia, Athens, GA, United States of America","Received 4 September 2020, Revised 11 August 2021, Accepted 11 August 2021, Available online 3 September 2021, Version of Record 17 September 2021.",https://doi.org/10.1016/j.jmva.2021.104817,Cited by (1), of these estimators are obtained. A simulation study shows that the new estimators perform considerably better than those obtained from the set-valued estimators of Wang et al. (2016).,"While many series record a single value for each time point, many other series record the observations as intervals. This is particularly so with financial data, where, e.g., stock market prices record the high and low prices for each market day (or, markets record the opening and closing values, or ...). A lot of meteorological data are recorded effectively as intervals with e.g., minimum and maximum daily (or weekly, or monthly) temperatures. There are countless examples. These are examples of series of dependent observations; an important class is modeled as autoregressive series. Other sets of interval time series data arise from the aggregation of larger data sets. For example, many examples occur from pixel-type observations, such as electroencephalogram (EEG) measurements or fMRI observations. Again, there are countless examples in today’s era of large data.====Interval-valued observations form one class of symbolic valued data (sometimes referred to simply as interval observations). In general, symbolic data can be lists, intervals, histograms, and so on. Observations are hypercubes or Cartesian products of distributions in ====-dimensional space ====, in contrast to the classical point observations in ====. Details of their description, how they arise, and some methodologies for symbolic data, can be found in Bock and Diday [11] , Billard and Diday [9], [10], Diday and Noirhomme-Fraiture [17], and Noirhomme-Fraiture and Brito [31], with a non-technical introduction in Billard [8].====Classical point observations are realizations of some underlying random variable. Likewise, symbolic observations are also realizations of those same (standard, so-to-speak) random variables, the difference being that realizations are symbolic-valued. That is, internal values within an interval are aggregations of these classical observations; so, e.g., the min–max daily stock price represents just that, the lowest and highest of all the aggregated point-valued prices recorded that day. Thus, an interval value ==== is quite different from an interpretation that says there are but two values for the day, 5 and 15. This latter interpretation ignores all the other values for the day. It is clearly important that analyses incorporate all the information in the intervals and not just the end points. Further, just as each observation follows a distribution (such as a normal distribution) with a point-valued ==== as mean, so too is the mean calculated from the intervals (i.e., from the aggregations) point-valued. Likewise, similar concepts prevail for all parameters and all descriptive statistics. That is, the parameters of a distribution of the random variable, such as ====, are still points, e.g., ==== and ====. This feature is especially evident when calculating descriptive statistics, such as the sample mean or sample variance of interval observations as in Bertrand and Goupil [6]. The output sample mean or sample variance of intervals is a point, and is not an interval such as might be the case when interval arithmetic is employed. Indeed, for symbolic data, standard classical arithmetic is in force (i.e., we do not use interval or histogram or related arithmetics which are in a different domain from ‘standard’ statistics).====Similarly, in the context of interval-valued time series, the sample estimators of the model parameters are points (i.e., they are not intervals). In that same vein, aggregated observations are still distributed according to the underlying distribution of each aggregated observation, e.g., normally distributed; however, it is assumed that those normally distributed observations are uniformly spread across the interval. The same applies for the sub-intervals of histogram-valued data, akin to the “group” data histogram problems of elementary applied statistics courses. This uniform spread assumption exists in almost all symbolic data analytic procedures to date; however, other forms of spread within the intervals could be possible.====Most results for symbolic data developed in the literature so far transform the interval ==== (say) into the interval center (====) and interval range (====); or equivalently into the two end points (====). This includes the work of Maia et al. [26], [27], Maia and de Carvalho [25], García-Ascanio and Maté [18], Arroyo et al. [3], [4], Teles and Brito [33], and Wang et al. [35], among others. There are statistical difficulties with this approach. An expanded discussion along with a wider coverage of the literature using these transformations can be found in Supplement S1. In a different direction, there are some studies focused on the use of interval arithmetic techniques, such as Han et al. [20] and Sun et al. [32]; however, these are outside the domain of the standard arithmetic numerics of symbolic data and so are outside the purview of the present work. Several likelihood-based approaches have been proposed for symbolic data (Le-Rademacher and Billard [23], Brito and Silva [12], Beranger et al. [5], Zhang et al. [37]). In particular, Zhang et al. [37] and Whitaker et al. [36] developed a composite likelihood approach for symbolic interval-valued and histogram-valued data, respectively. In the present paper, we propose a new composite likelihood procedure for symbolic interval-valued time series data.====For time series data, there is dependency between all realizations within an interval at time ==== and all those within the interval at time ====, i.e., between ==== and ====, and not just between the centers (==== and ====) nor the ranges (==== and ====). Therefore, all approaches available in the literature that use the center-range are inappropriate surrogates for analyzing symbolic interval-valued time series data. There is a loss of critical information. These internal variations and the time dependencies of the interval-valued time series data over time make it very difficult and challenging albeit interesting. We propose a novel, viable methodology to analyze the complexity and capabilities of interval-valued time series data.====Accordingly, in Section 2, lagged sample autocovariance functions and hence autocorrelation functions, based on the method of moments, are introduced for the interval-valued autoregressive model of order ==== (IAR(====)). Then, in Section 3, under various distributional situations, maximum likelihood estimators of the parameters are derived. In particular, the foundational groundwork for the derivation is laid in Section 3.1, where the model’s white noise error terms and hence the observations are assumed to be normally distributed, and the spread of the internal values of the interval are governed by some internal distribution. Difficulties arise because of the unique nature of dependencies between successive intervals that occur with time series data. Therefore, in Section 3.2, composite and pairwise likelihood functions are introduced for interval time series observations. In Section 3.3, theoretical results for an IAP(====) process are obtained. Thence, the impact of specific internal distributions is discussed in Section 4. Asymptotic properties are developed in Section 5. Simulated data sets study these estimators in Section 6, and real data are presented in Section 7; these results are compared with those obtained in Wang et al. [35]. All the proofs are presented in the Appendix A.",Analysis of dependent data aggregated into intervals,https://www.sciencedirect.com/science/article/pii/S0047259X21000956,3 September 2021,2021,Research Article,130.0
Quessy Jean-François,"Département de mathématiques et d’informatique, Université du Québec à Trois-Rivières, Trois-Rivières, Canada","Received 11 May 2021, Revised 10 August 2021, Accepted 11 August 2021, Available online 2 September 2021, Version of Record 10 September 2021.",https://doi.org/10.1016/j.jmva.2021.104815,Cited by (0),"This article extends to more than two vectors the popular moment inequality of Szekely and Rizzo and adapts it to test hypotheses about the ==== of a multivariate population. The framework that is adopted is very general, as it allows one to test various kinds of hypotheses about the ==== of random vectors like symmetry, ","A whole new branch of the statistical sciences has emerged during the 2000s around inferential tools for dependence structures, ====, copulas, following the pioneering papers of Genest and Rivest [11], Genest et al. [8], Frees and Valdez [7]. Copulas form a subspace of the class of all multivariate distributions, namely those whose marginals are uniform on ====. From a probabilistic point-of-view, copulas are the dependence structures of random vectors. Specifically, for any ====-variate random vector ====, a celebrated Theorem of Sklar [22] ensures the existence of a function ==== such that for all ====, ====If the marginal distributions are continuous, then ==== is unique and contains all the information about the dependence between the components of ====.====Recently, tests of shape hypotheses for dependence structures have retain attention. For example, tests of diagonal symmetry have been proposed by Genest et al. [10] and Quessy and Bahraoui [18] (see also Beare and Seo [3]), and have been extended to multivariate copula exchangeability by Harder and Stadtmüller [12]. Tests of radial symmetry have been investigated by Genest and Nešlehová [9] when ==== and by Bahraoui and Quessy [2] for the general case ====. Another kind of shape hypothesis is the equality of copulas, treated by Rémillard and Scaillet [19].====As observed by Quessy [17], all the above-mentioned hypotheses, and possibly many more, can be united inside the class of homogeneity hypotheses. Specifically, for a random variable ==== with support ==== such that ====, a homogeneity hypothesis about a copula ==== is one that can be stated as ====where ==== are such that ==== is a ====-variate copula for each ====. The formulation in (1) includes:====(i) Radial symmetry. A copula ==== is radially symmetric if ==== and ==== have the same distribution. It thus corresponds to ==== and ====.====(ii) Equality of copulas. Consider the subsets ==== of ==== of cardinality ====. The equality of the copulas ====, where ==== is the copula of ====, then corresponds to ====.====(iii) Pairwise exchangeability. A ====-variate copula is pairwise exchangeable if its bivariate marginal copulas are identical; hence, ==== and ====, where ====.====(iv) Bivariate symmetry and exchangeability. In the bivariate case, this hypothesis states that ==== for all ====, hence corresponds to ==== and ====. Its ====-dimensional extension states that ==== for all ==== and all permutations ==== of ====, so that ====.====This paper develops a statistical methodology to test any hypothesis of the form ==== around the Szekely–Rizzo inequality that first appeared in Székely and Mori [23] and latter popularized by Székely and Rizzo [25] and Székely and Rizzo [26], to name only a few. Specifically, for two pairs of random vectors ==== and ==== that are independent and identically distributed in ====, it establishes that for ====, ====where ==== is the Euclidean norm. The usefulness of inequality (2) for inference is that equality occurs if and only if ==== in distribution. This feature has been used by Székely and Rizzo [25] to test the normality assumption and by Niu et al. [16] and Chen et al. [5] for symmetry. See also Böttcher et al. [4], where a close link that exists between (2) and distance covariance is exploited to test for independence between random vectors.====Despite its many successes, inequality (2) has never been exploited to test about copula structures. It may appear surprising, since only a slight adaptation is necessary. Specifically, for a random vector ==== with continuous marginals ==== and unique copula ====, testing at the level of the copula means that the hypotheses to be confronted may be written in terms of the uniformized vector ====, where ==== for each ====. A copula version of (2) is then simply ====where ==== and ==== are independent and identically distributed on ====; equality holds if and only if ==== in distribution.====One reason that may explain why an inequality of the form (3) has never been considered yet is that when the marginal distributions ==== are unknown, ==== and ==== are unobservable. This apparently mild constraint brings many technical issues related to the use of empirically uniformized data (====, the ranks) instead of the raw observations. Indeed, while the asymptotic theory for statistics that arise from (2) is rather standard, dealing with so-called pseudo-observations in such a context is not straightforward, as will be seen in this paper.====The remainder of the paper is as follows. Section 2 derives a ====-populations version of inequality (2) from which a general test criterion is deduced. Section 3 describes a class of statistics to test hypotheses of the form ==== and derive their large-sample distributions both under the null and alternative hypotheses; a way to replicate the asymptotic behavior of the test statistics under ==== is proposed as well. Section 4 investigates the sampling properties of the new tests and offers comparisons with competing procedures. Section 5 concludes with some remarks and provides a short illustration on multivariate data. The technical details are postponed to two appendices and all the necessary code is freely available at ====.",A Szekely–Rizzo inequality for testing general copula homogeneity hypotheses,https://www.sciencedirect.com/science/article/pii/S0047259X21000932,2 September 2021,2021,Research Article,131.0
"Park Beomjin,Park Changyi","Department of Statistics, University of Seoul, Seoul 130-743, Republic of Korea","Received 18 February 2021, Revised 22 July 2021, Accepted 27 July 2021, Available online 1 September 2021, Version of Record 14 September 2021.",https://doi.org/10.1016/j.jmva.2021.104800,Cited by (0),Variable selection is important in statistical learning because it can increase ==== of the proposed method. Also we illustrate that our method can accurately select relevant variables and yield interpretable models on both simulated and real data sets.,"Support vector machines (SVMs) are one of the most widely used tools for classification in statistical learning, and have shown powerful performances in many applications [9], [37]. One of the advantages of SVMs is that they allow linear as well as nonlinear classifications via a mapping from the original input space to high-dimensional or infinite-dimensional space induced by a kernel function. Moreover, their statistical properties such as universal consistency [42] and asymptotic approximation to optimal Bayes rule [25] have been established well in the literature. However, SVMs have a few drawbacks. One drawback is that they are black-box models, which means that discerning the relationship between the covariates and the response is difficult. The other is that the presence of noisy redundant variables can degrade their performances [17]. Variable selection, also known as feature selection, is an alternative to remedy these drawbacks. For data with a large number of variables, variable selection can improve the accuracy in prediction by preventing overfitting and reducing the variance of predicted values. By eliminating redundant variables through variable selection, we can have an estimated function approximating the true underlying function closely and a better understanding of the relation between the covariates and the response.====Since the literature on variable selection is huge, let us limit our discussions to SVMs. Regularization methods such as ==== [3], [62], elastic net [50], and smoothly clipped absolute deviation (SCAD) [57] penalties have been proposed. In particular, the SCAD-regularization and its oracle property have been established in [32]. These regularization methods can alleviate the drawbacks mentioned above. However, these regularization methods can deal with only linear classifications. There are various selection methods for SVMs allowing for nonlinearity of the kernel function without specifying the model form, such as linearity. The recursive feature elimination (RFE) in [15], which selects variables by repeatedly eliminating variables through ranking criteria and is not limited to the linear kernel, is such an example. Multiple kernel learning (MKL) [2], [20] is also a popular variable selection method. Typically, SVMs select a kernel via data-driven methods such as cross-validation (CV). Since MKL combines multiple kernel functions, the problem of kernel selection can be avoided to some extent. Another variable selection approach is the component smoothing and selection operator (COSSO) proposed in [26]. [56] demonstrates that the COSSO SVM can select the relevant variables in binary classifications. These methods estimate the regression and classification functions via functional analysis of variance (ANOVA) decomposition and select the relevant variables with the penalty on the sum of functional component norms.====In recent years, derivative-based variable selection has been studied. The basic idea of the derivative-based variable selection is to measure the importance of a variable relative to the response. If a variable is informative to the response, the partial derivative of the classification function with respect to the variable will be substantially different from zero. [36] introduce a regularization scheme with partial derivatives in nonparametric regression and [54] propose a model-free variable selection method for nonparametric regression problems in reproducing kernel Hilbert spaces (RKHS). These studies attempt to estimate the gradient function based on the partial derivative and show the selection consistency for the case when the gradient functions of informative variables are sufficiently different from zero. [18] takes another direction that estimate the partial derivative from the estimated function rather than directly estimating it, which reduces the computational cost. So this direction seems to be better suited for high-dimensional data [18].====The problem of classifying data into one of three or more classes is common in practice. Since SVMs are originally designed for binary classifications, their multicategory extension is nontrivial. In the literature, there are two main approaches to tackle the multicategory classification. One approach is to solve the multicategory problem by constructing a series of binary classifiers: one-vs-one (OVO) constructs classifiers for each pair of classes, and one-vs-all (OVA) constructs the classifiers by discriminating each class from the rest. However, this approach have several disadvantages such as increases in computational cost and potential variance. More seriously, the target of estimation may not be the Bayes rule without a dominating class [23]. Another approach considers a multicategory loss function designed to estimate classification functions simultaneously. We adopt the angle-based large-margin framework. The angle-based framework estimates the classification function through a simplex coding without the sum-to-zero constraint and thus is computationally more efficient than the MSVM (multicategory support vector machine) in [23] with the sum-to-zero constraint.====Now let us review variable selection methods for multicategory SVMs in the literature. One is to extend the binary SVM-RFE to multiclass problems via the OVA approach. Some studies [8], [35] have employed this method for gene selection in multicategory classifications. As discussed in [61], the selected variables from a binary selection may not guarantee the performances in other binary selections. To tackle with this problem, [61] consider the SVM-RFE based on several multicategory approaches. [63] introduce a MKL adopting the multicategory loss function and [22] propose a variable selection method for the MSVM in [23] based on the COSSO.====In this study, we extend the derivative-based variable selection to multicategory SVMs. Our proposed method is driven by the angle-based large-margin framework, and we show its consistency in variable selection under certain conditions. The paper is organized as follows. In Section 2, we first review the derivative-based variable selection method for SVMs in binary case. Then we extend the selection method to multicategory case and discuss the selection of interaction effects as well. In Section 3, we derive asymptotic properties of the proposed method under a few regularity conditions. Section 4 discusses the strategy for the hyper-parameter selection in our method. Simulation study and real data analysis is provided in Sections 5 Numerical experiments, 6 Real data analysis, respectively. Finally, technical proofs are collected in Appendix A.",Kernel variable selection for multicategory support vector machines,https://www.sciencedirect.com/science/article/pii/S0047259X21000786,1 September 2021,2021,Research Article,132.0
"Gijbels Irène,Omelka Marek,Veraverbeke Noël","Department of Mathematics and Leuven Statistics Research Center (LStat), KU Leuven, Celestijnenlaan 200B, Box 2400, B-3001 Leuven (Heverlee), Belgium,Department of Probability and Statistics, Faculty of Mathematics and Physics, Charles University, Sokolovská 83, 186 75 Praha 8, Czech Republic,Center for Statistics, Hasselt University, Agoralaan-building D, B-3590 Diepenbeek, Belgium,Unit for BMI, North-West University, Potchefstroom, South Africa","Received 7 August 2020, Revised 23 July 2021, Accepted 23 July 2021, Available online 26 August 2021, Version of Record 10 September 2021.",https://doi.org/10.1016/j.jmva.2021.104804,Cited by (0),Conditional ==== describe the conditional dependence and the influence that ,"The dependence structure between two random variables ==== and ==== given a vector of covariates ==== is fully described by a conditional copula function ====, a function defined on the unit square. In a semiparametric setting one conveniently works with the form ====, i.e. the influence from the covariate vector only comes in via the parameter of the copula family that depends on ====.====This semiparametric setting then immediately raises the question what is the precise functional form of ====. In a goodness-of-fit approach one wants to test the null hypothesis that ==== takes on some specific parametric form, say ====, with ==== a specified function, but ==== a possibly unknown parameter. Several interesting issues arise. For example, the covariate vector may have a linear or quadratic influence, or some components of ==== have a linear influence and others have a non-linear influence; or some components of ==== have no influence whereas others do have an influence. A very special case is when ==== has no influence at all on the conditional dependence structure. In that setting the interest is in testing whether ==== is a constant. In a copula terminology this would mean that the simplifying assumption holds, i.e. the dependence structure between ==== and ==== does not alter with the realized value of the covariate vector ====.====A generalized likelihood ratio test for testing for a specific form of the function ==== in the above semiparametric setting was proposed by [3]. Such a test is likelihood-based, and hence the starting point is that the copula family ==== is well specified. In [11] the authors took a very different approach that relies on extending the use of the score test. Two important advantages of the latter testing approach is that it does not require the choice of smoothing parameters, and that it is rather insensitive, and hence robust, to the correct specification of the underlying true copula family. A disadvantage however of the approach in [11] is that it also requires some form for ==== under the alternative hypothesis. More precisely the parameter function ==== is assumed to be parametrized as ==== and the considered null hypothesis coincides with ==== being the null vector. Although simulations show that under some miss-specification of the form of ====, the score type test still performs well, there can be a loss in power.====In this paper we yet take another road and look into omnibus type of tests for the specification type of testing problem that we study. Our approach also starts from looking at the score of the log-likelihood in the semiparametric setting, but a main difference with the approach in [11] is that we develop an omnibus type of test that does not require a specific format for the function ==== under the alternative hypothesis. There are many papers in the literature that deal with constructing omnibus tests. For example, in specification tests for regression models [18], [19], [23] developed omnibus test statistics. Among others, see also [15] for a similar approach in nonparametric significance testing, [5], [20] in specification tests for quantile regression, and [21] in nonparametric checks for single-index assumptions. The approach we propose here still shares, with [11], the advantage of being rather insensitive to a correct specification of the copula family ====. The price to pay for having an omnibus test is similar to the one reported in [4]: compared to [11] there might be a loss in power.====In Section 2 we present the semiparametric framework and the testing problem. The discussion of the test statistic together with the main theoretical result is provided in Section 3. The main steps of the proof of the theoretical result are deferred to the Appendix, and further detailed steps are given in the Supplementary Material. Sections 4 Simulation study, 5 Real data application contain respectively a simulation study and an application to real data analysis.",Omnibus test for covariate effects in conditional copula models,https://www.sciencedirect.com/science/article/pii/S0047259X21000828,26 August 2021,2021,Research Article,133.0
"Betken Annika,Dehling Herold,Nüßgen Ines,Schnurr Alexander","Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, 7500 AE Enschede, The Netherlands,Faculty of Mathematics, Ruhr-University Bochum, 44780 Bochum, Germany,Department of Mathematics, Siegen University, 57072 Siegen, Germany","Received 3 December 2020, Revised 27 July 2021, Accepted 27 July 2021, Available online 21 August 2021, Version of Record 8 September 2021.",https://doi.org/10.1016/j.jmva.2021.104798,Cited by (3),"In this article, we show that the recently introduced ordinal pattern dependence fits into the axiomatic framework of general multivariate dependence measures, i.e., measures of dependence between two multivariate random objects. Furthermore, we consider multivariate generalizations of established univariate dependence measures like Kendall’s ====, Spearman’s ==== proves to take the dynamical dependence of random vectors stemming from multidimensional time series into account. Consequently, the article focuses on a comparison of ordinal pattern dependence and multivariate Kendall’s ==== are established under the assumption of near-epoch dependent data-generating time series. We analyze how ordinal pattern dependence compares to multivariate Kendall’s ==== and ordinal pattern dependence.","Recently, various attempts have been made to generalize classical dependence measures for one-dimensional random variables (like Pearson’s correlation coefficient, Kendall’s ====, Spearman’s ====) to a multivariate framework. The aim of these is to describe the degree of dependence between two random vectors with a single number. This has to be separated from the branch of research where the dependence ==== one vector is described by a single number (see [13], [14] and the references therein).====Roughly speaking, one can separate the following two approaches: (I) In a first step, the main properties which classical dependence measures between two random variables display, are extracted. In a second step, multivariate analogues of the dependence measures which satisfy canonical generalizations of these properties in a multivariate framework, are defined. However, often a canonical interpretation of these measures is not at hand. (II) Given two time series, one wants to describe their co-movement.====Along these lines, the definition of ordinal pattern dependence (see [15]) follows the latter approach. Originally, axiomatic systems are disregarded by the notion of ordinal pattern dependence, which is naturally interpreted as the degree of co-monotonic behavior of two time series. Against the background of this approach, limit theorems have been proved in the time series setting (see [16] for the SRD case and [11] for the LRD case).====Both approaches in defining multivariate dependence measures have proved to be useful, but by now, they have been analyzed separately. In the present paper, we close the gap between the two. To this end, we recall the definition of ordinal pattern dependence in the subsequent section and show that it is a multivariate dependence measure according to the definition introduced in [7]. In Section 3, we establish consistency and asymptotic normality for estimators of ordinal pattern dependence in the framework of i.i.d. random vectors. Section 4 deals with multivariate extensions of well-established univariate dependence measures. It turns out that multivariate Kendall’s ==== is the only one among these that captures the dynamical dependence between random vectors. Starting with approach (I), we prove limit theorems for an estimator of multivariate Kendall’s ==== in the time series context. In the last section, the different measures are compared from a theoretical point-of-view as well as by simulation studies.",Ordinal pattern dependence as a multivariate dependence measure,https://www.sciencedirect.com/science/article/pii/S0047259X21000762,21 August 2021,2021,Research Article,134.0
Yin Yanqing,"School of Mathematics and Statistics, Jiangsu Normal University, Xuzhou, 221116, PR China","Received 6 September 2020, Revised 31 May 2021, Accepted 27 July 2021, Available online 21 August 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.jmva.2021.104797,Cited by (0),"In this paper, we examine the problem of testing for high-dimensional mean vector under missing observations. By assuming missing at random, a test statistic is proposed and the ==== is established pursuant to the framework that the sample size and the dimension of population both tend to infinity. Simulation studies show that the test procedure performs well in various situations.","Test for mean vector of a multivariate population is a classical problem in multivariate statistical analysis. Let ==== where ==== for ==== be ==== observations of a population ====, whose mean vector is ==== and covariance matrix is ====, respectively. The interest is to test ====This problem arises in many aspects of modern science such as genomics, economics and biostatistics and there have been extensive study in this topic. Proposed by Hotelling [5] as the extension of ==== statistics in multivariate analysis, the Hotelling’s ==== statistics has been widely applied for eight decades due to its good properties such as it is uniformly the most powerful among the affine invariant test. However, that test is facing great challenging in high-dimensional situation since the Hotelling’s ==== statistics is not well-defined as the inverse of sample covariance matrix does not exist when the dimension ==== is larger than the sample size ====. What is more, it has been illustrated in [1] that the power of the classical Hotelling’s ==== test can be adversely affected even when ==== is less than but closes to ====. In the past two decades, a great deal of effort has been devoted to the study of high-dimensional statistics to meet the rapid development of data science. Among those, there has been tremendous interest in developing the test for mean vector that can be applied in analyzing high-dimensional or even ultra high-dimensional data sets. See for instance, Bai and Saranadasa [1] and Pan and Zhou [11] for the case where p and n both tend to infinity with the same rate and Srivastava and Du [14], Srivastava [13] and Srivastava et al. [15] for the case when ==== and ==== could tend to infinity with different rates. Motivated by Bai and Saranadasa [1] and Chen and Qin [3] proposed a test that is applicable without restriction on the divergence rate of ==== and ==== by removing some terms out from their statistic. Then in [16], the authors propose a non-parametric test for mean vector based on spatial signs of the observations. For some further developments in tests for high dimensional mean vector and covariance matrix, we refer the readers to Zheng et al. [18], Cai et al. [2], Li and Chen [10] and Hu and Bai [6].====In data analysis, missing observations is one of the most pervasive situations and occurs for various of reasons. However, the above tests are all proposed under the assumption that the data are completely observed and thus they cannot directly apply to the problem of testing mean vector for incomplete data. Not much attempt has been undertaken to develop the statistical theorem under this situation due to the mathematical difficulties and this problem is at present far from being solved especially under high-dimension. On account of paper related to this topic under classical “large ====, small ====” framework, we refer the readers to Krishnamoorthy and Pannala [9], Jian and Krishnamoorthy [8], Yu et al. [17] and Jamshidian and Schott [7] and reference therein. In this paper, for the first step in investigating the influence of missing observation in high dimension, we will consider the test for mean vector of high-dimensional or ultrahigh-dimensional population when the observations are missing at random. We propose a test statistic and establish the asymptotic normality of it under the assumption that both the sample size and the dimension of population tend to infinity.====The present paper is built up as following. In Section 2, we firstly introduce the model assumptions, then show the proposed testing procedure as well as the asymptotic properties of the test statistic. We devote Section 3 to some simulation studies to investigate the performance of our proposed test. Then Section 4 deals with the technical details of the proofs. Some further discussions are made in the last section.",Test for high-dimensional mean vector under missing observations,https://www.sciencedirect.com/science/article/pii/S0047259X21000750,21 August 2021,2021,Research Article,135.0
"Imoto Tomoaki,Abe Toshihiro","School of Management and Information, University of Shizuoka, 52-1 Yada, Suruga-ku, Shizuoka 422-8526, Japan,Faculty of Economics, Hosei University, 4342 Aihara, Machida, Tokyo 194-0298, Japan","Received 22 April 2020, Revised 31 July 2021, Accepted 31 July 2021, Available online 20 August 2021, Version of Record 3 September 2021.",https://doi.org/10.1016/j.jmva.2021.104799,Cited by (1),"In this paper, we propose a new method for constructing a toroidal distribution from independent circular distributions by specifying marginal distributions. The constructed ","In diverse scientific fields, a data sample is often represented as a point in the circumference of a unit circle. Typical examples are wind direction and event time measured on a 24-h clock. Such data are called circular data and should be modeled by a distribution defined on the circle, called circular distribution [8], [16].====In many cases, two or more circular variables are observed for finding relations such as wind directions in the morning and afternoon [9], peak times for two successive measurements of blood pressure [4], and positions of the orthologs between paired circular genomes [19]. Bivariate circular distribution, called toroidal distribution, is used to analyze such data. Several methods for constructing a toroidal distribution have been considered: maximum entropy [15], trivariate reduction [20], wrapping of bivariate distribution on the torus [3], [7], and specifying conditional distributions [18]. As a method of specifying marginal distributions, Wehrly and Johnson [21] considered a family of distributions; ====and Jones et al. [10] considered the circular analogue of copulas, called circulas; ====where ==== and ====, ====, are the circular density and its corresponding distribution function, respectively, and ==== and ==== are the univariate and bivariate circular densities, respectively.====In this paper, we propose a simple construction of a toroidal distribution from independent circular distributions as a method of specifying marginal distributions. Assume that the mean directions of the circular densities ==== and ==== are zero. Then, we can define the toroidal distribution with density ====for ==== and ====. Since the sine moments of the densities ==== and ==== are zero from the assumption, the function (3) is proven to be a probability density function. The parameters ==== and ==== control the locations, and ==== controls the dependence between two variables. The two variables are independent if and only if ====. This distribution is a special case of circulas when ==== in (2) and an extension of the family of sine-skewed circular distributions by Abe and Pewsey [2] in that the conditional distributions belong to the family. A similar extension for a cylindrical distribution is found in Abe and Ley [1]. We refer to the distribution obtained from (3) as a sine-correlated toroidal distribution and denote it by SCTD==== or SCTD, and hereafter, we assume that the random vector ==== is distributed as SCTD====. The merits of this construction are the lack of effect on the normalizing constant associated with density (3) and the nonnecessity of the distribution functions. These confer some desirable properties and easy calculations for statistical inference. Furthermore, when maximum likelihood estimation (MLE) is employed, the parameter ==== is orthogonal to the other parameters when the circular densities ==== and ==== are symmetric.====The remainder of this paper is structured as follows: The properties of SCTD====, including the marginal and conditional distributions, are listed in Section 2. To find the behavior of the SCTD, Section 3 presents the trigonometric moments and correlation measure. Section 4 considers the estimations of parameters by the method of moments and MLE. The tests of independence between two circular variables are also considered in this section. Section 5 describes two illustrative applications for comparing genome structures between paired bacteria. The properties and advantages of the proposed construction are summarized in Section 6 with some considerations. Additional supporting information may be found in the Appendix and Supplementary Material.",Simple construction of a toroidal distribution from independent circular distributions,https://www.sciencedirect.com/science/article/pii/S0047259X21000774,20 August 2021,2021,Research Article,136.0
"Faugeras Olivier P.,Rüschendorf Ludger","Université Toulouse 1 Capitole, Toulouse School of Economics, 1 Esplanade de l’université, 31080 Toulouse Cedex 06, France,Universität Freiburg, Mathematisches Institut, Zermelostr. 1, 79104 Freiburg, Germany","Received 19 February 2020, Revised 8 August 2021, Accepted 8 August 2021, Available online 18 August 2021, Version of Record 2 September 2021.",https://doi.org/10.1016/j.jmva.2021.104802,Cited by (0), and provide a valuable tool for their analysis.,None,"Functional, randomized and smoothed multivariate quantile regions",https://www.sciencedirect.com/science/article/pii/S0047259X21000804,18 August 2021,2021,Research Article,137.0
"Hobbad Lahoucine,Marchand Éric,Ouassou Idir","École Nationale des Sciences Appliquées-Marrakech, Université Cadi Ayyad, Morocco,Département de mathématiques, Université de Sherbrooke, Sherbrooke, Québec, Canada J1K 2R1","Received 25 February 2021, Revised 27 July 2021, Accepted 27 July 2021, Available online 18 August 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.jmva.2021.104794,Cited by (1),"We consider the problem of estimating the mean vector ==== of a ====-dimensional spherically symmetric distributed ==== based on balanced loss functions of the forms: (i) ==== and (ii) ====, where ==== is a target estimator, and where ==== and ==== and the target estimator ====, we provide Baranchik-type estimators that dominate ==== and ====.","The balanced loss function (BLF) was introduced and formulated by Zellner [18] in order to reflect two criteria, namely goodness of fit and precision of estimation. For estimating ==== based on ====, consider loss incurred by estimate ====
 ====where ==== is a target estimator of ====, ====, and ====. Zellner’s original BLF corresponds to ==== and ==== as a least-squares estimator in a regression framework. The above loss encapsulates a more general choice of the target estimator (e.g., [9]) and a more general choice of ==== (e.g., [8], [10]), with the first term measuring proximity of estimate ==== to the target ==== in comparison to the second term measuring proximity of ==== to the estimand ====, weighted by ==== and ==== respectively. Decision making under such a loss will necessarily lead to a compromise modulated by the amplitude of ====, with the case ==== corresponding to the so-called unbalanced loss (denoted ====) where decisions are not influenced by ====. Alternatively, one may view the first term as a penalty for estimate ==== diverging from the target ====. An example of this arises with the choice ==== connecting the balanced loss with a ridge regression or Tikhonov regularization framework. For such reasons, balanced loss functions are appealing for decision making and have interested researchers over the years.====A natural and interesting modification of Zellner’s original balanced loss function, introduced in [16] is given by ====where ==== is a target estimator of ====, ====, and ====. Such losses possess similar attractive features as those in (1), and are also especially appealing if ==== is concave or even bounded.====For the original squared error loss balanced loss function case with ==== or ====, it is known (e.g., [4], [9], [16]) that frequentist risk performance of estimators is directly related to the frequentist risk performance of associated estimators under unbalanced loss. For instance, we have the following:====Marchand & Strawderman [16] considered the estimation of the mean ==== of a multivariate normal or a scale mixture of normal distributions under losses (1), (2). They provided, for three dimensions or more, for increasing and concave ==== and ==== which also satisfy a completely monotone property, Baranchik-type estimators which dominate the benchmark ====. Their findings apply to a vast collections of ====’s and ====’s, and quite generally for scale mixtures of normal distributions, but they do not cover non completely monotone ==== and ====, as well as other spherically symmetric distributions.====In this paper, we provide extensions with respect to the choices of ==== and ====, as well to the class of spherically symmetric densities. More precisely, for ====, ====, with spherically symmetric Lebesgue density ====, we obtain, for estimating ==== for losses of types (1), (2) with target ====, Baranchik-type estimators [1] that dominate the benchmark estimator ====. Our results apply to increasing and concave ==== and ==== and do not require complete monotonicity. The Baranchik-type estimators studied are of the form: ====with ==== twice differentiable a.e., ====Such estimators include simple choices like ==== with ====, including James–Stein estimators for ====. It is worthwhile noting that the findings here can be applied to cases where a sample ==== is drawn from ==== and when inference is then based on an estimator (such as ====) which is a spherically symmetric and translation invariant function of these data (e.g., [3]).====The set-up here matches that of [16], but we provide extensions both with respect to the choice of loss ==== or ====, as well as to the choice of model density ====. More specifically:====The paper is organized as follows. In Sections 2 Risk analysis for loss, 3 Risk analysis for loss, we present dominance results for balanced losses (1), (2), respectively. Both sections include a subsection of illustrations and remarks. Several technical results are presented in these sections, and others are relegated to an Appendix. Finally, a detailed example is presented in Section 4, with a comparison of frequentist risks and some details worth sharing about the calculations themselves.",On shrinkage estimation of a spherically symmetric distribution for balanced loss functions,https://www.sciencedirect.com/science/article/pii/S0047259X21000725,18 August 2021,2021,Research Article,138.0
"Liu Youming,Ren Chunguang","Department of Applied Mathematics Beijing University of Technology, Beijing, 100124, PR China","Received 18 May 2020, Revised 31 July 2021, Accepted 1 August 2021, Available online 18 August 2021, Version of Record 4 September 2021.",https://doi.org/10.1016/j.jmva.2021.104795,Cited by (0), ==== of a ==== ==== . They posed an open problem for the ,"Canonical correlation analysis plays a fundamental role in identifying and measuring the associations between two sets of random variables. Gao et al. study an optimal estimation for canonical correlation directions for Gaussian population, when the directions are sparse (Gao et al. ==== ====[3], Gao et al. ==== ====[4]). In contrast to those work, Cai and Zhang [1] establish separate perturbation upper bound estimations without assuming the sparse condition. However, the optimality of their estimation is unclear. Motivated by Gao–Ma–Zhou’s work (Gao et al. ==== ====[4]), we first prove optimality, and then consider upper bound estimations for sub-Gaussian population.",Estimation of canonical correlation directions: From Gaussian to sub-Gaussian population,https://www.sciencedirect.com/science/article/pii/S0047259X21000737,18 August 2021,2021,Research Article,139.0
Loperfido Nicola,"Dipartimento di Economia, Società e Politica (DESP), Università degli Studi di Urbino “Carlo Bo”, Via Saffi 42, 61029 Urbino (PU), Italy","Received 9 November 2020, Revised 15 July 2021, Accepted 15 July 2021, Available online 18 August 2021, Version of Record 25 August 2021.",https://doi.org/10.1016/j.jmva.2021.104809,Cited by (4),"Invariant coordinate selection (ICS) is a multivariate statistical method aimed at detecting data structures by means of the simultaneous diagonalization of two scatter matrices. Statistical applications of ICS include cluster analysis, independent component analysis, ====, ====, finite mixtures and stochastic processes. Theoretical results highlight both appealing properties and limitations of kurtosis-based ICS as a tool for detecting data structures.","A scatter matrix ==== of a ====-dimensional random vector ==== is a symmetric, positive definite ==== matrix satisfying ==== for any ==== matrix ==== of full rank and for any ====-dimensional real vector ====
 (Tyler et al. [53]). Scatter matrices are often classified according to their robustness properties: Class I scatter matrices have low breakdown points and unbounded influence functions; Class II scatter matrices have bounded influence functions but low breakdown points; Class III scatter matrices have bounded influence functions and high breakdown points (Tyler et al. [53] and Archimbaud et al. [4]).====Invariant coordinate selection (ICS) is a multivariate statistical method aimed at detecting data features by the simultaneous diagonalization of two scatter matrices ==== and ==== Equivalently, ICS uses the eigenvectors of ==== to obtain informative data projections. Intuitively, the first scatter matrix whitens the data by removing their second-order structure. Then principal component analysis based on the second scatter matrix computed from the whitened data uncovers other data structures (Fischer et al. [11]). The scatter matrices ==== and ==== play a symmetrical role in ICS. A practitioner using ICS could then rely either on ==== or on ====, keeping in mind that the order of the two scatter matrices impacts on the scale of the invariant coordinates, on the inversion of the eigenvalues and on the order of the eigenvectors (Archimbaud et al. [4]).====The scatter matrix ==== is often chosen to be more robust than the scatter matrix ====
 (Alashwali and Kent [3], Archimbaud et al. [4]), so that ==== might be regarded as a measure of multivariate kurtosis (Tyler et al. [53]). Intuitively, a measure of multivariate kurtosis should behave like the default measure of univariate kurtosis, that is the fourth standardized moment, whose population value is high when the corresponding distribution is heavy-tailed and its sample value is high when outliers are present. By choosing ==== to be more robust than ==== we hope that the eigenvalues of the matrix ==== will be higher when the sampled distribution is heavy-tailed and therefore more likely to generate outliers.====Statistical applications of ICS mainly deal with model-based clustering (Caussinus and Ruiz-Gazen [9], Tyler et al. [53], Peña et al. [46], Alashwali and Kent [3] and Fischer et al. [11]). Independent component analysis constitutes another important field of application (Cardoso [7], Oja et al. [44], Tyler et al. [53] and Miettinen et al. [36]). Caussinus and Ruiz-Gazen [8], [9], as well as Archimbaud et al. [4] apply ICS to outlier detection. Kankainen et al. [16] use ICS when testing multivariate normality. Ilmonen et al. [15] mention other applications of ICS, including multivariate measures of shape and multivariate location testing. ICS also occurs in sliced inverse regression, a technique used to reduce the number of explanatory variables in regression (Li [22]). Loperfido [27] used simulations to explore the merits of the dominant eigenvector of a kurtosis matrix used in ICS as the starting value of the numerical procedure for kurtosis maximization in projection pursuit.====The statistical problem at hand should guide the choice of the two ICS scatter matrices: outlier detection (Caussinus and Ruiz-Gazen [8] and Tyler et al. [53]) requires different scatter matrices than cluster identification (Peña et al. [46] and Alashwali and Kent [3]). The choice of the scatter matrices is of paramount importance in ICS (Hallin [13] and Alashwali and Kent [3]). The scatter matrices ==== and ==== are often chosen to be the covariance matrix, assumed to be positive definite, and the quarticovariance of a ====-dimensional random vector ==== with finite fourth-order moments. The quarticovariance of ==== is the ==== matrix ====where ==== and ==== are the mean vector and the covariance matrix of ====. The name “quarticovariance” is a reminder of ==== being the expectation of a quartic function in the components of ==== which is also the covariance of a weighted version of ====. As an intuitively appealing implication, in the univariate case the product ==== coincides with the fourth standardized moment.====The matrix ==== appears in the statistical literature with different names, as for example “kurtosis-based matrix” (Alashwali and Kent [3]) and “matrix of fourth moments” (Nordhausen and Virta [42]). The name “quarticovariance” avoids any confusion with other kurtosis matrices (Kollo [18] and Peña et al. [46]) and other fourth moment matrices (Loperfido [25], [27]). The matrix ==== is often scaled by ====, since the quarticovariance of a normal, ==== -dimensional random vector with positive definite covariance matrix ==== is ====. The matrix ==== is also a one-step scatter M-functional based on fourth moments (Oja et al. [44]) and has been named either “scatter matrix functional based on fourth moments” (Nordhausen et al. [39], Ilmonen et al. [15]) or “scatter matrix of fourth moments” (Fischer et al. [11] and Archimbaud et al. [4]).====The choice of ==== and ==== is well-established within the ICS literature (Fischer et al. [11]). Archimbaud et al. [4] recommend it when looking for outliers. Both theoretical and empirical results support the same choice in cluster analysis (Peña et al. [46] and Alashwali and Kent [3]). On the other hand, Tyler et al. [53] do not generally recommend using ==== and ==== in ICS, albeit deeming them be of interest for their tractability and for their connections with independent component analysis and projection pursuit. In the following, we shall refer to ICS based on ==== and ==== as to kurtosis-based invariant coordinate selection (KICS).====Despite its widespread use, KICS has some limitations. Firstly, the quarticovariance matrix is not defined when fourth-order moments of the sampled distribution do not exist finite. Secondly, ICS based on more robust scatter matrices has some merits even when fourth-order moments exist finite. Tyler et al. [53] suggest using the covariance matrix and a more robust scatter matrix, such as the Minimum Covariance Determinant or the Minimum Volume Ellipsoid. Alashwali and Kent [3] show the advantages of ICS based on robust scatter matrices over KICS when sampling from mixtures of long-tailed distributions. Archimbaud et al. [4] suggest to reconsider the choice of the covariance and the quarticovariance as ICS matrices when a large proportion of outliers is present.====The joint diagonalization of ==== and ==== is routinely used in independent component analysis (ICA) and is referred to as to fourth order blind identification (FOBI). Nordhausen and Virta [42] thoroughly reviewed the FOBI literature. However, the statistical applications of the method are not limited to ICA and include virtually all the above mentioned applications of ICS, as for example cluster analysis, outlier detection, projection pursuit and multinormality testing. For this reason, we decided to refer to the joint diagonalization of ==== and ==== as to KICS rather than as to FOBI: we did not want to mislead any researcher into thinking that the statistical applications of the method are limited to ICA.====Both ICS and projection pursuit are multivariate statistical methods aimed at finding interesting data structures. It is therefore natural to compare them, as first done in the seminal paper by Tyler et al. [53]. Kent [17] raised the question of whether KICS is better suited to detect data structures than kurtosis-based projection pursuit (KPP). Theoretical results support the use of both methods when estimating the best linear discriminant function, under a normal, two-component location mixture model (Peña et al. [46] and Alashwali and Kent [3]). The simulations’ results in Peña et al. [46] clearly suggest that KICS approach outperforms KPP when the number of units is large with respect to the number of variables. On the other hand, KPP outperforms KICS when the number of variables is large with respect to the number of units. To the best of our knowledge, no multivariate distribution in the statistical literature provides a case where KPP detects a nonnormal feature while KICS does not.====When the data have been already standardized, KICS uses the kurtosis matrix introduced by Cardoso [7], that is ====The same matrix appears in the location-scatter-skewness–kurtosis model proposed by Nordhausen et al. [39]. The matrix is sometimes referred to as to the fourth moment functional and the fourth moment matrix when computed from the sampled distribution and a finite sample. Mòri et al. [38] independently proposed the variant ==== as a measure of multivariate excess kurtosis. We shall refer to ==== as to the canonical kurtosis matrix, to highlight its prominent role in summarizing all fourth-order standardized moments and to distinguish it from the less known kurtosis matrices proposed by Kollo [18] and Loperfido [27].====The only statistical models for which the analytical properties of the canonical kurtosis matrix have been thoroughly investigated are finite mixtures and independent components. There is a close connection between the eigenvectors of the canonical kurtosis matrix and the Fisher’s linear discriminant subspace (Tyler et al. [53], Peña et al. [46] and Alashwali and Kent [3]). The same eigenvectors are also helpful in recovering the mixing matrix in independent component analysis (Cardoso [7], Tyler et al. [53] and Miettinen et al. [36]).====The present paper investigates the analytical properties of the canonical kurtosis and the quarticovariance matrices under different assumptions, thus hoping to promote KICS among a wider audience of researchers. The rest of the paper is structured as follows. Section 2 addresses the fundamental properties of the canonical kurtosis and the quarticovariance matrix. Section 3 states some inequalities regarding the eigenvalues of the canonical kurtosis matrix. Sections 4, 5 Finite mixtures, 6 Stochastic processes investigate the theoretical properties of the same matrix for symmetric distributions, finite mixtures and stochastic processes. Section 7 illustrates the results in the previous sections with a known bivariate distribution. Section 8 contains some concluding remarks and hints for future research. All theorems’ proofs are in Appendix.","Some theoretical properties of two kurtosis matrices, with application to invariant coordinate selection",https://www.sciencedirect.com/science/article/pii/S0047259X21000877,18 August 2021,2021,Research Article,140.0
"Denuit Michel,Robert Christian Y.","Institute of Statistics, Biostatistics and Actuarial Science - ISBA, Louvain Institute of Data Analysis and Modeling - LIDAM, UCLouvain, Louvain-la-Neuve, Belgium,Laboratory in Finance and Insurance - LFA, CREST - Center for Research in Economics and Statistics, ENSAE IPP, Palaiseau, France","Received 9 July 2020, Revised 17 May 2021, Accepted 19 July 2021, Available online 18 August 2021, Version of Record 2 September 2021.",https://doi.org/10.1016/j.jmva.2021.104803,Cited by (4),"In Efron (1965), Efron studied the stochastic increasingness of the vector of independent random variables entering a sum, given the value of the sum. Precisely, he proved that log-concavity for the distributions of the random variables ensures that the vector becomes larger (in the sense of the usual multivariate stochastic order) when the sum is known to increase. This result is known as Efron’s “monotonicity property”. Under the condition that the random variables entering in the sum have density functions with bounded second derivatives, we investigate whether Efron’s ==== generalizes when sums involve a large number of terms to which a central-limit theorem applies.",None,Efron’s asymptotic monotonicity property in the Gaussian stable domain of attraction,https://www.sciencedirect.com/science/article/pii/S0047259X21000816,18 August 2021,2021,Research Article,141.0
"Peng Liuhua,Qu Long,Nettleton Dan","School of Mathematics and Statistics, The University of Melbourne, Australia,dMed Biopharmaceutical Co., Ltd., Shanghai, China,Department of Statistics, Iowa State University, Ames, IA, USA","Received 26 June 2020, Revised 14 July 2021, Accepted 15 July 2021, Available online 18 August 2021, Version of Record 31 August 2021.",https://doi.org/10.1016/j.jmva.2021.104807,Cited by (0),Variable selection for multi-sample problems is of great interest in ,"The multi-sample problem is of elemental importance and has wide applications. Most existing research focuses on testing the equality of distributions from independent random samples [12], [16]. With the explosive and continued advancement of data collection, simultaneous measurement of more and more variables from any single experimental subject has become increasingly affordable and is frequently used in many research areas, which leads to a multi-dimensional or even high-dimensional problem. Furthermore, the variables are expected to have very complex dependence structures that are not well characterized. In such a complex setting, only a small number of the most important variables are the primary targets for in-depth investigation. This motivates a great need for variable selection for multi-sample problems.====Although variable selection is not a new problem in statistics, existing methods for variable selection in such multi-response and high-dimensional contexts are still limited in capability. Current methods usually suffer from one or more of the following shortcomings: (i) the number of variables cannot exceed the sample size; (ii) variable importance is evaluated based on a comparison of univariate marginal distributions (e.g., marginal screening procedures [2], [3], [4], [9]); (iii) strong model assumptions, typically on the mean structure, are imposed during variable selection (e.g., regularized generalized linear models [6], [21]); (iv) the selected variables can only capture certain aspects of differences in distributions (e.g., limma [17]). Thus, there is a great gap between the needs of scientific researchers and the capability of existing statistical tools for variable selection. This paper aims at avoiding or alleviating these aforementioned shortcomings.====The Multi-response permutation procedure (MRPP) described by Mielke and Berry [12] is a powerful tool that can detect differences between multivariate distributions. The test statistic is based on a weighted average of within-treatment pairwise distances, and the testing procedure is carried out by permuting the observations. Moreover, under some mild conditions, the MRPP test is equivalent to the distance-based test proposed in Székely and Rizzo [18], which inspired the Energy distance (ED) [19] and Distance component (DISCO) analysis of Rizzo and Székely [16]. Distance-based methods, such as the MRPP, ED and DISCO, have good features when dealing with multivariate and even high-dimensional multi-sample problems, especially in capturing dependence structure among variables.====Inspired by former works, in this paper, we introduce distance-based variable importance measures for multi-sample problems that automatically take covariance structures into consideration. The importance measures are based on the idea of imposing a hypothetical perturbation on each dimension, and the importance is evaluated as the effect of the perturbation on the differences among or between distributions. Furthermore, we propose an importance-measure-based backward selection (IM-BWS) algorithm that can be used to select the most important variables. By eliminating irrelevant dimensions iteratively, we can lower the dimensions of the data to alleviate the effect of high dimensionality. In addition, a modified MRPP based on the IM-BWS is introduced to improve the power performance of the original MRPP. Our proposed methods inherit the nice properties from MRPP, ED and DISCO, which are model-free, work for high-dimensional data, and can capture important variables under different models. Examples in both simulation studies and real data show that our proposed method has good performance.====The paper is organized as follows. Preliminaries on MRPP, ED and DISCO are presented in Section 2. Two distance-based importance measures are introduced in Sections 3 Importance measures based on MRPP, 4 Importance measures based on the energy distance. Section 5 reports the importance-measure-based backward selection algorithm. A modified MRPP is presented in Section 6. Numerical studies including Monte Carlo simulations and data analysis are shown in Section 7. Finally, Section 8 contains a brief summary of this paper. Proofs are relegated to Appendix A and additional simulation results can be found in the supplementary materials.",Variable importance assessments and backward variable selection for multi-sample problems,https://www.sciencedirect.com/science/article/pii/S0047259X21000853,18 August 2021,2021,Research Article,142.0
"Wei Zheng,Kim Daeyoung","Department of Mathematics and Statistics, University of Maine, Orono, ME 04469-5752, USA,Department of Mathematics and Statistics, University of Massachusetts-Amherst, Amherst, MA 01003-9305, USA","Received 18 February 2020, Revised 19 July 2021, Accepted 19 July 2021, Available online 10 August 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jmva.2021.104793,Cited by (1),". It enables delineating the identified dependence in an exploratory manner. The checkerboard copula association measure quantifies the strength of the dependence identified by the checkerboard copula regression. We investigate the properties of checkerboard copula scores, checkerboard copula regression, its association measure, and their estimators. Finally, the performance of the proposed method is illustrated with simulation and real data.","The analysis of data with ordinal categorical responses has been an important task in various research fields such as social, medical, and public health sciences. The use of the ordering information in the categories of ordinal variables results in more powerful inferences and various methods designed for the ordinal response data have been developed (see [1], [2], [10], [22], [25], [31], [32], [34], [43], [47], [48] and references therein).====A primary step for properly analyzing data with ordinal responses is to explore the data and discover different types of dependence structures among the variables in as many ways as possible for exploratory and descriptive modeling. This step enables the identification of dependency patterns that are perhaps unknown or informally formulated, to summarize the identified dependencies, and to obtain clues for potential variables that may affect the ordinal response variable [6], [46]. This information would be beneficial toward the goals of formal modeling such as explanatory modeling to test underlying causal hypotheses and predictive modeling to predict new or future observations [40].====One of the dependence structures that has attracted the interest of researchers in the statistical modeling process is the regression dependence, where the main interest is to identify and measure the relationship of one variable designated as a response variable on the remaining variables designated as explanatory variables. Many model-based approaches explicitly modeling regression dependence in ordinal response data have been proposed, especially for the formal modeling (explanatory modeling or predictive modeling). Commonly used methods include proportional odds model (cumulative logit model) [35], other cumulative link models (with probit, log–log, and complementary log–log links) [3], [9], adjacent-categories logit model [17], [41], continuation ratio logit model [29], stereotype model [4], latent variable models, association models [16], [18], correspondence analysis models, and canonical correlation models [13], [14], [19], [20]. On the other hand, one may employ a variety of non-model-based approaches for ordinal response data, including (but not limited to) generalized Cochran–Mantel–Haenszel methods [30], [33], ordinal odds ratios [7], [16], [17], [35], and rank-based methods, for example, Kendall’s tau [26] and its variants [27], [45], Goodman and Kruskal’s gamma [21], Spearman’s rank-based correlation [1], Somers’ D [44], Kendall’s partial tau and its extension [23], [28]. These non-model based methods provide description and inference for ordinal data without explicit specification of the underlying dependence structure among the variables.====However, it is not straightforward to use the non-model-based methods listed above when the goal is to learn and discover the regression dependence in the multivariate categorical data with an ordinal response variable. This is because these methods are mainly designed for bivariate (marginal/conditional) association between two ordinal variables of interest (unconditional/conditional on the other variables), and some of these methods treat two ordinal variables symmetrically (i.e., no distinction between the response variable and the explanatory variable).====To this end, we propose a novel non-model based, data-driven exploratory method to identify and quantify the regression dependence in a multi-way contingency table with an ordinal response variable and categorical (ordinal or nominal) explanatory variables. The principal mathematical tool employed in the proposed method is the checkerboard copula, also called multilinear extension copula [11], [37], [39]. The checkerboard copula, constructed by multilinear interpolation of the joint distribution function of discrete marginal distributions, uniquely links the marginal distributions of discrete random variables to their joint distribution function. Recent researches show that the checkerboard copula best represents the dependence structure among multivariate discrete variables and it plays an important role in developing new statistical methods for ordinal contingency tables [5], [11], [12], [38].====The proposed methodology consists of three parts: checkerboard copula score, checkerboard copula regression, and checkerboard copula association measure. First, the checkerboard copula score is a new type of score for ordinal variables accounting for the ordinal nature of the categories. To exploit the fact that the ordering of the categories of ordinal variables is informative, the proposed scores will be utilized in the methods developed in this paper. Second, the checkerboard copula regression is a model-free approach to identify the regression dependence between an ordinal response variable and categorical explanatory variables. It also enables delineating the identified dependence in an exploratory manner via the prediction of an ordinal response variable for each combination of categories of explanatory variables. Third, the checkerboard copula regression association measure is an index to quantify the strength of the association identified by the checkerboard copula regression. It is the average proportion of variance in the response variable (with respect to its checkerboard copula score and its marginal distribution) attributable to the checkerboard copula regression. Note that the proposed methods, designed for contingency tables where all variables are ordinal, are also applicable for the tables with nominal explanatory variables.====A previous work relevant to the method proposed in this paper is the one introduced in [31], where a set of test statistics was developed to examine the association between two ordinal categorical variables after adjusting for continuous and/or categorical covariates. In their method, both ordinal variables are treated symmetrically, meaning that it does not distinguish which ordinal variable is to be considered the dependent variable and which is to be considered the explanatory variable. The first step of their approach is to separately fit the two ordinal variables on the covariates using ordinal categorical response models (requiring explicit modeling of the relationship between each ordinal variable and covariates) and obtain the predicted probability distributions for these two ordinal variables. The next step is to construct test statistics using the predicted probability distributions and bivariate association measures (such as Goodman and Kruskal’s gamma) and compute p-values for testing the null of conditional independence. The main features of the proposed method that differs from the work of [31] are four-fold: (i) the proposed method focuses on the regression dependence leading to distinguishing between a response variable and explanatory variables; (ii) it is designed for measuring the strength of the regression dependence, though it can be used to test the null of no regression association based on permutation test; (iii) it does not require an explicit model for regression dependence; (iv) it is applicable only when explanatory variables are categorical (ordinal or nominal).====The remainder of this article is organized as follows. In Section 2, we briefly review the checkerboard copula in a multi-way ordinal contingency table. We introduce the checkerboard copula score for the ordinal categorical variable, and propose the checkerboard copula regression and its association measure for a multi-way contingency table with an ordinal response variable and categorical explanatory variables. Their theoretical properties are also investigated. Section 3 introduces the estimation of the methods proposed in Section 2 and presents their asymptotic properties. To demonstrate the fundamental ideas of the proposed methodology, we consider an artificial two-way contingency table as a running example through Sections 2 Checkerboard copula score, checkerboard copula regression and its association measure for the multi-way contingency table with an ordinal response, 3 Estimation. In Section 4, we perform simulation and data analysis to evaluate the performance of the proposed method. Finally, Section 5 closes the paper with a discussion.",On exploratory analytic method for multi-way contingency tables with an ordinal response variable and categorical explanatory variables,https://www.sciencedirect.com/science/article/pii/S0047259X21000713,10 August 2021,2021,Research Article,143.0
"Zhang Lyuou,Zhou Wen,Wang Haonan","School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai 200433, PR China,Department of Statistics, Colorado State University, Fort Collins, CO 80523, USA","Received 22 December 2020, Revised 5 July 2021, Accepted 6 July 2021, Available online 17 July 2021, Version of Record 31 July 2021.",https://doi.org/10.1016/j.jmva.2021.104786,Cited by (1),"Large scale temporal data have flourished in a vast array of applications, and their sophisticated structures, especially the ==== among subjects with inter- and intra-temporal dependence, have fueled a great demand for new statistical models. In this paper, with ","Jointly modeling a large and possibly divergent number of temporally evolving subjects arise ubiquitously in genomics, proteomics, environmental science, econometrics, clinical studies, and neuroscience. An extensively used statistical model for explaining the interactions and co-movements among the temporally evolving subjects is ====, where ==== is the observation for the ====th subject at time point ====, ==== is a ====-dimensional regression coefficient, ==== is a ====-dimensional covariate vector that might evolve in time, and ==== is a vector time series with possible contemporaneous correlations. Here, the number of subjects ==== is allowed to diverge much faster than the number of time points ====. To name a few applications, ==== can model the expression level of the ====th gene at time point ==== in a time course sequencing experiment, see, e.g., [10], [40], the concentration of certain air pollutant in county ==== at day ====, see, e.g., [29], and the measurement from electroencephalograms at brain location ==== and time point ==== in a motor–visual task experiment, see, e.g., [33]. As ==== rapidly grows, heteroscedasticity across subjects becomes inevitable and brings substantial challenges to modeling, estimation, and inference [14], [46]. First, ignoring the subject-specific heteroscedasticity leads to inefficient estimation and inference on the regression coefficient ====. In addition, in the presence of contemporaneous and serial correlations, when ==== rapidly outnumbers ==== the high dimensionality of data makes it even more difficult to accurately estimate the covariance of ====, which compromises the estimation efficiency of ====.====In this paper, inspired by the approximate factor structure and its variants [11], [26], [38], [44], we introduce a flexible data-driven model, where the heteroscedasticity across subjects and serial dependence of ==== are assumed to arise from a product of the subject-specific effect and some latent stationary process. Specifically, motivated by [13], [14], and [19], with additional time invariant covariates ====, we model the subject-specific effect by ==== with nonparametric functions ====. For instance, ==== could be the genetic information in a clinical study or the market capitalization in the financial asset allocation. Then, consider a ====-dimensional zero-mean latent process ====, our semiparametric latent factor model with subject-specific heteroscedasticity is ====where the residual process ==== is independent of ====. Analogous to the traditional factor models, ==== and ==== can be viewed as the loading and factor, respectively. Particularly, ==== models the heteroscedasticity across subjects and, together with ====, retains the cross-sectional dependence while ==== and ==== characterize the serial dependence. Model (1) features a large number of widely used models. For example, when ==== is degenerate, (1) reduces to the partially linear additive models [41]; when ==== is known and ==== is Gaussian, (1) is the traditional linear mixed model; with ==== replaced by one-dimensional spatial locations, (1) is analogous to the spatio-temporal model [31]; and when ==== reduces to constants, (1) is equivalent to the traditional factor models [2], [11] or the panel data model with unobservable interactive effects [3], [4]. It is worth mentioning that, focusing on the high-dimensional factor analysis, [13] and [19] consider ==== in addition to ==== in the loading to account for that cannot be explained by ====. In contrast, one of our major goals is to efficiently estimate ==== in the presence of subject-specific heteroscedasticity and contemporaneous and serial correlations for any ==== and ====. To that end, it requires recovering ==== and ==== with satisfactory rates as well as accurate estimation of the long run variance of residues, while from Theorem 4.1 in [19] and Theorem 1, the desired rate for estimating the long run variance of residues will not be satisfied in the presence of ====, unless more stringent conditions on ==== and ====’s are imposed.====Like the partially linear model and the linear mixed model, regardless of its consistency, the ordinary least squares (OLS) estimator of ==== in (1) is inefficient without taking into account the dependence. Therefore, a careful estimation of the unobserved ==== and ==== are needed to guarantee some sort of efficiency in both estimation and inference on ====. In the literature, there are scatter approaches to estimate ==== and its loadings for models similar to (1). For instance, [13] employed a kernel method to estimate ==== given ==== with finite values, and [12] extended such estimates for general ====. Additionally, the consistency on estimated loading and latent factor shed light on estimating the large covariance matrix under assumptions of factor structures [18]. Motivated from these pioneering works, we propose a two-stage projection-based estimator of ====, ====, and ==== in (1). Roughly speaking, adopting a projection-based principal component type estimator [2], [19], we first estimate ==== and ==== from ==== for some preliminary ====. Using the estimated ==== and ==== from the first-stage, we then estimate ==== with a generalized least squares (GLS) type approach.====Theoretically, the asymptotic properties such as consistency on estimating ==== and ==== are not sufficient to guarantee the consistency and, especially the efficiency, of the second-stage estimator of ==== due to the lack of the finite sample characterization of errors from the first-stage [6]. In fact, it is known that a naive plug-in GLS type estimator does not necessarily guarantee the efficiency. To circumvent these challenges, a major contribution of this paper is a careful non-asymptotic analysis on the projection-based estimator of ==== and ====, by which we show that the consistency on estimating ==== and ==== is free from restrictions on the relationship between ==== and ====. With the exponential type concentration inequalities on estimating ==== and ====, we are able to obtain the finite sample deviation of the proposed two-stage estimator of ==== from the oracle GLS that enjoys full access to ==== and ====. These nontrivial results show that our estimator of ==== is overwhelmingly close to the oracle GLS, which establishes the efficiency of our estimator of ====. In addition, we show the asymptotic normality of our estimator of ==== for drawing inference. The established concentration results for recovering ==== and ====
 (Theorem 1) are of independent interest for extending the projection-based principal component analysis (PCA) to other high-dimensional problems such as modeling temporally evolving tensor data or the segmentation of high-dimensional time series.====After introducing our model with identification conditions in Section 2.1, we detail the two-stage projection-based estimation of the loading, latent factor process, and regression coefficients in Section 2.2. We carry out the non-asymptotic analysis of our estimator and explore its efficiency in Section 3. Section 4 presents the inference. Sections 5 Numerical studies, 6 Study on air quality and energy consumption data using TOPE report simulation studies and an application on air quality and energy consumption data in the United States to demonstrate our method. After discussing potential extensions of our method in Section 7, we conclude the paper with technical details in Section 8. Extra numerical results and a detailed discussion on the determination of unknown dimension ==== are deferred to the supplement.",A semiparametric latent factor model for large scale temporal data with heteroscedasticity,https://www.sciencedirect.com/science/article/pii/S0047259X21000646,17 July 2021,2021,Research Article,144.0
Sutradhar Brajendra C.,"Memorial University, St. John’s, NL, Canada","Received 3 July 2020, Revised 29 June 2021, Accepted 29 June 2021, Available online 9 July 2021, Version of Record 17 July 2021.",https://doi.org/10.1016/j.jmva.2021.104785,Cited by (1), ==== (BPM) based composite likelihood approach using local ,"The spatial data are realizations of random variables collected from a sequence of related geographical locations, where the responses (linear, counts or binary) collected from adjacent locations naturally become correlated. These correlations are referred to as the spatial correlations. In a spatial setup for linear data, there exist many studies dealing with spatial regression analysis after accommodating spatial correlations among the responses from neighboring locations. A big list of references on this research topic is available in Cressie and Wikle [8], for example. Most of these linear data based studies model the spatial correlations among responses from neighboring locations, by using a spatial auto-regression (SAR) type model [3], or a linear mixed model (LMM) with varieties of Gaussian or certain patterned covariance structure. Cressie [7], for example, suggests for using various correlation structure such as exponential covariance function, Gaussian covariance function and a reciprocal covariance function. Similar correlation functions but by using the spectral approach, were exploited by Jones and Vecchia [10] (see also, e.g., [15], [21], [22]) in order to define pairwise spatial covariances.====There is also a long history on spatial binary data analysis. See, for example, Besag [4], [5] and Besag [6], for some early studies. For some recent studies over the past three decades, see, for example, Rathban and Cressie [14], Heagerty and Lele [9], Lin and Clayton  [11], and Ainsworth et al. [1]. As far as the model for spatial binary data is concerned, these existing studies mostly used the CAR  [6], BPM [9], and WCQL [11] models to accommodate spatial binary correlations. For convenience, we provide a brief review of these CAR, BPM, and WCQL models in Section 3.1, along with certain difficulties encountered by these models. More specifically it is demonstrated that these widely used models fail to address the within and between spatial familial correlations while developing a spatial correlation structure. In contrary to these models, recently some studies developed a spatial correlation model by accommodating both within and between spatial familial correlations. See, for example, Mariathas and Sutradhar [12] in a linear setup, and Sutradhar and Oyet [18] in a binary setup. In this paper, we apply these ideas [12], [18] in Section 3.2 and develop a general spatial correlation model for exponential family based such as linear, count or binary data. The basic moment properties such as the mean and variance of the exponential family based spatial data are given in Section 2.====In Section 4, it is demonstrated that the proposed two-way correlation approach developed in Section 3.2 produces a block-banded spatial correlation (BBSC) matrix. Once evaluated, the elements of this BBSC matrix clearly exhibit spatial correlations among neighboring locations. More specifically, any zero correlations in a row or column away from the main diagonal elements of the matrix would indicate that the underlying two spatial locations are far away from each other such that all pair-wise distances between their family members are beyond a fixed threshold distance. In the same section, we then obtain the inversion of the BBSC matrix, which subsequently used in Section 5 to develop the so-called GQL (generalized quasi-likelihood) estimating equations for the spatial regression parameters. For the estimation of the random effects variance and correlation parameters we develop the second order response based moment equations. The asymptotic properties such as consistency of these GQL and moment estimators are also given in the same section. The paper concludes in Section 6.",Block-band behavior of spatial correlations: An analytical asymptotic study in a spatial exponential family data setup,https://www.sciencedirect.com/science/article/pii/S0047259X21000634,9 July 2021,2021,Research Article,145.0
"Stephanou Michael,Varughese Melvin","Department of Statistical Sciences, University of Cape Town, Cape Town, South Africa,School of Mathematics and Statistics, University of Western Australia, Perth, Australia","Received 20 January 2021, Revised 25 June 2021, Accepted 25 June 2021, Available online 6 July 2021, Version of Record 22 July 2021.",https://doi.org/10.1016/j.jmva.2021.104783,Cited by (17), on massive data sets.,"The statistical analysis of streaming data and one-pass analysis of massive data sets has become highly relevant in recent times. These settings necessitate online algorithms that are able to process observations sequentially, where ideally the time taken and memory used do not grow with the number of previous observations, i.e., ==== update time and memory requirements. In the univariate setting, certain statistical properties naturally lend themselves to efficient, sequential calculation such as the mean and variance [5], [22], [37], [38] and higher order moments [4], [24]. Similar developments include incremental formulae for cumulants up to fourth order [3], [8]. In addition, algorithms have been introduced for sequential estimation of probability densities (see Chapters 4 and 5 of [13] and Chapter 7 of [9] for a discussion of recursive kernel estimators and [30] for recursive estimators based on Bernstein polynomials), cumulative probabilities [18], [29], [33] and quantiles [6], [14], [15], [17], [21], [26], [27], [33], [35], [40]. In the context of the statistical analysis of bivariate streaming data and one-pass analysis of massive bivariate data sets, certain quantities again easily extend to online calculation, such as the Pearson product-moment correlation coefficient. By contrast, online algorithms for nonparametric measures of concordance such as the Spearman rank correlation coefficient (Spearman Rho) and Kendall rank correlation coefficient (Kendall Tau) have only recently been proposed [39]. These nonparametric correlation measures are suitable for all monotonic relationships and not just linear relationships as in the case of the Pearson correlation coefficient [10]. In addition, these nonparametric correlation measures are more robust than the Pearson correlation estimator [7]. Applications of nonparametric correlation measures include eliciting relationships for financial instruments [2], amongst others. We propose a novel approach to the sequential estimation of the most popular nonparametric correlation measure, Spearman rank correlation, based on bivariate Hermite series density estimators and Hermite series based cumulative distribution function (CDF) estimators [32], [33]. We leverage a key advantage of these estimators, namely their ability to maintain sequential estimates of the full density and full distribution function respectively. The central idea is that we can utilize the Hermite series CDF estimator and the bivariate Hermite series density estimator together with a large sample definition of the Spearman rank correlation estimator to furnish online estimates of the Spearman rank correlation. These estimates can be updated in constant, i.e., ==== time and require only a small and fixed amount of memory (==== memory requirements with respect to number of observations). By comparison, the standard estimator of Spearman rank correlation requires first sorting all observations to determine ranks implying an average time complexity of ==== for ==== observations. In addition, a naïve approach to updating a Spearman rank correlation estimate with a new observation would change all ranks (in general an operation with worse than constant time complexity) and necessitate a recalculation over the history of previous observations (==== time complexity). Our algorithms are useful in the stationary sequential estimation setting (i.i.d. observations from a bivariate distribution for example) as well as one-pass batch estimation in the setting of massive data sets. In the i.i.d. observation case, we are able to provide asymptotic guarantees on the rate of convergence in mean of this estimator to the large sample Spearman rank correlation estimate. The Hermite series based Spearman rank correlation estimation algorithm can also be modified to estimate the Spearman rank correlation for non-stationary bivariate data streams. To treat the case of sequential estimation in the non-stationary setting, we introduce a novel, exponentially weighted estimator for the Spearman rank correlation, which allows the local nonparametric correlation of a bivariate data stream to be tracked. To the best of our knowledge this is the first algorithm to be proposed for estimating a time varying Spearman rank correlation that does not rely on a moving window approach. The rest of the article is organized as follows: in Section 2 we review some relevant background on the Spearman rank correlation coefficient. In addition, we briefly review the bivariate Hermite series density estimator and the Hermite series distribution function estimator. In Section 2.2 we link the Hermite series based density and CDF estimators to the Spearman rank correlation coefficient. In Section 3.1 we present an algorithm for calculating the Spearman rank correlation coefficient applicable to the stationary sequential setting. In Section 3.2 we present an algorithm suitable for non-stationary data streams, based on an exponentially weighted Spearman rank correlation estimator. We provide the rate of convergence in mean of the Hermite series based Spearman rank correlation estimator applicable to the stationary setting for i.i.d data streams in Section 4. In Section 5 we investigate the variance (and hence standard error) properties of the exponentially weighted Spearman correlation estimator. In Section 6 we present simulation studies which demonstrate the effectiveness of our algorithms in practice both in the stationary and non-stationary settings. In the stationary setting, we demonstrate that our algorithm is competitive with an existing algorithm. In addition, we present an application of the non-stationary Spearman correlation estimation algorithm to real data in the form of streaming forex data in Section 7. We conclude in Section 8. Proofs and further technical details are collected in Appendix A Proofs, Appendix B Exponentially weighted Pearson’s correlation estimator. An ==== [25] package implementing the algorithms described in this article is available online [31].",Sequential estimation of Spearman rank correlation using Hermite series estimators,https://www.sciencedirect.com/science/article/pii/S0047259X21000610,6 July 2021,2021,Research Article,146.0
Ouimet Frédéric,"California Institute of Technology, Pasadena, USA","Received 24 February 2021, Revised 23 June 2021, Accepted 24 June 2021, Available online 5 July 2021, Version of Record 19 July 2021.",https://doi.org/10.1016/j.jmva.2021.104784,Cited by (10),"-dimensional simplex. Our results generalize the ones in Leblanc (2012a) and Babu et al. (2002), who treated the case ====, and significantly extend those found in Tenbusch (1994). In particular, our rates of convergence for the MSE and MISE are optimal.",None,Asymptotic properties of Bernstein estimators on the simplex,https://www.sciencedirect.com/science/article/pii/S0047259X21000622,5 July 2021,2021,Research Article,147.0
"Chen Yang,Luo Ziyan,Kong Lingchen","Department of Mathematics, Beijing Jiaotong University, Beijing, 100044, China","Received 24 January 2021, Revised 14 June 2021, Accepted 14 June 2021, Available online 24 June 2021, Version of Record 25 June 2021.",https://doi.org/10.1016/j.jmva.2021.104782,Cited by (0),"-norm based selection and estimation for multivariate GLMs. Under mild conditions, we give a necessary condition for selection consistency based on the notion of degree of separation, and propose the feature selection consistency as well as optimal coefficient estimation for the resulting ====-likelihood methods.","Consider multivariate generalized linear models (GLMs) with ==== observations ====, where ==== is a ====-dimensional predictor vector, ==== is a ====-dimensional response vector. The observations ==== are i.i.d. drawn from the exponential family with the canonical parameter vector ====: ====where ==== is the underlying measure, ==== is the sufficient statistic and ==== is a known link function. Multivariate GLMs assume that ==== and ====, with ==== the true underlying coefficient matrix. The density of ==== can be written as ====Note that setting ==== leads to traditional GLMs introduced by McCullagh and Nelder [20]. The maximum likelihood estimator of the multivariate GLM, termed as ====, is the optimal solution of the following minimization problem: ====More about multivariate GLMs can be found in the monographs [7] by Fahrmeir and Tutz, and [10] by Hardin and Hilbe.====In high-dimensional data analysis, feature selection becomes increasingly crucial since true underlying models admit a sparse representation. Meanwhile, it is worth noting that the inherited sparsity may possess group structures, such as the row sparsity of the coefficient matrix ==== in the setting of multivariate GLMs, resulting from the fact that multiple responses are possibly related to a group of common features. For example, in joint sparse signal recovery problems [22], multiple signals share a common sparse support set, and the sparsity is defined as the number of nonzero rows in the source matrix. In multiple classification problems [24], multiple tasks are related to a common sparse set of covariates, and the corresponding covariate matrix also possess the row sparsity. Our goal is to explicitly take this predefined group sparse structure into account for multivariate GLMs.====Comparing with classical sparse regression using ====-norm (i.e., the number of nonzero entries) [29], [30], [34] and its relaxation surrogates, see, e.g., [8], [16], [32], [41], [43], there are several compelling reasons to consider the row sparsity for multivariate GLMs. It is more reasonable since the group structure is an important piece of a prior knowledge in many applications, such as integrative genomics [27], visual classification [40] and mentioned above [22], [24]. In addition, taking the information of dependence of responses into account, the resulting estimator will perform better and be more interpretable [39]. Moreover, the use of group structure can avoid the dramatic increase in dimension caused by the direct use of the vectorization technique, especially for large-scale problems.====Write the coefficient matrix ==== as ====, where the row support set ==== is the index set of nonzero rows in ==== with the row sparsity ====, ==== is a matrix of ====’s with ==== the complementary set of ====. Then the ====-norm can be defined as ==== with ==== the size of a set. It is the most direct and accurate depiction of the row sparse structure. In addition, numerical results reported in [30] have shown that ====-norm constrained likelihood method outperforms the lasso under the univariate regression framework. In such senses, it is natural to formulate the group sparse regression for multivariate GLMs by solving ====where ==== is the ====-norm of ==== that counts the number of nonzero rows, and ==== is a regularization parameter which balances the magnitude of the loss function and row sparsity of the estimator. We refer to this method as the regularized ====-likelihood. Note that ====-norm has already been employed for group selection of multivariate linear models [25], in which the least squares loss ==== has been adopted, where ==== denotes the Frobenius norm with ==== for a matrix ====. Furthermore, if the true row sparsity is known a priori, the constrained counterpart of (3) would be a good alternative for variable selection and estimation, which is ====where ==== is a prescribed parameter that controls the row sparsity. Analogously, the constrained counterpart (4) is referred to as the constrained ====-likelihood. Although ====-norm is discontinuous and nonconvex, Wang et al. [35] have proved that there exists the global minimizer for the typical example of (3), optimization problems corresponding to the multinomial logistic regression.====There is a huge body of work on group selection through relaxation schemes for the involved ====-norm regularization under the multivariate regression (or multitask learning) framework. The two of the most popular methods are ====-norm (i.e., the sum of all Euclidean norms of rows) regularization [2], [3], [9], [18], [25] and ====-norm (i.e., the sum of all maximum absolute elements of rows) regularization [19], [23], [33]. Due to the hardness of ====-norm, little is known about statistical properties of the estimators resulting by the regularized as well as the constrained ====-likelihood, although these approximations methods have been extensively studied, particularly in the setting of multivariate linear regression. For example, feature selection consistency for the group lasso [25], oracle bounds of the estimated order of sparsity, prediction error and estimation error for the sparse group lasso [15], oracle properties [8] for a three-level variable selection approach based on group smoothly clipped absolute deviation (SCAD) penalty [11] and the generalized adaptive elastic-net [39].====In addition, considering necessary or sufficient conditions for feature selection consistency has been shown imperative for any sparse regression method, for example, a strong irrepresentable condition which is nearly necessary for the lasso [21], [42], a sparse Riesz condition (SRC) for the minimax concave penalty (MCP) [41] and the group lasso [36], [37], a necessary and a sufficient conditions for the multivariate group lasso [25]. Moreover, necessary conditions in [13], [29], [30] seem to have suggested that exponential candidate features in the sample size are possible for some methods to be selection consistent.====In this paper, we focus on the uninvestigated group sparse regression for multivariate GLMs. The original ====-norm penalty is directly used for feature selection instead of the existing relaxation schemes. We further make a statistical theory analysis for the regularized ====-likelihood method defined in (3), including the necessary condition for selection consistency, the two asymptotic results of feature selection consistency and optimal coefficient estimation. Meanwhile, all the results apply to the constrained ====-likelihood method defined in (4). In addition, we propose an ====-proximal gradient (====-PG) algorithm and an ====-improved iterative hard thresholding (====-IIHT) algorithm to solve the regularized model (3) and the constrained model (4), respectively. Numerical results on both synthetic and real data confirm the superiority of our proposed ====-likelihood method on the group sparsity and the use of original ====-norm.====The remainder of the article is organized as follows. In Section 2, we derive a necessary condition for selection consistency of any sparse regression. In Section 3, we give the theoretical results of mis-estimation error bound and asymptotic properties for both the regularized and the constrained ====-likelihoods. Section 4 reports some numerical studies to compare our ====-likelihood methods with several classical and popular sparse regression methods. Concluding remarks are drawn in Section 5. Proofs of the main results are presented in Appendix A.",-norm based selection and estimation for multivariate generalized linear models,https://www.sciencedirect.com/science/article/pii/S0047259X21000609,24 June 2021,2021,Research Article,148.0
"Arellano-Valle Reinaldo B.,Azzalini Adelchi","Departamento de Estadística, Pontificia Universidad Católica de Chile, Chile,Dipartimento di Scienze Statistiche, Università degli Studi di Padova, Italy","Received 4 September 2020, Revised 7 June 2021, Accepted 7 June 2021, Available online 12 June 2021, Version of Record 21 June 2021.",https://doi.org/10.1016/j.jmva.2021.104780,Cited by (3),"Several formulations have long existed in the literature in the form of continuous mixtures of normal variables where a mixing variable operates on the mean or on the variance or on both the mean and the variance of a multivariate normal variable, by changing the nature of these basic constituents from constants to random quantities. More recently, other mixture-type constructions have been introduced, where the core random component, on which the mixing operation operates, is not necessarily normal. The main aim of the present work is to show that many existing constructions can be encompassed by a formulation where normal variables are mixed using two univariate random variables. For this formulation, we derive various general properties, with focus on the multivariate context. Within the proposed framework, it is also simpler to formulate new proposals of ====, and we provide a few such instances. As a side product, the exposition provides a concise compendium of the main constructions of continuous normal-mixtures type, although a full overview of this vast theme is not attempted.",None,A formulation for continuous mixtures of multivariate normal distributions,https://www.sciencedirect.com/science/article/pii/S0047259X21000580,12 June 2021,2021,Research Article,149.0
"Kharin Yuriy,Voloshko Valeriy","Research Institute for Applied Problems of Mathematics and Informatics, Belarusian State University, 220030 Nezavisimosti ave. 4, Minsk, Office 702, Belarus","Received 9 July 2020, Revised 17 May 2021, Accepted 17 May 2021, Available online 1 June 2021, Version of Record 17 June 2021.",https://doi.org/10.1016/j.jmva.2021.104777,Cited by (2),"A topical problem of robust statistical estimation of parameters for binomial conditionally nonlinear autoregressive (BiCNAR) time series under innovation outliers is considered. This problem is solved by means of ====-prehistory. The new robust statistical estimator ==== for the BiCNAR parameters, that is constructed for the hypothetical model without outliers, is carried out for the situation with innovation outliers: ==== is found by minimization of the asymptotic risk w.r.t. ====. Statistical estimator for ==== based on the observed time series is constructed. Results of multiple computer experiments on simulated and real data illustrate the theory.","Modern stage in mathematical statistics and data science can be characterized by three main features: (A) analysis of multivariate and nonlinear stochastic dependencies in observed data; (B) modeling of discrete-valued data streams; (C) robust analysis of data under distortions of hypothetical model. A problem considered in this paper is characterized by features A, B and C simultaneously: construction of statistical inferences on discrete-valued time series ==== with outliers using multivariate nonlinear stochastic dependencies.====Theory of time series analysis is deep developed [3] only for “continuous” data when the observation space ==== is some Euclidean space or its subspace of nonzero Lebesgue measure: ====, ====. In practice, however, (because of “digitalization” of our real world) the statisticians often collide with discrete-valued time series [7], [40], when the observation space ==== is some discrete set with cardinality ====, ====. Let us give some applied areas where discrete-valued time series models are extremely helpful [7], [39], [40]: bioinformatics for analysis of genetic sequences (====); information systems for information protection (====); meteorology for weather prediction; social science for modeling of dynamics in social behavior; public health and personalized medicine; prediction of environmental processes; financial engineering; telecommunications; alarm systems.====Discrete-valued time series is a random process ==== on some probability space ==== with discrete time ==== and discrete state space ====, that can be finite (====) or countable (====). If ====, then the time series ==== is called count time series (CTS) [17]. If ==== is a finite set of “qualitative categories”, then ==== is called categorical time series (nominal or ordinal) [32] and binary time series [9]. For the countably-valued CTS (====) probabilistic models INAR, INMA, INARMA, INGARCH based on “thinning operators” are developed by McKenzie [31], Al-Osh and Alzaid [1], and other researchers [7]. A deep and fresh review of these models and results on their statistical analysis can be found in [39].====In this paper we consider finite-valued time series ==== with state space ==== consisted of ==== quantitative states or “qualitative categories” that are coded without loss of generality as ====; this means that ==== is a finite-valued CTS, and also it is a specially coded categorical time series. For the finite-valued CTS ==== a universal model catching deep stochastic dependencies is homogenous Markov chain of sufficiently large order ====
 (====) determined by the generalized Markov property [8]: ====where ==== is the memory depth; ==== are discrete values of the process at ==== previous time moments (====-prehistory); ==== is an ====-dimensional matrix of one-step transition probabilities. Number of independent parameters for the ==== model increases exponentially w.r.t. the memory depth ====: ====. To identify this model (1) we need to have a huge data set and a computational work of size ====. Note that the ==== can be equivalently represented as the ====-dimensional Markov chain of the first order, but the number of parameters in this representation remains the same. To avoid this “curse of dimensionality” the parsimonious (“small-parametric”) models of high-order Markov chains that are determined by a small number ==== of parameters ==== can be used [14], [19], [21], [22], [35].====Because of Binomial distribution adequacy to real discrete data [15] the Binomial conditional distribution ==== for ==== in (1) was used in [25], [26], where ==== is determined by some unknown parameter ====; this model is called BiCNAR (Binomial conditionally nonlinear autoregression). The BiCNAR model significantly differs from the proposed in [38] close by the title “autoregressive model for time series of Binomial counts” introduced as a probabilistic mixture of ==== lagged McKenzie’s Binomial AR(1) models: this model has a Binomial stationary distribution but the stationary distribution for BiCNAR model is a mixture of ==== Binomial distributions with different parameters. In contrast to [25], [26] the BiCNAR model in this paper is considered under outliers; explanation and practical significance of this model are given in the next section. Note that robustness in statistical analysis of discrete-valued time series is a new area of research; some review of the known results in this area (mostly for the case ====) can be found in [4], [10], [11], [13].====There are two main approaches to statistical estimation of parameters for finite-valued CTS (====) [29], [39]:====The first approach is a universal approach for cases ==== and ==== both. A shortage of these two approaches is in the known problem of numerical searching of the global maximum. We develop here an approach based on high-order Markovian properties of CTS that is free of the indicated shortage and generates statistical estimators for model parameters ==== under outliers in explicit form.====The rest of the paper has the following structure. Section 2 contains description of the considered mathematical model and problem setting. A robust estimator for the model parameters is constructed in Section 3. Section 4 is dedicated to the asymptotic properties and sensitivity analysis for the robust estimator, and also for the estimator constructed for the non-distorted model. Section 5 contains results of computer experiments illustrating theoretical results on performance and robustness of the developed estimators. Proofs section in Appendix A contains some proofs and auxiliary results. Description of an algorithm for fast recursive model extension is given in Appendix B.",Robust estimation for Binomial conditionally nonlinear autoregressive time series based on multivariate conditional frequencies,https://www.sciencedirect.com/science/article/pii/S0047259X21000555,1 June 2021,2021,Research Article,150.0
"Laverny Oskar,Masiello Esterina,Maume-Deschamps Véronique,Rullière Didier","Univ Lyon, Université Claude Bernard Lyon 1, CNRS UMR 5208, Institut Camille Jordan, F-69622 Villeurbanne, France,Mines Saint-Etienne, Univ Clermont Auvergne, CNRS, UMR 6158 LIMOS, Institut Henri Fayol, F - 42023 Saint-Etienne, France","Received 25 May 2020, Revised 22 February 2021, Accepted 14 May 2021, Available online 27 May 2021, Version of Record 8 June 2021.",https://doi.org/10.1016/j.jmva.2021.104776,Cited by (0),We construct the ,"Although the estimation of copula [15], [26], [38] is a wide-treated subject, most efficient estimators available in the literature are based on restricted, parametric estimation. Vine copulas [34], [35], [36], [37], [40], although useful in high dimensions, often use parametric models, such as Archimedean copulas, as base building blocks. On the other hand, graphical models [17], [27] assume a Gaussian dependence structure and therefore are fast but under restrictive assumptions. Classical nonparametric density estimators such as kernels [23], [45], [46], [47] or wavelets [18], [20], [33] are not suited to satisfy constraints such as the uniformity of margins (one counter-example may be found in [8], [16]). We explore here a specific class of non-parametric copula density estimators with tree-structured piecewise constant densities, and design an estimator that lies in this class, the CORT estimator.====The CORT estimator is based on the density estimation tree from [42], a tree-structured non-parametric density estimator, and on the framework of patchwork copulas from [11], [13], [14]. There already exist several other piecewise constant density estimators: the cascaded histograms of [22], the Dirichlet-based Polya tree [39], the distribution element trees by [31], the adaptive sparse grids of [41], the framework of Gaussian mixtures by [9], the Bayesian sequential partitioning techniques by [28], [29] with their interesting asymptotic consistency results, and the Wasserstein compression techniques provided by [30] are all worth noting in the field of non-parametric piecewise density estimation. But these models are built to estimate densities without taking into account uniformity of margins, and they do not always lead to proper copulas when applied on pseudo-observations or true copula samples.====The CORT estimator has the particularity of being tree-shaped which ensures on one hand that the computation of the estimated density and the distribution function on new data points is fast, and on the other hand that the storage of the model is efficient. Thus, it could be used for tasks such as re-sampling a dataset outside the already existing points, or for compression purposes, when dealing with big-data dependencies. Finally, under mild conditions, the estimator is a proper copula, where classical non-parametric estimators, such as Deheuvel’s empirical copula, are not.====This paper is organized as follows. Section 2 describes the class of piecewise linear copulas and gives some of their properties. In Section 3, we propose an estimation procedure, allowing localized dimension reduction, and we establish a convergence result for this procedure. Section 4 deals with ensemble models based on the CORT estimator: bagging techniques and out-of-bag generalization statistics are developed in the field of copula density estimation, and applied to the CORT estimator. Finally, Section 5 investigates the performance of the model by applications on some simulated examples, and Section 6 concludes.",Dependence structure estimation using Copula Recursive Trees,https://www.sciencedirect.com/science/article/pii/S0047259X21000543,27 May 2021,2021,Research Article,151.0
"Zhang Jun,Lin Bingqing,Zhou Yan","College of Mathematics and Statistics, Shenzhen University, 518060, Shenzhen, China","Received 26 August 2020, Revised 5 May 2021, Accepted 5 May 2021, Available online 17 May 2021, Version of Record 21 May 2021.",https://doi.org/10.1016/j.jmva.2021.104768,Cited by (6)," of ==== components, restricted estimators under the null hypothesis and test statistics are proposed. The ","In many applications of regression analysis, such as sociology, econometrics, and financial engineering multivariate regression models are routinely used in these areas where one predicts multiple responses with a single set of prediction variables. In this paper, we consider the following partially linear multivariate responses models ====where ==== is ====th response, ==== is ====-dimensional covariate vector and ==== is a univariate covariate. The model errors ==== are independent of ====, satisfying ====, ====. Each ==== is an unknown univariate function, ====. In model (1), we assume common effects for both ==== and ==== on different responses with identical ====.====Partial linear multivariate responses models (1) are a generalization of the classical partial linear models of regressing a single response on predictors to regressing ==== responses, and also encompasses many parametric, semiparametric and nonparametric regression models. When ==== model (1) reduces to the partially linear regression model [5], [8], [9], [10], [20], [26], [27]. For more applications and literature review of univariate partially linear models, one can refer to [5] and references therein. When ====, model (1) becomes the multivariate nonparametric regression model [19]. When ====, ====, model (1) becomes the classic multivariate linear regression model  [1], [2], [15].====Although the univariate regression model can be separately fitted to each response, joint regression models exploit correlations among responses and hence can be superior in estimation efficiency or prediction accuracy. The partial linear multivariate responses model (1) has been studied in literature, researchers usually adopted the profile least squares estimation [22] and also for the longitudinal data context [6], [11] where the parametric and nonparametric components are the same across different subjects. It is noted that the aforementioned works were successful in relaxing some or all of the distributional assumptions on the model errors, the statistical properties of profile least squares estimators usually require the condition of that the variances of model errors are all finite. To the best of our knowledge, no work has been done to relax such finite variances restrictions for parameters estimation with multivariate responses. It is thus the objective of this article to attempt to bridge this gap. The method proposed in this article makes use of multivariate kernel smoothing nonparametric techniques to estimate the unknown multivariate density function ==== of ====, which seems to be quite natural in view of the rapid development in semiparametric maximum likelihood estimators.====Suppose the multivariate density function ==== is known, the maximum likelihood is one of the best approach to estimating ====, i.e., we maximize ==== with respect to ====, where ====. The maximization of likelihood is equivalent to maximize ====. When the dimension of ==== is one, [3] proposed a kernel density based estimation by estimating one dimensional density function ==== as a first step, say ====, and then they estimated the regression coefficient in a linear regression model by maximizing ==== with respect to ====. To solve this maximum problem, [3] adopted the algorithm of [18], and this algorithm can optimize a high-dimensional nonlinear objective function with a fast computation. It is noted that the kernel density based estimation is equivalent to the smoothed score method proposed in [7], [13], [14], and the authors [7] constructed a profile-type smooth score function to make statistical inference for the univariate response partial linear varying coefficient model. Simulation studies in [3] show that the classical least squares estimator for univariate linear regression models definitely loses majority estimation efficiency when the error density function ==== is multimodal.====In this paper, we propose a kernel density based estimation for the partial linear multivariate responses model. We first obtain multivariate kernel smoothing nonparametric estimator ====, and then we maximum ==== to estimate ==== and study its root-==== asymptotic normality. At the population level, we use the residuals ==== to estimate parameter ====. The related algorithm reduces computational burden as it does not need to use the iterative procedure between the parameter ==== and nonparametric functions ====, ====. In the existing literature, the algorithm numerically solved these nonparametric functions by maximizing local multivariate kernel density estimator, coupling with the spline approximation for unknown ====, ====. The drawback of the algorithm between parameter and non-parameters is the initial values for the coefficients of splines, since these initial values are difficult to choose when the local least squares estimators are far too risky for heavy-tailed error densities, see for example [29]. The algorithm proposed in this paper only iterates the parameter itself. Moreover, the kernel density based estimator is also robust to the heavy-tailed error densities. Our simulation studies show that the proposed estimator performs much better than the profile least squares estimators when ==== is multimodal or has infinity variance. Next, we consider the problem of checking whether ==== satisfies some linear combinations or not. There is no literature to consider the hypothesis problems of parameter ==== in partial linear multivariate responses models. A restricted kernel density based estimator is proposed by introducing Lagrange multipliers under the null hypothesis. Simulation studies show that this restricted estimator can greatly improve the estimation efficiency when the null hypothesis holds true. Finally, a test statistic based on the restriction under the null hypothesis is proposed. The limiting distribution of the test statistic is shown to be a standard chi-squared distribution under the null hypothesis. In simulation studies, we conduct Monte Carlo simulation experiments to examine the performance of the proposed estimation and test procedures.====The paper is structured as follows. In Section 2, we propose the kernel density based estimator of the parameter, and derive related asymptotic results. A test statistic is therefore proposed for testing the hypothesis. In Section 3, we report the results of simulation studies. Section 4 is devoted to the analysis of the energy efficiency data. All technical proofs of the asymptotic results are given in the Appendix.",Kernel density estimation for partial linear multivariate responses models,https://www.sciencedirect.com/science/article/pii/S0047259X21000464,17 May 2021,2021,Research Article,152.0
"Hielscher Ralf,Lippert Laura","Faculty of Mathematics, Chemnitz Univesity of Technology, Germany","Received 17 July 2020, Revised 15 January 2021, Accepted 28 April 2021, Available online 13 May 2021, Version of Record 13 May 2021.",https://doi.org/10.1016/j.jmva.2021.104764,Cited by (2),The analysis of manifold-valued data using embedding based methods is linked to the problem of finding suitable embeddings. In this paper we are interested in embeddings of quotient manifolds ,"In the analysis of manifold-valued data there are two different approaches — intrinsic and extrinsic. Intrinsic methods solely rely on intrinsic properties of the manifold, e.g. the Riemannian curvature tensor, the exponential map or the Levi-Cevita connection. Those methods often work locally like moving least squares [10], multiscale methods [20] or subdivision schemes [25]. Other intrinsic approaches make use of function systems that are adapted to the geometry of the manifold, e.g. diffusion maps [5] or the eigenfunctions of the manifold Laplacian [11], [12], [14], [15], [19].====On the other hand, extrinsic methods rely on an embedding of the manifold into some higher dimensional vector space [2], [7], [22]. The advantage of embedding-based methods, compared to intrinsic methods, is that they often are straight forward generalizations of the corresponding linear methods. The central challenges for applying an embedding-based method to a specific manifold ==== are====In our paper we are concerned with the specific case when the manifold ==== is the quotient ==== of the rotational group ==== with respect to some finite symmetry group ====. Here the cosets in the quotient space are defined by ====. As a finite subgroup of ==== the symmetry group ==== is isomorphic to one of the following: the cyclic groups ==== for ====, the dihedral groups ==== for ====, the tetrahedral group ====, the octahedral group ==== and the icosahedral group ====. Since the group ==== is simple, the quotient ==== is not a group for all ==== but forms a homogeneous space with canonical left action of the Lie group ====.====To give the reader an idea about the quotient ==== we consider the representation of a rotation ==== as the composition of rotations about the axes ====, ====, ==== and Euler angles ====, ====. Let us furthermore assume that the subgroup ==== is represented by the rotations ====, ==== about the ====-axis. Then ==== enforces a periodicity of ==== on the last Euler angle ==== and the cosets in ==== are of the form ====Nice geometrical visualizations of these coset spaces can be found in  [16].====The analysis of data that are cosets ==== in the homogeneous space ==== is of central importance in various scientific areas. For instance, they are used to describe the alignment of crystals in crystallography, material science and geology [1], [4], [8], the alignment of molecules and proteins in biochemistry [3] or movements in robotics [26] and motion tracking [21].====Since, locally, the quotient manifolds ==== are isometric to the rotation group ==== itself all intrinsic methods for the rotation group can be easily adapted to work on the quotients as well. Unfortunately, this is not true for embedding based-methods, e.g. for the interpolation methods described in [9]. Explicit embeddings for the quotient manifolds ==== have been investigated first by R. Arnold, P. Jupp and H. Schaeben in [2]. Our paper aims to extend their results by developing a general framework for the construction of embeddings of the quotient manifolds ==== that include the embeddings described in [2]. Our embeddings pose several nice properties, e.g. they are all ====-equivariant (see Definition 3), their images are contained in a sphere and the image measure ==== induced by the rotational invariant measure ==== on ==== is centered in ====, i.e., has zero mean. Furthermore, we find within our framework locally isometric embeddings of ==== for all finite symmetry groups ==== and provide an efficient numerical method for the projection ====. The practical advantage of isometric embeddings is that locally isotropic methods in ==== translates into locally isotropic methods on ====.====Our paper is organized as follows. In Section 2.1 we introduce the generic embeddings and prove in Theorem 4 and Corollary 5 that they are ====-equivariant maps that map the quotient manifold into a subsphere of an Euclidean vector space. Furthermore, we provide in Table 1 the parameters such that our embeddings coincide with the embeddings found in [2]. In Section 2.2 we investigate rotational invariant subspaces of ==== and show in Theorem 9 that the embeddings can be centered such that their image is contained in a linear subspace of ==== which allows us to reduce the effective dimension of the embedding. In Section 2.3 we consider the rotational invariant Haar measure ==== on ==== and generalize it to a left invariant measure ==== on ====. Together with an embedding ==== this induces an image measure on ====. In Theorem 10 we show that the centered embeddings from Section 2.3 result in centered image measures. Finally, we propose in Section 2.4 an iterative algorithm for the numerical computation of the projection ==== of an arbitrary point in some neighborhood of the manifold back to the manifold. To this end, we derive in Theorem 12 the gradient of the distance functional.====In Section 3 we are interested in the discrepancy between the geodesic distance on the quotient manifold and the Euclidean distance in the embedding. A smooth embedding into ====, such that the pull back of the Euclidean metric tensor coincides with the metric tensor of the manifold, is called isometric. According to the Nash embedding theorem [18], there exists for every ====-dimensional Riemannian manifold an isometric embedding into ====. As all our quotient manifolds are three-dimensional the result guaranties the existence of an isometric embedding into the space ====. It turns out that our embeddings are sufficiently general to include locally isometric embeddings for the quotient manifolds ==== modulo all crystallographic symmetry groups ====. This result is proven separately for the different types of symmetry groups in Theorems 15, 18, 19, 20, 21, 22. The corresponding parameters as well as the dimension of the linear space are summarized in Table 2. The dimensions of the locally isometric embeddings vary from ==== to ==== depending on the symmetry group.====In Section 3.2 we investigate the global relationship between the geodesic distance on ==== and the Euclidean distance in the embedding. According to [24] it is possible to construct for each smooth and compact manifold ==== an embedding ==== such that the geodesic distance on the manifold and the Euclidean distance in the embedding differ only by a given ====, i.e., ====However, the dimension ==== of the vector space required for such an embedding is much too large for numerical applications. In Table 3 we provide similar bounds to those in (1) for the locally isometric embeddings defined in this paper. It turns out that locally isometric embeddings do not necessarily lead to globally optimal bounds. Parameters for our embeddings optimized with respect to global preservation of distances are provided in Table 4.",Locally isometric embeddings of quotients of the rotation group modulo finite symmetries,https://www.sciencedirect.com/science/article/pii/S0047259X21000427,13 May 2021,2021,Research Article,153.0
Zhou Zhiyang,"Department of Preventive Medicine, Northwestern University Feinberg School of Medicine, Chicago, IL 60611, United States","Received 17 December 2020, Revised 5 May 2021, Accepted 6 May 2021, Available online 12 May 2021, Version of Record 18 May 2021.",https://doi.org/10.1016/j.jmva.2021.104769,Cited by (2),. It is highlighted that our proposal is competitive in terms of both estimation and prediction accuracy but consumes much less execution time.,"In recent years, the functional data analysis has enjoyed a rapid development, due to the growing demands of digging out the rich information from complicated data structures like trajectories and images. As a fundamental model in functional data analysis, the function-on-function regression (FoFR, arguably first proposed by Ramsay and Dalzell [31]) is a generalization of multivariate regression with the coefficient vector evolving into a bivariate coefficient function. It helps to model the relationship between two stochastic processes.====Excellent contributions have been made to the investigation into FoFR. Among others, Cuevas et al. [8] estimated the coefficient function by interpolation. [11], [12] utilized the Nadaraya–Watson estimator in predicting the conditional expectation of response. A more prevailing stream of fitting FoFR, as documented in monographs like ([20], Section 8.3; [32], Chapter 16), is to reduce the intrinsically infinite dimension of coefficient function by focusing on a pre-determined lower dimensional space. There are various candidates for such a space: It can be a linear space spanned by wavelets, orthogonal polynomials, penalized splines, etc.; a more recent option is the reproducing kernel Hilbert space employed by [25], [35]. Nevertheless, one may prefer a data-driven strategy named the functional principal component regression (FPCR): It constructs the lower dimensional space from leading eigenfunctions of auto-covariance operators. Least-squares-type projections onto such a space were proposed by [5], [6], [43] as estimators for coefficient function; Wang [37] made the projection through random effect models. Incorporating a regularization into [6], Benatia et al. [3] enabled FPCR even for FoFR with ill-posed auto-covariance operators. It is known that FPCR fails to involve the correlation between response and predictor in truncating the Karhunen–Loève series, resulting in a possible loss in accuracy. A remedy for this point is the functional partial least squares (FPLS). FPLS is a terminology shared by a series of algorithms. At least three of them are applicable to FoFR: [4] included respective extensions of the nonlinear iterative PLS (NIPALS, [39]) and the statistically inspired modification of PLS (SIMPLS, [22]), both initially designed for the multivariate context. SigComp [26] sequentially maximizes penalized Rayleigh quotients subject to constraints on normalization and orthogonality. Although these three FPLS algorithms have shown their own numerical advantages, they all have to solve iterative eigen-problems which may take time.====Our work chooses to constrain the estimator to a subspace named after (Alexei) Krylov. This idea expands the alternative partial least squares (APLS, [9], an FPLS algorithm designed for the scalar-on-function regression) and is abbreviated as fAPLS with initial “f” emphasizing the application to functional response. In the context of scalar-on-function regression, APLS is equivalent to NIPALS and SIMPLS. Though this equivalency is unlikely to hold for FoFR, our fAPLS is still expected to have little difference with NIPALS or SIMPLS in terms of accuracy. Meanwhile, involving no eigen-problem, fAPLS would inherit multiple features, e.g., closed-form estimators and less running time, from APLS.====The remaining portion of this paper is organized as below. After clarifying the model settings, Section 2 presents two equivalent expressions of fAPLS estimator. They facilitate the empirical implementation and theoretical derivation, respectively. In Section 3 fAPLS is compared with competitors under distinct simulation scenarios. Its performance is evaluated in terms of both accuracy and execution time. We apply fAPLS to two real-world datasets in Section 4. fAPLS is adaptable to more complex settings, e.g., sparsely observed predictors, correlated subjects or non-linear modeling; we include corresponding discussions in Section 5. More theoretical and computational details, e.g., assumptions, proofs and code trunks, are relegated to the Appendix for conciseness.",Fast implementation of partial least squares for function-on-function regression,https://www.sciencedirect.com/science/article/pii/S0047259X21000476,12 May 2021,2021,Research Article,154.0
"Nowak Claus P.,Konietschke Frank","Charité – Universitätsmedizin Berlin, corporate member of Freie Universität Berlin and Humboldt-Universität zu Berlin, Institute of Biometry and Clinical Epidemiology, Charitéplatz 1, 10117 Berlin, Germany,Berlin Institute of Health at Charité – Universitätsmedizin Berlin, Charitéplatz 1, 10117 Berlin, Germany,TU Dortmund University, Faculty of Statistics, Dortmund, Germany","Received 16 June 2020, Revised 4 May 2021, Accepted 4 May 2021, Available online 8 May 2021, Version of Record 18 May 2021.",https://doi.org/10.1016/j.jmva.2021.104767,Cited by (3)," and ====, Somers’ ====, and Goodman and Kruskal’s ====. Simulation studies for a range of scenarios indicate that the proposed methods control the family wise error rate in the strong sense even when sample sizes are rather small. A case study on the iris flower data set demonstrates how to perform inference in practice.","A practical statistician is frequently asked to examine whether or how strongly the components of independent and identically distributed pairs of variables ====, ==== are related. Statistical problems of this kind may arise in many scientific fields, be it in the context of multivariate data, repeated measures designs, or generalized mixed models. We assume for the moment that ==== is continuous, but we will relax this assumption in the next section. In case of a linear relationship between ==== and ====, Pearson’s ====is usually assessed. In many situations, however, the distributions as well as the specific kind of relationship of the variables ==== and ==== such as linear or monotonic are unknown and indeed, if ====, the event ==== is usually more likely than ====, and vice versa if ==== [12], [14]. A purely nonparametric measure of association sensitive to monotonicity is ====which is half the probability of concordance of randomly selected pairs ==== and ====. A very well known correlation coefficient computing the probability of concordance minus the probability of discordance is Kendall’s ====and ==== according as ====, ====, ==== respectively [11], [14]. If the distributions are continuous, then Kendall’s ==== is nothing but a linear transformation of ==== onto ==== by ====see [6], [19], [22] among others. Discrete data and ties often occur in practice so we introduce a modified definition of ==== adjusted for ties in such a way that the linear relationship between ==== and the modified ==== as in (3) still holds. In case of ties, however, other versions such as Kendall’s ==== [12], [14], Somers’ ==== [24], or Goodman and Kruskal’s ==== [7], ====might be preferable. If ==== is continuous, then ====, otherwise the correlation measures in (4) are scaled versions of ====. The intuition behind ==== and ==== is that a variable correlated with itself, i.e., ====, should return a value of 1, while the definition of ==== is to mimic the slope parameter in a simple linear regression model. As with ====, we will regard the additional terms in (4), i.e., ====, ====, and ==== as linear transformations of probabilities, which are key to our estimation and asymptotic theory. Consequently, we refer to them as ====-probabilities in this paper.====Popular asymptotic inference methods [11], [12] allow for testing the null hypothesis of no association, with results on the asymptotic sampling distribution only valid under the null scenario. Thus confidence limits are often not reported. In that regard, we propose an estimator of the asymptotic variance consistent irrespective of the form of the marginal distributions and the strength of the true association. Moreover, in many situations, more than one correlation coefficient is of interest and one may wish to employ simultaneous inference procedures for several measures obtained from the same experimental units or compare several correlation coefficients for various endpoints under different groups. For instance, one may want to analyse whether the association of a disease progression score and a subjective pain scale is stronger in men than in women, or whether this association changes over time. For this purpose, we derive the unconditional asymptotic joint sampling distribution of various empirical correlation coefficients under the null and arbitrary alternatives. These results form the basis to develop multiple contrast tests that take the correlation across different test statistics into account and control the family wise error rate in the strong sense.====The remainder of this paper is organized as follows. In Section 2, we discuss ====-probabilities and propose consistent point estimators along with their asymptotic distribution in Section 3. Section 4 develops simultaneous test procedures, whose empirical behaviour we analyse with the aid of simulations in Section 5. Section 6 illustrates the practical application of the tests using the ==== flower data set as provided in ====. Finally, we discuss our results in Section 7. Proofs not presented in the main text are given thereafter. ==== scripts are provided as online supplementary material.",Simultaneous inference for Kendall’s tau,https://www.sciencedirect.com/science/article/pii/S0047259X21000452,8 May 2021,2021,Research Article,155.0
"Mirshani Ardalan,Reimherr Matthew","Department of Statistics, The Pennsylvania State University, University Park, PA, USA","Received 4 February 2020, Revised 27 April 2021, Accepted 27 April 2021, Available online 7 May 2021, Version of Record 12 May 2021.",https://doi.org/10.1016/j.jmva.2021.104765,Cited by (1),", while parameters lie in a subspace known as a Cameron–Martin space, ","In recent years, rapid advances in data gathering technologies and new complex modern studies have presented substantial challenges for the extraction of information from increasingly large and sophisticated data sets. Functional Data Analysis (FDA) is a branch of statistics for that conducts statistical inferences on data consisting of functions, images, surfaces, and similar objects [23], [27], [32]. In addition, the emergence of inexpensive genotyping technologies has produced a substantial need for tools capable of handling large numbers of scalar predictors [2], [9], [14], [22], [34], [35]. In this paper, we consider the function-on-scalar regression problem when the number of predictors is much larger than the number of subjects/units. We present a new approach, called Adaptive Function-on-Scalar Smoothing Elastic Net (AFSSEN), that can separately control the smoothness of the underlying functional parameter estimates as well as select important predictors.====As in classic statistical analyses, the functional linear model (FLM) is one of the principle modeling tools when working with functional data [30]. In these cases, either the outcome or the predictors are functional [33]. When the number of predictors is fixed, then techniques for fitting FLM and their statistical properties are now well understood [27], [30] in low dimensional FLM. However, in high-dimensional cases where the number of predictors is larger than the number of statistical units, comparatively little work has been done. Most previous research has focused on scalar-on-function models [18], [21], [28], [29].====In function-on-scalar regression, which is the problem we are considering, Chen et al. [13] considered a functional least squares with a Minimax Concave Penalty (MCP) [39] with a fixed number of predictors. A pre-whitening technique was used to exploit the within function dependence of the outcomes. Barber et al. [4] presented the Function-on-Scalar LASSO, FSL, which combines a functional least squares with an ==== penalty introduced in a separable Hilbert Space ====. Since FSL is a convex optimization problem, it is computationally efficient even with a large number of predictors (====). Additionally, FSL estimates achieve optimal convergence rates, but as with traditional LASSO, these estimates suffer from an asymptotic bias and do not achieve the functional oracle property. Fan and Reimherr [19] introduced the Adaptive Function-on-Scalar LASSO, AFSL, which uses a functional least squares with an adaptive ==== penalty to reduce the bias problem in FSL. They showed that AFSL is computationally as efficient as FSL but achieves a strong functional oracle property. However, AFSL provides limited control of the smoothness of the functional parameter estimates. Parodi and Reimherr [31] developed the Functional Linear Adaptive Mixed Estimation (FLAME), which simultaneously selects important predictors and estimates the smooth parameters. They assume that while the data lie in a general real separable Hilbert space, ====, the model parameters lie in a Reproducing Kernel Hilbert Space (RKHS), ====. The RKHS is a subspace of ====, which can be identified with a linear operator, ====. They demonstrated that FLAME achieved a weak functional oracle property, meaning it recovered the correct support with the probability tending to one and was equivalent to the oracle estimator only on certain nice projections. To show that FLAME achieved the strong oracle property required stronger structural assumptions. In their framework, they used a coordinate descent algorithm which made it computationally efficient.====A key hurdle for FLAME is having to simultaneously control the smoothness and sparsity with a single penalty and tuning parameter. In particular, a tuning parameter value that practically works well for smoothing may not work well for variable selection, and vice versa. To address this issue, we propose a method that more carefully controls smoothing and sparsity separately. We assume the data live in an arbitrary Hilbert space ====, but that some linear constraints of the parameters are forced to lie in a Cameron–Martin space (CMS), ====. CMS are closely related to Reproducing Kernel Hilbert Spaces (RKHS) when ====, and the two terms are often used interchangeably [10]. However, since our ==== will be more general, we refrain from using the term RKHS to avoid confusion. In our approach, we use an idea similar to the scalar adaptive elastic net penalty [41], [42], but for functional data. In particular, AFSSEN exploits a combination of a penalized functional least squares and an adaptive smoothing elastic net penalty containing an ==== term in ==== for variable selection and a separate ==== term in ==== for controlling the smoothness of the estimated parameters. The AFSSEN parameter estimates inherit properties from the kernel function of ====, such as smoothness or periodicity. This creates additional technical hurdles when developing asymptotic theory as there are now two complex sources of bias that must be carefully controlled. Despite this, we are able to show that AFSSEN enjoys better mathematical properties than AFSL or FLAME, even when relaxing the Gaussian error assumption to ====-sub-Gaussian [3], [12]. In particular, we show that AFSSEN achieves a strong oracle property in both ==== and the stronger norm ====.====To understand the practical impact of this work we refer interested readers to Craig et al. [14] and Craig et al. [15] to see a deep application of these tools to childhood obesity with a longitudinally measured phenotype alongside tens of microbiome measurements and hundreds of thousands of SNPs. We also provide a fast coordinate descent algorithm in the ==== programming language [1], whose backend is written in ==== via the ==== package [17].====The remainder of the paper is organized as follows. In Section 2, we provide the necessary background material and introduce our methodology. Section 3 provides the assumptions and main theoretical results. In Section 4, we provide a numerical implementation via a fast coordinate descent algorithm and discuss some practical considerations. A simulation study comparing the performance of AFSSEN and FLAME in two different smooth and rough scenarios is given in Section 5. We provide concluding remarks in Section 6. All theoretical results are proven in the Appendix.",Adaptive function-on-scalar regression with a smoothing elastic net,https://www.sciencedirect.com/science/article/pii/S0047259X21000439,7 May 2021,2021,Research Article,156.0
"Nanmo Hisayoshi,Kuroki Manabu","Department of Mathematical Science, Yokohama National University, 79-1 Tokiwadai, Hodogaya-ku, Yokohama 240-8501, Japan","Received 9 December 2020, Revised 28 April 2021, Accepted 28 April 2021, Available online 7 May 2021, Version of Record 17 May 2021.",https://doi.org/10.1016/j.jmva.2021.104766,Cited by (2),"In this paper, we assume that cause–effect relationships among variables can be represented by a Gaussian linear ==== and a corresponding directed acyclic graph. For a set of intermediate variables that satisfies the front-door criterion, we provide the variance formula of the estimated mean outcome under an external intervention in which a treatment variable is set to a specified constant value. The variance formula proposed in this paper is exact, in contrast to those in most previous studies on estimating total effects. In addition, based on the variance formula, we formulate the mean squared error between a future sample and the estimated mean outcome with the external intervention.",None,Exact variance formula for the estimated mean outcome with external intervention based on the front-door criterion in Gaussian linear structural equation models,https://www.sciencedirect.com/science/article/pii/S0047259X21000440,7 May 2021,2021,Research Article,157.0
"Bhattacharya Indrabati,Ghosal Subhashis","Department of Biostatistics and Computational Biology, University of Rochester, United States of America,Department of Statistics, North Carolina State University, United States of America","Received 7 October 2020, Revised 23 April 2021, Accepted 23 April 2021, Available online 30 April 2021, Version of Record 14 May 2021.",https://doi.org/10.1016/j.jmva.2021.104763,Cited by (1)," regression. The proposed approach involves modeling of related conditional distributions of a response vector given the ==== for estimating the model parameters. We illustrate our method with simulation studies, and a data containing blood pressures of 40 women. Finally, we provide a theoretical justification for the proposed method through posterior consistency and support properties of the prior.","Quantile regression, a popular alternative to the usual mean regression, models the relationship between the predictor and a specific quantile of the response. The quantiles give a more complete picture of the distribution of the response than the mean, and they are more robust than the method of moments estimators. Univariate linear quantile regression was first proposed by Koenker and Bassett Jr [20] and was extensively studied in the literature since then. Given covariate ====, for some ====, the ====th linear quantile regression model for the response ==== can be written as ====, for ====. Based on a sample ====, Koenker and Bassett Jr [20] estimated the regression coefficient ==== by the estimator ====with ====, where ==== is the indicator function. Fast algorithms to compute ==== were obtained in the literature, and an R package called ==== is available as well.====Yu and Moyeed [38] proposed a parametric Bayesian approach to the univariate quantile regression by assuming an asymmetric Laplace likelihood. This parametric Bayesian approach has been widely used since then [12], [22], [24], [32], [39]. However, the asymmetric Laplace assumption can be violated, just like a normality assumption may be violated. These violations may lead to model misspecification issues. To remedy this, more flexible semiparametric and nonparametric methods have been proposed in the literature. Kottas and Krnjajić [21] developed a Bayesian semiparametric modeling approach using Dirichlet Process Mixtures (DPM). Hjort and Petrone [16] and Hjort and Walker [17] also developed nonparametric quantile regression methods based on Dirichlet Process (DP) and quantile pyramid priors. These methods are based on flexible modeling of the error distributions, but the proposed quantile functions are linear. More general methods involve DPM modeling with covariate-dependent weights [27]. Taddy and Kottas [31] proposed to model the joint distribution of the response and covariates using a DPM, which induces a posterior on the quantile curves of the conditional distribution of the response given covariates. Simultaneous quantile regression on a one-dimensional predictor was considered by Tokdar and Kadane [33], and Das and Ghosal [7]. Reich et al. [26] and Yang and Tokdar [37] proposed Bayesian methods for estimating non-crossing quantile planes for multidimensional predictors.====The literature on the multivariate quantile regression is a little limited compared to that of the univariate quantile regression. There is no unique definition of quantiles in higher dimension due to the lack of a natural ordering in Euclidean space of higher dimension. Many different notions of multivariate quantiles have been proposed and studied over the years. Chaudhuri [5] introduced the notion of geometric quantiles which arises as a natural generalization of the spatial median [29], and Chakraborty [4] extended this idea to a regression framework. There are various other ways to define a multivariate quantile [28]. Hallin et al. [15] introduced the notion of directional quantiles for multivariate location and multiple-output regression problems. The Bayesian literature on multivariate quantiles seems limited; only a few papers exist to the authors’ knowledge. Waldmann and Kneib [35] considered bivariate quantile regression using a multivariate asymmetric Laplace likelihood, while Drovandi and Pettitt [9] used a copula approach. Recently, Guggisberg [14] proposed a Bayesian approach to the directional quantile framework developed in Hallin et al. [15]. Also, Chernozhukov et al. [6] introduced a new concept of data-depth, called the Monge–Kantorovich depth, which resulted in a new definition of multivariate quantiles as well. So far, Bayesian regression of geometric quantiles has not been considered in the literature, and here we address that.",Bayesian multivariate quantile regression using Dependent Dirichlet Process prior,https://www.sciencedirect.com/science/article/pii/S0047259X21000415,30 April 2021,2021,Research Article,158.0
"Xu Kai,Tian Yan,He Daojiang","School of Mathematics and Statistics, Anhui Normal University, Wuhu 241002, China","Received 25 May 2020, Revised 13 April 2021, Accepted 13 April 2021, Available online 23 April 2021, Version of Record 23 April 2021.",https://doi.org/10.1016/j.jmva.2021.104762,Cited by (1)," is greater than the sample size ====, namely the “large ====, small ","Let ==== and ==== be independent ====-dimensional random vectors with mean vectors ==== and ====, and covariance matrices ==== and ====, which are positive definite. Let ==== with ==== and ==== with ==== be independent and identically distributed copies of ==== and ====, respectively. In this article, we consider a high dimensional nonparametric procedure for testing the hypothesis ====Testing the structure of covariance matrices is one of the most fundamental problems in the theory of multivariate statistical analysis. The particular structure (1) that we consider in this paper is proportional covariance matrices and belongs to a classical problem in two-sample hypothesis testing [16]. The proportionality test of (1) is widely used in some important applications such as discriminant analysis [9], multivariate Behrens–Fisher problem [19] and principal component analysis [22].====There is a rich literature on testing the proportionality of two covariance matrices. To our knowledge, [7] was the first article to research on the proportionality of covariance matrices. This author suggested an approach based on a likelihood ratio test essentially for dimension ====. Subsequently, the publications related to testing the proportionality of covariance matrices were by [6], [8] and [21]. These authors considered the maximum likelihood approach in more general cases. In particular, [6] considered a Bartlett adjusted likelihood ratio test (LRT) for samples from ==== groups of normal populations with any fixed dimension, and proved that the test statistic has an asymptotic chi-square distribution with ==== degrees of freedom in the conventional “fixed dimension, large sample size” setting. Let ==== and ==== denote the estimators of ==== and ==== based on the joint sufficient statistics ==== and obtained by the iterative algorithm in Theorem 3.2 of [6]. For ==== and under the null (1), the statement is that ====where ==== denotes convergence in distribution, ====, ====, ====, sample covariance matrices ==== and ==== are defined as ==== and ==== with ==== and ==== and ==== is the Bartlett adjusted factor. To accommodate nonnormal multivariate data, [23] suggested a Wald test when sampling from any ==== populations with fixed dimensions as long as each population has finite fourth moments. Specifically, the Wald test statistic for ==== is given by ====where ==== and ==== for ====, ====, ==== for ====. Here, ==== and ==== for two matrices ==== and ====. Theorem 3 in [23] indicated that under ====, ==== also has the asymptotic chi-square distribution with ==== degrees of freedom when the dimension ==== is fixed and sample sizes ==== both go to infinity.====A prominent feature of data collection nowadays is that the number of variables is comparable with the sample size. Testing the hypothesis in (1) becomes very challenging for high dimensional data because traditional multivariate approaches aforementioned earlier do not necessarily work, which were established for the case of the sample size tending to infinity and the dimension remaining fixed. To accommodate moderately high dimensional data, several tests based on modern random matrix theory have been proposed in the literature. Noticing that the hypothesis (1) is equivalent to the hypothesis ==== and borrowing the successful idea from [1], [27] proposed a pseudo-likelihood ratio test based on the test statistic given by ====The asymptotic normality property for ==== was established for two different populations with finite fourth-moments as ==== together with the ratios ==== and ====. In other words, ==== allows moderately high dimensional data with ====. In order to make a test adaptable to a wide range of moderately high dimensionality with ==== and based on the work by [13] and [15], [18] noticed that the hypothesis (1) is also equivalent to the hypothesis ====, and considered the following test statistic ====The asymptotic normality property for ==== was proved under ==== with ==== and ==== and can be available without normal population assumptions. In summary, ==== and ==== can be viewed as the corrected LRT and Wald tests respectively, and are designed for the “moderate ====, large ====” situations. However, they are no longer applicable in the setting ====. Consequently, it is challenging to test the proportionality test of (1) in the “large ====, small ====” paradigm.====Recently, [5] developed a high dimensional nonparametric test that accommodates the “large ====, small ====” setting via testing the equality of two high dimensional spatial sign covariance matrices based on the Frobenius norm of the difference between two spatial sign covariance matrices. For a given vector ====, its spatial sign function is defined as ==== if ==== and ==== if ====, where ==== denotes the Euclidean norm. The test suggested by [5] is based on the statistic ====where ====, ==== and ==== and ==== are the spatial median estimators [11], [17] of ==== and ====, respectively. The asymptotic normality of ==== was justified in high dimension, which can allow the case of large dimension and small sample sizes. Despite its usefulness, the CLPZZ test has two limitations. One is that its validity is established under ====. The issue preventing ==== from growing faster than ==== is that a higher-order expansion is required for estimating bias correction. The other is that it requires estimation of masses of nuisance parameters ==== and ====, which can adversely affect its performances under high dimensionality. On the other hand, the null hypothesis (1) holds if and only if ====. More recently, based on this fact, [26] introduced a test statistic for testing (1) by considering ==== as a scaled distance measure. The test statistic they use is ====where ====, ====, ====, ==== with ====, and ====. It is worth emphasizing that the TM test can accommodate situations where the data dimension is much larger than the sample size. Nevertheless, the TM test also has two limitations, as mentioned in the work by [26]. One is the normality assumption and the other is that its limiting behavior was derived with the following restriction on the dimensionality and the sample size, namely, ==== with ====, where ==== means that there exist constants ==== such that ==== for all sufficiently large ====.====It is very urgent to develop a high dimensional method that is robust against both the data dimension and the data distribution, to deal with the proportionality test of (1). In this paper, we shall propose such a test by developing a bias correction to the TM test statistic. The newly high dimensional test for the hypothesis (1) is applicable without the normality assumption and without specifying an explicit relationship between ==== and ====. The underlying reason for such accommodation is to borrow the successful idea from [3] and [4] to obtain more accurate and reliable estimators of ====, ====, ====, ====, ==== and ==== under both nonnormality and high data dimensionality.====The article is organized as follows. Section 2 introduces the estimators, the new test statistic and the assumptions. Section 3 gives the asymptotic normality of the suggested test statistic and discusses power property of the proposed test. Simulation studies are carried out in Section 4 to investigate the numerical performance of our proposal. All technical details are relegated to the Appendix.",A high dimensional nonparametric test for proportional covariance matrices,https://www.sciencedirect.com/science/article/pii/S0047259X21000403,23 April 2021,2021,Research Article,159.0
"Huang Shih-Hao,Huang Su-Yun","Department of Mathematics, National Central University, Taiwan,Institute of Statistical Science, Academia Sinica, Taiwan","Received 17 July 2020, Revised 5 April 2021, Accepted 8 April 2021, Available online 18 April 2021, Version of Record 24 April 2021.",https://doi.org/10.1016/j.jmva.2021.104761,Cited by (1),"Dimension reduction methods for matrix or tensor data have been an active research field in recent years. Li et al. (2010) introduced the notion of the Kronecker envelope and proposed dimension folding estimators for supervised dimension reduction. In a data analysis of cryogenic electron microscopy (cryo-EM) images (Chen et al., 2014), Kronecker envelope principal component analysis (PCA) was used to reduce the dimension of cryo-EM images. Kronecker envelope PCA is a two-step procedure, which consists of projecting data onto a multilinear envelope subspace as the first step, followed by ordinary PCA on the projected core tensor. The multilinear envelope subspace preserves the natural ","Dimension reduction methods for matrix or tensor data have been an active research field in recent years. Li et al. [6] introduced the notion of the Kronecker envelope and proposed three dimension folding estimators for supervised dimension reduction, called folded SIR, folded SAVE and folded DR. For unsupervised dimension reduction, principal component analysis (PCA) has long been one of the most fundamental and widely applied tools for data reduction. For multiway arrays (also known as tensors), multilinear PCA has been proposed and studied for data reduction [5], [7], [10]. Assume that ====where ==== is the input instance, ==== is the grand mean, ==== and ==== are orthogonal matrices, ==== is a random core matrix, and ==== and ====, ====, are stochastically independent. Under model (1), we have ====, which can be re-expressed as ====where ==== and ====. Here we denote the projections ==== and ==== by ==== and ====, respectively, and denote ==== and ====. Further factorize ==== into ====, where ==== is the diagonal matrix of positive and distinct eigenvalues ====, and ==== consists of eigenvectors. We have ====Denote ==== and ====, i.e., ==== are the leading principal component (PC) directions and ==== are associated eigenvalues indicating the variations on the corresponding directions.====In sample level with data ====, the sample covariance matrix is given by ====We can directly estimate ==== and ==== by the ==== eigenvector and eigenvalue of the sample covariance matrix, respectively, which are the ordinary PCA estimators, and are denoted by ==== Alternatively, we can estimate ==== and ==== in a two-step procedure. First, estimate ====, see for example, Hung et al. [5], which spans the Kronecker envelope subspace. Next, project data onto this subspace and form the core tensor. Then, carry out ordinary PCA on the core tensor. Therefore, we have ==== where ==== is the ==== eigenvector of the projected sample covariance ====.",On the asymptotic normality and efficiency of Kronecker envelope principal component analysis,https://www.sciencedirect.com/science/article/pii/S0047259X21000397,18 April 2021,2021,Research Article,160.0
"Beck Nicholas,Di Bernardino Elena,Mailhot Mélina","Department of Decision Sciences, HEC Montréal, 3000 chemin de la Côte-Sainte-Catherine, Montréal (QC), Canada H3T 2A7,Laboratoire J.A. Dieudonné, UMR CNRS 7351, Université Côte d’Azur, Parc Valrose, 06108 Nice, Cedex 2 France,Department of Mathematics and Statistics, Concordia University, 1455 De Maisonneuve Blvd. W., Montréal (QC), Canada H3G 1M8","Received 1 July 2020, Revised 15 March 2021, Accepted 3 April 2021, Available online 18 April 2021, Version of Record 28 April 2021.",https://doi.org/10.1016/j.jmva.2021.104758,Cited by (0),"This paper focuses on semi-parametric estimation of multivariate expectiles for extreme levels of risk. Multivariate expectiles and their extremes have been the focus of plentiful research in recent years. In particular, it has been noted that due to the difficulty in estimating these values for elevated levels of risk, an alternative formulation of the underlying optimization problem would be necessary. However, in such a scenario, estimators have only been provided for the limiting cases of tail dependence: independence and comonotonicity. In this paper, we extend the estimation of multivariate extreme expectiles (MEEs) by providing a consistent estimation scheme for random vectors with any arbitrary ","For more information on the concept of regular variation and how it relates to MEEs, see Section 1 of Maume-Deschamps et al. [42]. Drawing further from Maume-Deschamps et al. [42], the focus here is on marginal distributions with asymptotically equivalent tails. This can be stated in the following assumption.",Semi-parametric estimation of multivariate extreme expectiles,https://www.sciencedirect.com/science/article/pii/S0047259X21000361,18 April 2021,2021,Research Article,161.0
"Li Mengyan,Li Runze,Ma Yanyuan","Department of Mathematical Sciences, Bentley University, Waltham, MA 02452, USA,Department of Statistics, Pennsylvania State University, University Park, PA 16802, USA","Received 13 October 2020, Revised 6 April 2021, Accepted 7 April 2021, Available online 16 April 2021, Version of Record 30 April 2021.",https://doi.org/10.1016/j.jmva.2021.104759,Cited by (5),For a high dimensional linear model with a finite number of ==== can be constructed. The finite-sample performance of the proposed inference procedure is examined through simulation studies. We further illustrate the proposed procedure via an empirical analysis of the real data example mentioned above.,"High dimensional data become more and more common in diverse fields such as computational biology, economics and climate science. Many statistical procedures have been developed for analysis of high dimensional data. However, most of them often assume that all covariates are measured accurately. In reality, measurement errors are ubiquitous in many high dimensional problems, for example, measurements of gene expression with cDNA or oligonucleotide arrays [18] and sensor network data [20]. This work was motivated by an empirical analysis of a real data example in Section 4.2, where both low dimensional phenotypic variables and high dimensional genotypic variables, single nucleotide polymorphisms (SNPs), are available. One of the phenotypic variables, the average asthma symptoms as recorded in daily diary cards by patients, is of clinical interest with multiple measurements available but usually measured with errors. The high dimensional SNPs are usually assumed to be measured accurately in the literature since each SNP has three possible levels and it is expensive to get repeated/multiple measurements.====The classical measurement error models, where the number of covariates ==== is fixed or is smaller than the sample size ====, have been studied systematically, see [5], [10], [16], [23]. Penalized methods have been developed for high dimensional linear measurement error models with ====. With measurement errors, [19] showed that the true selection is likely to be outside of the feasible set of the Dantzig selector. [21] analyzed the impact of measurement error on the standard Lasso and showed that treating the error-prone surrogates as the latent unobserved covariates leads to erroneous results. To correct the bias caused by measurement errors, a corrected penalized least squares type of loss function has been proposed, which is no longer convex in high dimensional settings. Research has been conducted to overcome the difficulties caused by the non-convexity, see [2], [6], [8], [14], [15].====The aforementioned works focus on the theory and numerical algorithms of regularization methods rather than the statistical inference. It is important to quantify the uncertainty of certain estimators in high dimensional linear measurement error models. Significant progress has been made regarding hypothesis testing on low dimensional sub-parameters in high dimensional sparse models. Under this framework, [11], [13] and [24] proposed inference procedures for either high dimensional sparse linear or generalized linear models.  [17] provided a general framework for high dimensional inference based on decorrelated score functions.====Inference for high dimensional measurement error models is believed to be a difficult topic due to the bias and extra variability introduced by measurement errors as well as high dimensional nuisance parameters. Recently, [1] constructed simultaneous confidence regions for the parameters of interest in high dimensional linear models where all covariates are measured with errors using the multiplier bootstrap. [22] employed a de-biasing approach and constructed component-wise confidence intervals in a sparse high dimensional linear model when some covariates of the design matrix are missing completely at random.====In this paper, we consider a sparse high dimensional linear model where only a fixed number of covariates are measured with errors, which is different from the settings in [1] and [22]. Our goal is to develop statistical inference procedures for the coefficients associated with these error-prone covariates. In practice, it is common that not all covariates are corrupted. For example, in the real data example analyzed in Section 4.2, covariates such as gender, age and SNPs are measured precisely. In the spirit of semiparametrics, we employ a decorrelation operation to control the impact of high dimensional nuisance parameters, and construct a corrected decorrelated score function which is uncorrelated with the corrected nuisance score functions. In this case, the efficiency of the estimators for the parameters of interest will not be impaired provided that the estimators for the nuisance parameters are consistent at a sufficient rate. Although a general framework of high dimensional inference based on decorrelation was provided in [17], the existence of measurement errors imposes many special challenges in methodology and theoretical proofs, which requires innovative technical treatments, as illustrated in the main text of the paper. The performance of our corrected decorrelated score test relies on the convergence rate of the initial estimator. Here, we take the CoCoLasso estimator [8] as an example. Indeed, any estimator with sufficient convergence rate can be served as the initial estimator in forming the decorrelated score function. Different from the settings in [8], we assume that the design is sub-Gaussian and only a fixed number of covariates, without loss of generality, one covariate, is measured with error. We re-derive the theoretical properties of the CoCoLasso estimator in our new setting, which is one of the contributions of this work. Our corrected decorrelated score test statistics retain power under the local alternatives around zero, because we essentially do not impose any penalty on the parameter of interest in the construction. We further construct confidence intervals by proving the limiting distribution of the score type estimator, which is the root of the estimating equation based on the estimated corrected decorrelated score function.====Although for convenience, we present our methodology and theory for one variable with measurement error, the proposed method is directly applicable to the cases where a fixed number of covariates are measured with errors. Additionally, we allow the error covariance matrix to be singular, which allows us to conduct inference for parameters associated with covariates both subject to error and error free. Details can be found in Section 7.====We specify our model and develop the methodology in Section 2, which includes construction of the corrected decorrelated score function, statistical properties of the initial estimator as well as the algorithm. Asymptotic properties of the score test statistic and the corresponding estimator are established in Section 3. To assess the performance of our method, we conduct simulation studies and perform an empirical data analysis in Section 4. Discussions are given in Section 5, and proofs of asymptotic results are given in Section 6. Last, we extend our method and results to the setting where more than one covariates are measured with error in Section 7.====: Before we pursue further, let us introduce some notations and preliminaries. For a vector ====, we define ====, where ==== and ==== is the cardinality of a set ====. Denote ==== and ====. For ====, let ==== and ==== be the complement of ====. For a matrix ====, let ====, ==== and ====. If ==== is symmetric, then ==== and ==== are the minimal and maximal eigenvalues of ====. For two positive sequences ==== and ====, we use ==== to denote ==== for some constant ====, and use ==== to denote ==== for some constants ====. Denote ==== to be the cumulative distribution function of the standard normal distribution. For simplicity, we use ==== and ==== to denote the expectation and probability calculated under the true model, respectively.====The sub-exponential norm of a random variable ==== is defined as ====. Note that ==== for some constant ====, if ==== is sub-exponential. The sub-Gaussian norm of ==== is defined as ====. Note that ==== for some constant ====, if ==== is sub-Gaussian. More properties regarding sub-exponential and sub-Gaussian random variables are given in Supplementary Materials S2.1.",Inference in high dimensional linear measurement error models,https://www.sciencedirect.com/science/article/pii/S0047259X21000373,16 April 2021,2021,Research Article,162.0
"Mai Jan-Frederik,Wang Ruodu","XAIA Investment GmbH, Sonnenstr. 19, 80331, München, Germany,Department of Statistics and Actuarial Science, University of Waterloo, Canada","Received 29 December 2020, Revised 6 April 2021, Accepted 6 April 2021, Available online 16 April 2021, Version of Record 24 April 2021.",https://doi.org/10.1016/j.jmva.2021.104760,Cited by (2),"We derive a stochastic representation for the probability distribution on the positive orthant ====-norm symmetric survival function. On the other hand, this result is leveraged to construct an exact simulation algorithm for max-infinitely divisible probability distributions on the positive orthant whose exponent measure has ====-norm symmetric survival function. Both applications generalize existing results for the case ==== to the case of arbitrary ====.","We fix ==== and write ==== throughout to simplify notation. Let ==== be a measure on ==== with the property that its survival function takes the specific form ====for some function ==== of one variable. Since the survival function of ==== is invariant with respect to changes in the ====-norm of its argument ====, it is called ====-norm symmetric. Probability measures ==== of the kind (1), as studied in [2], widely appear in many areas of applications including finance, risk management, and environmental sciences; we refer to [3] for background, examples, and statistical inference. Non-exchangeable extensions of (1) are discussed in [9], [10].====When integrating with respect to a measure in ====, it is sometimes convenient to perform this integration in two steps: first integrate with respect to a “direction” specified by some measure on the (bounded) unit ball of some norm, second integrate with respect to a one-dimensional “radial” measure. The representation (1) for the survival function of ==== suggests that such a decomposition is possible with a “directional” measure on the unit ball of the ====-norm and ==== accounting for the “radial” part. If ==== is a probability measure, intuitively this means that in order to simulate a random vector ==== one may first simulate a bounded random vector taking values within the unit ball of the ====-norm, and subsequently multiply this random vector with an independent radius. We also consider the case of non-finite Radon measures ==== on ====, meaning that ==== is non-finite but assigns finite measure to all closed sets that are bounded away from the origin ====. These play an important role in the context of max-infinitely divisible probability distributions, which are parameterized in terms of such a Radon measure, called the exponent measure, see [17] for a textbook account. A random vector is max-infinitely divisible if for arbitrary ==== it can be represented in distribution as the component-wise maximum of ==== independent and identically distributed random vectors. Canonical stochastic representations for such random vectors rely on the notion of Poisson random measure with mean measure ====. If one accomplishes a decomposition into directional and radial parts for the mean measure ====, this can be leveraged to construct an exact simulation algorithm of the associated max-infinitely divisible probability law, as we will demonstrate for ==== satisfying (1).====In order to prepare the reader for the technical tools involved in the present article, it is instructive to notice that the function ==== in (1) is necessarily ====-monotone. We recall that a function ==== is ====-monotone if the derivatives ==== exist for ====, and ==== is non-negative, non-increasing and convex. Such functions play an important role not only in the context of ====-norm symmetric multivariate survival functions, but also in the context of ====-symmetric multivariate characteristic functions, see [7]. A result of R.E. Williamson in [21] provides a representation for ====-monotone functions as integrals over certain simple functions with respect to a uniquely associated probability distribution on ====. Thus, they arise as analytical transforms of probability measures, generalizing the notion of a Laplace transform in a certain sense. In general, this transform can be inverted to obtain the associated probability distribution, but in concrete cases this inversion is not simple to figure out in a feasible form. Key to our results is the inversion of the ====-monotone function ====, whose associated probability distribution is shown to be related to a finite mixture of certain beta distributions.====If ==== with survival function (1) is a probability measure on ====, it follows from results in [2], [13] (this logic being recalled in (11)) that ==== is ====-monotone with ==== and ==== is the distribution of ====where ==== is a random variable on ==== whose distribution ==== depends solely on ====, the vector ==== is uniformly distributed over the standard unit simplex ====, ==== is a random variable on ==== whose distribution ==== solely depends on ====, and ====, ==== and ==== are independent. Here and throughout, raising a vector to a power ====, as well as applying other functions of one variable to a vector, should always be understood component-wise. Equivalently, ==== can be factored as ====The distribution function ==== of ==== has not been explicitly found to date. We derive an exact representation for this probability distribution, finding that a random variable ==== satisfies the distributional equality ====where, independently of ====, we denote by ==== the order statistics of independent standard uniform random variables ==== and ====, and the mixture probabilities ==== can conveniently be computed by the recursive relationship ====with initial value ==== and auxiliary notation ====. This finding implies an efficient simulation algorithm for random vectors ====. Existing simulation algorithms to date are either restricted by the assumption that ==== is completely monotone (which is a special case of ====-monotone in which ==== is a Laplace transform), or rely on computations of partial derivatives as in [2, Proposition 5.3], which is infeasible for large ====.====The case ====, in which ====, is well-established, see [13]. It is convenient to study the dependence structure between components of ==== in terms of the survival copula of ==== in that case. The latter equals the distribution function of the random vector ====, which is called an Archimedean copula with Archimedean generator ====, see [13] for background. The particular choice ==== corresponds to the random vector ====, i.e. ====, and this choice minimizes the association between components of ====. In other words, randomness of the radial variable ==== increases the strength of association between components of ==== when compared to a non-random radius. In the general case ==== the situation is analogous and ==== is a stochastic representation for the random vector whose association between its components is minimal among all random vectors with ====-norm symmetric survival functions. Our exact representation for the law of ==== thus implies a stochastic model for the ====-norm symmetric survival function with minimal association between components. Interestingly, the survival copula of ==== in the general case ==== is also an Archimedean copula, but with Archimedean generator given by ====.====A further application of our findings concerns the case when ==== in (1) is a non-finite Radon measure on ====. The results in [6] imply that the function ==== is ====-monotone and a bijection on ====. The same factorization (2) of ==== is valid, only with the probability distribution ==== being replaced by a non-finite “radial” Radon measure ==== on ==== that solely depends on ====. As already mentioned, our explicit derivation of the probability distribution ==== can be leveraged to derive an exact simulation algorithm for max-infinitely divisible random vectors with exponent measure ====. To the best of our knowledge, such algorithm is unknown to date.====The remainder of this article is organized as follows. Section 2 provides background on different concepts of ====-norm symmetry in the context of multivariate probability distributions. Section 3 derives the explicit form of the aforementioned distribution ====. Section 4 presents an efficient simulation algorithm for ==== and thus for arbitrary random vectors with ====-norm symmetric survival functions. Section 5 proves a stochastic representation for max-infinitely divisible random vectors ==== on ==== whose exponent measure has ====-norm symmetric survival function, and explains how to simulate ==== exactly from it.",Stochastic decomposition for ,https://www.sciencedirect.com/science/article/pii/S0047259X21000385,16 April 2021,2021,Research Article,163.0
"Nakagawa Tomoyuki,Watanabe Hiroki,Hyodo Masashi","Department of Information Sciences, Faculty of Science and Technology, Tokyo University of Science, Noda-shi, Chiba 278-8510, Japan,Department of Health Sciences, Oita University of Nursing and Health Sciences, Oita City, Oita Prefecture, 870-1201, Japan,Department of Economics, Kanagawa university, Kanagawa 221-8686, Japan","Received 16 November 2020, Revised 14 March 2021, Accepted 15 March 2021, Available online 26 March 2021, Version of Record 5 April 2021.",https://doi.org/10.1016/j.jmva.2021.104756,Cited by (3), study to examine the finite sample performance of the proposed selection method. Our simulation results show that the selection method frequently selects the set containing non-redundant variables. We also observed that the discrimination rules constructed from the selected variables reduce EPMC more than the discrimination rules constructed from all variables.,"Selecting the appropriate variables for discriminant analysis is often considered the most important and difficult part of discriminant model building. If the non-redundant variables, affect the classification rule, are not included, the expected probabilities of misclassification (EPMC) are large. On the other hand, EPMC may be large even if the redundant variables, which does not affect the classification rule, are included. Over the past years, numerous studies have developed methods of variable selection in various discriminant analyses (e.g., [4], [5], [7], [8], [9], [10], [11], [14], [20], [21]). For example, Fujikoshi [7], [8] derived Akaike’s information criterion (AIC) for the canonical discriminant analysis. Fujikoshi [9] also modified AIC in a high-dimensional framework when the sample sizes and dimensions tended to infinity. In general, a full search of variable selection based on information criteria requires an enormous amount of computation as the dimensions increase, as such; it is difficult to apply it to high-dimensional data. Kick-one-out (KOO) method introduced by [19], [24], has been proposed as the ideal solution to the computation problem of classical AIC, BIC, and ====, as it can drastically reduce the number of computational statistics. In addition, Fujikoshi and Sakurai [11] and Oda et al. [20] proved that the KOO method based on information criteria is capable of variable selection consistency especially in canonical discriminant analysis. However, many of the theoretical results obtained in these works require that the dimension ==== be smaller than the total sample size ==== or the group-conditional distributions be of multivariate normal distribution or homogeneous to the covariance matrices.====In this study, we focus on variable selection for the Euclidean distance-based classifier in high-dimensional settings where ==== does not necessarily have to be assumed. Since the pooled sample covariance matrix required to compute linear discriminant function is singular in high-dimensional settings, i.e., when ====, the classification rule cannot be computed. A straightforward solution to this problem is to use the Euclidean distance-based classifier, which is discussed in [2], [3]. The classification rule by using the Euclidean distance-based classifier does not necessarily have to assume that ====, normality or homogeneous to the covariance matrices. Furthermore, Aoshima and Yata [2], [3] also proved that EPMC of the Euclidean distance-based classifier tends to zero under some conditions, e.g., when the sample sizes and dimensions tend to infinity. For these reasons, the Euclidean distance-based classifier is useful in high-dimensional settings when the redundant variables, which are defined in Section 3, are not included in the feature values. However, when the redundant variables are included for the Euclidean distance-based classifier, it is not only that EPMC does not tend to zero even with increasing dimensions, but EPMC may be increasing with increasing dimensions. This point is explained in detail in Section 3. Thus, it is necessary to consider a variable selection method that helps reduce EPMC.====Our goal is to provide a variable selection method for the Euclidean distance-based classifiers, which has the consistency of variable selection in the context of high dimensionality. First, we introduce asymptotic EPMC to highlight the impact of redundant variables on EPMC. Next, we establish KOO-based criteria for selecting non-redundant variables for the Euclidean distance-based classifier. Finally, we prove the consistency of proposed variable selection in the context of high dimensionality. Our theoretical results are derived without assuming homogeneity of covariance matrices, multivariate normality for the group-conditional distribution, and ====. In addition, asymptotic approximations of EPMC are derived under the elliptical distribution families, which are broader than the distribution families treated in previous studies.====This paper is organized as follows: In Section 2, we review a normal approximation of EPMC, which is derived by [2], [23]. Furthermore, we derive a new asymptotic approximation of EPMC under the elliptical populations. In Section 3, using the asymptotic result obtained in Section 2, we show that the Euclidean distance-based classifier with only non-redundant variables reduces asymptotic EPMC more than the Euclidean distance-based classifier with all variables. In Section 4, we propose a KOO-based variable selection method and prove the consistency of this method. In Section 5, we investigate the finite sample performance of the proposed selection method via Monte Carlo simulation and apply the proposed variable selection method to two real datasets.",Kick-one-out-based variable selection method for Euclidean distance-based classifier in high-dimensional settings,https://www.sciencedirect.com/science/article/pii/S0047259X21000348,26 March 2021,2021,Research Article,164.0
Zhang Hong-Fan,"Wang Yanan Institute for Studies in Economics, Xiamen University, China","Received 22 May 2020, Revised 4 March 2021, Accepted 4 March 2021, Available online 19 March 2021, Version of Record 24 March 2021.",https://doi.org/10.1016/j.jmva.2021.104753,Cited by (3)," method to induce sparse solutions. We show that the resulting sparse estimator can be calculated in a computationally efficient manner, and the associated BIC-type criterion can consistently select relevant variables. Experimental simulations and a real data analysis demonstrate the effectiveness and usefulness of the proposed methods.","Since the seminal work of Li [9], dimension reduction has gained a lot of attention during the last few decades. Cook [2] and Cook and Li [3] respectively introduced the notions of the Central Subspace (CS) and the Central Mean Subspace (CMS), under which the conditional distribution of ==== and only the conditional mean function ==== are captured by a few linear combinations of ====. Suppose that ==== is a univariate response, ==== is the predictors, and ==== is an orthogonal matrix. Then, dimension reduction amounts to saying that ==== for the CS and ==== for the CMS, where ==== denotes the conditional independence. The subspace spanned by the columns of ====, denoted by ====, which stands for the CS or the CMS, is thus the objective to be estimated. As stated in the review article of Ma and Zhu [19], the current approaches to dimension reduction can be generally classified into three categories. The first category is the inverse regression based method, including SIR (Li [9]), SAVE (Cook and Weisberg [4]), PHd (Li [10]) and DR (Li and Wang [13]), etc. The second category is the semiparametric method such like Ma and Zhu [18], Zhang, Zhu and Ma [37] and some others. The third class is the forward nonparametric method that is famous for the MAVE method of Xia et al. [29], Wang and Xia [23] and Yin and Li [32], etc. In this paper, we focus on the MAVE method and develop its estimation version when multivariate response exists.====When it comes to the multivariate response case in which ==== with ====, many estimation methods have been proposed. For example, the inverse regression based methods such like Li et al. [12], Yin and Bura [31], Lue [17], Li, Wen and Zhu [14], Zhu, Zhu and Wen [39], Hilafu and Yin [8] and Coudret, Girard and Saracco [5]; the semiparametric method such like Zhang, Zhu and Ma [37]; and the distance covariance based method such like Chen, Yuan and Yin [1]. Amongst them, Li, Wen and Zhu [14] proposed the projective resampling method, that is first to project the multivariate response along randomly sampled directions, and second to apply the usually used univariate response dimension reduction method such as SIR and SAVE to each sample, and third to average all the estimated candidate matrices to recover the objective subspace ====. The “PR” (projective resampling) approach provides a unified mechanism that allows other dimension reduction methods incorporated. Clearly, the PR-MAVE method can be naturally defined in a proper way.====However, as far as we know, almost all the MAVE-type methods with multivariate response solve the CS instead of the CMS. This argument can be verified by retrospecting the revolution process of the MAVE-type methods. In fact, the original MAVE method of Xia et al. [29] only targets the CMS, not the CS. To remedy this deficiency, Xia [27] and Wang and Xia [23] respectively proposed the density based MAVE (dMAVE) and the sliced regression based MAVE (sMAVE) to exhaustively estimate the CS. However, the three MAVE-type methods are only applicable to univariate response. Later, Yin and Li [32] introduced the ensemble MAVE method (eMAVE) that characterizes the CS by the CMS of a family of functions of the response. The eMAVE method unifies dMAVE and sMAVE as two special estimators, and is applicable to both univariate and multivariate responses. Consequently, as illustrated by Fig. 1, we know that, eMAVE and PR-MAVE are capable of handling the multivariate response CS, however, for the multivariate response CMS, what should be the right type of the MAVE method is less well understood. It thus motivates us to fill this gap by proposing a weighted version of the MAVE method. The weighted MAVE method takes account of the correlations among the multiple responses by using the estimated inverse of the covariance as the weighted matrix. Similar to all of the MAVE-type methods mentioned above, assumptions on the distribution of the predictors are also free of severity and computation of the estimator is very simple. We show that the proposed MAVE method is effectively valid for dimension reduction where the responses have strong correlations.====In addition, variable selection is another important issue. A common way to induce sparse solutions is to incorporate the Lasso regularization method. With regard to dimension reduction problems, Li [11] and Li and Yin [15] developed the SIR based regularization method; Wang and Yin [25] proposed the sparse MAVE method that uses the ==== norm penalty on the column values of ====; Wang, Xu and Zhu [24] considered the penalized MAVE method using a group bridge penalty for the ==== norms of the rows of ====, which can achieve row-wise variable selection; Zeng, He and Zhu [35] and Zeng and Zhu [36] suggested another MAVE-Lasso method that simultaneously penalizes the row values of ==== and the derivative of the nonparametric regression function for single index models and multiple index models respectively. However, some of these methods are not flawless for variable selection purposes. For example, the method in Wang and Yin [25] is designed for only element screening instead of variable selection; the non-convex bridge penalty used in Wang, Xu and Zhu [24] could make the estimator get trapped in local minima; the theoretical properties of the method used in Zeng and Zhu [36] are generally unknown and could be difficult to derive. On the other hand, incorporating the well known adaptive group Lasso method has not been considered before, to our knowledge. Therefore, in this paper, we consider combining the ==== norm based adaptive group Lasso method with the weighted MAVE method under the multivariate response setting. The proposed MAVE-Lasso estimator has attractive properties: easy calculation, estimation sparsity and asymptotic normality. We also suggest a BIC-type criterion to select the involved penalization parameters. Simulation results show that our proposed sparse MAVE method can simultaneously achieve both sparse estimation and variable selection in a high accuracy level.====The remaining part of the paper is organized as follows. Section 2 proposes the weighted version of the MAVE method for the multivariate response CMS. The corresponding algorithm is included. Asymptotic properties of the proposed estimators are also presented. In Section 3, the adaptive group Lasso method is introduced to incorporate the MAVE method. Some relevant issues such as its computation, the BIC criterion and its theoretical properties are also investigated. Section 4 and Section 5 correspond to simulation experiments and a real data analysis. Section 6 ends the article with a conclusive discussion. Technical conditions and proofs are given in the Appendix.",Minimum Average Variance Estimation with group Lasso for the multivariate response Central Mean Subspace,https://www.sciencedirect.com/science/article/pii/S0047259X21000312,19 March 2021,2021,Research Article,165.0
"Bauer Jan O.,Drabant Bernhard","Baden-Wuerttemberg Cooperative State University Mannheim, Coblitzallee 1-9, 68163 Mannheim, Germany","Received 6 July 2020, Revised 7 March 2021, Accepted 7 March 2021, Available online 18 March 2021, Version of Record 10 April 2021.",https://doi.org/10.1016/j.jmva.2021.104754,Cited by (4),"This paper proposes a tool for dimension reduction where the dimension of the original space is reduced: the principal loading analysis. Principal loading analysis is a tool to reduce dimensions by discarding variables. The intuition is that variables are dropped which distort the ==== only by a little. Our method is introduced and an algorithm for conducting principal loading analysis is provided. Further, we give bounds for the noise arising in the sample case.","When data is of high dimension it is often beneficial to reduce dimension. Two ways of reducing dimension exist: either by transforming the variables to a reduced set of variables or by selecting a subset of the existing variables [2], [10]. We propose a new approach pursuing the latter one despite adopting ideas from principal component analysis (PCA).====Classical PCA was formulated by [4], [14] and has been extended over the years while still being an active field of research. The basic goal is to transform a set of variables into a subspace spanned by orthogonal variables containing most of its original variance (see for example [7] for an overview). One extension to a non-linear approach using kernels was given by [17], to the so called kernel PCA. Further, [20] proposed a sparse principal component analysis (sparse PCA) to reduce the dimension as well as the number of used variables. This is done by implementing a further restriction in the underlying maximization problem. Regarding the purpose of discarding variables, [5], [6] presented four methods and provided examples for artificial as well as real data respectively. The methods are based on the idea to select variables that are highly present in the largest eigenvectors i.e. in the eigenvectors associated with the largest eigenvalues, or to discard variables that are highly present in the smallest eigenvectors i.e. in the eigenvectors associated with the smallest eigenvalues. In a regression context, [1] propose an iterative method to select the covariates using PCA and [11] to discard variables considering the respective increase in the residual sum of squares.====[9] provided a broad overview of existing theories regarding the asymptotics of eigenvectors and eigenvalues of sample correlation and covariance matrices respectively. [3] pointed out that the assumption of simple eigenvalues is needed because the orthonormal basis of the eigenmanifold of eigenvalues with a multiplicity greater than one can be obtained by rotation hence asymptotics for the corresponding eigenvalues are problematic to obtain.====However, to our knowledge, there has been no effort in developing a technique where dimension is reduced by selecting a subset of the observed variables based on non-impact in the eigenvectors. Our main contribution is a method as such: the principal loading analysis (PLA). We investigate the underlying form of the sample covariance matrix needed to conduct PLA where we take the presence of perturbations caused by a small sample size and due to the fuzziness of PLA into account. We further provide an algorithm to conduct PLA in practice.====The rest of the paper is organized as follows: Section 2 provides assumptions needed for the remainder of this work. In Section 3, we recap the methodology of PCA and explain the idea of PLA. Our focus lies on Section 4, where we introduce the PLA method and the underlying covariance matrix structure needed for PLA. We will provide bounds for the sample counterparts essential for PLA: for the sample covariance, sample eigenvectors and sample eigenvalues. We suggest using a cut-off threshold for application and Section 5 complements this step of PLA. We give recommendations for threshold values in Section 6 based on simulation studies. We give simulated examples in Section 7 and take a resume as well as suggest extensions in Section 8.",Principal loading analysis,https://www.sciencedirect.com/science/article/pii/S0047259X21000324,18 March 2021,2021,Research Article,166.0
"Yang Yihe,Zhou Jie,Pan Jianxin","College of Mathematics, Sichuan University, Chengdu 610065, China,Department of Mathematics, The University of Manchester, Oxford Road, Manchester M13 9PL, UK","Received 14 April 2020, Revised 14 February 2021, Accepted 14 February 2021, Available online 9 March 2021, Version of Record 22 March 2021.",https://doi.org/10.1016/j.jmva.2021.104739,Cited by (4),) and high-dimensional (,"Covariance matrix, which characterizes the pairwise linear correlation of multiple variable, plays an important role in statistical research and application. In financial risk assessment [13], longitudinal data analysis [21], spatial and spatio-temporal data analysis [11] and signal processing [27], for instance, covariance matrix is always indispensable and fundamentally important. With the fast development of technology, datasets generated in various fields become increasingly large and may be high-dimensional, that is, the number of variables is bigger than the sample size (i.e., ====), which seriously challenges standard statistical methods. For example, the sample covariance matrix, the most commonly used estimator of the population covariance matrix, degenerates as the dimension of data is close to, or larger than, the sample size. In addition, most of the available algorithms for high-dimensional covariance estimation are inevitably computationally intensive and numerically instable. In this paper, we focus on a broad class of covariance structures — Toepliz structure, which contains many commonly used covariance structures as special cases, such as order-one moving average (MA(1)), order-one autoregression (AR(1)) and order-one autoregressive and moving average (ARMA(1,1)) among many others. We then propose a novel method that selects the optimal Toeplitz covariance structure and estimates the high-dimensional covariance matrix, simultaneously.====In the literature the issue of high-dimensional covariance matrix estimation has been widely studied. The assumption of sparsity is vital to study the estimation of high-dimensional covariance matrix partially because otherwise there are too many parameters to estimate and the resulting covariance estimator may not be invertible. To impose sparsity on the covariance matrix estimator, Bickel and Levina [3] proposed a thresholding approach that removes any entry of the sample covariance matrix as long as it is smaller than a given threshold. Since then many good extended methods have been developed, such as the generalized thresholding method [30], adaptive thresholding method [7] and principal orthogonal complements thresholding method (POET) [13] among others. These methods improve Bickel and Levina’s [3] method by shrinking the entries in a soft way, so that the bias of covariance estimator can be reduced. However, a drawback of such approaches is that the resulting thresholded covariance estimator is not guaranteed to be positive definite. In fact, all these thresholding approaches can be expressed as a mathematical optimization problem in the form of “Loss ==== Penalty”, where the loss function is usually taken as the Frobenius norm (or F-norm) and the penalty plays a role of imposing sparsity. For example, the penalty function may take the least absolute shrinkage and selection operator (lasso) [32] or the smoothly clipped absolute deviation (SCAD) [12]. It is clear that the F-norm only measures the discrepancy of pair entries of two matrices and does not take into account the constraint of positive definiteness when estimating the covariance matrix. To account for the positive definiteness, it was suggested to replace the F-norm loss function by certain measures that quantify the discrepancy of eigenvalues of the matrices. For example, Bien and Tibshirani [5] proposed to estimate the covariance matrix through negative log-likelihood function with the lasso regularizer. Lam and Fan [19] extended the lasso regularizer to a family of nonconvex penalties and further studied the sparsistency and convergence rate of the estimator. Xue and Zou [34] and Rothman [29] imposed a positive semidefinite constraint and a small log-determinant barrier on optimizations of thresholding approaches, respectively. The algorithms for optimizations they considered include the majorization–minimization algorithm [18], coordinate descent method [2] and alternating direction method of multipliers (ADMM) [6].====An alternative study for high-dimensional covariance estimation assumes that multivariate variable ==== has a natural order, for instance, arising in longitudinal study and time series analysis. In particular, two components ==== and ==== are considered to be nearly uncorrelated when their associated lag, say ====, becomes large. For this kind of covariance matrix, it is common to use a banding approach [4] that shrinks the entries of the sample covariance matrix to zero through off-diagonals. Cai et al. [9] provided the optimal convergence rate of banding estimator. In addition, the covariance matrix estimation was also considered within the framework of a modified Cholesky decomposition, in which the lower-triangular Cholesky factors are the minus of autoregressive coefficients of ==== and the diagonal innovation matrix elements are the associated innovation variances. The Cholesky factor matrix is very likely to be sparse with the increasing of lag. Working with the log-likelihood function, Huang et al. [17] and Levina et al. [20] shrunk the entries of the Cholesky factor matrix through lasso regularizer and nested lasso regularizer, respectively, to estimate the covariance matrix of ==== that has a natural order.====In longitudinal study, under certain regularity conditions the generalized estimating equation approach [21], a commonly used strategy, yields consistent estimators of the mean parameters whatever the working covariance structure is. But a good covariance structure produces more efficient estimators of the mean parameters. In geostatistics [11], for example, an appropriate correlation structure of the latent spatial process improves the spatial trajectory prediction for an unobserved individual on regular grids. Commonly encountered structures include exponential structure, Matérn structure and spherical structure for spatial data, and MA(1), AR(1) and ARMA(1,1) for longitudinal data. A very important issue in such fields is to select the most appropriate or optimal structure for the covariance matrix. Lin et al. [22] and Cui et al. [10] suggested to pre-select an appropriate structure from a class of potential candidate structures through certain criteria such as the F-norm or entropy loss function, and estimate the parameters involved in the structured covariance matrix. Zhong et al. [39] and Zheng et al. [38] proposed to specify the underlying covariance structure through hypothesis test. Huang et al. [16] proposed a calibration method which finds the optimal positive definite surrogate for any given but not necessarily positive definite covariance estimator in the sense of the F-norm. Filipiak et al. [14] gave an approach that projects a covariance matrix estimator onto an asymptotic cone of positive definite Toeplitz matrices.====In this paper we focus on the consideration of high-dimensional Toeplitz covariance matrix. We propose a novel method that selects the optimal Toeplitz structure and estimates the corresponding covariance matrix, simultaneously. We first express the objective Toeplitz structured covariance matrix of multivariate random variable ==== in terms of a linear combination of a series of Toeplitz base matrices. We then propose to use an entropy loss function to measure the discrepancy between the sample covariance matrix and the targeted Toeplitz covariance matrix, which directly works on the eigenvalues of the associated covariance matrices to ensure the positive definiteness of the covariance estimator. To account for sparsity, a shrinkage penalty is imposed to the entropy loss function and an optimal Toeplitz structure for the covariance matrix is selected by minimizing the penalized entropy loss function. We also provide the convergence rates of the structured covariance estimators, which is closely related to linear spectral statistics of the sample covariance matrix [37].====The rest of the paper is organized as follows. In Section 2, the Toeplitz covariance structure and the entropy loss functions are introduced. In Section 3, an objective optimization called Toeplitz Estimation under Entropy-loss (TEE) method is proposed to estimate a high-dimensional Toeplitz covariance matrix. In Section 4, the asymptotic properties of the resulting covariance estimators are discussed. In Section 5, simulations studies are conducted to assess the performance of the proposed methods. In Section 6, two empirical data sets are analyzed using the proposed methods. Concluding remarks are summarized in Section 7. All the technical proofs of theorems are relegated to Appendix.",Estimation and optimal structure selection of high-dimensional Toeplitz covariance matrix,https://www.sciencedirect.com/science/article/pii/S0047259X21000178,9 March 2021,2021,Research Article,167.0
"Xie Junshan,Zeng Yicheng","School of Mathematics and Statistics, Henan University, Kaifeng, China,Department of Mathematics, Hong Kong Baptist University, Hong Kong,Research Center for Statistics and Data Science, Beijing Normal University, Zhuhai, China","Received 24 August 2019, Revised 21 February 2021, Accepted 22 February 2021, Available online 4 March 2021, Version of Record 23 March 2021.",https://doi.org/10.1016/j.jmva.2021.104742,Cited by (2),"Consider the ====, when two samples of sizes ==== and ==== from the two populations are available, we construct its corresponding sample version. In the high-dimensional regime where both ==== and ==== are proportional to ","In the last few decades, with the remarkable development in storage devices and computing capability, the demand for processing complex-structured data increases dramatically. One of the features, which is also the challenge caused by these data sets, is their high dimensions. The difficulty is that the classical limit theory for multivariate statistical analysis fails to ensure reliable inference for high-dimensional data analysis. Classical limit theorems require “small ==== large ====” to keep their validity, which conflicts with the situation “large ==== large ====” in high-dimensional settings in the sense that ==== as the corresponding asymptotic properties become rather different. To attack the relevant issues, random matrix theory (RMT) serves as a powerful tool in addressing statistical problems in high dimensions. The first research of random matrices in multivariate statistics was about the Wishart matrices in [18]. Abundant research has been conducted for various topics in this field during the past half-century, especially in recent years. In the area of RMT in statistics, we refer to monographs [2], [19] for systematical studies and [12] for a comprehensive review.====A relevant topic in multivariate statistics is about testing the equality of two covariance matrices: ====where ==== and ==== are two covariance matrices corresponding to two ====-variate populations, and ==== is a non-negative definite matrix with rank ====. Let ==== and ==== be the sample covariance matrices from these two populations, respectively. When ==== is invertible, the random matrix ==== is called a Fisher matrix.====The difference between the null hypothesis and the alternative hypothesis relies on those extreme eigenvalues of ====. Under the null hypothesis, ====, [16] established the well-known Wacheter distribution as the limiting spectral distribution (LSD) of ====. Some extensions were built later (see examples in [13], [14], [15]). Furthermore, [1] pointed out the fact that the largest eigenvalue of ==== converges in probability to the upper bound of the support of the LSD of ====. Under the alternative hypothesis, ==== is called a spiked Fisher matrix (see [17]), because ==== has a spiked structure similar to that of a spiked population model proposed by [10]. More specifically, the matrix ==== is assumed to have the spectrum ====where ====. When the rank ==== of ==== is finite, [6] showed the phase transition phenomenon of the extreme eigenvalues of ==== under Gaussian population assumption. That is, for ====, the ====th largest eigenvalue of ==== will depart from the upper bound of the support of LSD of ==== if and only if ==== exceeds a certain phase transition point. [17] relaxed Gaussian assumption and established central limit theorems for outlier eigenvalues of ====.====We in this paper consider, as a reasonable extension in theory and applications, the case of divergent ==== with respect to the dimension ====. We will investigate convergence in probability and central limit theorems for the spiked eigenvalues of the spiked Fisher matrices. We formulate our problem as follows.====Assume that ====are two independent arrays of independent real-valued random variables with zero mean and unit variance. We consider two samples ==== and ====, then their corresponding sample covariance matrices can respectively be written as ====Also, define the Fisher matrix ==== as a sample version of matrix ====. We aim to investigate asymptotics of the eigenvalues of ====. As the eigenvalues of ==== remain invariant under the linear transformation ====we can assume ==== throughout this paper without loss of generality. Under the assumption (2), the eigenvalues of ==== are ====. Recalling (1) that ==== is a rank ==== perturbation of ====, we simply assume ====For the sake of brevity and readability, we write the eigenvalues of ==== in a descending order ====, simplifying the double subscripts as single ones. Note that ==== depends on the sample size ====.====We now summarize some related work and our contributions. When all the spiked eigenvalues ====, ====, with fixed ====, are bounded, there are some results in the literature, for instance, the almost sure convergence (strong consistency) and the central limit theorem (CLT) [17], and the asymptotic Tracy–Widom distribution for the largest non-spiked eigenvalue [7], [8]. In this paper, we consider the case where the number of spiked eigenvalues ==== as ====, and the spiked eigenvalues ====, ==== diverge as ====. To the best of our knowledge, there is no relevant result in the literature. A relevant paper is [5] which studied spiked population models, where the asymptotics for spiked eigenvalues, including the convergence in probability (weak consistency), the CLT, and the Tracy–Widom law for the largest nonspiked eigenvalue were built under a general framework. Unlike the case of fixed ==== and bounded spikes ====, ====, normalization for ====, ==== is needed for the divergent ==== case. We consider the convergence of the normalized eigenvalues ==== and the CLT of ====, where ==== is a centered parameter defined later.====A basic approach for proving the asymptotics about spiked eigenvalues is to analyze a ==== random matrix corresponding to the ==== spikes. When ==== is bounded, [17] derived the almost sure entrywise convergence of the ==== matrix (and hence the convergence in norm) and then the almost sure limits of ==== spiked eigenvalues. This argument fails to work in the divergent ==== case where the entrywise convergence does not imply the convergence in norm of a ==== matrix. Instead, we use the CLT for random sesquilinear forms in [3] to derive the convergence rate of each entry, and then use Chebyshev’s inequality to put all entries together to derive the convergence rate of the matrix in ==== norm. In this way, we achieve the convergence in probability as well as the CLT of spiked eigenvalues (after proper normalization). This approach is similar to that used in [5], some technical assumptions will be imposed similarly.====The remaining parts of the paper are organized as follows. Section 2 contains the main results, including the convergence in probability of ==== and central limit theorems of ====, for spiked eigenvalues of the spiked Fisher matrix ====. Here, ====, ====, is a sequence of centering parameters defined in this section. In Section 3, we present the proofs of our main results. The proofs of some technical lemmas are displayed in Section 4.",Limiting laws for extreme eigenvalues of large-dimensional spiked Fisher matrices with a divergent number of spikes,https://www.sciencedirect.com/science/article/pii/S0047259X21000208,4 March 2021,2021,Research Article,168.0
"Jadhav Sneha,Ma Shuangge","Department of Mathematics and Statistics, Wake Forest University, Winston-Salem, NC, 27109, USA,Department of Biostatistics, Yale University, New Haven, CT, 06520, USA","Received 28 February 2020, Revised 22 February 2021, Accepted 22 February 2021, Available online 4 March 2021, Version of Record 17 March 2021.",https://doi.org/10.1016/j.jmva.2021.104740,Cited by (5),We consider the problem of testing for association between a functional variable belonging to a ,"A fundamental problem that arises often in statistics is to determine whether a pair of random variables are independent. This problem gets increasingly difficult as technological advancements lead to variables of a complex nature. An important example of such a complex variables is a random function.====There have been studies that develop measures of association between two functions [4], [6], [19]. In this article, we focus on testing for association between a scalar variable and a random function. In many scientific fields, functional data routinely arise, and accordingly, extensive statistical developments have been carried out. In the literature, the most common way to test association is via linear modeling, where the response variable is continuously distributed and the regressor is functional (of note, there are also studies that reverse the roles). Examples include [2], [8], which develop testing procedures for functional linear models based on the cross covariance operator. Some other studies [9], [14] develop testing frameworks for generalized functional linear models. Another study [11] extends the Wald, score, likelihood ratio, and F tests to functional linear models. A restricted likelihood ratio based test for functional linear models can be found in [22]. A function-on-scalar regression for testing associations in genetic studies is developed in [18]. In [5] mixed models are used for constructing confidence intervals in penalized functional regression. Some studies [12], [21] develop methods of ordering and selecting principal components to construct tests in functional linear models. Our literature review suggests that most of the existing methods are parametric (based on functional linear models) and limited to checking for linear associations. There are also a handful of nonparametric regression approaches [3]. Among these existing methods only a few allow for sparsely observed and noisy functional data. As will be evident from our simulations, performance of these methods is very sensitive to model assumptions. In this study, we aim to overcome these limitations by developing a nonparametric rank-based method.====Specifically, we take a significantly different strategy and directly address the problem of association detection, as opposed to first modeling the relationship between two variables and then determining association. Our approach is based on U-statistics [7] which represents an important class of statistics with broad applications in estimation and inference. A popular application in the context of association testing is Kendall’s Tau, which is based on the concept of concordance. Briefly, suppose that ==== are identically distributed and independent copies of random vector ====. Variables ==== and ==== are in concordance if ==== and ====, or ==== and ====. Equivalently, concordance can be determined by ====, which is the crux of the Kendall’s Tau based test. The idea of concordance has also been referred to as monotone association, which means that if ==== increases then ==== increases [17]. The case of association in the reverse direction, i.e. if ==== increases then ==== decreases, is just a special case of monotone association and can be captured by ====, where ====. Here, we extend this idea to the functional analysis paradigm and propose a U-statistic that takes values in a Hilbert space that is, a UH-statistic. The central limit theorem of UH-statistics is used to determine the distribution of the test statistic under the null hypothesis. We also examine the case where functional data is sparsely observed by using the conditioning step developed in [23]. Akin to Kendall’s Tau, our method has power against the alternative of monotone association. As linear correlation is a special case of monotone association, our non-parametric method has broader applicability than most existing alternatives.====Simulations demonstrate the advantages of our method over the alternatives based on functional linear regression under various conditions. Our method is better than the parametric alternatives at detecting linear relations, when the dimension (number of components in a basis expansion) of the functional data increases. In addition, the proposed approach is also advantageous with its simplicity — it directly addresses association testing and avoids complex estimations. All of these factors render our method an edge in real applications. In this article, we consider two applications. In the first application, the functional variable is the cerebral white matter tract of a person with multiple sclerosis (MS), and the scalar variable is cognitive ability. The second application consists of the price trajectory of Palm M515 personal digital assistants (PDA) and their opening prices in a 7-day auction on e-Bay. For both applications, there is no reason to believe that the relationship is linear, and thus, may not be captured by the methods based on linear models. Hence, it is more sensible to apply the proposed approach.====The rest of the article is organized as follows. In Section 2, we introduce the test statistic, establish its properties, and discuss its implementation. We also examine the case of sparse functional data. Section 3 consists of simulation studies that compare the proposed method with the alternatives based on functional linear models. In Section 4, applications to the diffusion tensor imaging and Ebay auction data are presented. Proofs are deferred to the Appendix.",An association test for functional data based on Kendall’s Tau,https://www.sciencedirect.com/science/article/pii/S0047259X2100018X,4 March 2021,2021,Research Article,169.0
"Forcina Antonio,Kateri Maria","Dipartimento di Economia, University of Perugia, via Pascoli 12, 06100, Italy,Institute of Statistics, RWTH Aachen University, Germany","Received 12 October 2020, Revised 20 February 2021, Accepted 20 February 2021, Available online 4 March 2021, Version of Record 19 March 2021.",https://doi.org/10.1016/j.jmva.2021.104741,Cited by (4)," and reconstruction formulas for the ====. These results are the key to show that, given marginal logits, the generalized interactions introduced in this paper determine uniquely the ","The RC association models for two-way contingency tables were introduced by Goodman [14] with, essentially, two purposes: (i) when the rows and columns categories are ordered, take this into account by associating categories with scores; (ii) provide a class of models which are somewhere in between the model of independence and the saturated model. For an overview on modeling contingency tables with ordinal classification variables, refer to Agresti [1, Sections 10.4, 10.5]. Though association models were originally conceived for the analysis of ordinal variables, whenever the scores are treated as parameters to be estimated, they can be employed also for the analysis of partially ordered or even nominal variables for which they provide a class of non-saturated models that impose meaningful restrictions on the underlying association, Kateri [18, Chapter 6].====RC models were extended by [15] and Gilula et al. [13] to include models of correspondence analysis which had been developed, initially, with partially different objectives. Later, Kateri and Papaioannou [20] formulated a general class of RC association models that includes the original association models and the correspondence analysis model as special cases. They also showed that each model in their class is optimal with respect to a divergence measure which determines the form of a class of generalized interactions.====Bartolucci and Forcina [4] noted that the original RC association models could be seen as defined by a rank constraint on the matrix of local log-odds ratios and suggested that a similar rank constraint could be imposed to the matrix of general log-odds ratios (LORs for short in the following) defined by choosing logits of a specific type (local, global, continuation, reverse continuation) for the row and column variables. These choices may be combined as in Douglas et al. [9] leading to a variety of interaction parameters which can be subjected to a rank constraint. Thus, for each chosen pair of logits, a corresponding RC association model may be defined and fitted. More recently, Espendiller [11, Chapter 5] extended the notion of ====-scaled interactions in Kateri and Papaioannou [20], introducing generalized odds ratios for certain pairs of logit types, without however exploring or fitting the corresponding RC association models.====This paper defines a new class of interaction parameters for two-way tables (with the generalized odds ratios of Espendiller [11] as special cases) by combining the work of Kateri and Papaioannou [20] and Bartolucci and Forcina [4]. An element of this class may be defined by selecting: (i) the type of logit for the row and column variable, (ii) a divergence measure and the ====-scaled interactions that are implied, (iii) the rank of the corresponding matrix of interactions. In practice, we restrict to the Cressie and Read [7] family of divergence measures, indexed by a single parameter ====, with each of the 16 types of LORs in Douglas et al. [9] generalized along a uni-dimensional continuum. The flexibility and the potentials of the new class of models in applications are highlighted and discussed in Section 7. We provide evidence that, keeping the complexity of the model fixed, the sign of ====, jointly with the logit types, controls the location of cells with higher probabilities compared to the corresponding probabilities under the standard association models which hold when ==== tends to 0.====An efficient algorithm for computing maximum likelihood (ML) estimates is provided; it exploits the results on rank constrained matrices in Bartolucci and Forcina [4] and applies the regression algorithm by Evans and Forcina [12] which relies on the Aitchison and Silvey [2] approach to constrained ML estimation. Numerical efficiency of the proposed algorithm allows one to perform a quick model selection by exploring different combination of ====-scaled interactions and logit types.====Concerning the theoretical properties of the new models, we prove a representation theorem which extends the results in Kateri and Papaioannou [20], by showing that each element of our class of models arises naturally by minimizing a specific measure of divergence between a given probability table and the corresponding independence table, under suitable constraints. This result is the key to show that, given row and column margins, the new interaction parameters determine uniquely the bivariate joint distribution. For the case when both classification variables are ordinal, for each pair of logit types, we determine the form of positive association, as in Douglas et al. [9], which is implied by all extended interactions being non negative.====The paper is organized as follows. The working framework and some preliminaries on generalized logits, LORs and association models are presented in Section 2. The new extended association models, along with a representation theorem, are defined in Section 3, while results on the uniqueness of the mapping and on positive association are presented in Section 4. An algorithm for ML estimation is presented in Section 5. The examples presented in Section 6 show that the new models can fit substantially better relative to the standard association model and the discussion provides insights on why this may happen. Finally, Section 7 summarizes our results and outlines possible extensions.",A new general class of RC association models: Estimation and main properties,https://www.sciencedirect.com/science/article/pii/S0047259X21000191,4 March 2021,2021,Research Article,170.0
"Valdevino Félix de Lima Wenia,David Costa do Nascimento Abraão,José Amorim do Amaral Getúlio","Universidade Federal de Pernambuco, Departamento de Estatística, Recife, PE, Brazil","Received 14 November 2019, Revised 22 February 2021, Accepted 22 February 2021, Available online 4 March 2021, Version of Record 23 March 2021.",https://doi.org/10.1016/j.jmva.2021.104743,Cited by (2)," distribution. These quantities are then used as distance-based hypothesis tests to assess if two planar shape samples come from the same distribution. Even though useful for many models having Euclidean supports, we prove that the ==== KL discrepancy is invariant in rotation, what restricts its use. Four new homogeneity hypothesis tests are proposed, three among them may also be used like tests involving mean shape. The performance of the proposed tests is quantified and compared (in terms of both test size and power and robustness) to that due to other four mean shape tests of the SSA literature: Hotelling ====, Goodall, James and lambda. Finally, an application in evolutionary biology is done to assess sexual dimorphism in the Pan troglodytes (chimpanzee) species. Results point out our proposals may offer meaningfully advantages comparatively to considered literature tests.","The Statistical Shape Analysis (SSA) can be understood as a set of procedures for analyzing shapes of two-, three- and higher-dimensional objects from a statistical perspective [9], [16], [56]. Some of SSA main goals are: (i) to estimate the mean shape, (ii) to determine variability structures and (iii) to make hypothesis tests and confidence regions for shape-based features. The SSA core concept is the “shape”, which may be defined as what remains when location, rotation and scale are removed. According to Kendall [28], a way to study shapes is to determine key points (from (i) anatomical, (ii) mathematical and (iii) imputation points of view, Brombin et al. [9] on objects called ====. In this paper, we assume that under-study objects are in the plan, known as ====. Over the years, SSA has received great attention due to its applicability in several areas:====In order to detect changes in shape it is necessary to assume suitable statistical models. One of main distributions for planar shape is the complex Bingham (====) model. Its real version was initially introduced by Bingham [6], like a generalization of the Dimroth–Watson model [61]. The ==== distribution and some of its mathematical properties have been derived by Kent [30]. One of its simulation procedures has been proposed by Kent et al. [31]. They have shown such algorithm depends only of the truncated multivariate exponential (TME) distribution. In this paper, we use to produce synthetic experiment a truncated simplex method combined with the TME law.====Several works have been developed using the real Bingham distribution and its complex counterpart. Kent [29] has proposed an asymptotic expansion for the Bingham normalization constant. Kume and Wood [33] have shown that an arbitrary joint partial derivative of the Bingham normalizing constant may be proportional to the normalizing constant of a Bingham distribution of higher dimension. Amaral et al. [2] have provided a novel bootstrap hypothesis test based on the lambda statistic for directional and planar shape data. Amaral et al. [1] have derived bootstrap procedures for constructing confidence regions for mean shape of objects, labeled by landmarks in two dimensions. Dryden et al. [17] have provided a mathematical treatment for (i) the statistical analysis of high-dimensional spherical and shape data and (ii) approximations for maximum likelihood inference in several models, such as ==== and complex Watson (====) distributions. Dore et al. [14] have developed a bias-corrected estimation method for the ==== distribution. In the Bayesian context, Micheas et al. [41] have proposed a maximum posteriori probability estimators having the ==== model like the posterior distribution. Amaral et al. [3] have furnished methods to detect influential observations in samples of preshape under such model.====From numerical evidences we present in this paper, some of well-defined SSA hypothesis tests may provide very low test power curves. In order to derive new alternatives to overcome this issue, we use information theory measures; in particular, stochastic divergences and distances. According to Salicrú et al. [51], divergence measures between two probability distributions play an important role in statistical theory; specifically, into estimation and hypothesis inference procedures under large samples. Several works have been developed using these quantities. Van Erven and Harremos [59] have provided a brief survey listing important properties of Rényi and Kullback–Leibler (KL) divergences; such as convexity, continuity and limits of sigma-algebras. Bhattacharya and Dunson [5] have furnished a broad class of mixture models on planar shape space, following a Bayesian approach and using the KL divergence.====Nascimento et al. [44] and Frery et al. [21] have derived statistical tests based on stochastic distances for speckled intensity and polarimetric synthetic aperture radar (SAR) data, respectively. Morio et al. [43] have proposed an Information Theory-based approach for polarimetric interferometric (SAR) images.====In this paper, we propose new distance-based two-sample tests in planar shape. First we derive the Rényi, KL, Bhattacharyya and Hellinger distances for the ==== distribution and, as a consequence, provide four relating hypothesis tests. Further we prove that the KL-based statistics is invariant under rotation and, therefore, it is able only to study homogeneity. Adopting as figures of merit both (i) test size and power and (ii) robustness, we make a Monte Carlo study to quantify the performance of our proposals, in comparison with other four SSA tests: Hotelling ====, Goodall, James and lambda. Results indicate divergence-based tests have empirical test size as controlled (closed to adopted nominal level) as the literature ones; but, our proposals outperform the remainder in terms of both empirical test power and robustness. Finally, an application to real data is made in the O’Higgins and Dryden [46] context. Results provide evidence in favor of all our proposals, with exception of the KL test (which has statistics invariant under rotation).====The remainder of this paper is organized as follows. In Section 2, we present a brief discussion about the ==== distribution, involving some of its properties and an estimation procedure. Section 3 approaches a survey of some of main planar shape literature tests. Section 4 addresses the proposal of new distance-based hypothesis tests. Section 5, we furnish numerical results from synthetic and actual experiments. Concluding remarks are presented in Section 6. Subsequently, an Appendix provides details about the derived results.",Distance-based tests for planar shape,https://www.sciencedirect.com/science/article/pii/S0047259X2100021X,4 March 2021,2021,Research Article,171.0
"Lachos Victor H.,Prates Marcos O.,Dey Dipak K.","Department of Statistics, University of Connecticut, Storrs, CT 06269, USA,Departamento de Estatística, Universidade Federal de Minas Gerais, MG CEP 31270-901, Brazil","Received 14 June 2020, Revised 7 February 2021, Accepted 8 February 2021, Available online 1 March 2021, Version of Record 13 March 2021.",https://doi.org/10.1016/j.jmva.2021.104737,Cited by (3), package ====.,"Sample selection (SL) often occurs in many fields, like economics, biostatistics, finance, medical surveys, sociology and political science, to name a few. For example, in a sample of individual consumers with expenditures below or above some threshold, where the unobserved random variable (decision to spend) is related to the spending amount. As expenditure is not independent of the decision to spend, this sample may represent only a subset of the full population, and thus selection bias arises. In a classic example, Heckman [12] proposed the SL model, aiming to estimate the wage offer function of women. Because housewives’ wages are not observed, the sample collected is subject to the self-selection problem. SL is a special case of a more general concept known in the econometrics literature as limited dependent variables or variables observed over a limited range of their support, however, censoring [21] is much simpler than SL.====The classical Heckman SL model was introduced by Heckman [11] when he proposed a parametric approach to the parameter estimation under the assumption of bivariate normality (SLn). However, it is well-known that several phenomena are not always in agreement with this assumption, yielding residuals with a distribution with heavy tails or skewness. These characteristics can be circumvented by data transformations, as proposed by Lee [18], which can render approximate normality with reasonable empirical results. However, some possible drawbacks of these methods are: (i) transformations provide reduced information on the underlying data generation scheme; (ii) component wise transformations may not guarantee joint normality; (iii) parameters may lose interpretability in a transformed scale and (iv) transformations may not be universal and usually vary with the data set. Hence, from a practical perspective, there is a need to seek an appropriate theoretical model that avoids at transformations.====There are two-ways of estimating the SLn model, via maximum likelihood (ML) and using a two-step procedure [12]. A drawback of the ML estimation is less robust than the two-step procedure and is sometimes difficult to get it to converge [28]. However, ML estimation will be more efficient if the random errors really are jointly normally distributed. In this context, many robust methods have been proposed over the years to broaden the applicability of the SLn model to situations where the Gaussian error term assumption may be inadequate. For instance, the semiparametric SL model proposed by Ahn and Powell [1] and the nonparametric SL model proposed by Das et al. [7]. From a Bayesian perspective, Kai [14] proposed a Bayesian inference procedure for the SL model using data augmentation. More recently, Kim et al. [15] proposed a flexible nonparametric SL model using Bernstein Polynomial.====In the context of parametric models, Marchenko and Genton [20] introduced the Heckman selection-t model (SLt) model that extends the conventional SLn model by Heckman [12] to have a bivariate Student’s t error distribution. This model provides a greater flexibility for modeling heavier-tailed data than the SLn model by introducing only one extra parameter, the degrees of freedom, controlling the tails of the distribution. The authors considered ML estimation of the parameters via Newton–Raphson procedures available in statistical packages such as ==== and ====. They demonstrated the robustness aspects of the SLt model against outliers through extensive simulations. More recently, Zhao et al. [30] have proposed EM-type algorithms for ML estimation of the SLn model, which have the advantages of easy implementation and numerical stability. Moreover, ML estimation via the EM algorithm yields better estimators than the 2-step procedure.====Motivated by Zhao et al. [30], in this paper we propose a novel, simple and efficient EM-type algorithm for iteratively computing ML estimates of the parameters in the SLt model. We show that the E-step reduces to computing the first two moments of a truncated multivariate Student’s t (MVT) distribution. The general formulas for these moments were derived efficiently by Galarza et al. [10], for which we use the ==== package in ====. The likelihood function is easily computed as a byproduct of the E-step and is used for monitoring convergence and for model selection (AIC and BIC). Furthermore, we consider a general information-based method for obtaining the asymptotic covariance matrix of the ML estimate. The method proposed in this paper is implemented in the ==== package ====, which is available for download from Github (====).====The remainder of the paper is organized as follows. In Section 2, we briefly discuss some preliminary results related to the multivariate Student’s-==== distribution and its truncated version. Some of its key properties are also discussed. In Section 3, we present the SLn model proposed by Heckman [12] and the related EM algorithm for ML estimation. In Section 4, we present the robust SLt model, including the EM-type algorithm for ML estimation, and derive the empirical information matrix analytically to obtain the standard errors. In Sections 5 Simulation study, 6 Application, numerical examples using both simulated and real data are given to illustrate the performance of the proposed method. Finally, some concluding remarks are presented in Section 7.",Heckman selection-t model: Parameter estimation via the EM-algorithm,https://www.sciencedirect.com/science/article/pii/S0047259X21000154,1 March 2021,2021,Research Article,172.0
"Gayen Atin,Kumar M. Ashok","Discipline of Mathematics, Indian Institute of Technology Palakkad, Kerala 678557, India","Received 8 August 2020, Revised 2 February 2021, Accepted 2 February 2021, Available online 17 February 2021, Version of Record 2 March 2021.",https://doi.org/10.1016/j.jmva.2021.104734,Cited by (8),We extend ,"Divergence is a non-negative extended real-valued function ==== defined for any pair of probability distributions ==== satisfying ==== if and only if ====. Minimum divergence (or distance) method is popular in statistical inference because of its many desirable properties including robustness and efficiency [6], [51]. Minimization of information divergence (====-divergence) or relative entropy is closely related to the maximum likelihood estimation (MLE) [25, Lem. 3.1]. MLE is not a preferred method when the data is contaminated by outliers. However, ====-divergence can be extended by replacing the logarithmic function by some power function to produce divergences that are robust to outliers [5], [16], [36]. In this paper, we consider three such families of divergences that are well-known in the context of robust statistics. They are defined as follows.====Let ==== and ==== be probability distributions having a common support ====. Let ====.====Throughout the paper we assume that all the integrals are well defined over ====. The integrals are with respect to the Lebesgue measure on ==== in the continuous case and with respect to the counting measure in the discrete case. Many well-known divergences fall in the above classes of divergences. For example, Chi-square divergence, Bhattacharyya distance [8] and Hellinger distance [7] fall in the ====-divergence class; Cauchy–Schwarz divergence [54, Eq. (2.90)] falls in the ====-divergence class; squared Euclidean distance falls in the ====-divergence class [5]. All three classes of divergences coincide with the ====-divergence as ==== [16], where ====In this sense, each of these three classes of divergences can be regarded as a generalization of ====-divergence.====-divergences also arise as generalized cut-off rates in information theory [21]. ====-divergences belong to the Bregman class which is characterized by transitive projection rules [20, Eq. (3.2), Theorem 3], [37, Example 3]. ====-divergences (for ====) arise in information theory as a redundancy measure in the mismatched cases of guessing [57], source coding [41] and encoding of tasks [14]. The three classes of divergences are closely related to robust estimation, for ==== in case of ==== and ====, and ==== in case of ====, as we shall see now.====Let ==== be an independent and identically distributed (i.i.d.) sample drawn from an unknown distribution ====. Let us suppose that ==== is a member of a parametric family of probability distributions ====, where ==== is an open subset of ==== and all ==== have a common support ====. MLE picks the distribution ==== that would have most likely caused the sample. MLE solves the so-called score equation or estimating equation for ====, given by ====where ====, called the score function and ==== stands for gradient with respect to ====. In the discrete case, the above equation can be re-written as ====where ==== is the empirical measure of the sample ====.====Let us now suppose that the sample ==== is from a mixture distribution of the form ====, ====, where ==== is supposed to be a member of ====; ==== is regarded as the distribution of “true” samples and ====, that of outliers. Assume that support of ==== is a subset of ====. While the usual MLE tries to fit a distribution for ====, robust estimation tries to fit for ====. Throughout the paper, the above will be the setup in all the estimation problems, unless otherwise stated. Thus for robust estimation, one needs to modify the estimating equation so that the effect of outliers is down-weighted. The following modified estimating equation, referred as generalized Hellinger estimating equation, was proposed in [4], where the score function was weighted by ==== instead of ==== in (6): ====where ====. This was proposed based on the following intuition. If ==== is an outlier, then ==== will be smaller than ==== for sufficiently smaller values of ====. Hence the terms corresponding to outliers in (7) are down-weighted (c.f. [6, Section 4.3] and the references therein).====Notice that (7) does not extend to continuous case due to the appearance of ====. However in literature, to avoid this technical difficulty, some smoothing techniques such as kernel density estimation [7, Section 3], [6, Section 3.1, 3.2.1], Basu–Lindsay approach [6, Section 3.5], Cao et al. modified approach [15] and so on are used for a continuous estimate of ====. The resulting estimating equation is of the form ====where ==== is some continuous estimate of ====. To avoid this smoothing, Broniatowski et al. derived a duality technique where one first finds a dual representation for the Hellinger distance and then minimizes the empirical estimate of this dual representation to find the estimator. The empirical estimate of this dual representation does not require any smoothing. See [9], [10], [11], [12], [48], [60] for details.====The following estimating equation, where the score function is weighted by power of model density and equated to its hypothetical one, was proposed by Basu et al. [5]: ====where ====. Motivated by the works of Field and Smith [29] and Windham [66], an alternate estimating equation, where the weights are further normalized, was proposed by Jones et al. [36]: ====where ====. Notice that (9), (10) do not require the use of empirical distribution. Hence no smoothing is required in these cases. The estimators of (8), (9), (10) are consistent and asymptotically normal [5, Theorem 2], [36, Section 3], [7, Theorem 3]. They also satisfy two invariance properties, one when the underlying model is re-parameterized by a one-one function of the parameter [5, Section 3.4], and the other when the samples are replaced by some of their linear transformation [59, Theorem 3.1], [5, Section 3.4]. They coincide with the ML-estimating equation (5) when ==== under the condition that ====. The estimating equations (5), (8), (9), (10) are, respectively, associated with the divergences in (4), (1), (2), and (3) in a sense that will be made clear in the following.====Observe that the estimating equations (5), (8), (9), and (10) are implications of the first order optimality condition of maximizing, respectively, the usual log-likelihood function ====and the following generalized likelihood functions ==== The above likelihood functions (12), (13), (14) are not defined for ====. However it can be shown that they all coincide with ==== as ====.====It is easy to see that the probability distribution ==== that maximizes (12), (11), (13) or (14) is same as, respectively, the one that minimizes ==== or the empirical estimates of ====, ==== or ====. Thus for MLE or “robustified MLE”, one needs to solve ====where ==== is either ====, ====, ==== or ====; ==== when ==== is ==== or ==== and ==== when ==== is ====. Notice that (8) for ====, (9), (10) for ====, do not make sense in terms of robustness. However, they still serve as first order optimality condition for the divergence minimization problem (15). A probability distribution that attains the infimum is known as a reverse ====-projection of ==== on ====.====A “dual” minimization problem is the so-called forward projection problem, where the minimization is over the first argument of the divergence function. Given a set ==== of probability distributions with support ==== and a probability distribution ==== with the same support, any ==== that attains ====is called a forward ====-projection of ==== on ====. Forward projection is usually on a convex set or on an ====-convex set of probability distributions. Forward projection on a convex set is motivated by the well-known maximum entropy principle of statistical physics [34]. Motivation for forward projection on ====-convex set comes from the so-called non-extensive statistical physics [40], [61], [62], [63]. Forward ====-projection on convex set was extensively studied by Csiszár [18], [19], [22], Csiszár and Matúš [23], [24], Csiszár and Shields [25], and Csiszár and Tusnády [26].====The forward projections of either of the divergences in (1)–(4) on convex (or ====-convex) sets of probability distributions yield a parametric family of probability distributions. A reverse projection on this parametric family turns into a forward projection on the convex (or ====-convex) set, which further reduces to solving a system of linear equations. We call such a result a projection theorem of the divergence. These projection theorems were mainly due to an “orthogonal” relationship between the convex (or the ====-convex) family and the associated parametric family. The Pythagorean theorem of the associated divergence plays a key role in this context.====Projection theorem of the ====-divergence is due to Csiszár and Shields [25, pp. 24] where the convex family is a linear family and the associated parametric family is an exponential family. Projection theorem for ====-divergence was established by Kumar and Sundaresan [41, Theorem 18 and Theorem 21], where the so-called ====-power-law family (====-family) plays the role of the exponential family. Projection theorem for ====-divergence was established by Kumar and Sason [39, Theorem 6], where a variant of the ====-power-law family, called ====-exponential family (====-family), plays the role of the exponential family and the so-called ====-linear family plays the role of the linear family. Projection theorem for more general class of Bregman divergences, in which ==== is a subclass, was established by Csiszár and Matúš [24] using techniques from convex analysis. (See also [50].) We observe that the parametric family associated with the projection theorem of ====-divergence is closely related to the ====-power-law family, which we call a ====-family.====Thus projection theorems enable us to find the estimator (MLE or any of the generalized estimators) as a forward projection if the estimation is done under a specific parametric family. While for MLE the required family is exponential, for the generalized estimations, it is one of the power-law families.====Our main contributions in this paper are the following.====Rest of the paper is organized as follows. In Section 2, we first generalize the power-law families to the continuous case and show that the Student and Cauchy distributions belong to this class. We also introduce the notion of regularity to these power-law families and establish the relationship among them in this section. In Section 3, we establish projection theorems for the general power-law families. In Section 4, we apply projection theorems to Student and Cauchy distributions to find generalized estimators for their parameters. We also perform some simulations to analyze the efficacy of such estimators. We end the paper with a summary and concluding remarks in Section 5. In the supplementary article, we establish projection theorem of ====-divergence in the discrete case using elementary tools and identify the parametric family associated with this divergence. We also present the detailed derivation of some of the results in this part.",Projection theorems and estimating equations for power-law models,https://www.sciencedirect.com/science/article/pii/S0047259X21000129,17 February 2021,2021,Research Article,173.0
"Wang Jia,Cai Xizhen,Li Runze","Department of Statistics, Pennsylvania State University, University Park, PA 16802, USA,Department of Mathematics and Statistics, Williams College, Williamstown, MA 01267, USA","Received 4 March 2020, Revised 31 January 2021, Accepted 31 January 2021, Available online 13 February 2021, Version of Record 24 February 2021.",https://doi.org/10.1016/j.jmva.2021.104733,Cited by (1),Most existing methods of variable selection in partially linear models (PLM) with ultrahigh dimensional ,"Semiparametric model attracts considerable attention in the literature since it retains the interpretability of the parametric models and keeps some flexibility of the nonparametric models. In this paper, we study a type of commonly used semiparametric model, the partially linear model (PLM). The PLM assumes that, the response ==== depends both linearly on some covariates ==== of interest, and nonparametrically on another univariate continuous covariate ==== defined on ====. Suppose that the observed data ====, is a random sample from the following PLM ====This PLM specifies a parsimonious linear function in the parametric part, while allowing a nonparametric component to be unconstrained and subject to empirical estimation. In this paper, a new one-step Bayesian approach is proposed to select variables for PLM with ultrahigh dimensional covariate ====, that is ====. Specifically, our proposed method simplifies the procedure by avoiding estimating the infinite dimensionality brought by the nonparametric component and results in sparsity in the linear component.====The estimation procedure for PLM with fixed dimension ==== of ==== has been extensively studied. Engle et al. used the penalized least squares method to estimate ==== and the nuisance function ==== simultaneously by adding a penalty on the roughness of ====, which was referred as the partial smoothing splines [6], [7], [12], [23]. Since ==== is of primary interest, some other methods brilliantly avoid the estimation of ====. For example, Robinson [24] introduced a profile least squares estimator based on the idea of partial residual, which later became one of the commonly used approaches to eliminate the nonparametric component in PLM. Another type of approach to eliminate the nonparametric component is the difference-based method [27], [29]. It estimates the coefficients in linear component by taking differences of the ordered observations. The resulting estimator is proven to be asymptotically efficient under finite dimensions. See Section 2.1 for more details about the difference-based method.====Variable selection for PLM can be accomplished by adding another penalty function on ==== to the loss function of the aforementioned partial smoothing splines method. The least absolute shrinkage and selection operator (LASSO) [25], the nonnegative garrote [2], [31], the smoothly clipped absolute deviation (SCAD) [8], the elastic net [34], and the minimum concave penalty (MCP) [32] are all among popular choices of penalty functions. Xie and Huang [28] used the SCAD penalty to achieve the sparsity in the linear part and used the polynomial splines to estimate the nonparametric component simultaneously. The resulting estimator ==== was shown to be consistent with ====. An alternative is to use a two-step procedure, in which ==== and ==== are first regressed on ==== separately to get partial residuals, then the variable selection is further applied on the transformed model. For example, the consistency for both the linear and the nonparametric components has been well studied by Zhu et al. [33] under the regime of ====. Liang and Li [18] discussed this approach in the presence of measurement errors. More recently, Liu et al. [20] proposed a selection procedure via recursively test on the partial correlation among the partial residuals and among the covariates when ====. The method was referred as the thresholded partial correlation on partial residuals (TPC-PR). However, to the best of our knowledge, there is nearly no literature on variable selection in the high-dimensional setting based on the extension of difference-based method.====Bayesian approach puts priors on the parameters and the model space, and selects the model with the highest posterior probability. There have been multiple developments for variable selection using Bayesian approach with linear and generalized linear models. George and McCulloch [11] proposed a milestone method of Bayesian variable selection via stochastic search. They introduced a latent binary vector to indicate the inclusion of variables in linear models, and then placed a mixture spike and slab prior on each coefficient conditioning on this latent vector. Following this approach, many other selection procedures with similar structure have been proposed. The distinction between them is mostly in the form of the spike and slab priors, or in the form of the prior on the model space. To alleviate the difficulty in choosing specific prior parameters, several approaches have been proposed, see [10], [14], [30]. However, these papers focused on small-scaled questions and did not discuss any possible extension to the high-dimensional setting. More recently, Ishwaran and Rao [15] established the oracle property of the posterior mean as ==== converges to infinity with fixed ==== under certain conditions on the prior variances for linear models. Johnson and Rossell [16] proved selection consistency under ==== for a non-local prior in linear model settings. Liang et al. [19] proposed a point-mass spike prior with a slab prior depending on the model size, and proved the posterior consistency under ==== in generalized linear models, but the corresponding conditions are relatively strong. Additionally, the step-wise estimation procedure is not efficient. Narisetty and He [21] also used Gaussian prior but argued the prior should be sample-size dependent, referred to as Bayesian shrinking and diffusing priors (BASAD), and obtained strong selection consistency when ==== for linear models under mild assumptions. However, BASAD is not computationally practical for large-==== problems, since it requires to update ==== from a ====-dimensional multivariate normal distribution in each iteration. Recently Narisetty et al. [22] proposed Skinny Gibbs (SG) algorithm to address this computation issue via sparsifying the precision matrix. They referred to this kind of update as Skinny Gibbs (SG) and argued that it is a scalable method, namely the required computation time grows approximately linearly in ====. The selection consistency was proved for the logistic regression. While spike and slab priors have been widely used in applications for its attractive interpretability, the theory for spike and slab models has not caught up with the applications. Again, all the aforementioned papers focused on linear or generalized linear models, and the corresponding work on semiparametric or nonparametric models under high dimensional setting is limited.====In this paper, we propose a Bayesian subset selection procedure for the partially linear model. We incorporate the difference-based method in the prior for the nonparametric component. For the parametric component, we adopt a modified version of Bayesian shrinking and diffusing priors (BASAD) [21] and propose the novel Bayesian subset modeling with diffusing prior (BSM-DP). We use a normal distribution with a diverging variance as the slab prior and a normal distribution with a small variance as the spike prior. Differently from BASAD, the response variable in our model only depends on the active covariates. This conveniently allows us to sample coefficients separately for the active and the inactive sets during the estimation. In fact, the spike prior has no impact on the theoretical result, so any proposal including a point mass will work. As a practical note, we recommend a Gaussian distribution with a small variance, which allows more flexibility for the Markov chain to explore the model space, and hence avoids local trap. As a result, the proposed methods are more computationally efficient than BASAD. We also notice that the Skinny Gibbs (SG) [22] is a special case of BSM-DP when the variance of spike prior is set to be proportional to ====. Their original paper [22] discussed logistic regression only. We establish the selection consistency for the parametric component in partially linear models when ==== under mild conditions.====The rest of the paper is organized as follows. In Section 2 we present the Bayesian subset modeling with diffusing prior (BSM-DP) and discuss variable selection for partially linear model, followed by the estimation procedure, regularity conditions and theoretical results. Performances of several numerical studies are presented in Section 3 to demonstrate reliability of the proposed model. We further apply the proposed method on the supermarket data set. Proofs for lemmas and theorems are given in Section 4, followed by discussions in Section 5.",Variable selection for partially linear models via Bayesian subset modeling with diffusing prior,https://www.sciencedirect.com/science/article/pii/S0047259X21000117,13 February 2021,2021,Research Article,174.0
"Liu Jialuo,Chu Tingjin,Zhu Jun,Wang Haonan","Department of Statistics, Colorado State University, United States of America,School of Mathematics and Statistics, University of Melbourne, Australia,Department of Statistics, University of Wisconsin-Madison, United States of America","Received 19 February 2020, Revised 4 February 2021, Accepted 5 February 2021, Available online 13 February 2021, Version of Record 24 February 2021.",https://doi.org/10.1016/j.jmva.2021.104735,Cited by (0),"Spatio-temporal processes with a continuous index in space and time are useful for modeling spatio-temporal data in many scientific disciplines such as environmental and health sciences. However, approaches that enable simultaneous estimation of the mean and ==== for such spatio-temporal processes are limited. Here, we propose a flexible spatio-temporal model with partially linear regression in the mean function and local ","In this paper, we develop new semiparametric methodology and theory for spatio-temporal processes where both space and time are continuously indexed, which often arise in many scientific disciplines [see, e.g., 34]. An illustrative data set comprises measurements of a health hazard taken in an indoor environment by both static sensors at fixed sampling locations and roving sensors at varying sampling locations over time [31]. The spatio-temporal sampling design is non-standard due to data irregularity and sparsity in both space and time, calling for development of novel methodology and theory.====In spatial statistics, geostatistical data with continuous spatial index and lattice data with discrete spatial index often require different modeling techniques. For example, to account for spatial dependence, a Matérn covariance function is typically used for geostatistical data, while a spatial weight matrix is used for lattice data [8]. For spatio-temporal datasets, the distinction between continuous and discrete index applies to both spatial and temporal dimensions. To analyze datasets with continuous spatial index and discrete temporal index, time series methods for temporal data are often combined with geostatistical methods for spatial data. For example, Stroud et al. [41] developed a state space model where spatial variability is captured by a locally weighted mixture of linear regressions while the regression coefficients are allowed to vary with time. Sun et al. [42] proposed a profile likelihood based estimation procedure for a semiparametric spatial dynamic model with a nonlinear spatial trend. Al-Sulami et al. [1] considered a nonlinear spatio-temporal model to investigate the relationship between housing price index (HPI) and consumer price index (CPI) for individual states in the USA. The aforementioned spatial time series methods can capture nonlinearity and nonstationarity in space and/or time, assuming that data are observed at regular and discrete time points. In contrast, for spatio-temporal data with continuous temporal index, approaches that enable simultaneous estimation of the mean and covariance functions are limited. While the existing methods focus primarily on linear regression models [see, e.g., 11], we will develop semiparametric methods and theory for continuously indexed spatio-temporal processes.====The underlying spatio-temporal process can be decomposed into a mean trend and a spatio-temporal error process. Partially linear models offer a flexible way to model the mean trend of spatio-temporal data. For independent data, partially linear models have been extensively studied [14], [22], [26], [27], [37]. For spatio-temporal data, Gao et al. [18] proposed an estimation procedure based on marginal integration for geostatistical partially linear models, and Lu et al. [29] developed spatio-temporal varying coefficient models, which can be applied to spatio-temporal partially linear models. Theoretical property is established for both works under the spatio-temporal mixing conditions. Since both works focus on estimating the mean trend of spatio-temporal data, spatio-temporal error is treated as independent in estimation. In practice, there is considerable interest in spatio-temporal covariance functions, which characterize the spatio-temporal dependence of underlying processes. Furthermore, to interpolate unsampled spatial locations and time points (spatio-temporal kriging), spatio-temporal covariance functions are a key building block. Thus, there is clearly a need for statistical methods to estimate spatio-temporal covariance functions and here, we aim to develop new methodology which allows simultaneous estimation of the mean and covariance functions.====Various types of spatio-temporal covariance functions have been developed [7], [9], [16], [19], [35], [39]. However, the dependence structure in spatio-temporal data poses challenges for establishing the asymptotic properties. In spatial statistics, there are three commonly used asymptotic frameworks, namely, increasing-domain asymptotics, fixed-domain asymptotics and mixed-domain asymptotics. For increasing-domain asymptotics, the spatial domain expands as the number of observations increases [6], [10], [32]. For fixed-domain asymptotics, the spatial domain is fixed and the sampling locations get denser [28], [38], [45], [46]. A mixed-domain asymptotic framework allows both spatial domain and sampling density to increase [4], [21], [23], [30]. For spatio-temporal processes, Bandyopadhyay et al. [3] considered an increasing temporal domain and a mixed spatial domain for a Fourier analysis.  Chu et al. [5] proposed a spatio-temporal expanding distance (STED) asymptotic framework in a fixed spatio-temporal domain, which extends the aforementioned asymptotic frameworks for spatial domain to spatio-temporal domain for exploring the asymptotic properties of statistical inference for spatio-temporal processes. The STED framework also paves the way for studying the local behavior of a spatio-temporal process, especially the second-order properties. Here, we will consider a locally stationary spatio-temporal covariance function, introduced by Chu et al. [5], to study the slowly-varying second-order nonstationarity under the STED asymptotic framework.====In essence, the mean trend of spatio-temporal data is modeled through partially linear models, and spatio-temporal dependence is accounted by locally stationary spatio-temporal covariance functions. The resulting model provides a flexible way to analyze continuously indexed spatio-temporal datasets. For estimation, the main challenge is to incorporate spatio-temporal covariance functions, which we overcome by profiling the spatio-temporal likelihood function. In addition, the theoretical property of proposed method is investigated under STED framework, and both consistency and asymptotic normality are established. Furthermore, a proper bandwidth selection is critical for estimation. For ==== data, various methods have been studied, notably cross validation [see, e.g., 13], but are known to not perform well for non-==== data [2], [12], [24], [33]. Here, we show that cross-validation is asymptotically biased in the presence of spatio-temporal correlated errors for most commonly used kernels. We also propose a cross-validation based method with bimodal kernels to alleviate this bias in bandwidth selection.====The remainder of the paper is organized as follows. Section 2 introduces the spatio-temporal model and the profile likelihood method. The asymptotic properties of the profile likelihood estimation are established in Section 3 under suitable regularity conditions. In Section 4, we discuss the choice of kernel functions and develop a procedure for bandwidth selection. Numerical examples including a simulation study and the health hazard data example are given in Sections 5 Simulation study, 6 Data example, respectively. Appendix A contains the technical details including proofs, while additional simulation results are given as Supplementary Materials.",Semiparametric method and theory for continuously indexed spatio-temporal processes,https://www.sciencedirect.com/science/article/pii/S0047259X21000130,13 February 2021,2021,Research Article,175.0
"Zhang Xu,Tian Yahui,Guan Guoyu,Gel Yulia R.","School of Mathematical Sciences, South China Normal University, Guangzhou, China,Key Laboratory for Applied Statistics of the MOE, and School of Mathematics and Statistics, Northeast Normal University, Changchun, China,Department of Biostatistics and Data Sciences, Boehringer Ingelheim, China,School of Economics and Management, Northeast Normal University, Changchun, China,Department of Mathematical Sciences, University of Texas in Dallas, USA","Received 24 July 2020, Revised 27 January 2021, Accepted 27 January 2021, Available online 11 February 2021, Version of Record 2 March 2021.",https://doi.org/10.1016/j.jmva.2021.104732,Cited by (2)," of the new classification approach and validate its finite sample properties via extensive simulations. The proposed geometrically-enhanced classification method is illustrated in application to user analysis of the one of the largest Chinese social media platforms, Sina Weibo.","Classification constitutes one of the key tasks in modern statistical and data sciences, with methods ranging from more conventional tools such as logistic regression and linear discriminant analysis, to advanced machine learning techniques such as deep learning, see, e.g., [13], [14], [22], [27], [28]. Many traditional classification methods tend to rely on the assumption of independence among subjects. However, in the real world, “connections” or “relationships” are typically inevitable, and information conveyed by the link often provides an important insight into the classification process. With the recent progress of data acquisition technology, the collection of such relational information becomes realistically feasible, thereby making it easier to integrate this critical aspect into the classification process.====In turn, relational data, formed by a collection of entities and connections among them can be described using the concept of complex networks [3], [33]. Here, nodes represent, for example, observed individuals, with a set of the associated node attributes, for instance, socio-demographic information, and edges correspond to relationships between individuals. Among the most recent classification approaches for such relational data are machine learning tools, e.g., embedding approaches [12], neural networks [39] and network-based regularization algorithms [4], [24], which have gained a particular popularity in bioinformatics and health care applications [34]. In addition, various logistic regression techniques, e.g., network-based logistic regression model, are widely adopted as one of the primary benchmark approaches in statistical sciences and machine learning for addressing the classification problem for relational data [40]. Intuitively, such relational data and the associated resulting complex networks are likely to be both heterogeneous and exhibit a highly nontrivial geometric structure, for instance, driven by various hidden communities and substructures that are not detectable with conventional Euclidian-based metrics. Nevertheless, despite an ever increasing interest in systematic assessment of network dependence in classification and prediction tasks and a growing evidence of the key role of data shape in organization of complex systems, most existing classification tools for relational data tend to neglect the intrinsic geometry of the observed relational data.====In this paper we introduce the concept of data depth to classification of relational data which we describe as a complex network with multiple node attributes. Data depth is a nonparametric data-driven method that systematically accounts for the underlying data geometry by assigning an ordering to each data point with respect to its position within a given data cloud or probability distribution [26], [42]. A higher value of a data depth implies a higher centrality in the data cloud. The depth contour with such a natural center-outward ordering serves as a topological map of the data. As a result, clusters and outliers can be then evaluated simultaneously in a quick and visual manner. In addition to statistical sciences, data depth is rapidly gaining its popularity in machine learning and data sciences due to its wide applicability in classification, anomaly detection, and data visualization, for overview see, e.g., [15], [17], [19], [25], [37] and references therein. Despite a high utility of depth methods in multivariate and functional data analysis, data depth remains largely under-explored in complex network analysis. Among recent efforts in this direction are unsupervised depth-based clustering of graphs [7], [35], depth-based classification (without node attributes) [36], and depth-based analysis of a random sample of graphs following a probability model on the space of all graphs of a given size [10].====Our goal in this paper is to explore utility of data depth to classification of relational data in a form of a complex network with multiple node attributes. We propose a probabilistic model for the observed data that consists of two parts. First, a logistic regression model is employed to describe relationships between the class label and attributes of each node. Second, we introduce a network model where the link probability between any two nodes is assumed to depend on their class labels and data depths of the nodes in the respective classes. The above two parts constitute the depth-based network classification model (DNC). The parameters of the DNC can then be estimated by a maximum likelihood method within a logistic regression framework [14]. Third, the approximate prediction rules of the DNC are obtained using approximate simplification from posterior probability of the unknown class label. Finally, we derive asymptotic properties of DNC as the network order tends to infinity under some mild conditions.====The rest of the paper is organized as follows. Section 2 introduces the definition of data depth, the DNC model and its corresponding approximate prediction rule, and the asymptotic classification theory. In Section 3, the proposed geometrically-enhanced classification approach is validated on a broad range of synthetic data and is illustrated in application to user classification of the one of the largest Chinese social media, Sina Weibo. The paper concludes in Section 4 with a discussion and future research directions. All theoretical derivations are presented in the Appendix.",Depth-based classification for relational data with multiple attributes,https://www.sciencedirect.com/science/article/pii/S0047259X21000105,11 February 2021,2021,Research Article,176.0
"Ding Hao,Qin Shanshan,Wu Yuehua,Wu Yaohua","Department of Mathematics and Statistics, York University, Toronto, ON M3J 1P3, Canada,Department of Statistics and Finance, University of Science and Technology of China, Hefei 230026, China","Received 5 May 2019, Revised 26 January 2021, Accepted 26 January 2021, Available online 9 February 2021, Version of Record 25 February 2021.",https://doi.org/10.1016/j.jmva.2021.104730,Cited by (1),", the number of observations, are large with ====. Unlike previous works that focus on a sparse regression vector ====, we consider a more interesting situation in which ","Consider the following linear model, ====where ====, ====, ==== is an unknown regression vector, and ==== are unobservable random errors. A well-known method for estimating ==== is the least-squares estimation (LSE) which is not only mathematically convenient but also efficient for normally distributed errors. However, the LSE is sensitive to outliers and not stable with respect to deviations from various assumptions. Huber [15], [16] thus introduced M-estimation of ==== by minimizing ====for a discrepancy function ====, which plays an important and complementary role in the development of robust estimation methods. In the past half century, many procedures based on M-estimation have been proposed in the literature, and their asymptotic properties have been investigated. Generally speaking, asymptotic theories of M-estimation in linear regression models are under three main regimes: (i) the classical regime that allows the sample size ==== to go to infinity but fixes the number of predictors ==== (see, e.g., [5], [13], [16]); (ii) the second regime permits both ==== and ==== to go to infinity but requires that ==== [3], [14], [21]; (iii) modern papers consider the following two cases: (a) ==== with ==== [11], [18] and with ==== [9], [10]; (b) ==== [22], [23], [26]. El Karoui et al. [11] proposed a nonlinear system of two deterministic equations to characterize the behavior of M-estimator under random designs where both ==== and ==== are random. In addition, El Karoui [9] studied ridge-regularized estimators under these conditions. Recently, El Karoui [10] also presented rigorous proofs for more general situations, with ==== following an ‘elliptical-like’ distribution and ==== being heavy-tailed. In [18], Lei et al. investigated the asymptotic distributions of regression M-estimates components under the fixed design case where ====’s are the only source of randomness.====In many statistical applications, there is more than one correlated response variable. The following general multivariate linear regression model has thus been considered, ====where ==== is an unknown regression vector, ==== are response vectors (==== is fixed), ==== are independently and identically distributed (i.i.d.) random error vectors, with unknown covariance matrix ====, and ==== are random matrices, having covariance matrix ==== among rows and ==== among columns. The matrix-variate now is very common in applications pertaining to neuroimaging, financial market trading, macroeconomics analysis [20], environmental studies [32], and among others. This model encompasses the traditional linear regression model when ====. Bai et al. [2] developed asymptotic theories of M-estimation of ==== for model (1) under the regime (i). In this paper, we study this model under the regime (iii), with ====, ====, which is more challenging and has been rarely studied in the literature. A natural question that one may ask is why we consider this setting, but not others. In a real application where both ==== and ==== are given, it is easy to find asymptotic properties of the estimator under this regime, as is justified in [33]. The framework of ==== works well for a wide range of ==== pairs with ==== or ====.====Denote the Kronecker product of matrices ==== and ==== by ====, and define an ====-dimensional vector Vec==== of a matrix ==== by stacking its column vectors. To simplify the description, throughout this paper, we further assume that ==== follows matrix normal distribution, ====, then equivalently, Vec====. This can be done by normalizing ==== given ====, or ==== given the covariance matrix ====, where ====. Couillet et al. [6] demonstrated that the robust scatter matrix estimator of a multivariate elliptical population, including multivariate normal as a special case, behaves similar to a well-known random matrix model in the limiting regime where the dimension ==== and sample size ==== grow at the same speed. In the context of high-dimensional settings, most of the estimation methods rely on structural assumptions on the covariance matrix. One can refer to [32] for more details. Another way to address the normalization is to estimate ==== and ====. Dutilleul [7] explored the maximum likelihood estimation (MLE) of both covariance matrices when ==== is matrix normal distribution and ==== are fixed. Leng and Pan [19] studied a novel Kronecker-structured approach for estimating such matrices, which allows the dimensionality of data to have arbitrary order even for fixed sample size, and works for flexible distributions beyond normality. Recently, Niu and Zhao [27] shed lights on the covariance matrix of a high-dimensional matrix-variate associated with Kronecker expansion using a transelliptical distribution and Kendall’s ==== correlation. Zhang et al. [32] proposed a class of distribution-free regularized covariance estimation methods for matrix-valued data, under a separability condition and a bandable covariance structure, which permits both ==== and ==== to be much larger than ====.====Given the high-dimensionality of ====, it is usually assumed that ==== is a sparse vector, whose components are mostly zeros. In practice, however, one may encounter the situations where the majority of components are small but possibly not zero. Under this circumstance, a sparse assumption may no longer be reasonable since the error caused by the small components of ==== is nonnegligible. We thus consider that the regression coefficients are composed of two groups. The components of ==== in group I are as large as ====, while other components of ==== that consist of group II are small but maybe nonzero. Denote ====, where ==== and ==== are respectively the supports of ====, with large and small values. Specifically, ==== for ==== and ==== for ====
 (====). Past literature has focused on the estimation of ==== but not ====. This study aims to explore the asymptotic behavior of the ridge-regularized high-dimensional multivariate M-estimator of ====. By extending the double leave-one-out method of [9], [10] to the general multivariate linear regression model (1), we successfully derive a nonlinear system composed of two deterministic equations, the unique solution of which characterizes the risk behavior of the M-estimator of ====. An analogous result of the unregularized high-dimensional multivariate M-estimator of ==== can be derived by letting the regularization parameter ==== be 0.====Our study makes two major contributions. First, we consider an interesting assumption of ==== that allows a finite number of components to be large while the rest to be small. We examine the asymptotic behavior of the M-estimator of ==== using a nonlinear system under the framework of ==== in the model (1). Based on the solution of the system, we also derive asymptotic normality for each component of the M-estimator. Second, in contrast to [9], [10], we consider the case where the covariates are matrix-variates and the responses, random errors, as well as the regression coefficients, are vector-valued. Although we leverage the system derivation and the associated proofs of [9], [10], we overcome many technical challenges associated with the matrix-valued framework. Some proof techniques for the univariate case are not applicable here. Main challenges are the approximations of leaving one observation out residuals and leave one predictor out residuals, which are not straightforward extensions of a univariate case. Specifically, we examine the approximations of two vectors or matrices, and we need to estimate their differences in all entries. Another technical novelty is that we introduce a vector-valued function ====, ====, which is essential in the proofs of Theorem 2.====The remainder of this paper is organized as follows. In Section 2, we state the model assumptions and the main theoretical results. We provide validating examples that demonstrate our proposed methodology in Section 3. We present some conclusions in Section 4. We detail the double leave-one-out procedure in Appendix A. Proofs of the main results and propositions are given in Appendix B.====Throughout the rest of this paper, the following notations and definitions will be used.====Note that ==== hereafter denotes a positive constant independent of ==== and ====, which may take different values in different formulas.",Asymptotic properties on high-dimensional multivariate regression M-estimation,https://www.sciencedirect.com/science/article/pii/S0047259X21000087,9 February 2021,2021,Research Article,177.0
"He Daojiang,Liu Huanyu,Xu Kai,Cao Mingxiang","School of Mathematics and Statistics, Anhui Normal University, Wuhu 241002, China","Received 18 December 2019, Revised 25 January 2021, Accepted 25 January 2021, Available online 5 February 2021, Version of Record 15 February 2021.",https://doi.org/10.1016/j.jmva.2021.104731,Cited by (2),"In the high dimensional setting, this article explores the problem of testing the complete independence of random variables having a ====. A natural high-dimensional extension of the test in Schott (2005) is proposed for this purpose. The newly defined tests are asymptotically distribution-free as both the sample size and the number of variables go to infinity and hence have well-known critical values, accommodate situations where the number of variables is not small relative to the sample size and are applicable without specifying an explicit relationship between the number of variables and the sample size. In practice, as the true alternative hypothesis is unknown, it is unclear how to choose a powerful test. For this, we further propose an adaptive test that maintains high power across a wide range of situations. An extensive simulation study shows that the newly proposed tests are comparable to, and in many cases more powerful than, existing tests currently in the literature.","With the rapid development of science and technology, high-dimensional data has a wide range of applications in various fields, such as internet portals, microarray analysis, hyperspectral images, DNA and other fields. More examples of high-dimensional data can be found in [7] and [16]. Statistical analysis of high-dimensional data has attracted widespread attention in the literature.====Classical statistical methods assume that the dimension ==== remains unchanged with the increase of the sample size ====. However, the feature of high-dimensional data setting is that the sample size ==== is not much larger, or even far less than the dimension ====, [27], [28]. Obviously, classical statistical methods cannot be directly applied to these high-dimensional data. Therefore, more and more methods for processing high-dimensional data are proposed, see, e.g., [1], [6], [13], [17], [26] among others. In many fields of statistical research, the independence test of variables has been paid much attention, the traditional theory of variables independence has been widely studied, such as [10], [23], [31]. Recently, there are some work on the high-dimensional complete independence test, see for example [4], [5], [9], [12], [18], [19], [20], [21], [22], [25], [28], [30].====Given a random vector ==== of multivariate normal distribution with mean vector ==== and covariance matrix ====, let ==== be the population correlation matrix. We are interested in testing whether the ==== variables ==== are independent, that is, whether the covariance matrix ==== is a diagonal matrix, or the correlation coefficient matrix ==== is an identity matrix. Suppose that we have a random sample ====,…,====, where the sample size is ====, the sample covariance matrix ==== and the correlation matrix ====. Then the hypothesis of the complete independence is equivalent to test ====[23] proposed a likelihood ratio test, which is a function of the determinant of the correlation matrix ====, it is quite effective to test the independence of variables in the context of classic multivariate analysis. Later, [11], [14], [15] obtained the limiting distribution of the likelihood ratio test statistic when ==== and ====. However, when ====, this procedure is invalid, since ==== in this case.====[28] presented a sum-of-squares-type test to test (1), the test statistic is ====Under ====, assuming that ====, [28] showed that ==== converges to a normal distribution with mean 0 and variance ====. Under the same assumption, [12] used different statistics to study similar complete independence test problems.====[29] further proposed a test statistic for the diagonality of the covariance matrix: ====where ====, ====, ==== and ====. It is shown in [29] that under ==== and some assumptions, ==== converges to the standard normal distribution.====It is worth noting that, when the dependence among data is weak, the power of Schott’s test may be poor since ==== is very close to 0 in this case. Consequently, an improved test was proposed by [19], which is ====where ====. Under the same assumptions, [19] obtained similar asymptotic results as [28]. The simulation results showed that in some cases of weak dependence, the test ==== is more powerful than that in [28]. In addition, [20] also showed that the test statistic ==== in [28] still converges asymptotically to a normal distribution without the requirement that ====.====Recently, more and more literatures began to study sum-of-powers test for different test problems, such as [9], [18], [32]. Moreover, [18] proposed a new testing procedure ==== against more general alternatives for high-dimensional independence tests. The tests introduced in this paper are inspired by [9], who proposed tests for (1) that are based on the sample covariance matrix. The main advantage of using the sample correlation matrix over the sample covariance matrix is that it is scalar-invariant and thus is particularly useful when different components have different scales in high-dimensional data. Based on the test statistic in [28], we propose a sum-of-powers test to test the independence of variables. Our main goal is to include multiple tests in the process so that at least one of them can generate high power under different covariance structures. Firstly, we prove that the asymptotic distribution of test statistics converges to normal distribution. Secondly, we show that several sets of test statistics are asymptotically independent. Finally, on the basis of sufficient theory, we develop a new way for calculating asymptotic ====-value for adaptive test.====The rest of the paper is organized as follows. The proposed tests and asymptotic results are presented in Section 2. A simulation study is carried out to show the performance of the proposed tests in Section 3. All the proofs of theorems are deferred to Appendix.",Generalized Schott type tests for complete independence in high dimensions,https://www.sciencedirect.com/science/article/pii/S0047259X21000099,5 February 2021,2021,Research Article,178.0
"Riva-Palacio Alan,Leisen Fabrizio","Universidad Nacional Autónoma de México, Mexico,University of Nottingham, UK","Received 22 February 2020, Revised 19 January 2021, Accepted 19 January 2021, Available online 4 February 2021, Version of Record 16 February 2021.",https://doi.org/10.1016/j.jmva.2021.104728,Cited by (2),"Lévy ==== are an important tool which can be used to build dependent ====. In a classical setting, they have been used to model financial applications. In a ","Vectors of subordinators, namely a real valued non-decreasing stochastic process with independent increments, are an important class of processes which have been used for the modeling of data arising from multiple components. For example, [33] solve the ruin problem for a bivariate Poisson process, [30] uses a vector of Gamma processes to construct a multivariate variance gamma model for financial applications and [7] perform parameter estimation for bivariate compound Poisson processes which they apply to an insurance dataset. [8] focus on parameter estimation for a vector of stable processes, [17] deal with a vector of gamma processes, and [9] present a two-step estimation method for general multivariate Lévy processes. In the context of Bayesian non-parametric statistics, vectors of subordinators have been used to construct dependent priors to model heterogeneous data; the celebrated Dirichlet Process, introduced in [10], can be seen as a normalized Gamma subordinator. In the context of survival analysis, [5] employs 1-dimensional subordinators to build the so-called neutral to the right priors. More complex Bayesian nonparametric priors based on vectors of subordinators have been proposed such as, the vectors of dependent random measures in [22], obtained through entrywise normalization of a vector of subordinators or the multivariate survival priors in [6] and [26] which use vectors of subordinators to extend the neutral to the right priors into a partially exchangeable setting. [16] proposed a vector of generalized gamma processes and [21] proposed a vector of Poisson–Dirichlet processes constructed using a vector of stable processes. More recently [1], [2] proposed flexible dependent priors to model data heterogeneity.====The dependence structure for the entries of a vector of subordinators is particularly important for application purposes. In this context, the main approach to model the dependence is the one of Lévy copulas where in analogy with distributional copulas, see [25], the marginal behavior of the vector of subordinators can be decoupled from the dependence structure. As highlighted in [31], Lévy copulas have found important applications in statistical inference for vectors of Lévy processes, the study of multivariate regular variation and risk management applications. In Section 4 we will focus on parameter inference as discussed in [7], [8], [9].====In a Bayesian non-parametric framework [12] introduced a class of vector of subordinators which relies on a one-dimensional subordinator and a ====-variate probability distribution in ==== to determine the corresponding vector. Although their construction relies on the concept of completely random measures, see [19], in this work we use the equivalent setting of subordinators and present new results regarding these vectors which henceforward, we call compound vectors of subordinators. In particular, we present a novel compound vector of subordinators which exhibits asymmetry in its related Lévy copula. We provide a series representation for compound vectors of subordinators and exemplify its use for simulation purposes. We also provide a new criteria for compound vector of subordinators to be well defined and give formulas for the associated fractional moments of order less than one, means, variances and covariances. [12] showed the structure of the Lévy copula associated to a compound vector of subordinator in a particular case, namely when what they call the score distribution in their construction has independent and identically distributed marginal distributions; further exploration of the Lévy copula structure was not performed. In the present work we explore the general Lévy copula structure associated to a vector of compound subordinators. On the other hand, we give a tractable example of an asymmetric family of Lévy copulas arising from a compound vector of subordinators. This new family is interesting as it contains the symmetric Clayton Lévy copula as a particular case and preserves the behavior of a parameter modulating between independence and complete dependence while allowing an extra parameter ==== to modulate asymmetry. In a similar fashion to [7], we show the use of such new family for the modeling of a bivariate compound Poisson process and the related parametric inference. A simulation study for a vector of stable processes and a real data study pertaining insurance are performed. We conclude that broader classes of Lévy copulas are needed and the compound vector of subordinators approach is a valid and tractable way to do so.====The outline of the paper is the following. Section 2 introduces vectors of dependent subordinators and Lévy copulas. Section 3 and Section 4 are devoted to illustrate the main results of the paper. Section 5 includes application of the new model to simulated and real data sets. Section 6 concludes with a discussion. All the proofs of the results are in Appendix.",Compound vectors of subordinators and their associated positive Lévy copulas,https://www.sciencedirect.com/science/article/pii/S0047259X21000063,4 February 2021,2021,Research Article,179.0
Ogasawara Haruhiko,"Otaru University of Commerce, 3-5-21, Midori, Otaru 047-8501, Japan","Received 30 April 2020, Revised 21 January 2021, Accepted 22 January 2021, Available online 30 January 2021, Version of Record 15 February 2021.",https://doi.org/10.1016/j.jmva.2021.104729,Cited by (12),A unified formula for various moments of the ,None,A non-recursive formula for various moments of the multivariate normal distribution with sectional truncation,https://www.sciencedirect.com/science/article/pii/S0047259X21000075,30 January 2021,2021,Research Article,180.0
"Laketa Petra,Nagy Stanislav","Charles University, Faculty of Mathematics and Physics, Department of Probability and Math. Statistics, Sokolovská 83, 186 75 Praha 8, Czech Republic","Received 22 May 2020, Revised 3 January 2021, Accepted 12 January 2021, Available online 26 January 2021, Version of Record 4 February 2021.",https://doi.org/10.1016/j.jmva.2021.104727,Cited by (6)," assigns its depth, being a function ====. The depth of ==== quantifies how much centrally positioned a point ==== is with respect to ====, our objective is to reconstruct the measure ====. We focus on ==== atomic with finitely many atoms, and present a simple method for the reconstruction of the position and the weights of all atoms of ====, from its depth only. As a consequence, (i) we recover generalizations of several related results known from the literature, with substantially simplified proofs, and (ii) design a novel reconstruction procedure that is numerically more stable, and considerably faster than the known algorithms. Our analysis presents a comprehensive treatment of the halfspace depth of those measures whose depths attain finitely many different values.",None,Reconstruction of atomic measures from their halfspace depth,https://www.sciencedirect.com/science/article/pii/S0047259X21000051,26 January 2021,2021,Research Article,181.0
"Nakada Ryumei,Ghosh Malay,Karmakar Sayar","Graduate School of Economics, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan,Faculty of Economics, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan,Department of Statistics, University of Florida, 102 Griffin-Floyd Hall, Gainesville, FL 32611, United States of America","Received 10 June 2020, Revised 9 January 2021, Accepted 10 January 2021, Available online 19 January 2021, Version of Record 25 January 2021.",https://doi.org/10.1016/j.jmva.2021.104726,Cited by (0),"The paper considers estimation of the multivariate normal mean under a multivariate normal prior with a singular precision matrix. Such a setup appears in the multi-task averaging, serial and spatial ====. The empirical and hierarchical ","Stein [18], in his seminal paper, made the surprising discovery of the inadmissibility of the sample mean for estimating the multivariate normal mean in three or higher dimension under squared error loss. An explicit estimator dominating the sample mean was later provided by James and Stein [11]. These papers spurred a long and continued area of research which has not abated even today. A notable contribution in this regard is due to Efron and Morris, who in a series of articles (see for example, Efron and Morris [5]) provided an empirical Bayes interpretation of the James–Stein estimator. This led to a wider acceptance of the James–Stein estimator because of its applicability in real life problems.====The sample mean is a minimax estimator of the multivariate normal mean in any arbitrary dimension under a fairly general loss, including but not limited to the squared error loss. Thus any estimator dominating the sample mean also possesses the minimaxity property. Indeed, over the years, a long list of minimax estimators dominating the sample mean have been proposed under different scenarios.====James–Stein estimators, or its variants, are examples of shrinkage estimators, shrinking the sample mean towards a point or some surface. In the context of small area estimation, Fay and Herriot [6] proposed empirical Bayes shrinkage estimators which shrink area level sample means towards some regression vector.====The Fay–Herriot estimators, however, do not dominate the sample means in the original sense of Stein [18]. Indeed, to our knowledge, we are not aware of any estimators which dominate the sample mean under the Fay–Herriot model. Among others, we may refer to the estimators of Fay and Herriot [6], and also the ones proposed by Prasad and Rao [15] and Datta and Lahiri [3].====One of our original objectives was to revisit the Fay–Herriot model and find (if any) empirical or hierarchical Bayes estimators dominating the sample mean in a frequentist set up as considered by Stein and others. Our estimators are different from the ones existing in the literature. In the process, we were able to prove a result in a general framework going above and beyond the Fay–Herriot model. We can include as examples the multi-task averaging as proposed by Feldman, Gupta and Frigyik [7], the serial smoothing as considered by Yanagimoto and Yanagimoto [20], and even a spatial analogue of the latter. In addition, we are able to prove that our sufficient conditions ensuring dominance over the sample mean are exactly necessary for a wide range of empirical Bayes estimator and near necessary for the hierarchical Bayes estimator. Specifically, we have been able to prove that the unbiased estimators of the risks of our estimators are smaller than those of the sample mean under squared error loss.====The prior considered in this paper is Gaussian with a singular precision matrix. This occurs quite naturally in the Fay–Herriot model as well as in the examples mentioned earlier. Besides, our model includes the ones considered earlier in Besag, Green, Higdon and Mengersen [2], Ghosh, Natarajan, Stroud and Carlin [8] and Ghosh, Natarajan, Waller and Kim [9].====Some of the highlights of our findings are as follows. First, we have been able to show that our sufficient conditions for dominating the sample mean are very close to being necessary. This is quite different from the existing literature where one finds only conditions sufficient for the dominance. Moreover, we show that the necessary and sufficient conditions for both empirical and hierarchical Bayes estimator are related to a single restriction on eigenvalue ratios of prior covariance matrix (cf. Remarks 1, 2 and inequality in (20)). Secondly, ours is the first result under the Fay–Herriot model where minimaxity of empirical Bayes estimators is provided. Interestingly, even in this random effect model necessary for small area estimation, the dominance condition is exactly the same as above. Overall both these results generalizes Stein’s conditions for minimaxity for a general class of prior precision/covariance matrix.====The paper is organized as follows: Section 2 discusses the general framework of estimation of the multivariate normal mean under the improper singular prior model. Four examples of the singular prior distribution are also given. In Section 3, conditions for minimaxity of the empirical and hierarchical Bayes estimators are provided. Section 4 discusses the application of the singular prior model to small-area estimation, where area-level spatial information is built into random effects. Borrowing the arguments of Datta, Rao and Smith [4], we derive the second-order unbiased estimator of the conditional MSE. In Section 5, we investigate the performance of the empirical Bayes estimators through numerical and empirical studies. The concluding remarks are provided in Section 6. All the proofs are given in Appendix.",Shrinkage estimation with singular priors and an application to small area estimation,https://www.sciencedirect.com/science/article/pii/S0047259X2100004X,19 January 2021,2021,Research Article,182.0
"Hosseini Toktam,Jabbari Nooghabi Mehdi","Department of Statistics, Ferdowsi University of Mashhad, Mashhad, Iran","Received 2 January 2020, Revised 5 January 2021, Accepted 5 January 2021, Available online 14 January 2021, Version of Record 22 January 2021.",https://doi.org/10.1016/j.jmva.2021.104725,Cited by (2),"Inaccuracy measure is an important measure in ====. We provide examples to evaluate the results. Finally, in supplementary material section, by introducing estimators for the introduced inaccuracy measures, we examine some of the results using simulation methods and provide an example with real data.","In information theory, there are various measures, which have been considered recently by many researchers because of their effectiveness in various scientific fields such as the copula theory, reliability theory, and survival analysis. Systems dealing with the transmission, storage, and processing of information have become commonplace at all levels of society. We live in a population commonly referred to as the information society and one of a key role is information. Hence it is not surprising that all different sectors want to know what is information in fact and thus to acquire more knowledge in order to use their effects as far as possible. This theory is a science that deals with the concept of the size and application of information. Entropy, introduced by Shannon [33], is one of the important measures in this theory that relates information with uncertainty. So it is an uncertainty measure; see Jan [13] for more details. The Shannon entropy for the absolutely continuous random variable ==== with support ==== is defined by ====where ==== is the probability density function of ====. This measure can indicate the magnitude of the error in estimating the probability of an event occurring due to inaccurate information in the observations and can also be used as a measure to detect a model as well as determining the model dispersion. Using the concept of entropy, Popel [28] showed that the transition from a weakly turbulent plasma state to a strongly turbulent one, can be treated as a nonequilibrium phase transition. More knowledge about the concepts of entropy and its applications can be found in Cover and Thomas [6].====Inaccuracy is another measure in information theory. Kerridge [15] defined an inaccuracy measure for two discrete random variables ==== and ==== with probability mass functions ==== and ==== and countable supports ==== and ====, respectively. This measure (Kerridge’s inaccuracy) for determining ==== from ==== reads as follows: ====Subsequently, Nath [24] defined this measure as an important generalization of the Shannon entropy, for two absolutely continuous random variables ==== and ==== by ====where ==== and ==== are the probability density functions of the variables ==== and ==== and ==== and ==== are supports of ==== and ====, respectively. Kerridge [15] about one of the main reasons to pay attention to the inaccuracy measure said: “Suppose that an experimenter states the probabilities of the various possible outcomes of an experiment. His statement can lack precision in two ways: he may not have enough information and so his statement is vague, or some of them, which he has, may be incorrect. All statistical estimations and inference problems are concerned with making statements which may be inaccurate in either or both of these ways. The communication theory of Shannon and Weaver [34] provides a general theory of uncertainty which enables us to deal with the vagueness aspect of inaccuracy: the usefulness of this in statistical problems has been shown by several authors, including Lindley [19] and Kullback and Leibler [16]. However, the theory so far has not been able to deal with inaccuracy in its wider sense, and so its use has been limited. This limitation is now removed with introduce inaccuracy measure”. Also he said: “In the ordinary development of communication theory there is an interesting duality between information and entropy, or uncertainty. This is because one reasonable measure of the uncertainty of a situation is the amount of knowledge which would have to be obtained before certainty could be achieved”. Then he showed that the inaccuracy can be related to an amount of missing information.====The inaccuracy measure indicates the error that the experimenter commits in estimating the probability of events occurring due to incorrect use of the ==== distribution rather than the correct distribution of ==== and also due to incorrect information in the observations. Clearly, if ==== and ==== are identically distributed, then ====. On the other hand, since the inaccuracy measure establishes equations ==== and ====and due to the nonnegativity of the last integral, this measure and even its generalizations can be used as a statistic of goodness-of-fit test; see Park et al. [27] for more details. Also Kumar et al. [17] introduced a dynamic measure of inaccuracy between two past lifetime distributions over the interval ====, and they studied a characterization problem for this dynamic inaccuracy measure based on proportional reversed hazard rate model. Kundu et al. [18] studied the cumulative residual and past inaccuracy measures for truncated random variables, which are extensions of the corresponding cumulative entropies, and they obtained several properties for them. Psarrakos and Di Crescenzo [30] introduced and studied an inaccuracy measure concerning the relevation transform of two nonnegative continuous random variables. The inaccuracy measure was extended from univariate to multivariate. In this paper, the bivariate inaccuracy is considered. Suppose that ==== and ==== are two continuous bivariate random vectors with joint probability density functions ==== and ==== and joint supports ====respectively, such that ==== and ====, for ====. In this case, the inaccuracy measure in the bivariate situation is ====Ghosh and Kundu [8] defined the bivariate cumulative past inaccuracy in terms of the bivariate distribution function as ====where ==== and ==== are the joint distribution functions of the random vectors ==== and ====, respectively. In (2), if the joint distribution functions are replaced by the joint survival functions, then a bivariate cumulative residual inaccuracy is obtained as follows: ====where ==== and ==== are the joint survival functions of the random vectors ==== and ====, respectively. If ==== and ==== are identically distributed, then Eqs. (1), (2), and (3) are converted to bivariate entropy, bivariate cumulative past entropy, and bivariate cumulative residual entropy, respectively. In recent years, the use of copula functions in information theory for obtaining new results has attracted the attention of researchers. Indeed according to the authors researches, little research has been done on the subject of inaccuracy measure based on copula theory. Ahmadi et al. [1] developed a copula-based approach aiming to express the dynamic mutual information for past and residual bivariate lifetimes in an alternative way. Ma and Sun [20] provided a new way of understanding and estimating the mutual information using the copula function. Hao and Singh [11] proposed a maximum entropy copula method for multisite monthly streamflow simulation, in which the temporal and spatial dependence structure is imposed as constraints to derive the maximum entropy copula. Gronneberg and Hjort [9] adapted the arguments leading to the original Akaike information criterion (AIC) formula, related to empirical estimation of a certain Kullback–Leibler distance. This resulted a significantly different formula compared with the AIC, which is named the copula information criterion. Pougaza and Djafari [29] were interested to find the bivariate distribution when they knew only its marginals. They determined a multivariate distribution maximized an entropy with given marginals. Mohtashami and Amini [22] obtained various measures in view of copulas for bivariate distributions. They investigated properties of information measures and their links with copula. Hosseini and Ahmadi [12] discussed the inaccuracy in terms of copula density function as well as copula function and introduced two new inaccuracy measures, namely, copula density inaccuracy and copula cumulative past inaccuracy, which are, respectively, of the form ====and ====where ==== and ==== are two important functions in copula theory and known as the copula density function and copula function, respectively.====Accordingly, in this paper, we intend to extend the use of copula theory concepts to bivariate inaccuracy measures (1), (2), and (3) in information theory. To this end, in the second section, we first recall the basic concepts of the copula function. In Section 3, we introduce a new inaccuracy measure, which is known as the co-copula inaccuracy, and then we obtain upper and lower bounds for the proportional hazard rate model. We also obtain results and inequalities such as the triangle inequality for co-copula inaccuracy by establishing a proportional hazard rate model or lower orthant random order between random vectors. In the fourth section, we introduce another measure of inaccuracy based on the copula dual function, and we call it as the copula dual inaccuracy measure. Also, under the assumption of radial symmetry, we show that these two new inaccuracies are equal and obtain a characterization property from equality of these two inaccuracies under the conditions for radially symmetric distributions. In addition, for these two new inaccuracy measures, we derive results and inequalities, including the triangle inequality, by establishing a proportional (reversed) hazard rate model or upper and lower orthant random order between random vectors. In Section 5, we give some examples to further examine some of the results. In supplementary material section, by introducing estimators for the inaccuracy measures, we examine some of the results obtained by using simulation methods, and we also provide an example with real data. It should be noted that further results and examples are provided in Appendix A.",Discussion about inaccuracy measure in information theory using co-copula and copula dual functions,https://www.sciencedirect.com/science/article/pii/S0047259X21000038,14 January 2021,2021,Research Article,183.0
"Chung Hee Cheol,Ahn Jeongyoun","Department of Statistics, Texas A&M University, College Station, TX 77843, USA,Department of Statistics, University of Georgia, Athens, GA 30602, USA","Received 1 April 2019, Revised 4 December 2020, Accepted 4 December 2020, Available online 29 December 2020, Version of Record 9 January 2021.",https://doi.org/10.1016/j.jmva.2020.104713,Cited by (1),"We propose a new two-stage procedure for detecting multiple outliers when the dimension of the data is much larger than the available sample size. In the first stage, the data are split into two ====. The power of the proposed test is studied under a high-dimensional asymptotic framework, and its finite-sample exactness is established under mild conditions. ==== based on simulated examples and face recognition data suggest that the proposed approach is superior to the existing methods, especially in terms of false identification of non-outliers.","High-throughput data are usually a product of long and complex experiments in laboratories or fields. Due to the multi-step process when generating data, a concern for possible contamination in high-dimensional data is typically more severe than low-dimensional counterparts. However, even when it is suspected that the data contain some abnormal observations, it is difficult to identify outliers when the dimension is much higher than the sample size. The difficulty arises from the fact that there are insufficient observations to characterize the “regular” behavior of uncontaminated data. Nevertheless, it is crucial in any data analysis to identify outliers since a blind application of a statistical method without being aware of the existence of outliers will likely yield misleading scientific conclusions. For instance, in the area of classification analysis, mislabeled observations can disguise the contrast between classes, resulting in an unreliable classifier. Furthermore, the existence of clustered outliers in a given class may indicate the presence of a sub-class from which meaningful scientific findings may result. Accordingly, outlier detection is of great importance and a main objective of analysis in many situations.====In this work, we address the problem of detecting outliers among the observations in ====, when ==== is much larger than the sample size ====, i.e., in the High Dimension and Low Sample Size (HDLSS) setting. The major difficulties of HDLSS outlier detection are two-fold. First, with increasing dimensionality, data points become deterministic converging to the vertices of a simplex [14]. It makes all data points equidistant to each other [2], [8], and thus the concept of proximity becomes less meaningful; see also Hinneburg et al. [15], Aggarwal et al. [2], Kriegel et al. [22], and Aggarwal [1, Ch. 5]. Second, it is challenging to construct a valid hypothesis test in testing abnormal observations. Most of the existing methods that can be applied to HDLSS data rely on a null distribution based on large sample approximation. Hence, due to the small sample size and high-dimensionality, these methods may result in unstable results. We have found from our empirical study that, possibly because of these issues, existing methods tend to suffer from large false positives. That is, the methods are able to identify the true outliers correctly; but, they are likely to misclassify non-outliers as outliers. False detection of outliers in HDLSS is a much more serious issue than in low-dimensional cases due to the high cost of data collection, e.g., high-throughput omics data and medical images.====Although there is abundant literature on multivariate outlier detection for the case of ====, only a few existing methods can be applied to the HDLSS setting. Sajesh and Srinivasan [29] introduced Comedian distance for high-dimensional outlier detection, which is a variation of Mahalanobis distance based on robust estimates of the mean and covariance matrix. When the dimension is large, the Comedian method is computationally expensive since it requires the inversion of a ==== matrix. Ro et al. [28] suggested a method based on a minimum covariance determinant estimator. Due to the strong assumption that the population covariance matrix is diagonal, their method may not perform well in real world situations with correlated variables. Filzmoser et al. [12] proposed to use the kurtosis of the principal component (PC) scores as a measure for outlyingness. However, this method may not be preferred in HDLSS as it is known that sample PC directions are not generally consistent with increasing dimensionality [19]. Ahn et al. [4] considered a distance-based HDLSS outlier detection method motivated by the high-dimensional geometric representation [6], [14]. Our numerical study has found that their method produce many false negatives when outliers are clustered, possibly due to their one-by-one elimination process. Kriegel et al. [22] proposed an angle-based outlier detection (ABOD) method that uses angles instead of distances for quantifying abnormality of the observation. However, this method is not equipped with a formal testing procedure.====Our contributions in this paper are the following. First, we propose a novel outlier detection method for data with much larger dimensionality than the sample size. Numerical studies suggest that our method produce much less false discoveries than existing methods. Second, we extend a random rotation test [23], [27] to a more general distributional family, with which we conduct hypothesis tests on the abnormality of data vectors. Third, we provide theoretical properties of the rotation tests such as unbiasedness and test size. Fourth, we show that, as the dimension increases, the proposed method would detect multiple outliers correctly and the false discovery rate diminishes to zero with probability converging to one.====The proposed outlier detection procedure consists of two stages: screening and testing. Let ==== be an observed data matrix. In the screening step, we partition the data into two row-wise sub-matrices, say ==== and ====, respectively consisting of non-outliers and potential outliers which we call candidate outliers. Then each observation in ==== is tested based on how far it is from ==== in the testing step. Abnormality of a candidate outlier is quantified by the orthogonal distance to the affine hyperplane generated by the non-outliers in ==== [5]. This distance measure is computationally efficient and has been found to be effective in the context of HDLSS classification [5], clustering [4], and outlier detection [3].====The hypothesis test in the testing stage is conducted by a randomization test that generates data that possess the same scatters but free of outliers, which we call the subspace rotation test. The idea of testing via random rotations was suggested by Wedderburn in his unpublished manuscript in 1975. It has been considered in the context of multivariate regression with normal errors in Langsrud [23] and Perry and Owen [27], and with left-spherical errors in Solari et al. [32]. However, theoretical justification on the rotation procedure has been limited to normal population. Moreover, the properties of the hypothesis test have not been fully established. In this work, we investigate the left-spherical distribution family to formally construct the rotation test and establish its finite-sample exactness. We also explore the asymptotic power of the test when the dimension tends to infinity in the context of outlier detection.====The rest of this paper is organized as follows. In Section 2, we construct the rotation test and explore its finite-dimensional properties. Section 3 introduces the screening and testing steps for the proposed high-dimensional outlier detection method. We justify the proposed procedure by studying its high-dimensional asymptotic behaviors in Section 4. The numerical performance of the proposed method is compared with existing approaches using simulated examples in Section 5 and face data examples in Section 6. We conclude the paper with discussion in Section 7. Technical details are provided in Appendix A.",Subspace rotations for high-dimensional outlier detection,https://www.sciencedirect.com/science/article/pii/S0047259X20302943,29 December 2020,2020,Research Article,184.0
"Li Yunfan,Datta Jyotishka,Craig Bruce A.,Bhadra Anindya","Department of Statistics, Purdue University, West Lafayette, IN 47907, USA,Department of Mathematical Sciences, University of Arkansas, Fayetteville, AR 72704, USA","Received 20 December 2019, Revised 15 December 2020, Accepted 15 December 2020, Available online 29 December 2020, Version of Record 7 January 2021.",https://doi.org/10.1016/j.jmva.2020.104716,Cited by (6),". Extensive performance comparisons are provided with both ==== and Bayesian alternatives, and both estimation and prediction performances are verified on a genomic data set.","Multiple predictors–multiple responses regression, sometimes also known as the problem of multi-task learning in machine learning literature, is a common modeling framework in quantitative disciplines as diverse as finance, chemometrics and genomics. To take a concrete example from the field of genomics, this problem arises in simultaneously regressing the expression levels of multiple genes on multiple markers or regions of genetic variation, which is known as an expression quantitative trait loci (eQTL) analysis. Early studies have shown that each gene expression level is expected to be affected by only a few genomic regions [8], [28] so that the regression coefficients in this application are expected to be sparse. In addition, the expression levels of multiple genes have been shown to possess a sparse network structure that encodes conditional independence relationships [20], which, in the case of a multivariate Gaussian model, are encoded by the off-diagonal zeros in the inverse covariance matrix. Therefore, an eQTL analysis, if formulated as a multiple predictors–multiple responses regression problem, presents with non-independent error terms. In high dimensions, this necessitates regularized estimates of both the regression coefficients and the error inverse covariance matrix. Similar problems arise in econometrics, for example, in predicting the set of several correlated stock prices using a common set of covariates.====A natural question then is: what is there to be gained by treating all responses jointly rather than separately regressing each response on the set of covariates, possibly adjusting for multiplicity in the responses? In multivariate regression problems with correlated error terms, early works by Zellner [31] established that joint estimation of regression coefficients improves efficiency. Zellner [31] went on to propose the seemingly unrelated regression framework where the error correlation structure in multiple responses is leveraged to achieve a more efficient estimator of the regression coefficients compared to separate least squares estimators. Holmes et al. [18] adopted the seemingly unrelated regression framework in Bayesian regressions. However, these early methods in the seemingly unrelated regression framework considered a relatively modest dimension of the responses, and did not encourage sparse estimates of either the regression coefficients or the error inverse covariance matrix. Therefore, these methods cannot be applied directly to analyze modern genomic or financial data. Much more recently, both Bayesian and frequentist approaches that encourage sparsity have started to attract considerable attention in a seemingly unrelated regression framework [1], [6], [9], [29], [30]. Precise descriptions of some of these competing approaches and understanding their strengths and limitations require some mathematical formalism. This is reserved for Section 2.====In this article, we propose a fully Bayesian method for high-dimensional seemingly unrelated regression problems with an algorithm for efficient exploration of the posterior. We impose the horseshoe prior [11] on the regression coefficients, and the graphical horseshoe prior [21] on the precision matrix. In univariate normal regressions, the horseshoe prior has been shown to possess many attractive theoretical properties, including improved Kullback–Leibler risk bounds [11], asymptotic optimality in testing under ====–==== loss [14], minimaxity in estimation under the ==== loss [24], and improved risk properties in linear regression [4]. The graphical horseshoe prior inherits the properties of improved Kullback–Leibler risk bounds, and nearly unbiased estimates, when applied to precision matrix estimation [21].====The beneficial theoretical and computational properties of the horseshoe and graphical horseshoe are combined in our proposed method, resulting in a prior that we term the horseshoe-graphical horseshoe or HS-GHS. The proposed method is fully Bayesian, so that the posterior distribution can be used for uncertainty quantification, which in the case of horseshoe is known to give good frequentist coverage [25]. For estimation, we derive a full Gibbs sampler, inheriting the benefits of automatic tuning and no rejection that come with it. The complexity of the proposed algorithm is linear in the number of covariates and cubic in the number of responses. To our knowledge, this is the first fully Bayesian algorithm with a linear scaling in the number of covariates that allows arbitrary sparsity patterns in both the regression coefficients and the error precision matrix. This is at a contrast with existing Bayesian methods that require far more restrictive assumptions on the nature of associations. For example, Bhadra and Mallick [6] require that either a predictor is important to all the responses, or to none of them. The proposed method is also at a contrast with approaches that require special structures on the conditional independence relationships. For example, both Bhadra and Mallick [6] and Banterle et al. [1] require that the graphical model underlying the inverse covariance matrix is decomposable. Such assumptions are typically made for computational convenience, rather than any inherent problem-specific motivation, and the current work delineates a path forward by dispensing with them. In addition to these methodological innovations, the performance of the proposed method is compared with several competing approaches in a yeast eQTL data set and superior performances in both estimation and prediction are demonstrated.",Joint mean–covariance estimation via the horseshoe,https://www.sciencedirect.com/science/article/pii/S0047259X20302979,29 December 2020,2020,Research Article,185.0
"Zhang Yongli,Rolling Craig","Department of Mathematics and Statistics, North Carolina A&T State University, Greensboro, NC 27411, United States of America,Benson Hill, St. Louis, MO 63132, United States of America,School of Statistics, University of Minnesota, Minneapolis, MN 55455, United States of America","Received 5 April 2020, Revised 22 November 2020, Accepted 22 November 2020, Available online 26 December 2020, Version of Record 12 January 2021.",https://doi.org/10.1016/j.jmva.2020.104710,Cited by (1),"In economic and business data, the ","Estimates and forecasts of time-dependent correlation matrices are important for accurate risk assessment and financial asset pricing. Often large numbers of correlations are involved in the estimate and forecast, such as when dealing with a diverse portfolio. Such correlations are often changing and exhibit seasonality, making estimation and prediction more difficult. In a landmark paper, Engle [9] called this estimation problem Dynamic Conditional Correlation (DCC) and proposed a class of models to estimate and forecast time-varying correlation matrices. Engle’s DCC models have become very popular because of their easy implementation and fast computation [1], especially when the number of correlations to estimate is large and classical methods, such as multivariate generalized auto-regressive conditional heteroskedasticity (MGARCH) [3], break down.====Despite the popularity of DCC models, they do possess some limitations [7], and overall the problem of using sample data to forecast large, time-varying correlation matrices has proven to be theoretically and computationally challenging. The large parameter space can present difficulties with estimation and computation. Since a correlation matrix must be positive definite, it would be desirable for its estimator to have that property as well; however, it can be difficult to guarantee this property. Assuming Gaussian errors simplifies the problem but may be unrealistic. Often seasonal patterns are assumed to be present, but it is unclear how best to model them.====The many practical implications and challenges of estimating a large number of time-varying volatilities have resulted in a large body of literature and many diverse approaches that complement Engle’s DCC models. Bai and Shi [2] provide a review of covariance matrix estimation and categorize the various estimation methods, including (linear) observable factor models and latent factor models. Latent factor models are also discussed in Tsay [26, Chapter 9]. The approach of Conditionally Uncorrelated Components [11] also resides in the latent factor category. Creal et al. [8] use the multivariate ==== density to analyze dynamic correlations of heavy-tailed distributions, and Long et al. [20] propose a semiparametric, two-stage approach to allow for non-linearity and non-normality.====In this article, we propose a nonlinear common factor (NCF) model for estimating and forecasting conditional correlation matrices when the dynamic correlations of interest are associated with one or more observable common factors. This method utilizes generalized additive models [GAM, 15] and multivariate adaptive regression splines [MARS, 12] to model nonlinearities. Our solution results in a closed-form expression for the conditional correlation whose parameters have clear economic meaning. Our proposed method has clear computational advantages over conventional MGARCH in that the number of parameters to be estimated by the model may be much lower than the number of series. Thus, NCF can be used to estimate very large correlation matrices. The conditional correlation matrix estimate from NCF is exogenous in that the correlation matrix is not modeled directly but is a byproduct of the series estimates and forecasts; therefore, positive definiteness of the estimated correlation matrix is automatically guaranteed. NCF allows for mean reversion, spikes and seasonal volatility and thus is an easy-to-use, understandable tool for forecasting and risk management.====The rest of the article is organized as follows. Section 2 develops the Nonlinear Common Factor (NCF) model and provides an algorithm to estimate conditional correlations via the NCF. Monte Carlo studies comparing NCF to Dynamic Conditional Correlation [9] are presented in Section 3. Section 4 uses the method to estimate the correlation structure between electricity and natural gas prices in Boston using the common factors of temperature and humidity, and a brief summary in Section 5 concludes. The Appendix A contains a proof of consistency for the NCF estimator.",Estimating and forecasting dynamic correlation matrices: A nonlinear common factor approach,https://www.sciencedirect.com/science/article/pii/S0047259X20302918,26 December 2020,2020,Research Article,186.0
"Langworthy Benjamin W.,Stephens Rebecca L.,Gilmore John H.,Fine Jason P.","Department of Biostatistics, University of North Carolina - Chapel Hill, 135 Dauer Drive, 3101 McGavran-Greenberg Hall, Chapel Hill, NC 27599, United States of America,Department of Psychiatry, University of North Carolina - Chapel Hill, 101 Manning Dr 1, Chapel Hill, NC 27514, United States of America","Received 3 May 2020, Revised 10 December 2020, Accepted 10 December 2020, Available online 23 December 2020, Version of Record 11 January 2021.",https://doi.org/10.1016/j.jmva.2020.104715,Cited by (3), package implementing this method is available at github.com/blangworthy/transCCA.,"Canonical correlation analysis (CCA), first introduced by Hotelling [21], is a useful dimension reduction technique for exploring the relationship between two sets of variables. CCA finds the linear combinations of the two sets of variables that have maximal Pearson correlation. After the first direction, further directions are defined as the linear combinations that are maximally correlated subject to the constraint that they are uncorrelated with all previous directions. A small number of directions may be used to summarize the relationship between the two sets of variables.====In Section 4 we present an example where CCA is useful in understanding the relationship between the structure of white matter brain tracts and executive function in six-year-old children. Many of the variables show excess skewness or kurtosis relative to the normal distribution. This suggests transformations may be needed for CCA using Pearson’s correlation to fully capture the association between the two sets of variables. However it is not clear how to optimally transform the data, especially for heavy tailed distributions where transforming may weaken linear associations. In such settings standard CCA may be problematic, and alternative approaches are valuable.====In the finite dimensional setting when all second moments exist, CCA is valid based on an eigendecomposition involving the sample covariance matrix. In settings where the empirical covariance estimator is either inconsistent or inefficient, including when second moments do not exist or when there are outliers contaminating the observed data, the CCA estimates based on the empirical covariance matrix will also be either inconsistent or inefficient. There is a rich literature on robust estimators of the covariance matrix that are insensitive to outliers and heavy tailed distributions, and may improve the performance of standard CCA based on Pearson correlation. Examples of these are the minimum covariance determinant (MCD) [40], the S-estimator [31], and Tyler’s M-estimator [46]. There have been studies examining the performance of CCA using robust estimators of the covariance matrix or by maximizing other robust correlation measures [4], [8], [44], [47]. Many of these robust methods emphasize eigendecompositions employing robust estimates of the covariance or Pearson correlation matrix, which do not exist in the absence of finite moments. Further assumptions are needed to interpret robust CCA in these settings.====We explicitly define a version of CCA for distributions with elliptical copulas that does not require the existence of moments using properties of Kendall’s tau for elliptical distributions. For elliptical distributions there is a known monotone relationship between Pearson’s correlation and Kendall’s tau rank correlation. We utilize this relationship to define CCA using Kendall’s tau instead of Pearson correlation such that it is well defined when moments do not exist and has the same canonical directions and correlations as standard CCA for elliptical distributions when moments do exist. Perhaps most importantly this definition of CCA does not make any assumptions about the marginal distributions of the variables, so it can be easily extended to a family of distributions known as transelliptical distributions. The transelliptical family consists of all multivariate distributions which can be transformed into an elliptical distribution using monotone marginal transformations, or equivalently all multivariate distributions with a copula from an elliptical distribution [2], [11], [12], [27], [30]. Standard CCA is inadequate to describe the relationship between two sets of variables which are transelliptically distributed and have potentially non-linear associations. CCA using Kendall’s tau identifies the linear relationships in the elliptical distribution which characterizes the transelliptical distribution. This is desirable because within elliptical distributions linear relationships describe meaningful association between the variables. We show that CCA for transelliptical distributions can be estimated without transforming the variables to an elliptical distribution, by estimating the scatter matrix based on transformations of Kendall’s tau for all pairs of variables [30]. We establish that the resulting estimates for CCA directions and non-zero correlations are consistent and asymptotically normal. This result is more general than previous results which require affine equivariant estimators of the scatter matrix for data generated from elliptical distributions [5], [44]. Interestingly, the estimate based on transformations of Kendall’s tau for all pairs of variables is not affine equivariant. Simulations indicate that these results can be used to construct confidence intervals that perform similar to bootstrap confidence intervals with close to the desired coverage for the first canonical directions. Confidence intervals for higher order canonical directions and correlations do not perform as well whether using bootstrap or asymptotic results to construct the confidence intervals. This highlights the difficulty in accounting for variability in the estimates due to added constraints for finite samples.====We also develop a testing procedure to identify non-zero canonical correlations using bootstrap bias and standard error estimates. This is necessary because although the asymptotic results for non-zero canonical correlations can be used to construct confidence intervals, asymptotic results for zero canonical correlations are not as straightforward. However based on previous results [6] it can be expected that the zero canonical correlations will converge at rate ==== rather than ====. Therefore by inverting a normal bootstrap confidence interval we derive a test that is consistent and conservative for large sample sizes. This testing procedure can be used for CCA estimated using Kendall’s tau or standard methods. This testing procedure is necessary because previously derived asymptotic tests assume the data are generated from a multivariate normal distribution [37], [38], [48]. Even permutation based tests assume that zero correlation implies independence, which is not true for non-Gaussian elliptical copulas. In non-Gaussian elliptical copulas the canonical directions may not be independent even when they are not informative of any associations between the two sets of variables, and therefore permutation tests which test for independence are not useful in determining which canonical directions capture meaningful associations between the two sets of variables. Our bootstrap based testing procedure makes minimal assumptions, and can even be useful even when data are not generated from a distribution with an elliptical copula.====The rest of the paper is structured as follows. Section 2 overviews the theoretical framework for rank estimation of CCA in the elliptical and transelliptical distributions and provides theoretical results for consistency and asymptotic normality of the estimates. Section 3 reports the results of simulation studies under elliptical and transelliptical distributions. Section 4 provides an analysis of associations between white matter structure and executive function in six-year-old children. Section 5 overviews the paper and concludes with remarks.",Canonical correlation analysis for elliptical copulas,https://www.sciencedirect.com/science/article/pii/S0047259X20302967,23 December 2020,2020,Research Article,187.0
"Kao Ming-Hung,Khogeer Hazar","School of Mathematical and Statistical Sciences, Arizona State University, Tempe, AZ, 85287-1804, USA","Received 21 March 2020, Revised 2 December 2020, Accepted 2 December 2020, Available online 9 December 2020, Version of Record 16 December 2020.",https://doi.org/10.1016/j.jmva.2020.104712,Cited by (3),. The complete class results facilitate the search of optimal designs via some general-purpose optimization techniques. Extensions of some previous results for characterizing optimal designs are also provided.,"Experiments involving two or more response variables are ubiquitous. For cases with only continuous responses, optimal designs for multivariate regression models are developed in several previous works; see, e.g., [19], [24], [35], and Chapter 5 of [11]. These designs allow experimenters to collect informative data for making precise and valid statistical inferences; they also can serve as benchmarks for evaluating the quality of the designs selected by the experimenter. However, there exist many cases where some of the multivariate responses are continuous, whereas the others are, say, categorical. Examples of this sort, and some multivariate data analysis methods to jointly analyze responses of mixed variable types can be found in, e.g., [32] and [6]. For such a situation, the optimal designs developed in the previously mentioned studies can be inappropriate, and the identification of new optimal designs is needed.====This work is concerned with optimal designs for multivariate regression analysis of responses of mixed variable types. For convenience, we refer to such multivariate responses as mixed responses, and a corresponding model is termed as a mixed responses model. Two common likelihood-based approaches for building mixed responses models include the latent variable methods and factorization methods [5], [32]. The former approach assumes that the categorical responses are induced from some continuous, unobserved latent variables. With this approach, Fedorov et al. [12] obtained optimal designs for models where the binary response ==== is assumed to be a dichotomization of a continuous latent variable whose joint distribution with the continuous response ==== is a bivariate normal distribution. For the factorization method, the joint distribution of ==== and ==== is expressed as ==== for specific marginal distribution ==== and conditional distribution ==== [5], [7], [14]. Recently, Kim and Kao [23] proposed some optimal designs for a mixed responses regression model that is built based on the factorization method. The obtained optimal designs determine the optimal set of distinct values of a continuous covariate ==== (e.g., dose levels in a dose–response study), and the frequency of occurrences of each ====-value in the experiment.====Following this research direction, we develop some optimal design results for mixed responses regression models under the factorization approach. For clarity, we put our focus on cases with one continuous response and one binary response. But in contrast to the previous study, we allow our models to include both quantitative and qualitative factors; the qualitative factors divide experimental subjects into homogeneous subject groups. In addition, our results can be applied to a larger collection of models than that of [23]. These models, which include some popularly used ones, may or may not assume common parameters across submodels (to be defined in Section 2), and/or across subject groups. The main tool that we adapt to facilitate the identification of optimal designs is the complete class approach by Yang and Stufken [34]. Roughly speaking, this approach gives a small class of designs within which an optimal design can be found. It often greatly reduces the number of decision variables in the optimization problem to allow or facilitate the use of some well-developed optimization techniques such as those considered in [16] and [25].====The previously mentioned complete class approach of [34] is with respect to the Loewner ordering, and can thus be considered for most commonly used optimality criteria. In addition to obtaining a core of the information matrix (see Definition 1), a key ingredient of this approach is to identify vectors of functions that form a Chebyshev system (Definition 2). Numerous previous works provide Chebyshev systems useful for deriving optimal designs; e.g., [10], [11], [17], [18], [29], [33], [34], [36]. Here, we add to this literature by presenting some additional Chebyshev systems that can be used to derive complete classes for mixed responses models. The result can also be applied in other settings where the elements of the information matrix or its core lie within the space spanned by the Chebyshev systems (Theorem 3). We also provide useful extensions of previous results to give insights into some properties of optimal designs. Optimal designs are then identified by using a well-developed computational approach. We note that optimal designs in the current setting depend on some model parameters, possibly including error variances. To address this issue, we work on locally optimal designs [4] that are optimal for given parameter values; see, e.g., [15] for the usefulness of locally optimal designs. We also follow previous design works to consider the approximate design approach in the sense of Kiefer [22]; i.e., the relative frequency of appearances of a design point can be any real value between 0 and 1. A rounding of the obtained approximate design may then be considered to obtain an exact design for a given sample size; e.g., [28].====In Section 2, we introduce our mixed responses models. Our developed Chebyshev systems, complete classes, and related results can be found in Section 3. Some optimal designs are provided in Section 4, and a conclusion is in Section 5. Proofs for some results are deferred to the  Appendix.",Optimal designs for mixed continuous and binary responses with quantitative and qualitative factors,https://www.sciencedirect.com/science/article/pii/S0047259X20302931,9 December 2020,2020,Research Article,188.0
"Lai Tingyu,Zhang Zhongzhan,Wang Yafei,Kong Linglong","Beijing University of Technology, Beijing, 100124, China,University of Alberta, Edmonton, AB, T6G 2R3, Canada","Received 23 August 2020, Revised 21 November 2020, Accepted 22 November 2020, Available online 8 December 2020, Version of Record 14 December 2020.",https://doi.org/10.1016/j.jmva.2020.104711,Cited by (5),"We propose a new nonparametric independence test for two functional random variables. The test is based on a new dependence metric, the so-called angle covariance, which fully characterizes the independence of the random variables and generalizes the projection covariance proposed for random vectors. The angle covariance has a number of ====, including the equivalence of its zero value and the independence of the two functional variables, and it can be applied to any functional data without finite moment conditions. We construct a ====-statistic estimator of the angle covariance, and show that it has a Gaussian chaos limiting distribution under the independence null hypothesis and a normal limiting distribution under the alternative hypothesis. The test based on the estimated angle covariance is consistent against all alternatives and easy to be implemented by the given ==== method. Simulations show that the test based on the angle covariance outperforms other competing tests for functional data.","Over recent decades, functional data analysis (FDA) has been developed rapidly and become an important area of statistics. FDA offers effective tools for the analysis of high or infinite dimensional data, and meets the growing needs for data collection and analysis with the progress in technology. Many aspects of FDA, such as functional regression [11], clustering and classification of functional data [27], have been extensively investigated, and a number of excellent monographs about FDA have been published, see, for example, [3], [7], [8], [14]. However, relatively little works focus on measuring and testing the dependence of two functional random variables.====Testing the independence of random elements is a fundamental problem in statistics and has important applications. It has been studied by many authors, and several excellent methods have been developed, including the distance correlation [21], [22], the kernel based criterion [4], [5], [20], the maximal information coefficient [15], the copula based measures [16], [19], the projection correlation [28], the ball covariance [12], and others (see, for example, [23]). Many of these methods were developed for random variables in Euclidean spaces and may not be directly applied to functional data, which have infinite dimensions. Among them, an indispensable method is the distance correlation [21], [22], which can be used to measure and test the dependence between random vectors ==== and ====, provided that ====. It has been shown to work well in a number of situations, and extended to strong negative type metric space [10], and thus can be applicable for functional data. In spite of its advantages, the distance covariance requires the finite first moment. When the condition is violated, it may not behave well, as illustrated in Zhu et al. [28]. To remove the moment condition, Zhu et al. [28] proposed a projection correlation for two random vectors based on the projections of the random vectors. The projection correlation test is powerful in some cases, but it only deals with the random vectors in the Euclidean setting, and cannot be used straightforwardly to functional data. Actually, our simulations show that a naive use of the projection correlation test could suffer a loss of power for functional data. Another powerful tool suitable for functional data is the ball covariance [12]. It is designed to measure and test the dependence of random elements in Banach spaces, and can be applied to many kinds of data. Both the distance covariance and the ball covariance test statistics are constructed with the metrics of the underlying spaces. Besides the metrics endowed in the underlying spaces, one may utilize more information such as the features of the data or the geometric structures of the underlying spaces. For example, a functional variable is usually considered as an element of a Hilbert space and has some properties of functions such as periodicity or monotonicity. It could improve the performance of an independence test if we combine these kinds of information in an appropriate way.====In view of the preceding discussion, we would like to establish a general independence test for two random elements valued in two separable Hilbert spaces, which requires less strict conditions such as finite moments, and uses more information of the data in hand. More precisely, let ==== be a probability space, and ==== be a vector of random elements, where ==== are two separable Hilbert spaces. We aim to test ====To attain this goal, we first construct a quantity with projection and integration skill in separable Hilbert space, called angle covariance, to measure and test the dependence of the random elements. The angle covariance involves the inner products of the Hilbert spaces and some nondegenerate Gaussian measures on the spaces, thus it can combine the geometric properties of Hilbert spaces and the features of the data by choosing appropriate Gaussian measures. The angle covariance of ==== and ==== is always nonnegative and is equal to zero if and only if ==== and ==== are independent, and it is equal to the projection covariance in Zhu et al. [28] in finite dimensional settings with the choice of the standard multivariate normal distributions (see Remark 1 in Section 2). Then, we provide an empirical estimator of the angle covariance, and give its asymptotic properties. It is shown that the estimator is ====-consistent if ==== and ==== are independent, and ====-consistent otherwise. Correspondingly, the test of independence based on the angle covariance is consistent against all alternatives, and easy to be implemented. The proposed test does not require any restriction to the underlining distributions. Through numerical simulations, we show that it is more powerful than those based on other dependence metrics, such as distance covariance [22], projection covariance [28], and ball covariance [12] for functional data.====The rest of the paper is organized as follows: In Section 2, we explore the procedure of constructing angle covariance in Hilbert spaces. In Section 3, we give an estimator of the angle covariance and show its asymptotic properties. In Section 4, we present the test procedure based on angle covariance in practice and provide an empirical criterion for the choice of Gaussian measures. A group of finite sample simulation studies is carried out in Section 5. In Section 6, some discussions are included. All technical proofs are presented in the  Appendix.",Testing independence of functional variables by angle covariance,https://www.sciencedirect.com/science/article/pii/S0047259X2030292X,8 December 2020,2020,Research Article,189.0
"Ansari Jonathan,Rüschendorf Ludger","Abteilung für Quantitative Finanzmarktforschung, Wirtschafts- und Verhaltenswissenschaftliche Fakultät, Albert-Ludwigs-Universität Freiburg, Rempartstraße 16, 79098 Freiburg, Germany,Abteilung für Mathematische Stochastik, Albert-Ludwigs-Universität Freiburg, Ernst-Zermelo-Straße 1, 79104 Freiburg, Germany","Received 18 January 2020, Revised 19 November 2020, Accepted 19 November 2020, Available online 5 December 2020, Version of Record 11 December 2020.",https://doi.org/10.1016/j.jmva.2020.104709,Cited by (5)," of the individual risks and the common risk factor. The moderate dependence assumptions on this type of models allow flexible applications and, in consequence, are relevant for improved risk bounds in comparison to the marginal based standard bounds.","In the first part of this paper, we extend and strengthen some basic stochastic ordering results for multivariate normal distributions to the frame of elliptical distributions. As a consequence, we obtain in the second part of the paper upper bounds in classes of elliptical distributions under restrictions on (partial) correlations as well as extensions and strengthening of several recent results on risk bounds in partially specified factor models (PSFMs) with elliptical specification of the dependence structure of the individual risks with a common risk variable.====A classical result of Slepian [32, Lemma 1.1] gives one-sided (lower orthant) comparison criteria of normal distributions by the increase of the off-diagonal correlations. In the paper of Block and Sampson [11, Theorem 2.1 and Corollary 2.3] it is stated that an increase of the off-diagonal correlations even implies the supermodular comparison of these distributions. The argument in [11] is shown in Müller and Scarsini [24, Section 4] to be incomplete. In their paper, these authors give a complete proof of the strengthened comparison result for the normal case. In the present paper, we use the ideas in these papers to characterize the supermodular ordering for the general elliptical case. We also derive a related ordering criterion for the comparison of elliptical distributions w.r.t. (with respect to) the directionally convex order.====The ordering results are used in Sections 3 Risk bounds in classes of elliptical models, 4 Worst case distributions in partially specified factor models of the paper to derive unique worst case distributions for several relevant classes of risk models. These include models with elliptical dependence structure and additional bounds on correlations and partial correlations corresponding to a C-vine structure. We also show that a generalization to D-vine structures and, thus, to arbitrary regular vine structures is not possible. A second type of applications concerns PSFMs which do not need a full specification of the dependence structure and, thus, are a particular flexible tool for applications. Under various constraints on the specifications, i.e., on the dependence structure of the individual risks with the risk factor, unique worst case distributions are determined. As consequence, these results imply relevant improvements of standard (upper) risk bounds based only on marginal information on the risk vectors.",Ordering results for elliptical distributions with applications to risk bounds,https://www.sciencedirect.com/science/article/pii/S0047259X20302906,5 December 2020,2020,Research Article,190.0
"Shih Jia-Han,Emura Takeshi","Institute of Statistical Science, Academia Sinica, Taipei, Taiwan,Department of Information Management, Chang Gung University, Taoyuan, Taiwan","Received 10 February 2020, Revised 17 November 2020, Accepted 17 November 2020, Available online 26 November 2020, Version of Record 5 December 2020.",https://doi.org/10.1016/j.jmva.2020.104708,Cited by (8),"The correlation ratio has been used to measure how much the behavior of one variable can be predicted by the other variable. In this paper, we derive a new expression of the correlation ratio based on ====. We represent the copula correlation ratio in terms of Spearman’s rho of the ====-product of two ====. Our expression provides a new way to obtain the copula correlation ratio, which is especially useful when a copula is closed under the ====-product operation. Moreover, we propose a Kendall’s tau copula correlation ratio that has not been considered in the literature. We apply the new expressions to investigate the theoretical properties of the copula correlation ratios, including difference and discontinuity. For multivariate copulas, we propose to define the copula correlation ratio matrices, and show their ====.","In analyzing a multivariate distribution, studying dependence structure between random variables is essential. Many measures of association have been proposed by various authors to describe association structure between two random variables such as Pearson’s correlation coefficient, Spearman’s rho, and Kendall’s tau. However, many of them do not describe the asymmetric relationship between two random variables. For two random variables ==== and ====, the association of ==== on ==== can be different from the association of ==== on ====.====To deal with this problem, Dabrowska [7] investigated a regression-based measure of association through the correlation ratio. The fundamental idea of regression association is to measure how much the behavior of one variable can be predicted by the other variable. If ==== is predictable from ====, we say that there is regression association of ==== on ====. The analysis of a bivariate distribution focuses on the contrast between two regressions: (==== on ====) and (==== on ====). It is possible that ==== can be perfectly predicted by ====, whereas ==== is ‘almost independent’ from ==== (Siburg and Stoimenov [38]).====Sungur [40] proposed a copula-based correlation ratio, that is, the correlation ratio after eliminating the influence of the marginal distributions. Dette et al. [10] introduced a general concept of regression dependence and proposed a copula-based nonparametric measure of regression dependence. Based on the work of Dette et al. [10], an almost opposite case of regression dependence was discussed by Siburg and Stoimenov [38]. Recently, Chatterjee [6] investigated the same measure as in Dette et al. [10] without the aid of copulas. For real applications of copula-based regression association and prediction, we refer to Chang and Joe [5], Emura et al. [19], Kim and Kim [26], Kim et al. [24], and Kim et al. [25].====In this paper, we derive a new expression of the copula-based correlation ratio that was defined by Sungur [40]. By utilizing the ====-product operator introduced by Darsow et al. [8], we show that the copula correlation ratio is equal to Spearman’s rho of the ====-product of two copulas. Our expression provides a new way to obtain the copula correlation ratio, which is especially useful when a copula is closed under the ====-product operation. Moreover, our new expression also suggests a natural generalization of the copula correlation ratio by allowing Spearman’s rho to be replaced by any other measure of association. Based on our new expression, we investigate the theoretical properties of the copula correlation ratios, including difference and discontinuity. For multivariate copulas, we propose to define the copula correlation ratio matrices, and show their invariance property.====This paper is organized as follows: Section 2 reviews some basic properties of copulas and the correlation ratio. Section 3 provides a new expression for the copula correlation ratio and proposes its generalization. Section 4 studies some theoretical properties of the copula correlation ratios. Section 5 concludes the paper. Additional results for the paper including estimation, simulation, and data analysis are provided in the Supplementary Material.",On the copula correlation ratio and its generalization,https://www.sciencedirect.com/science/article/pii/S0047259X2030289X,26 November 2020,2020,Research Article,191.0
"Gijbels Irène,Kika Vojtěch,Omelka Marek","Department of Mathematics and Leuven Statistics Research Center (LStat), KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium,Faculty of Mathematics and Physics, Department of Probability and Mathematical Statistics, Charles University, Sokolovská 83, 186 75 Prague, Czech Republic","Received 5 February 2020, Revised 31 October 2020, Accepted 31 October 2020, Available online 23 November 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.jmva.2020.104704,Cited by (5),"In this paper the interest is to elaborate on the generalization of ==== association measures, namely Spearman’s rho, Kendall’s tau, Blomqvist’s beta and Gini’s gamma, for a general dimension ====. ==== and axioms for such generalizations are discussed, where special attention is given to the impact of the addition of: (i) an independent random variable to a random vector; (ii) a conical combination of all components; (iii) a set of arbitrary random components. Existing generalizations are evaluated with respect to the axiom set. For a ====-variate Gini’s gamma, a simplified formula is developed, making its analytical computation easier. Further, for ==== increases is studied. ==== of the considered generalizations of multivariate association measures is reviewed and a nonparametric estimator of the multivariate Gini’s gamma is introduced. The practical use of multivariate association measures is illustrated on a real data example.","We have a ====-variate random vector ==== and we want to study the association between its components. More specifically, we are interested in the tendency of the components to simultaneously take large or small values. Originally, association measures were only explored for pairs of random variables. Bivariate association measures were introduced by [3], [9], [12], [30], among others. Later on, attempts to measure dependence within random vectors of general length ==== arose. Some early references include [11] who expanded Spearman’s, Blomqvist’s and Kendall’s bivariate association measures. With increasing popularity of copula theory, the latter became the main tool to study dependence and thus further multivariate association measures were introduced as functionals of a copula. These generalizations are to be found in works of [2], [16], [31], among others. The latter paper, as well as [27] also discussed properties to be expected from a reasonable multivariate association measure. The behaviour of some multivariate association measures in dimension growing to infinity was studied in an Archimedean copula setting by [36].====In this paper, we first elaborate further on the set of desirable properties, axioms, for multivariate association measures and comment on whether these hold for selected multivariate generalizations of bivariate association measures. Special attention is given to a situation when an independent random variable is added to the random vector. One would expect that this would lead to a decrease of an overall association of the starting ====-dimensional random vector. We show that not all the generalizations behave as expected despite their analogous way of derivation. For one of the generalizations of Gini’s gamma, we provide a simplified formula which avoids ====-dimensional integration and makes this measure thus more computationally feasible. Further when duplicating one component of ==== (or more generally adding a conical combination of all components) one would expect the extended random vector to show an increasing association. We establish for which multivariate association measures such a property holds. Finally, we extend Wysocki’s results on asymptotic properties in terms of growing dimension (see [36]) to other measures. In addition we obtain some partial results in this matter for meta-elliptical copulas.====The organization of this paper is as follows. In Section 2, a discussion about a set of axioms to be fulfilled by multivariate association measures is provided. In Section 3 several examples of multivariate association measures are briefly reviewed together with the investigation of the validity of the axioms. Section 4 looks into further properties of the multivariate association measures, including the evolvement for increasing dimension. In Section 5, the limiting behaviour when the dimension tends to infinity is investigated for Archimedean and meta-elliptical copulas. The results for Archimedean copulas complement those of [36]. Section 7 is devoted to estimation of multivariate association measures. An illustrative example is in Section 6, whereas a real data application in Section 8 demonstrates the practical use of the association measures. Section 9 provides an overview of our findings. A study on the multivariate Blomqvists’s beta, further illustrative examples and explanations on how to obtain standard errors, as well as additional material on the real data application and the overview are provided in the Supplementary Material.",On the specification of multivariate association measures and their behaviour with increasing dimension,https://www.sciencedirect.com/science/article/pii/S0047259X20302852,23 November 2020,2020,Research Article,192.0
"Ma Xuejun,Wang Shaochen,Zhou Wang","School of Mathematical Sciences, Soochow University, Suzhou, 215006, PR China,School of Mathematics, South China University of Technology, Guangzhou, 510640, PR China,Department of Statistics and Applied Probability, National University of Singapore, 6 Science Drive 2, 117546, Singapore","Received 19 July 2019, Revised 18 October 2020, Accepted 1 November 2020, Available online 20 November 2020, Version of Record 3 December 2020.",https://doi.org/10.1016/j.jmva.2020.104705,Cited by (3),"In this paper, a new method called mean-of-quantile is introduced to estimate multivariate quantiles. The consistency and ","Univariate quantiles are well understood and have been frequently used in the literature. Multivariate quantiles (MQs) are generalizations of univariate ones. Similar to univariate quantiles, MQs also describe the main features of a multivariate population, such as multivariate location and dispersion. Since there is no natural order among multivariate vectors, much effort has been devoted to the extension of univariate quantiles to MQs. For example, Chaudhuri [5] proposed geometric MQ; Chaudhuri [3] also suggested affine equivariant MQs by an adaptive transformation; Serfling [18] discussed spatial depth function based on MQs; Kong and Mizera [9] introduced the directional quantile envelops; Ley, Sabbah and Verdebout [10] developed MQs and depth for directional data. One can refer to [19], [20] and the references therein for comparison of several definitions of MQs. However, all these references have one common drawback, i.e., it is difficult to make inference about the population MQs based on the corresponding empirical versions since their asymptotic variances contain unknown parameters which are not easy to estimate.====In this paper, we propose a simple estimator for MQs under the framework of massive data. Massive data are popular in these days especially in finance, internet and so on. Generally, there are mainly two methods to handle massive data. One is the so-called “divide-and-conquer” (Dac) method developed by Fan, Lin and Cheng [7] and Zhang, Duchi and Wainwright [23]. Another is the subsampling method considered by Ma, Mahoney and Yu [11] and Wang, Zhu and Ma [22]. From the point view of complexity, Dac is simple to use, which just splits the full data set into several parts, and then averages the estimator of each part. To the best of our knowledge, there is no literature on the asymptotic theories of MQs for the Dac and subsampling method in the case of massive data. For inference aspect, Kleiner et al. [8] proposed the bag of little bootstrap (BLB) to assess the quality of estimators. Sengupta, Volgushev and Shao [17] developed the subsampled double bootstrap (SDB) method which costs less computation than the BLB. One should note that both BLB and SDB only make use of partial data set.====Although our proposed estimator falls into the general framework of Dac, we will study its asymptotic properties in detail. This new method of estimating MQs is called mean-of-quantiles (MOQs) by us. One can make inference on MQs based on the MOQs easily and accurately. In this paper, after the step of “divide”, we will apply the well-known empirical likelihood (EL), an important nonparametric tool to make inference on parameters (see [14], [15]), to MQs. The advantage of this method over normal approximations is that we do not need to estimate the unknown asymptotic variances. More details are provided in Section 5.====The contributions of this paper can be summarized by the following three points. (1) We propose MOQ as a new estimator of MQ, which has simple form and good theoretical properties. (2) We develop the MQ test for the first time under very mild conditions. (3) When we define the MOQ estimator, the number of blocks is vital. In this paper, we construct two information criteria which are similar to AIC (see [1]) and BIC (see [16]) to determine the number of blocks.====The rest of this paper is organized as follows: In Section 2, we give the precise definition of MOQ, and establish its theoretical properties including consistency and asymptotic normality. The inference on MQ is studied in Section 3. The information criteria are discussed in Section 4. Section 5 contains some numerical studies and an empirical example. All technical proofs are postponed to the Appendix.",Testing multivariate quantile by empirical likelihood,https://www.sciencedirect.com/science/article/pii/S0047259X20302864,20 November 2020,2020,Research Article,193.0
Pumi Guilherme,"Programa de Pós-Graduação em Estatística - Instituto de Matemática e Estatística - Universidade Federal do Rio Grande do Sul - Av. Bento Gonçalves, 9500, Porto Alegre - RS, Brazil","Received 21 October 2019, Revised 5 November 2020, Accepted 6 November 2020, Available online 17 November 2020, Version of Record 4 December 2020.",https://doi.org/10.1016/j.jmva.2020.104703,Cited by (5),"In this work, we develop the ==== of the Detrended Fluctuation Analysis (DFA) and Detrended Cross-Correlation Analysis (DCCA) for trend-stationary stochastic processes without any assumption on the specific form of the underlying distribution. All results are presented and derived under the general framework of potentially overlapping boxes for the polynomial fit. We prove the ==== study and an empirical application to econometric time series.","Calculating basic statistics as variance, correlation, cross-correlation among others from non-stationary data is a challenging problem. In this context, commonly applied statistics such as sample autocorrelation, sample variance and sample cross-correlation lose their traditional meaning. Given the importance of such quantities, circumventing this problem becomes an essential matter.====The Detrended Fluctuation Analysis (DFA), introduced by [7], is often heuristically described as an indirect way to quantifying variation in trend-stationary time series (understood as the sum of a stationary process plus a polynomial trend). A generalization of the DFA for the context where the interest lies in the joint behavior of a pair of time series is the Detrended Cross-Correlation Analysis (DCCA), introduced by [13] based on the detrended covariance of [9] and the DFA. In this sense, the DCCA is an indirect quantifier of cross-correlation.====Applications of the DFA and DCCA are abundant. A list of near 100 applications of DFA is presented in [4], [6] and references therein. Applications include areas such as physics, medicine, economics, bioinformatics, meteorology, among others. In applications, the DFA is employed as a tool to detect and quantify long-range dependence in trend-stationary time series. As for the DCCA, it is usually applied as a measure of long-range cross-correlation between time series. For instance, applications of the DFA and DCCA in economics include the study of the correlation and cross-correlation structure between Brazilian stock and commodity markets [10] and between Chinese A-share and B-share markets [11]. In the modeling of traffic flow, the DFA and DCCA are applied to the study of long-range correlation and cross-correlation between different traffic fluctuations signals [12]. Recently, [14] applies the DFA and DCCA to analyze the relationship between relative air humidity and temperature. Other applications of the DCCA can be found in [8] and reference therein.====In the literature, both DFA and DCCA, are usually defined in a constructively fashion based on a sample from a given stochastic process, that is, as estimators. Interestingly, the literature is remarkably vague about their theoretical counterparts. Instead, the focus usually lies on the relationship between the DFA/DCCA and the underlying time series, especially in the context of long-range dependence non-stationary time series, which are the core of applications of these methodologies. In other words, what does the DCCA and DFA measure is still unknown. In this paper, we make an effort to solve this problem by investigating and giving meaning to the DFA and DCCA theoretical counterparts. Incidentally, the precise definition of the DFA and DCCA’s theoretical counterparts will open the possibility of discussion about the properties of the DFA and DCCA as estimators, such as consistency and unbiasedness, absent in the current literature.====Large sample results for the DFA and DCCA are known under restrictions on the underlying process. For the DFA and DCCA, [1] presents large sample results in the context of fractional Gaussian noise and fractional Brownian Motion. For the DCCA, asymptotic theory is available only for long-range dependent trend-stationary time series decomposable as a sum of a polynomial trend plus a fractional Gaussian noise [2], [3]. To the best of our knowledge, large sample results under the general scope of stationary processes are not available. In time series, long-range dependence is often regarded as a complicated and delicate subject, especially when compared to classical ARMA processes. So a fair question is: why the asymptotic theory for the DCCA and DFA is established under non-stationary fractional Gaussian noise assumptions? We can think of three good reasons for that. First, the DFA and DCCA were designed with long-range dependent data in mind. So it is only natural that the theory has been developed in this direction. Second, mathematically speaking, the context of fractional Gaussian noise (or fractional Brownian motion) is very convenient not only because it presents a well-developed theory, but also because when working with DFA/DCCA, it entails several simplifications that hold specifically for these processes, but not for general stationary time series. Third, adding a polynomial trend to the base stationary process allows working in the more general context of trend-stationary time series. Such a trend, however, does not affect either the DFA or the DCCA, in such a way that results valid for stationary processes will also be valid for trend-stationary time series, without any modification.====In this work, we are interested in developing the theory of DCCA for jointly trend-stationary processes. As we shall show, it is sufficient to work in the context of jointly stationary processes because deterministic polynomial trends play no role in the asymptotic theory. Some of the established literature consider non-overlapping boxes to calculate the DFA and DCCA. This approach allows for some simplifications, but there is no theoretical reason for doing so and, in practice, applying overlapping boxes can be advantageous, especially in small sample sizes. Hence, we shall consider the more general framework of potentially overlapping boxes. In this work, we shall derive several results regarding DFA and DCCA under stationarity conditions and the existence of the appropriate moments. Among these results, we highlight the stationarity of the DFA and DCCA, derivation of closed forms for the moments up to second-order (including the covariance structure for DFA and DCCA) as well as a miscellany of law of large number related results. We also present a Monte Carlo simulation study and an empirical application.====The paper is organized as follows. In the next section, we define the DFA and DCCA and introduce some notation. Section 3 is concerned with stationarity results for the DCCA and DFA and the study of the DFA and DCCA’s theoretical counterparts. The covariance structure of the DCCA and DFA (as stochastic processes) and a miscellany of law of large number results are derived. In Section 4 we discuss some special cases of general interest. The proofs of all results are presented in the Appendix. This paper accompanies a supplementary material, which contains further examples, a Monte Carlo simulation study and also an empirical application of the DCCA to the study of the joint behavior of 4 stock indexes (S&P500, Nasdaq, Dow Jones and Ibovespa) and the Bitcoin cryptocurrency.",On the behavior of the DFA and DCCA in trend-stationary processes,https://www.sciencedirect.com/science/article/pii/S0047259X20302840,17 November 2020,2020,Research Article,194.0
"Monnez Jean-Marie,Skiredj Abderrahman","Université de Lorraine, CNRS, Inria, , IECL, , F-54000 Nancy, France,CHRU Nancy, INSERM, CIC-P, , F-54000 Nancy, France,Université de Lorraine, CNRS, IECL, F-54000 Nancy, France,Ecole Nationale Supérieure des Mines de Nancy, , F-54000 Nancy, France","Received 12 April 2019, Revised 27 October 2020, Accepted 28 October 2020, Available online 11 November 2020, Version of Record 4 December 2020.",https://doi.org/10.1016/j.jmva.2020.104694,Cited by (2)," of a random matrix while relaxing the i.i.d.==== ====assumption on the observed random matrices ==== and assuming either ==== converges to ==== or ==== converges to ==== where ==== is the sigma-field generated by the events before time n. As an application of this generalization, the online PCA of a random vector ==== can be performed when there is a data stream of i.i.d.==== ====observations of ====, even when both the metric ==== used and the expectation of ==== are unknown and estimated online. Moreover, in order to update the stochastic approximation process at each step, we are no longer bound to using only a mini-batch of observations of ","Streaming data are data that arrive continuously such as process control data, web data, telecommunication data, medical data, financial data, etc. In this setting, recursive stochastic algorithms can be used to estimate online, among others, parameters of a regression function (for example, see [12] and references therein) or centers of clusters in unsupervised classification [8] or principal components in a principal component analysis (PCA) (for example, see [13, p. 343]). More precisely, each arriving observation vector is used to update the estimate sequence until it converges to the quantity of interest. When using such processes, it is not necessary to store the data and, due to the relative simplicity of the computation involved, much more data can be taken into account during the same duration of time than with non sequential methods. Moreover, this type of method uses less memory space than a batch method [1]. In this article, we propose a general framework of stochastic approximation processes and subsequently apply the latter to the case of streaming PCA. This general framework is sufficiently flexible to cover the case of streaming normed PCA and other related methods while allowing the stochastic algorithm to absorb a greater amount of data at each step or even all of the previously observed data up to the current step without any additional memory storage. This can be extremely efficient for dealing with large data streams.====Let us define some notations. Let ==== denote the transpose of a matrix ====. Let ==== be a positive definite symmetric ==== matrix called metric, ==== and ==== be respectively the inner product and the norm in ==== induced by ====: ====, ==== denoting the transpose of the column vector ====. For vectors in ====, ====-orthogonal and ====-normed respectively mean orthogonal and normed with respect to the metric ====. Recall that a ==== matrix ==== is ====-symmetric if ====; then ==== has ==== real eigenvalues and there exists a ====-orthonormal basis of ==== consisting of eigenvectors of ====. The norm of a matrix ==== is the spectral norm denoted ====. The abbreviation a.s.==== ====stands for almost surely.====Numerous articles have been devoted to the problem of estimating eigenvectors and corresponding eigenvalues in decreasing order of the expectation ==== of a random symmetric ==== matrix, using an i.i.d.==== ====sample of the latter. These include, among others, the algorithms of Benzécri [2], Krasulina [16], Oja [19], Karhunen and Oja [15], Oja and Karhunen [20], Brandière [3], [4], [5], Brandière and Duflo [6] and Duflo [13] in the case of PCA. We consider here the commonly used normed process of Oja [15], [20] ====, whose rate of convergence is studied in [1], recursively defined by: ====the random matrices ==== being mutually independent and a.s.==== ====bounded, ====, ====, ====, ====. A commonly used choice of ==== is ====, ==== (for example, see [12]). This process converges a.s.==== ====to a normed eigenvector of ==== corresponding to its largest eigenvalue.====Consider the application presented in Section 3 to PCA of a random vector ==== with unknown expectation ==== and covariance matrix ====. Suppose there is a data stream ==== of i.i.d.==== ====observations of ==== and that the metric ==== used to define the distance between two observations of ==== is unknown, for example the diagonal matrix of the inverses of variances of the components of ==== in normed PCA. To estimate the principal components, we must estimate eigenvectors of the ====-symmetric matrix ==== (or of the symmetric matrix ====, see Section 3). We can estimate online ==== and ==== respectively at step ==== by ====, the empirical mean of ====, and ==== depending on ====, and define ====. It is clear that the random matrices ==== do not satisfy the assumptions of Oja. In fact, these assumptions would be verified if the expectations and variances of the components of ==== were known a priori in the case of normed PCA or other characteristics in other types of PCA. This is the case for example if we have at our disposal a massive data vectors set with computed characteristics and we randomly draw a data vector from this data set at each step [13, p. 343]. Here we suppose that data arrive continuously and are drawn from an unknown distribution. The convergence of the process in such a case is not proven.====The general convergence results are presented in Section 2, the application to streaming PCA and related methods in Section 3, the conclusion in Section 4, and the proofs of all theorems in Section 5.====Let ==== be a metric in ====, ==== a ====-symmetric matrix and, for ====, ==== the ====-field generated by the events before time ====; ==== are ====-measurable. We prove the almost sure convergence of the process of Oja assuming that the conditional expectation of ==== with respect to ====-field ==== converges almost surely to ==== as ==== goes to infinity (Section 2.1, Theorem 1, first part). This allows proving the convergence of the process in the case of streaming PCA when a mini-batch of observations of ==== is taken into account at each step (Section 3.1). A method of Duflo [13, p. 343] is used in the proof (Section 5), but with more general assumptions. The proof of the convergence of processes ====, ====, of the same type, obtained by a Gram–Schmidt orthonormalization with respect to ====, to unit eigenvectors of ==== corresponding to the ==== largest eigenvalues in decreasing order (Section 2.2, Corollary 2, first part), not given in [13], is established in Section 5.====Moreover, we prove the almost sure convergence of the process of Oja, with an entirely different method, in the non-classical case where ==== converges almost surely to ====
 (Section 2.1, Theorem 1, second part and Section 2.2, Corollary 2, second part; proofs in Section 5). This applies to PCA of a random vector ==== while allowing the process to be updated at each step by using all previous observations ==== up to the current step without the need to store the latter (Section 3.2). Hence, we define a type of processes different from the classical processes that used a mini-batch of observations at each step. The conducted experiments (see the Appendix), show that these processes are generally faster than the classical processes.====Finally, the scope of these processes is further widened to other factorial methods such as multiple factor analysis [21] or generalized canonical correlation analysis [10] (Section 3.3).",Widening the scope of an eigenvector stochastic approximation process and application to streaming PCA and related methods,https://www.sciencedirect.com/science/article/pii/S0047259X2030275X,11 November 2020,2020,Research Article,195.0
Langer Sophie,"Fachbereich Mathematik, Technische Universität Darmstadt, Schlossgartenstr. 7, 64289 Darmstadt, Germany","Received 12 October 2020, Revised 29 October 2020, Accepted 29 October 2020, Available online 10 November 2020, Version of Record 5 December 2020.",https://doi.org/10.1016/j.jmva.2020.104696,Cited by (42),"-dimensional, smooth function on a compact set with a rate of order ====, where ==== is the smoothness of the function. Unfortunately, these rates only hold for a special class of sparsely connected DNNs. We ask ourselves if we can show the same approximation rate for a simpler and more general class, i.e., DNNs which are only defined by its width and depth. In this article we show that DNNs with fixed depth and a width of order ==== achieve an approximation rate of ====. As a conclusion we quantitatively characterize the approximation power of DNNs in terms of the overall weights ==== in the network and show an approximation rate of ====. This more general result finally helps us to understand which network topology guarantees a special target accuracy.","The outstanding performance of deep neural networks (DNNs) in the application on various tasks like pattern recognition [11], [17], speech recognition [5] and game intelligence [19] have led to an increasing interest in the literature in showing good theoretical properties of these networks. Till now there is still a lack of mathematical understanding of why neural networks with many hidden layers, also known as ====, are so successful in practice. Recently, there are different research topics to also prove the power of DNNs from a theoretical point of view. One key question we search an answer for is the approximation capacity of DNNs, i.e., we are interested in how multivariate functions can be approximated by DNNs. Several results already contributed to this: The analysis of neural networks with one hidden layer resulted in the so-called universal approximation theorem, stating that any continuous function can be approximated arbitrarily well by single-hidden-layer networks provided the number of neurons is large enough (see, e.g., [2], [6]). In case that the regarded function is differentiable, it could also been shown that the networks are able to approximate the first order derivative of the function [14]. Nevertheless, the number of required neurons per layer have to be very large in some cases. An overview of approximation results by shallow neural networks is given in [16]. [13] could show the same approximation result for deep neural networks as for shallow ones for a class of compositional functions, with the main advantage that in case of deep networks a lower number of training parameters is needed for the same degree of accuracy. [3] showed, that three-layer networks with a small number of parameters are as efficient as really large two-layer networks. [20] stated examples of functions that cannot be efficiently represented by shallow neural networks but by deep ones. General approximation results concerning multilayer neural networks were presented in [4] for continuous functions and in [15] for functions and their derivatives. For smooth activation functions satisfying ==== and ==== and some further properties [1] could show, that networks with two hidden layers are able to approximate any smooth function with an approximation error of size ====, where ==== is the number of nonzero weights in the network (see Theorem 2 in [1]). The functions under study are ====-smooth, i.e., they fulfill the following definition:====A similar result for networks with ReLU activation function was presented by [18] (see Theorem 5 in [18]). Unfortunately, both results only hold for a special class of sparsely connected DNNs, where the number of nonzero weights ==== is much smaller than the number of overall weights ====. A simpler class of DNNs, so-called fully connected DNNs, was analyzed by [9] and [21]. Those networks are only defined by its width and depth and do not depend on a further sparsity constraint. [9] divided their work in two different cases: (1) DNNs with varying width and logarithmic depth 2) DNNs with varying depth and fixed width. For the first case they derived an approximation rate of ==== and in their second case they even improved this rate to ====. The second case was also shown in [21]. [12] went one step further and presented a result where both width and depth are varied simultaneously. Beside [1], all the above mentioned results focus on the ReLU activation function. While [1] already shows an approximation error of order ==== for sigmoidal networks, this result only holds for a special class of DNNs. In this article we define our DNNs only by its depth and width and show that for a fixed-depth DNN with bounded weights and width of order ==== we can achieve an approximation rate of ====. This in turn generalizes the result of [1], since this also proves an rate of ==== in terms of the overall number of weights ====. The topology of our networks is clearly defined, and can therefore been seen as a guideline of how the network architecture has to be chosen to receive good approximation results for different function classes. In the proofs we generalize the techniques from the proof of the approximation results in [9] to smooth activation function. This enables us to derive the same approximation results as in [1] but with respect to the supremum norm on a cube.====Our class of DNNs is defined as follows: As an activation function ==== we choose the sigmoid activation function ====The network architecture ==== depends on a positive integer ==== called the ==== or ==== and a ==== ==== that describes the number of neurons in the first, second, ====, ====th hidden layer. A feedforward DNN with network architecture ==== and sigmoid activation function ==== is a real-valued function defined on ==== of the form ====for some ==== and for ====’s recursively defined by ====for some ====, ==== and ====for some ====. The space of DNNs with depth ====, width ==== and all coefficients bounded by ==== is defined by ====Here it is easy to see that DNNs of the class ==== are not restricted by a further sparsity constraint and are only defined by the depth ====, width ==== and a bound ==== for the weights in the network.",Approximating smooth functions by deep neural networks with sigmoid activation function,https://www.sciencedirect.com/science/article/pii/S0047259X20302773,10 November 2020,2020,Research Article,196.0
Langer Sophie,"Fachbereich Mathematik, Technische Universität Darmstadt, Schlossgartenstr. 7, 64289 Darmstadt, Germany","Received 12 October 2020, Revised 29 October 2020, Accepted 29 October 2020, Available online 10 November 2020, Version of Record 4 December 2020.",https://doi.org/10.1016/j.jmva.2020.104695,Cited by (19),"-factors). In our result the number of hidden layers is fixed, the number of neurons per layer tends to infinity for sample size tending to infinity and a bound for the weights in the network is given.","Deep neural networks (DNNs) have been shown great success in various tasks like pattern recognition and nonparametric regression (see, e.g., the monographs [1], [7], [10], [13], [14], [23]). Unfortunately, little is yet known about why this method is so successful in practical applications. In particular, there is still a gap between the practical use and the theoretical understanding, which have to be filled to provide a method which is efficient and reliable. This article is inspired to contribute to the current statistical theory of DNNs. The most convenient way to do this is to analyze DNNs in the context of nonparametric regression.",Analysis of the rate of convergence of fully connected deep neural network regression estimates with smooth activation function,https://www.sciencedirect.com/science/article/pii/S0047259X20302761,10 November 2020,2020,Research Article,197.0
"Zhao Bangxin,Liu Xin,He Wenqing,Yi Grace Y.","Department of Statistical and Actuarial Sciences, University of Western Ontario, London, Ontario, Canada, N6A 5B7,School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai, 200433, China,Department of Statistical and Actuarial Sciences, Department of Computer Science, University of Western Ontario, London, Ontario, Canada, N6A 5B7","Received 28 April 2020, Revised 14 October 2020, Accepted 14 October 2020, Available online 4 November 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.jmva.2020.104693,Cited by (1),"Variable screening is a commonly used procedure in high dimensional data analysis to reduce dimensionality and ensure the applicability of available statistical methods. Such a procedure is complicated and computationally burdensome because ==== commonly exist among ====, while important ==== may not have large marginal correlations with the response variable. To circumvent these issues, in this paper, we develop a new screening technique, the “dynamic tilted current correlation screening” (DTCCS), for high dimensional variable screening. DTCCS is capable of selecting the most relevant predictors within a finite number of steps, and takes the popularly used sure independence screening (SIS) method and the high-dimensional ordinary least squares projection (HOLP) approach as its special cases. The DTCCS technique has sure screening and consistency properties which are justified theoretically and demonstrated numerically. A real example of gene expression data is analyzed using the proposed DTCCS procedure.","As the computational power increases astoundingly and the cost of data collection decreases significantly, high dimensional data become more available than ever. Data with tens of thousands of variables are frequently seen in modern scientific researches, such as oncology data, financial data, image data, satellite data and genomics data (e.g., [1], [16], [17], [18]). In those datasets, the dimension, say ====, of predictors is much larger than the sample size, say ====, but only a small portion of the predictors are believed to be significantly relevant to the response of interest. Consequently, it is imperative to perform screening for relevant variables before building a certain formal statistical model to extract truly useful underlying information from the data.====For this purpose, variable screening has received an increasing attention in the literature and many techniques have been investigated in recent years. [9] proposed the sure independent screening (SIS) method to select important variables in ultra-high dimensional linear models. The SIS method ranks the importance of predictors according to their marginal correlation with the response variable and includes those having strong marginal correlations with the response variable into the model. [30] showed that the forward regression variable screening (FRVS) method can also identify all relevant predictors consistently. [10] extended the SIS approach to generalized linear models (GLM) by ranking the maximum marginal likelihood estimates (MMLE). [6] extended the correlation estimates to marginal nonparametric estimates which can be used in sparse ultra-high dimensional additive models. [35] introduced a screening approach under a unified framework covering both parametric and semi-parametric models. Merging the ideas of the SIS method and the robust estimator of correlation, [20] and [21] proposed robust rank correlation screening (RRCS) and robust rank SIS (RSIS), respectively, to screen variables in ultra-high dimensional settings. To protect us against model misspecification, [22] developed a robust SIS procedure based on the distance correlation (DC-SIS) under general settings including linear models. [31] proposed factor profiled sure independence screening to achieve selection consistency of ultra high-dimensional variable screening. [3] proposed a tilting procedure for variable screening which can efficiently reduce spurious correlations among predictors. [32] used the Moore–Penrose inverse to perform new correlation-based screening, a method called “the high-dimensional ordinary least squares projection” (HOLP).====To reduce high spurious correlation among predictors, we propose a correlation estimator between the predictors and the current residuals to form a path of predictors entering the model, and this path is then employed for variable screening. This new screening technique is termed as “dynamic tilted current correlation screening” (DTCCS). Our proposed method is appealing in several aspects. It simultaneously retains the important predictors which may have small marginal correlations with the response and excludes unimportant predictors which may have relatively large correlations with the response. Like the SIS method, the DTCCS approach makes use of the correlation estimates, and thus, preserves the sure screening property. Unlike the SIS method, the DTCCS algorithm employs the tilted current correlation to measure the importance of predictors by taking the spurious correlation among predictors into consideration.====The rest of the article is organized as follows. The relevant notations and the framework are introduced in Section 2. The proposed high-dimensional correlation estimator and the screening procedure based on this estimator are described in Section 3. Numerical investigations are reported in Section 4. Possible extensions are discussed in Section 5. All technical proofs are deferred to Appendix.",Dynamic tilted current correlation for high dimensional variable screening,https://www.sciencedirect.com/science/article/pii/S0047259X20302748,4 November 2020,2020,Research Article,198.0
"Lee Seonjoo,Shen Haipeng","Department of Psychiatry and Biostatistics, Columbia University, New York, NY, USA,Mental Health Data Science, New York State Psychiatric Institute and Research Foundation for Mental Hygiene, Inc., New York, NY, USA,Innovation and Information Management, Faculty of Business and Economics, University of Hong Kong, Hong Kong, China,Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","Received 5 August 2019, Revised 9 October 2020, Accepted 10 October 2020, Available online 22 October 2020, Version of Record 2 November 2020.",https://doi.org/10.1016/j.jmva.2020.104692,Cited by (1), are further validated using simulation studies.,"Independent component analysis (ICA) offers an effective data-driven approach for blind source extraction encountered in many signal and image processing problems. The ICA problem can be formally described by viewing the observed signal matrix ==== as a linear combination (mixture) of independent latent random variables (sources) ==== so that ====where each column of the mixture ====, ====, is observed at ==== channels for ==== time points, the matrix ==== is non-random and known as the unmixing matrix, and each column ==== of the source matrix ==== contains ==== unknown independent sources. The objective of ICA is to estimate the unmixing matrix ==== to recover the hidden sources through ====.====There exist many ICA methods including Infomax [5], [27], maximum likelihood estimation [12], JADE [8], fastICA [20], probabilistic ICA [1], estimating score functions [40], kernel [3], [9], smoothing splines [18], logsplines [26], and log-concave projection [38]. In addition, ICA based on two scatter matrices have been developed [23], [33], [35]. For additional extensive literature review, see [13] and [20].====The theoretical properties of the methods mentioned above have been studied. Examples include methods for the fourth moment based ICA algorithms, such as fast ICA and JADE, [32], [34], [39], [42] and semiparametric approaches including [10], [11], [16], [22], [24], [37], [38]. They study ====-consistency (in some cases, asymptotic normality) of the unmixing matrix ==== under some smooth density assumptions on the underlying sources ====. Recently [29] proposed an ICA method based on distance covariance and showed its consistency.====The above papers intrinsically assume that each source is independent and identically distributed. However, it may not be so desirable to apply these instantaneous ICA methods to many practical applications where each independent source is known to possess specific correlation structures. For example, in brain imaging studies, the independent signals usually are auto-correlated. [28] proposed a color ICA (cICA) procedure that makes uses of the spectral density functions of the source signals instead of their marginal density functions. The cICA models the sources using parametric time series models such as autoregressive moving average (ARMA) linear processes and estimates the model parameters as well as the unmixing matrix ==== through maximizing the Whittle likelihood [43], obtained in the spectral domain. The cICA numerically outperformed other ICA methods for the auto-correlated sources.====The purpose of this paper is to study the theoretical properties of the cICA to strengthen the cICA application to areas such as statistical inferential issues arising in medical brain imaging studies and signal processing problems. The main obstacle here is the identifiability of the mixing matrix, which has been examined in the i.i.d. [10], [11] and autocorrelated [6] settings using the equivalent class concept. In particular, a second-order blind identification (SOBI) algorithm based on the sample autocovariance matrices was introduced along with some sampling properties [6], [30]. In the context of auto-correlated sources, there are several differences in cICA and SOBI. First, cICA is formulated in the frequency domain while SOBI is in the time domain in which the estimation of the sample autocovariance is not efficient [7]. Second, cICA estimates both the mixing matrix and the color source spectral parameters. In contrast, SOBI used asymptotic properties of the sample auto-covariance matrix to approximate the interference to signal ratio (ISR) [6]. Third, cICA exploits the sampling properties of FFT leading to the Whittle likelihood based estimation of the latent parameters and the associated results will be exact under the Gaussian assumption [14]; while this assumption is a leading case in cICA (that is, for motivation only) but it plays a critical role in developing the SOBI asymptotic properties. Lastly, the Whittle likelihood approach allows one to formulate various statistical inferential procedures using the standard asymptotic theory.====In this paper, we continue to exploit the ideas of cICA and extend the earlier work of [28] with several aims. The first aim is to establish the sampling properties by proving the consistency and asymptotic normality of the cICA estimates, for both the linear process parameters and the entries of the unmixing matrix ====. The second aim is to study the theoretical properties of cICA when combined with prewhitening (or sphering), which is routinely performed prior to ICA to reduce computational intensity. This extension over [28] enables one to deal with arbitrary non-orthogonal mixing matrices, while the earlier work only studies scenarios with orthogonal mixing matrices. Finally, we perform extensive simulation studies to provide empirical finite-sample validation for our theoretical results. The desirable numerical properties presented here further support the practical applicability of cICA.====The remainder of the paper is organized as follows. Section 2 provides a brief background on multivariate spectral density estimation, the Whittle likelihood approach for parameter estimation, and the cICA method. Section 3 presents the main theorems that state the ====-consistency and asymptotic normality of the cICA estimates. Section 4 reports the simulation studies. All the proofs are provided in Section 5. Some theoretical derivations and computational details can be found in the Supplementary document.",Sampling properties of color Independent Component Analysis,https://www.sciencedirect.com/science/article/pii/S0047259X20302736,22 October 2020,2020,Research Article,199.0
"Lu Wenqi,Qin Guoyou,Zhu Zhongyi,Tu Dongsheng","Department of Statistics, School of Management, Fudan University, Shanghai, China,Department of Biostatistics, School of Public Health and Key Laboratory of Public Health Safety, Fudan University, Shanghai, China,Canadian Cancer Trials Group, Queen’s University, Kingston, Canada","Received 28 October 2019, Revised 22 September 2020, Accepted 23 September 2020, Available online 7 October 2020, Version of Record 15 October 2020.",https://doi.org/10.1016/j.jmva.2020.104691,Cited by (6),Subgroup identification serves as an important step towards ==== of the proposed method are established under some ====. The numerical performance is illustrated in simulations and the proposed method is applied to the quality of life data from a breast cancer trail.,"The development of precision medicine and other individualized modelling applications motivates researchers to identify subgroups from heterogeneous population. A class of widely used approaches of subgroup analysis view data as coming from a mixture of groups and each group has its own set of parameters, such as Gaussian mixture model approaches [1], [7] and structured logistic-normal mixture model [21]. However, most mixture model approaches need to specify underlying distributions and number of groups in advance. To avoid this, several authors proposed to use pairwise fusion penalization methods [2], [9], [16] which allow clustering and estimating number of clusters simultaneously. Ma and Huang [14] introduced the pairwise fusion penalization idea into subgroup analysis based on mean regression model. They represented subgroups by group specific intercepts and penalized the pairwise differences of intercepts. As an alternative to mean regression, Zhang et al. [27] utilized penalized median regression, which not only can detect and identify subgroups automatically as a result of pairwise penalization, but also is more robust against outliers and heteroscedasticity in random errors than mean regression. Zhang et al. [27] also developed an efficient algorithm which leads to faster convergence than the alternating direction method of multipliers (ADMM) algorithm used in Ma and Huang [14].====However, Zhang et al. [27] only considered the case of independent and complete data. In medical studies, more complex data is commonly seen, for example, the longitudinal data with missingness collected in a clinical trial conducted by Canadian Cancer Trials Group [10]. In the trial, patients were randomized to received one of two chemotherapy regimens, and their the quality of life (QoL) was measured 14 times during and after the chemotherapy treatments by a self-answered Breast Cancer Questionnaire (BCQ). Some patients dropped out before the end of the trial, therefore their answers were missing after some time points. To the best of our knowledge, subgroup identification approaches for this kind of incomplete longitudinal data have not been reported possibly due to the challenges in method development and computation algorithm.====In order to deal with missing data, a simple idea is to exclude the observations whose responses are missing, and analyse using only the remaining data. But when data are not missing completely at random (see the definition in Little and Rubin [12]), this method may lead to invalid inference. One of the most popular approaches for handling missing data is inverse probability weighting (IPW) methods [11], [17], [18], [19], [26]. However, the IPW based methods provide consistent estimator only if the underlying model for the probability of being observed, which is also called propensity score [20], is correctly specified. This assumption is restrictive in practice especially when the data generating process is unknown. Therefore, Han [5] proposed an estimation method allowing multiple models for the propensity score and data distribution, which can provide consistent estimates if one of the multiple models is correctly specified. Then Han et al. [6] applied this method to quantile regression.====In this paper, we propose a weighted penalized median regression approach to achieve subgroup identification for longitudinal data with dropouts which has several advantages. First, we can identify the true subgroups as long as one of the multiple models for the propensity score is correctly specified. Therefore, the proposed method is robust against misspecification of propensity score models. Second, the proposed method is robust against heavy-tailed errors or outliers since the median regression is adopted, which is also indicated in Zhang et al. [27]. Finally, by applying the pairwise fusion penalization approach, we can estimate the number of groups and identify members of each group simultaneously. Under some regularity conditions, the oracle property of the proposed method is established. And the selection consistency of a modified Bayesian information criterion (BIC) is established.====The rest of the paper is organized as follows: In Section 2, we propose the method and algorithm. In Section 3 we discuss the asymptotic properties. We then investigate the finite sample properties via simulation studies in Section 4. In Section 5, we apply the proposed method to the analysis of the QoL data. Some relative discussions are made in Section 6. Finally, the proofs of the theoretical results are presented in Appendix A.",Multiply robust subgroup identification for longitudinal data with dropouts via median regression,https://www.sciencedirect.com/science/article/pii/S0047259X20302724,7 October 2020,2020,Research Article,200.0
Kashlak Adam B.,"Mathematical & Statistical Sciences, University of Alberta, Edmonton, Canada, T6G 2G1","Received 26 October 2019, Revised 22 September 2020, Accepted 22 September 2020, Available online 3 October 2020, Version of Record 16 October 2020.",https://doi.org/10.1016/j.jmva.2020.104690,Cited by (1),"Estimation of a high dimensional precision matrix is a critical problem to many areas of ==== including Gaussian ==== is also considered. This methodology is distribution free, but is particularly applicable to the problem of Gaussian network recovery. We also consider applications to constructing gene networks in genomics data.","Attempting to estimate the graphical structure of a high dimensional network is problematic when edges are rare but the number of nodes is large. In standard statistical classification problems, we fix a palatable false positive rate and aim to recover as many true positives as possible. Thus, we treat support recovery as a binary classification problem. For ==== nodes, the number of potential edges to consider is on the order of ====. With a sample size ====, there are too many parameters to accurately estimate. However, when trying to classify edges as significant or not under the assumption of sparsity, the assumption that most edges are not significant, if we have even a small false positive rate, then this will result in many erroneous connections potentially obscuring follow up research. For example, a genomics study considering the conditional correlation structure of, say, 2000 genes will have to consider nearly two million potential edges. A standard 1% false positive rate will result in tens of thousands of erroneous connections.====To address this issue, we consider the extreme estimation setting of recovering the support of a precision matrix ==== in the high dimensional setting, ====, assuming ==== belongs to a class of sparse positive definite matrices, and for false positive rates ====. For a given index ====, the false positive rate is the probability that we falsely reject the hypothesis that ==== in favour of ====. As is common with large scale hypothesis testing [9], familywise error rate control is too conservative in this setting. Instead, we follow up the investigation into the false positive rate with a discussion on how to extend this method to control the false discovery rate. Making use of debiased estimators [4], [13], [14], [26], the non-asymptotic results of [16], and a clever subsampling methodology, we can achieve finite sample guarantees in this extreme setting. Algorithm 1 is implemented in the ====
 ==== package [15].====There has been much past work on covariance and precision matrix estimation much of which is summarized in the survey [10]. Most estimators for high dimensional precision matrices are based on ==== penalization including the graphical lasso [11], the CLIME and ACLIME estimators [6], [7], and the debiased estimator of [13] used for constructing confidence sets and running hypothesis tests. These articles all rely on high dimensional asymptotics, which is that estimation is successful in the limit as ==== and ==== grow to infinity together generally such that ====, or some variant thereof, is ====. Our work differs as it considers guaranteed results for controlling the support recovery of ==== as the false positive rate ==== for fixed finite ====, which is asymptotic in ====, a controllable tuning parameter, instead of in ==== and ====, which are generally fixed in the real world experimental setting. More details on such estimators can be found in the Appendix A Other precision matrix estimators, Appendix B Proofs, Appendix C Additional simulations.====Our method can be deconstructed into two steps. The first is to find a bias-corrected initial estimate ==== for the precision matrix ====. The second is to construct a ball in the operator norm topology corresponding to the target false positive rate and then search this ball for a sparse estimator. In actuality, given a false positive rate of ==== and a number of iterates ==== for Algorithm 1, we compute step size ====, and the algorithm returns a sequence of sparse precision estimators with successive false positive rates of ====. This sequence of estimators have nested supports and can be used in conjunction for multiple testing correction. Depending on the sample size, we can randomly partition our data set into smaller sets, apply this two-step procedure to each subsample in parallel, and combine the results for improved performance. In step two, we use a binary search procedure which converges rapidly minimizing the computational burden.====Our search methodology is effectively a method for controlled thresholding of the initial debiased estimator. The literature on thresholding for precision matrices is quite light when compared to thresholding covariance matrices [1], [2], [5], [16], [23]. The challenge is that unlike the unbiased empirical covariance estimator, there is no unbiased empirical precision estimator. Debiasing such precision matrix estimators allows for thresholding as mentioned in [13]. Furthermore, [13] proposed an entrywise thresholding method given some false positive rate ====, which while working well in some asymptotic sense, proved to not achieve the target false positive rate on simulated data. In [20]—which pre-dates the debiased graphical lasso [13]—false discovery rate control for Gaussian graphical models is achieved via thresholding on bias controlled estimators from the lasso or Dantzig selector. While the goal of [20] is the same as ours, theirs is an entrywise approach using the sup-norm and uniform convergence to normality whereas ours is a spectral approach using the operator norm and with no strict requirement of Gaussianity in Theorem 3.1.====This article is initially similar to [16], which focuses on estimating the covariance rather than the precision matrix. However, the technical challenges of estimating the precision matrix are much more extensive. In [16], the classic empirical covariance estimator is used as a starting point for controlled thresholding; this estimator is unbiased and severely rank deficient when ====. Nonetheless, nonasymptotic confidence sets can be constructed using concentration of measure and distributional assumptions such as log concavity are investigated. Proofs rely on symmetrization [18] and sub-Gaussian concentration results [19]. In contrast, this new work begins with the debiased graphical lasso estimator [13], which is slightly biased and full rank. The notion of confidence sets is eschewed in favour of greater focus on error control, and the proofs rely on tools from random matrix theory [24] adapted to the sparsity of the underlying precision matrix. By not relying on notions of confidence sets or the specifics of the empirical covariance estimator – i.e. low rank and unbiased – this new approach is more versatile and adaptable. With further investigation, the proposed algorithm in Section 2.3 is general enough to handle general matrix estimation with false positive control given a suitable low-bias full-rank initial estimator. Examples include estimation of sparse coefficient matrices in multivariate regression and an improved estimation algorithm for the covariance matrix.====In Section 2, the methodology is introduced mainly in Algorithm 1. False discovery rate control and a extended methodology involving subsampling are also discussed. Theoretical justification for this methodology is detailed in Section 3 and in the Appendix A Other precision matrix estimators, Appendix B Proofs, Appendix C Additional simulations. Section 4 contains simulated and real data applications. Proofs, additional discussion, and more simulation experiments are contained in the Appendix A Other precision matrix estimators, Appendix B Proofs, Appendix C Additional simulations.",Non-asymptotic error controlled sparse high dimensional precision matrix estimation,https://www.sciencedirect.com/science/article/pii/S0047259X20302712,3 October 2020,2020,Research Article,201.0
"Peyhardi Jean,Fernique Pierre,Durand Jean-Baptiste","IMAG, Université de Montpellier, CNRS, 34090 Montpellier, France,Biostatistics Department, Limagrain Field Seeds Research, Chappes Research Center, Chappes, France,Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP (Institute of Engineering Univ. Grenoble Alpes), LJK, 38000 Grenoble, France","Received 24 March 2020, Revised 8 September 2020, Accepted 8 September 2020, Available online 28 September 2020, Version of Record 9 October 2020.",https://doi.org/10.1016/j.jmva.2020.104677,Cited by (3),. It will be shown that most common ==== properties for estimators are obtained. Mixtures of splitting regression models are used on a mango tree dataset in order to analyze the patchiness.,"The analysis of multivariate count data is a crucial issue in numerous application settings, particularly in the fields of biology [2], ecology [7] and econometrics [34]. Multivariate count data can be defined as the number of items of different categories issued from sampling within a population, whose individuals are grouped. Denoting by ==== this number of categories, multivariate count data analysis relies on modeling the joint distribution of the discrete random vector ====. In genomics for instance, the data obtained from sequencing technologies are often summarized by the counts of DNA or RNA fragments within a genomic interval (e.g., RNA sequencing data). The most usual models in this framework are multinomial and Dirichlet multinomial regression to take account of some environmental covariate effects on these counts. In this way, Xia et al. [36] and Chen and Li [5] studied the microbiome composition (whose output are ==== bacterial taxa counts), while Zhang et al. [38] studied the expression count of ==== exon sets.====However, the multinomial and Dirichlet multinomial distributions are not appropriate for modeling the variability in the total number of counts in multivariate count data, because of their support: the discrete simplex ====. This particular support also induces a strong constraint in terms of dependencies between the components of ====, since any component ==== is deterministic when the ==== other components are known. This kind of distribution is said to be singular and will be denoted by ====. The parameter ====, being related to the support, is intentionally noted as an index of the distribution, distinguishing it from other parameters ==== used to define the probability mass function (pmf). Note that initially, singular versions of some multivariate distributions have been defined by Patil [22] and Janardan and Patil [13]. However, these distinctions were unheeded until now, leading to misuse of these distributions [38]. Therefore, a distribution will be considered as a ====-multivariate distribution if====Another problem that occurs when defining multivariate distributions, is the independence relationships between components ====. For instance, the multiple Poisson distribution described by Patil and Bildikar [23] involves ==== mutually independent variables. Therefore, a multivariate distribution will be considered as a sensu stricto multivariate distribution if:====Additionally, such a distribution is considered as an extension of a univariate distribution if:====Even if a singular distribution is not a sensu stricto ====-multivariate distribution, it is very versatile as soon as the parameter ==== is considered as a random variable. It then becomes a map between spaces of univariate and multivariate distributions. Assuming that ==== follows a univariate distribution ==== (e.g., binomial, negative binomial, Poisson etc …), the resulting compound distribution, denoted by ====, is called a splitting distribution. For instance, the multivariate hypergeometric – resp. multinomial and Dirichlet multinomial – splitting distribution, denoted by ==== – resp. by ==== and ==== – has been introduced by Peyhardi and Fernique [25]. They studied the graphical model of independence for such distributions according to the sum distribution ====. Jones and Marchand [15] studied the Dirichlet multinomial splitting distributions, and named them sum and Polya share distributions. They focused on the Dirichlet multinomial splitting negative binomial distribution, denoted here by ====. Here, we propose several extensions of their model, both regarding the sum and splitting distributions. As a consequence, our work is also related to the discrete Schur-constant distribution introduced by Castañer et al. [4], which can be viewed as a specific Dirichlet multinomial splitting distribution with ====. The framework proposed in this article can thus be viewed as a unifying formalism including several existing multivariate distributions and facilitating their generalization or the specification of new distributions.====Under mild assumptions, splitting distributions can be considered as sensu stricto multivariate distributions. They include all usual multivariate discrete distributions and several new ones. Many advantages derive from the compound aspect of splitting distributions. The interpretation is simply decomposed into two parts: the sum distribution (intensity of the distribution) and the singular distribution (repartition into the ==== components). The log-likelihood can also be decomposed according to these two parts and thus easily computed and maximized. This also facilitates the derivation of asymptotic and independence properties for maximum likelihood and Bayesian estimators. All usual characteristics (support, pmf, expectation, covariance and probability generating function (pgf)) are also easily obtained using this decomposition. Finally, the generalization to regression models is naturally achieved by compounding a singular regression by a univariate regression. This new framework eases the definition of generalized linear models (GLMs) for multivariate count responses, taking account of the dependence between response components.====This article is organized as follows. In Section 2 notations used throughout the paper are introduced. The definition of singular distributions, is used as a building block to introduce splitting distributions. Positive and symmetric singular distribution is introduced, easing respectively the study of criteria (i)-(ii), (iii)–(iv) for resulting splitting distributions. In Section 3 the subclass of additive convolution splitting distributions is introduced in order to simplify the calculation of marginal distributions. Sections 4 Multinomial splitting distributions, 5 Dirichlet multinomial splitting distributions focus on splitting distributions obtained with the multinomial and the Dirichlet multinomial distributions since they are both positive and additive (e.g., the multivariate hypergeometric is an additive but non-positive convolution distribution). This leads us to precisely describe fifteen multivariate extensions (among which five are natural extensions) of usual univariate distributions giving their usual characteristics. Some detailed attention is given to maximum likelihood and Bayesian parameter estimation regarding multinomial splitting distributions. Conditional independence properties and asymptotic normality for sum and singular distribution parameters are discussed in this framework. It is then shown that multinomial splitting regression constitutes an appropriate framework to introduce a family of GLMs for multivariate count responses. In Section 6 a comparison of splitting distributions and their mixtures is provided, based on an application to a mango tree dataset. The Appendix contains the proofs of all theorems, corollaries and properties.",Splitting models for multivariate count data,https://www.sciencedirect.com/science/article/pii/S0047259X2030258X,28 September 2020,2020,Research Article,202.0
"Abdi Me’raj,Madadi Mohsen,Jamalizadeh Ahad","Department of Statistics, Faculty of Mathematics and Computer, Shahid Bahonar University of Kerman, Kerman, Iran,Department of Mathematics and Statistics, McMaster University, Hamilton, Ontario, Canada","Received 30 April 2020, Revised 11 September 2020, Accepted 11 September 2020, Available online 25 September 2020, Version of Record 6 October 2020.",https://doi.org/10.1016/j.jmva.2020.104679,Cited by (11)," and ==== distribution, several multivariate measures of skewness are computed and compared. Because some measures of skewness are scalar and some are vectors, in order to evaluate them properly, a simulation study is carried out to determine the power of tests, based on sample versions of skewness measures as test statistics for testing the fit of the ==== distribution. Finally, two real data sets are used to illustrate the usefulness of the proposed model and the associated inferential methods.","The multivariate normal distribution plays a fundamental role in statistical analyses and applications. One of the most basic properties of the normal distribution is the symmetry of its density function. However, in practice, data sets do not follow the normal distribution or even possess symmetry, and for this reason, researchers search for new distributions to fit data with different features allowing flexibility in skewness, kurtosis, tails and multimodality; see for example, [18], [21]. Several new families of distributions have been introduced for modeling skewed data, including normal distribution as a special case. One such prominent distribution in the univariate case is the skew normal(====) distribution due to [6], [7]. The multivariate version of the ==== distribution has been introduced in [10]. This distribution has found diverse applications such as portfolio optimization concepts and risk measurement indices in financial markets; see [15]. A complete set of extensions of multivariate ==== distributions can be found in [8], [9]. [13] calculated and compared several different measures of skewness for the multivariate ==== distribution. [12] proposed a test to assess if a sample comes from a multivariate ==== distribution. Here, we use ==== and ==== to denote the probability density function (PDF) and the cumulative distribution function (CDF) of the ====-variate normal distribution, with mean ==== and covariance matrix ====, respectively, and also ==== and ==== to denote the PDF and CDF of the univariate standard normal distribution, respectively.====From [9] and [10], a ====-dimensional random vector ==== follows a multivariate ==== distribution if it has the PDF ====with stochastic representation ====where ==== stands for equality in distribution, ====, ==== and univariate random variable ==== has a standard normal distribution within the truncated interval ====, independently of ====. Truncated normal distribution in the interval ==== with parameters ==== is denoted by ====. The vector ==== is the skewness parameter vector, such that ====, for ====. The matrix ==== is a diagonal matrix formed by the standard deviations of ==== and ====. Here, ==== is identity matrix of size ====. The Hadamard product of matrices ==== and ==== is given by the ==== matrix ====. In the stochastic representation in (1), positive definite matrices ==== and ==== are covariance and correlation matrices, respectively. The parameters ====, ==== and ==== are the location, scale and skewness parameters, respectively.====Upon using the stochastic representation in (1), a general new family of mixture distributions of multivariate normal distribution can be introduced based on arbitrary random variable ====. A ====-dimensional random vector ==== follows a multivariate mean mixture of normal (====) distribution if, in (1), ==== is an arbitrary random variable with CDF ====, independently of ====, indexed by the parameter ====. Then, we say that ==== has a mean mixture of multivariate normal (====) distribution, and denote it by ====.====[37] presented a new family of distributions as a mixture of normal distribution and studied its properties in the univariate and multivariate cases. These authors defined a ====-dimensional random vector ==== to have a multivariate mean mixture of normal distribution if it has the stochastic representation ====, where ==== and ==== is an arbitrary positive random variable with CDF ==== independently of ====, indexed by the parameter vectors ==== and ====. The stochastic representation used by [37] is along the lines of the stochastic representation of the restricted multivariate ==== distribution (see [8]), but in this work, we use a different stochastic representation in (1). [37] examined some properties of this family in the univariate case for general ====, and also two specific cases of the family. In the present work, we consider the multivariate form of this family and study its properties.====In (1), if the random variable ==== is a skewed random variable, then the ====-dimensional vector ==== will also be skewed. In the ==== family, skewness can be regulated through the parameter ====. If in (1) ====, the ==== family is reduced to the multivariate normal distribution. The extended form of the ==== distribution is obtained from (1) when ==== is distributed as ==== variable truncated below ==== instead of ====, for some constant ====. The representation in (1) means that the ==== distribution is a “mean mixture” of the multivariate normal distribution when the mixing random variable is ====. Specifically, we have the following hierarchical representation for the ==== distribution: ====According to (2), in the ==== model, just the mean parameter is mixed with arbitrary random variable ====, and so this class cannot be obtained from the Normal Mean–Variance Mixture (====) family. The family of multivariate ==== distributions, originated by [14], is another extension of multivariate normal distribution, with a skewness parameter ====. A ====-dimensional random vector ==== is said to have a multivariate ==== distribution if it has the representation ====where ==== and ==== is a positive random variable and the CDF of ====, ====, is the mean–variance mixing distribution. Both families of distributions in (1), (3) include the multivariate normal distribution as a special case and can be used for modeling data possessing skewness. In (3), both mean and variance are mixed with the same positive random variable ====, while in (1) just the mean parameter is mixed with ====, however, the class in (1) cannot be obtained from the class in (3).====Besides, skewness is a feature commonly found in the returns of some financial assets. For more information on applications of skewed distributions in finance theory, one may refer to [2]. In the presence of skewness in asset returns, the ==== and skew-t (====) distributions have been found to be useful models in both theoretical and empirical work. Their parametrization is parsimonious, and they are mathematically tractable, and in financial applications, the distributions are interpretable in terms of the efficient market hypothesis. Furthermore, they lead to theoretical results that are useful for portfolio selection and asset pricing. In actuarial science, the presence of skewness in insurance claims data is the primary motivation for using ==== distribution and its extensions. In this regard, the ==== family that is developed here will also prove useful in finance, insurance science, and other applied fields.====[40] proposed that the ====-dimensional vector of returns on financial assets should be represented as ====. The ====-dimensional vector ==== is assumed to have a multivariate elliptically symmetric distribution, independently of the non-negative univariate random variable ====, having an unspecified skewed distribution. The vector ====, whose elements may take any real values, induces skewness in the return of individual assets. [3] have described multivariate versions of the normal–exponential and normal-gamma distributions. Both of them are specific cases of the model proposed in [40].  [1] and [3] used the representation in [40], with specific choices of ==== and ====, and introduced a number of distributions such as ====, extended ====, ====, normal–exponential, and normal-gamma, and investigated the corresponding distributions and their applications in capital pricing, return on financial assets and portfolio selection.====In this paper, with an arbitrary random variable ==== for the ==== family with stochastic representation in (1), basic distributional properties of the class such as the characteristic function (CF), the moment generating function (MGF), the first four moments of the model, distributions of linear and affine transformations, the canonical form of the family and the mode of the model are derived in general. Also, the maximum likelihood estimation of the parameters by using an EM-type algorithm is discussed, and then different measures of multivariate skewness are obtained. The special cases when ==== has standard gamma and standard exponential distributions, with the corresponding distributions denoted by ==== and ==== distributions, respectively, are studied in detail. For the ==== distribution, in addition to all the above basic properties of the distribution the infinitely divisibility of the model is also discussed. For the ==== distribution, the basic properties of the distribution as well as log-concavity of the model are discussed. The maximum likelihood estimates of the parameters of the ==== distribution are evaluated using the bias and the mean square error by means of a simulation study. Moreover, various multivariate measures of skewness are computed and compared. Finally, for two real data sets, the ==== distribution is fitted and compared with the ==== and ==== distributions in terms of log-likelihood value as well as AIC and BIC criteria.","Family of mean-mixtures of multivariate normal distributions: Properties, inference and assessment of multivariate skewness",https://www.sciencedirect.com/science/article/pii/S0047259X20302608,25 September 2020,2020,Research Article,203.0
"Haddouche Anis M.,Fourdrinier Dominique,Mezoued Fatiha","École Nationale Supérieure de Statistique et d’Économie Appliquée (ENSSEA), LAMOPS, Tipaza, Algeria,Université de Normandie, UNIROUEN, UNIHAVRE, INSA Rouen, LITIS, avenue de l’Université, BP 12, 76801 Saint-Étienne-du-Rouvray, France","Received 11 August 2019, Revised 15 September 2020, Accepted 15 September 2020, Available online 23 September 2020, Version of Record 1 October 2020.",https://doi.org/10.1016/j.jmva.2020.104680,Cited by (6),"The problem of estimating the scale matrix ==== (where ==== is a positive constant) perform poorly, we propose estimators of the general form ====, where ==== is the Moore–Penrose inverse of ==== and ==== is a correction matrix. We provide conditions on ==== such that ==== improves over ==== under the quadratic loss ====. We adopt a unified approach to the two cases where ==== is invertible and ==== is singular. To this end, a new Stein–Haff type identity and calculus on eigenstructure for ==== are developed. Our theory is illustrated with a large class of estimators which are orthogonally invariant.","Let ==== be an observed ==== matrix under the multivariate additive model ====where ==== denotes the unknown ==== matrix of parameters, ==== is an ==== elliptically symmetric distributed noise with unknown covariance matrix proportional to ==== (see (9)). Here, ==== is a ==== invertible scale matrix and ==== is the ====-dimensional identity matrix. The class of elliptically symmetric distributions encompasses a large number of important distributions such as the Gaussian, Cauchy, exponential, Student-t distributions and the Weibull distribution. Our main assumption is that the column space of ==== is known (or can be approximated by a known linear subspace). Also its known rank ==== satisfies ====where ====.====This model has been addressed by Candès et al. [3] who assumed that, even if ==== is unknown, its rank is known or can be approximated by a low-rank matrix. See also Candès and Recht [2], Ji et al. [21] and Nguyen et al. [26] in an applied setting. Note that Canu and Fourdrinier [4] extended to an elliptical context the Gaussian approach adopted by Candès et al. [3]. While these authors were interested in estimating the mean matrix ====, here, the parameter of interest is the scale matrix ====, which coincides with the covariance matrix in the Gaussian setting.====Thanks to the low-rank assumption (2), the additive model (1) can be presented in a canonical form (====) which separates information about the mean structure (a ==== matrix ====) and the information concerning the scale (a ==== matrix ====, where ==== is a ==== matrix). In this canonical context (see Section 2.1), the usual estimators are represented by ====where ==== is the sample covariance matrix and ==== is a positive constant. As these natural estimators perform poorly, we consider alternative estimators under the quadratic loss function ====where ==== is an estimator of ==== and ==== denotes the trace of the matrix ====.====In the literature, two cases are distinguished: the case where ==== is invertible ==== and the case where ==== is singular (====). As pointed out by James and Stein [20], the estimators in (3) are inadmissible under a normal multivariate distribution, which naturally remains true in the general elliptical setting. Since then, numerous authors have suggested improved estimators which improve over the unbiased estimator ==== and over the maximum likelihood estimator ====. In the invertible case ====, referred here as the low-dimensional setting, the literature includes, Stein [28], [29], Efron and Morris [8], Haff [15], Takemura [30], Dey and Srinavasan [7], Sheena [27]. Also, Kubokawa and Srivastava [24] showed that, under the Stein loss function ====, the improved estimators proposed in the normal setting remain robust, in the sense that they are still improved estimators within the elliptical distributions class. However, to our knowledge, a similar extension under the quadratic loss (4), which is more difficult to handle than the Stein loss, has not yet been obtained. More recently, with the massive amount of high throughput data, much interest has turned to the non-invertible case ====, often referred to the high-dimensional setting. Many authors proposed improved estimators in the Gaussian setting such as, Chen et al. [5], Ikeda et al. [19], Tsukuma and Kubokawa [32], Tsukuma [31] and Konno [22], who extended the result due to Haff [15], in the invertible case, to the high-dimensional setting and proposed improved estimators under the quadratic loss (4). However, as in the invertible case (====), no results have been yet established in the general elliptical framework under this loss.====Our main objective is the derivation of dominance results for alternative estimators of the form ====over the usual estimators ==== where ==== is a positive constant and ==== is a ==== matrix function. The two main features of our approach is that we treat the general elliptically symmetric distributions context and we unify the two cases where ==== is singular and ==== is invertible. For that purpose, we denote by ==== the Moore–Penrose inverse when ==== is singular, and the regular inverse ==== when it is invertible.====The remainder of this paper is organized as follows. The primary decision-theoretic results are presented in Section 2. We give sufficient conditions on the correction matrix function ==== for which ==== improves on ====. To do this, we derive a new version of the so-called Stein–Haff identity for this setting, which is the basis on which the development of improved estimators depends. In Section 3, based on a new calculus on eigenstructure of ====, we apply the results of Section 2 to the class of orthogonally invariant estimators. We also extend the estimator due to Haff [17] and Konno [22], in the low-dimensional setting and under the Gaussian assumption, to the class of elliptical symmetric distributions. In Section 4 examples illustrate the theory. In Section 5 we investigate the amount of improvement provided by the Konno estimator (see [22]) through numerical study. Finally, an Appendix contains technical results and the proofs of some of the findings in this paper.",Scale matrix estimation of an elliptically symmetric distribution in high and low dimensions,https://www.sciencedirect.com/science/article/pii/S0047259X2030261X,23 September 2020,2020,Research Article,204.0
"Ramsay Kelly,Durocher Stephane,Leblanc Alexandre","Department of Statistics and Actuarial Science, University of Waterloo, 200 University Ave W, Waterloo, ON, N2L 3G1, Canada,Department of Computer Science, University of Manitoba, 66 Chancellors Cir, Winnipeg, MB, R3T 2N2, Canada,Department of Statistics, University of Manitoba, 66 Chancellors Cir, Winnipeg, MB, R3T 2N2, Canada","Received 20 January 2020, Revised 8 September 2020, Accepted 8 September 2020, Available online 18 September 2020, Version of Record 30 September 2020.",https://doi.org/10.1016/j.jmva.2020.104678,Cited by (1),"The projection median as introduced by Durocher and Kirkpatrick (2005); Durocher and Kirkpatrick (2009) is a robust multivariate, nonparametric ====. It is a weighted average of points in a sample, where each point’s weight is proportional to the fraction of directions in which that point is a univariate median. The projection median has the highest possible asymptotic breakdown and is easily approximated in any dimension. Previous works have established various geometric properties of the projection median. In this paper we examine further robustness and ====.","Multivariate medians have been studied increasingly over recent decades. These medians include the Tukey or half-space median [36], the simplicial median [22], medians based on projection depth [38] of which the Stahel–Donoho estimator is a special case [8], [34] and many more. As in the univariate case, we expect a multivariate median to be robust against outliers. Robustness is most commonly assessed by the breakdown point; many multivariate medians have been shown to have high breakdown points, e.g., ==== for the half-space median under symmetry [9], ==== for the projection depth based estimators [38]. Other measures of robustness such as the influence function and related quantities [17] are much less common in the literature, possibly due to their mathematical complexity. Despite this fact, it remains important to assess the robustness of multivariate medians beyond the breakdown point. When influence function related results have been investigated, they have been favourable [5], [7], [39], [40]. That being said, most multivariate medians rely on solving an optimization problem which leads to computational issues when the sample size ==== or the dimension ==== are large as well as leading to difficulties deriving theoretical error bounds for approximation algorithms [23], [24]. The projection median as introduced by [11], [12], denoted ==== is a high-breakdown location estimator that has been previously demonstrated to have favourable computational properties, including implementable approximation algorithms that have theoretical error guarantees [12], [13]. Note that the projection median studied here is not the same as that of [38], [39]. The projection median studied in [38], [39] is denoted as ==== throughout this manuscript in order to distinguish the two. With its breakdown point and computational attributes established, it remains to study the asymptotics and further robustness properties (via the influence function approach) of ====. This is the subject of this paper.====Specifically, we derive the influence function of ====. We then use the influence function to produce bounds for the maximum bias and contamination sensitivity of ==== in both the general case and under symmetry assumptions. Additionally, we derive an exact expression for the gross error sensitivity in the general case. We compare the robustness results to that of popular multivariate medians, specifically, the Zuo projection median based on projection depth ==== [38] and the half-space median ==== [36]. We show that ==== has a lower maximum bias than these estimators in some simple contexts. The lower maximum bias comes at the cost of affine equivariance, as ==== is only similarity equivariant. We provide an algorithm for computing approximate versions of the presented robustness quantities for any distribution and dimension, which does not typically exist for other medians. In terms of asymptotics, we show that ==== is strongly consistent and asymptotically normal. An algorithm for estimating and computing the asymptotic covariance matrix of ==== is also provided. Lastly, we present a real data application, using the Wisconsin breast cancer dataset [28], [35].",Robustness and asymptotics of the projection median,https://www.sciencedirect.com/science/article/pii/S0047259X20302591,18 September 2020,2020,Research Article,205.0
"Klüppelberg Claudia,Krali Mario","Department of Mathematics, Technical University of Munich, Boltzmannstrasse 3, 85748 Garching, Germany","Received 6 December 2019, Revised 28 August 2020, Accepted 28 August 2020, Available online 17 September 2020, Version of Record 1 October 2020.",https://doi.org/10.1016/j.jmva.2020.104672,Cited by (14),A recursive max-linear ,"Our human society is continuously faced with challenges arising from factors of both uncontrollable and/or synthetic nature. The former is manifested through events such as natural disasters, in particular climate extremes like heavy rainfall or storms, unusually high/low temperatures, or river flooding. Similarly, synthetic factors correspond to those catastrophes influenced by human intervention, for instance industry fire, terrorist attacks, or a financial market crash. Such events occur rarely in isolation, but are rather interconnected, and occur simultaneously across certain instances; for example, floods disseminate through a river network, or extreme losses occur across several financial sectors. Such events make it necessary to not only understand dependencies between rare events, but also their causal structure.====When modelling rare events, one faces by definition a limited amount of data. While extremes in a univariate setting are well studied, multivariate extremes still remain a focus of present research. This is partly due to the augmented dimensionality problem, which affects crucially non-parametric methods (see de Haan and Ferreira [17], Chapter 7), but also by the lack of a parametric family to characterize interdependencies (see Beirlant et al. [2], Chapters 8, 9).====Recently, there has been interest in graphical models for modelling dependencies between extreme risks, which brings not only a potential complexity reduction, but also allows for modelling cause and effect in the context of extreme risk analysis. The model we consider in the present paper originates from Gissibl and Klüppelberg [13], where max-linear structural equation models (SEMs) have been proposed and investigated. The underlying graphical structure of the model is a directed acyclic graph (DAG), also called a Bayesian network. We refer to Lauritzen [24] and Diestel [6] for details on graphical modelling and graph theory, respectively.====Some other methods for combining graphical modelling with extremes have been proposed recently. In Segers [29], Markov trees with regularly varying node variables are investigated using the so-called tail chains. In Engelke and Hitz [11], a new approach using conditional independence relations between node variables is introduced, when considering undirected graphical models for extremes. This work is based on the assumption of a decomposable graph as well as the existence of density, which then leads to a Hammersley–Clifford type factorization of the latter into a lower dimensional setting. The method is applied to the estimation of flood in the Danube river network. A recursive max-linear model has been fitted to data from the EURO STOXX 50 Index in Einmahl et al. [8], where the structure of the DAG is assumed to be known.====High dimensions are a serious challenge for dependence modelling of extreme events, and as a consequence most of the applications so far have focused on a lower dimensional setting. One exception is Cooley and Thibaud [5], who present a new approach to extract the dependence structure from a regularly varying random vector. The authors propose the use of a dependence summary similar to the extreme dependence measure from Larsson and Resnick [23], which can be considered an analogue to the covariance. Aiming at reducing the complexity, the authors propose a decomposition technique alike that of the Principal Component Decomposition for normal distributions. Other approaches aiming at dimension reduction of extremes involve Chautru [4], and Janssen and Wan [19], who present clustering approaches, or Haug et al. [18], who propose a factor analysis for extremes.====In the present paper we develop a new structure learning and estimation algorithm for the recursive max-linear model from Gissibl and Klüppelberg [13], which allows us to first estimate a causal order, and then the dependence parameters of the model. Our approach is motivated by Cooley and Thibaud [5] and applies to regularly varying node variables, which is a common assumption for extreme risk modelling. Multivariate node distributions have heavy-tailed marginals and are eponymous to those which lie in the domain of attraction of multivariate Fréchet distributions; see Resnick [26], Section 5.4.2 (Proposition 5.15). We refer to Resnick [26], [28] for further details on regular variation.====We focus on the max-linear model for two reasons. Firstly, the setting of a max-linear Bayesian network captures the common experience that the largest shocks from ancestral nodes have a dominating role on their descendants. Following a winner-takes-it-all principle, this type of dependence follows extreme shocks propagating through the network, thus identifying certain nodes with primary influence to the other components. To this end, and in contrast to the classical (linear) SEM, the recursive max-linear model embeds such non-linear dependencies between extreme quantities, while respecting its network structure. Secondly, max-linear models have the property of approximating any dependence structure between extremes arbitrarily well as the number of involved factors grows, a property which makes them an attractive and interesting object of study in extreme value theory; see Fougerès et al. [12].====Our multivariate regular variation setting is similar to that in Gissibl et al. [15], which investigates the use of the tail dependence coefficients matrix towards the recovery of a causal order and identifiability of the max-linear coefficient matrix. Their method has the severe drawback that the source nodes have to be known. For instance, in a DAG with two nodes and one edge there can only be one source node, which cannot be determined by the tail dependence coefficient as it is symmetric. This problem is to be encountered also in a DAG of larger size with several source nodes, thus being a serious disadvantage to find a causal order. In contrast, other than regular variation itself, our methodology is free of assumptions. More recently, in a heavy-tailed setting, Gnecco et al. [16] propose a method for identifying a causal order from the estimated conditional means of the integral transforms of pairs of nodes.====For arbitrary recursive max-linear models, a different identification and estimation method based on a generalized MLE can be found in Gissibl et al. [14] and Klüppelberg and Lauritzen [21]. An extension of this method to models with observational noise has been investigated in Buck and Klüppelberg [3]. In these papers all innovations, in addition to being independent, have to be identically distributed, which is stronger than the tail assumptions imposed by regular variation and used in our paper.====We develop a new non-parametric methodology aimed at applying recursive max-linear models to extreme phenomena in a multivariate regular variation setting. Firstly, targeting the problem of recovering a causal structure as a graphical model on a DAG, we propose a new technique via the scaling parameters of multivariate marginal distributions. This scaling technique allows for the manipulation of the dependence structure between extremes by simple scalar multiplication. These manipulations then uncover specific parts of the spectral measure, which fully characterize the dependence structure of interest to pave the way for estimating the causal dependence structure of the model. Secondly, we estimate the spectral measure empirically, where we focus on the relevant parts for the estimation of the required scaling. Asymptotic properties of the empirical spectral measure proven as an extension of a result of Larsson and Resnick [23] lead to consistent and asymptotically normal estimates of all dependence parameters.====The application of the proposed methodology to financial data and to food dietary data shows that the recursive max-linear model can model multivariate extremes from real-life data with the goal of inferring causality for high risks, thus advocating for the application of the model in such areas.====Our paper is structured as follows. Preliminaries on graph theoretical terminology and regular variation, including the scaling parameters, are introduced in Section 2. Section 3 provides relevant properties of recursive max-linear models with regularly varying node variables. In Section 4 we show how the dependence structure of a recursive max-linear model can be identified from the scaling parameters of the model. Section 5 prepares for the causal inference by applying the scaling technique to find source nodes as well as to reorder all other components into generations. Section 6 deals with statistical inference of the model. We propose non-parametric estimators of the relevant scalings, which also yield estimators of the dependence parameters. This allows us to estimate a partial order of the nodes and, in particular, a well-ordered graphical model on a DAG. We also show the asymptotic normality of the model dependence parameters. Finally, Section 7 is dedicated to structure learning and estimation and two applications, namely to a real world financial data set of industry portfolio returns, as well as food dietary interview data. The Supplementary Material [20] has four sections: S1 shows the performance of our algorithms in a simulation study, S2 gives more background information on the industry portfolio data and some explorative analysis, S3 and S4 show some figures on bivariate extremes of two different data sets.",Estimating an extreme Bayesian network via scalings,https://www.sciencedirect.com/science/article/pii/S0047259X20302530,17 September 2020,2020,Research Article,206.0
"Mohammedi Mustapha,Bouzebda Salim,Laksaci Ali","Université Djillali Liabès, BP 89, 22000, Sidi Bel Abbès, Algérie L.S.P.S., Sidi Bel Abbès, Algerie,Université de Technologie de Compiègne, L.M.A.C., Compiègne, France,Department of Mathematics, College of Science, King Khalid University, Abha, 61413, Saudi Arabia","Received 4 March 2020, Revised 28 August 2020, Accepted 28 August 2020, Available online 9 September 2020, Version of Record 12 September 2020.",https://doi.org/10.1016/j.jmva.2020.104673,Cited by (13),"The aim of this paper is to nonparametrically estimate the expectile regression in the case of a functional predictor and a scalar response. More precisely, we construct a kernel-type estimator of the expectile ====. The main contribution of this study is the establishment of the ","Risk measures such as quantiles and expected shortfall have been studied and developed by several authors from a practical and a theoretical point of view, both risk measures have their own merits and defects. In this paper, we investigate an alternative way based on the least asymmetrically weighted squares estimation, borrowed from the econometrics literature, that is one of the basic tools in statistical applications. This method often involves [44] concept of expectiles, a least squares analogue of the traditional quantiles. The proposed risk measure allows to overcome some drawbacks of the classical measures. It is expressed by ====such that ====with ==== is the scoring function, the level ==== is called the asymmetry parameter and ==== stands for the indicator function of the event ====. Such risk model was introduced by [3], [44], in the context of linear regression models. It is worth noticing that (2) generalizes the conditional ====tation of ==== given ====, which coincides with ==== when specifically ====. On the other hand, (2) is similar to the conditional ====-quan==== of ==== given ====, which can be obtained by replacing ==== by ==== in (2). This motivates the name the conditional ====-expectile. Hence the expectile regression is constructed by combining the ideas of least square regression to those of the conditional quantile allowing to accumulate the advantages of both models. In particular, its high sensitivity to the extreme value permits for more prudent and reactive risk management, and can be considered as an important advantage, since if we are measuring potential losses, we want that our measure to be sensitive to the extreme tail losses. The main aim of this work is to analyze the asymptotic behavior of this model using the functional nonparametric approach.====Recall that, in a multivariate parametric context, the expectile model has received growing interest. We cite for instance [9] for dapper discussion on the advantages of this model over the classical risk measures such as the quantiles and the expected shortfall. They indicate that the expectiles are perfectly reasonable alternatives to the value at risk (VaR) and the expected shortfall (ES) risk measures. The expectile based risk measure is coherent for ====. Moreover, according to [10], [12], the expectile is the only coherent as well as elicitable risk measure. For more details about the statistical significance of the property of elicitability and coherence, we refer the interested reader to the papers of [8], [19], [25], [55]. It turns out that, similar to the conditional mean or the percentile, the expectile regression of order ==== (or the conditional ====-expectile) can be uniquely be defined by ====Although they present differences in their construction, both quantiles and expectiles share similar properties. The main reason for this, as shown in [31], is the fact that expectiles are precisely quantiles but for a transformation of the original distribution. [2] established an important feature is that quantiles and expectiles of the same distribution coincide under the hypothesis of weighted symmetry and pointed out that inference on expectiles is much easier than inference on quantiles. Notice that the quantiles are not always satisfactory and can be criticized for being somewhat difficult to compute as the corresponding loss function is not continuously differentiable. The key advantage of the expectile over the quantile is its efficiency and computing expedience, although it has not a direct interpretation as the quantile in terms of the relative frequency, see [15]. Another substantial difference is that the conditional expectile is based on fundamentally different information and relies on more complete information to measure the risk in the sense that it depends on the form of the entire distribution, and it is easy to see that the conditional expectiles are characterized by the tail expectations in the same way that the conditional quantiles are characterized by the conditional distribution function as shown by the following equation, for more details refer to [1], [44]: ====where ==== represents the conditional distribution function of ==== given ==== and ==== is the conditional expectile of order ====.====The conditional expectile has been widely studied in the applied area such as econometrics, finance and actuarial science, see, for instance [37] as pioneer work, [18]. Despite this importance, the expectile regression is unexplored compared to both competitive regressions (conditional mean and quantile). [11] has generalized the conditional expectation to the conditional expectile by means of the minimization of an asymmetric quadratic loss function and exposed their main properties. [33] has defined the regression quantile estimators via an asymmetric absolute loss, the same authors in [34] have studied a new approach of regression quantiles based upon the estimates of the coefficients of the regression equation. Making use of the expectiles, [16] has proposed the estimations of the VaR and the ES or the CVaR. More details and results on regression are given by many references see [17], [28], [36], [43]. While, these cited works consider the finite dimensional case, in the present work we use this model to analyze the effect of a functional covariate on a scalar response variable. Therefore, we treat in this paper the functional case. Note that analogously to the quantile regression, the conditional expectile allows to a get an exhaustive information on the impact of explanatory variable on the response one by exploring its conditional distribution. On the other hand, the expectile regression can be employed for other standard regression model such as the curves discrimination or the prediction problems. Such questions of functional data analysis are particularly interesting, for many applied areas. Some basic materials in this modern branch of statistics can be found in the monographs of [24], [29], [30], [35], [48] or in the special issues [5], [6], [26]. It should be noted that the principal motivation of the functional statistics is the recent technological development of the measuring instruments allowing the data recording over thinner discretization grids. In this context, the economics or financial data, which are the principal applied areas of the expectile model, constitute a natural source of functional data. Accordingly, the inference statistic by the functional expectile is motivated by the diversity of its application areas including the risk analysis, the discriminant analysis and the forecasting problem.====Notice that the nonparametric estimation in the conditional models when the regressor takes its values in an infinite dimensional space, is an interesting subject and further details can be found in [24], with more details. [1] studies the nonparametric weighted symmetry tests, while the almost complete convergence of the conditional quantile kernel estimators is proved by [24] in the independent and identically distributed observations (i.i.d.). The dependent case is considered by many authors, see for example [23], [38], [39]. The papers of [14], [20] investigated the asymptotic normality of this estimator in both cases, i.i.d. and strong mixing variables. Moreover, the kernel method based on the locally linear fit is adapted to the nonparametric estimation of regression expectiles and percentiles by using the ALS approach, and under the assumptions of stationarity and ====-mixing, the asymptotic normality for the estimators of conditional expectiles is thoroughly discussed in the paper of [54].====In this contribution we establish the asymptotic properties of the kernel-type estimator of the expectile regression in the case of a functional predictor and a scalar response. To our best knowledge, this problem was open up to present, and it gives the main motivation to our paper. We focus on the almost complete consistency by giving its convergence rate, and we will establish the asymptotic normality of the estimator. The obtained results are stated under standard assumptions in nonparametric functional statistics. Nevertheless, a lot of attention has been paid to other nonparametric functional regression models such as the conditional expectation and the conditional quantiles. Among the wide literature concerning the nonparametric treatment of these functional models we only refer to [4], [24], [38], [39] and the references therein. Finally, we refer the reader to the survey paper [41] for a state of the art as well as some important future tracks on this topic of the nonparametric functional data analysis.====The layout of the article is as follows. In Section 2, we first present and describe the nonparametric model of the expectile regression. In Section 3, we establish our main results of the almost complete convergence of the kernel estimators and the asymptotic normality under some mild conditions in the i.i.d case. In Section 3.2, we extend our results to the dependent setting by showing the consistency in probability for the ====-mixing sequences. In Section 3.3, we discuss an application of our results for the construction of the confidence interval. In Section 4, we discuss some connections of the conditional expectile in the supervised classification, prediction and financial risk analysis problems. The implementation of the expectile regression model in practice is given in Sections 4.3 Simulation study, 4.4 Data example, where a comparison with the quantile regression by illustration on real data is provided. Some concluding remarks are given in Section 5. To avoid interrupting the flow of the presentation, all mathematical developments are relegated to Section 6. For a better understanding of our proofs, we recall some relevant results in the Appendix.",The consistency and asymptotic normality of the kernel type expectile regression estimator for functional data,https://www.sciencedirect.com/science/article/pii/S0047259X20302542,9 September 2020,2020,Research Article,207.0
Asta Dena Marie,"Department of Statistics, The Ohio State University, 1958 Neil Ave, Columbus, OH 43210, USA","Received 31 December 2019, Revised 1 September 2020, Accepted 2 September 2020, Available online 9 September 2020, Version of Record 7 November 2020.",https://doi.org/10.1016/j.jmva.2020.104676,Cited by (5),.,"Data, while often expressed as collections of real numbers, are often more naturally regarded as points in non-Euclidean spaces. To take an example, radar systems can yield the data of bearings for planes and other flying objects; those bearings are naturally regarded as points on a sphere [13]. To take another example, diffusion tensor imaging (DTI) can yield information about how liquid flows through a region of the body being imaged; that three-dimensional movement can be expressed in the form of symmetric positive definite ====-matrices [13]. To take yet another example, the nodes of certain hierarchical real-world networks can be regarded as having latent coordinates in a hyperboloid [1], [8]. In all such examples, the spaces can be regarded as subsets of Euclidean space even though Euclidean distances do not reflect true distances between points. An ordinary kernel density estimator (KDE) applied to sample data generally will not be optimal in terms of the ====-risk with respect to the volume measure on the non-Euclidean manifold.====The idea of kernel density estimation is to smooth out, or convolve, an empirical estimator (an average of Dirac distributions centered at the data) with a smooth rapidly decaying kernel so as to obtain a smooth estimate of the true density. The literature offers some variants of kernel density estimation on non-Euclidean spaces. A simple variant, for compact manifolds [11] or more general compact subsets of manifolds [2], [3], applies a Euclidean kernel having supports small enough to fit inside the charts. A more general version, defined on complete manifolds, generalizes the kernel to be defined on the tangent bundle — in effect, requiring a kernel for each point [7]. Minimax rates of convergence have been proven for all of these different variants in terms of a Hölder class exponent [7], [11]. It is desirable to refine these convergence rates based on Sobolev constraints on the true densities.====On symmetric spaces like Euclidean space, a kernel need only be defined at one point and transported everywhere else. All symmetric spaces ==== can be decomposed into symmetric spaces of Euclidean, compact, and noncompact type in such a way that a KDE on ==== can be constructed from KDEs on the three types. Symmetric spaces of the first two type admit KDEs with minimax convergence rates. The goal of this paper is to begin to complete the picture for symmetric spaces of noncompact type, by proving the upper bound part of a conjectured minimax rate.====Kernel density estimation for random variables can be interpreted as an estimate of a Fourier transform of the density. Thus for manifolds on which Fourier analysis generalizes, there should exist some generalization of the KDE. One of the earliest such Fourier-based generalizations of the KDE is defined for compact manifolds and shown to be minimax [5]. Certain density estimators on the non-compact Poincaré halfplane [6] and the space of symmetric positive definite matrices, based on Helgason–Fourier Analysis, have been shown to be minimax for estimation from corrupted samples. In the special case where the noise is non-existent, these estimators can be regarded as special cases of a KDE where the kernel is a natural generalization of the sinc kernel. However, these estimators have not been shown to be minimax for estimation from uncorrupted samples. Moreover, the full of generality of Helgason–Fourier Analysis is not exploited in defining and analyzing this estimator.====The Helgason–Fourier transform, unlike the ordinary Fourier transform, sends functions on a symmetric space of noncompact type to functions on a different frequency space. The exact form of this frequency space, much less a usable formula for the transform, depends on a geometric understanding the original space. Countless symmetric spaces of interest in applications have well-understood geometries. When the original space is the Poincaré halfplane, for example, the frequency space is a cylinder. The Helgason–Fourier transform, like the ordinary Fourier transform, is an isometry on ====-function spaces and sends convolutions to products in a certain sense.====This paper uses the Helgason–Fourier transform to construct and analyze a version of a KDE on symmetric spaces of noncompact type. We define a kernel density estimator for symmetric spaces of non-compact type ====, for which Helgason–Fourier transforms are defined. Unlike the non-Fourier-based variant [11] for compact manifolds, this variant is differentiable everywhere and estimates densities with non-compact support. The analogue of a kernel is often just a density on a space ==== of isometries invariant with respect to the subgroup ==== of ==== for which ====. An example is a Gaussian, a solution to the ====-invariant heat equation, We bound risk in terms of bandwidth ====, the number ==== of sample points, a Sobolev parameter ====, and the sum of the restricted roots of ==== [Theorem 1]. Optimizing ==== in terms of ====, we obtain an upper bound of ====for the convergence rate, where ==== is a Sobolev parameter, under natural assumptions on the density space [Theorem 1]. We then obtain a simplified formula, that can be implemented on a computer, for the special case where ==== is the ====-dimensional hyperboloid of constant curvature ====. The proof for the upper bound of the convergence rates adapts techniques used in [6]. We conjecture that the same upper bound yields a lower bound and hence a minimax rate, and a proof is reserved for future work.",Kernel density estimation on symmetric spaces of non-compact type,https://www.sciencedirect.com/science/article/pii/S0047259X20302578,9 September 2020,2020,Research Article,208.0
"Wang Zhendong,Xu Xingzhong","School of Mathematics and Statistics, Beijing Institute of Technology, Beijing, 100081, China,Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing, 100081, China","Received 27 August 2020, Accepted 27 August 2020, Available online 9 September 2020, Version of Record 12 September 2020.",https://doi.org/10.1016/j.jmva.2020.104674,Cited by (0), has become an active area in contemporary ,"Recent years have seen advances in data gathering skills and rapid development of high performance computing, which bring challenges in analyzing data with dimension ==== larger than the sample size ==== ubiquitous in various fields ranging from natural language processing, medical images studies and financial data analysis to DNA microarray analysis. Statistical inference about this high dimensional data often concerns testing whether the population covariance matrix ==== satisfies a prescribed structure, such as identity structure and sphericity structure. Specifically speaking, for a random vector ==== with covariance matrix ====, it is of interest to test whether ==== satisfies the following hypotheses: ====and ====For testing hypotheses (1), there exist some conventional methods whose asymptotic properties are established in the regime where ==== is fixed and ==== tends to infinity. See, for example, the likelihood ratio test in [3] and the test in [22] based on scaled distance. However, the likelihood ratio is unbounded and the consistency of Nagao’s test is no longer preserved when ==== grows to infinity as demonstrated in [18]. As a result, to remedy the incapability of the traditional tests and address the large dimension issues, new methodologies are needed for tackling the high dimensional hypotheses testing problems. In the last decades, a large body of tests have been proposed from different perspectives. In their pioneer work, [4] corrected the classical Likelihood Ratio Test (LRT) when ==== and derived its asymptotic distribution under the null hypothesis. [15] extended the results to the case ==== and ==== by using the Selberg integral. [31] further generalized the results when the population mean is unknown with the so-called ====. In [9], the authors obtained the central limit theorem of LRT both under the null and the alternative as ====. Based on some natural distance such as the squared Frobenius norm of ====, [18] proposed a modified Nagao’s test for testing identity and showed its consistency when ====. [11] developed a new test procedure by some well constructed ====-statistics and studied their asymptotic behavior regardless of normality and the growth rate of ==== relative to ====. By giving unbiased and consistent estimators of ====, [25] put forward a new test statistic that is asymptotically normal when ====, ====. For more extensions and issues on this line, see [12], [26].====For sphericity test (2), the earliest study can date back to [21] which adopted the LRT as the test statistic. [16] proposed a locally most powerful invariant test for sphericity. [18] revisited the test proposed in [16] and ensured the robustness of the test statistic under normality assumption when ====. [29] released the Gaussian assumption and derived the asymptotic distribution of John’s test for general population provided a finite fourth moment. In [20], it was found that John’s test possesses the same limiting null distribution with any ====-asymptotics. The alternative distribution together with the power function was also given. Without assuming any explicit relation between ==== and ====, [11] proposed a ====-statistic based test procedure with the observations following a multivariate model. By utilizing the ratio of arithmetic means of the sample eigenvalues, [13], [25] developed two tests from the Cauchy–Schwarz inequality. For more literature dealing with testing hypotheses (2), see, among others, [14], [23].====Different from the existent frequentist testing methods, we put forward in this paper a novel test based on ==== (PBF), which is a Bayesian-motivated testing procedure. This paper aims to build a unified framework of PBF for testing high dimensional covariance matrices when ====, regardless of normality. Furthermore, the results are suitable for the complex random vectors with the aid of random matrix theory. We also study the asymptotic power of the test under spiked and general alternatives when ==== and ====, respectively. The superiority of the proposed test over the existing methods is further confirmed by a series of numerical experiments.====The rest of this paper is structured as follows. Firstly, the explicit forms of PBF for testing high-dimensional covariance matrices and some preliminary results are presented in Section 2. In Section 3, we investigate the asymptotic null distribution of PBF with any ====-asymptotics, i.e., ====, ====, based on which we put forward a new test procedure for the identity test. Section 4 establishes the limiting normal distribution for PBF under spiked alternatives with ====, and general alternatives with ====, ====. In Section 5, we derive the theoretical properties of PBF for the sphericity test when ====. Extensive simulation results are provided in Section 6 to compare the finite-sample performance of the proposed test with some existing methods in the literature. An empirical study on a real data example is outlined in Section 7. Section 8 includes some concluding remarks. The proofs are postponed to the Appendix A and all the tables and figures together with the technical details of the proofs are provided in the supplementary material.",Testing high dimensional covariance matrices via posterior Bayes factor,https://www.sciencedirect.com/science/article/pii/S0047259X20302554,9 September 2020,2020,Research Article,209.0
"Song Jun,Li Bing","Department of Mathematics and Statistics, University of North Carolina at Charlotte, United States,Department of Statistics, The Pennsylvania State University, United States","Received 7 May 2019, Revised 28 August 2020, Accepted 29 August 2020, Available online 9 September 2020, Version of Record 28 September 2020.",https://doi.org/10.1016/j.jmva.2020.104675,Cited by (13),We introduce a nonlinear additive functional principal component analysis (NAFPCA) for vector-valued functional data. This is a generalization of functional principal component analysis and allows the relations among the random functions involved to be nonlinear. The method is constructed via two additively nested ,"Modern technologies have made functional data increasingly prevalent in sciences and industries. For example, functional magnetic resonance imaging (fMRI) records brain activities as a collection of functions on a time interval; handwriting data, which can be regarded as two-dimensional vector-valued functions, are widely collected by electronic devices; health data such as blood pressures are routinely recorded by smart wearable devices. Since functional data are intrinsically infinite-dimensional, it is important to extract useful and interpretable information from them by suitable dimension reduction methods.====Principal component analysis (PCA) is one of the most popular methods in exploratory data analysis for extracting useful information from a sample of vectors. Intuitively, it seeks directions in the vectors that represent the greatest variation. Let ==== be a ====-dimensional random vector. At the population level, PCA solves the following problem ====This optimization is performed successively in a sequence of orthogonal spaces, resulting in a sequence of vectors ====. The projected random variables, ====, are then used as the principal components. See, for example, [15].====PCA was generalized to the nonlinear case by [28] by finding the nonlinear function of ==== in a similar way. The idea is to replace the space of linear functions of ==== with a Hilbert space of nonlinear functions of ====, say ====, so as to capture the nonlinearity of the random vector ====. Specifically, the first nonlinear principal component ==== is an element in ==== that satisfies ====Thus, the nonlinear PCA seeks the function of ==== in a much larger space so that it captures more complex features than the linear PCA. The particular Hilbert space ==== used by [28] is the reproducing Kernel Hilbert space (RKHS), which is computationally convenient. For this reason, the nonlinear-type PCA is called the kernel principal component analysis (KPCA).====Another useful generalization of PCA is to functional data, where ==== is a function rather than a vector. The idea is to enlarge ==== to an infinite-dimensional Hilbert space to accommodate the random function ====, and replace the Euclidean inner product by a functional inner product. More specifically, let ==== be a Hilbert space of functions defined on an interval and ==== be a random element in ====. In functional PCA (FPCA), we seek the member ==== of ==== such that ====See Ramsay and Li [24], Ramsay and Silverman [25], and [21].====In this paper, we further extend the functional PCA to accommodate nonlinear functions of functional data. The nonlinear and functional nature of our problem requires two separable Hilbert spaces, say ==== and ====; ==== is the space where ==== resides, and ==== includes nonlinear functions from ==== to ====, which captures the nonlinear feature of ====. To avoid the curse of dimensionality, we assume ==== to be an additive space; that is, each kernel only contains one component of ====. We develop the numerical procedure and theoretical properties of this method, both at the population level and at the asymptotic level.====The rest of the paper is organized as follows. In Section 2, we propose the Nonlinear Additive Functional Principal Component Analysis (NAFPCA) at the population level. In Section 3, we present the sample-level estimation algorithm. In Section 4, we develop the asymptotic results of the NAFPCA. In Section 5, we conduct simulation comparisons between FPCA and NAFPCA. Some concluding remarks are made in Section 6. Real data applications to handwritten digits and EEG data sets are shown in the supplementary material.",Nonlinear and additive principal component analysis for functional data,https://www.sciencedirect.com/science/article/pii/S0047259X20302566,9 September 2020,2020,Research Article,210.0
"Yu Long,He Yong,Zhang Xinsheng","School of Management, Fudan University, Shanghai, 200433, China,School of Statistics, Shandong University of Finance and Economics, Jinan, 250014, China","Received 18 August 2018, Revised 28 August 2019, Accepted 28 August 2019, Available online 6 September 2019, Version of Record 9 September 2019.",https://doi.org/10.1016/j.jmva.2019.104543,Cited by (17)," and time dimensions ==== go to infinity. Simulation study shows that the new estimators perform much better in heavy-tailed data setting while performing comparably with the state-of-the-art methods in the light-tailed Gaussian setting. At last, a real macroeconomic data example is given to illustrate its empirical advantages and usefulness.","Factor models provide a flexible way to extract main features and summarize information from large datasets with relatively smaller number of common factors, and are widely applied in research areas such as finance and biology. A fundamental topic is to consistently determine the number of latent factors in large-dimensional settings, where cross-sections ==== and time dimensions ==== go to infinity simultaneously. Plenty of literatures have focused on this topic for the static, dynamic and continuous-time factor models, including (but not limited to) [1], [2], [3], [4], [5], [6], [8], [15], [22], [23], [24], [30], [31], [33], [34], [35].====Chamberlain and Rothschild [9] proposed the static approximate factor models, from which the factor number is often assumed to be known in advance rather than determined by the data until Bai and Ng [5] first presented consistent estimators for the number of common factors in the large-dimensional setting. The proposed information criteria borrow idea from the Akaike information criterion (AIC) while with the penalty term specified as a function of both time dimensions ==== and cross-sections ====. Alessi et al. [3] added a tuning parameter on the penalty of the criteria in [5] and improved stability in the finite samples case as well as the case with large idiosyncratic disturbances. Another line of research on determining factor number mainly relies on the random matrix’s eigenvalue theory. Onatski [31] provided simple algorithms based on the empirical distribution of the sample covariance matrix’s eigenvalues. Lam and Yao [23] and Ahn and Horenstein [1] proposed the eigenvalue-ratio type estimators separately and independently, which remain reliable even when the idiosyncratic errors are cross-sectionally dependent and serially correlated. Xia et al. [34] further improved these estimators by transformation and shrinkage, resulting in better performance in scenarios when weak, strong or dominated factors exist. The list of literatures here is only illustrative rather than comprehensive.====Both the information-criterion methods and the eigenvalue-based methods perform well only when some moment constraints are satisfied. The literatures mentioned above all assume that the fourth moments of common factors and idiosyncratic errors are bounded. However, in real data application, it is often the case that we are encountered with heavy-tailed data and the bounded fourth moment constraints are not satisfied, especially in the areas of finance and economics. Fig. 1 shows the frequency histogram of the sample kurtosis for 128 macroeconomic variables. The dataset was originally provided in [27], named as FRED-MD, and is continuously updated. We considered the period from 1959/01 to 2018/02, the most recent dataset when the paper was finished. After removing the time trend, over 1/3 of the 128 variables show larger sample kurtosis than the value 9, which is the theoretical kurtosis of ==== distribution. Thus it is more reasonable to model the macroeconomic variables with heavier-tailed distributions such as ==== distribution.==== Fig. 2 further demonstrates the vital importance of taking heavy-tailed feature into consideration when determining the factor number. The empirical performances of two methods are compared, one is the “ER” method proposed by [1], and the other is a modified version named “MKER” proposed in this paper. In this example, the true number of factors is 3 and the detailed data-generating procedure is presented in Section 4. Fig. 2 shows the barplots for the frequency of the estimated factor number based on 1000 replications. For simulated Gaussian data, “ER” and “MKER” both perform well. However, for simulated heavy-tailed data from ==== distribution and Cauchy distribution, “MKER” still performs very well and shows robustness while the “ER” method gradually loses power as the tail becomes heavier.====Recently, some researchers focus on heavy-tailed factor models [7], [13], [21]. Fan et al. [13] considered Elliptical Factor Models (EFM) for large-scale covariance estimation. Calzolari and Halbleib [7] proposed a factor model structure with ====-stable distributions, and proposed an indirect inference method for parameter estimation. However, both of the above two papers treated the factor number as given. Klüppelberg and Kuhn [21] proposed a testing procedure to determine the number of common factors under the elliptical copula factor model. They primarily focused on the case with fixed ==== and cross-sectionally uncorrelated errors. To the best of our knowledge, the current paper provides the first method to specify the factor number for heavy-tailed data with large ==== and ====.====In this paper, we propose two consistent estimators for the number of common factors in the EFM framework. Advantages of the proposed methods lie in the following aspects. Firstly, we do not assume any moment constraints, thus the proposed estimators are consistent even when the observations are from heavy-tailed distributions such as ==== or Cauchy. Secondly, the theoretical properties are guaranteed in the large-dimensional setting where the dimension ==== can be much larger than the sample size ====. Actually, ==== is sufficient for guaranteeing the consistency of the proposed estimators. Thirdly, the new estimators are eigenvalue-based, thus it is convenient to do the similar transformations or shrinkages as in [1] and [34] to improve their performances when weak, strong or dominated factors exist.====We introduce the notations adopted throughout the paper. For a real number ====, denote ==== as the largest integer smaller than or equal to ====. Let ==== be the indicator function. Let ==== be a ==== diagonal matrix, whose diagonal entries are ====. It also holds when ====, ==== are square matrices. For a matrix ====, let ==== (or ====) be the ====th row, ====th column entry of ====, and let ==== be the transpose of ==== and ==== be the trace of ====. Denote ==== as the ====th largest eigenvalue of ==== and let ==== be the spectral norm of ====, ==== be the Frobenius norm of ====. For a nonnegative definite matrix ====, ====. For two random variable series ==== and ====, ==== means ==== and ====. For two random variables (vectors) ==== and ====, ==== means the distributions of ==== and ==== are the same. The constants ==== in different lines can be nonidentical.====The rest of this paper is organized as follows. Section 2 introduces the static Elliptical Factor Model (EFM) framework and the multivariate Kendall’s tau matrix. The construction of the estimators and main theoretical results are shown in Section 3. Section 4 displays simulation results and Section 5 contains a real data example, to empirically illustrate the superiority of the proposed estimators. Conclusions and discussions are provided in Section 6. Technical proofs and more simulation results are delegated to Appendix A.",Robust factor number specification for large-dimensional elliptical factor model,https://www.sciencedirect.com/science/article/pii/S0047259X18304378,6 September 2019,2019,Research Article,217.0
"Fan Guo-Liang,Xu Hong-Xia,Liang Han-Ying","School of Economics and Management, Shanghai Maritime University, Shanghai, 201306, PR China,Department of Mathematics, Shanghai Maritime University, Shanghai, 201306, PR China,School of Mathematical Sciences, Tongji University, Shanghai 200092, PR China","Received 23 March 2019, Revised 21 August 2019, Accepted 21 August 2019, Available online 29 August 2019, Version of Record 4 September 2019.",https://doi.org/10.1016/j.jmva.2019.104542,Cited by (2), consistent under some mild conditions. The structural dimension is determined by a BIC-type criterion and the consistency of its estimator is established. Comprehensive simulations and a real data analysis show that the proposed method works promisingly.,"With rapid advance of technology, observations for a large number of variables can be easily collected. High-dimensional regression analysis is one of the most popular tools that help us to gain insight into relationships between two sets of high dimensional variables. Suppose that ==== is multivariate response and ==== is a covariate vector. The general object of interest is to focus on the conditional mean function ====. However, nonparametric estimation of ==== involves a high-dimensional predictor ====, which suffers from the “curse of dimensionality”. Sufficient dimension reduction (SDR) has been proposed to reduce the dimension of ==== while preserving its information in ====. The aim is to search a matrix ==== such that ====, where ==== can be estimated efficiently if ==== is small. Noting that ==== is not identifiable, Cook and Li [5] defined the central mean subspace ==== as the column space of ==== with the smallest column dimension such that ====. The column dimension of ====, denoted by ====, is called the structural dimension. When the data are fully observed and ==== is univariate (====), since the work of Li [12] who proposed the sliced inverse regression method, there have been many approaches developed to estimate ====. Specifically, Li [13] proposed the principal Hessian directions method to recover ==== through the eigenvectors. Discussions on the principal Hessian directions method were given by Cheng and Zhu [3], Cook [4] and Cook and Li [5], etc. Assuming that ==== is a sufficiently smooth function of ====, Xia et al. [22] suggested a minimum average variance estimation method to recover ====. Ma and Zhu [15], [17] proposed semiparametric methods to estimate ==== when either ==== or ==== is assumed to be smooth. When the data are fully observed and ==== is multivariate, Zhu and Zhong [28] proposed a profile least squares approach to perform estimation and inference on the central mean subspace.====In practice, missing data are a prevailing problem in any data analyses. A variable is considered missing if the value of the variable is not observed in an observation. In most analyses in the medical literature, the most common way of dealing with missing response data is to just omit those participants who have any missing data among its variables. Such an analysis is called a complete case (CC) analysis. The CC analysis is quite popular, because it is the default analysis for most standard statistical softwares. However, in many situations, the CC analysis is not an appropriate way to proceed and it may decrease the power of the analysis by decreasing the effective sample size. Data are said to be missing at random (MAR) if, given the observed data, the failure to observe a value does not depend on the data that are unobserved. For example, in cancer clinical trials, information on the size of a primary tumor is often missing, and the size of the primary tumor may depend on the type of the primary tumor, which is often fully observed. If the probability of primary tumor size being missing only depends on the type of primary tumor, then the missingness is considered to be MAR. Many approaches have been developed on how to handle missing values in multivariate analysis. For example, Liao et al. [14] investigated existing imputation approaches for phenomic data, proposed a novel “imputability” concept with a quantitative imputability measure to characterize whether a missing value is imputable or not, and proposed a self-training selection scheme to select the best imputation approach; Zhao and Long [25] investigated several approaches of using regularized regression and Bayesian lasso regression to impute the missing values for high-dimensional data; Wei et al. [21] developed a computationally efficient alternating expectation conditional maximization algorithm for parameter estimation of the generalized hyperbolic factor analyzers model in the presence of the missing values as well as heavy-tailed and/or asymmetric clusters. In regression analysis, when the response is univariate (====), some interesting problems have been investigated with the MAR setting. For example, Hu et al. [11] focused on the estimation of the marginal mean response when the response is MAR and covariates are available; Yang and Wang [24] developed a dimension-reduction based kernel imputation method for the sliced inverse regression; Deng and Wang [6] developed dimension reduction estimating methods for probability density with response MAR when covariables are present. Other related works can be found in Guo et al. [9], who proposed selection probability assisted recovery and complete case assisted recovery methods, and Zhu et al. [27], who proposed a parametric imputation procedure based on the sliced inverse regression.====In this article, we address the dimension reduction estimation for ==== when the responses ==== are MAR and the predictors ==== are present. Specifically, the random sample ==== for ==== of the incomplete data comes from ==== where all the ==== are observed, ====, and ==== if ==== is observed and ==== otherwise for ====. With incomplete observations, we consider the estimation of the central mean subspace ==== under MAR, which implies that ==== and ==== are conditionally independent given ====, or equivalently, for ====, ====
 where ==== is called a selection probability function. In general, to understand how the conditional mean functions of ==== vary with ====, we let ====, or equivalently, ====, where ====, ==== is a ==== matrix with an unknown ====. All ====, ==== and ==== have to be estimated based on the observed data, and all ==== share an identical ==== to ensure that the ==== is identifiable (see [28]).====The rest of this article is organized as follows. In Section 2, we propose a profile least squares method to estimate the central mean subspace based on the inverse probability weighted (IPW) approach, and establish asymptotic properties for the resultant estimators. We present the performance of the proposed methods via a comprehensive simulation study and a real data analysis in Section 3. The concluding remarks are given in Section 4. Some additional simulations and all technical proofs are deferred to the Appendix.",Dimension reduction estimation for central mean subspace with missing multivariate response,https://www.sciencedirect.com/science/article/pii/S0047259X19301605,29 August 2019,2019,Research Article,218.0
"Pergamenchtchikov Serguei,Tartakovsky Alexander G.","Laboratoire de Mathématiques Raphaël Salem, UMR 6085 CNRS-Université de Rouen Normandie, France,National Research Tomsk State University, International Laboratory of Statistics of Stochastic Processes and Quantitative Finance, Tomsk, Russia,Space Informatics Laboratory, Moscow Institute of Physics and Technology, Moscow, Russia,AGT StatConsult, Los Angeles, CA, USA","Received 27 March 2019, Revised 22 August 2019, Accepted 22 August 2019, Available online 28 August 2019, Version of Record 30 August 2019.",https://doi.org/10.1016/j.jmva.2019.104541,Cited by (12),"A weighted Shiryaev–Roberts change detection procedure is shown to approximately minimize the expected delay to detection as well as higher moments of the detection delay among all change-point detection procedures with the given low maximal local probability of a false alarm within a window of a fixed length in ==== for the log-likelihood ratios between the “change” and “no-change” hypotheses, and we also provide sufficient conditions for a large class of ergodic ====. Examples related to multivariate Markov models where these conditions hold are given.",None,Asymptotically optimal pointwise and minimax change-point detection for general stochastic models with a composite post-change hypothesis,https://www.sciencedirect.com/science/article/pii/S0047259X19301733,28 August 2019,2019,Research Article,219.0
"Falk Michael,Padoan Simone A.,Wisheckel Florian","Institute of Mathematics, University of Würzburg, Würzburg, Germany,Department of Decision Sciences, Bocconi University of Milan, Milano, Italy,Institute of Mathematics, University of Würzburg, Würzburg, Germany","Received 14 January 2019, Revised 16 August 2019, Accepted 16 August 2019, Available online 21 August 2019, Version of Record 28 August 2019.",https://doi.org/10.1016/j.jmva.2019.104538,Cited by (14),", called ====-norm. The characteristic property of a generalized Pareto copula is its exceedance stability.====They might help to end the following debate: What is a multivariate ====As an application we derive nonparametric estimates of the ==== that a random vector, which follows a generalized Pareto copula, exceeds a high threshold, together with confidence intervals. A case study on joint exceedance probabilities for air pollutants completes the paper.",None,Generalized Pareto copulas: A key to multivariate extremes,https://www.sciencedirect.com/science/article/pii/S0047259X19300296,21 August 2019,2019,Research Article,220.0
"Cousido-Rocha Marta,de Uña-Álvarez Jacobo,Hart Jeffrey D.","Department of Statistics and Operations Research & SiDOR Research Group, University of Vigo, Campus Lagoas-Marcosende, Vigo 36310, Spain,Centro de Investigaciones Biomédicas (CINBIO), University of Vigo, Campus Lagoas-Marcosende, Vigo 36310, Spain,Department of Statistics, Texas A&M University, College Station, TX 77843, USA","Received 23 January 2019, Revised 7 August 2019, Accepted 7 August 2019, Available online 12 August 2019, Version of Record 28 August 2019.",https://doi.org/10.1016/j.jmva.2019.104537,Cited by (7),", of variables but a small sample size. In this context our aim is to address the problem of testing the null hypothesis that the marginal distributions of ==== variables are the same for two groups. We propose a test statistic motivated by the simple idea of comparing, for each of the ","Nowadays the problem of dealing with high-dimensional data arises in many different areas of science, such as genetics, medicine, pharmacy and social science. The main property of high-dimensional data is that the data dimension ====, i.e., the number of variables or features (e.g. genes), is large while the sample size ==== is relatively small, i.e., ====. Classical methods may be inappropriate for analyzing such data, creating a need for new methods.====In the two-sample setting, the goal of microarray studies is to compare the distributions of gene expression levels for two groups of individuals, for example, groups corresponding to two types of tumors. One possible approach to address this issue is to compare the multivariate distribution of the gene expressions through both their mean vectors and covariance structures; see, e.g., [6]. An alternative approach that may entail a larger statistical power is to restrict attention to the marginal distributions of the gene expression levels. The purpose of this paper is indeed to provide a formal nonparametric test of the null hypothesis of equality of the ==== marginal distributions for the two groups being compared. When the null hypothesis is rejected, one concludes that some of the genes are not equally distributed in the two groups, thus revealing discriminatory information. Formally, the null hypothesis to be tested is the intersection of ==== null hypotheses corresponding to ==== different two-sample problems. We allow for a rather general form of dependence among the ==== variables of interest. In particular, we assume that the dependence between the ==== variables under consideration is strongly mixing [17].====As we mentioned previously, traditional methods may not work well in the high dimensional context, and so new statistical ideas have arisen to deal with this problem. For testing the equality of high dimensional mean vectors several contributions have been made, e.g., [3], [13], [27], and references therein. [3] and  [13] assume that both the sample size and dimension tend to infinity, whereas in the current paper only the dimension ==== tends to infinity, with the sample size fixed. [2], [30] and others have proposed methods for testing the equality of high dimensional covariance matrices. These methods are only suitable for location or scale problems, whereas the aim of the current paper is a test which can detect any type of differences between distributions. Recently, some advances on testing the equality of two joint distributions in the high dimensional framework have been made. [6] proposed a nonparametric two-sample test based on inter-point distances. Their null hypothesis asserts that the ====-variate distribution is the same for the two groups being compared, whereas the aim of our method is to test the equality of the univariate marginals of the ====-variate distributions in the two populations. In addition, our dependence structure on the observations is more general than the one considered in [6], who imposed independence between the individual multivariate vectors; see Section 4 in the Supplementary Material. Other related contributions are [7] and [35]. The first of the latter two articles proposed a multivariate two-sample test based on the shortest Hamiltonian path, while the second proposed a multivariate two-sample test based on nearest neighbor ideas. The null hypothesis and the structure of dependence considered in [7] and [35] match those of [6]. [32] proposed a jacknife empirical likelihood test for the equality of distributions, but the proposed test does not consider a dependence structure in the high-dimensional data sets. Furthermore they test the equality of the characteristic functions of the two groups at a given fixed point ====, and the limiting distribution of the test statistic is derived under the assumption that both the sample size and dimension tend to infinity. A more recent contribution is [42] in which a binary linear classifier is proposed as a means of detecting differences between two high-dimensional distributions. They proposed several binary linear classifiers, but their theoretical results are only developed for the centroid projection direction for normally distributed data.====Another approach to test the intersection of ==== null hypotheses corresponding to ==== different two-sample problems is to test each of the ==== hypotheses separately via a nonparametric test such as Kolmogorov–Smirnov (KS) or Cramér–von Mises (CvM). Then a multiple comparison procedure (MCP) may be applied to the ====-values corresponding to the individual tests to check the global null hypothesis. When one is interested in testing the intersection of the ==== null hypotheses (i.e., the global null hypothesis) and not in determining which of the ==== null hypotheses should be rejected, the higher-criticism method introduced by [41] and extended by [16] is a useful procedure. [11] propose a MCP based on the idea of higher criticism which is recommended when the aim is to test the global null hypothesis. However these procedures are developed for the setting where the ====-values are independent, which is not the case in the current paper. [23] consider higher criticism for the case of dependent data; they provide a critical value which depends on unknown quantities and, therefore, the practical application of their test involves estimation issues which are so far apparently unsolved. There are methods, however, in the MCP literature for dependent ====-values; see, e.g., [4] and [5].====The rest of the paper is organized as follows. In Section 2 we introduce the required notation and the proposed statistic to test the null hypothesis of equality of the two sets of marginal distributions. The proposed statistic is motivated by the simple idea of comparing, for each of the ==== variables, the empirical characteristic functions computed from the two samples. The asymptotic normality of the test statistic is established in Section 3. In this section we also introduce three consistent estimators of the variance of our proposed statistic. We investigate the power of our test under reasonable fixed alternatives in Section 4. In Section 5 we propose an alternative global test based on the ====-values derived from permutation tests. A simulation study to investigate the finite sample properties of the proposed methods is carried out in Section 6. Real data illustrations are given in Section 7, and in Section 8 we give the main conclusions of our research as well as some final discussion.====Our proposed test is implemented in the user-friendly ==== package [14] of the free software ==== which makes it easy to apply in practice.",A two-sample test for the equality of univariate marginal distributions for high-dimensional data,https://www.sciencedirect.com/science/article/pii/S0047259X19300521,12 August 2019,2019,Research Article,221.0
"Abramovich Felix,Pensky Marianna","Department of Statistics and Operations Research, Tel Aviv University, Tel Aviv 69978, Israel,Department of Mathematics, University of Central Florida, 4393 Andromeda Loop N Orlando, FL 32816, USA","Received 19 May 2019, Revised 24 July 2019, Accepted 24 July 2019, Available online 2 August 2019, Version of Record 21 August 2019.",https://doi.org/10.1016/j.jmva.2019.104536,Cited by (21),"The objective of the paper is to study accuracy of multi-class classification in high-dimensional setting, where the number of classes is also large (“large ====, large ====, small ====” model). While this problem arises in many practical applications and many techniques have been recently developed for its solution, to the best of our knowledge nobody provided a rigorous theoretical analysis of this important setup. The purpose of the present paper is to fill in this gap.====We consider one of the most common settings, classification of high-dimensional normal vectors where, unlike standard assumptions, the number of classes could be large. We derive non-asymptotic conditions on effects of significant features, and the low and the upper bounds for distances between classes required for successful feature selection and classification with a given accuracy. Furthermore, we study an asymptotic setup where the number of classes is diverging with the dimension of feature space and while the number of samples per class is possibly limited. We point out on an interesting and, at first glance, somewhat counter-intuitive phenomenon that a large number of classes may be a “blessing” rather than a “curse” since, in certain settings, the precision of classification can improve as the number of classes grows. This is due to more accurate feature selection since even weaker significant features, which are not sufficiently strong to be manifested in a coarse classification, being shared across the classes, have a stronger impact as the number of classes increases. We supplement our theoretical investigation by a simulation study and a real data example where we again observe the above phenomenon.","Classification has been studied in many contexts. In the modern era one is usually interested in classifying objects that are described by a large number of features and belong to many different groups. For example the large hand-labeled ImageNet data set ==== contains 10,000,000 labeled images depicting more than 10,000 object categories where each image, on the average, is represented by ==== pixels (see [22] for description and discussion of this data set). The challenge of handling large dimensional data got the name of “large ==== small ====” type of problems which means that dimensionality of parameter space ==== by far exceeds the sample size ====. It is well known that solving problems of this type requires rigorous model selection. In fact, the results of Bickel and Levina [2], Fan and Fan [11], Shao et al. [23] demonstrate that even for the standard case of two classes, classification of high-dimensional normal vectors without feature selection is as bad as just pure random guessing. However, while analysis of high-dimensional data has become ubiquitous, to the best of our knowledge, there are no theoretical studies that examine the effect of large number of classes on classification accuracy. The objective of the present paper is to fill in this gap.====At first glance, the problem of successful classification when the number of classes is large seems close to impossible. On the other hand, humans have no difficulty in distinguishing between thousands of objects, and the accuracy of state-of-the-art computer vision techniques is approaching human accuracy. In fact, in some settings, the accuracy of classification improves when the number of classes grows. How is this possible? One of the reasons why multi-class classification succeeds is that selection of appropriate features from a large sparse ====-dimensional vector becomes easier when the number of classes is growing since even weaker significant features that are not sufficiently strong to be manifested in a coarse classification with a small number of classes may nevertheless have a strong impact as the number of classes grows. Simulation studies in [7] and [21] support such a claim. Arias-Castro, Candès and Plan [1] reported on a similar occurrence for testing in the sparse ANOVA model. Our paper establishes a firm theoretical foundation under the above phenomenon and confirms it via simulation studies and a real data example.====Although there exists an enormous amount of literature on classification, most of the existing theoretical results have been obtained for the binary classification (====) (see [4] and references therein for a comprehensive survey). In particular, binary classification of high-dimensional sparse Gaussian vectors was considered in [2], [8], [9], [11], [17] and [23] among others.====In the meantime, a significant amount of effort has been spent on designing methods for the multi-class classification in statistical and machine learning literature. We can mention here techniques designed to adjust pairwise classification to multi-class setting [10], [14], [18], adjustment of the support vector machine technique to the case of several classes [5], [19] as well as a variety of approaches to expand the linear regression and the neural networks techniques to accommodate the multi-category setup (see, e.g., [13]). Pan, Wang and Li [20] and Tewari and Bartlett [24] generalized theoretical results for binary classification to the case of multi-class classification and established consistency of the proposed classification procedures. However, all above-mentioned investigations considered only the “small ====, large ====, small ====” setup, where the number of classes was assumed to be ====.====This paper is probably the first attempt to rigorously investigate “large ====, large ====, small ====” classification and the impact of the number of classes on the accuracy of feature selection and classification. In particular, we explore the somewhat counter-intuitive phenomenon, where the large number of classes may become a “blessing” rather than a “curse” for successful classification as more significant features may be revealed. For this purpose, we consider a well-known problem of multi-class classification of high-dimensional normal vectors. We assume that only a subset of truly significant features really contribute to separation between classes (sparsity). For this reason, we carry out feature selection and, following a standard scheme, assign the new observed vector to the closest class w.r.t. the scaled Mahalanobis distance in the space of the selected significant features. Our paper considers a realistic scenario where the number of classes as well as the number of features is large while the number of observations per class is possibly limited (“large ====, large ====, small ====” model). We do not fix the total number of observations since in the real world the experience of each new class comes with its own, usually finite, set of observations.====We start with a non-asymptotic setting and derive the conditions on effects of significant features, and the low and upper bounds for the distances between classes required for successful feature selection and classification with a given accuracy. All the results are obtained with the explicit constants and remain valid for any combination of parameters. Our finite sample study is followed by an asymptotic analysis for a large number of features ====, where, unlike previous works, the number of classes ==== may grow with ==== while the number of samples per class may grow or stay fixed. Our findings indicate that having larger number of classes aids the feature selection and, hence, can improve classification accuracy. On the other hand, larger number of classes require having larger number of significant features ==== for their separation which automatically leads to a “large ====” setting. Nevertheless, due to increasing point isolation in high-dimensional spaces (see, e.g., [12], Section 1.2.1), those separation conditions become attainable when ==== is large.====We ought to point out that our paper does not propose a novel methodology for feature selection or classification. Rather than that, it studies one of the most popular Gaussian setting and adapts to the case of a large number of classes a standard general scheme, where feature selection is implemented by a thresholding technique with the properly chosen threshold and classification is carried out on the basis of the minimal Mahalanobis distance (we consider both the known and the unknown covariance matrix scenarios). This is a common widely used general scheme for classification and feature selection in such setting (see, e.g., [11], [20] and [23] for similar approaches that differ mostly by selections of thresholds and distances). Nevertheless, the setup is simple enough for derivations of conditions required for successful classification with a specified precision when the number of classes is large. Therefore, in our simulation study we do not compare these simple and well known techniques with the state of the art classification methodologies but instead investigate how these popular procedures perform when ==== is large and both the number of classes ==== and the number of significant features ==== are growing. In particular, simulations support our finding that classification precision can improve when ==== is increasing. The real data example confirms that the phenomenon above is not due to an artificial construction and is possible in a real life setting.====The rest of the paper is organized as follows. In Section 2 we present the feature selection and multi-class classification procedures and derive the non-asymptotic bounds for their accuracy. An asymptotic analysis is considered in Section 3. Section 4 discusses adaptation of the procedure in the case of the unknown covariance matrix. In Section 5 we illustrate the performance of the proposed approach on simulated and real-data examples. Some concluding remarks are summarized in Section 6. All the proofs are given in Appendix.",Classification with many classes: Challenges and pluses,https://www.sciencedirect.com/science/article/pii/S0047259X19302763,2 August 2019,2019,Research Article,222.0
"Couillet Romain,Tiomoko Malik,Zozor Steeve,Moisan Eric","GIPSA-lab, University Grenoble Alpes, France,L2S, CentraleSupélec, University of Paris Saclay, France","Received 11 February 2019, Revised 27 June 2019, Accepted 27 June 2019, Available online 22 July 2019, Version of Record 25 July 2019.",https://doi.org/10.1016/j.jmva.2019.06.009,Cited by (0),"Given two sets ==== and ==== (or ==== and ==== (or ====), respectively, this article provides novel estimators for a wide range of distances between ==== and ==== (along with divergences between some zero mean and covariance ==== or ==== probability measures) of the form ==== (with ==== the eigenvalues of matrix ====). These estimators are derived using recent advances in the field of random matrix theory and are asymptotically consistent as ==== with non trivial ratios ==== and ==== (the case ==== is also discussed). A first “generic” estimator, valid for a large set of ==== functions, is provided under the form of a complex integral. Then, for a selected set of atomic functions ==== which can be linearly combined into elaborate distances of practical interest (namely, ====, ====, ==== and ====), a closed-form expression is provided. Besides theoretical findings, simulation results suggest an outstanding performance advantage for the proposed estimators when compared to the classical “plug-in” estimator ==== (with ====), and this even for very small values of ====. A concrete application to kernel spectral clustering of covariance classes supports this claim.","In a host of statistical signal processing and machine learning methods, distances between covariance matrices are regularly sought for. These are notably exploited to estimate centroids or distances between clusters of data vectors mostly distinguished through their second order statistics. We may non exhaustively cite brain graph signal processing and machine learning (from EEG datasets in particular) which is a field largely rooted in these approaches [12], hyperspectral and synthetic aperture radar (SAR) clustering [7], [25], patch-based image processing [15], etc. For random independent ====-dimensional real or complex data vectors ====, ====, having zero mean and covariance matrix ====, and for a distance (or divergence) ==== between covariance matrices ==== and ==== (or probability measures associated to random variables with these covariances), the natural approach is to estimate ==== through the “plug-in” substitute ====. For well-behaved functions ====, this generally happens to be a consistent estimator as ==== in the sense that ==== almost surely as ==== while ==== remains fixed. This is particularly the case for all subsequently introduced distances and divergences.====However, in many modern applications, one cannot afford large ====
 (====) values or, conversely, ==== may be commensurable, if not much larger, than the ====’s. When ==== in such a way that ==== remains away from zero and infinity, it has been well documented in the random matrix literature, starting with the seminal works of Marc̆enko and Pastur [17], that the operator norm ==== no longer vanishes. Consequently, this entails that the aforementioned estimator ==== for ==== is likely inconsistent as ==== at a commensurable rate.====This said, it is now interesting to note that many standard matrix distances ==== classically used in the literature can be written under the form of functionals of the eigenvalues of ==== (assuming at least ==== is invertible). A first important example is the squared Fisher distance between ==== and ==== [8] given by ====with ==== the Frobenius norm, ==== the nonnegative definite square root of ====, and ==== for symmetric ==== (and ==== the matrix transpose) in its spectral composition. This estimator arises from information geometry and corresponds to the length of the geodesic between ==== and ==== in the manifold of positive definite matrices.====Another example is the Bhattacharyya distance [6] between two real Gaussian distributions with zero mean and covariances ==== and ====, respectively (i.e., ==== and ====), which reads ====which can be rewritten under the form ====In a similar manner, the Kullback–Leibler divergence [5] of the Gaussian distribution ==== with respect to ==== is given by ====More generally, the Rényi divergence [4] of ==== with respect to ==== reads, for ====, ====(one can check that ====).====Revolving around recent advances in random matrix theory, this article provides a generic framework to consistently estimate such functionals of the eigenvalues of ==== from the samples in the regime where ==== are simultaneously large, under rather mild assumptions. In addition to the wide range of potential applications, as hinted at above, this novel estimator provides in practice a dramatic improvement over the conventional covariance matrix “plug-in” approach, as we subsequently demonstrate on a synthetic (but typical) example in Table 1. Here, for ==== (having the same eigenvalues as ====) a Toeplitz positive definite matrix, we estimate the (squared) Fisher distance ==== (averaged over a large number of realizations of zero mean Gaussian ====’s), for ====, ==== and ==== varying from ==== to ====. A surprising outcome, despite the theoretical request that ==== must be large for our estimator to be consistent, is that, already for ==== (while ====), our proposed estimator largely outperforms the classical approach; for ==== or less, the distinction in performance between both methods is dramatic with the classical estimator extremely biased. Our main result, Theorem 1, provides a consistent estimator for functionals ==== of the eigenvalues of ==== under the form of a complex integral, valid for all functions ==== that have natural complex analytic extensions on given bounded regions of ====. This estimator however assumes a complex integral form which we subsequently express explicitly for a family of elementary functions ==== in a series of corollaries (Corollaries 1 to 4). Distances and divergences of practical interest (including all metrics listed above) being linear combinations of these atomic functions, their resulting estimates follow from merely “linearly combining” these corollaries. Table 2 provides the mapping between distances and functions ====. While Corollaries 1–3 provide an exact calculus of the form provided in Theorem 1, for the case ====, covered in Corollary 4, the exact calculus leads to an expression involving dilogarithm functions which are not elementary functions. For this reason, Corollary 4 provides a large ==== approximation of Theorem 1, thereby leading to another (equally valid) consistent estimator. This explains why Table 1 and the figures to come (Fig. 1, Fig. 2) display two different sets of estimates. In passing, it is worth noticing that in the complex Gaussian case, the estimators offer a noticeable improvement on average over their real counterparts; this fact is likely due to a bias in the second-order fluctuations of the estimators, as discussed in Section 5.==== Technically speaking, our main result unfolds from a three-step approach: (i) relating the limiting (as ==== with ====) eigenvalue distribution of the sometimes called Fisher matrix ==== to the limiting eigenvalue distribution of ==== by means of a functional identity involving their respective Stieltjes transforms (see definition in Section 3), then (ii) expressing the studied matrix distance as a complex integral featuring the Stieltjes transform and proceeding to successive change of variables to exploit the functional identity of (i), and finally (iii) whenever possible, explicitly evaluating the complex integral through complex analysis techniques. This approach is particularly reminiscent of the eigenvalue and eigenvector projection estimates proposed by Mestre in 2008 in a series of seminal articles [18], [19]. In [18], Mestre considers a single sample covariance matrix ==== setting and provides a complex integration approach to estimate the individual eigenvalues as well as eigenvector projections of the population covariance matrix ====; there, step (i) follows immediately from the popular result [23] on the limiting eigenvalue distribution of large dimensional sample covariance matrices; step (ii) was then the (simple yet powerful) key innovation and step (iii) followed from a mere residue calculus. In a wireless communication-specific setting, the technique of Mestre was then extended in [11] in a model involving the product of Gram random matrices; there, as in the present work, step (i) unfolds from two successive applications of the results of [23], step (ii) follows essentially the same approach as in [18] (yet largely simplified) and step (iii) is again achieved from residue calculus. As the random matrix ==== may be seen as a single sample covariance matrix conditionally on ====, with ==== itself a sample covariance, in the present work, step (i) is obtained rather straightforwardly from applying twice the results from [23], [24]; the model ==== is actually reminiscent of the so-called multivariate ====-matrices, of the type ==== but with ====, extensively studied in [3], [22], [32]; more recently, the very model under study here (that is, for ====) was analyzed in [28], [31]. Step (i) of the present analysis is in particular consistent with Theorem 2.1 of [31], yet our proposed formulation is here more convenient to our purposes.==== The main technical difficulty of the present contribution lies in steps (ii) and (iii) of the analysis. First, as opposed to [11], [18], the complex integrals under consideration here involve rather non-smooth functions, and particularly complex logarithms. Contour integrals of complex logarithms can in general not be treated through mere residue calculus. Instead, we shall resort here to an in-depth analysis of the so-called branch-cuts, corresponding to the points of discontinuity of the complex logarithm, as well as to the conditions under which valid integration contours can be defined. Once integrals are properly defined, an elaborate contour design then turns the study of the complex integral into that of real integrals. As already mentioned, in the particular case of the function ====, these real integrals result in a series of expressions involving the so-called dilogarithm function (see e.g., [30]), the many properties of which will be thoroughly exploited to obtain our final results.====A second difficulty arises in the complex contour determination which involves two successive variable changes. Working backwards from a contour surrounding the sample eigenvalues to reach a contour surrounding the population eigenvalues, one must thoroughly examine the conditions under which the expected mapping is achieved. The scenarios where ==== or ==== notably lead to non-trivial contour changes. This particularly entails the impossibility to estimate some functionals of the eigenvalues of ==== under these conditions.====The remainder of the article presents our main result, that is the novel estimator, first under the form of a generic complex integral and then, for a set of functions met in the aforementioned classical matrix distances, under the form of a closed-form estimator. To support the anticipated strong practical advantages brought by our improved estimators, Section 4 concretely applies our findings in a statistical learning context.",Random matrix-improved estimation of covariance matrix distances,https://www.sciencedirect.com/science/article/pii/S0047259X1930082X,22 July 2019,2019,Research Article,223.0
"Chen Yichao,Pun Chi Seng","School of Physical and Mathematical Sciences, Nanyang Technological University, 637371, Singapore","Received 23 November 2018, Revised 17 July 2019, Accepted 17 July 2019, Available online 22 July 2019, Version of Record 25 July 2019.",https://doi.org/10.1016/j.jmva.2019.104535,Cited by (5),"In this study, we examine ","Functional time series (FTS) data emerge in many applications such as the time series of intra-day stock (index) price curves (see Shang [43]), age-specific medical data (see Erbas et al. [17]), intra-day environmental data (see Shang [44]), and intra-day electricity price curves (see Liebl [35]). Moreover, applications of FTS are not limited to the measurement frequency of curves and can be extended to any frequency of interest (e.g., monthly, yearly, or even hourly or minute-by-minute), which provides new insights for high-dimensional data analysis. Over the past two decades, the theoretical developments of functional (dependent) data analysis have become popular, as using the functional representation of the collated data provides a more flexible and precise way to investigate the relation between data on different measured periods; see Ramsay and Silverman [42] and Horváth and Kokoszka [25]. Early FTS models were functional autoregressive (AR) models; see Bosq [6] and Kargin and Onatski [28]. However, recent studies such as Aue et al. [1], Hörmann and Kokoszka [22], Hörmann and Kokoszka, Hörmann et al. [21], [20] have extended these to a non-linear dependence structure.====Consequently, hypothesis testing for the various assumptions of FTS models has received considerable research attention including Gabrys et al. [18] for error correlation, Horváth et al. [24] for stability,  Kokoszka and Reimherr [29] for the order of an autoregression, Horváth et al., Kokoszka and Young [27], [30] for stationarity, Constantinou et al. [11] for the separability of space–time domains, Aue et al. [2] for structural break detection, Górecki et al. [19] for normality, and Hörmann et al. [23] for periodicity, to name a few. Most such testing procedures rely on the limiting distribution of the corresponding test statistic, which requires a large sample size to validate the tests. However, while such an asymptotic approximation is common in the literature on functional testing, we may only be able to obtain relatively small samples in practice.====As a practical alternative, the bootstrap-based functional testing procedure has thus been introduced for functional data. Benko et al. [3] and Paparoditis and Sapatinas [37] propose two bootstrap tests for testing the equality of two sample mean functions using different test statistics. The simple bootstrap (SB) method adopted in these studies is initiated by Efron [15], Efron and Tibshirani [16]. In terms of statistical estimation, the SB method is applied to functional data by Cuevas et al. [12] and to stationary FTS by Paparoditis [36], Shang [45]. Pilavakis et al. [39] extend the use of bootstrap methods for independent functional data to weakly dependent functional data by applying the moving block bootstrap (MBB) method of Künsch [31]. In the bootstrap-based functional testing of FTS, the stationarity assumption plays an important role. Although the bootstrap unit root test is available in a multivariate setting (see Chang and Park [10], Chang [9]), the application of bootstrap methods to non-stationary FTS remains a problem.====We take a step toward overcoming this challenge by investigating the bootstrap methods used to test trend-stationary FTS, also known as the Kwiatkowski–Phillips–Schmidt–Shin (KPSS) test (see Kwiatkowski et al. [32]). The KPSS test, introduced as an alternative to the Dickey–Fuller unit root test, has been found to have better utility in econometrics; see Dickey and Fuller [13], [14], Lee and Schmidt [34]. Following the time-domain approach in Kwiatkowski et al. [32], Horváth et al. [27] and Kokoszka and Young [30] derive the limiting distributions of a generalized KPSS test statistic for FTS under the stationarity and trend-stationarity hypotheses, respectively. In this study, we bridge this research gap by applying bootstrap methods to the functional KPSS (FKPSS) test in the case of small or moderate sample sizes. We validate our proposed bootstrap FKPSS test theoretically and numerically through simulations with various alternative hypotheses. Our theoretical results shed light on the validity of bootstrap methods for non-stationary FTS, while our simulation results show that the bootstrap-based FKPSS test has good empirical sizes and powers in various settings.====The remainder of the paper is organized as follows. Section 2 states the problem and introduces the preliminary results of the FKPSS test. Section 3 proposes a bootstrap-based functional testing procedure and proves the bootstrap validity on average. The proof of the main result (Theorem 3.1) is documented in Appendix. Then, we present simulation studies in Section 4, in which we used the model parameters calibrated to a real dataset and compared the proposed bootstrap tests with the Monte Carlo test in Kokoszka and Young [30]. Finally, Section 5 concludes.",A bootstrap-based KPSS test for functional time series,https://www.sciencedirect.com/science/article/pii/S0047259X18306146,22 July 2019,2019,Research Article,224.0
"Bachoc François,Bevilacqua Moreno,Velandia Daira","Institut de Mathématiques de Toulouse, France,Departamento de Estadística, Universidad de Valparaíso, Millennium Nucleus Center for the Discovery of Structures in Complex Data, Chile,Departamento de Estadística, Universidad de Valparaíso, Chile","Received 19 July 2018, Revised 12 July 2019, Accepted 12 July 2019, Available online 18 July 2019, Version of Record 25 July 2019.",https://doi.org/10.1016/j.jmva.2019.104534,Cited by (10),None,"Gaussian processes are widely used in statistics to model spatial data. When fitting a Gaussian field, one has to deal with the issue of estimating its covariance function. In many cases, it is assumed that this function belongs to a given parametric model or family of covariance functions, which turns the problem into a parametric estimation problem. Within this framework, the maximum likelihood estimator (MLE) [45], [55] of the covariance parameters of a Gaussian stochastic process observed in ====, ====, has been deeply studied in the two following asymptotic frameworks.====The fixed-domain asymptotic framework, sometimes called infill asymptotics [16], [45], corresponds to the case where more and more data are observed in some fixed bounded sampling domain (usually a region of ====). The increasing-domain asymptotic framework corresponds to the case where the sampling domain increases with the number of observed data. Furthermore, the distance between any two sampling locations is bounded away from ====. The asymptotic behavior of the MLE of the covariance parameters can be quite different under these two frameworks [60].====Under increasing-domain asymptotics, for all covariance parameters, the MLE is consistent and asymptotically normal under some mild regularity conditions. The asymptotic covariance matrix is equal to the inverse of the Fisher information matrix [3], [33], [41].====The situation is significantly different under fixed-domain asymptotics. Indeed, two types of covariance parameters can be distinguished: microergodic and non-microergodic parameters. A covariance parameter is microergodic if two Gaussian measures constructed from different values of it are orthogonal, see [26], [45]. It is non-microergodic if two Gaussian measures constructed from different values of it are equivalent. Non-microergodic parameters cannot be estimated consistently, but misspecifying them asymptotically results in the same statistical inference as specifying them correctly [42], [43], [44], [59]. In the case of isotropic Matérn covariance functions with ====, [59] shows that only a reparametrized quantity obtained from the scale and variance parameters is microergodic. The asymptotic normality of the MLE of this microergodic parameter is then obtained in [30]. Similar results for the special case of the exponential covariance function were obtained previously in [57].====The maximum likelihood method is generally considered the best option for estimating the covariance parameters of a Gaussian process (at least in the framework of the present paper, where the true covariance function does belong to the parametric model, see also [2], [4]). Nevertheless, the evaluation of the likelihood function under the Gaussian assumption requires to solve a system of linear equations and to compute a determinant. For a data set of ==== observations, the computational burden is ====. This makes maximum likelihood computationally impractical for large data sets. This fact motivates the search for estimation methods with a good balance between computational complexity and statistical efficiency. Among these methods, we can mention low rank approximation (see [46] and the references therein for a review), sparse approximation [24], covariance tapering [21], [29], Gaussian Markov random fields approximation [17], [38], submodel aggregation [12], [18], [25], [39], [48], [50] and composite likelihood (CL).====CL indicates a general class of objective functions based on the likelihood of marginal or conditional events [52]. This kind of estimation method has two important benefits. First, it is generally appealing when dealing with large data sets. Second, it can be helpful when it is difficult to specify the full likelihood. In this paper we focus on a specific type of CL called pairwise likelihood. Examples of the use of pairwise likelihood in the Gaussian and non Gaussian case can be found in [9], [10], [20], [22], [23] to name a few.====The increasing-domain asymptotic properties of various weighted pairwise likelihood estimators in the Gaussian case can be found in [9]. Under this setting, for all covariance parameters, these pairwise likelihood estimators are consistent and asymptotically normal under some mild regularity conditions.====There are no results under fixed-domain asymptotics for CL to the best of our knowledge. In this paper we study the fixed-domain asymptotic properties of the pairwise likelihood estimation method. A one-dimensional Gaussian process with exponential covariance model is considered. This covariance model is particularly amenable to theoretical analysis of the MLE. Indeed, its Markovian property yields an explicit (matrix-free) expression of the likelihood function [57]. Thus, the MLE for this model has been studied in [14], [15], [57], and also in higher dimension in [1], [51], [58].====Under the parametric model given by the one-dimensional exponential covariance function, the microergodic parameter is the product of the variance and the scale (see [57] and Section 2). Under the same setting as in [57], we study the consistency and asymptotic normality of the weighted pairwise maximum likelihood estimator (WPMLE) and of the weighted pairwise conditional maximum likelihood estimator (WPCMLE) of the microergodic parameter. Our results show that the WPMLE is inconsistent if the range of admissible values for the variance parameter is too large compared to that for the scale parameter (Theorem 1, (i) and (iii)). Conversely, they show that the WPMLE is consistent if the range of admissible values for the variance parameter is restricted enough compared to that for the scale parameter (Theorem 1, (ii) and (iv)). In contrasts, we prove that the WPCMLE is consistent regardless of the ranges of admissible values for the scale and variance (Theorem 2). Both estimators are also asymptotically Gaussian in the cases where they are consistent (Theorem 3). The asymptotic variance of these estimators is no smaller than that of the MLE. Furthermore, it is strictly larger for equispaced observation points, when non-zero weights are given to pairs of non successive observation points.====The proofs of the asymptotic results rely on Lemma 1. This lemma expresses the weighted pairwise (conditional) likelihood criteria as combinations of full likelihood criteria for subsets of the observations. This enables to exploit the asymptotic approximations obtained by [57] for the full likelihood. The consistency results are then obtained similarly as in [57]. The inconsistency results are obtained by a careful analysis of the limit weighted likelihood criterion, together with results on extrema of Gaussian processes (see the proof of Theorem 1). Finally, the asymptotic normality results are obtained by means of a central limit theorem for weakly dependent variables, together with the use of Lemma 1 and of the approximations in [57].====It turns out that a suitable choice of the symmetric weights used in the pairwise likelihood is crucial for improving the asymptotic statistical efficiency of the method. In particular, the choice of compactly supported weights with a specific cut-off generally decreases the asymptotic variance. As a special case, if the pairwise likelihood is constructed only from adjacent pairs, then the asymptotic efficiency of the MLE is attained.====These results are consistent with the increasing-domain setting. Indeed, it has been shown, via simulation, that weighting schemes downweighting observations that are far apart in time and/or in space, are preferable to the pairwise likelihood involving all possible pairs [9], [27], [53]. A simulation study compares the finite sample properties of the weighted pairwise likelihood estimators with their asymptotic distributions, using the MLE as a benchmark. Finally, we provide an additional inconsistency result for the WPMLE of a variance parameter, when the correlation function is known and allowed to be significantly more general than the exponential one.====The paper falls into the following parts. In Section 2, pairwise (conditional) likelihood estimation is introduced. In Section 3, the consistency and inconsistency results are provided. In Section 4, the asymptotic normality results are provided, together with the analysis of the asymptotic variance. In Section 5, the numerical results are discussed. Concluding remarks are given in Section 6. The proofs are postponed to Appendix A. The more general inconsistency result of the WPMLE is provided in Appendix B.",Composite likelihood estimation for a Gaussian process under fixed domain asymptotics,https://www.sciencedirect.com/science/article/pii/S0047259X18303841,18 July 2019,2019,Research Article,225.0
"Mahdiyeh Zahra,Kazemi Iraj","Department of Statistics, University of Isfahan, Isfahan 81746, Iran","Received 6 June 2018, Revised 11 July 2019, Accepted 11 July 2019, Available online 16 July 2019, Version of Record 26 July 2019.",https://doi.org/10.1016/j.jmva.2019.104533,Cited by (0),This paper presents an attractive extension of multivariate mixed-effects models to allow the modeling of correlated responses. By initiating a new multivariate ,"An important topic in statistical modeling is devoted to longitudinal data wherein a set of subjects are repeatedly measured on specified conditions or periods. In the analysis of longitudinal data, any working model should allow measurements of the same response for each subject to be correlated, while observations from different subjects are independent. Furthermore, the correlation between measurements of different response variables should be considered for each subject. This is often performed by using joint regression models with postulating random effects as shared-parameter. A key disadvantage of shared parameter models is that they often involve very strong, sometimes unrealistic, assumptions about the association between multiple responses. This restriction can be relaxed by amending a separate mixed-effects model [20] for each response and combining them by allowing the random effects of assorted responses to be correlated [11].====A routine assumption in mixed-effects modeling is the normality of random effects which makes the individual response vector follow a multivariate normal. This conventional assumption can often be violated [35] mainly when certain latent sub-populations exist in the data generating process [29], or, when some important categorical covariates are omitted from the fixed part of the model. Hence, the implementation of suitable joint mixed-effects models is challenged. In this case, to avoid misleading inference and to guarantee its robustness, a model-based clustering is typically suggested. Consequently, the collected measurements can be classified based on the adoption of a multimodal distribution for random effects. A simple idea to set up a multimodal structure is according to fitting a mixture of multiple unimodal distributions. Several cases of mixture distributions have been proposed in the literature for certain purposes. In particular, the finite mixture distribution with normal components is widely used [1], [31]. The application of normal mixtures as the random-effects distribution in the linear mixed-effects (LME) models was proposed by [32] and described further by [33] and [34].====As is documented, any continuous distribution may be well approximated by a finite mixture distribution [19]. However, in practical applications, there are several issues on using mixture distributions in fitting multivariate LME models. One is the identifiability issue [14] as the unstructured form of covariance matrices generates a large number of unknown parameters. It can make the use of basic estimation procedures complicated. Any mixture distribution also needs convincing prior information on the choice of true number of components to perform effectively. Moreover, fixing the number of components to avoid overfitting and the normality assumption of components about each cluster are rather strong. It requires setting up some extra prior information.====These typical challenges motivated us to investigate alternative techniques. Consequently, we introduce a new family of multivariate multimodal distributions which offers great flexibility in jointly modeling of responses with the multimodal structure. This is originated from a mixing strategy addressed in the univariate case by [15] together with our initiation of a multimodal extension of the multivariate normal distribution (see also [12], [16], [23]). While a mixture distribution tends to impose additional components and parameters to capture more peaks, our proposed multivariate multimodal normal ==== distribution is able to cover most of the peaks through a limited number of parameters, without any requirement of prior information.====To model joint responses with multimodal structures, we let random effects follow the ==== distribution. It is shown that both joint and marginal distributions of responses belong to the class of multimodal distributions. Several main advantages of this model include (i) presenting great flexibility to model correlations which often exist within subjects and between multiple responses, (ii) covering various types of responses with multimodal and asymmetric behaviors, (iii) acknowledging for highly unbalanced data, (iv) being available to model more than two responses, (v) being simplifying the interpretation of parameter estimates, (vi) non-increasing the dimension of integration in the marginal distribution of response vector and non-involvement the additional computational complexity, (vii) being useful of its stochastic hierarchical representation for implementing easy estimation processes, (viii) dealing with the effect of hidden sub-populations with different behavior in terms of peaks that cannot be directly observed through the value of responses, (ix) being effective when a knowledge discovery of clusters is not available or choosing the number of clusters is not visible, (x) avoiding misleading inference when the classification of responses occurs due to the omission of important categorical covariates and violation of the normality assumption of error terms or random effects. To make the inference of model parameters using the maximum likelihood approach, the corresponding likelihood function appears in a non-closed form of known distributions, due to complicated integrals. Consequently, some advanced numerical integration techniques, stochastic methods or analytically approximations must be implemented. In this paper, we utilize two estimation approaches that are commonly used in practice. First, we focus on the Expectation–Conditional–Maximization (ECM) algorithm [8], [25]. Here, as is underlined in the related contexts, the unobserved random effects are treated as missing values which makes the estimation process highly computationally intensive. However, in construction of the E-step, deriving some associated conditional expectations of unobserved quantities, given the observed data, requires numerical methods to approximate such terms.====A general Bayesian computational technique that is particularly designed for complex models that include the high-dimensional integrals, is the Markov chain Monte Carlo (MCMC) approach. It has been developed extensively in situations where the evaluation of the marginal posterior likelihood is computationally expensive. Specifically, we include data augmentation strategies into the MCMC scheme to contribute random components in the joint likelihood function along with gaining our proposed stochastic hierarchical representation for the ==== distribution to provide efficient MCMC. Then, a combination of the Gibbs sampling and Metropolis–Hastings algorithms is used to facilitate the generation of samples from specified full conditional posterior distributions of all unknown quantities. With these specifications, underlying multivariate mixed-effects models with the ==== distribution can be readily fitted in the freely available software packages, such as ==== ====
 [21] , or ==== in ==== ====
 [9], [27]. Consequently, Bayesian inference of parameters is performed using summarized MCMC outputs.====The rest of the article is organized as follows. In Section 2, we introduce univariate and multivariate multimodal normal distribution and report their main properties. Section 3 presents in short, the specification of multivariate mixed effects models. We also extend modeling strategies for analyzing multivariate multimodal responses. Section 4 demonstrates how the parameters estimation process can be performed using Bayesian computing techniques. Section 5 conducts a simulation study to evaluate the performance of our proposed model. Finally, Section 6 is devoted to highlighting the usefulness of our methodological findings in real-life data analyses of the low-back pain measurements and the height of school-girls magnitudes.",An innovative strategy on the construction of multivariate multimodal linear mixed-effects models,https://www.sciencedirect.com/science/article/pii/S0047259X18302938,16 July 2019,2019,Research Article,226.0
"Lund Adam,Hansen Niels Richard","Københavns Universitet, Institut for Matematiske Fag, Universitetsparken 5, 2100 København Ø, Denmark","Received 22 October 2018, Revised 10 July 2019, Accepted 10 July 2019, Available online 16 July 2019, Version of Record 23 July 2019.",https://doi.org/10.1016/j.jmva.2019.104532,Cited by (0), package ==== available from CRAN.,"Neural field models are models of aggregated membrane voltage of a large and spatially distributed population of neurons. The neuronal network is determined by spatio-temporal synaptic weight functions in the neural field model, and we will refer to these weight functions as the ====. This network determines how signals are propagated hence it is of great interest to learn the propagation network from experimental data, which is the inverse problem for neural field models.====The literature on neural fields is vast and we will not attempt a review, but see [2], [8] and the references therein. The typical neural field model considered is a deterministic integrodifferential equation. The inverse problem for the deterministic Amari equation was treated in [18] and [28], and stochastic neural field models were, for instance, treated in Chapter 9 in [8] and in [16]. One main contribution of the latter paper, [16], was to treat a stochastic version of the Amari equation in the well developed theoretical framework of functional stochastic evolution equations.====Despite the substantial literature on neural fields, relatively few papers have dealt directly with the estimation of neural field components from experimental data. Pinotsis et al. [27] demonstrated how a neural field model can be used as the generative model within the dynamical causal modeling framework, where model parameters can be estimated from electrophysiological data. The modeling of voltage sensitive dye (VSD) imaging data in terms of neural fields was discussed in [7], and Markounikau et al. [25] estimated parameters in a neural field model directly from VSD data.====In this paper, VSD imaging data is considered as well. This ==== imaging technique has a sufficiently high resolution in time and space to detect propagation of changes in membrane potential on a mesoscopic scale, see [29]. A prevalent notion in the neuroscience literature is that the network connecting the neurons, and through which brain signals are propagated, is sparse, and that the propagation exhibits a time delay. If a spiking neuron, for instance, only affects neurons via synaptic connections to a very localized region the network is spatially sparse, while connections to remote regions result in temporal sparsity and long range dependence, see, e.g., [3], [4], [31], [32], [34]. Finally the possibility of feedback waves in the brain, e.g., as suggested in [29], could also be explained by spatio-temporal dynamics depending on more than just the instantaneous past. These considerations lead us to suggest a class of stochastic neural field models that allows for time delay, and a proposed estimation methodology that provides sparse nonparametric estimation of synaptic weight functions. Thus we do not make assumptions about spatial homogeneity or isotropy of the functional connectivity, nor do we assume that the signal propagation is instantaneous.====In order to derive a statistical model that, from a computational viewpoint, is feasible for realistically sized data sets, a time and space discretized version of the infinite dimensional dynamical model is obtained by replacing the various integrals with Riemann–Itô type summations and relying on an Euler scheme approximation. This approximation scheme makes it possible to derive a statistical model with an associated likelihood function such that regularized maximum-likelihood estimation becomes computationally tractable. Especially, we show that by expanding each component function in a tensor product basis we can formulate the statistical model as a type of multi-component linear array model, see [23].====The paper is organized as follows. First we give a brief technical introduction to the stochastic dynamical model that formsthe basis for the paper. Then we present the aggregated results from the application of our proposed estimation methodology to part of a VSD imaging data set. The remaining part of the paper presents the derivation of the linear array model and the key computational techniques required for the actual estimation of the model using array data. The appendix contains further technical proofs, a meta algorithm and implementation details. Finally to further illustrate our results we also provide a Shiny app available at ==== as well as supplementary material, [22], containing results from fitting the model to individual trials and the aggregated result for the entire data set.",Sparse network estimation for dynamical spatio-temporal array models,https://www.sciencedirect.com/science/article/pii/S0047259X18305554,16 July 2019,2019,Research Article,227.0
"Yang Xinfeng,Yan Xiaodong,Huang Jian","School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai, 200433, PR China,School of Economics, Shandong University, Jinan 250100, PR China,Department of Statistics and Actuarial Science, University of Iowa, Iowa City, IA 52242, USA","Received 2 October 2018, Revised 21 June 2019, Accepted 21 June 2019, Available online 5 July 2019, Version of Record 10 July 2019.",https://doi.org/10.1016/j.jmva.2019.06.007,Cited by (12),This paper studies integrative analysis of multiple units in the context of high-dimensional linear regression. We consider the case where a fraction of the ,"Multiple large datasets are typically gathered over different time periods, at multiple locations using possibly different data-collection methods. It is not reasonable to assume that the coefficients of the covariates are the same for all the units, especially for high-dimensional datasets. Some covariates may have different effects on the response across various units. Ignoring such heterogeneity can lead to bias and incorrect conclusions. In this study, our major interest is about how to efficiently identify the heterogeneous covariates, homogeneous covariates, and recover the sparsity, the grouping structures of the heterogeneous covariates, and estimate the coefficients of the heterogeneous and homogeneous covariates.====A flurry of recent researches focused on homogeneous statistical modeling for massive data and took advantage of computational algorithm. For example, divide and conquer algorithm [14], Bag of Little Bootstraps [10], sequential updating approaches [19], subsampled average mixture approach [29], bootstrap Metropolis–Hastings algorithm [15], sparse linear regression modeling in the distributed setting [4], distributed inference for quantile regression processes [23].====However, the results of homogeneous statistical modeling due to underlying differences among multiple data units may be misleading. Emerging literatures focused on heterogeneity analysis with low dimension. Bonhomme and Manresa [2] and Ando and Bai [1] used ====-means to estimate the grouping structure of intercepts and slopes. Shen and He [20] considered a structured logistic–normal mixture model by quantifying the subgroup membership with logistic regression. Zhao et al. [30] considered a partially linear framework for modeling massive heterogeneous data. Some of the other major themes for heterogeneous data can be found in work on mixture models [7], [22], varying coefficient models [6], [8], change-point models [12], [13]. Furthermore, Ma and Huang [16] proposed a pairwise fusion penalized approach for automatically identifying subgroup structures as well as estimating subgroup-specific intercepts. Ma et al. [17] adopted the automatic fusion approach to conduct subgroup analysis and estimate subgroup-specific treatment effects. Zhu et al. [31] and Jeon et al. [9] simultaneously employed sparse-induced penalty and pairwise fusion method to conduct the group pursuit of coefficients, where they pursued the homogeneous pattern of coefficients across covariates, not observations.====However, integrative analysis of multiple high-dimensional units with heterogeneity, homogeneity and sparsity has not been systematically studied. We propose a new method through penalizing the difference between any two of the coefficients of the same covariate, which can identify heterogeneous and homogeneous coefficients, combined with commonly-used sparsity structure penalty to select the relevant covariates. To implement the proposed approach, we devise an alternating direction method of multipliers algorithm (ADMM, [3]) with special attention to effective matrix inversion which can be more computationally efficient and effectively utilize the storage and reading of the datasets. By using a data-driven procedure for determining the penalty parameters, the method is able to automatically recover heterogeneity, homogeneity and sparsity. Under appropriate conditions, we show that the sequence generated by the proposed procedure has a subsequence which converges to the stationary point, and the oracle least squares estimator with ==== knowledge of the true grouping structure and sparsity structure is a local minimizer of the proposed objective function with probability tending to one.====The rest of this paper is organized as follows. Section 2 describes the unified linear model under heterogeneity, homogeneity and sparsity, and the penalized least squares objective function with double penalization method. To compute the penalized least squares estimator, we propose an ADMM algorithm in Section 3. The theoretical properties of the resulting estimator are established in Section 4. The finite-sample properties of the proposed method are evaluated through simulation studies in Section 5 and our method is illustrated through a real data example in Section 6. Concluding remarks are given in Section 7, and the technical proofs are provided in the Appendix.",High-dimensional integrative analysis with homogeneity and sparsity recovery,https://www.sciencedirect.com/science/article/pii/S0047259X18305165,5 July 2019,2019,Research Article,228.0
"Wang Yining,Wang Jialei,Balakrishnan Sivaraman,Singh Aarti","Machine Learning Department, School of Computer Science, Carnegie Mellon University, the United States of America,Department of ISOM, Warrington College of Business, University of Florida, the United States of America,Department of Statistics and Data Science, Carnegie Mellon University, the United States of America,Department of Computer Science, University of Chicago, the United States of America","Received 11 August 2018, Revised 22 June 2019, Accepted 22 June 2019, Available online 5 July 2019, Version of Record 22 July 2019.",https://doi.org/10.1016/j.jmva.2019.06.004,Cited by (8),We consider the problems of estimation and of constructing component-wise confidence intervals in a sparse high-dimensional linear regression model when some ,"High-dimensional statistics concerns the setting where the dimension of the statistical model is comparable to, or even far exceeds, the sample-size. In this context, meaningful statistical estimation is impossible in the absence of additional structure. Accordingly, significant research in high-dimensional statistics (see for instance [9], [15], [16], [17], [33]) has focused on high-dimensional linear regression with sparsity constraints where the goal is to estimate or perform inference on a sparse, high-dimensional vector ==== given access to noisy linear measurements.====Modern datasets are frequently afflicted with missing-values and corruptions. As a canonical example consider the gene-expression dataset from [28]. This dataset records ==== genes for ==== patients with soft tissue tumors. A total of 6.7% entries are missing; furthermore, 78.6% of the 5520 genes and all of the 46 patients have at least one missing covariate. Motivated by the analysis of corrupted high-dimensional datasets several researchers have considered settings with corrupted covariates: focusing on developing high-dimensional analogues of the classical Expectation–Maximization (EM) algorithm [32], studying their algorithmic convergence properties [2], [35], [36], and understanding statistical rates of convergence for other estimators [5], [13], [23], [24], [25], [30], [31].====Despite extensive past work, several challenging and important open questions remain in establishing the correct dependence of the rates of convergence in missing data problems on model parameters (the sparsity level ====, the expected fraction of unobserved covariates ====, and the signal strength ====). Understanding these dependencies for the problems of high-dimensional estimation and inference are the focus of this work.",Rate optimal estimation and confidence intervals for high-dimensional regression with missing covariates,https://www.sciencedirect.com/science/article/pii/S0047259X18304238,5 July 2019,2019,Research Article,229.0
"Arbel Julyan,Crispino Marta,Girard Stéphane","Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France","Received 21 December 2018, Revised 26 June 2019, Accepted 26 June 2019, Available online 5 July 2019, Version of Record 16 July 2019.",https://doi.org/10.1016/j.jmva.2019.06.008,Cited by (6),"We study a broad class of asymmetric ==== insures uniform margins, thus relaxing a constraint of Liebscher’s original construction. Besides, we show that this ==== proves useful for inference by developing an Approximate Bayesian computation sampling scheme. This inferential procedure is demonstrated on simulated data and is compared to a likelihood-based approach in a setting where the latter is available.","Let ==== be a continuous random vector with ====-variate cumulative distribution function (cdf) ====, and let ====, ====, be the marginal cdf of ====. According to Sklar’s theorem [36], there exists a unique ====-variate function ==== such that ====The function ==== is referred to as the copula associated with ====. It is the ====-dimensional cdf of the random vector ==== with uniform margins on ====.====A copula is said to be symmetric (or exchangeable) if for any ====, and for any permutation ==== of the first ==== integers ====, it holds that ====. The assumption of exchangeability may be unrealistic in many domains, including quantitative risk management [7], reliability modeling [39], and oceanography [41]. The urge for asymmetric copula models in order to better account for complex dependence structures has recently stimulated research in several directions, including Rodrıguez-Lallena and Úbeda-Flores [32], Alfonsi and Brigo [1], Durante [8], Wu [39] and Durante et al. [9]. We focus here on a simple yet general method for building asymmetric copulas introduced by Liebscher [22, Theorem 2.1, Property (i)]:====Theorem 1 provides a generic way to construct an asymmetric copula ====, henceforth referred to as ====, starting from a sequence of symmetric copulas ====. This mechanism was first introduced by [19] in the particular case where ==== and with the functions ==== assumed to be power functions, for each ==== and ====, that satisfy condition (2). The class of Liebscher copulas covers a broad range of dependencies and benefits from tractable bounds on dependence coefficients of the bivariate marginals [22], [23], [27]. However, there are two main reasons why the practical implementation of this approach is not straightforward: (i) it is not immediate to construct functions that satisfy condition (2); and (ii) the product form complicates the density computation even numerically, which makes it difficult to perform likelihood inference on the model parameters [27].====The aim of this paper is to deepen the understanding of Liebscher’s construction in order to overcome drawbacks (i) and (ii). Our contributions in this regard are three-fold. First, we provide theoretical properties of the asymmetric copulas in (1), including exact expressions of tail dependence indices, thus complementing the partial results of Liebscher [22], [23]. Second, we give an iterative representation of (1) which has the advantage to relax assumption (2) by automatically satisfying it. Third, we develop an inferential procedure and a sampling scheme that rely on the newly developed iterative representation.====The Bayesian paradigm proves very useful for inference in our context as it overcomes the problematic computation of the maximum likelihood estimate, which requires the maximization of a very complicated likelihood function (see recent contributions [29], [38]). General Bayesian sampling solutions in the form of Markov chain Monte Carlo are not particularly well-suited neither since they require the evaluation of that complex likelihood. Instead, we resort to Approximate Bayesian computation (ABC), a technique dedicated to models with complicated, or intractable, likelihoods (see [18], [25], [31] for recent reviews). ABC requires the ability to sample from the model, which is straightforward with our iterative representation of Liebscher copula. The adequacy of ABC for inference in copula models was leveraged by Grazian and Liseo [13], although in the different context of empirical likelihood estimation. A reversed approach to ours is followed by Li et al. [21], who make use of copulas in order to adapt ABC to high-dimensional settings.====Since its introduction, the construction by Liebscher has received much attention in the copula literature (e.g., [10], [20], [34]). However, most studies have been limited to simple cases where the product in (1) has only two terms. We hope that our paper will contribute to the further spreading of Liebscher’s copulas, because it allows to exploit their full potential by: (i) better understanding their properties; (ii) providing a novel construction, which facilitates their use with an arbitrary number ==== of terms in (1); and (iii) giving a strategy to make inference on them.====On top of what has been presented above, an additional contribution of this paper is to derive specific results for the subclass of Liebscher’s copula when two or more comonotonic copulas are combined, which we call comonotonic-based Liebscher copula. This subclass is characterized by an arbitrary number of singular components. To the best of our knowledge, this is the first paper to investigate this copula’s properties and to provide an inference procedure.====The paper is organized as follows. Section 2 provides some theoretical results concerning the properties of asymmetric Liebscher copulas, also presenting the novel iterative construction. In Section 3, we introduce and analyze the comonotonic-based Liebscher copula. Section 4 is dedicated to the inference strategy. It demonstrates our approach on simulated data and provides a comparison with a likelihood-based approach for a class of Liebscher copulas where maximum likelihood estimation is feasible. We conclude with a short discussion in Section 5. Proofs are postponed to the Appendix.",Dependence properties and Bayesian inference for asymmetric multivariate copulas,https://www.sciencedirect.com/science/article/pii/S0047259X18306547,5 July 2019,2019,Research Article,230.0
"Dharmawansa Prathapasinghe,Nadler Boaz,Shwartz Ofer","Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka,Department of Computer Science, Weizmann Institute of Science, Rehovot, 76100, Israel","Received 3 June 2018, Revised 19 March 2019, Accepted 20 May 2019, Available online 2 July 2019, Version of Record 9 July 2019.",https://doi.org/10.1016/j.jmva.2019.05.009,Cited by (12),"The largest eigenvalue of a single or a double Wishart matrix, both known as Roy’s largest root, plays an important role in a variety of applications. Recently, via a small noise perturbation approach with fixed dimension and degrees of freedom, Johnstone and Nadler derived simple yet accurate approximations to its distribution in the real valued case, under a rank-one alternative. In this paper, we extend their results to the complex valued case for five common single matrix and double matrix settings. In addition, we study the finite sample distribution of the leading ====. We present the utility of our results in several signal detection and communication applications, and illustrate their accuracy via simulations.","Wishart matrices, both real and complex valued, play a central role in statistics, with numerous engineering applications, specifically signal processing and communications. Of particular interest are the roots of a single Wishart matrix ====, and of a double Wishart matrix ====, with ==== and ==== independent [1]. The latter can be viewed as the multivariate analogue of the univariate ==== distribution and is also closely related to the multivariate beta distribution [32, Section 3.3]. Here we consider the largest eigenvalue ==== of either the matrix ==== or the matrix ====, a test statistic proposed by Roy [38], [39], known as Roy’s largest root [32, Section 10.6]. Specifically, we focus on the complex-valued case where ==== are independent complex-valued Wishart matrices. Throughout this paper, we consider ==== matrices, where ==== follows a complex valued central Wishart distribution with ==== degrees of freedom and identity covariance matrix ====, denoted ====. The distribution of the matrix ==== will either be central ====, or non-central ====. For the definition of central and non-central complex valued Wishart matrices, see for example [15] and [19, Section 8].====Obtaining simple expressions, exact or approximate, for the distribution of this top eigenvalue, denoted by ====, in the single or double matrix case has been a subject of intense research for more than 50 years. Khatri [27] derived an exact expression for the distribution of ==== in the single central matrix case with an identity covariance matrix (====). His result was generalized to several other settings, such as an arbitrary covariance matrix or a non-centrality matrix [24], [28], [36], [37], [41]. The resulting expressions are, in general, challenging to evaluate numerically. More recently,Zanella et al. [44] derived simpler exact, yet recursive expressions, both for the central case with arbitrary ==== and for the non-central case but with ====. Alternative recursive formulas in the real-valued case and in the complex-valued case were derived by Chiani [4], [5], [6].====A different approach to derive approximate distributions for the largest eigenvalue when ====, is based on random matrix theory. Considering the limit as ==== and ==== (and in the double matrix case also ====) tend to infinity, with their ratios converging to constants, ==== in the single matrix case and ==== in the double matrix case, asymptotically follow a Tracy–Widom distribution [20], [21], [22]. Furthermore, with suitable centering and scaling, the convergence to these limiting distributions is quite fast [10], [31].====In this paper, motivated by statistical signal detection and communication applications, we consider complex valued Wishart matrices ==== whose population covariance is a rank-one perturbation of a base covariance matrix. Specifically, in the central case we assume ====, where ==== is a measure of signal strength, the unit norm vector ==== is its direction, and ==== denotes the conjugate transpose of ====. Similarly, in the non-central case, we assume that ====, with a rank-one non-centrality matrix ====. Our goal is to study the distribution of ==== and its dependence on ====, which as discussed below is a central quantity of interest in various applications. A classical result in the single-matrix case is that with dimension ==== fixed, as ====, the largest eigenvalue of ==== converges to a Gaussian distribution [1]. In the random matrix setting, as both ==== and ==== tend to infinity with their ratio tending to a constant, Baik et al. [3] and Paul [34] proved that if ==== then ==== still converges to a Gaussian distribution, but with a different variance. In the two-matrix case, the location of the phase transition and the limiting value of the largest eigenvalue of ==== were recently studied by Nadakuditi and Silverstein [33]. Dharmawansa et al. [8] proved that above the phase transition, ==== converges to a Gaussian distribution and provided an explicit expression for its asymptotic variance.====Whereas the above results assume that dimension and degrees of freedom tend to infinity, in various common applications these quantities are relatively small. In such settings, the above mentioned asymptotic results may provide a poor approximation to the distribution of the largest eigenvalue ====, which can be quite far from Gaussian, see Fig. 1 (left) for an illustrative example. Accurate expressions for the distribution of ====, for small dimension and degrees of freedom, were recently derived for single and double real-valued Wishart matrices by Johnstone and Nadler [23], via a small noise perturbation approach. In this paper, we build upon their work and extend their results to the ==== and to the study of the distribution of the leading sample eigenvector, not considered in their work. As discussed below, both are important quantities in various applications.====Proposition 1, Proposition 2, Proposition 3, Proposition 4, Proposition 5 in Section 2 provide approximate expressions for the distribution of ==== under the five single-matrix and double-matrix cases outlined in Table 1. In Section 3 we study the finite sample fluctuations of the leading eigenvector and its overlap with the population eigenvector. Next, in Section 4 we illustrate the utility of these approximations in signal detection and communication applications. Specifically, Section 4.1 considers the power of Roy’s largest root test under two common signal models, whereas Section 4.2 considers the outage probability in a specific multiple-input and multiple-output (MIMO) communication system [24]. For a rank-one Rician fading channel, we show analytically that to minimize the outage probability it is preferable to have an equal number of transmitting and receiving antennas. This important design property was previously observed via simulations [24].",Roy’s largest root under rank-one perturbations: The complex valued case and applications,https://www.sciencedirect.com/science/article/pii/S0047259X18302896,2 July 2019,2019,Research Article,231.0
Chen Xiongzhi,"Department of Mathematics and Statistics, Washington State University, Pullman, WA 99164, USA","Received 10 September 2018, Revised 17 June 2019, Accepted 17 June 2019, Available online 26 June 2019, Version of Record 11 July 2019.",https://doi.org/10.1016/j.jmva.2019.06.003,Cited by (7),"The proportion of false null hypotheses is a very important quantity in ==== and inference based on the two-component mixture model and its extensions, and in control and estimation of the ==== and false non-discovery rate. Most existing estimators of this proportion threshold p-values, deconvolve the mixture model under constraints on its components, or depend heavily on the location-shift property of distributions. Hence, they usually are not consistent, applicable to non-location-shift distributions, or applicable to discrete ","The proportion of false null hypotheses and its dual, the proportion of true null hypotheses, play important roles in statistical modelling and multiple hypotheses testing. For example, they are components of the two-component mixture model of [14], its extensions by [5], [28] and [35], and their induced statistics including the “local false discovery rate (local FDR)” of [14] and “positive FDR (pFDR)” and ====-value of [39]. They also form the optimal discovery procedure (ODP) of [40]. However, without information on the proportions, decision rules based on the local FDR cannot be implemented, and none of the pFDR, ====-value and ODP decision rule can be computed in practice. On the other hand, these proportions form upper bounds on the FDRs and false non-discovery rates (FNRs) of all FDR procedures including those of [2], [3], [17], [37] and [41]. Therefore, accurate information on either proportion helps better control and estimate the FDR and FNR, and thus potentially enables an adaptive FDR procedure to be more powerful or have smaller FNR than its non-adaptive counterpart. Since neither proportion is known in practice, it is very important to accurately estimate the proportions.====In this work, we focus on consistently estimating the proportions without employing the two-component mixture model, requiring p-values to be identically distributed under the alternative hypotheses, or assuming that statistics or p-values have absolutely continuous cumulative distribution functions (CDFs). The main motivation for dealing with discrete p-values or statistics is the wide practise of FDR control in multiple testing based on discrete data in genomics [1], [36], genetics [18], vaccine efficacy studies [31], drug safety monitoring [8] and other areas, where Binomial test (BT), Fisher’s exact test (FET) and exact Negative Binomial test (NBT) have been routinely used to test individual hypotheses. In the sequel, we refer to an estimator of the proportion of true (or false) null hypotheses as a “null (or alternative) proportion estimator”.",Uniformly consistently estimating the proportion of false null hypotheses via Lebesgue–Stieltjes integral equations,https://www.sciencedirect.com/science/article/pii/S0047259X18304688,26 June 2019,2019,Research Article,232.0
"Kojadinovic Ivan,Stemikovskaya Kristina","CNRS / Université de Pau et des Pays de l’Adour / E2S UPPA, Laboratoire de mathématiques et applications – IPRA, UMR 5142, B.P. 1155, 64013 Pau Cedex, France,Universidad del País Vasco, Intelligent Systems Group, Campus de Guipuzcoa, 20018 Donostia - San Sebastian, Spain","Received 27 November 2018, Revised 27 May 2019, Accepted 27 May 2019, Available online 8 June 2019, Version of Record 18 June 2019.",https://doi.org/10.1016/j.jmva.2019.05.007,Cited by (8),"A key tool to carry out inference on the unknown ==== values computed from subsamples of the ====. One of its advantages in the rank-based context under consideration is that the formed subsamples do not contain ties. Another advantage is its asymptotic validity under minimalistic conditions. In this work, we show the asymptotic validity of subsampling for several (weighted, smooth) empirical copula processes both in the case of serially independent observations and time series. In the former case, subsampling is observed to be substantially better than the empirical bootstrap and equivalent, overall, to the multiplier bootstrap in terms of finite-sample performance.","Let ==== denote a stretch ==== from a stationary time series ==== of ====-dimensional random vectors. The distribution function (d.f.) of each ==== is denoted by ==== and is assumed to have continuous univariate margins ====. By Sklar’s theorem [48], it is then well-known that ==== can be expressed as ====where ==== is the unique ==== (a ====-dimensional d.f. with standard uniform margins) associated with ====.====Eq. (1) is at the root of the so-called ==== to the modeling of multivariate continuous distributions, which is increasingly applied in numerous fields such as quantitative risk management [32], econometrics [33], or environmental modeling [41]. Indeed, in order to obtain a parametric estimate of ====, the decomposition in (1) suggests to model ==== by appropriate univariate parametric d.f.s and ==== by an adequate parametric copula family. The recent infatuation for such a two-step approach in the literature is mostly due to the fact that it has the potential of providing better estimates of the multivariate d.f. ==== than if a direct classical multivariate approach were used; see, for instance, [28] and the references therein for more details.====The modeling of the univariate margins ==== of ==== can be based on classical statistical inference techniques. Inference on the unknown copula ==== is, however, typically carried out using specific methods exploiting the two-step nature of the underlying modeling. Among the latter methods, rank-based approaches display particularly good properties [20], [28]. One of their key ingredients is a nonparametric rank-based estimator of ==== called the ==== [13], [39]. In the absence of ties in the component samples of the available data ====, it is natural to define the latter simply as the empirical d.f. of the multivariate ranks obtained from ==== scaled by ====. Two smooth versions, with better small-sample properties, are the ==== [22], [24] and the ==== [45].====Whatever type of empirical copula is used in inference procedures on the unknown copula ==== in (1), it is almost always necessary to rely on resampling techniques to compute corresponding approximate confidence intervals or p-values. A frequently used approach is the so-called ==== [38], [43]. When ==== consists of ==== independent and identically distributed (i.i.d.) copies of ====, Bücher and Dette [6] empirically found the latter resampling scheme to have better finite-sample properties than approaches consisting of adapting the empirical (multinomial) bootstrap of Efron [17]. Both the i.i.d. version of the multiplier bootstrap and its extension to time series investigated in [7] are however characterized by a high implementation cost which may deter their use in copula inference procedures. The main aim of this work is to investigate the use of another resampling technique, known as ==== [34], [35], to carry out inference on the unknown copula ==== in (1).====In the case of i.i.d. data, subsampling consists of taking subsamples of size ==== without replacement from the initial data. The statistic of interest is then recomputed for a large number of such subsamples and its sampling distribution is approximated by the empirical distribution of its subsample values. In the time series case, subsamples are restricted to consecutive observations to preserve serial dependence. Note in passing that, in the i.i.d. setting, subsampling is connected to the so-called delete-==== jackknife [47], [54]; see also Section 2.3 in [46] and, in particular, Remark 2.1 in [34].====A theoretical advantage of subsampling is its asymptotic validity under minimalistic assumptions, weaker than those of the empirical bootstrap for instance; see [34], [35], [36] for details. From a practical perspective, subsampling is very simple to implement, its only drawback being the necessity of choosing the subsample size ====. In the context of copula modeling based on ranks (and, more generally, in the context of rank-based statistics), it is particularly attractive because subsamples do not contain ties unlike, for instance, resamples of size ==== in the case of the empirical bootstrap. The latter cannot therefore be directly used for rank-based statistics as shall be discussed, for instance, in Section 5 for (certain functionals of) the empirical copula. For this reason, subsampling appears as a simple way to obtain approximations of the sampling distributions of the empirical checkerboard and empirical beta copulas, even in a time series context.====Notice that, in the case of i.i.d. observations, subsamples of size ==== could also be obtained by sampling ==== from ==== [3], [50]. The resulting resampling technique, sometimes referred to as the ==== ==== ==== ==== (and which coincides with the empirical bootstrap when ====), thus forms subsamples with ties and therefore suffers from the same inconvenience as the empirical bootstrap in the rank-based context under consideration. For this reason, we shall not investigate it theoretically in this work. We shall however mention this alternative technique again when summarizing the results of our Monte Carlo experiments.====The rest of this article is organized as follows. In the second section, we introduce the main versions of the empirical copula appearing in the literature and define the corresponding empirical copula processes. The third section establishes the asymptotic validity of the subsampling methodology for the latter processes, while the fourth section states weighted versions of such results, thereby providing a first simple way to carry out inference on quantities which can be expressed as functionals of weighted empirical copula processes. The fifth section summarizes the results of Monte Carlo experiments in the i.i.d. and time series cases, and provides recommendations for the choice of the subsample size in the i.i.d. case. Finally, concluding remarks are gathered in Section 6.====All proofs are deferred to a sequence of appendices. Additional simulation results are provided in a supplement. The following notation is used in the sequel. The arrow ‘====’ denotes weak convergence in the sense of Definition 1.3.3 in [52], and, given a set ====, ==== (resp. ====) represents the space of all bounded (resp. continuous) real-valued functions on ==== equipped with the uniform metric. All convergences are for ==== if not mentioned otherwise.",Subsampling (weighted smooth) empirical copula processes,https://www.sciencedirect.com/science/article/pii/S0047259X18306183,8 June 2019,2019,Research Article,233.0
"Le Brigant Alice,Puechmorel Stéphane","ENAC, Université de Toulouse, Toulouse, France","Received 2 July 2018, Revised 29 May 2019, Accepted 29 May 2019, Available online 4 June 2019, Version of Record 7 June 2019.",https://doi.org/10.1016/j.jmva.2019.05.008,Cited by (8),"The goal of quantization is to find the best approximation of a probability distribution by a discrete measure with finite support. When dealing with empirical distributions, this boils down to finding the best summary of the data by a smaller number of points, and automatically yields a ","Most of the statistical tools developed so far are dedicated to data belonging to vector spaces, since it is the most convenient setting for algorithms as well as for theoretical derivations. However, when dealing with real world applications, such a framework may not fit with the structure of the data. It is obviously the case for geostatistics over world-sized datasets, but it is also true in many other fields: shapes in computer vision, diffusion tensor images in neuroimaging, signals in radar processing do not belong to a Euclidean space, but rather to a differentiable manifold. Riemannian geometry provides a convenient framework to deal with such objects. It allows a straightforward generalization of basic statistical notions such as means and medians [1], [16], [20], covariances [33], Gaussian distributions [38], and of methods based on linear operations, such as principal component analysis [15], [39]. The use of these statistical tools has met a growing interest in various fields, including shape analysis [21], computational anatomy [15], medical imaging [14], probability theory [4], and radar signal processing [2], [24].====In air traffic management (ATM), a major concern is the ability to infer an estimation of the complexity as perceived by a controller from the knowledge of aircraft trajectories in a given airspace, as depicted in Fig. 1. Many interdependent factors are involved in the cognitive process of a human controller, making the problem extremely difficult to solve, if even possible. However, there is a consensus among the experts on the importance of traffic disorder. As detailed in [28], a good way to estimate traffic disorder is to assume the spatial distribution of the aircraft velocities to be Gaussian and use the covariance function as an indicator for local complexity. This model requires to estimate a covariance matrix at each sample point of the airspace, of which we adopt a planar representation through a stereographic projection, and therefore yields a mapping from the plane to the space of symmetric, positive definite (SPD) matrices. Such a mapping will be referred to as an SPD image in the sequel. Working directly with the SPD images is an extremely computationally expensive task, that is unrealistic in practice. Moreover, the information provided is highly redundant, making it cumbersome to use in statistical analysis. To cope with this problem, we seek to produce summaries of the images. To do so, we model an SPD image as a realization of a random field with values in the space of SPD matrices. The values collected at each sample point in the image define an empirical probability distribution, supported on the space of SPD matrices. We propose to produce a summary of this empirical distribution using optimal quantization.====Optimal quantization is concerned with finding the best approximation, in the sense of the Wasserstein distance, of a probability distribution ==== by a discrete measure ==== with finite support ==== (see [17] or the survey paper [31], and references therein). When dealing with empirical distributions, this boils down to finding the best summary of the data by a smaller number of points. In the same setting, quantization naturally yields a clustering of the data points, which coincides with the solution given by the ====-means algorithm. In our application, probability measures are supported on the space of SPD matrices, and therefore points actually refer to SPD matrices. The main challenge lies in the fact that SPD matrices do not form a vector space, but rather a Riemannian manifold [34], whereas most work on optimal quantization is suited for vector data [6], [32], or functional data [3], [30]. Nonetheless, the case of probability distributions on Riemannian manifolds has recently received attention [18], [22]. In particular, the asymptotic behavior of the quantization error, i.e., the evolution of the error made by approximating ==== by ==== as ====, was studied for the manifold case in [18]. However, to the best of our knowledge, no numerical schemes have yet been introduced in this setting.====In this work, we introduce Competitive Learning Riemannian Quantization (CLRQ), a Riemannian counterpart of Competitive Learning Vector Quantization (see for example [31]). It is a gradient descent algorithm that computes the best approximation ==== of a probability measure ==== over a Riemannian manifold using observations sampled from ====. For empirical distributions, this allows to summarize a manifold-valued dataset of size ==== by a smaller number ==== of points, which additionally correspond to the centers of a ====-means-type clustering. Recall that the classical Riemannian ====-means algorithm computes at each step a partition of the data space (of the size of the desired number of clusters) and computes the geometric (Karcher) mean of each cell of the partition [41]. These two steps are usually very computationally expensive in the Riemannian setting, where the geodesic distance does not always have a closed form and the mean can only be obtained by an iterative procedure. Other clustering procedures relying on the estimation of distance-based kernels or graphs, such as mean-shift [40], or persistence-based clustering [10], require at least the computation of the matrix of pairwise-distances between the data points, if not the distances from the current (new) point to all the data points at each step. It is also the case of kernel-based approaches which first transport the data into a Reproducing Kernel Hilbert Space (RKHS) before applying a clustering procedure, less expensive in the vector setting. In [44], the authors propose to accelerate the procedure through random projections, which require to compute the distance matrix of a small subset of points instead of the whole dataset. In contrast, CLRQ is an online algorithm (i.e., processing one data point at a time) and requires at each step as many distance computations as the number of desired clusters, which is usually very small compared to the number of data. This leads us to argue that CLRQ is well-suited for large datasets.====After proving the convergence of CLRQ under the relevant assumptions, we use it to perform air traffic complexity analysis as presented above. Applying the algorithm to the empirical distribution of an SPD image yields two desirable results: (1) a summary of the image through ====, and (2) a clustering of the image, and thereby of the corresponding airspace, into different zones homogeneous with respect to complexity. The latter means that to each point of the image is assigned a level of complexity according to the class it belongs to, and the former, that different traffic images can easily be compared through the comparison of their summaries, using the Wasserstein distance. This is an interesting prospect, since it allows for the indexation of air traffic situations in view of creating automatized decision-making tools to help air traffic controllers. Naturally, CLRQ can conceivably be applied to many other application-driven problems, requiring either the construction of summaries or the clustering of geometric data. A natural example of such data with a positive curvature is given by GPS positions on the Earth (spherical data); in negative curvature, one can think of Gaussian distributions, which can be parameterized by the hyperbolic half-plane in the univariate case, and by higher dimensional Riemannian symmetric spaces in the multivariate case [8], [26].====The paper is organized as follows. In Section 2, we introduce the Riemannian setting and give an example of a statistical object on a manifold (the Riemannian center of mass), before introducing the context of optimal quantization. In Section 3, we present the CLRQ algorithm and show its convergence. After showing some simple illustrations on the circle, the sphere and the hyperbolic half-plane in Section 4, we present our main application in air traffic management in Section 5.",Quantization and clustering on Riemannian manifolds with an application to air traffic analysis,https://www.sciencedirect.com/science/article/pii/S0047259X18303361,4 June 2019,2019,Research Article,234.0
"Tang Niansheng,Wang Wenjun","Yunnan Key Laboratory of Statistical Modeling and Data Analysis, Yunnan University, Kunming 650091, China,Department of Mathematics, Hainan Normal University, Haikou 510000, China","Received 19 June 2018, Revised 27 May 2019, Accepted 27 May 2019, Available online 3 June 2019, Version of Record 6 June 2019.",https://doi.org/10.1016/j.jmva.2019.05.006,Cited by (3),This paper considers the parameter estimation problem in generalized estimating equations (GEEs) with a finite mixture of working correlation matrices and missing ,"Longitudinal data are frequently encountered in economics, behavioral science, biomedicine and clinical trials. The characteristics of longitudinal data are that observations from the same subject are usually correlated, but those from different subjects are independent. To this end, generalized estimating equations (GEEs) are widely used to analyze longitudinal data (see, e.g., [19]) due to the aspects that the GEE approach only requires the first two moments of response variables, and the resultant estimators of parameters are consistent even if the working correlation matrix is misspecified. But the efficiency of parameter estimators obtained from the GEE method heavily depends on the working correlation matrix. To address the issue, various statistical methods have been developed to improve the efficiency of parameter estimators; see, e.g., [6], [23], [28], [29]. The aforementioned literature involves selecting a working correlation structure from a small set of candidate correlation structures prespecified. In application, it is almost impossible to achieve a correct correlation structure of working correlation matrix. To address the misspecification problem of working correlation structure, Leung et al. [14] proposed a hybrid GEE method by combining multiple GEEs with different linearly independent working correlation matrices. But, when the estimated correlation matrices are nearly singular or sample size is small compared with the cluster size, the efficiency of the hybrid GEE method is sometimes unsatisfactory and unstable numerically. To overcome these deficiencies, Xu et al. [32] presented a mixture GEE (mix-GEE) approach to improve the finite sample efficiency and numerically stability of the hybrid GEE approach using a finite mixture model to specify the working correlation matrix, which can adaptively select the true working correlation structure according to estimates of mixture proportions. The mix-GEE method is efficient only when data are from a Gaussian mixture model and one of the mixture components gives the correct correlation structure, which limits its application. To this end, Zhou and Qu [35] adopted a linear combination of candidate basis matrices that contain either 0 or 1 as components to approximate the empirical estimator of the correlation matrix; Hu et al. [7] utilized eigenvector-based basis matrices to approximate the inverse of the empirical correlation matrix and determine the number of basis matrices via model selection. However, the aforementioned methods are developed in the absence of missing covariates. When covariates are subject to missingness, the widely used technique is to naively exclude subjects with missing values, i.e., the well-known complete-case (CC) analysis, which may yield inefficient estimation of parameters (see, e.g., [20]) when missing data mechanism is not missing completely at random. Hence, this paper aims to develop an efficient approach to impute missing values of covariates and improve the efficiency of parameter estimator.====The widely used imputation methods for missing data include linear regression imputation (see, e.g., [25]), empirical likelihood imputation (see, e.g., [8]), nonparametric regression imputation (see, e.g., [2]), semiparametric partially linear regression imputation (see, e.g., [17]) and the conditional quantiles imputation (see, e.g., [4]). Recently, the imputation technique via matrix completion algorithm has also received a lot of attention because it is a data-driven imputation method and matrix completion algorithm is implemented on the basis of the nuclear norm minimization model and low-rank matrix factorization model. For example, see Cai et al. [1] for the singular value thresholding (SVT) algorithm, Chen et al. [3] for the fixed point continuation algorithm, Wen et al. [30] for the low-rank matrix fitting algorithm, Lee and Seung [11] for the non-negative matrix factorization (NMF) based on the positively constrained projection, which can obtain an accurate approximation of missing data, and yield sparse, spatially localized and part-based subspace representations by finding a low-rank matrix approximation of the original data, and Li et al. [16] for the robust non-negative matrix projection (RNMFP) method by combining the NMF method and the expectation–maximization (EM) algorithm. Compared with the traditional principal component analysis, the NMF method is calculated in a matrix, and has more fully data information to complete matrix. Motivated by the aforementioned features of RNMFP, we here develop a weighted RNMFP (WNMFP) approach to interpolate missing covariates, which debates different weights of missing covariates via propensity scores specified by a logistic regression model (see, e.g., [26], [27]).====The proposed WNMFP method has the following merits. First, the WNMFP algorithm is more robust to the misspecification of missingness data mechanism model and working correlation matrix than the RNMFP method. Second, unlike the CC method and inverse probability weighting (IPW) approach for estimating parameters, the proposed WNMFP approach has better performance in terms of the simulated relative efficiency. Third, coverage probabilities of parameter estimators evaluated using the proposed method are closer to the pre-specified confidence level than the CC method and the IPW method. Fourth, under some regularity conditions, we show the convergence of the proposed WNMFP algorithm and the consistence and asymptotic normality of the proposed mix-GEE estimator.====The rest of this article is organized as follows. Section 2 introduces the Mix-GEE approach to handle unknown correlation structure, and the WNMFP approach to impute missing values of covariates. Section 3 studies convergence of the proposed WNMFP algorithm and asymptotic properties of the proposed Mix-GEE estimators. Simulation studies and a real data example are presented in Section 4. Some concluding remarks are given in Section 5. Technical details are given in Appendix A.",Robust estimation of generalized estimating equations with finite mixture correlation matrices and missing covariates at random for longitudinal data,https://www.sciencedirect.com/science/article/pii/S0047259X18303178,3 June 2019,2019,Research Article,235.0
"Lin Hongmei,Lian Heng,Liang Hua","School of Statistics and Information Shanghai University of International Business and Economics Shanghai, China,Department of Mathematics The City University of Hong Kong, Kowloon Tong, Hong Kong,Department of Statistics, George Washington University, USA","Received 10 October 2018, Revised 26 May 2019, Accepted 26 May 2019, Available online 3 June 2019, Version of Record 7 June 2019.",https://doi.org/10.1016/j.jmva.2019.05.005,Cited by (0),"When a regression problem contains ====, ====, that can deal with discrete responses and is useful for classification. Numerical results are reported to illustrate the finite sample performance of the estimator. We also establish an improved convergence rate of the rank reduction approach compared to the standard estimator and extend it to sparse modeling to deal with an even larger number of predictors.","Statistical estimation of multivariate functions requires very large sample sizes and suffers from severe efficiency problems with limited samples available. Semiparametric models that involve only estimation of univariate functions in the setting with multiple predictors are thus popular.====For a variety of types of responses, generalized linear models [15] extend the framework of linear models, by allowing for non-Gaussian data and nonlinear link functions. They have become a favored tool for Binomial or Poisson type responses, among possible other types. The relatively strong parametric assumptions inherent in generalized linear models motivated the more general specification of generalized additive models where each predictor can enter the model with flexible nonparametric structures [10].====In practice, sometimes the statistical analyst collects a large number of predictors for statistical analysis. High dimensionality is an important characteristic of many modern data sets. Then, even though additive models only involve univariate functions, the sheer number of univariate functions (====) makes it a challenge to estimate a large number of functions given the limited sample size. On the other hand, even with a great many predictors available to include into initial modeling, many of them may not be relevant and the inclusion of these decreases the accuracy of prediction. Indeed, in the literature, one popular way out is to assume that only a few of the ==== functions are nonzero in the true model and use sparsity-inducing penalties to identify the nonzero functions [12], [16], [17], [24]. One can imagine that such a strategy typically works well when the number of nonzero functions is small and each contains sufficiently large signals (for example as measured by the ==== norm of the component functions).====We herein propose a dimension reduction approach which assumes that the different functions are functionally related which allows us to borrow information among the ==== functions to achieve more a stable and efficient estimation. More specifically, we assume the existence of ==== unknown functions with ==== such that each component function in the additive model is a linear combination of these ==== functions. We call these ==== functions the ‘latent functions’. Thus the estimation of the ==== functions is reduced to the estimation of these ==== latent functions as well as some additional coefficients in the linear combinations. When ==== is much smaller than ====, the proposed model typically consists of much fewer parameters to estimate, resulting in a reduction in the dimension of the estimation problem. Furthermore, we will also combine dimension reduction with sparsity-inducing penalties to take advantage of the benefits of both worlds. Our numerical results will demonstrate the usefulness of our approach. We realize our proposal in the setting of generalized additive models, although it is applicable to any models involving a large number of nonparametric functions to be estimated.====The rest of the paper is organized as follows. In Section 2, we detail the dimension reduction methodology based on latent functions and establish the asymptotic properties of the proposed estimator. In Section 3, we further combine dimension reduction with sparsity-inducing penalties for variable selection and establish the oracle property of the estimator. Section 4 reports some simulations results, after a discussion of the computational algorithm used to obtain the estimators. Section 5 contains an empirical analysis of the breast cancer data. The paper is concluded in Section 6 and the technical proofs are relegated to Appendix.",Rank reduction for high-dimensional generalized additive models,https://www.sciencedirect.com/science/article/pii/S0047259X18305293,3 June 2019,2019,Research Article,236.0
Krebs Johannes T.N.,"Department of Statistics, University of California, Davis, One Shields Avenue, 95616, CA, USA","Received 26 June 2018, Revised 25 May 2019, Accepted 25 May 2019, Available online 30 May 2019, Version of Record 4 June 2019.",https://doi.org/10.1016/j.jmva.2019.05.004,Cited by (3),"We consider the double functional regression model ====, where the response variable ",None,The bootstrap in kernel regression for stationary ergodic data when both response and predictor are functions,https://www.sciencedirect.com/science/article/pii/S0047259X18303324,30 May 2019,2019,Research Article,237.0
"Bücher Axel,Volgushev Stanislav,Zou Nan","Heinrich-Heine-Universität Düsseldorf, Mathematisches Institut, Universitätsstr. 1, 40225 Düsseldorf, Germany,Department of Statistical Sciences, University of Toronto, 100 St. George St. Toronto, M5S 3G3 Ontario, Canada","Received 31 August 2018, Revised 30 April 2019, Accepted 30 April 2019, Available online 29 May 2019, Version of Record 4 June 2019.",https://doi.org/10.1016/j.jmva.2019.04.011,Cited by (8), is examined in detail; we find that this class contains models for which the second order parameter is smaller for the BM method and vice versa. The theory is illustrated by a small simulation study.,"Extreme value theory is concerned with describing the tail behavior of a possibly multivariate distribution. Respective statistical models and methods find important applications in fields like finance, insurance, environmental sciences, hydrology or meteorology. In the multivariate case, a key part of statistical inference is estimation of the dependence structure. Mathematically, the dependence structure can be described in various equivalent ways (see, e.g., [2], [18], [25]): by the stable tail dependence function ==== [21], by the exponent measure ==== [1], by the Pickands dependence function ==== [24], by the tail copula ==== [27], by the spectral measure ==== [19], by the madogram ==== [23], or by other less popular objects.====Estimators for these objects typically rely on one of two basic principles allowing one to move into the tail of the distribution: the ==== (BM) and the ==== (POT). More precisely, suppose that ====, with ====, is an i.i.d. sample from a multivariate cumulative distribution function ====. For some large number ==== (in the asymptotics, one commonly considers ==== such that ====), let ====that is, ==== comprises all observations for which at least one coordinate is large. Any estimator defined in terms of these observations represents the multivariate POT method. The vanilla nonparametric estimator within this class is probably the ==== [21].====To introduce the BM approach, let ==== denote a large block size, and let ==== denote the number of blocks (again, in the asymptotics, one commonly considers ==== such that ====). For ====, let ==== denote the vector of componentwise block-maxima in the ====th block of observations of size ====, that is, ====. Any estimator defined in terms of the sample ====represents the BM approach.====Asymptotic theory for estimators based on the POT approach is typically formulated under a suitable second order condition (see Section 2 for details). The asymptotic variance of resulting estimators is then typically of the order ==== (see, e.g., [13], [14], [16], [21], [27], among others), whereas the rate of the bias is typically (up to slowly varying factors) given by ====, with ==== denoting the ==== in the aforementioned second order condition. In case ==== and without any slowly varying factor, balancing the bias and variance leads to the choice ====, which results in an asymptotic MSE of order ====. For a particular class of models, this resulting convergence rate is in fact minimax-optimal [12].====Perhaps surprisingly, results on asymptotic theory for estimators based on the BM approach are typically based on the assumption that the block size ==== is fixed and that the sample ==== is a genuine i.i.d. sample from the limiting attractor distribution (see, e.g., [17] and references therein). Thereby, a potential bias is completely ignored and a fair comparison between estimators based on the POT and the BM approach is not feasible. This imbalance has recently been recognized by [5], [9], [10], [15] in the univariate case; see also the overview article [6]. To the best of our knowledge, the only reference in the multivariate case is [4]. In analogy to the POT case, the results in the latter paper can be simply reformulated in terms of a suitable second order condition (see Section 2 for details). Based on these results, an estimator for the Pickands dependence function can then be shown to have asymptotic variance of order ====, while the bias is again typically governed by a second order parameter ==== and has order ====, up to possible slowly varying terms. Similar calculations as in the preceding paragraph show that the best possible MSE is typically of order ====.====As indicated by the above discussion, “best” convergence rates for the BM and POT approaches depend on the second order parameters in their respective second order conditions. This motivates us to study the relationship between the two types of second order conditions. Our first major contribution is to show that a natural POT second order condition, in case ====, implies a natural BM second order condition with ====, and vice versa. As a consequence, if ====, the best attainable rates for POT and BM estimators coincide. The situation changes when ====, in which case we obtain that, under mild additional conditions, ====; similarly we prove that typically ==== implies ====. This identifies scenarios in which either BM or POT estimators can attain better rates of convergence. Finally, when ====, both ==== and ====
 are possible (and vice versa), and additional conditions to verify which of the two cases occurs are provided. Should one be interested in an efficiency comparison in a case where ==== (so the best attainable rates are the same, possibly up to slowly varying terms), the second order conditions are still informative since they allow to derive exact expressions for the asymptotic bias terms of specific estimators (see Section 4 for examples). In addition, our results may also considerably simplify the verification of second order conditions in one of the approaches if it has already been verified in the other. Finally, note that a similar relationship between second order parameters has been worked out in [11] for the univariate case.====As a second major contribution, we provide a detailed analysis of second order conditions (BM and POT) for the class of Archimax copulas [7]. Simple sufficient conditions are formulated in terms of the Archimedean generator associated with such copulas. In particular, we show that the class of Archimax copulas contains examples where either the POT or BM method can lead to faster convergence rates. This is also illustrated in a small finite-sample simulation study in Section 4.====The remaining part of this article is organized as follows: in Section 2, we introduce the second order conditions of interest and work out the connections between the two, including the above mentioned main result. In Section 3, we work out details in two particular examples: the general class of Archimax copulas and outer power transforms of the Clayton copula. In Section 4, we illustrate the consequences for the rate of convergence of respective estimators, both by theoretical means and by a simulation study. The proofs of some technical lemmas are deferred to an Appendix.",On second order conditions in the multivariate block maxima and peak over threshold method,https://www.sciencedirect.com/science/article/pii/S0047259X18304536,29 May 2019,2019,Research Article,238.0
Kraus David,"Department of Mathematics and Statistics, Masaryk University, Kotlářská 2, 611 37 Brno, Czech Republic","Received 19 September 2018, Revised 14 May 2019, Accepted 15 May 2019, Available online 27 May 2019, Version of Record 3 June 2019.",https://doi.org/10.1016/j.jmva.2019.05.002,Cited by (10),"In functional data analysis it is usually assumed that all functions are completely, densely or sparsely observed on the same domain. Recent applications have brought attention to situations where each functional variable may be observed only on a subset of the domain while no information about the function is available on the complement. Various advanced methods for such partially observed functional data have already been developed but, interestingly, some essential methods, such as ","Functional data analysis is an established field [17], [28], [34], [54] with well-developed methodologies for common types of observation of random curves, i.e., full (or dense) and sparse observation regimes. Due to new applications recent years have seen the emergence of a new type of observation of functional data, called functional fragments or partially observed functional data. For various examples see Bugni [6], Delaigle and Hall [14], Liebl [38], Gellar et al. [21], Goldberg et al. [23], Kraus [35], Delaigle and Hall [15], Gromenko et al. [24], Kneip and Liebl [32], Dawson and Müller [13], Mojirsheibani and Shaw [45], Stefanucci et al. [55], Descary and Panaretos [16], Kraus and Stefanucci [37] or Liebl and Rameseder [40].====Functional data are collections of observations of random elements of a function space, such as curves, images, surfaces, spatio-temporal fields. We consider random functions in a separable Hilbert space. Without loss of generality we work with the space ==== of square-integrable functions on ==== equipped with inner product ==== and norm ==== but our results are applicable to more general spaces. Partially observed functional data consist of realizations of random functions that are not observed on the entire domain. Each function in the sample may be observed on a different subset of the domain and no information is available on the function values at arguments in the complement of this subset. For the ====th functional variable ==== there is a subset ==== such that ==== is observed for ==== and not observed for ====. The observation sets may be random, corresponding to data that are missing by happenstance, or non-random for designed experiments. We assume that the observation sets are mutually independent and independent of the curves. We refer to Liebl and Rameseder [40] for a study of the case of dependent missingness.====Although some advanced procedures, such as goodness-of-fit tests, regression, classification and reconstruction methods, have been developed for functional fragments, basic methods of inference about the fundamental characteristics of functional variables are still missing. In particular, the asymptotic distribution of estimators of the mean function and covariance operator, ====-sample tests of equal means or covariances, and confidence intervals for eigenvalues and eigenfunctions have not been studied yet in the setting of incomplete functions. Users who wish to perform these basic tasks currently have the only option: to omit the partially observed functions and apply existing procedures to the complete data only. This approach is not only clearly sub-optimal due to a possibly large loss of information and resulting decay of power and accuracy, but also hardly or totally inapplicable in situations where the data contain few or no complete curves.====In this paper, we address this deficiency of existing methodology and develop essential methods of inference about the mean and covariance structure of incomplete functional data. Random functions are characterized by the mean function ==== and the covariance operator ==== defined as ====where ==== is the covariance function, assuming it exists. The covariance structure is best understood via principal component analysis or eigendecomposition of ==== in the form ====where ==== are the eigenvalues, ==== are the corresponding orthonormal eigenfunctions, and ==== for ====. For a theoretical background see, e.g., Bosq [5].====We find appropriate assumptions on the observation pattern that enable us to establish the asymptotic distribution of estimators of ==== and ====. We develop tests for comparing the mean functions in ==== populations of functional data based on samples of fragments. Next, we propose several tests of equal covariance operators in ==== samples. We also construct confidence intervals for the eigenvalues and eigenfunctions estimated from incomplete data.====The practical implementation of methods for functional fragments is more complicated than for complete curves. The main difficulty is that temporal averaging (e.g., in inner products for dimension reduction) is impossible due to missing values. This leads to asymptotic distributions whose parameters follow rather complicated formulas. More importantly, since dimension reduction is not possible, the asymptotic distributions are, upon discretization, characterized by large objects (matrices or arrays) that are difficult or even impossible to store and manipulate in computer memory. The bootstrap turns out to be a solution to this problem. We provide specific algorithms for resampling functional fragments for mean and covariance testing and for confidence intervals for eigenelements.====In a simulation study we investigate the performance of the proposed tests, focusing in particular on the impact of missingness on the different tests and on the effect of the interplay between missingness and the form of differences between groups. The study shows that the proposed methods are superior to the currently only available approach based on omitting incomplete curves.====The proposed methodology is applied to a data set of temporal profiles of heart rate. The data consist of several hundred curves recorded by an automatic device during several hours in the evening during the transition from the day to night regime of heart activity. The profiles are not observed always available on the entire domain of interest because either the device did not measure or record measurements, or the person switched off the device. These fragmentary data were previously analysed in Kraus [35], where further details can be found.====Section 2 develops methods of inference about means in one and ==== samples. Section 3 deals with tests about covariance operators and with inference about principal components. Section 4 presents bootstrap approximations. Results of the simulation study and the data example are reported in Sections 5 Simulation results, 6 Application to partially observed heart rate temporal profiles. In the Appendix A A central limit theorem, Appendix B Proofs we provide a central limit theorem for non-identically distributed functional variables needed in the asymptotic analysis of fragments, and proofs of all theorems. Additional simulation results and further results of the data analysis.",Inferential procedures for partially observed functional data,https://www.sciencedirect.com/science/article/pii/S0047259X18304950,27 May 2019,2019,Research Article,239.0
"He Yong,Zhang Liang,Ji Jiadong,Zhang Xinsheng","School of Statistics, Shandong University of Finance and Economics, Jinan, 250014, China,School of Management, Fudan University, Shanghai, 200433, China","Received 15 May 2019, Accepted 15 May 2019, Available online 24 May 2019, Version of Record 31 May 2019.",https://doi.org/10.1016/j.jmva.2019.05.003,Cited by (13),None,"In the last decades, data sets with large dimensionality have arisen in various areas such as finance, chemistry, etc. due to the great development of the computer storage capacity and processing power and feature selection with these big data is of fundamental importance to many contemporary applications. The sparsity assumption is common in high dimensional feature selection literatures, i.e., only a few variables are critical for model fitting and forecasting of certain response of interest. Specifically, for the linear regression setting, statisticians care about how to select out the important variables from thousands or even millions of variables. In fact, a huge amount of literature spring up since the appearance of the Lasso estimator [27]. To name a few, there exist SCAD by [8], Adaptive Lasso by [32], MCP by [30], the Dantzig selector by [4], group Lasso by [29]. This research area is very active, and as a result, the list of references here is illustrative rather than comprehensive. The aforementioned feature selection methods perform well when the dimensionality is not “too” large, theoretically in the sense that it is of polynomial order of the sample size. However, in the setting where the dimensionality is of exponential order of the sample size, which is typically referred to as ultra-high dimension, the aforementioned methods may encounter both theoretical and computational issue. For example, the Uniform Uncertainty Principle (UUP) condition to guarantee the oracle property of Dantzig selector estimator may be difficult to satisfy, and the computational cost would increase dramatically by implementing linear programs. [9] first proposed Sure Independence Screening (SIS) and its further improvement named Iterative Sure Independence Screening (ISIS), to alleviate the computational burden in ultra-high dimensional setting. The basic idea goes as follows. In the first step, reduce the dimensionality to a moderate size against the sample size by sorting the marginal Pearson correlation between covariates and the response, and removing those covariates whose marginal correlation with response is lower than a certain threshold. In the second stage one may exploit Lasso, SCAD, etc. to the variables survived in the first step. The SIS (ISIS) has the sure screening property under certain conditions, that is, with probability tending to 1, the SIS procedure can select out all important variables. The last decade has witnessed plenty of variants of SIS to handle the ultra-high dimensionality for more general regression models. [7] proposed a feature screening procedure for ultra-high dimensional additive models. [10] proposed a feature screening procedure for ultra-high dimensional varying coefficient models. [25] proposed censored rank independence screening of high-dimensional survival data which is robust to predictors that contain outliers and works well for a general class of survival models. [31] and [6] proposed model-free feature screening. [17] proposed to screen Kendall’s ==== correlation while [19] proposed to screen distance correlation which both show robustness to heavy-tailed data. [18] also proposed a robust rank SIS (RSIS) which is robust to outliers. [16] proposed to screen the canonical correlation between the response and all possible sets of ==== variables, which performs well particularly for selecting out variables that are pairwise jointly important with other variables but marginally insignificant. This list of references for screening methods is also illustrative rather than comprehensive. For the development of the screening methods in the last decade, we refer to the review paper of [22].====The main contribution of the paper is two-fold. On the one hand, we innovatively propose a very flexible semi-parametric regression model called Elliptical Copula Regression (ECR) model, which can capture the thick-tail property of variables and the tail dependence between variables. In specific, the ECR model has the following representation: ====where ==== is the response variable, ==== are predictors, ==== is the error term, ==== are univariate monotonic functions. We say ==== satisfies a Elliptical copula regression model if the marginally transformed random vectors ==== follows elliptical distribution. From the representation of ECR model in (1), it can be seen that the ECR model covers a large class of linear and nonlinear regression models such as the additive regression model and the linear transformation model, which makes it more applicable in many areas such as econometrics, finance and bioinformatics. On the other hand, we propose a robust dimension reduction procedure for the ECR model in the ultra-high dimensional setting. The robustness is achieved by combining two types of correlation, which are Kendall’s ==== correlation and canonical correlation. The canonical correlation is employed to capture the joint information of a set of covariates and the joint relationship between the response and this set of covariates. Note that for ECR model in (1), only ==== is observable rather than the transformed variables. Thus the Kendall’s ==== correlation is exploited to estimate the canonical correlations due to its invariance under strictly monotone marginal transformations. The dimension reduction procedure for ECR model is achieved by sorting the estimated canonical correlations and leaving the variable that attributes a relatively high canonical correlation at least once into the active set. The proposed screening procedure enjoys the sure screening property and reduces the dimensionality substantially to a moderate size under mild conditions. Numerical results show that the proposed approach performs much better than some existing feature screening methods.====We introduce some notations adopted in the paper. For any vector ====, let ==== denote the ==== vector by removing the ====th entry from ====. ====, ====, ==== and ====. Let ====. ====, ==== and ====. For a matrix ====, ==== denote the ====th row of ==== with its ====th entry removed and ==== denote the ====th column of ==== with its ====th entry removed, ==== denotes a ==== matrix obtained by removing the ====th row and ====th column of ====. We use ==== and ==== to denote the smallest and largest eigenvalues of ==== respectively. For a set ====, denote by ==== or Card(====) the cardinality of ====. For a real number ====, denote by ==== the largest integer smaller than or equal to ====. For two sequences of real numbers ==== and ====, we write ==== if there exists a constant ==== such that ==== holds for all ====, write ==== if ====, and write ==== if there exist constants ==== and ==== such that ==== for all ====. In addition, a ====-combination of a set ==== is a subset of ==== distinct elements of ====.====The rest of the paper is organized as follows: in Section 2, we introduce the Elliptical copula regression model and present the proposed dimension reduction procedure by ranking the estimated canonical correlations. In Section 3, we present the theoretical properties of the proposed procedure with more detailed proofs collected in Appendix. In Section 4, we conduct thorough numerical simulations to investigate the empirical performance of the procedure. In Section 5, a real gene-expression data example is given to illustrate its empirical usefulness. In the last section, we give a brief discussion on possible future research directions.",Robust feature screening for elliptical copula regression model,https://www.sciencedirect.com/science/article/pii/S0047259X19302684,24 May 2019,2019,Research Article,240.0
"Yauck Mamadou,Rivest Louis-Paul","Département de mathématiques et de statistique, Faculté des sciences et de génie, Université Laval, pavillon Alexandre-Vachon 1045, av. de la Médecine, bureau 1056, Québec (Québec), G1V 0A6, Canada","Received 26 April 2019, Revised 29 April 2019, Accepted 29 April 2019, Available online 20 May 2019, Version of Record 20 May 2019.",https://doi.org/10.1016/j.jmva.2019.04.010,Cited by (0), and in a numerical example about the estimation of the size of dolphin populations discussed by Santostasi et al. (2016).,"The methodology for the estimation of population sizes in capture–recapture studies is well understood for closed and open population models. Closed population models assume that the population does not change during the study. In open population models, the population experiences births and deaths during the study so that its size varies. For closed population models, one distinguishes the multinomial and the conditional estimator, see [8]. Inference on population sizes can be carried out using a Poisson likelihood as pointed out in [18] and [5]. For open population models, [9] and [21] built a likelihood by assuming that the number of unmarked animals prior to each capture occasion are fixed parameters; they derived maximum likelihood estimators for the so-called Jolly–Seber model. Poisson [4] and multinomial [20] likelihoods have also been considered for this problem. This work is concerned with a hierarchical study design where a capture–recapture experiment, involving ====, is carried out within each ==== (SP) of an open population model. This is the robust design introduced by [13]. [16] obtained the maximum likelihood estimators of the population sizes through a Poisson regression. This involves a dependent vector of size ====, where ==== is the total number of capture occasions. When ==== the dependent vector and the associated design matrix exceed the storage capacity of standard routines for generalized linear models and their estimators cannot be implemented.  [10] proposed a moment-type estimator for abundance and, in a recent paper, [26] proposed a sequential estimation procedure for the parameters of the robust design when the closed population models within each primary period belong to a family considered in [17].====A robust design involves information within and between the primary periods of the underlying open population experiment. These two sources of information are combined when estimating population sizes. This paper shows that for a large class of such population models, the between and the within period estimators of population sizes are asymptotically independent. In this case the robust design maximum likelihood estimator of population size is asymptotically equivalent to a weighted sum involving the estimators for the Jolly–Seber model and for the closed population model. This estimator differs from the moment estimator of [10]. We show that the moment estimator is not efficient as it is not a maximum likelihood estimator; a formula for its efficiency is provided. The loss of precision associated with this estimator is evaluated in a Monte Carlo study and in a numerical example about the estimation of the size of dolphin populations discussed by [19].====To apply the methodology proposed in this work to a robust design data set, one needs to select closed population models for the data on the secondary capture occasions within each primary period. Methods to do so are discussed in [15], [3, Ch. 16] and [26]. The class of models for the within primary period data considered in this work belongs to the family of closed population models presented in [17].",On the estimation of population sizes in capture–recapture experiments,https://www.sciencedirect.com/science/article/pii/S0047259X19302325,20 May 2019,2019,Research Article,241.0
Atchadé Yves F.,"Department of Mathematics and Statistics, Boston University, 111 Cummington Mall, Boston, MA 02215, USA","Received 7 April 2018, Revised 14 March 2019, Accepted 14 March 2019, Available online 18 May 2019, Version of Record 6 June 2019.",https://doi.org/10.1016/j.jmva.2019.03.005,Cited by (3),", where ==== is the number of nodes in the graph, ==== the sample size, and ","We consider the problem of fitting large Gaussian graphical models from limited data. More precisely, our goal is to estimate a sparse precision matrix ==== from ====-dimensional Gaussian observations ====, where ==== denotes the cone of ==== of symmetric positive definite matrices. The frequentist approach to this problem has generated an impressive literature over the last decade or so; see, e.g., [7], [13] and the references therein.====There is an interest, particularly in biomedical research, for statistical methodologies that can allow practitioners to incorporate external information in fitting such graphical models [18], [22]. This problem naturally calls for a Bayesian formulation and significant progress has been made in recent years [4], [10], [14], [15], [22]. Another appealing aspect of the Bayesian framework is that it synthesizes all the available information on the parameter into a probability distribution for easy uncertainty quantification. However, most existing Bayesian methods for fitting graphical models do not scale well with the number of nodes in the graph. The main difficulty is computational, and hinges on the ability to handle interesting prior distributions on ==== when ==== is large.====The most commonly used class of priors distributions for Gaussian graphical models is the class of G-Wishart distributions [1]. However G-Wishart distributions have intractable normalizing constants, and become impractical for inferring large graphical models, due to the cost of approximating the normalizing constants [10], [15]. Following the development of the Bayesian Lasso of [20] and other Bayesian shrinkage priors for linear regressions [8], several authors have proposed prior distributions on ==== obtained by putting conditionally independent shrinkage priors on the entries of the matrix, subject to a positive definiteness constraint [14]. However, this approach does not give a direct estimation of the graph structure, which in many applications is the key quantity of interest. Furthermore, dealing with the positive definiteness constraint in the posterior distribution requires careful MCMC design, and becomes a limiting factor for large ====.====The above discussion suggests that when dealing with large graphical models, some form of approximation is inescapable. Building on [3], we propose a quasi-Bayesian approach for fitting large Gaussian graphical models using the pseudo-likelihood function that underpins the neighborhood selection method of Meinshausen and Bühlmann [17]. This choice gives a quasi-posterior distribution ==== that factorizes, and leads to a drastic improvement in the computing time needed for MCMC computation when a parallel computing architecture is used. We illustrate the method in Section 4 using simulated data where the number of nodes in the graph is ====.====The idea of replacing the likelihood function by a pseudo-likelihood function is well-known. We refer the reader to [29] for an in-depth discussion in the fixed-dimensional setting. The basic idea behind the use of pseudo-likelihood functions is to approximate a statistical model by a set of small-dimensional sub-models — typically constructed from conditional distributions. Each sub-model identifies only a small piece of the parameter of interest. A product of these sub-models is then used to identify the full-parameter. The idea is similar to inference by moment conditions [16], and is known to be a robust modeling approach, since only the sub-models are specified. In fixed-dimensional classical statistics, inference using pseudo-models is known to be consistent, with the same ==== convergence rate as the maximum full-likelihood estimator; see [29]. In this context, the price to pay for using a pseudo-model estimator is typically a larger asymptotic variance. In that sense, quasi-likelihood inference gives an approach to strike a better trade-off between robustness and computational tractability on one side, and statistical accuracy on the other.====We study the contraction properties of the quasi-posterior distribution ==== as ====. Under some restrictions on the true precision matrix, we show that ==== contracts in the spectral norm at the rate of ==== (see Theorem 1 and Eq. (14) for a precise statement), where ==== is the maximum degree in the un-directed graph defined by the true precision matrix. The condition on the sample size ==== for the results mentioned above to hold is ====, which shows that the quasi-posterior distribution can recover the true precision matrix, even in cases where ==== exceeds ====. The rate matches the frequentist rate of neighborhood selection obtained in [28]. A full likelihood inference of ==== yields the convergence rate of ====, where ==== is the number of non-zero entries of the true precision matrix. The full-likelihood rate was derived in the frequentist setting by Rothman et al. [25], and in the Bayesian setting by Banerjee and Ghosal [4]. Note that typically, ====. Hence, these rates seem to highlight an interesting high-dimensional phenomenon where a quasi-model converges at a faster rate than a full-likelihood inference, in addition to yielding computationally faster procedures.====The rest of the paper is organized as follows. Section 2 provides a general discussion of quasi-models and quasi-Bayesian inference. We specialized the discussion to Gaussian graphical models in Section 3. The theoretical analysis focuses on the Gaussian case, and is presented in Section 3, but the proofs are postponed to Section 5. The numerical experiments are presented in Section 4. A ==== implementation of the method is available from the author’s website.",Quasi-Bayesian estimation of large Gaussian graphical models,https://www.sciencedirect.com/science/article/pii/S0047259X18301891,18 May 2019,2019,Research Article,242.0
"Xu Kai,Hao Xinxin","School of Mathematics and Statistics, Anhui Normal University, Wuhu 241002, PR China","Received 1 November 2018, Revised 2 March 2019, Accepted 7 May 2019, Available online 17 May 2019, Version of Record 27 May 2019.",https://doi.org/10.1016/j.jmva.2019.05.001,Cited by (2),In this paper we consider the problem of ==== studies demonstrate that it has good size and power in a wide range of settings. A real data example is also considered to illustrate the efficacy of the approach.,"Along with the rapid development and widespread application of information technologies, analyses involving a large number of variables are becoming more prevalent in statistical applications. Particular attention has been paid to the estimation of a covariance matrix ====, as they play a major role in longitudinal data analysis [8], [27], risk management [21], and graphical modeling [9], [11] among others. The total number of parameters needed for specifying a covariance matrix of a multivariate vector with dimension ==== is ====. High dimensionality can significantly degrade the statistical efficiency of the usual sample covariance estimator, which makes interpretation difficult. Therefore, it is desired to select the covariance structure so that the number of parameters needing to be estimated is reduced and an easy interpretation can be obtained. As a consequence, there has been a flurry of recent activity in the literature on deriving test procedures for rather restrictive covariance structures. Examples include procedures for testing identity and sphericity [6], [17], [18], [20], [22], as well as diagonal structure [19], [25], [28]. An arguably more practically important problem is testing the block-diagonal structure of high-dimensional covariance matrix.====Let ====, ==== and ====, where ====. The covariance matrix of ==== is partitioned as ====so that ====, ==== and ====. Assume that we have a sample of ==== independent and identically distributed observations ==== from this distribution ==== and denote the sample covariance matrix by ====where ====. Accordingly, we consider partition of the sample covariance matrix given by ====Our interest is to test ====Significant advances of the sample testing problem have been achieved under the assumption of normality. Two typical test statistics are the likelihood ratio test [32, LRT] : ==== and the trace criteria [1]: ====. They are justified under a framework where the sample size ==== tends to infinity while the dimension ==== remains fixed. [15], [16] and [34] have examined these two conventional tests for high-dimensional normally distributed random vectors when ====
 (====). Recently, based on ==== and ====, [4] proposed several variance-type statistics for the hypothesis of a block diagonal covariance matrix when the dimension of the vectors increases with the sample size. Although ====, ==== and variance-type statistics can accommodate the “moderate ====, large ====” situations and are invariant under scalar transformations, i.e., ====, where ==== and ====s are nonzero constants, they still have the limitation that they cannot be applied to the case of ==== due to the singularity of ==== and ==== and do require the normality assumption.====To accommodate the “large ====, small ====” situations, [30] modified the trace criteria ==== and proposed ==== given by ====The SR statistic, extended by [14] to the case of two more sets of normally distributed statistical vectors, is based on the idea that under normality, ==== holds so that ==== as well as ==== should be small. The denominator of the SR statistic can be understood by replacing ==== and ==== by their estimators ==== for some ====. The numerator can be understood by replacing ==== by its estimator ====. However, direct substitution of ====, ==== and ==== by ====, ==== and ==== produces tests with Type I error rates that are much larger than the nominal levels when normality is violated. This is mainly due to bias from estimating some functions of covariance matrix in high dimension under non-normality [13]. To solve the problem, [33] proposed a novel test under non-normality. [33] noticed that for (2), a distance measure between ==== and ==== is ====, which is non-negative and zero if and only if the null hypothesis ==== holds. A test can be constructed via estimating the distance measure. Specifically, based on an unbiased estimator of ====, they considered the test statistic given by ====where ====, ====, ==== with ====, and ==== with ====.====Both the SR-test and YHN-test are based on an empirical distance between the full and a block diagonal covariance matrix and are invariant under an orthogonal transformation, i.e., invariant under ==== where ==== is a nonzero constant, ==== is a ====-dimensional constant vector and ==== is an orthogonal transformation, i.e., ==== where ==== is an identity matrix. Intuitively speaking, the power of these statistics would heavily depend on the underlying variance magnitudes since they do not use the information from the diagonal elements of the sample covariance, i.e., the variances of each variables. When all the components are homogeneous, they would be very powerful, whereas their superiority would be highly affected if the component variances differ much. Thus, under such specific situations, scalar transformation invariant tests may have advantage over orthogonal transform invariant tests from the consideration of power. On the other hand, [30] also proposed a test (henceforth SRs-test) which has the property of scalar transformation invariance and is applicable in the “large ====, small ====” paradigm. More accurately, ====where ====and ====. Simulation results from [30] imply that the SRs-test obtains more power than the SR-test when ==== is a diagonal matrix except identity and sphericity, and the data are normal. However, the SRs test has two limitations. One is the normality assumption and the other is that [30] used the following conditions: ====for the derivation of asymptotic distribution of SRs-test where ====is the population correlation matrix. Such conditions specify explicitly the growth rate of the dimension relative to the sample size and can remove the case of ==== for some ====. Exploring the literature, we also find a related work by [3] who constructed the so-called Schott type statistic for (2) by generalizing Schott’s statistic for complete independence test in [25]. However, the test in [3] also has two limitations. One is the normality assumption and the other is that its limiting behavior was derived with the following restriction on the dimensionality and the sample size, i.e., ====
 (====).====In this paper, we will develop a correction that makes the SRs-test statistic robust against both the data distribution and the data dimension, and introduce a new scalar transformation invariant test. We show that the proposed test statistic is asymptotically normal without the normality assumption and without specifying an explicit relationship between the dimension and the sample size. The rest of this paper is organized as follows. Section 2 introduces the new test statistic and the assumptions, gives the asymptotic normality of the proposed test statistic and discusses power property of the proposed test. Both simulation results and real data illustration are conducted in Section 3 to investigate the numerical performance of the proposed test. The article concludes with a short discussion in Section 4. All the technical proofs are gathered in Appendix.",A nonparametric test for block-diagonal covariance structure in high dimension and small samples,https://www.sciencedirect.com/science/article/pii/S0047259X18305785,17 May 2019,2019,Research Article,243.0
Slaoui Yousri,"Université de Poitiers, France","Received 15 January 2019, Revised 29 April 2019, Accepted 29 April 2019, Available online 17 May 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.jmva.2019.04.009,Cited by (13),"We propose and investigate a new kernel ==== based on the minimization of the mean squared relative error. We study the properties of the proposed recursive estimator and compare it with the recursive estimator based on the minimization of the mean squared error proposed by Slaoui (2018). It turns out that, with an adequate choice of the parameters, the proposed estimator performs better than the recursive estimator based on the minimization of the mean squared error. We illustrate these theoretical results through a real chemometric dataset.","Functional data have become more and more popular in modern statistics because of the progress in computing technology, in terms of both memory capacity and computing speed which have made it possible to record vast amounts of data. It concerns many statistical methods dealing with random variables valued in some infinite-dimensional space, called functional variables. Thus, a very large number of variables can be observed for the study of the same phenomenon. This type of data appears in several fields including climatology, economics, psychology, linguistics, medicine, and so on.====There has been an increasing interest in Functional Data Analysis (FDA) in the past decades, as it is highlighted by the popular monograph of Ramsay and Silverman [43], who provide a detailed exposition of both theoretical and practical aspects of functional data analysis. Statistical inference for FDA has been widely investigated (see, e.g., [28], [29]). The existing literature contains numerous studies on functional linear models (see, e.g., [7], [8], [26]). The nonparametric treatment has been popularized by the book of Ferraty and Vieu [23] and now takes a large place in the FDA literature; see the discussions in the recent surveys by Cuevas [13] who offers a short tutorial as well as a partial survey of the FDA theory. We point also to the work [36] which gives a survey of nonparametric FDA and presents a wide scope of open questions. Aneiros et al. [2] present various contributions into two categories: papers promoting new methodology for data varying over a continuum, and papers concerned with very high- but finite-dimensional problems.====In the last decade, data streams have become an increasingly important area of research. Common data streams include Internet packet data, Twitter activity, Facebook news stream, and credit card transactions. In those situations, data arrives regularly so that it is impossible to store them in a traditional database. In such a context, building a recursive estimator which does not require to store all the data in memory and can be updated easily in order to deal with online data is of great interest.====This work concerns nonparametric recursive estimation of the regression operator when the explanatory data are curves and the response is real-valued, based on the minimization of the Mean Squared Relative Error (MSRE). This problem can be formulated by considering ==== a sample of independent and identically distributed couples, where ==== is real-valued and ==== takes values in some functional space ==== equipped with a semi-norm ====. A common nonparametric model of this relationship is for ==== given by ====where ==== is a random variable. In ordinary predictions, we estimate the operator ==== by minimizing the expected squared loss function: ====and we obtain as predictor the quantity ====, which gives the Mean Squared Error (MSE). However, when ==== or in the presence of outliers, the use of the classical loss function can lead to unreasonable results, since all variables have the same weight. Therefore, it is of interest to consider the response level ==== rather than ==== and then it is necessary to consider the MSRE: ====Minimizing this loss function leads to predicting the quantity (see Park and Stefanski [40]) ====By assuming that ====, for ====, we define the regression functional as ====Moreover, we set ====, for all ====, ====, and ====. Then, we have the following relation, ====.====The purpose of this study is to extend the work proposed first in the case of a real explanatory variable in Slaoui [46] and then in the case of a functional explanatory variable in Slaoui [47]. The two previous estimators are based on the minimization of the MSE. In the current work we propose to use the MSRE rather than the MSE criterion. Thus, our proposed estimator is: ====with ====where ==== and ==== are sequences of positive real numbers which are converging to ====, ==== is a kernel and ====. The recursive property (1) is particularly useful for large samples, because ==== can be updated easily using additional observation. Throughout this paper, we suppose that ====, for ====; and we let ====. Then, we can estimate the operator ==== by ====Despite that the MSRE is widely used in practice as a measure of performance, the theoretical properties of this alternative regression were not frequently used until the work of Narula and Wellington [39]. Since this work, we can find some criteria based on minimizing the sum of Absolute Relative Errors (ARE) and others on the sum of squared relative errors (SRE). For example, see [33] for some models in software engineering, [9] for some examples in medicine or [10] for some financial applications. Note that most of the recent methods are concerned with the estimation of unknown parameters (see, [49] for recent advances and references).====In the framework of nonparametric estimations, Jones et al. [31] studied the asymptotic properties of an estimator minimizing the sum of the squared relative errors by considering the kernel estimation methods and a local linear approach. Moreover, in the functional framework, only the paper by Demongeot et al. [16] has paid attention to the study of nonparametric prediction via relative error regression. As far as we know, there is no work on recursive estimators in the functional nonparametric literature based on the minimization of the MSRE. The works of Amiri et al. [1] and Slaoui [47], both proposed recursive estimation of the operator ==== based on the minimization of the MSE.====Furthermore, the functional bootstrap literature is not widely developed. Politis and Romano [41] developed some weak convergence results for approximating sums of weakly dependent stationary Hilbert space valued random variables under the asymptotic validity of a stationary bootstrap method. Cuevas et al. [14] presented a Monte Carlo study analyzing the performance of the bootstrap confidence bands of several functional estimators. Raña et al. [42] considered a naive and a wild bootstrap procedure to construct pointwise confidence intervals for a nonparametric regression function when the predictor is of functional nature and when the data are dependent. More recently, Aneiros et al. [3] proposed a naive and a wild bootstrap procedure to approximate the distribution of kernel-based estimators under ====-mixing conditions, whereas Shang [45] considered bootstrap methods for estimating the long-run covariance of stationary functional time series.====The first purpose of this paper, is to study, under some general conditions, the asymptotic properties of an alternative functional recursive kernel estimator of the regression operator ====. Secondly, we propose an automatic bandwidth selection through a wild bootstrap method. Thirdly, we compare the performance of the proposed recursive estimator ==== based on the minimization of the MSRE to the recursive functional regression estimator introduced by Slaoui [47], the latter is based on the minimization of the MSE and defined as ====The layout of the present paper is as follows. Section 2 is devoted to the main results of the present work. Section 3 is dedicated to application results by using a real dataset. We conclude the article in Section 4. To avoid interrupting the flow of this paper, all mathematical developments are relegated to the Appendix.",Wild bootstrap bandwidth selection of recursive nonparametric relative regression for independent functional data,https://www.sciencedirect.com/science/article/pii/S0047259X19300363,17 May 2019,2019,Research Article,244.0
"Ho Zhen Wai Olivier,Dombry Clément","Laboratoire de mathématiques de Besançon, UMR 6623, Université Bourgogne Franche-Comté, 16, route de Gray, 25030 Besançon, France","Received 26 December 2017, Revised 25 April 2019, Accepted 25 April 2019, Available online 11 May 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.04.008,Cited by (5),"We revisit multivariate extreme-value theory modeling by emphasizing multivariate regular variation and a multivariate version of Breiman’s Lemma. This allows us to recover in a simple framework the most popular multivariate extreme-value distributions, such as the logistic, negative logistic, Dirichlet, extremal- ","Regular variation is a fundamental notion in extreme-value theory that was widely popularized by Resnick [23]. As a simple illustration of the importance of regular variation in univariate extreme-value theory, consider an independent and identically distributed (i.i.d.) sequence ==== of positive random variables with cumulative distribution ====. For every integer ====, let ==== be the quantile of order ==== of ====. Then the following statements are equivalent.====The equivalence (i)–(ii) dates back to Gnedenko [10], the equivalence (ii)–(iii) is due to Balkema and de Haan [1], and the equivalence (i)–(iv) can be found in Resnick [23]. As will be reviewed in Section 2.1, a similar result holds in the multivariate setting and multivariate regular variation is crucial in multivariate extreme-value theory.====Historically, multivariate extreme-value theory has been developed by considerations on the asymptotic behavior of i.i.d. random vectors. Key early contributions are the papers by Tiago de Oliveira [22], Sibuya [27], de Haan and Resnick [12], and Deheuvels [8]. The general structure of multivariate extreme-value distributions has been characterized by de Haan and Resnick [12] in terms of the so-called spectral representation. Domains of attraction have been characterized by Deheuvels [8], who highlighted the convergence of the dependence structure to an extreme-value copula. Since then, a rich literature has emerged on modeling and statistical aspects of the theory, of which a nice recent review from the copula viewpoint is provided by Gudendorf and Segers [11].====More recent developments focus on exceedances over high threshold in a multivariate setting and the so-called multivariate generalized Pareto distributions. Seminal papers in that direction are those of Coles and Tawn [6] and Rootzén and Tajvidi [26]. Further recent development on modeling and statistical aspects include Rootzén et al. [25] and Kiriliouk et al. [18]. Typical applications, as developed in Section 6 of the latter reference, are the modeling of extreme log-returns of assets in finance or of extreme rainfalls in environmental science.====In this framework, the motivation of the present paper is twofold. In the first part, corresponding to Section 2, we revisit multivariate extreme-value theory models with emphasis on regular variation and the limiting homogeneous measure. More precisely, a multivariate extension of the celebrated Breiman Lemma due to Davis and Mikosch [7] allows us to construct a regularly varying random vector as a product of a heavy tailed random variable, regarded as a radial component, and a sufficiently integrable random vector interpreted as a spectral component. The limiting homogeneous measure is easily characterized and, for specific choice of the spectral component, we recover standard parametric models from multivariate extreme-value theory such as the Hüsler–Reiß [15], extremal-==== [21], logistic, negative logistic or Dirichlet models [6]. We believe that emphasizing the exponent measure is important since it is the fundamental notion that unifies maxima, exceedances or point processes approaches in extreme-value theory. In contrast, from the copula point of view, the multivariate Breiman Lemma provides a general framework for deriving extreme-value copula models closely related to the results by Nikoloulopoulos et al. [21] or Belzile and Nešlehová [3].====The second part of the paper, which corresponds to Sections 3 The Hüsler–Reiß Pareto model, 4 The generalized Hüsler–Reiß Pareto model, proposes a thorough study of the so-called Hüsler–Reiß Pareto model, i.e., the exceedance Pareto model associated with the max-stable Hüsler–Reiß model [15]. The exceedances of the related Brown–Resnick spatial model were considered recently by Wadsworth and Tawn [29], who proposed inference via censored maximum likelihood; see also Kiriliouk et al. [18]. We here focus on the finite-dimensional multivariate Hüsler–Reiß Pareto model and notice that it has a simple exponential family structure [3] which seems to have been overlooked in the literature. We propose in Section 3 an extensive study of this exponential family structure and consider also maximum likelihood inference as well as exact simulation. We extend these results in Section 4, where we consider the non-standard Hüsler–Reiß Pareto model that incorporates different tail parameters for the different margins. Maximum likelihood estimators are shown again to be asymptotically normal and an alternating optimization procedure is considered. To conclude, we propose a maximum likelihood ratio procedure for testing the equality of the different marginal tail parameters. The results are illustrated in simulation experiments presented in Section 5, where we assess the finite-sample properties of the maximum likelihood estimators. Most of the proofs are collected in Section 6.====Throughout this article, we denote by ==== the max-norm on ==== and by ==== an arbitrary norm, ==== is a ==== vector with all components equal to ====. Operations on vectors are usually meant component-wise. The component-wise maximum of vectors is denoted ====, the comparison of vectors ==== is meant component-wise so that ==== means that some component of ==== is larger than the corresponding component of ====. For ====, let ==== denote the cube ==== and ====.",Simple models for multivariate regular variation and the Hüsler–Reiß Pareto distribution,https://www.sciencedirect.com/science/article/pii/S0047259X17307984,11 May 2019,2019,Research Article,245.0
"Yang Baoying,Yin Xiangrong,Zhang Nan","Department of Statistics, College of Mathematics, Southwest Jiaotong University, Chengdu, Sichuan 611756, China,Department of Statistics, 319 Multidisciplinary Science Building, University of Kentucky, Lexington, KY 40536, USA,Department of Statistics, 204 Statistics Building, 101 Cedar Street, University of Georgia, Athens, GA 30602, USA","Received 17 November 2018, Revised 19 April 2019, Accepted 20 April 2019, Available online 29 April 2019, Version of Record 9 May 2019.",https://doi.org/10.1016/j.jmva.2019.04.006,Cited by (6),"We propose two sufficient variable selection procedures, i.e., one- and two-stage approaches using independence measures for continuous response, illustrated by distance correlation and the Hilbert–Schmidt Independence Criterion correlation. We show the advantages of the proposed procedures over some existing marginal screening methods through simulations and a real data analysis. Our procedures are model-free and thus robust against model mis-specification. They are particularly useful when some active predictors are marginally independent of the response.","Variable selection has become increasingly important in various research fields, as data are being collected at a relatively low cost due to modern technology. Many methods have been proposed over the last two decades, such as the least absolute shrinkage and selection operator (Lasso) [37], the smoothly clipped absolute deviation (SCAD) [10], and the Dantzig selector [2]. These methods have shown promise in dealing with high-dimensional data.====For ultrahigh-dimensional data, however, Fan and Lv [11] pointed out that the aforementioned methods have limitations due to the challenges of computational cost, statistical accuracy, and algorithmic stability. These concerns led to the sure independent screening (SIS) method in [11] for ultrahigh-dimensional data. The SIS method is based on the marginal Pearson correlation learning and is designed for linear regressions with Gaussian predictors and responses. SIS not only can speed up variable selection drastically but can also improve the estimation accuracy when dimensionality is ultrahigh. Many other methods have been developed in recent years, following SIS with specified models, both parametric and semi-parametric; see, e.g., [3], [4], [9], [12], [13], [24], [33]. However, specifying a correct model for ultrahigh-dimensional data may be challenging.====As the aforementioned model-specific screening procedures may not be robust to model mis-specification, model-free sure screening procedures have been developed; see, e.g., [1], [8], [19], [20], [21], [23], [25], [26], [32], [41]. Fan and Lv [11] pointed out that the marginal screening procedure may miss some active predictors that are marginally independent of the response, and they proposed iterative sure independence screening (ISIS) to overcome the problem. Although this idea has been empirically demonstrated in [9], [13], [23], [41], its theoretical justification still remains unclear. Mai and Zou [25], [26] discussed the subtle difference between variable selection and variable screening: the former uses fine methods to exactly select the active set of predictors, while the latter uses rough but fast methods to select a set containing the active set of predictors. The existing variable screening methods may not always select such a set, though they often work in practice. This motivates us to explore new procedures to overcome the drawback of existing screening approaches, and to seek theoretical guarantees that they select a set containing all active predictors.====In this paper, focusing on continuous response, we propose two new sufficient variable selection approaches based on theoretical results from the sufficient dimension reduction literature. These two new approaches translate conditional independence in sufficient variable selection to alternative measures of independence. The independence statistic is illustrated by distance correlation [35], [36] and Hilbert–Schmidt Independence Criterion correlation [15]. Although fine statistical tests could be developed for these procedures, we only use the screening approach for the purpose of sufficient variable selection as it is fast and cost-efficient, even though the selected set may be larger than the set of active predictors. Our approach is model-free. Thus, it is robust against model mis-specification, which is an attractive property in practice. Also, our methods allow for arbitrary regression relationships, which makes them more effective than the model-specific marginal approaches. More importantly, our proposed procedures are advantageous when some active predictors are marginally independent of the response.====The rest of this paper is organized as follows. Section 2 describes both distance correlation and Hilbert–Schmidt Independence Criterion correlation for sufficient variable screening. Section 3 reports some of their theoretical properties, while Section 4 contains simulation studies and a real data application. A short discussion follows in Section 5. Related proofs of theorems and additional simulations can be found in the Online Supplement.====Throughout this paper, we assume that ==== is a univariate or multivariate response variable, and ==== is a ==== vector. The notation ==== means that ==== and ==== are independent given ====.",Sufficient variable selection using independence measures for continuous response,https://www.sciencedirect.com/science/article/pii/S0047259X18306079,29 April 2019,2019,Research Article,246.0
"Chen Jia,Li Degui,Xia Yingcun","Department of Economics and Related Studies, University of York, YO10 5DD, United Kingdom,Department of Mathematics, University of York, YO10 5DD, United Kingdom,Department of Statistics and Applied Probability, National University of Singapore, 117546, Singapore,School of Mathematical Sciences, University of Electronic Science and Technology of China, 610054, China","Received 15 November 2017, Revised 18 April 2019, Accepted 18 April 2019, Available online 26 April 2019, Version of Record 8 May 2019.",https://doi.org/10.1016/j.jmva.2019.04.005,Cited by (1),None,"Nonparametric panel data models have received increasing attention in the past two decades [16], [17], [25], [34], [35]. When the dimension of explanatory variables in nonparametric panel regression is large, to circumvent the “curse of dimensionality”, some nonparametric and semiparametric modeling techniques, such as functional-coefficient models, additive models, partially linear models and single-index models, have been extensively studied in the literature [3], [5], [12], [24], [30]. In this paper, we consider functional-coefficient panel data models as they are a natural generalization of classical linear regression models and provide a flexible framework for depicting the relationship between the response and explanatory variables. A detailed introduction on estimation and inference of functional-coefficient models with independent or weakly dependent data can be found in [2], [6], [13], [14], [31] and the references therein. In this paper, we do not impose any restriction about the serial correlatedness of the model error terms and allow for arbitrary serial correlation and heteroscedasticity to give our model and method wider applicability.====Consider a set of panel data ==== with ==== and ====, where ==== is the response variable of interest, ==== is a ====-dimensional vector of explanatory variables whose first element is 1 and ==== is a univariate random variable. The variable ==== can be chosen as calendar time or some other index variable in practical applications. The functional-coefficient panel model is defined, for ==== and ====, by ====where ==== is a ====-dimensional vector of functional coefficients, ==== is the random error that is cross-sectionally independent but serially correlated and satisfies ====almost surely (a.s.), where ====, ==== and ====. Throughout the paper, we annotate the true parameters and functions of the data generating process with ==== in their subscripts.====In this paper, we consider short panels, where the number of cross-sectional units ==== is large, but the number of time series observations ==== is fixed and relatively small and may vary across units. Our main interest is to estimate the functional coefficients ==== in model (1). However, when the dimension ==== is large or even moderately large, the nonparametric estimation of the functional coefficients is unstable. Therefore, an appropriate dimension reduction approach needs to be employed to reduce the number of nonparametric coefficient functions to be estimated. Motivated by [20], we achieve this dimension reduction by extracting the principal components of the functional coefficients. More specifically, we assume that there exist a vector of functions ====, a ====-dimensional vector of parameters ==== and a ==== (with ====) matrix of parameters ==== in which ====, for each ====, is a ====-dimensional column vector, such that ====We call ==== the principal functional coefficients since they can be seen as the principal functions of ====. The positive integer ==== is usually unknown in practice but typically much smaller than ====. This integer can be estimated by a simple ratio criterion [21] given in Section 5.1. We call model (1) with the imposition (3) a rank-reduced functional-coefficient panel data model. By imposing the structure (3) on the functional coefficients ====, we reduce the number of nonparametric functions from ==== to ====.====In practice, economic panel data are often found to be serially correlated. There are two possible ways to deal with serial correlation in econometric analysis. The first is to treat it as a result of model misspecification and then adjust the model accordingly to eliminate serial correlation, e.g., by using a dynamic model instead of a static model. The second is to correct for serial correlation in estimation and statistical inference directly without changing the model specification. In the present paper, we will take the second approach.====In the presence of serial correlation, a direct application of the estimation procedure proposed by Jiang et al. [20], which ignores the serial correlation, would certainly affect the efficiency of functional coefficients estimation. To account for serial correlation, some modified nonparametric and semiparametric methods have been introduced in the statistics literature, making certain functional transformation or nonparametric/semiparametric estimation of the serial covariance matrices ==== [11], [23], [26], [36]. In this paper, we adjust for serial correlation by using a Cholesky decomposition on the serial covariance matrices ==== to obtain a transform of the original panel data model so that the errors of the transformed model are free from serial correlation. We then apply a semiparametric estimation method to the transformed model to obtain estimates of ====, ==== and ====. To the best of our knowledge, this paper is among the first to combine the rank-reduced structure on the model functional coefficients and the Cholesky decomposition on the serial covariance matrices in the estimation methodology and systematically study the relevant asymptotic properties. Under some regularity conditions, we establish the asymptotic distribution theory for the proposed semiparametric estimators. In particular, we show that using the rank-reduced structure on the functional coefficients and the Cholesky decomposition on the serial covariance matrices can improve the estimation efficiency of the principal functional coefficients and thus that of the functional coefficients when the serial covariance structure is correctly specified up to a constant multiple.====However, the true serial covariance structure is usually unknown, and its misspecification could lead to efficiency loss. Hence, we introduce two different approaches for consistent estimation of the lower triangular matrix in the Cholesky decomposition for balanced and unbalanced panel data. By using these consistent estimates, we attain the same efficiency gain as when the serial covariance matrices are correctly specified. The simulation studies show that the developed semiparametric approach works reasonably well in finite samples.====The functional-coefficient panel modeling framework (1) can cover a special case where the univariate index variable is time-invariant. In this case, for each ====, ====, the functional-coefficient panel data model (1) becomes ====and the moment conditions in (2) become ====where ==== is the time-invariant index variable. The estimation methodology developed in our paper is applicable to the above model setting. In fact, model (4) is of interest for economic panel studies where the relationships under study are cross-sectionally heterogeneous but do not vary significantly over time, as in the econometric growth study we provide in Section 6. In this empirical study, the index variable is chosen as the income level of a country at the beginning of the sampling period (i.e., 1986), as previous research (e.g., [4]) has found that differences in initial conditions can account for a large amount of the variation in the effects of various economic factors on growth. By using the proposed approach, two principal functional coefficients are identified out of the 10 functional coefficients. Furthermore, our method is shown to have a better out-of-sample prediction performance than the one that does not employ the rank-reduced structure or the one that ignores the serial correlation in the data.====The rest of the paper is organized as follows. In Section 2, we introduce identification conditions and two semiparametric estimation procedures. In Section 3, we present the asymptotic theory for the proposed estimators. In Section 4, we consider estimation of the lower triangular matrix in the Cholesky decomposition for balanced panel data, and the serial covariance matrices for unbalanced panel data. In Section 5, we discuss methods to obtain initial parameter estimates and determine the value of ==== and conduct Monte Carlo simulation studies. Section 6 gives an empirical application to panel data on economic growth. Section 7 concludes the paper. Proofs of the asymptotic theorems are given in Appendix A.",Estimation of a rank-reduced functional-coefficient panel data model with serial correlation,https://www.sciencedirect.com/science/article/pii/S0047259X17306784,26 April 2019,2019,Research Article,247.0
"Kong Yinfei,Li Yujie,Zerom Dawit","Department of Information Systems and Decision Sciences, California State University, Fullerton, CA, USA,Shandong University of Finance and Economics, China","Received 11 April 2019, Revised 19 April 2019, Accepted 20 April 2019, Available online 25 April 2019, Version of Record 4 May 2019.",https://doi.org/10.1016/j.jmva.2019.04.007,Cited by (2),We propose a variable importance measure called partial ,"Suppose we have mutually independent observations ====, where for each ====, ==== is a continuous response or outcome variable and ==== is a vector of ====-dimensional variables. It will be assumed that categorical factors have already been converted to 0–1 dummy-code variables. Given observed data, our goal is to quantify the association of ==== with ==== when the nature of the association of the variables evolves across the conditional quantiles of ====. To this end we consider, for given ====, the ====th linear quantile regression model defined, for all ====, by ====where ==== and ==== satisfies ====. We consider the case where the number ==== of variables is allowed to increase with the sample size ====, and ==== can also be high-dimensional, i.e., ====. We denote the full model by ==== and its size by ====. Although ==== can be very large, we assume quantile sparsity which means that only a small subset of the variables are associated with the ====th conditional of ==== given ====. Sparsity may appear restrictive, but by considering several quantiles ====, one may be able to identify a large number of relevant variables that are truly associated with the various parts of the conditional distribution of the response variable of interest. We denote by ==== the number of true variables. Both ==== and the set of true variables (or the true model) ==== are unknown. Note that ====. Although ==== and ==== may vary with ====, we suppress their dependence on ==== for notational simplicity.====When the aim is identifying the true model ==== in the presence of a large number of variables, several novel regularization approaches have been proposed. Among others, these include penalized quantile regression approaches using the LASSO penalty (see, e.g., [1], [19]) and those using non-convex penalty such as SCAD (see, e.g., [28], [29]). In addition to regularization methods and when faced with ultra-high dimensional variables (the case where ==== may grow at an exponential rate of ====), marginal screening methods have also become increasingly popular. Following the pioneering work of Fan and Lv [5] that provided the theoretical framework for justifying marginal screening, a variety of marginal screening methods for different model settings have been introduced; see, e.g., [4], [6], [7], [14], [18], [20], [31]. The rationale for these marginal screening methods is to reduce the ultra-high dimension into a moderately high dimension so that regularization methods can be applied on the reduced set. Although not as extensive as in the usual regression (conditional mean) context, marginal screening methods have also been extended to quantile regression. To measure the relative importance of variables in the quantile setting, He et al. [9] suggested using nonparametrically estimated marginal quantile functions. Bypassing estimation of marginal quantile functions, Wu and Yin [30] also introduced a computationally appealing marginal quantile utility (MQU) metric. An important feature of both [9] and [30] is that they enjoy the sure independence screening property, originally defined in [5], in the sense that the screened set of variables contains the true set with overwhelming probability.====For the linear quantile regression setting in (1), we propose an effective and computationally simple variable screening/selection tool that can be used as a valuable alternative to existing approaches. In particular, we make three main contributions, as described below.====First, by generalizing MQU in [30] that deals only with marginal relationships, we introduce a criterion called partial quantile utility (PQU) to measure the relationship between a candidate variable and the ====th conditional quantile of ==== given existing variables. A closely related measure called quantile partial correlation is also proposed in [17] and later adopted by [21] for variable selection. Although both the PQU and the quantile partial correlation share the same purpose, the use of the latter is limited to continuous variables. In contrast, partial quantile utility applies to both continuous and dummy-code variables making it more generally applicable.====As a second major contribution, following [27], we propose a quantile forward regression (QFR) method to conduct variable screening by adopting the proposed PQU as a criterion to rank candidate variables. Theoretically, we show that QFR enjoys the sure independence screening property even when ==== grows at an exponential rate of ====. Although marginal screening approaches such as those in [9] and [30] have also been shown to be powerful in the sense of simplifying ultra-high dimensional problems while maintaining the sure screening property, they suffer from two major weaknesses. First, an important variable that is marginally unimportant at some ==== but jointly important cannot be picked up, and hence will not enter the screened set (Weakness 1). Second, some unimportant variables that are correlated (even moderately) with important variables can have high priority to be selected than other important variables that are correlated (albeit weakly) with the ==== conditional quantile response (Weakness 2). By design, QFR can mitigate the above weaknesses. Although not in the forward regression manner, He et al. [9] also remarked that their marginal screening metric can still be used in an iterative fashion following [5] to achieve the same goal of QFR.====Our third major contribution, guided by QFR screening and adopting the Quantile Bayesian Information Criterion (QBIC) in [16], is to introduce a direct (non-penalized) variable selection method to further reduce the QFR screened set. We show that this significantly reduced set also enjoys the sure independence screening property. As an alternative selection strategy, one may also apply standard penalization approaches [1], [19], [28], [29] on the QFR screened set.====To numerically assess the finite-sample performance of the proposed QFR-based screening and selection, we conduct an extensive simulation experiment using two data generating models. The first model is an elaborate simulation design that is meant to highlight the screening advantage of the proposed QFR over marginal screening approaches. In our comparative investigation, we consider the marginal screening methods in [9], [30] which are referred to as QaSIS and QSIS, respectively. The findings show that the proposed QFR screening is able to correctly identify the jointly but not marginally important variables. The second model introduces a variable that has a heterogeneous association with the response as another design feature. For such scenario, QFR screening is also able to correctly identify the heterogeneous variable with overwhelming frequency. With regard to variable selection, we find that direct as well as penalized selection methods (guided by prior QFR-screening) offer excellent performances in both simulation designs.====The established link between child stunting (or stunted growth) and adverse long-term outcomes, as well as the relative ease of measuring child height (versus, say, keeping a comprehensive food diary for a child) has led to a widespread use of height as a marker of child malnutrition [11]. In order to identify potential risk factors of childhood malnutrition in India, Fenske et al. [8] and Koenker [13] used quantile regression by focusing on the 10% conditional quantile of heights. With the goal of offering yet another modeling option, in addition to boosting in [8] and LASSO-based penalization in [13], we apply our proposed method to re-examine the problem of selecting important childhood malnutrition risk factors. In particular, we investigate the 10% quantile of children height (the response variable) conditional on several risk factors. In the empirical example, 23 of the 29 potential risk factors are 0–1 dummy-code variables. The proposed QFR screening and selection are well suited to this data analysis as the proposed PQU is designed for both continuous and dummy-code variables. Using the direct QFR guided selection, we have identified six important variables (with at least 93% selection rate among 100 random partitions): age of the child, length of breast feeding, years of education of the mother, if a child is the fifth born, the sex of the child and if the household has electricity. Therefore, girls tend to be shorter than boys, children later in the birth order and particularly those born fifth are shorter than their older siblings and children from households with electricity tend to be taller. Interestingly, child’s mother education level is negatively associated with child’s childhood malnutrition.====The rest of the paper is organized as follows. In Section 2, we introduce the partial quantile utility (PQU) ranking criterion. Section 3 provides a detailed account of the proposed screening and selection methods that rely on PQU. In Section 4, we present the theoretical properties for the proposed methods. In Section 5, we report screening, selection and prediction performances using simulated data. In Section 6, we present an empirical application with the goal of identifying risk factors that are associated with acute childhood malnutrition. Section 7 concludes.",Screening and selection for quantile regression using an alternative measure of variable importance,https://www.sciencedirect.com/science/article/pii/S0047259X19302040,25 April 2019,2019,Research Article,248.0
"Robin Geneviève,Josse Julie,Moulines Éric,Sardy Sylvain","CMAP, École Polytechnique, route de Saclay, 91128 Palaiseau Cedex, France,XPOP, INRIA, 1 rue Honoré d’Estienne d’Orves, Bâtiment Alan-Turing, Campus de l’École Polytechnique, 91120 Palaiseau, France,Section de mathématiques, Université de Genève, 2-4, rue du Lièvre, CH 1211, Genève 4, Switzerland","Received 2 October 2018, Revised 15 April 2019, Accepted 15 April 2019, Available online 22 April 2019, Version of Record 4 May 2019.",https://doi.org/10.1016/j.jmva.2019.04.004,Cited by (9), package ==== on the Comprehensive Archive Network (CRAN).,"Let ==== be an ==== observation matrix of counts. Let also ==== and ==== be matrices containing row and column covariates, respectively. In our ecological application in Section 6, the rows of the contingency table represent ecological sites, and the columns represent years. For ====, ==== is the number of waterbirds counted at site ==== during year ====. The row feature ====, ====, embeds geographical information about site ==== (latitude, longitude, distance to coast, etc.) while the column feature ====, ====, codes meteorological characteristics of year ==== (precipitation, etc.). In addition, some entries of ==== are missing. For example ecological sites are sometimes inaccessible because of meteorological or political conditions, and therefore cannot be counted.====Frequency tables of this sort are often analyzed using low-rank models [11], [16], [17], [21], [23], [24], imposing a low-rank structure to an underlying parameter matrix. We assume a probabilistic framework with independent entries ==== following a Poisson model, viz. ====and focus on the estimation of ==== based on a low-rank assumption. The generalized additive main effects and multiplicative interaction model, or row–column model (see, e.g., [16], [21]), in which ====is adequate for the purpose. In this model, ==== is an offset, the terms which only depend on the index of the row or column (==== and ====) are called main effects, and the terms which depend on both (here ====) are called interactions; see p. 87 in Section 4.1.2 of [26].====To incorporate covariates in this framework, a natural idea is to express the row and column effects ==== and ==== as regression terms on the covariates. In other words, for ====, ====, ==== and ====, ====Such an extension is useful in practice for two main reasons. First, estimated covariates coefficients (and in particular their signs) can be used to determine whether the studied covariates have positive or negative effects on the counts; this is particularly useful in ecology to check whether meteorological, geographical or political conditions favor or endanger species. Second, when the proportion of missing values is large, which is often the case in bird monitoring, incorporating (relevant) covariates can improve the imputation significantly.====Models related to (3) have been considered for statistical ecology applications in [5], [6]. However, to the best of our knowledge, their theoretical and empirical properties have not been thoroughly studied. In contrast, the literature on convex low-rank matrix estimation is abundant and benefits from a substantial theoretical background, although few software packages with ready-made solutions are available for practitioners, and applications for count data outside image analysis [8], [37], [42] and recommendation systems [22] have not been attempted. The purpose of this paper is to develop a complete methodology for the inference of model (3), bridging the gap between convex low-rank matrix completion and model-based count data analysis.====After detailing related work in Section 1.1, we introduce in Section 2 a general model which includes (3). We propose an estimation procedure through the minimization of a data fitting term penalized by the nuclear norm of the matrix ====, which acts as a convex relaxation of the rank constraint. Building up on existing results on nuclear norm regularized loss functions, we derive statistical guarantees in Section 2.1. In particular, we provide an upper bound for the Frobenius norm of the estimation error. In Section 3, we propose an optimization algorithm, and two methods to choose the regularization parameter automatically.====We provide a simulation study in Section 4 revealing that LORI outperforms state-of-the-art methods when the proportion of missing values is large and the interactions are of significant order compared to the main effects. In Section 5, we show on plant abundance data with side information how the results of our procedure can be interpreted through visual displays. In particular, the arising interpretation is consistent with known results from the original study [10]. In Section 6, we use LORI to analyze a water-birds abundance data set from the French national agency for wildlife and hunting management (ONCFS).====The proofs of the statistical guarantees are postponed to the Appendix, and the method is available as an ==== package [40] called ==== (LOw-Rank Interaction) on the Comprehensive ==== Archive Network (CRAN) at ====.",Low-rank model with covariates for count data with missing values,https://www.sciencedirect.com/science/article/pii/S0047259X18305141,22 April 2019,2019,Research Article,249.0
"Kalogridis Ioannis,Van Aelst Stefan","Departement Wiskunde, KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium","Received 2 August 2018, Revised 10 April 2019, Accepted 10 April 2019, Available online 16 April 2019, Version of Record 30 April 2019.",https://doi.org/10.1016/j.jmva.2019.04.003,Cited by (22),". Their influence function is also studied. Simulation experiments show that the proposed estimators have reasonable efficiency, protect against outlying observations, produce smooth estimates, and perform well in comparison to existing approaches.","Nowadays practitioners frequently observe data that may be described by mutually independent and identically distributed pairs ====, where for each ====, ==== is a scalar random variable and ==== is a continuous-time stochastic process defined on some compact interval ====. The relationship between ==== and ==== is often of interest and the functional regression model extends the standard multiple regression model to this case, so that ====where ==== and ====, for some function space ====, are unknown quantities that need to be estimated from the data. The random errors ==== are assumed to be mutually independent and identically distributed with zero mean and finite variance ====, and they are also assumed to be independent of the predictor curves.====The model has applications in a vast number of fields, essentially whenever curves, spectra or images need to be associated with scalar random variables. Examples include meteorology [34], chemometrics [14], and diffusion tensor imaging tractography [17]. More generally, the functional framework can be used to deal with ultrahigh-dimensional regression problems under minimal smoothness assumptions on the coefficient vector. Functional linear regression techniques have also been extended to generalized functional linear models; see, e.g., [16], [33].====Due to the model’s usefulness, recent years have seen an explosion of relevant research and many novel methods have been proposed. The core of these methods is either dimensionality reduction of the predictors or regularization of the coefficient function in the form of a roughness penalty. For an overview, one may consult the monographs of Ramsay and Silverman, Horváth and Kokoszka, Hsing and Eubank [34], [23], [24] and more recently, Kokoszka and Reimherr [27]. See also the recent review papers by Febrero-Bande et al. [11] and Reiss et al. [36]. Since many of these methods are direct extensions of classical (least squares), principal component and partial least squares procedures, a drawback is that they produce estimators that are not resistant to atypical observations. As a result, a single gross error or outlying observation may significantly affect the quality of these estimators. Maronna and Yohai [32] further observed that atypical observations may negatively impact the smoothness of the estimated coefficient function. Therefore, they proposed a robust alternative based on a ridge regression-type procedure that aims to limit the impact of outlying observations while also yielding smooth estimates by penalizing the integrated squared second derivative of the coefficient function.====We take a different approach in this article and, benefitting from recent advances in robust functional data analysis, we propose a robust estimator that stems from the dimensionality reduction principle. In particular, we use functional principal components based on projection-pursuit proposed by Bali et al. [4]. In combination with regression MM estimation [48], robust functional principal components yield a computationally feasible, resistant estimator that is well-suited for the analysis of high-dimensional complex data sets. We then build upon this idea and propose a conceptually simple transformation of the estimators that improves the smoothness of the estimates. For regular data, the performance of the resulting estimators is comparable to popular least squares based estimates while at the same time the estimator shows good robustness in presence of contamination.====The rest of the paper is structured as follows. Section 2 lays down the basic framework while Section 3 describes the proposed estimators in detail. Section 4 presents asymptotic results for the proposed estimators. In particular, sufficient conditions for Fisher-consistency and convergence in probability are given. The influence function is derived in Section 5, where it is also compared to the influence function of a classical estimator. It is shown that the influence function of the proposed estimators is bounded with respect to outliers in the response space but unbounded for good leverage points in the predictor space. Numerical experiments in Section 6 demonstrate the need for robustness and the advantages of our approach regarding both prediction of the conditional mean and estimation of the coefficient function. The proofs of the theoretical results are relegated to Appendix A.",Robust functional regression based on principal components,https://www.sciencedirect.com/science/article/pii/S0047259X1830407X,16 April 2019,2019,Research Article,250.0
"Merola Giovanni Maria,Chen Gemai","Department of Mathematical Sciences, Xi’an Jiaotong-Liverpool University, 111 Renai Road, Suzhou Industrial Park, Suzhou, Jiangsu Province, 215123, PR China,Department of Mathematics and Statistics, University of Calgary, Calgary, Alberta, Canada T2N IN4","Received 17 October 2018, Revised 3 April 2019, Accepted 4 April 2019, Available online 12 April 2019, Version of Record 30 April 2019.",https://doi.org/10.1016/j.jmva.2019.04.001,Cited by (7),"We propose a new sparse principal component analysis (SPCA) method in which the solutions are obtained by projecting the full cardinality principal components onto subsets of variables. The resulting components are guaranteed to explain a given ====. The computation of these solutions is very efficient. The proposed method compares well with the optimal least squares sparse components. We show that other SPCA methods fail to identify the best sparse approximations of the principal components and explain less variance than our solutions. We illustrate and compare our method with others with extensive simulations and with the analysis of the computational results for nine datasets of increasing dimensions up to 16,000 variables.","Principal components analysis (PCA) is the oldest and most popular data dimensionality reduction method used to approximate a set of variables in a lower dimensional space [19]. Effective use of the method can approximate a large number of variables by a few linear combinations of them, called principal components (PCs). PCA has been extensively used over the past century and in recent times the interest in this method has surged, due to the availability of very large datasets. Applications of PCA include the analysis of gene expression analysis, market segmentation, handwriting classification, image recognition, and other types of data.====PCs are usually difficult to interpret and not informative on important features of the dataset because they are combinations of all the observed variables, as already pointed out by Jeffers [10]. A common approach used to increase their interpretability is to threshold the coefficients of the combinations defining the PCs, which are called loadings. That is, variables corresponding to loadings that are lower than a given threshold are ignored. However, this practice can give misleading results [11] and the retained variables can be highly correlated among themselves. This means that the variables included in the interpretation actually carry similar information.====In recent years a large number of methods for sparse principal components analysis (SPCA) have been proposed; see, e.g., [12], [17], [23], [24], [25], [26]. These methods compute solutions in which some of the coefficients to be estimated are equal to zero. In addition to increased interpretability of the results, sparse methods are recommended under the sparsity principle [7].====Conventional SPCA methods replace the ordinary PCs with PCs of subsets (blocks) of variables. The resulting sparse PCs (SPCs) are combinations of only a few of the observed variables. That is, the SPCs are linear combinations of all the variables with only few loadings not equal to zero, the number of which is called cardinality. The difference among conventional SPCA methods is in the optimization approach used to select the variables to be included in the blocks. In this context, the variable selection problem is non-convex NP-hard [17], hence computationally intractable. Some methods use a genuine cardinality penalty (improperly called ==== norm), others an ==== penalty. The most popular of these methods seems to be the Lasso based SPCA [26].====SPCA methods are expressly recommended for large fat datasets [8], i.e., samples with fewer observations than variables. By the nature of the objective function maximized, the components computed maximize the variance explained of each block, instead of that of the whole data matrix. As a consequence, the selected blocks contain highly correlated variables [16]. Furthermore, with this approach the exact sparse reduction of the PCs of fat matrices cannot be identified, as we will show later.====A least squares SPCA method (LS SPCA) in which the sparse components are obtained by minimizing the ==== norm of the approximation error was proposed in [16]. This approach produces sparse components that explain the largest possible proportion of variance for a given cardinality. LS SPCA can identify the equivalent sparse representation of the PCs of fat matrices. However, the variable selection approaches suggested are not scalable to large matrices because they are top-down and require the computation of (generalized) eigenvectors of large matrices.====In this paper we suggest an efficient variable selection strategy for LS SPCA, based on projecting the full cardinality PCs on blocks of variables. This approach is based on a property we prove which says that if the regression of a PC on a block of variables yields an ==== statistics equal to ====, then the LS SPCA components computed on that block of variables will explain a proportion of the variance not smaller than ====. With this approach, the NP-hard SPCA variable selection problem is reduced to a more manageable univariate regression variable selection problem. This procedure, to which we refer to as Projection SPCA (PSPCA), also gives as by-products the projections of the PCs, which are sparse components in themselves.====We show that algorithms using PSPCA variable selection are very efficient for computing LS SPCA components, having a growth order of about the number of variables squared. We also show that the performance of the projected PCs is comparable to that of the LS SPCA components. This is relevant because these projections are easier to understand by researchers and also easier and more economical to compute.====In the next section we review PCA and LS SPCA, and give a novel interpretation of the latter. The methodological details of PSPCA are discussed in Section 3. In Section 4 we discuss the use of PSPCA for variable selection. We compare its performance on fat matrices with that of conventional SPCA methods and explain the details of the PSPCA algorithm. The proposed method is illustrated by using simulated and real datasets in Section 5. We give some final comments in Section 6. The Appendix contains some of the proofs.",Projection sparse principal component analysis: An efficient least squares method,https://www.sciencedirect.com/science/article/pii/S0047259X18305463,12 April 2019,2019,Research Article,251.0
"Sugasawa Shonosuke,Kawakubo Yuki,Datta Gauri Sankar","Center for Spatial Information Science, The University of Tokyo, 5-1-5 Kashiwanoha, Kashiwa-shi, Chiba 277-8568, Japan,Graduate School of Social Sciences, Chiba University, 1-33 Yayoi-cho, Inage-ku, Chiba 263-8522, Japan,Department of Statistics, University of Georgia, Athens, GA 30602-1952, USA","Received 16 January 2018, Revised 5 April 2019, Accepted 5 April 2019, Available online 11 April 2019, Version of Record 30 April 2019.",https://doi.org/10.1016/j.jmva.2019.04.002,Cited by (4),"In small area estimation methodology, selection of the suitable ","Direct survey estimators, based only on small area-specific samples, are known to produce unreliable estimates of small area population means with unacceptably large standard errors. To overcome the problem, model-based methodology has been developed to “borrow strength” from related areas through applying mixed effects models to produce indirect estimators with higher precision. The technique is generally called small area estimation (SAE), and there has been a large body of literature concerned with developing effective methods. For comprehensive overviews for SAE; see [13], [15].====The basic area-level model for SAE was first proposed by Fay and Herriot [6], which is now known as the Fay–Herriot (FH) model and is widely used as a standard model in SAE. The model can be expressed as a simple linear mixed model in which, for all ====, ====where ==== is the direct estimator of the population area mean, ====, which is represented as ====, ==== and ==== are vectors of known covariates and unknown regression coefficients, respectively, ==== is an area-specific random effect, and ==== is a sampling error. It is conventionally assumed that ==== and ==== are mutually independent, and ==== and ====, where ==== is an unknown variance parameter and ==== is assumed known. In practice, ==== are obtained by smoothing the variance estimates ==== and then treating the smoothed estimates as the true ====; see [15]. Hence, the unknown model parameters are ====. In the model given by (1), ==== is the true area mean and we are interested in estimating (predicting) ====.====For each ====, let ==== be a general estimator of ==== as a function of ==== as well as ====, and ====, where the expectation is taken with respect to the joint distribution of ==== and ==== in (1), be the mean squared prediction error (MSPE) of ====. Then, it is well-recognized that the MSPE is minimized at ====, where ====which is known as the best predictor (BP) of ====. It may be observed that the BP (2) shrinks the direct estimator ==== toward the synthetic mean ==== and the amount of shrinkage is determined by the two variances ==== and ====. Given that the BP (2) depends on unknown model parameters ====, we need to replace them by the estimated counterpart.====Under given ====, the typical method for estimating ==== is the generalized least squares (GLS) estimator given by ====where ====, ==== and ====. We assume that ====. Regarding the estimation of ====, several methods have been used, e.g., the (residual) maximum likelihood estimator [4], [5], [7], and moment estimators [6], [14]. Then, the practical version of (2) can be obtained by putting ====, known as the empirical best linear unbiased predictor (EBLUP).====The available estimators of ==== are known to perform well when ==== themselves are parameters of interest. However, as pointed out by Jiang et al. [9], such estimators are not necessarily designed to minimize the MSPE of ====, thereby Jiang et al. [9] proposed the observed best prediction (OBP) method for estimating unknown model parameters ====. The OBP estimator of ==== is defined as the minimizer of an unbiased estimator of the total MSPE, ====, which was shown to equivalently minimize ====where ====. Under given ====, the OBP estimator of ==== has the form ====which is slightly different from the GLS estimator (3). The OBP estimator of ==== is defined as the minimizer of ==== with ====.====In practice, before fitting the model (1), we need to select suitable explanatory variables (covariates) to be included in the model. Usually, this procedure is performed in advance based on some information criteria, such as AIC [1] and BIC [16], or more advanced methods like the fence methods [10], conditional AIC [20]. A comprehensive review over the model selection in linear mixed models is given in [11]. However, these methods are not necessarily designed to select models by minimizing the MSPE whereas they have some nice properties in terms of model selection. Instead, it would be more natural and efficient to consider model selection and estimation simultaneously and carry out both processes by paying attention to the MSPE. Therefore, in this paper, we propose a new prediction method of ====, called observed best selective prediction (OBSP), in which model selection and estimation are considered simultaneously, and both procedures are designed to minimize an unbiased estimator of MSPE. Hence, the proposed method has a similar philosophy and it can be regarded as an extension of the OBP method.====The rest of the paper is organized as follows. In Section 2, we describe the OBSP method in the Fay–Herriot model (1), and provide some asymptotic properties of the OBSP estimator. In Section 2.3, we derive two types of estimators of the area-specific MSPE of the estimator of ====. In Section 3, we carry out some simulation studies investigating the prediction errors of OBSP together with some existing methods, and finite-sample performance of the proposed area-specific MSPE estimators. We also demonstrate the OBSP method by using Japanese survey data. In Section 4, we provide some conclusions and discussions.",Observed best selective prediction in small area estimation,https://www.sciencedirect.com/science/article/pii/S0047259X18300319,11 April 2019,2019,Research Article,252.0
"Cossette Hélène,Gadoury Simon-Pierre,Marceau Etienne,Robert Christian Y.","École d’actuariat, Université Laval, 2425, rue de l’Agriculture, Québec (Québec), Canada G1V 0A6,Univ Lyon, Université Lyon 1, ISFA, LSAF EA2429, 50, avenue Tony-Garnier, F-69007 Lyon, France","Received 5 July 2018, Available online 10 April 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.03.008,Cited by (9),We consider the family of hierarchical ,"Copulas are well-known probabilistic tools which are used to model the dependence within a vector ==== of random variables with respective marginal distribution functions ====. Assuming that the latter are continuous, the unique underlying copula ==== associated with this random vector is the joint cumulative distribution function (cdf) of the vector ==== of dependent random variables which are uniformly distributed on the interval ====.====Particularly popular is the family of Archimedean copulas [7], [23], which have the useful property of being defined solely in terms of a continuous and strictly decreasing Archimedean generator ====, where ==== and ==== as ====. Let ==== be the class of all such functions ==== which are completely monotone, i.e., such that for all ====, we have ====. Then, we can define a ====-dimensional copula as Archimedean with completely monotone generator if there exists ==== such that, for every integer ==== and all ====, ====While Archimedean copulas have an algebraically closed form in every dimension, they are exchangeable, i.e., for any arbitrary permutation ==== of ==== and ====, we have that ====. This is a major drawback of this class of copulas, especially for applications in high dimensions. To allow asymmetry, hierarchical Archimedean copulas have been developed, such as nested Archimedean copulas [18], hierarchical Archimedean copulas constructed via Lévy subordinators [15], and hierarchical Archimedean copulas based on the Kendall distribution [1].====Another approach to the construction of such copulas which circumvents the constraints on nesting conditions was proposed by Cossette et al. [2]. It mainly relies on compound distributions. In both their paper and earlier work by Hering et al. [15], a hierarchical Archimedean copula is constructed from the joint survival function of a multivariate mixed exponential distribution. The dependence structure of the multivariate mixed exponential distribution is defined through a vector of dependent mixing random variables which are defined using Lévy subordinators. While a common random time is considered in [15], random sums with a common counting random variable are used in [2]. Section 4.2 in [2] gives a detailed comparison of these two different approaches with a view to proposing new hierarchical Archimedean copulas which circumvent the nesting condition required in the seminal work by Joe [18].====There is little literature on simultaneous estimation of the structure of hierarchical copulas and their parameters. Okhrin et al. [25] perform these tasks using a bottom-up procedure. A multi-stage procedure is proposed to simultaneously determine the structure of the tree and estimate the parameters of the hierarchical Archimedean copulas. The structure determination is done with different approaches (under the assumption that the copula generators are known) and the parameters are mostly estimated by maximum likelihood but the copula densities are not known analytically and hence must be computed numerically. Okhrin et al. [26] study distributional properties of hierarchical Archimedean copulas but do not investigate any estimation procedure for such structures. Segers and Uyttendaele [29] propose an estimation procedure for the target phylogenetic tree of a nested Archimedean copula which relies on the possibility to represent a hierarchical Archimedean copula as a set of trivariate structures which can then be estimated on their own. This procedure, compared to that of Okhrin et al. [25], does not require any assumption in regard to the generators of the nested Archimedean copula.====Górecki et al. [9] and Górecki and Holeňa [12] propose an estimator for both the determination of the structure and the parameters of the hierarchical Archimedean copulas. The algorithm for agglomerative hierarchical clustering relies on Kendall’s tau to measure the similarity between pairs of leaves and on complete-, single- and average-linkage clustering to identify the tree structure. In contrast to Segers and Uyttendaele [29], who only focus on the tree structure determination, Górecki et al. [9] and Górecki and Holeňa [12] propose to fully estimate the target nested Archimedean copula. However, their procedure can only output binary tree structures, which makes their estimation procedure unsuitable for tree structures that are not binary. Note that the structure determination algorithm based on a Kendall correlation matrix was originally proposed by Górecki and Holenǎ [13]. Górecki et al. [10] and Górecki and Holeňa [12] also use Kendall’s tau for structure determination (through hierarchical agglomerative methods with different distances, as well as clustering algorithms) and for parameter estimation (through its inversion).====Uyttendaele [30] revisits estimation procedures previously proposed and presents an alternative to the tree structure estimation approach of Segers and Uyttendaele [29] based on supertrees. Zhu et al. [33] extend the work of Hering et al. [15] on Lévy subordinated hierarchical Archimedean copulas. Notably, they propose an estimation procedure to determine the tree structure of LSHACs and estimate the parameters involved. The optimal tree structure is obtained based on a so-called s-Euclidean metric and the parameters are estimated with an augmented inference-for-margin method. Matsypura et al. [22] propose the use of a network approach to the hierarchical Archimedean copulas estimation problem. Most papers discuss the estimation of hierarchical Archimedean copulas constructed by nesting copulas belonging to the same parametric Archimedean family of copulas. In [11], the estimation of hierarchical Archimedean copulas involving different Archimedean families is tackled with goodness-of-fit testing directly in the estimation procedure due to the need to deal with the sufficient nesting condition.====As previously mentioned, Cossette et al. [2] propose a new hierarchical Archimedean copula construction based on multivariate compound distributions. This technique relies on the construction of a multivariate exponential mixture distribution through compounding. The absence of nesting and marginal conditions, contrary to the nested Archimedean copulas approach, leads to major advantages, such as a flexible range of possible combinations in the choice of distributions, or the existence of explicit formulas for the distribution of the sum. Hering et al. [15] also propose hierarchical Archimedean copulas that circumvent a nesting condition but with more complicated mathematical tools than the random sums used in [2].====Assuming that the multivariate compound distributions are characterized by finite-dimensional parameters, we propose an estimation procedure for the class of copulas considered by Cossette et al. [2]. First we use an agglomerative hierarchical clustering method based on the matrix of Spearman’s rho values to estimate the tree structure. Combining this method with a bootstrap procedure, we decide whether it is necessary to group the binary structures. Note that the Spearman’s rho matrix is used in the clustering method due to the asymptotic statistical test, provided by Gaißer and Schmid [6], for the hypothesis that all pair-wise Spearman’s rank correlation coefficients in a multivariate random vector are equal. To our knowledge, this is the only asymptotic statistical test based on a concordance measure that has been derived from its asymptotic behavior. However, we note that Perreault et al. [28] investigate block exchangeability detection in correlation matrices, which is another way to identify the dependence within the tree structure. Unfortunately, their method is designed for partially-exchangeable tree structures (i.e., one-level tree structures) and hence does not immediately apply in the present context.====As our structure is multi-level, it requires a more general technique. We have thus turned our attention to the theory of hypothesis testing, via the Neyman–Pearson paradigm. Second we introduce a top-down composite likelihood approach using the property that the parameters of the Archimedean copulas are also naturally defined recursively in a new top-down way. Full likelihood estimation is in general not possible for our hierarchical Archimedean copulas, but when it is, it is computationally prohibitive and not necessarily much more efficient.====This paper is structured as follows. In Section 2 we present the family of multi-level hierarchical Archimedean copulas introduced in [2]. We recall the assumptions of the model, introduce notations and definitions, and provide the Archimedean families of copulas of subvectors of leaves within our tree structure.====In Section 3 we present a two-step procedure that provides a tree structure that suits our multi-level hierarchical Archimedean copulas. This procedure only uses the matrix of Spearman’s rho values as input. The first step brings into play an agglomerative hierarchical clustering algorithm, while the second step presents a reconstruction algorithm that transforms the tree derived from the first step to a tree compatible with our model. Note that such a reconstruction is not necessary in the case of nested Archimedean copulas. In practice, we can only use empirical Spearman’s rho values that differ from the theoretical values and lead to a binary hierarchical tree structure that can be different from the true structure derived from the first step when using the theoretical Spearman’s rho.====In Section 4 we propose a bootstrap procedure to decide whether it is necessary to eliminate nodes of the empirical structure or not, and that is based on statistical hypothesis tests of exchangeability developed in [6]. Such a procedure has been implemented in ==== and is available in the package ====; see [5]. Then, we present our innovative top-down composite likelihood approach using the property that the parameters of the Archimedean copulas are defined recursively in a top-down way. We prove the asymptotic normality of the estimators. In Section 5 two numerical examples illustrate the different steps of the procedure, and a comparison with the full maximum likelihood method is made to show the efficiency of our approach in a test case where the full maximum likelihood can be computed.",Composite likelihood estimation method for hierarchical Archimedean copulas defined with multivariate compound distributions,https://www.sciencedirect.com/science/article/pii/S0047259X18303488,10 April 2019,2019,Research Article,253.0
"Nasri Bouchra R.,Rémillard Bruno N.,Bouezmarni Taoufik","Department of Mathematics and Statistics, McGill University, Montréal (Québec), Canada H3A 0B9,Département de sciences de la décision, HEC Montréal (Québec), Canada,Département de mathématiques, Université de Sherbrooke, Sherbrooke (Québec), Canada J1K 2R1","Received 22 January 2018, Revised 21 March 2019, Accepted 21 March 2019, Available online 28 March 2019, Version of Record 10 April 2019.",https://doi.org/10.1016/j.jmva.2019.03.007,Cited by (7),"In this paper, we consider non-stationary random vectors, where the marginal distributions and the associated ","In many applications, copulas have been used to model dependence between several variables and very often the copula is assumed to be constant. However, in practice, data are typically non-stationary. For example, in hydrology, because of climate change, the distribution of a hydrological time series is likely to vary over time and/or according to some time-dependent covariates. In the literature, some attempts have been made to deal with non-stationarity for copula-based models. Recently, Jiang et al. [15] modeled the dependence between bivariate response variables by letting the copula parameters depend on covariates. Ahn and Palmer [1] did something similar. They proposed to use maximum pseudo-likelihood for the estimation of parameters but they did not study the convergence of the proposed estimators. Note that in the stationary case, their model is a particular case of the so-called single-index copula [5] where the copula between the multivariate responses is indexed by a parameter depending on covariates.====In this paper, we propose to model the joint distribution of random vectors (response variable and covariates) by a time-dependent copula, which is a more general approach than the single-index copula setting. In view of applications, we consider only a univariate response variable but our results can be extended to multivariate response vectors, which could be used for spatial dependence. This problem will be investigated in a forthcoming paper. More precisely, the main objective of the present paper is to propose a flexible model by allowing two extensions of the copula-based method. First, we consider a combination of time-dependent and iid variables. The distribution of the iid variables (response variable or covariates) is estimated nonparametrically by using the empirical distribution functions, while parametric estimators are used to fit the distribution of the time-dependent variables. To distinguish between these two types of variables, one can use, e.g., a change point test; see p. 344 in [26]. Second, we model the dependence between the variables by fitting a (time-dependent) parametric family of copulas. Again, a change point test [14] can be used to detect non-stationarity.====The paper is organized as follows. In Section 2 we establish the limiting distribution of the estimators of the copula and the conditional copula, together with a parametric bootstrap method for constructing confidence bands around the estimator and for testing the adequacy of the model. The proofs of these results are given in Appendix B. In Section 3 we consider three examples of functionals of the copula-based model under non-stationarity: conditional quantiles, conditional means, and conditional expected shortfalls. The asymptotic distribution of the estimation errors is shown to be Gaussian, and bootstrapping methods are proposed to estimate their asymptotic variances. The finite-sample performance of our estimators is investigated in Section 4 through Monte Carlo experiments, while in Section 5 the usefulness of our method is illustrated with one simulated dataset and two case studies, one from hydro-climatology and the other from finance. Concluding comments are given in Section 6.",Semi-parametric copula-based models under non-stationarity,https://www.sciencedirect.com/science/article/pii/S0047259X18300435,28 March 2019,2019,Research Article,254.0
Zhou Zhiyang,"Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, BC, Canada V5A 1S6","Received 11 September 2018, Revised 20 March 2019, Accepted 20 March 2019, Available online 27 March 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.jmva.2019.03.006,Cited by (5),"Functional principal component regression (PCR) can fail to provide good prediction if the response is highly correlated with some excluded functional principal component(s). This situation is common since the construction of functional principal components never involves the response. Aiming at this shortcoming, we develop functional continuum regression (CR). The framework of functional CR includes, as special cases, both functional PCR and functional ==== (PLS). Under certain circumstances, functional CR is more accurate than functional PCR and functional PLS both in estimation and prediction; evidence to this effect is provided through simulations and numerical case studies. Also, we demonstrate the consistency of estimators given by functional CR.","With the development of technology, the demand for functional data analysis (FDA) is increasing. It is frequent to encounter data that are recorded continuously within a nondegenerate and compact interval ====. Let paired observations ==== be independently and identically distributed (iid) as ==== with scalar ==== and random process ==== whose argument, often referred to as “time”, takes values in ====. All the functions herein are assumed to be square-integrable, i.e., to belong either to ==== or to ====, the ====-spaces on ==== or ==== with respect to the Lebesgue measure. The covariance operator of ====, written as ====, is given, for all ====, by ====where ====. The assumption ==== implies that it possesses countably many nonnegative eigenvalues sorted in a decreasing order, say ====. We furthermore require ====. All of the above are commonly assumed in FDA research.====Suppose there is a scalar-on-function regression model linking response ==== to the (integral) inner product of ==== and unknown nonrandom function ==== which is assumed to be orthogonal to the null space of ====. To be precise, the linkage between ==== and ==== is of the form ====where ==== is positive and white noise ==== is of zero mean and unit variance. The notation ==== is short for the Lebesgue integral ====. In addition, ==== stands for the ====-norm, i.e., ==== amounts to the square root of ==== or to ==== according as ==== or ====.====The infinite-dimensional structure of functional space makes data analysis challenging: the dimension of the parameter space exceeds the number of observed subjects, and hence dimension-reduction techniques are indispensable in model fitting. To estimate the coefficient function ==== and to predict the conditional expectation ====for ==== distributed as ====, the standard approach is to express ==== in terms of a linear combination of functions ==== truncated from countably many basis functions ==== in ====. This inspires us to approximate ==== and ====, respectively, by ====and ====where ==== is the linear space spanned by ====. Note that ==== is the slope of the best approximation in the ==== sense (within ====) to ==== by a linear function of ====. In particular, ====if ==== are mutually orthogonal and of norm ====. Further write ==== and ====.====Correspondingly, we have estimates for (4), (5), respectively, ====and ====where ==== and ====. In the estimation, though it is possible to employ a choice of ==== independent of the data (e.g., polynomial basis, Fourier basis, etc.), it is more reasonable to force it to adapt to data. In that case, ==== are often unknown a priori and need to be estimated. Under this framework, the accuracy of estimates ==== and ==== varies with the estimating quality of ====. Two well-known options are discussed below, leading to functional principal component regression (PCR) and functional partial least squares (PLS), respectively.",Functional continuum regression,https://www.sciencedirect.com/science/article/pii/S0047259X1830472X,27 March 2019,2019,Research Article,255.0
"Nagler T.,Bumann C.,Czado C.","Fakultät für Mathematik, Technische Universität München, Boltzmanstraße 3, 85748 Garching, Germany","Received 29 January 2018, Available online 19 March 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.03.004,Cited by (37),Vine copulas allow the construction of flexible dependence models for an arbitrary number of variables using only ,"Following the 2008 financial crisis, academic research and public media identified unrealistic mathematical models of the dependence between financial assets as one of its key causes [15], [31]. In the aftermath, modeling these dependencies became a topic of intense study in finance and statistics. One of most promising tools that has emerged are vine copulas [2], [5]. Vine copula models construct the dependence structure from bivariate building blocks, called pair-copulas. Each pair-copula captures the conditional dependence between a pair of variables. Because each pair-copula can be parametrized differently, vine copulas allow each pair to have a different strength and type of dependence. This flexibility is the key reason why vine copulas have become so popular for modeling dependence between financial assets; for a review, see [1].====The flexibility of vine copulas comes at the cost of a large number of model parameters. A vine copula on ==== variables consists of ==== pair-copulas, and each pair-copula can have multiple parameters. In financial applications, the number of model parameters quickly exceeds the number of variables. Suppose each pair-copula can have up to two parameters. Then a model for 50 assets has ==== possible parameters; a model for 200 has almost ====. By contrast, five years of daily stock returns consist of roughly ==== observations.====In such situations, there are two major challenges that we want to address: the risk of overfitting and the computational burden to fit thousands of parameters. Both issues make it desirable to keep the model sparse. A vine copula model is called sparse when a large number of pair-copulas are the independence copula. The key question is: which of the pair-copulas should we set to independence?====The most common strategy is to select the pair-copula family by either AIC or BIC [4], [32]. The AIC is known to have a tendency to select models that are too large, which conflicts with the need for sparsity; see [12]. In contrast, the BIC is able to consistently select the true model, but only when the number of possible parameters grows sufficiently slowly with the sample size ====. As explained above, this assumption should be seen critically in a high-dimensional context. Another problem is that the BIC is derived under the assumption that all models are equally likely. Under this assumption, we expect only half of the pair-copulas to be independence, a model we would not consider sparse.====We propose a new criterion called modified Bayesian Information Criterion for Vines (mBICV) that addresses both issues and is specifically tailored to vine copula models. It is based on a modification of the prior probabilities that concentrates on sparse models and is motivated by statistical practice. This modification turns out to be enough to relax the restrictions on the rate at which ==== diverges. The idea behind the mBICV is similar to other modified versions of the BIC that were developed for linear models; see [42].====The mBICV is useful for two things: selecting pair-copulas individually and selecting hyper-parameters of sparse model classes. One such class are truncated vine copulas [8], [25]. A vine copula model is called truncated if all pair-copulas that are conditioned on more than ==== variables are set to independence. We further propose an alternative class of sparse models called thresholded vine copulas. They induce sparsity by setting a pair-copula to independence when the encoded strength of dependence falls short of a threshold. This idea has been around for a long time and heavily used, but the threshold was commonly tied to the ====-value of an independence test [14]. In the more general form, the threshold is a free hyper-parameter. Such classes of sparse models give a computational advantage, since a substantial number of pair-copulas never have to be estimated.",Model selection in sparse high-dimensional vine copula models with an application to portfolio risk,https://www.sciencedirect.com/science/article/pii/S0047259X18300630,19 March 2019,2019,Research Article,256.0
"Birr Stefan,Kley Tobias,Volgushev Stanislav","Ruhr Universität Bochum, Fakultät für Mathematik, Lehrstuhl für Stochastik, 44780 Bochum, Germany,University of Bristol, School of Mathematics (Faculty of Science), University Walk, Bristol BS8 1TW, United Kingdom,Department of Statistical Sciences, University of Toronto, 100 St. George Street, Toronto, Ontario, Canada M5S 3G3","Received 4 April 2018, Available online 18 March 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.03.003,Cited by (9),Finding ==== that accurately describe the ,"Nonparametric methods provide valuable tools for dependence modeling. If a parametric candidate model is available, we can compare the corresponding estimate with a nonparametric one to evaluate how well the chosen model describes the data. If no candidate model is available, nonparametric techniques can be used to get a first impression of the underlying dependence and information about potentially suitable parametric models.====In time series analysis, methods that are based on spectral densities and periodograms have a long and successful history. Priestley [28] suggests the use of spectral densities as a graphical tool for model validation by comparing the spectral shape of a dataset with standard ones from well-known parametric models. Tools based on spectral distributions were considered, among others, by Bartlett [2], [3], who proposed using the normalized cumulative periodogram to assess whether a process is uncorrelated and to detect hidden periodicities. Rigorous tests for the hypothesis ==== for a fixed ==== were derived by Anderson [1], while the more general testing problem ====, where ==== is a parametric class of spectral densities, was treated by Paparoditis [27]. Fan and Zhang [14] consider generalized likelihood ratio tests for the same hypothesis. There is also a rich literature on nonparametric comparison of the multivariate spectra of two time series; recent references include [9], [11], [12], [21], [26], but this list is far from complete.====All of the references cited above deal with classical spectral analysis which is based on the autocovariance function and therefore restricted to the aspects of time series dynamics that can be captured by second-order moments. The autocovariance function does provide a complete description of the dependence of Gaussian processes, but it can completely miss dependencies in a non-Gaussian setting. One such example arises in financial applications when first-order differences of stock market data are analyzed. For illustration, Fig. 1 shows the autocorrelations of the log-returns ==== and the squared log-returns ==== calculated from the S&P 500 between 2005 and 2010. While the observations ==== appear to be uncorrelated, we can clearly see positive correlation in the squared observations ====. This shows that ==== in fact exhibits strong dependence, which cannot be described through the autocovariance function and therefore completely escapes classical spectral analysis.====This limitation has recently attracted much attention, and new frequency domain tools that can capture non-linear dynamics have been proposed. Pioneering contributions in that direction were made by Hong [17], [18] who introduced generalized spectra that are based on joint distributions and joint characteristic functions, respectively. Generalized spectra were later utilized by Hong and Lee [19], [20] and Escanciano [13] to test for the validity of various forms of parametric time series models.====More recently, related approaches were taken by Li [24], [25], who coined the names of Laplace spectrum and Laplace periodogram. Those ideas were further developed by Hagemann [16] and extended to cross-spectrum and spectral kernel concepts by Dette et al. [10] and Kley et al. [23], who introduced the notion of copula spectral densities, Baruník and Kley [4], who introduced quantile coherency to measure dependence between economic variables, and Birr et al. [6] who consider copula spectra for strictly locally stationary time series.====In the present paper, we utilize copula spectral densities to develop a graphical tool to determine suitable parametric models for time series. We would like to emphasize that our main goal is not to construct yet another test for the hypothesis that a time series is generated by a certain parametric model. Rather, we provide a graphical tool that can indicate whether a chosen model accurately reflects the dependence present in the observed data. By providing useful information about which aspects of the dependence are not captured in case the model is inappropriate, our procedure goes beyond goodness-of-fit tests that merely aim to reject a class of candidate models.====The rest of the paper is organized as follows. Section 2.1 contains a summary of basic properties of copula spectral densities and provides guidelines for their interpretation. In Section 2.2 we provide details on the proposed algorithm. Section 3 gives a theoretical justification for the graphical approach and Section 4 contains a simulation study and a real data example. All proofs are deferred to the Appendix. The Online Supplement contains additional plots and material pertaining to the data analysis.",Model assessment for time series dynamics using copula spectral densities: A graphical tool,https://www.sciencedirect.com/science/article/pii/S0047259X18301842,18 March 2019,2019,Research Article,257.0
"Bhaumik Dulal K.,Nordgren Rachel K.","Department of Psychiatry, University of Illinois at Chicago, Chicago, IL 60612, USA,Division of Epidemiology and Biostatistics, University of Illinois at Chicago, Chicago, IL 60612, USA","Received 9 July 2018, Revised 5 March 2019, Accepted 5 March 2019, Available online 18 March 2019, Version of Record 4 April 2019.",https://doi.org/10.1016/j.jmva.2019.03.001,Cited by (1),The standard approach for prediction of multiple correlated outcome measures overpredicts the unknown observation in the linear model setup if associated ,"Prediction and calibration are two important related problems in statistics; the former estimates the outcome measure(s) for a known set of corresponding covariates, and the latter estimates covariate(s) for given outcome measure(s). In recent years, prediction has drawn an enormous amount of attention in classification, where subjects are classified into different populations, and calibration also plays an important role in that process when covariates are missing. There are numerous examples of predictions of interest, including student success as predicted by test scores, profit or sales volume as predicted by cost and demand, new costs as predicted by past costs, and growth curves as predicted by age and gender. The degree to which precision in prediction is important varies by context, but in all cases the desire is to have the most accurate predictor. In this article, we develop statistical methodologies for prediction and calibration in the context of multivariate linear models, correcting for the bias in the least square based predictor induced by correlated outcome measures.====Estimation of models examines the fit of the model to the observed data (retrospective fit), while prediction is concerned with the fit of the model to some future data (prospective fit). Since the data is being reused to evaluate the retrospective fit, it makes sense that it would be overly optimistic in its ability to predict. If potential combinations of covariates are limited and known, or if the observed data has a large number of samples, we can treat those future covariates as fixed, and the problem is straightforward. This paper deals with optimizing prediction for future observation(s), about which we know nothing beyond the fact that they come from the same populations as our previously observed data (referred to as a Construction Sample (CS)). For example, a medical patient drawn from a particular patient population, or a student drawn from a classroom or school. An optimized predictor would have good performance averaged across the entire population, which makes a risk function a logical choice to assess prospective fit.====The term shrinkage is used to denote adjusting for the optimism in a retrospective fit as compared to a prospective fit. Copas [12] showed that a predictor with specified shrinkage dominates a predictor without shrinkage in the univariate regression case. This can be extended to a multivariate regression, however, a single number is no longer sufficient to describe the difference between predicted and observed fit. Since the outcomes are presumably correlated, we need a way to adjust for that correlation before summing across outcomes. In this paper we use a risk function which incorporates both the difference in values as well as the covariance among outcomes, and then uses the expectation to average over the entire population.====As mentioned above, in some applications, we are faced with predictor variables that can take a wide range of values. Similar to the Bayesian idea of treating a parameter as a random variable rather than a fixed quantity, we consider these predictor variables as occurring randomly. An example of this in the behavioral sciences is when subjects are randomly chosen from a pool (such as a school or classroom) to predict the effects of a newly developed educational procedure. Allowing ==== to belong to a distribution poses a challenge, since our model needs to accurately predict across a wide range of values of ====. A standard assumption is that any future ==== is drawn at random from the population as a whole, while our model was developed using a CS that also came from that population. The randomness in future ====, and the fact that our model depends on the data contained in the CS (which may not be entirely representative of the population), has the potential to introduce a bias into our prediction.====There has been a continued interest in this topic, most notably the February 2012 issue of ====, which had an article titled “A tribute to Charles Stein” [20] as well as 10 newly published shrinkage-related articles. Recent papers have examined admissibility and dominance, including under spherically symmetric distributions [4], and with a geometric interpretation of shrinkage [7]. George et al. [19], Cai [8], and Tan [36] investigated estimating multivariate normal predictive densities within a Bayesian context. Other authors have attempted to create a synthesis of their results between frequentist and Bayesian approaches, across a variety of topics including confidence sets for the mean [9], small area estimation [14], improved estimators of loss [17], and regression to the mean in multilevel datasets where some sample sizes are quite small [31]. Chételat and Wells [10] proposed a James–Stein estimator for the multivariate normal mean in the high-dimensional case. Indeed, popular “big data” techniques such as LASSO [37] employ a form of shrinkage, and the univariate multiple regression shrinkage problem can be formulated with a penalty that is a function of the sum of squares of the predicted values, allowing the two to be related directly; see Chapter 7 in [39]. Within a multivariate context, most authors have focused on estimation of the mean vector under quadratic loss. For predictions involving randomly occurring predictor variables, Breiman and Friedman [5] proposed a “curds and whey” method that uses cross-validation.====The calibration problem mainly deals with the construction of a confidence region for an unknown non-stochastic ====, when corresponding ==== is given. Construction of a confidence region in the multivariate setup has been well addressed by Mathew and Zha [28], where the authors construct a conservative confidence region for an unknown explanatory variable ==== using a pivot suggested by Fujikoshi and Nishii [18], as well as Davis and Hayakawa [15], for asymptotic multivariate calibration models. The conservative confidence region developed by Mathew and Zha [28] for the finite sample case is non-empty, easy to construct and is based on the invariance property. In the same context, Brown [6] and Oman [33] also constructed confidence regions for finite sample cases, but Brown’s region can be empty and Oman’s region does not have the invariance property. On the other hand, though the region proposed by Mathew and Kasala [25] is non-empty and invariant, it is difficult to study the shape of the region, as it is derived using a complicated pivot. All aforementioned articles have constructed confidence regions for a single unknown ====, when the corresponding ==== is given. Using a multivariate linear model approach, we develop a theoretical foundation for constructing a simultaneous confidence region for more than one ====’s, when corresponding ====’s are given.====In this article, we study the nature of shrinkage when the predictor variables are random, in the context of prediction for a multivariate linear model. In Section 2, we propose a very general multivariate regression model, and quantify the expected overprediction and develop our proposed estimator. In this section we also consider a James–Stein type estimator and define its risk function. We then prove that the risk of this estimator is always less than the risk of the original predictor, and provide a lower bound for the absolute value of the difference. Next, we perform a simulation study to estimate the risk difference for several combinations of parameters. Results of our methodology are illustrated using a real behavioral science dataset. In Section 3, we consider a very general multivariate calibration model, and develop a confidence region for a set of non-stochastic covariates. In addition, we propose and implement an algorithm to determine the coverage probability of the confidence region via simulation and examine the results for a combination of parameters. We discuss and conclude our effort in Section 4.",Prediction and calibration for multiple correlated variables,https://www.sciencedirect.com/science/article/pii/S0047259X18303555,18 March 2019,2019,Research Article,258.0
"Nasri Bouchra R.,Rémillard Bruno N.","Department of Mathematics and Statistics, McGill University, 805, rue Sherbrooke ouest, Montréal (QC), Canada H3A 0B9,Department of Decision Sciences, HEC Montréal, 3000, chemin de la Côte Sainte-Catherine, Montréal (QC), Canada H3T 2A7","Received 14 April 2018, Available online 8 March 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.03.002,Cited by (12),"In this paper, we propose an intuitive way to couple several dynamic ==== even when there are no innovations. This extends previous work for modeling dependence between innovations of ====. We consider time-dependent and time-independent ","Considering the dependence between economic or financial variables is an important issue, as emphasized by the 2008 financial crisis, where the risk of contagion was underestimated, mainly because of overly simplified dependence models. Therefore better models must be developed to capture the dependence between multiple time series. As advocated by Embrechts et al. [10], one cannot rely on Pearson’s correlation as a measure of dependence, unless the series are jointly Gaussian, which is rarely the case in practice. Copulas are a much more flexible tool for dependence modeling. As explained by Patton [26], [27], there are typically two ways to exploit copulas in time series modeling. Copulas can be used either to model the dependence between successive values of a univariate time series, or they can be used to model the conditional dependence of a random vector, given some information about its past, thereby leading to time-varying copulas.====In most papers where copula-based models for multivariate time series are proposed, individual series are typically modeled first, and a copula is used to capture the dependence between serially independent innovations; see, e.g.,[8], [20], [21], [25]. However, not all dynamic models have innovations. Take for example univariate time series modeled by hidden Markov models. One aim of this paper is to propose a natural way to model dependence between any dynamic time series, even those without innovations, by using time-dependent copulas. We also propose a way to estimate the parameters and validate the model.====To fix ideas, consider a multivariate time series ==== where, for each ====, ====. We suppose that each univariate time series is a “generalized error model” [9], meaning that for any ====, ==== are iid with continuous distribution function ==== and density ====, for some ====-measurable transformation ==== with ====. It means that for any ====, the parameters of the conditional distribution ==== are ====-measurable. These dynamic models contain the innovation models, including all stochastic volatility models, Hidden Markov Models (HMM), and the models of Bai [2] as particular cases. For example, in Bai [2], ==== is the uniform distribution over ====, meaning that ====. Note that linking together generalized errors has been proposed implicitly in Patton [24] who considered the bivariate case where ==== and ==== are uniform cdfs and ====. However, only stochastic volatility models were studied.====To model the dependence between the time series, consider ====, where ==== is the smallest sigma-algebra containing ====, and assume that for any ==== and any ====, ====Condition (1) means that the ==== are iid with conditional distribution ==== given ====. It then follows from Theorem 1 in Patton [24] or Theorem 2 in Fermanian and Wegkamp [12] that there exists a sequence of ====-measurable copulas ====, so that the joint conditional distribution function ==== of ==== given ==== is ====for any ====. In fact, ====, for every ====.====Since the generalized errors ==== are not observable, ==== being unknown, the latter must be estimated by a consistent estimator ====. One can then compute the pseudo-observations ====, where ==== for all ==== and ====.====In order to present the main results, and in view of applications, we consider two cases: time-independent copulas (Case 1) and time-dependent copulas (Case 2).====For implementing the proposed methodologies, we suggest performing the following steps:====The paper is structured as follows. In Section 2, under minimal conditions, and assuming a time-independent copula model, we study the asymptotic behavior of empirical processes constructed from the pseudo-observations associated with estimated generalized error models. It is shown that the asymptotic behavior of the sequential processes used for testing change points does not depend on the unknown parameter ====. Furthermore, the asymptotic distribution of the empirical copula process does not depend on the parameter ==== either. Then, in Section 3, we consider time-dependent copulas. We start by showing that in two cases of interest, the asymptotic behavior of the maximum pseudo-likelihood estimation is independent of the estimated parameters of the generalized errors. As a particular case, one recovers the results of Chen and Fan [7] obtained for univariate stochastic volatility models. We also consider goodness-of-fit tests in this context, using conditional Rosenblatt transforms. It is shown that the asymptotic behavior of the associated empirical process does not depend on the unknown parameter ====. In Section 4, Monte Carlo experiments are performed to assess the power of the proposed goodness-of-fit test. Finally, an illustrative application is provided in Section 5.",Copula-based dynamic models for multivariate time series,https://www.sciencedirect.com/science/article/pii/S0047259X18302008,8 March 2019,2019,Research Article,259.0
"Bengs Viktor,Eulert Matthias,Holzmann Hajo","Fb. 12 - Mathematik und Informatik, Philipps-Universität Marburg, Hans-Meerwein-Straße 6, 35032 Marburg, Germany","Received 17 April 2018, Revised 27 February 2019, Accepted 28 February 2019, Available online 5 March 2019, Version of Record 20 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.017,Cited by (0),"We construct uniform and point-wise asymptotic confidence sets for the single edge in an otherwise smooth image function which are based on rotated differences of two one-sided ====. Using methods from M-estimation, we show consistency of the estimators of location, slope and height of the edge function and develop a uniform linearization of the contrast process. The uniform confidence bands then rely on a Gaussian approximation of the score process together with anti-concentration results for ====. The finite-sample performance of the point-wise proposed methods is investigated in a simulation study. An illustration to real-world image processing is also given.","Statistical methodology, in particular nonparametric regression, plays an important role nowadays in image reconstruction and denoising. In two-dimensional image functions, interest often focuses on the location of edges, i.e., discontinuity curves of the intensity function of the image. Edge estimation has been studied in the statistics literature from a minimax point-of-view in the monograph by Korostelev and Tsybakov [11]. Qiu [24] gives an overview of more practical reconstruction methods, which are partially motivated from the literature on computer vision.====Apart from mere estimation, statistical modeling allows for the construction of confidence regions, in which the object of interest is located with high probability. In this paper, we construct asymptotic confidence sets for the single edge in an otherwise smooth image function based on the rotational difference kernel method by Qiu [22] and Müller and Song [15], which is related to the Sobel edge detector from the image processing literature [23].====Confidence sets are by now well-developed for various problems in nonparametric statistics, e.g., for nonparametric density estimation [2], [4], [9], smooth regression functions [6], [18], and deconvolution and errors-in-variables problems [3], [5], [20]. Mammen and Polonik [13] and Qiao and Polonik [21] focus on more geometrical features, and construct confidence regions for density level sets and the density ridge, respectively.====When studying nonparametric regression problems with discontinuities, the focus may either be on the detection of potential jumps, or, assuming the existence of jumps, on estimation and the construction of confidence sets. In the univariate setting with a regression curve having a single jump, the latter problem was studied under various design assumptions by Müller [14], Loader [12], Gijbels et al. [8] and Seijo and Sen [26], among others. The problem of jump detection was additionally addressed, e.g., in [16], [19], [28], [30].====In bivariate problems, Korostelev and Tsybakov [11], Müller and Song [15], Qiu and Yandell, Qiu [25], [23], and Garlipp and Müller [7] focus on the estimation of a given discontinuity curve, while Wang [29], Qiu and Yandell [25], Kang and Qiu [10] and Qiu [23] also investigate edge detection. In the latter three papers, the set of jump-location curves is estimated as a point-set, where a certain criterion function exceeds a threshold value.====However, currently there seem to be no methods available to construct a confidence set for the edge in the bivariate case. Our approach to this problem uses the criterion function of the rotational difference kernel method [15], [22], for which we obtain necessary additional kernel conditions for asymptotic normality. The uniform confidence bands then rely on a Gaussian approximation of the score process together with anti-concentration results for suprema of Gaussian processes from Chernozhukov et al. [4], while point-wise bands are based on asymptotic normality. A technical difficulty in the problem are the distinct rates of the estimators of location and slope. As a byproduct of our analysis, we obtain uniform rates of convergence for the estimators of the jump-location-curve, the slope-curve as well as the height-curve which are optimal up to a logarithmic factor.====The paper is structured as follows. In Section 2 we introduce the model as well as the estimators for location and slope of the edge. The main theoretical results can be found in Section 3. Outlines of the proofs for these results are provided in Section 6. Section 4 contains a simulation study and an illustrative application of the proposed method to a real-world image, while Section 5 concludes. Detailed proofs are provided in the Online Supplement [1].====We shall use the following notation. If a sequence of random variables ==== converges in probability to a random variable ====, we write ==== or ====. If the convergence is in distribution, we write ====. Let ==== denote the ====-dimensional Lebesgue measure. For ====, ==== symmetric and positive definite, let ==== be the normal distribution with expectation ==== and covariance matrix ====. For a vector ====, we denote by ==== the ==== diagonal matrix with diagonal entries ====. Given ====, we denote by ==== the ====-quantile of a random variable ==== resp. by ==== the ====-quantile of a distribution ====. Let ==== denote the Euclidean scalar product on ====.",Asymptotic confidence sets for the jump curve in bivariate regression problems,https://www.sciencedirect.com/science/article/pii/S0047259X18302045,5 March 2019,2019,Research Article,260.0
"Hong Hyokyoung G.,Zheng Qi,Li Yi","Department of Statistics and Probability, Michigan State University, 19 Red Cedar Road, East Lansing, MI 48823, USA,Department of Bioinformatics and Biostatistics, University of Louisville, 485 East Gray Street, Louisville, KY 40202, USA,Department of Biostatistics, University of Michigan, 1415 Washington Heights Ann Arbor, MI 48109-2029, USA","Received 6 September 2018, Revised 18 February 2019, Accepted 19 February 2019, Available online 5 March 2019, Version of Record 15 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.011,Cited by (11),"Forward regression, a classical variable screening method, has been widely used for model building when the number of ","New biotechnologies have generated a vast amount of high-throughput data. In the Boston Lung Cancer Survival Cohort study, a hospital-based study for lung cancer patients, identifying high-throughput predictors such as molecular profiles that are associated with patients’ survival is a major research goal for understanding disease progression processes and designing more effective gene therapies. When the number ==== of covariates is less than the sample size ====, the Cox proportional hazards model has been routinely used for modeling survival data in many practical settings. When ====, penalized partial likelihood methods have been proposed by various authors [30], [41] and the oracle properties and statistical error bounds of estimation have been established [19]. However, when ====, these methods often fail because of serious challenges in “computational expediency, statistical accuracy, and algorithmic stability” [8]. Recently, Bradic et al. [3] established the oracle properties of the regularized partial likelihood estimates under a high-dimensional setting. However, the results require the estimates to be unique and global optimizers, which is, in general, difficult to verify, especially when the dimension of covariates is exceedingly high.====Forward regression has been widely used for model selection, but it has often been criticized for not achieving selection consistency as it fails to account for multiple comparisons in the model building process. Recently, some authors, e.g., [5], [14], [20], [24], [34], [39] have revamped forward regression in the context of linear regression or varying-coefficient linear models. The advantages can be summarized as follows. First, these authors have shown that, with some proper stopping criteria, forward regression can achieve screening consistency even in high-dimensional settings. Second, the variables are sequentially selected into the final model with the entry order determined by the size of the likelihood increment, which might reflect the relative importance of each selected variable. Third, the implementation is simple as no cross-validation for tuning parameters is needed. Finally, the method only needs assumptions on the original model and does not require restrictive faithfulness assumptions, in which the marginal models reflect the original model. However, to our knowledge, the aforementioned forward regression approaches are either based on the sum of residual squares from linear models [5], [34] or Lasso estimation [24]. It is unclear whether forward regression can be applied to more general regression settings, such as the Cox proportional hazards models.====In contrast, there has been active research in developing high-dimensional screening tools for survival data. These works include principled sure screening [37], feature aberration at survival times screening [13] and conditional screening [17], quantile adaptive sure independence screening [15], a censored rank independence screening procedure [27], and integrated powered density screening [16]; see [18] for an extensive review. However, the screening methods require a threshold to dictate how many variables to retain, for which unfortunately there are no clear rules. Zhao and Li [37] did tie the threshold with false discoveries, but it still needs to prefix the number of false positives that users are willing to tolerate. Recently, Li et al. [22] designed a model-free measure, namely the survival impact index, that sensibly captures the overall influence of a covariate on the survival outcome and can help guide selecting important variables. However, even this method, like the other screening methods, does not directly lead to a final model, for which extra modeling steps have to be implemented.====We introduce a new forward variable selection procedure for survival data based on partial likelihood. It selects important variables sequentially according to the increment of partial likelihood, with a stopping rule based on EBIC. We show that if the dimension of the true model is finite, within a finite number of steps forward regression can discover all relevant predictors, with the entry order determined by the size of the likelihood increment.====Our work is novel in the following aspects. It likely registers as the first attempt to thoroughly investigate the forward regression in high-dimensional survival settings, methodologically, theoretically and numerically. Technically this paper is also novel. First, our work represents technical advances and a broadened scope compared to the existing forward regression [5], [24], [34]. This may be the first work that investigates the partial likelihood-based forward regression in survival models with high-dimensional predictors, and establishes rigorous selection consistency results when the extended Bayesian information criterion (EBIC) [4] is used. It improves the partial likelihood-based variable selection developed by Volinsky and Raftery [33], Xu et al. [35] for survival data in low dimensional settings. Second, as partial likelihood is not a regular density-based likelihood, it fails to satisfy the requirements for theories of forward regression. We revisit partial likelihood and develop some new inequalities, based on which we establish the desired sure screening properties. The derived theoretical framework and techniques will facilitate the extension of the procedure to other general likelihood-based settings, such as generalized linear regression models. Finally, we note that forward selection starts with an empty model or some important variables identified a priori, and then sequentially recruits variables given important variables identified in the previous steps. This may resemble the conditional screening approach [17], which incorporates prior knowledge into variable screening. However, our method is valid even in the absence of such information.====The rest of the paper is organized as follows. In Section 2, we introduce the proposed forward regression procedure. In Section 3, we rigorously establish forward regression’s screening consistency and asymptotic normality under some regularity conditions. We carry out simulation studies to assess the performance of the proposed method in Section 4, and apply the method in Section 5 to analyze a subset of the Boston Lung Cancer Survival Cohort study, our motivating study for identifying biomarkers related to lung cancer patients’ survival. We conclude the paper with a natural extension of the proposal in Section 6. Technical proofs and all of the lemmas are presented in the Appendix.",Forward regression for Cox models with high-dimensional covariates,https://www.sciencedirect.com/science/article/pii/S0047259X18304615,5 March 2019,2019,Research Article,261.0
"Li Xinyi,Wang Li,Nettleton Dan","SAMSI / Department of Statistics and Operations Research, University of North Carolina at Chapel Hill, NC 27709, USA,Department of Statistics, Iowa State University, Ames, IA 50011, USA","Received 16 April 2018, Revised 17 February 2019, Accepted 18 February 2019, Available online 1 March 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.010,Cited by (2),"The additive partially linear model (APLM) combines the flexibility of nonparametric regression with the parsimony of regression models, and has been widely used as a popular tool in multivariate nonparametric regression to alleviate the “curse of dimensionality”. A natural question raised in practice is the choice of structure in the nonparametric part, i.e., whether the continuous ","In the past three decades, flexible and parsimonious additive partially linear models (APLMs) have been extensively studied and widely used in many statistical applications, including biology, econometrics, engineering, and social science. Examples of recent work on APLMs include [19], [20], [22], [23], [24], [28], [29]. APLMs are natural extensions of classical parametric models with good interpretability and are becoming more and more popular in data analysis.====Suppose we observe ====. For subject ====, ==== is a univariate response, ==== is a ====-dimensional vector of covariates that may be linearly associated with the response, and ==== is a ====-dimensional vector of continuous covariates that may have nonlinear associations with the response. We assume that ==== form a random sample from the distribution of ====, satisfying the model ====where ==== is the intercept, ==== are unknown regression coefficients, ==== are unknown smooth functions, and each ==== is centered with ==== to make model (1) identifiable. The ==== is a ====-dimensional vector of zero-mean covariates having density with a compact support. Without loss of generality, we assume that each covariate ==== can be rescaled into an interval ====. The ==== terms are iid random errors with mean zero and variance ====.====The APLM is particularly convenient when ==== is a vector of categorical or discrete variables, and in this case, the components of ==== enter the linear part of model (1) automatically, and the continuous variables usually enter the model nonparametrically. In practice, we might have reasons to believe that some of the continuous variables should enter the model linearly rather than nonparametrically. A natural question is how to determine which continuous covariates have a linear effect and which continuous covariates have a nonlinear effect. If the choice of linear components is correctly specified, then the biases in the estimation of these components are eliminated and root-==== convergence rates can be obtained for the linear coefficients. However, such prior knowledge is rarely available, especially when the number of covariates is large. Thus, structure identification, or linear and nonlinear detection, is an important step in the process of building an APLM from high-dimensional data.====When the number of covariates in the model is fixed, structure identification in additive models (AMs) has been studied in the literature. Zhang et al. [33] proposed a penalization procedure to identify the linear components in AMs in the context of smoothing splines ANOVA. They demonstrated the consistency of the model structure identification and established the convergence rate of the proposed method specifically under the tensor product design. Huang et al. [13] proposed another penalized semiparametric regression approach using a group minimax concave penalty to identify the covariates with linear effects. They showed consistency in determining the linear and nonlinear structure in covariates, and obtained the convergence rate of nonlinear function estimators and asymptotic properties of linear coefficient estimators; but they did not perform variable selection at the same time.====For high-dimensional AMs, Lian et al. [18] proposed a double penalization procedure to distinguish covariates that enter the nonparametric and parametric parts and to identify significant covariates simultaneously. They demonstrated the consistency of the model structure identification, and established the convergence rate of nonlinear function estimators and asymptotic normality of linear coefficient estimators. Despite the nice theoretical properties, their method heavily relies on the local quadratic approximation in [9], which is incapable of producing naturally sparse estimates. In addition, employing the local quadratic approximation can be extremely expensive because it requires the repeated factorization of large matrices, which becomes infeasible when the number of covariates is very large.====Note that all the aforementioned papers [13], [18], [33] about structure identification focus on the AM with continuous explanatory variables. However, in many applications, a canonical partitioning of the variables exists. In particular, if there are categorical or discrete explanatory variables, as in the case of the SAM data studies (see the details in Section 5) and in many genome-wide association studies, we may want to keep discrete explanatory variables separate from the other design variables and let discrete variables enter the linear part of the model directly. In addition, if there is some prior knowledge of certain parametric forms for some specific covariates, such as a linear form, we may lose efficiency if we simply model all the covariates nonparametrically.====The above practical and theoretical concerns motivate our further investigation of the simultaneous variable selection and structure selection problem for flexible and parsimonious APLMs, in which the features of the data suitable for parametric modeling are modeled parametrically and nonparametric components are used only where needed. We consider the setting where both the dimension of the linear components and the dimension of nonlinear components is ultra-high. We propose an efficient and stable penalization procedure for simultaneously identifying linear and nonlinear components, removing insignificant predictors, and estimating the remaining linear and nonlinear components. We prove the proposed ====parse ====odel ====dentification, ====earning and ====stimation (referred to as SMILE) procedure is consistent. We propose an iterative group coordinate descent approach to solve the penalized minimization problem efficiently. Our algorithm is very easy to implement because it only involves simple arithmetic operations with no complicated numerical optimization steps, matrix factorizations, or inversions. In one simulation example with ==== and ====, it takes less than one minute to complete the entire model identification and variable selection process on a regular PC.====After variable selection and structure detection, we would like to provide an inferential tool for the linear and nonparametric components. The spline method is fast and easy to implement; however, the rate of convergence is only established in mean squares sense, and there is no asymptotic distribution or uniform convergence, so no measures of confidence can be assigned to the estimators. In this paper, we propose a two-step spline-backfitted local-linear smoothing (SBLL) procedure for APLM estimation, model selection and simultaneous inference for all the components. In the first stage, we approximate the nonparametric functions ==== with undersmoothed constant spline functions. We perform model selection for the APLM using a triple penalized procedure to select important variables and identify the linear vs. nonlinear structure for the continuous covariates, which is crucial to obtain efficient estimators for the non-zero components. We show that the proposed model selection and structure identification for both parametric and nonparametric terms are consistent, and the estimators of the nonzero linear coefficients and nonzero nonparametric functions are both ==== norm consistent. In the second stage, we refit the data with covariates selected in the first step using higher-order polynomial splines to achieve root-==== consistency of the coefficient estimators in the linear part, and apply a one-step local-linear backfitting to the projected nonparametric components obtained from the refitting. Asymptotic normality for both linear coefficient estimators and nonlinear component estimators, as well as simultaneous confidence bands (SCBs) for all nonparametric components, are provided.====The rest of the paper is organized as follows. In Section 2, we describe the first-stage spline smoothing and propose a triple penalized regularization method for simultaneous model identification and variable selection. The theoretical properties of selection consistency and rates of convergence for the coefficient estimators and nonparametric estimators are developed. Section 3 introduces the spline-backfitted local-linear estimators and SCBs for the nonparametric components. The performance of the estimators is assessed by simulations in Section 4 and illustrated by application to the SAM data in Section 5. Some concluding remarks are given in Section 6. Appendix A evaluates the effect of different smoothing parameters on the performance of the proposed method. Technical details are provided in Appendix B.",Sparse model identification and learning for ultra-high-dimensional additive partially linear models,https://www.sciencedirect.com/science/article/pii/S0047259X18302021,1 March 2019,2019,Research Article,262.0
"Darolles Serge,Fol Gaëlle Le,Lu Yang,Sun Ran","Université Paris-Dauphine, PSL Research University, CNRS, DRM, Finance, 75016 Paris, France,Université Paris 13, Sorbonne Paris Cité, CNRS, CEPN, 93430, Villetaneuse, France","Received 15 August 2018, Revised 20 February 2019, Accepted 20 February 2019, Available online 28 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.015,Cited by (12),") and BINAR(====, which significantly facilitates non-linear forecasting and likelihood based estimation. Finally, a BINAR==== model with memory persistence is applied to open-ended mutual fund purchase and redemption order counts.","Nonnegative low count processes have been widely used in domains such as marketing [4], economics [3], finance [22], insurance [20] and beyond, ever since the seminal work of McKenzie [34]. Our interest in this paper lies in the monitoring of the liquidity risk of an open-ended mutual fund (MF). An MF channels investors’ cash investment into less liquid assets, and is thus structurally vulnerable to liquidity risk. This risk has recently received much attention from the regulators [1], [10], but its quantification and management remain difficult. Indeed, from the modeling point of view, the liquidity risk is quite different from traditional market risks in that it involves the daily counts of redemption and purchase orders, which are (i) most of the time low integers or zero, but also have a non-null probability of taking mildly large values; (ii) both cross-sectionally and serially dependent, with significant heteroscedasticity.====Recently, the MF industry has started to record purchase and redemption order count data separately. This allows to distinguish auto-correlation effects and cross-effects between the two count processes, which have different economic interpretations. For instance, the clustering of the redemption counts corresponds to fund run, whereas a fund manager usually reacts to past redemptions by seeking new investors in order to stabilize the fund size, leading to a positive feedback effect between past redemption and current purchase counts. Therefore, a bivariate count analysis can be of great interest to understand clients’ behavior and the manager’s reaction to exogenous liquidity shock.====Yet the literature on bivariate count processes is still in its infancy. The benchmark approach is the first order Bivariate INteger-valued AutoRegressive model [27], [36], which assumes that, for each time ====, ====where given ====, the binomial thinning operators are defined as follows: for each ====, the variable ==== is binomial with size ==== and success probability ====. Moreover, these variables are conditionally independent, and are also independent of the i.i.d. innovation sequence ====.====This approach has several drawbacks. First, the conditional independence assumption between the thinning operators restricts significantly the dependence feature. Second, so far only Latour [27] has considered higher-order models, but he suggests to base the estimation and forecasting solely on conditional expectation, i.e., as if the observations are continuous, and nothing is said about the empirical performance of this approach. This is due to the fact that the term structure of predictive distributions of higher-order BINAR process is yet to be derived and is so far (wrongly) considered intractable. These downsides seriously limit their usefulness for risk management and forecasting purposes.====Besides BINAR processes, other non-thinning-based models have been introduced. Quoreshi [37] and Livsey et al. [29] proposed parameter-driven models with flexible (auto-)correlation, but the likelihood estimation and forecasting in these models are way too cumbersome to be feasible. Another popular approach is the bivariate INGARCH model [28], which assumes that given the past, ==== and ==== follow some simple (say, Poisson) distributions, with parameters that are exponentially weighted averages of past observations. Then the contemporaneous conditional dependence between ==== and ==== is captured by a copula [2], [13], [22]. The downsides of the latter approach are that (i) it is sometimes unclear whether such processes are strictly stationary and how memory persistence can be introduced; (ii) forecasting formulas beyond the conditional expectation necessitate numerical integration; (iii) it remains an open question as to whether the copula is identified in the count data setting, as discussed, e.g., in [16], [17]. Recently, some alternative, non-INGARCH models were proposed in [9], [21], [40], and they allow for rather flexible serial dependence. However, it is computationally difficult to extend them to models of arbitrary orders. For instance, the model of [9], as well as INGARCH models, are necessarily infinite-order Markov, whereas that of [21] is first-order Markov. Further, [9], [21] assume ex ante the conditional distributions of ==== and ==== given the past to be equi- (resp. over-) dispersed.====The paper that is closest to ours is that of Scotto et al. [40]. While these authors are mainly interested in bounded counts, they mention in their conclusion some possible extensions of model (1), which they conjecture to be appropriate for unbounded count data. In this paper, we show that one of these extensions has indeed tractable properties, even after extension to higher-order cases.====More precisely, we contribute to the BINAR literature in two ways. First, we extend model (1), called independent BINAR(====) henceforth, by introducing (positively or negatively) dependent thinning operators and arbitrary innovation distribution. We show that the process belongs to the compound autoregressive (CaR) family, and possesses intuitive aggregation and stationarity properties. We then go on to clarify that in this family of BINAR models, the predictive distributions at various horizons are easily computable via a matrix-based algorithm. This largely facilitates likelihood-based inference and non-linear forecasting, especially when it comes to the prediction of extreme events. Second, we extend our model to higher-order dependent BINAR(====) and BINAR(====) processes, which can better capture slowly decaying serial correlation patterns.====The paper is organized as follows. The dependent BINAR(====) model is introduced in Section 2 and extended in Section 3 to higher-order BINAR==== and BINAR==== models. The predictive distributions are computed in Section 4 and the model is applied in Section 5 to forecast the counts of share purchase and redemption of an MF. Section 6 contains concluding remarks. Proofs and technical details are gathered in Appendix A.",Bivariate integer-autoregressive process with an application to mutual fund flows,https://www.sciencedirect.com/science/article/pii/S0047259X18304329,28 February 2019,2019,Research Article,263.0
Pham Ngoc Thanh Mai,"Laboratoire de mathématique d’Orsay, Université Paris-Sud, CNRS, Université Paris-Saclay, 91405 Orsay, France","Received 7 August 2018, Revised 14 February 2019, Accepted 14 February 2019, Available online 27 February 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.009,Cited by (5), error are derived. These theoretical results are illustrated with simulations.,"Directional data arise in many fields such as wind direction for the circular case, and astrophysics, paleomagnetism, geology for the spherical case. Much effort has been made to devise statistical methods to tackle the density estimation problem. We refer to [19] and more recently to [18] for a comprehensive review. Nonparametric procedures have been well developed.====In this article we focus on kernel density estimation. Various works [3], [11] have used projection methods on localized bases adapted to the sphere. Classical references for kernel density estimation with directional data include the seminal papers [2], [9]. It is well known that the choice of the bandwidth is a key and intricate issue when using kernel methods. In practice, various techniques for selecting the bandwidth have been suggested since the popular cross-validation rule in [9]. Let us mention the plug-in and refined cross-validatory methods in [21], [24] for the circular case, and [5] on the torus.====Recently, García-Portugués [6] devised an equivalent of the rule-of-thumb of [23] for directional data, and Amiri et al. [1] explored computational problems with recursive kernel estimators based on the cross-validation procedure of [9]. To the best of our knowledge, however, the various rules that have been proposed so far for selecting the bandwidth in practice have not been assessed from a theoretical point of view. In particular, there are no results proving that cross-validation is adaptively rate-optimal, even in the linear case. From a theoretical point of view, Klemelä [13] studied convergence rates for ==== error over some regularity classes. Unfortunately, the asymptotically optimal bandwidth in [13] depends on the density and its degree of smoothness, which is infeasible in practice.====In the linear case, kernel bandwidth selection rules have been proposed, leading to adaptive estimators which attain optimal rates of convergence. By adaptive we mean that the estimator is adaptive to the degree of smoothness of the underlying density: the method does not require the specification of the regularity of the density. In this regard, we may cite the remarkable series of papers [8], [15], [16] and the recent work of Lacour et al. [14]. The drawback of the methods in [8], [15], [16] is that they involve tuning parameters. It is well known that in nonparametric statistics, minimax and oracle theoretical results rarely give optimal choices for tuning parameters from a practical point of view with very conservative choices. The major interest of the procedure in [14] is that it is free of tuning parameters, which constitutes a great advantage in practice. The approach in [14] called PCO (Penalized Comparison of Overfitting) is based on concentration inequalities for ====-statistics.====In the present paper, we aim at filling the gap between theory and practice in the directional kernel density estimation literature. Our goal is to construct a fully data-driven bandwidth selection rule providing an adaptive estimator which reaches minimax rates of convergence for ==== risk over some regularity classes. This motivates our choice to adapt the method of Lacour et al. [14] to the directional setting. Our procedure is simple to implement and in examples based on simulations, it shows quite good performances in a reasonable computation time.====This paper is organized as follows. In Section 2, we present our estimation procedure. In Section 3 we provide an oracle inequality and rates of convergences of our estimator for the MISE (Mean Integrated Squared Error). Section 4 gives some numerical illustrations. Section 5 gives the proofs of theorems. Finally, the Appendix gathers technical propositions and lemmas.====The following notation is used throughout. For two integers ====, ====, we denote ==== and ====. For arbitrary ====, ==== denotes the integer part of ====. Depending on the context, ==== denotes the classical ==== norm on ==== or ====. For any integer ====, we denote the unit sphere of ==== by ==== and the associated scalar product by ====. For a vector ====, ==== stands for the Euclidean norm on ==== while ==== is the usual ====-norm on ====. Finally, the scalar product of two vectors ==== and ====, is denoted by ====, where ==== is the transpose operator.",Adaptive optimal kernel density estimation for directional data,https://www.sciencedirect.com/science/article/pii/S0047259X18304160,27 February 2019,2019,Research Article,264.0
"Krupskii Pavel,Joe Harry","Department of Statistics, University of British Columbia, Vancouver, Canada V6T 1Z4","Received 28 February 2018, Available online 27 February 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.02.013,Cited by (10),We propose three methods for estimating the joint tail ==== based on a ====-variate ==== with dimension ,"Classical multivariate statistics assumes multivariate normality, which may be fine for inferences about central quantities but not for joint tail probabilities. However, departures from normality are often observed in multivariate data, in the form of tail dependence or tail asymmetry. Incorrect inferences from misspecified multivariate normal models can be serious for risk analysis. Flexible copula models can handle different joint tail behavior and have become popular in finance [2], applications to spatial data [18], and extremes data [15], among others.====Different copula families have different tail properties and good choices of the copula model are important for tail inferences. Unless the sample size is very large, it can be very difficult to distinguish copulas with different tail properties because the likelihood approach puts more weight on the middle of the multivariate distribution. This means parametric methods might be unreliable when extrapolating to joint tail probabilities or multivariate tail dependence coefficients. Accurate estimates of joint tail probabilities are required in finance, for example, to estimate tail risk or for stress testing, in hydrology, for example, to estimate return periods of an extreme event when several factors contribute to the event, and other applications.====We propose three methods for estimating the joint upper tail probabilities nonparametrically. The joint upper tail probability that we focus on here is that each of ==== continuous random variables exceeds its upper ==== quantile where ==== is small, e.g., 0.01. This probability can be expressed in terms of the underlying ====-variate copula ==== as ====, where ==== is a vector of ====s of length ====, and we make use of a tail expansion of ====, the survival function of ====. The approaches can be extended to small unequal upper ==== quantiles for the ====th variable because there is a similar tail expansion for all rays to the corner.====The family of tail-weighted measures of dependence ==== in [20] can be extended to functions of ==== which we denote as ====. Tail-weighted measures of dependence are important for assessing tail asymmetry of the joint upper tail versus the joint lower tail, because they can be estimated more precisely than tail dependence coefficients. However, our family of tail-weighted dependence measures also leads to estimates of the multivariate tail dependence coefficient ====.====Estimates of the tail coefficients and tail probabilities can be used as a diagnostic tool for selecting an appropriate copula family for multivariate data and assessing the adequacy of fit in the tail region. Also by checking the pattern of these tail quantities for several different nested margins, one gets a clearer idea of (i) the strength of joint tails as the dimension increases and (ii) the relative strength of dependence in the joint upper tail versus the joint lower tail.====In the first method of estimating the joint upper tail probability ==== for small ====, we note that this is a univariate tail probability for the maximum of random variables with distribution ====, so that the coefficients of the tail expansion of ==== can be estimated using the maximum likelihood approach and the maxima data beyond a tail threshold. For the second method, coefficients of the tail expansion of ==== can be estimated using the maximum likelihood approach with univariate maxima below a threshold that are based on pseudo-replicated data, and the estimated coefficients can be used to estimate the joint tail probability for the original variables. These two methods have analogies to the threshold method for univariate extremes. The third method uses the tail expansion of ==== for large ====, and interprets ==== as an expectation so that it has a simple empirical version. The coefficients of this expansion can be estimated using a linear regression and they can be used to compute the joint tail probabilities. The three methods can be also used to estimate multivariate tail dependence coefficients for a ====-variate copula with dimension ====.====A key difference between our methods and others in the literature is the use of a second order term in the expansion when the copula has tail dependence, or is in the domain of attraction of an extreme-value distribution with strictly positive dependence. The authors of [12] assume tail dependence for their estimation method. In [5] an estimator of extreme failure probabilities is proposed in the bivariate case and the generalization to the multivariate case is discussed. An M-estimator is proposed in [7] and in [6] an updating weighted least squares estimator of parameters of the tail function for a given parametric model is developed; see also [27] for a comparison of the tail function estimators. This assumption can be replaced by some other regularity assumptions; see [26] and references therein. However these methods are not suitable when ==== where the tail expansion could have the form ==== as ====, where ==== and ==== is a slowly varying function. This corresponds to what is called asymptotic independence in the extreme value literature, but tail orthant independence only occurs for ==== and there is still dependence in the joint tail if ====.====For other approaches, to estimate bivariate tail probabilities, see [4] in which several nonparametric estimators of the tail dependence coefficient of a bivariate copula are proposed, [8] in which different estimation methods for bivariate tail dependence coefficients are discussed, and [22] in which properties of different nonparametric estimators of multivariate tail dependence functions are studied. These methods also do not make use of a second order term of tail expansions.====The rest of the paper is organized as follows. In Section 2 we introduce the tail expansions, the three estimation methods and we show how parameters of the copula tail expansion can be estimated. In Section 3 we provide more details on the maximum likelihood estimation and asymptotic properties of the estimators, and in Section 4 we evaluate the proposed methods on simulated data sets generated using different copula families. In Section 5, we apply these methods to analyze tail properties of a data set of financial returns. Section 6 concludes with a discussion of further research directions. Some technical details are deferred to the Appendix A Proof of, Appendix B Tail quantities for factor copulas.",Nonparametric estimation of multivariate tail probabilities and tail dependence coefficients,https://www.sciencedirect.com/science/article/pii/S0047259X18301222,27 February 2019,2019,Research Article,265.0
"Díaz Mateo,Quiroz Adolfo J.,Velasco Mauricio","Center for Applied Mathematics, 657 Frank H.T. Rhodes Hall, Cornell University, Ithaca, NY 14853, USA,Departamento de Matemáticas, Universidad de los Andes, Carrera 1 No. 18a 10, Edificio H, Primer Piso, 111711 Bogotá, Colombia","Received 7 May 2018, Revised 19 February 2019, Accepted 20 February 2019, Available online 26 February 2019, Version of Record 12 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.014,Cited by (9),"For data living in a manifold ==== and a point ==== which estimates the variance of the angle between pairs ==== of vectors, for data points ====, ====, near ==== at ==== is found under minimal regularity assumptions. Performance of the proposed methodology is compared against state-of-the-art methods on simulated data and real datasets.","Understanding complex datasets often involves dimensionality reduction. This is particularly necessary in the analysis of images, and when dealing with genetic or text data. Such datasets are usually presented as collections of vectors in ==== and it often happens that there are non-linear dependencies among the components of these data vectors. In more geometric terms, these non-linear dependencies amount to saying that the vectors lie on a submanifold ==== whose dimension ==== is typically much smaller than ====. The expression manifold learning has been coined in the literature for the process of finding properties of ==== from the data points.====Several authors in the artificial intelligence literature have argued about the convenience of having methods to find or approximate these low-dimensional manifolds [3], [14], [26], [28], [30], [31]. Procedures for achieving this kind of low dimensional representation are called manifold projection methods. Two fairly successful such methods are Isomap by Tenenbaum et al. [31] and the Locally Linear Embedding method of Roweis and Saul [26]. For these and other manifold projection procedures, a key initial ingredient is a precise estimation of the integer ====, ideally obtained at low computational cost.====The problem of estimating ==== has been the focus of much work in statistics starting from the pioneering work of Grassberger and Procaccia [15]. Many of the most recent dimension identification procedures appearing in the literature are either related to graph-theoretic ideas [7], [8], [22], [23], [34] or to nearest neighbor distances [12], [20], [21], [24]. A key contribution of the latter group is the work of Levina and Bickel [20], who propose a “maximum likelihood” estimator of intrinsic dimension. To describe it, let ==== be the distance from the sample point ==== to its ====th nearest neighbor in the sample with respect to the Euclidean distance in the ambient space ====. Levina and Bickel show that asymptotically, the expected value of the statistic ====coincides with the intrinsic dimension ==== of the data. As a result, they propose the corresponding sample average ==== as an estimator of dimension. Asymptotic properties of this statistic have been obtained in the literature (see Theorem 2.1 in [23]) allowing for the construction of confidence intervals. Both the asymptotic expected value and the asymptotic distribution are independent of the underlying density from which the sample points are drawn and thus lead to a truly nonparametric estimation of dimension.====In addition to distances, Ceruti et al. [11] propose that angles should be incorporated in the dimension estimators. This proposal, named ====, combines the idea of norm concentration of nearest neighbors with the idea of angle concentration for pairs of points on the ====-dimensional unit sphere. The resulting dimension identification procedure is relatively involved. The method combines two ideas. On one hand it uses the Kullback–Leibler divergence to measure the distance between the estimated probability density function (pdf) of the normalized nearest neighbor distance for the data considered and the corresponding pdf of the distance from the center of an ====-dimensional unit ball to its nearest neighbor under uniform sampling. On the other hand, it uses a concentration result due to Södergren [29] for angles corresponding to independent pairs of points on a sphere.====The main contribution of this article is a new and simple dimension identification procedure based solely on angle concentration. We define a ====-statistic which averages angle squared deviations over all pairs of vectors in a nearest neighbor ball of a fixed point and determine its asymptotic distribution. In the basic version of our proposed method, there is no need of calibration of distributions. Moreover, our statistic is a ====-statistic among dependent pairs of data points and it is well known that these offer fast convergence to their mean and asymptotic distribution; see, in our specific context, Proposition 1.====Our method has been called ANOVA in the literature (see [6]), given that the ====-statistic used, ==== to be defined below, is an estimator of the variance of the angle between pairs of vectors among uniformly chosen points in the sphere ====. Our main results are to prove the consistency of the proposed method of estimation (Proposition 5) and the description of the (suitably normalized) asymptotic distribution of the statistic considered (Theorem 5).====Statistics related to angles between uniformly distributed random points on unit spheres have been considered in the literature due to their potential applications on a number of fields, including physics, engineering, and biology. For a good account of the available results regarding the average angle distribution, maximum and minimum angle distribution and sphere packing distributions, see [9] and [35].====We describe our proposed method in Section 2, where some statistical properties are established related to angle-variance estimation. More theoretical justification for the proposed method, in the context of manifolds, is provided in Section 3. Sections 4 Estimators, 5 Numerical results discuss the details of our implementation of the dimension identification procedure together with some empirical improvements. It also contains the result of performance evaluations on simulated examples and real datasets, including comparisons with current state-of-the-art methods.",Local angles and dimension estimation from data on manifolds,https://www.sciencedirect.com/science/article/pii/S0047259X18302446,26 February 2019,2019,Research Article,266.0
"Gunawardana Asanka,Konietschke Frank","Charité, — Universitätsmedizin Berlin, Corporate Member of Freie Universität Berlin, Humboldt-Universität zu Berlin, and Berlin Institute of Health, Institute of Biometry and Clinical Epidemiology, Charitéplatz 1, 10117 Berlin, Germany,Berlin Institute of Health (BIH), Anna-Louisa-Karsch-Straße 2, 10178 Berlin, Germany,Department of Mathematical Sciences, The University of Texas at Dallas, Richardson, TX 75080, USA","Received 26 September 2018, Revised 19 February 2019, Accepted 19 February 2019, Available online 26 February 2019, Version of Record 9 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.016,Cited by (8),We develop purely nonparametric multiple inference methods for general ,"In many scientific fields, for example in chemistry, biology, medicine, or in agriculture, the most frequently used experimental or observational study designs rely on multivariate layouts. Such designs can have more than two possibly correlated response variables (endpoints) observed on each experimental unit and should allow comparisons across different treatment groups. The classical parametric assumption of existing inference methods is that the observations follow multivariate normal distributions with equal covariance matrices across the groups. Such assumptions are almost impossible to justify in real observations. Furthermore, data do not follow a normal distribution in many practical situations (e.g., count data or waiting times) and the different response variables may even be measured on different scales (e.g., body weights and blood pressure); hence the assumption of specific covariance structures is inadequate. In fact, existing methods that rely on the assumption of equal covariance matrices tend to be highly liberal or conservative when the covariance matrices of the different groups are actually different ([32], [9]). Therefore, a nonparametric approach is desirable that is valid even when covariance matrices are different — even under the null hypothesis of no treatment effect (()).====In multivariate data analysis, it is particularly important that the statistical inference methods can be used to test the global null hypothesis, i.e., all treatment groups have the same effect, as well as provide multiplicity adjusted ====-values for multiple comparisons and simultaneous confidence intervals (SCIs) for the effects of interest. Most existing nonparametric multivariate methods are global testing procedures, which (i) can only be used to test hypotheses ==== formulated in terms of the distribution functions of the data and thus assume identical covariance matrices across the groups (see, e.g., [4] and references therein) and (ii) cannot provide compatible multiple comparisons and SCIs. Here, compatibility means that whenever any individual null hypothesis has been rejected by the multiple comparison procedure, the corresponding SCI does not contain the null — the hypothetical value of no treatment effect. It is also important to note that dependency across the endpoints decreases precision and power unless the correlations are taken into account by the multiplicity adjustment. Up to now, no adequate purely nonparametric inference methods are available to compute such compatible multiple comparisons and SCIs for multivariate data.====Thus, there is a necessity for nonparametric statistical procedures, which can be used for (i) testing the global null hypothesis, (ii) performing multiple comparisons, and (iii) for the computation of compatible SCIs that take the correlations between the test statistics and thus also the correlation between the endpoints into account without assuming any particular distribution of the observations.====The remainder of the paper is organized as follows. In Section 2, the need for such methods is illustrated by evaluating a real toxicological study with existing methods. The statistical model, treatment effects, and hypotheses are presented in Section 3. Consistent estimators for the treatment effects and their asymptotic distributions are derived in Section 4, where the asymptotic covariance structure of the statistics is also developed in a simple and unified way. In Section 5, multiple contrast tests (MCTs) and compatible SCIs are derived. A variance-stabilized test statistic and an approximation for small sample sizes are presented in Section 6. To investigate the quality of the approximations of the proposed methods, extensive simulation studies are carried out in Section 7. In Section 8, the application of the new methods is illustrated using the motivating example. The paper closes with a discussion of the proposed methods and new directions for future research in Section 9. All technical details, proofs, and derivations are provided in the Appendix.====Throughout the paper, we use the following notation. The vectors ==== and ==== denote column vectors of 0s and 1s, respectively. Furthermore, ==== is the ==== identity matrix and ==== is the ==== matrix of ones. The notations ==== and ==== denote the direct sum and the Kronecker product of the matrices ==== and ====, respectively. Further, ==== denotes the vector operator that stacks the columns of a matrix on top of one another.",Nonparametric multiple contrast tests for general multivariate factorial designs,https://www.sciencedirect.com/science/article/pii/S0047259X18305086,26 February 2019,2019,Research Article,267.0
"Hofert Marius,Oldford Wayne,Prasad Avinash,Zhu Mu","Department of Statistics and Actuarial Science, University of Waterloo, 200 University Avenue West, Waterloo, Ontario, Canada N2L 3G1","Received 12 January 2018, Available online 26 February 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.02.012,Cited by (8),". Finally, data applications to bioinformatics and finance are presented along with a general graphical assessment of independence between groups of random variables.","While there are numerous well established methods to measure dependence between random variables, the extension to random vectors poses a significant challenge. This challenge arises from the lack of a unique axiomatic framework that states desirable properties a measure of association between random vectors should exhibit. There is no unique way of extending bivariate measures of association to arbitrary dimensions and the available multivariate measures of association do not naturally capture dependence between more than one random vectors, as is of interest for applications in areas such as bioinformatics, finance, insurance or risk management.====Proposed solutions to this problem are rather difficult to find in the literature. A classical methodology for summarizing linear dependence between random vectors is the well known canonical correlation coefficient; see [28]. A non-linear extension of canonical correlation has been suggested through the use of kernel functions in [5] and [19]. A faster version of the kernel canonical correlation method has been developed by adopting the idea of randomized kernels; see [33]. Székely et al. [47] proposed a novel distance covariance coefficient, defined as a weighted ====-norm between the joint characteristic function and the product of marginal characteristic functions of the random vectors under consideration. In the context of copula modeling, Grothe et al. [22] recently derived versions of Spearman’s rho and Kendall’s tau between random vectors and corresponding estimation procedures. Our framework will generalize their approach and allow us to derive various results as by-products.====Note that there is neither a generally accepted nor a canonical way of measuring dependence between random vectors. As a result, one can think of multiple ways of quantifying such dependence. Approaches are primarily motivated by the purpose, for example, by the detection or ranking of dependencies, or by the dataset under investigation. In this paper, we subsume several such approaches under a general framework which allows us to detect, quantify, and visualize dependence between random vectors.====The paper is organized as follows. In Section 2 we present a framework for measuring dependence between random vectors. In Section 3 we then present non-parametric estimators for the measures of association arising from the framework and their corresponding asymptotic properties. Section 4 develops the notion of a collapsed distribution function and a collapsed copula, and presents analytical formulas for a number of collapsing functions. Empirical examples from the areas of bioinformatics and finance are covered in Section 5. In addition, a visual assessment of independence between groups of random variables is introduced. Section 6 provides concluding remarks.",A framework for measuring association of random vectors via collapsed random variables,https://www.sciencedirect.com/science/article/pii/S0047259X1830023X,26 February 2019,2019,Research Article,268.0
"Chen Feifei,Meintanis Simos G.","School of Statistics, Renmin University of China, Beijing, PR China,Department of Economics, National and Kapodistrian University of Athens, Athens, Greece,Unit for Business Mathematics and Informatics, North–West University, Potchefstroom, South Africa,Department of Mathematics, Hong Kong Baptist University, Hong Kong","Received 26 April 2018, Revised 13 February 2019, Accepted 13 February 2019, Available online 22 February 2019, Version of Record 7 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.006,Cited by (20)," are conducted to examine the performances of the criteria. It is shown that the normal weight function, which is the hitherto most often used, is seriously suboptimal. The procedures are biased in the sense that the corresponding test criteria degenerate in high dimension and hence a bias correction is required as the dimension tends to infinity.","In this paper we are concerned with three major problems of statistical inference, namely those of testing homogeneity, testing symmetry and testing for independence. Specifically, and in the context of each problem, we will formulate a population measure which characterizes the underlying stochastic property of homogeneity, symmetry or independence in the sense of taking value zero under the corresponding null hypothesis while being strictly positive otherwise. We also propose empirical versions of the three population measures and study several aspects of the resulting test criteria including asymptotic as well as finite-sample behavior. The common theme of the aforementioned population measures is that they are expressed as weighted ====-type distances involving the characteristic function (CF) of the underlying law. Particular instances of these population measures have appeared as characterizations earlier in the literature and corresponding test statistics have gained considerable popularity. We will show that most of these special cases result from choosing a particular member of the spherical stable family of distributions in the aforementioned weighting scheme. In this connection we will make repeated use of the fact that the CF of a spherical stable law is given by ====where ==== stands for the density of a spherical stable law in ==== with characteristic exponent ====, and ==== denotes the CF of a given random variable ====, ==== denotes the Euclidean norm. When clear from the context we will suppress the dimension and simply write ====. The spherical stable family includes the multivariate standard Gaussian and Cauchy distributions as special cases, for ==== and ====, respectively. Further information regarding these distributions, including expressions for corresponding distribution functions and densities, may be found in [32], [48].====The remainder of the paper is as follows. In Section 2 we introduce the three null hypotheses separately and in each case we deduce the appropriate distance measure and the corresponding characterization; we also provide connections between these quantities and the existing literature. Section 3 provides interpretations of these distance measures in terms of density deviations. In Section 4, three test criteria are computed, each for the respective null hypothesis considered and it is seen that earlier test criteria are particular instances of our quantities. A Bayesian type interpretation for the choice of the weight function and affine invariant versions of the tests statistics are also discussed. Asymptotic results can be found in Section 5, while the results of a series of Monte Carlo trials are presented and discussed in Section 6. The paper concludes in Section 7 with a summary of findings. All proofs of the theoretical results are presented in the Appendix.","On some characterizations and multidimensional criteria for testing homogeneity, symmetry and independence",https://www.sciencedirect.com/science/article/pii/S0047259X18302203,22 February 2019,2019,Research Article,269.0
"Petrella Lea,Raponi Valentina","MEMOTEF, Sapienza, University of Rome, Rome, Italy,Imperial College Business School, Imperial College London, London, United Kingdom","Received 16 May 2018, Revised 14 February 2019, Accepted 14 February 2019, Available online 20 February 2019, Version of Record 28 February 2019.",https://doi.org/10.1016/j.jmva.2019.02.008,Cited by (32),This paper proposes a maximum likelihood approach to jointly estimate marginal conditional ,"Quantile regression has become a widely used technique in many empirical applications, since the seminal work of Koenker and Basset [30]. It provides a way to model the conditional quantiles of a response variable with respect to a set of covariates in order to have a more complete picture of the entire conditional distribution than the ordinary least squares regression. This approach is quite suitable to be used in all the situations where specific features, like skewness, fat-tails, outliers, truncation, censoring and heteroscedasticity arise. In fact, unlike standard linear regression models, which only consider the conditional mean of a response variable, quantile regression allows one to assume that the relationship between the response and explanatory variables can vary across the conditional distribution of the dependent variable.====Many univariate quantile regression methods are now well consolidated in the literature and have been implemented in a wide range of different fields, like medicine [14], [39], [44], survival analysis [31], financial and economic research [4], [24], [42], and environmental modeling; see, e.g., [24], [41] for a discussion. Koenker [28] provides an overview of the most used quantile regression techniques in a classical setting. In longitudinal studies, quantile regression models with random effects are also analyzed, in order to account for the dependence between serial observations on the same subject; see, e.g., [21], [27], [29], [38] for references. Bayesian versions of quantile regression have also been extensively proposed; see [6], [34], [35], [53].====It is well known in the literature that the univariate quantile regression approach has a direct link with the asymmetric Laplace (AL) distribution. In fact, while the frequentist quantile regression framework relies on the minimization of the asymmetric loss function introduced by Koenker and Basset [30], the Bayesian approach introduces the asymmetric Laplace (AL) distribution as an inferential tool to estimate model parameters; see the seminal work [53]. The two approaches are justified by the well established relationship between the loss function and the AL density. That is, the loss function minimization problem is equivalent (in terms of parameter estimates) to the maximization of the likelihood associated with the AL density; see, e.g., [37]. Therefore, the AL distribution could offer a convenient device to implement a likelihood-based inferential approach when dealing with quantile regression analysis.====Still in the context of univariate regression framework, part of the literature is concerned with estimation of multiple quantiles, say ====, of a given response variable ====. In this case, joint estimation of these ==== quantiles provides a gain in efficiency compared to traditional sequential estimation of multiple quantiles. Cho et al. [13], for example, show that the asymptotic covariance obtained from the multiple quantile regression model is always smaller than the one obtained from the single quantile regression model. Even though only one quantile regression is of particular interest, their proposed simultaneous multiple quantile estimation may also be desirable when simultaneous estimation is supported by reasonably sized samples. For example, a more efficient median regression estimator could be obtained by estimating the 25th and 75th quantile regression estimators together. Hence, a multiple quantile estimation approach is more efficient than either ignoring the inter-quantile correlation or estimating regression parameters individually.====When multivariate response variables are concerned, the existing literature on quantile regression is less extensive. The multivariate quantile problem has the goal to estimate the multivariate quantile, ====, of a multivariate response variable ====, with ==== and where the index ==== is a scalar. In this case, the main challenge concerns the definition of a multivariate quantile, given that there is no natural ordering in a ====-dimensional space [11], [23], [32]. Other attempts and extensions can be also found in [2], [8], [33], [40], [46].====The main goal of the present paper is to extend the univariate linear quantile regression methodology to a multivariate context. In particular we want to generalize the inferential approach based on the AL distribution to a multivariate framework, by using the multivariate asymmetric Laplace (MAL) distribution introduced by Kotz et al. [36]. In this way we are not concerned to define a multivariate quantile. Instead, we are conducting a simultaneous inference on the marginal conditional quantiles of a multivariate response variable, taking also into account for the possible correlation among marginals.====This need could arise in many situations where different responses might have similar distributions or might be affected by the same set of covariates at different parts of their distributions other than the mean. Hence, by jointly modeling them, we can borrow information across responses and conduct joint inference at the marginal quantiles level, rather than defining a central point for their distributions.====Similar attempts in the literature have been proposed by Jun and Pinkse [26], who introduced a seemingly unrelated quantile regression approach which entails a nonparametric estimation of a set of moment conditions for the conditional quantiles of interest. Waldmann and Kneib [49] proposed a bivariate quantile regression using a Bayesian approach and showed how to estimate the conditional correlations between the two response variables. Their work, however, has not been generalized to the multivariate case.====Our paper offers a likelihood-based approach for modeling and estimating conditional marginal quantiles jointly, by using the MAL distribution as working likelihood in a linear regression framework. Specifically, we propose a slight reparameterization of the MAL distribution, subject to some specific constraints, which allows us to estimate regression coefficients via maximum likelihood (ML), accounting for the possible association among the responses. The inferential problem is solved by developing a suitable Expectation–Maximization (EM) algorithm, which exploits the mixture representation of the MAL distribution; see also [3].====Using simulation exercises, we assess the validity and the robustness of our approach by considering different model distributional settings. We find that the estimation of the regression coefficients is not highly affected by the MAL distributional assumption. Moreover, the estimation efficiency of our multivariate approach is higher than the one obtained by running separate single quantile regression models on the marginals when estimating model parameters. That is, taking into account for the potential association among the response variables can significantly reduce the root mean square error of the estimated coefficients, hence improving the precision of the estimates.====When dealing with multivariate regression, the high dimensionality setting is an intrinsic part of the model building problem. In order to gain in parsimony and to conduct a variable selection procedure, we consider the penalized Least Absolute Shrinkage and Selecting Operator (LASSO) approach proposed by Tibshirani [47]. In particular, we propose a penalized version of the EM algorithm (PEM) accounting for an ==== penalty term. We evaluate the estimation performance of the proposed approach through a simulation exercise, where we compute the bias and the root mean square error of the estimated parameters at different quantile levels.====The relevance of our approach is also shown empirically, contributing to the increasingly widespread literature that uses quantiles as measures of risk. In the recent years, due the financial and economic crises, particular attention has been devoted to measuring and quantifying the level of financial risk and financial distress within a firm or investment portfolios. In this respect, many risk measures developed in literature are based on quantile values, like for example the Value-at-Risk; see [17], [50]. Moreover, quantile regression methods turn out to be very helpful to quantify either the magnitude and the causes of riskiness; see, e.g., [18], [51].====In this paper, we implement the proposed quantile regression approach to investigate the main determinants of financial distress on a sample of 2020 Italian firms. In particular, we use the definition of financial distress adopted in [5], which classifies a firm as financially distressed if its earnings before interest and taxes depreciation and amortization (EBITDA) are under the first quartile of the sample or if its leverage is above the third quartile of the sample. Hence, starting from this definition, we apply our methodology to analyze the relationships between financial distress and firms’ characteristics and evaluate how it may vary when considering different (more extreme) quantiles of the distribution of leverage and EBITDA. In this way we are able to assess not only what are the main determinants for a firm’s risk of financial distress, but also how these factors matter as more serious levels of distress are considered.====The rest of the paper is organized as follows. In Section 2, we introduce the main notation and briefly revise the univariate quantile regression model. Section 3 introduces the joint quantile regression framework, while Section 4 proposes the EM-based maximum likelihood approach and the related penalized EM (PEM) algorithm to estimate model parameters. In Section 5 we provide simulation results, while the empirical application is presented in Section 6. Section 7 summarizes our conclusions.",Joint estimation of conditional quantiles in multivariate linear regression models with an application to financial distress,https://www.sciencedirect.com/science/article/pii/S0047259X18302574,20 February 2019,2019,Research Article,270.0
"Dey Rounak,Lee Seunggeun","Department of Biostatistics, University of Michigan School of Public Health, 1415 Washington Heights, Ann Arbor, MI 48109-2029, USA","Received 19 January 2018, Revised 14 February 2019, Accepted 14 February 2019, Available online 19 February 2019, Version of Record 8 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.007,Cited by (5),"With the development of high-throughput technologies, principal component analysis (PCA) in the high-dimensional regime is of great interest. Most of the existing theoretical and methodological results for high-dimensional PCA are based on the spiked population model in which all the population eigenvalues are equal except for a few large ones. Due to the presence of local correlation among features, however, this assumption may not be satisfied in many real-world datasets. To address this issue, we investigate the ==== of PCA under the generalized spiked population model. Based on our theoretical results, we propose a series of methods for the consistent estimation of population eigenvalues, angles between the sample and population ====, ==== between the sample and population principal component (PC) scores, and the shrinkage bias adjustment for the predicted PC scores. Using numerical experiments and real data examples from the genetics literature, we show that our methods can greatly reduce bias and improve prediction accuracy.","Principal component analysis (PCA) is a very popular tool for analyzing high-dimensional biomedical data, where the number ==== of features is often substantially larger than the number ==== of observations. PCA is widely used to adjust for population stratification in genome-wide association studies [21] and to identify overall expression patterns in transcriptome analysis [23]. However, the asymptotic properties of PCA in high-dimensional data are profoundly different from the properties in low-dimensional (==== finite, ====) settings. In high-dimensional settings, the sample eigenvalues and eigenvectors are not consistent estimators of the population eigenvalues and eigenvectors [12], [20], and the predicted principal component (PC) scores based on the sample eigenvectors can be systematically biased toward zero [14].====There has been extensive effort to investigate the asymptotic behavior of PCA in high-dimensional settings. To provide a statistical framework for PCA in these settings, Johnston introduced a spiked population model, which assumes that all the eigenvalues are equal except for finitely many large ones called the spikes. A spiked population covariance matrix is basically a finite-rank perturbation of a scalar multiple of the identity matrix. A typical example of a spiked population with two spikes is shown in Fig. 1a. This two-spike eigenvalue structure arises if the population consists of three sub-populations, and the features are largely independent with equal variances. Under this model, convergence of sample eigenvalues, eigenvectors and PC scores have been extensively studied [4], [11], [14], [20].====In many biomedical data, however, the assumption of the equality of non-spiked eigenvalues can be violated due to the presence of local correlation among features. In genome-wide association studies, for example, the genetic variants are locally correlated due to linkage disequilibrium. In gene-expression data, since genes in the same pathway are often expressed together, their expression measurements are often correlated. These local correlations can cause substantial differences in non-spiked eigenvalues. To illustrate this phenomenon, we obtained eigenvalues with an autoregressive within-group correlation structure rather than the independent structure of the previous example. Fig. 1b shows that the equality assumption is clearly violated. Thus, if methods developed under the equality assumption are applied to these types of data, we will obtain biased results.====The generalized spiked population model [3] has been proposed to address this problem. The condition that the non-spikes have to be equal is removed in this generalization. In this model, the set of population eigenvalues consists of finitely many large eigenvalues called the generalized spikes, which are well separated from infinitely many small eigenvalues. Although the generalized spiked population model has a great potential to provide more accurate inference in high-dimensional biomedical data, only limited literature is available on the asymptotic properties of PCA under this model and their application to real data. Bai and Yao [3] and Ding [8] provided results regarding convergence of eigenvalues and eigenvectors. However, their work remained largely theoretical. Moreover, to the best of our knowledge, no method has been developed for estimating the correlations between the sample and population PC scores, and adjusting biases in the predicted PC scores under the generalized spiked population model.====In this paper, we systematically investigate the asymptotic behavior of PCA under the generalized spiked population model, and develop methods to estimate the population eigenvalues and adjust for the bias in the predicted PC scores. We first propose two different approaches to consistently estimate the population eigenvalues, the angles between the sample and population eigenvectors, and the correlation coefficients between the sample and population PC scores. We compare these two methods and show the asymptotic equivalence of the estimators across them. Finally, we propose a method to reduce the bias in the predicted PC scores based on the estimated population eigenvalues.====The paper is organized as follows. We begin in Section 2 by providing the definition of the generalized spiked population model and present existing theoretical results. We develop our methods to consistently estimate the population spikes in Section 3. In Section 4, we construct consistent estimators of the angles between the sample and population eigenvectors, and the correlation coefficients between the sample and population PC scores. We also propose the bias-reduction technique for the predicted PC scores. Section 5 presents the algorithm [9] to estimate the population limiting spectral distribution and the non-spiked eigenvalues. In Section 6, we present results from simulation studies and an example from the Hapmap project to demonstrate the improved performance of our method over the existing one. Finally, we conclude the paper with a discussion.",Asymptotic properties of principal component analysis and shrinkage-bias adjustment under the generalized spiked population model,https://www.sciencedirect.com/science/article/pii/S0047259X18300393,19 February 2019,2019,Research Article,271.0
"Arteaga-Molina Luis A.,Rodríguez-Poo Juan M.","Departamento de Economía, Universidad de Cantabria, Spain","Received 28 May 2018, Revised 13 February 2019, Accepted 13 February 2019, Available online 18 February 2019, Version of Record 5 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.005,Cited by (1),"In this paper local empirical likelihood-based inference for nonparametric categorical varying coefficient panel data models with fixed effects under cross-sectional dependence is investigated. First, we show that the naive empirical likelihood ratio is asymptotically standard chi-squared using a nonparametric version of Wilks’ theorem. The ratio is self-scale invariant and the plug-in estimate of the limiting variance is not needed. As a by product, we propose also an empirical ","In recent years, there has been an increased interest in the study of panel data models combined with nonparametric techniques. The results have been promising, even though the inherent disadvantages of nonparametric techniques such as the curse of dimensionality [16] remain valid in this context. Varying-coefficient models appear as a reasonable avenue to overcome this drawback.====Varying-coefficient models encompass a great variety of simple models applied by econometricians, including partially linear models or fully nonparametric models. In applied microeconomic problems, however, it is often difficult to access all explanatory variables of interest. For this reason, many applied economists have turned their attention to panel data models. As it is well known, in a regression model, these techniques enable us to estimate the objects of interest consistently by allowing for individual heterogeneity of unknown form.====Nowadays, we have at our disposal a pleiad of varying-coefficient estimators that exhibit good asymptotic properties under rather different sets of assumptions such as random effects, fixed effects or cross-sectional dependence; see, e.g., [28], [33], [34] for comprehensive surveys of the literature. Among others, the problem of considering varying coefficients that depend on discrete data has attracted attention because discrete variables are common in economic analysis. A semiparametric varying coefficient model with purely categorical covariates is proposed in [18] and in [12], this setting is extended to include fixed effects and cross-sectional dependence.====Although extensive results are reported, e.g., in [12], [18] on the asymptotic behavior of estimator, inference is not always an easy task. Typically, asymptotic normal approximations are obtained. In the discrete covariate case, under fairly general conditions, if the bandwidth is selected using a cross-validation criterion, the asymptotic bias of the estimator is negligible and therefore inference based on the asymptotic distribution is more feasible than in the continuous covariate case where some undersmoothing is needed [20]. Unfortunately, the problem becomes much more complex if one also wishes to incorporate cross-sectional dependence. Besides, using confidence bands as a testing device is not straightforward as uniform confidence bands are necessary to do so; see [19].====As an alternative, Owen [24] introduced techniques based on the empirical likelihood. This approach, which combines the reliability of nonparametric methods with the effectiveness of the likelihood approach, has several advantages. For instance, no limiting variance estimation is necessary. For further discussion on the advantages of the empirical likelihood technique, see, e.g., [10], [14], [15], [17], [21], [24], [25], [26], [27], [30].====Owing to its good properties, the empirical likelihood approach has already been applied to longitudinal data varying-coefficient models with random effects; see, e.g., [37]. As for the fixed-effects case, see [3], [38]. However, we are not aware of any results for the panel data discrete/categorical varying coefficient setting. In [3], empirical likelihood confidence bands are obtained for the varying coefficients, ====, under rather strong assumptions such as the continuity of all the vector of covariates, ====, and the assumption of independent and identically distributed idiosyncratic error terms both across units and along time. Although the kernel weights considered in this paper are well suited for continuous data, they are inappropriate for discrete/categorical data. Furthermore, the authors derive the asymptotic theory for ==== fixed and ====.====In this paper, we develop empirical likelihood ratios and derive a nonparametric version of Wilks’ theorem for a fixed-effects varying-coefficient panel data model, where all covariates are assumed to be discrete/categorical. We further derive the maximum empirical likelihood estimator of the varying parameters and its asymptotic theory when cross-sectional dependence in the idiosyncratic error term is allowed. Based on these results, we can build up confidence regions for the parameter of interest through a standard chi-square approximation.====The rest of this paper is organized as follows. In Section 2 we propose to construct confidence bands for the unknown functions by using a naive empirical likelihood technique. In Section 3, as a by-product, we provide an alternative maximum empirical likelihood estimator of the fixed-effect categorical varying parameters. In Section 4, we illustrate the proposed technique in an application that reports estimates of strike activities from ==== OECD countries for the period 1951–85. Concluding remarks are in Section 5 and the proofs of the main results are in the Appendix.",Empirical likelihood based inference for a categorical varying-coefficient panel data model with fixed effects,https://www.sciencedirect.com/science/article/pii/S0047259X1830277X,18 February 2019,2019,Research Article,272.0
"Näf Jeffrey,Paolella Marc S.,Polak Paweł","Department of Banking and Finance, Universität Zürich, Switzerland,Department of Statistics, Columbia University, NY, USA,Seminar for Statistics, ETH Zürich, Switzerland,Swiss Finance Institute, Switzerland","Received 2 April 2018, Available online 16 February 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.02.004,Cited by (13)," iid case, the model corresponds to the standard CAPM model, but enriched with a filter for capturing the news impact associated with both the market and asset excess returns. An empirical application using a portfolio of highly tail-heterogeneous cryptocurrencies and realistic transaction costs shows superior out-of-sample portfolio performance compared to numerous competing models. A model extension to capture asset-specific asymmetry is also discussed.","The global financial crisis of 2008, along with the ensuing global recession and European sovereign debt crisis, was arguably the worst since the Great Depression of the 1930s, resulting in the highly politicized bailout of banks by national governments, prolonged unemployment, and substantial declines in consumer wealth. The aftermath still continues to unfold. In 2015, a federal US judge ruled that Nomura Holdings and Royal Bank of Scotland knowingly sold defective mortgage bonds containing misrepresentations and errors, which contributed to Fannie Mae and Freddie Mac being rescued – by taxpayers – in September 2008. Other banks avoided court and settled to the tune of billions of dollars.====With the now oft-used quote of Paul Romer in mind, “A crisis is a terrible thing to waste”, an arguably positive aspect of its aftermath is the recognition of requiring more realistic models for the risk associated with financial products and portfolios of assets. However, despite the large academic literature, very few models are suitable for genuine application in large-scale multivariate modeling. This is not for lack of importance: the successful modeling and prediction of co-movements of financial asset returns is crucial for successful risk management, and can play a key role in avoiding future financial crises.====Part of the reason for the dearth of successful models for large-scale problems stems from the massive proliferation of parameters as the dimension increases, and the ensuing estimation problem. Some multivariate models immediately disqualify themselves, as they can be estimated only for literally up to about five assets, this being useless in genuine settings for financial institutions that manage on the order of thousands of assets. Three strategies to deal with tenable models include: (i) the use of factor models (see, e.g., [3], [4], [65], [67]); (ii) the use of shrinkage estimation (see, e.g., [19], [32], [34], [36], [47] and the references therein); and (iii) (related to shrinkage), the use of prior information, in particular the popular and successful framework of Black and Litterman [10]. Most of these constructions are limited to Gaussianity, exceptions being the non-Gaussian factor model of Madan [39] and the non-Gaussian extensions of the Black–Litterman model considered in [25], [35], [42].====The imposition of Gaussianity at the daily or higher frequency level is a drawback more severe in terms of Sharpe ratio performance than omitting, for example, GARCH-type effects; see, e.g., Paolella and Polak [51]. This is the second problem. It is important for risk prediction and portfolio performance to devise a model that not only addresses the aforementioned estimation problem, but also adequately accounts for the features in the data. The major stylized facts of asset returns are well-established. For individual stock and other asset returns, these include substantial leptokurtosis even after application of a GARCH-type filter, asymmetry, and conditional heteroscedasticity. With respect to the joint set of assets, non-ellipticity is prominent (see, e.g., [14], [41] and the references therein for evidence), and heavy-tailed models that allow for some forms of non-ellipticity via asymmetries statistically outperform their elliptic counterparts with respect to risk assessment and asset allocation; see [51], [53].====A further non-elliptic feature of asset returns is the differing tail behaviors of the individual asset returns — some stocks are blatantly more risky than others, even after conditional heteroscedasticity is controlled for; see, e.g., [52], [54]. The so-called COMFORT model of [51], [53] is fast to estimate, even for hundreds of assets, and accommodates the aforementioned stylized facts of asset returns, including some forms of non-ellipticity, but cannot allow for genuinely different tail behaviors of the assets. Models exist that can account for this, with copula-based models being perhaps the best example; see the seminal Embrechts et al. [18], the survey of Genest and Nešlehová [23], as well as [22], [24], [30], [56]. However, it appears that the literature is moving towards more sophisticated and complex copula constructions that are more able to adequately capture the co-movements and dependency structures in asset returns, particularly for large dimensions; see, e.g., [1], [2], [5], [15], [20], [29], [56], [59] and the references therein. This, in turn, may not be necessary or, from a practical implementation point of view, desirable; see, e.g., [52], [54].====This paper develops a new construction that generalizes the multivariate generalized hyperbolic distribution, hereafter MGHyp, such that the tail behavior of each margin is not restricted to be the same. To this end, ==== independent latent variables are introduced into the probabilistic structure, allowing for much greater flexibility over the usual single latent variable continuous mean–variance mixture that gives rise to the MGHyp. A related generalized mixture distribution with ==== independent latent variables was introduced in Luciano and Semeraro [37]. Our paper has some similarities, but specifies a structure that lends itself to fast and efficient likelihood-based estimation, while additionally facilitating portfolio optimization. In contrast, Luciano and Semeraro [37] focus on deriving various theoretical properties of the generalized mixture and do not consider an application of their proposed distribution. Instead, they study a special case of the general mixture framework that is very different from our approach, and briefly discuss a possible two-step estimation procedure.====In our proposed construction, complex dependence is introduced by combining the latent variables via a lower triangular matrix, such that each component is a sum of independent generalized hyperbolic (GHyp) random variables. This is done via a Cholesky decomposition of the dispersion matrix, which depends on the latent random vector. The idea of using a Cholesky decomposition in the context of multivariate models for financial data is not new. In particular, Darolles et al. [16] propose a linear time-series model with time-varying coefficients such as in a CAPM framework. While this is reflected in our construction as well, we are instead interested in building a latent variable model with a rich dependency structure able to capture the stylized facts of the data. Furthermore, Darolles et al. [16] demonstrate their model on the estimation of time-varying betas in a Fama–French setting, whereas we concentrate on out-of-sample density prediction and on the, arguably most challenging, issue of portfolio optimization.====As in Darolles et al. [16], the scale terms of each univariate margin can be endowed with a GARCH-type law of motion. In this paper, we only consider the independent and identically distributed (iid) case, noting that, while the predictive distribution with time-varying volatilities is more accurate, the ensuing larger turnover in portfolio allocation exercises becomes detrimental to performance when realistic transaction costs are taken into account; see Paolella et al. [55].====The paper proceeds as follows. Section 2 introduces the model and illustrates its basic idea for the bivariate case. Section 3 presents a fast Expectation–Maximization Either (ECME) algorithm to conduct maximum likelihood estimation (MLE) of the model parameters. Section 4 augments this algorithm to allow for a form of shrinkage by putting priors on a subset of parameters. Section 5 develops the tools required for using the model for portfolio optimization, and Section 6 presents the results of an extensive backtesting exercise. Section 7 concludes, and discusses extensions of the model and further points to consider. The Appendix contains technical derivations and further model aspects. Additional details for Section 6, a discussion on backtest overfitting, as well as the proof of Lemma 1, Lemma 2, may be found in the Online Supplement [45].",Heterogeneous tail generalized COMFORT modeling via Cholesky decomposition,https://www.sciencedirect.com/science/article/pii/S0047259X18301799,16 February 2019,2019,Research Article,273.0
"Hamori Shigeyuki,Motegi Kaiji,Zhang Zheng","Graduate School of Economics, Kobe University. Kobe, Hyogo 657-8501, Japan,Institute of Statistics and Big Data, Renmin University of China. Haidian District, Beijing 100080, China","Received 28 March 2018, Revised 7 February 2019, Accepted 7 February 2019, Available online 15 February 2019, Version of Record 5 March 2019.",https://doi.org/10.1016/j.jmva.2019.02.003,Cited by (8),"This paper investigates the estimation of semiparametric ==== between observed and whole groups. Our proposed estimators do not require the estimation of the missing mechanism, and they enjoy stable performance even when the sample size is small. We prove that our estimators satisfy consistency and ","Copula models are a compelling tool for analyzing complex interdependence of multiple variables. A key characteristic of copula models is that, as Sklar [51] proved, any multivariate joint distribution can be recovered by inputting univariate marginal distributions to a correctly specified copula. The copula approach is capable of capturing a wide range of interdependence among variables with relatively small computational burden. There is a vast and growing literature applying copula models to economic and financial data, among others; see, e.g., [13], [43], for extensive surveys and [37], [39], [40], [41], [50] for more recent contributions.====Especially popular are semiparametric copula inference approaches, which involve nonparametric marginal distributions and parametric copulas. Genest et al. [14] proposed the widely used maximum pseudo-likelihood estimator for the copula parameter. Chen and Fan [5] proposed pseudo-likelihood ratio tests for model selection. Chen and Fan [6] studied the estimation of a class of copula-based semiparametric stationary Markov models.====Most papers in the copula literature, including Genest et al. [14], assume complete data. In practice, missing data frequently appear in a broad range of research. In survey analysis, for example, respondents may refuse to report their personal information such as age, education, gender, race, salary, and weight. A primitive way of handling missing data is list-wise deletion, which picks individuals with complete data and treats them all equally. The list-wise deletion delivers consistent inference if data are Missing Completely At Random (MCAR), where target variables ==== and their missing status ==== are independent of each other. Wang et al. [55] studied the estimation of Gaussian copulas under the MCAR condition. Hasler et al. [24] studied estimation in vine copula models under the MCAR assumption and monotone non-response. In practice, the MCAR condition is often violated, and in such a case list-wise deletion can deliver heavily biased estimators.====It is therefore desirable to work under a more general assumption called Missing At Random (MAR), originally explored by Rubin [49], where ==== and ==== are independent of each other given some observed covariates ====. Ding and Song [11] proposed an EM algorithm for estimating the Gaussian copula under the MAR condition. We are not aware of any systematic study on the estimation of general copula models with data MAR, and we fill that gap.====Missing data problems have a close connection with survey sampling, where the parameter of interest is the population total of a survey variable, while only a portion of response outcomes can be obtained. In survey sampling, population totals of certain auxiliary variables can be accurately ascertained from census data. Survey statisticians use auxiliary information in many ways to improve survey estimates, and calibration is one of the most popularly used techniques. Deville and Särndal [10] originally proposed a class of calibration estimators to improve the estimation of finite population totals by utilizing information from auxiliary data. A core insight of Deville and Särndal [10] is to make calibrated weights as close as possible to the original sampling design weights under a given distance measure, subject to a set of constraints. The calibration has been extensively studied in the survey sampling literature [3], [9], [29], [30], [31], [33], [34], [36].====Applying calibration to missing data problems has attracted considerable research interest recently, and has brought many interesting results [8], [18], [19], [20], [21], [22], [45], [46], [52]. Despite those close connections, there still exists a major difference between survey sampling and missing data analysis: design weights are known in the former but not in the latter. Therefore, existing work that applies calibration to missing data problems is forced to parameterize design weights (namely propensity score functions). If the propensity score models are misspecified, the resulting estimators can be substantially biased.====In causal inference with binary treatments, Chan et al. [2] recently proposed a novel estimation technique to estimate the average treatment effects. They constructed a class of nonparametric calibration weights by balancing the moments of covariates among treated, controlled, and combined groups. Their method bypasses an explicit specification of a propensity score function. Moreover, calibration weights satisfy certain moment constraints in both finite and large samples, so that extreme weights are unlikely to arise. As a result, the calibration estimation attains significantly better finite-sample performance than other nonparametric approximation methods.====As is well known, causal inference with binary treatments is a variant of missing data problems since we can observe one and only one of potential outcomes. Being motivated by such an intimate connection, we extend the maximum pseudo-likelihood approach of Genest et al. [14] by adapting the calibration procedure of Chan et al. [2] in order to perform semiparametric copula inference with data MAR. Our estimator satisfies consistency and asymptotic normality. We also present a consistent estimator for the asymptotic variance of our estimator.====We show via extensive Monte Carlo simulations that our proposed estimator dominates existing alternatives. First, the list-wise deletion leads to severe bias under the MAR condition. Second, the parametric approach based on a specific functional form of propensity score suffers from substantial bias when the propensity score model is misspecified. Third, nonparametric estimators of Hirano et al. [26] exhibit serious sensitivity to the dimension of the approximation sieve, ====. Our estimator achieves a remarkably sharp and stable performance compared with the other methods.====The remainder of this paper is organized as follows. In Section 2, we explain our notation and basic set-up. In Section 3, we propose our estimator and study its large-sample properties. In Section 4, we present a nonparametric consistent estimator for the asymptotic variance of our estimator. In Section 5, we propose a data-driven approach to determine the tuning parameter ====. In Section 6, we perform Monte Carlo simulations. In Section 7, we provide some concluding remarks. Proofs of selected theorems are presented in technical appendices. Omitted proofs and complete simulation results are collected in the Online Supplement [17].",Calibration estimation of semiparametric copula models with data missing at random,https://www.sciencedirect.com/science/article/pii/S0047259X18301714,15 February 2019,2019,Research Article,274.0
"Perrone Elisa,Solus Liam,Uhler Caroline","Laboratory for Information and Decision Systems, and Institute for Data, Systems and Society, Massachusetts Institute of Technology, Cambridge, MA, USA,KTH Royal Institute of Technology, Stockholm, Sweden","Received 28 March 2018, Available online 13 February 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.01.014,Cited by (7), and the discrete geometry literature.,"Copulas are widely used for modeling stochastic dependence among random variables [11], [34]. Key to the power of copulas is Sklar’s Theorem, which states that the joint distribution function ==== of any ====-dimensional random vector ==== in ==== with univariate margins ==== can be expressed as ====where the function ==== is a ====-dimensional copula [41]. As a result of Sklar’s Theorem, the dependence structure of any random phenomenon can be represented by the associated copula which can exhibit different stochastic dependence properties such as exchangeability, positive/negative concordance, or tail dependence. The relation in Eq. (1) uniquely identifies the copula ==== associated to ==== on the set ====. In the particular case of purely discrete random vectors, Sklar’s theorem identifies the so-called discrete copulas, i.e., restrictions of copulas on (square or non-square) uniform grid domains.====The subfamily of discrete copulas is particularly useful for constructing the empirical joint distribution of a given multivariate sample. Indeed, the class of discrete copulas includes the so-called empirical copulas, which are the foundation of rank-based (nonparametric) copula approaches to inference [13], [40]. Discrete copulas are known to admit a representation as a convex polytope [26], and this geometric property has been exploited in copula-based approaches in various applications in the environmental sciences [1], [39]. A (convex) polytope is a bounded convex body in ==== that consists of the points ==== satisfying finitely many affine inequalities ====where ====. A collection of such inequalities is called an ==== of the associated polytope. The unique irredundant ====-representation of a polytope ==== is called its minimal ====-representation. If inequality (2) is included in the minimal ====-representation of ====, then the collection of points in ==== on which inequality (2) achieves equality is the associated facet of ====. Thus, the size of the minimal ====-representation of ==== is the number of facets of ====. Polytopes are fundamental objects in the field of linear optimization, where a key goal is to decide if a polytope has a small minimal ====-representation so as to more efficiently solve the associated linear programming problem.====Having a polytopal representation has been particularly helpful for constructing copulas with maximum entropy, i.e., copulas that correspond to the least constrained distributions given the available data [5], [6], [29]. For instance, the geometric description of discrete copulas has been used to derive checkerboard copulas with maximum entropy that match a given Spearman’s rank correlation coefficient [36], [37]. In applications with limited data, such as hydrology and climatology, the checkerboard copula with maximum entropy is an important tool to generate synthetic data; see [1] for a comprehensive analysis of entropy-copula methods in hydrology and climatology, and [39] for a particular recent study on rainfall total.====In this paper, we provide polytopal representations of various subfamilies of discrete copulas with desirable stochastic properties. This allows the application of similar convex optimization techniques in the identification of copulas that combine maximum entropy and particular desirable stochastic properties for various applications. We focus on analyzing component-wise convex bivariate copulas, known as ultramodular bivariate copulas [23], [24]. As discussed in Chapter 5 of [34], the component-wise convexity has an important probabilistic interpretation as a form of negative dependence for bivariate random vectors known as stochastic decreasingness. This type of negative dependence appears in practice: for example, various parametric families of copulas popular in hydrological applications admit representatives in the class of ultramodular copulas [24]. Notable examples are the families of Farlie–Gumbel–Morgenstern, Ali–Mikhail–Haq, Clayton, and Frank copulas; see, e.g., [43] for an application to rainfall data.====We show here that bivariate discrete copulas with the property of ultramodularity admit polytopal representations. As a consequence, the selection of ultramodular copulas is amenable to techniques from convex geometry and linear optimization. We connect our work with existing entropy-copula methods in hydrology and discuss how to select copulas that have maximum entropy and are ultramodular, a property of interest for example for analyzing rainfall data. In addition, we study the convex space of the more general class of quasi-copulas, i.e., the lattice theoretic completion of the class of copulas [35]; we identify the minimal ====-representation for the family of discrete quasi-copulas on non-square grid domains and the subfamily of discrete quasi-copulas with convex sections residing within. Notably, by doing so, we generalize Theorem 3.3 of Striker [42] by identifying the minimal ====-representation of the well-known alternating transportation polytope [25], a result of independent interest in discrete geometry.====The remainder of this paper is organized as follows. In Section 2, we provide basic definitions and known results connecting copulas and discrete geometry. In Section 3, we present our first main result (Theorem 1), in which we show that the collection of ultramodular bivariate discrete copulas is representable as a polytope, and we identify its minimal ====-representation. We then discuss how to apply our findings to select ultramodular copulas with maximum entropy. In Section 4, we give our second main result (Theorem 3), in which we identify the minimal ====-representation of the polytope of discrete quasi-copulas, thereby generalizing a result in discrete geometry [42]. In addition, we identify the minimal ====-representation of a subpolytope corresponding to the discrete quasi-copulas with convex sections. In Section 5, we analyze alternative representations of the polytopes introduced here; namely, we study their sets of vertices. Finally, in Section 6, we show that bivariate discrete (quasi-)copulas defined on non-uniform grid domains admit a characterization in terms of the most extensive generalization of the Birkhoff polytope known in the discrete geometry literature, thereby completely unifying these two hierarchies.",Geometry of discrete copulas,https://www.sciencedirect.com/science/article/pii/S0047259X18301684,13 February 2019,2019,Research Article,275.0
Meilă Marina,"Department of Statistics, University of Washington, Box 345322, Seattle, WA 98195-4322, USA","Received 15 February 2018, Revised 23 December 2018, Accepted 23 December 2018, Available online 11 February 2019, Version of Record 15 February 2019.",https://doi.org/10.1016/j.jmva.2018.12.008,Cited by (7),"If we have found a “good” clustering ==== of a data set, can we prove that ==== is not far from the (unknown) best clustering ==== of these data? Perhaps surprisingly, the answer to this question is sometimes yes. This paper gives spectral bounds on the distance ==== for the case when “goodness” is measured by a quadratic cost, such as the squared distortion of K-means clustering or the Normalized Cut criterion of spectral clustering. The bounds exist only if the data admit a “good”, low-cost clustering. The results in this paper are non-asymptotic and model-free, in the sense that no assumptions are made on the data generating process. The bounds do not depend on undefined constants, and can be computed tractably from the data.",None,Good (K-means) clusterings are unique (up to small perturbations),https://www.sciencedirect.com/science/article/pii/S0047259X18300964,11 February 2019,2019,Research Article,276.0
"Fourdrinier Dominique,Marchand Éric","Université de Normandie, INSA Rouen, UNIROUEN, UNIHAVRE, LITIS, avenue de l’Université, BP 12, 76801 Saint-Étienne-du-Rouvray, France,Département de mathématiques, Université de Sherbrooke, Sherbrooke, Québec, Canada J1K2R1,Department of Statistics, Rutgers University, 501 Hill Center, Busch Campus, Piscataway, NJ 08855, USA","Received 11 September 2018, Revised 4 February 2019, Accepted 4 February 2019, Available online 11 February 2019, Version of Record 16 February 2019.",https://doi.org/10.1016/j.jmva.2019.02.002,Cited by (1),"Let ==== be independent distributed as ====, ====, and ====, or more generally spherically symmetric distributed with density ====, with unknown parameters ==== and ====, known density ====, and ====. Based on observing ====, we consider the problem of obtaining a predictive density ==== for ==== as measured by the expected Kullback–Leibler loss. A benchmark procedure is the minimum risk equivariant density ====, which is generalized Bayes with respect to the prior ====. In dimension ====, we obtain improvements on ====, and further show that the dominance holds simultaneously for all ==== subject to finite moment and finite risk conditions. We also obtain that the Bayes predictive density with respect to the harmonic prior ==== dominates ==== simultaneously for all scale mixture of normals ====. The results hinge on duality with a point prediction problem, as well as posterior representations for ====, which are very much of interest on their own. Namely, we obtain for ====, point predictors ==== of ==== that dominate the benchmark predictor ==== simultaneously for all ====, with ==== increasing and concave on ====, and including the squared error case ====.",None,On efficient prediction and predictive density estimation for normal and spherically symmetric models,https://www.sciencedirect.com/science/article/pii/S0047259X18304755,11 February 2019,2019,Research Article,277.0
"Ramsay Kelly,Durocher Stéphane,Leblanc Alexandre","Department of Statistics and Actuarial Science, University of Waterloo, Canada,Department of Computer Science, University of Manitoba, Canada,Department of Statistics, University of Manitoba, Canada","Received 3 August 2018, Revised 31 January 2019, Accepted 1 February 2019, Available online 10 February 2019, Version of Record 28 February 2019.",https://doi.org/10.1016/j.jmva.2019.02.001,Cited by (8), and dd-plots.,"Multivariate data are ubiquitous in modern statistics. To analyze and understand these datasets, it is essential to extend univariate tools to the multivariate setting. Among many other applications, data depth extends the concept of rank to the multivariate setting. Measures of data depth provide a center-outward ordering of points in any dimension. Many definitions of data depth have been proposed, based on a wide variety of concepts such as half-spaces [27], random simplices [12], zonoid regions [22], points represented as curves [20], and many more [1], [5], [24], [31]. Many of these depth measures have complex definitions that are not intuitive or can be difficult (or impossible) to compute precisely in high dimensions [24], often requiring optimization. In contrast, defining depth measures through integration makes them easily approximable in high dimensions, via Monte Carlo techniques.====In 2009, Cuevas and Fraiman [3] introduced an integrated depth measure called integrated dual depth, or ID depth for short. ID depth can be used as a measure of depth for random elements in any Banach space with a separable dual. In this paper, we focus on random elements from distributions over ====. We integrate with respect to the Haar measure, following traditional projection pursuit methods [3]. Lastly, we normalize the depth to the interval ====. This allows ID depth to be interpreted as a point’s average univariate simplicial depth over all directions.====Note that in the above definition, and for the remainder of the paper, we denote the standard Euclidean inner product for two vectors of the same dimension ==== and ==== by ====. Cuevas and Fraiman [3] suggest the possibility of generalizing (1) to the alternate integrated depth measure based on univariate Tukey depth, viz. ====We call this depth integrated-rank weighted depth, because the univariate depth ==== can be expressed as normalized center-outward ranks.====Both IRW and ID depths admit sample versions, where ==== is simply replaced with ====, the empirical distribution function of a sample. Cuevas and Fraiman [3] mention that when ==== is continuous, the contours of the two depths admit the same level sets, including the deepest point or region. They do not compare the two definitions beyond this statement. In this paper we investigate the properties of IRW depth. The aim is to identify similarities and differences between the two depth measures to ascertain whether using the integrated method with different univariate depth measures adds anything new. For example, multivariate Tukey depth is decreasing along rays, which is an important property for a depth measure [31]. However, multivariate simplicial depth does not have this property [31]. The motivation is that, by using univariate Tukey depth, IRW depth may also have this property. Furthermore, we aim to extend and further study the properties of these two depth measures.====We demonstrate that many of the properties of the ID depth measure are shared with IRW depth, including invariance under similarity transformations, maximality at center, vanishing at infinity, continuity under continuous distributions, consistency and asymptotic normality. It should be noted that many of these properties, such as asymptotic normality, cannot be shown using the proof techniques of Cuevas and Fraiman [3], making this non-trivial. We provide a result on the breakdown point of both the IRW and ID medians. We show that IRW and ID depth are decreasing along rays under half-space symmetric distributions. We also show that both depths are continuous under discrete and mixed distributions, except at atoms of the distribution. We further provide a weighted average representation of the two depth measures and consequentially, an algorithm for exact computation.====We conclude that the depths are similar in terms of properties and computation. Furthermore, we find that when studying further properties of the ID medians, it is useful to look at both the ID and IRW definitions. In other words, the use of ==== can allow for easier proofs of some properties. This may be useful for studying the influence function of these depth measures and their associated location estimators. In fact, it may be useful to provide some general properties that apply to all integrated versions of a class of univariate depth measures.====The paper proceeds as follows. In Section 2 we show that sample IRW and ID depths can be represented as a weighted average of a finite set of univariate ranks. This naturally leads to an algorithm for exact computation of IRW depth in any dimension, and consequentially, any function of univariate projected cumulative distribution functions, including ID depth. We discuss the approximation algorithm for high dimensions from [3]. Next, along with ID depth, IRW depth is also compared with other depth measures and can be described as a smoothed version of Tukey depth. We show that like ID depth, IRW depth satisfies most of the properties for depth measures described by Zuo and Serfling [31]. Again, like ID depth, the sample depth produced by this depth measure is shown to be universally strongly consistent and uniformly weakly consistent using the results of Nagy et al. [23] and, under mild conditions, asymptotically normal.====Section 3 discusses the deepest point, the maximizer of IRW depth, as a location estimator. Under continuous distributions, this maximizer is equivalent to the maximizer of ID depth [3]. We show that this estimator is strongly consistent and relate it to the Tukey median. Using this relation, we obtain a lower bound on the depth of the deepest point for both the ID and IRW medians. We also bound the finite-sample and asymptotic breakdown point of the IRW and ID medians below by the respective breakdown points of the Tukey median. We also mention a related in-sample location estimator that is easier to compute when ==== is very large.====In Section 4, we illustrate a potential use of IRW depth (and ID depth) in dd-plots, which are used to assess distribution differences between two samples in any dimension [11], [15]. We produce and analyze dd-plots for real and simulated data sets; the real data sets include the famous iris data [2], [8], as well as a very high-dimensional (====) data set obtained from male patients with and without prostate cancer [28].====Section 5 gives a brief conclusion and some ideas for future study.",Integrated rank-weighted depth,https://www.sciencedirect.com/science/article/pii/S0047259X18304068,10 February 2019,2019,Research Article,278.0
"Liang Liang,Ma Yanyuan,Carroll Raymond J.","Department of Biostatistics, Harvard School of Public Health, Boston, MA 02115, USA,Department of Statistics, Penn State University, University Park, PA 16802, USA,Department of Statistics, Texas A&M University, 3143 TAMU, College Station, TX 77843, USA,School of Mathematical and Physical Sciences, University of Technology Sydney, PO Box 123, Broadway NSW 2007, Australia","Received 20 January 2018, Revised 17 January 2019, Accepted 17 January 2019, Available online 8 February 2019, Version of Record 22 February 2019.",https://doi.org/10.1016/j.jmva.2019.01.006,Cited by (3),"Case-controls studies are popular epidemiological designs for detecting gene–environment interactions in the etiology of complex diseases, where the genetic susceptibility and environmental exposures may often be reasonably assumed independent in the source population. Various papers have presented analytical methods exploiting gene–environment independence to achieve better efficiency, all of which require either a rare disease assumption or a distributional assumption on the genetic variables. We relax both assumptions. We construct a semiparametric estimator in case-control studies exploiting gene–environment independence, while the distributions of genetic susceptibility and environmental exposures are both unspecified and the disease rate is assumed unknown and is not required to be close to zero. The resulting estimator is semiparametric efficient and its superiority over prospective ====, the usual analysis in case-control studies, is demonstrated in various numerical illustrations.","The etiology of most complex diseases, such as cancers and cardiovascular diseases, is the joint effect of genetic susceptibility and environmental or non-genetic exposures, as well as their interactions. Even subtle differences in genetic factors between people, when exposed to the same environmental factors, can lead to dramatically different responses. In other words, people with certain genes may have a low risk of developing a disease whereas others may be more vulnerable when exposed to an identical environmental agent. One common example is that sunlight exposure results in higher risk of developing skin cancer among fair-skinned individuals than people with dark skin [17], [22]. Studying gene–environment interactions is thus of great importance to understand disease mechanisms and develop new treatments and prevention strategies.====The case-control study design is commonly used to investigate the intricate interplay of genetic susceptibility and environment effects. It is cost-efficient and convenient to implement compared to a cohort study, especially when dealing with relatively rare diseases [6]. Instead of taking a random sample from the underlying source population, the case-control design randomly draws a fixed number of cases (diseased subjects) and a comparable number of controls (non-diseased subjects) from the respective case and control subpopulations. Genetic and environmental factors are measured and recorded for these sampled subjects. The standard approach for the analysis of such a case-control study is prospective logistic regression, which ignores the underlying retrospective nature of the case-control design. Cornfield [10] showed the equivalence of prospective and retrospective odds ratios, which validates the prospective approach. Prentice and Pyke [24] further showed that prospective logistic regression analysis gives an efficient estimator, in the sense that it yields the maximum likelihood estimates of the odds ratio parameters under a semiparametric model that allows an arbitrary covariate distribution.====Despite this, prospective logistic regression treatment in a case control study can still require a large sample size to obtain adequate statistical power for detecting gene–environment interactions or testing other hypotheses of interest. As a consequence, epidemiological researchers often exploit the potential efficiency gain from further assuming certain parametric or semiparametric structures for the covariate distribution. For example, in practice, a common assumption is that genetic susceptibility and environmental exposure are independent in the underlying source population [23], possibly given strata. Under such a model, prospective logistic regression analysis is still valid but may not be efficient because it ignores gene–environment independence.====A growing number of articles have been published in the last two decades, proposing analytical methods that exploit gene–environment independence assumption [5], [14], [15], [20], [21], [23]. Piegorsch et al. [23] showed that under gene–environment independence and a rare disease assumption, the multiplicative interaction odds-ratio parameter can be estimated by cases alone and the resulting estimator is more precise than the estimator from traditional prospective logistic regression analysis using both cases and controls. However, the misuse of a rare disease assumption in analyzing diseases with moderate prevalence or diseases with small marginal probability in the source population but high risk for certain combination of genetic and environmental exposures can lead to considerable bias in the estimation. Noting this fact, Chatterjee and Carroll [5] developed a semiparametric maximum likelihood estimator employing the gene–environment independence assumption but not requiring any rare-disease assumption. Their approach leaves the distribution of the environmental exposures totally unspecified but restricts genetic susceptibility to have a discrete distribution that takes values in a finite and fixed set. Ma [20] proposed a semiparametric efficient estimator in the same setting as Chatterjee and Carroll [5] except the distribution of genetic susceptibility is allowed to be either discrete or continuous with a finite-dimensional parameter. The key ingredient of this approach is to construct a hypothetical population with infinite population size and a disease to non-disease ratio of ====, where ==== and ==== are the numbers of cases and controls in the case-control sample. Section 2 of Ma [20] showed that the case-control sample can be viewed as a size ==== random sample of independent and identically distributed observations from this hypothetical population, and hence classical semiparametric analysis is applicable. The validity and usefulness of such a hypothetical population was established in Ma [20]. Instead of assuming independence of gene and environment, there is a literature based on parametric modeling of the relationship between them [8], [9], [18], [19]: we make no such parametric assumptions.====In this paper, we consider a more general setting which keeps the gene–environment independence assumption, while further allowing an unknown disease rate and completely nonparametric distributions for both the genetic susceptibility and the environmental exposure. Under such a model setting, we adopt the hypothetical population framework of Ma [20] and derive the semiparametric efficient estimator by employing a semiparametric approach, which links the efficient estimator with the efficient score function. Throughout our work, the underlying source population is referred to as the true population to emphasize the difference between the underlying source population and the hypothetical population. The inherent connection between the two populations allows us to transport parameter estimation and inference results derived in the hypothetical population directly to those in the true population, see Theorem 1. Although general semiparametric theory applies in the hypothetical population framework, computing the efficient estimator in this context is technically challenging because the efficient score does not have an explicit form and must be solved from an integral equation. We adopt a simple numerical approach to solve the integral equation by discretizing the distribution of the genetic susceptibility when it is continuous. The resulting estimator, when properly implemented, is asymptotically linear with optimal efficiency.====The rest of the paper is organized as follows. The specific model and the hypothetical population framework are presented in Section 2, with the corresponding identifiability conditions provided in Appendix A.1. In Section 3, we formulate the problem by using a conventional semiparametric approach. The analytic expression of our semiparametric efficient estimator as well as its detailed implementation are discussed in this section. Section 4 illustrates the asymptotic properties of the resulting estimator. Several simulation studies are conducted in Section 5 to demonstrate the numerical performance of our semiparametric efficient estimator compared with prospective logistic regression. A real data analysis is provided in Section 6, followed with a brief discussion in Section 7. Technical details and proofs are given in an Appendix and in the Online Supplement.",A semiparametric efficient estimator in case-control studies for gene–environment independent models,https://www.sciencedirect.com/science/article/pii/S0047259X18300290,8 February 2019,2019,Research Article,279.0
"Tu I-Ping,Huang Su-Yun,Hsieh Dai-Ni","Institute of Statistical Science, Academia Sinica, Taiwan","Received 17 August 2017, Revised 25 January 2019, Accepted 25 January 2019, Available online 4 February 2019, Version of Record 16 February 2019.",https://doi.org/10.1016/j.jmva.2019.01.010,Cited by (8)," of squares for model fitting and a penalty on the model complexity referred as the generalized degrees of freedom (GDF). We allocate each term in the GDF to either the number of parameters used in the model or the complexity in separating the signal from the noise. Compared with AIC and BIC and their modification methods, this criterion reaches higher accuracies in a thorough simulation study. Importantly, it has potential for more general application because it makes fewer model assumptions.","Dimension reduction plays a central role in multivariate data analysis. With the advent of massive data, often endowed with tensor structures, its importance is greater than ever. For instance, digital gray-scale images, color images, and color videos can be viewed as tensors of order 2, 3 and 4, respectively. Gene–gene-environment interactions are another example of order-3 tensor.====Although it is possible to rearrange tensor data into vectors, to which traditional dimension reduction methods can be applied, the tremendous size of dimensionality due to vectorization burdens the computation and requires an enormous sample size to estimate the target subspace in a reasonable way. To avoid these drawbacks of vectorization, various approaches have been proposed to process tensor data, including high-order singular value decomposition (HOSVD) [4], ====PCA [15], and multilinear principal component analysis (MPCA) [5], [7], [8].====For general tensor data ====, Hung et al. [7] developed a probabilistic model for MPCA, viz. ====where ==== is the signal component and ==== is the noise component. Let ====. In the signal component, ==== is the (non-random) grand mean tensor, ==== with ==== denotes an orthonormal basis for the leading ====-dimensional subspace in the ====-mode space, and ==== is a random coefficient tensor, called a core tensor. The ====-mode multiplication ==== of a tensor ==== by a matrix ==== is defined, for all ====, by ====so that ==== is a tensor of dimension ====. This simple model has shown its strength in applications of gait recognition [8], facial image reconstruction [7], and cryo-EM image clustering [2], wherein MPCA on tensor data gives much higher prediction accuracy than PCA on vectorized data.====Rank selection for MPCA is to choose a proper tensor size for the core ====. It is very natural to generalize the component selection methods developed for PCA, including subjective ones such as the percentage of total variation method, but also more objective techniques such as AIC and BIC [1], [6] and minimum risk criterion by Stein’s unbiased risk estimate (SURE) [12]. Hung et al. [7] proposed a hypothesis testing statistic for the percentage of total variation method for MPCA given a pre-specified percentage. Yet, to determine a specified percentage sometimes is not an easy task. An alternative is to employ naive AIC or BIC on MPCA by ignoring the data tensor structure and working on the vector form of the data as for PCA, except that the estimated covariance matrix is under the MPCA model constraint. However, the vectorization step will create a long vector whose length can easily exceed the sample size, so that the criterion becomes invalid. To avoid vectorization of the data, Tao et al. [11] proposed a mode-decoupled probabilistic model for tensor data by imposing a strong condition that the elements of ==== are independent Gaussian random variables. Their AIC and BIC were formulated in such a way that the information criteria were applied independently to each projected tensor mode.====In this article, we develop a Stein’s unbiased risk estimate for the MPCA model. Stein’s unbiased risk estimate has been applied on minimum risk criteria in many statistical models including the linear regression model [13], the factor model [3] and the PCA model [12]. Ye [13] proposed a method called generalized degrees of freedom (GDF) for the differential term in SURE, viz. ====, when for each ====, ====. GDF happens to be the number of parameters for the linear regression model and can be more generally interpreted as the model complexity to estimate ====. We present the GDF for MPCA in a compact expression consisting of terms that match the number of parameters for the grand mean and for ====, as well as terms that quantify the complexity for estimating the mode-wise eigenvectors.====Both Ulfarsson and Solo [12], Chen et al. [3] found that rank selection based on the SURE approach performs better than that the BIC approach when the sample size is moderate. In additional simulation studies reported herein, we observe that our SURE method has higher accuracy in selecting the rank even when the data are generated according to the condition required by AIC and BIC in [11]. In the special case where ==== with diagonal matrices ==== and ====, the mode-decoupled AIC and BIC can possibly perform better than the SURE method. However, this strongly constrained mode-decoupled model often seems unrealistic in practice.====We use order-2 tensors as a demonstration for the SURE derivation. The higher order tensor will be a straightforward extension. For order-2 tensors, the tensor model (1) becomes ====where ====, ====, ==== and the core matrix ==== is a random matrix with ==== and ====. Note that we only impose a parametric distribution assumption on the noise, not on the core ====.====The rest of this article is organized as follows. We give a brief introduction of MPCA in Section 2. In Section 3, we derive the generalized degrees of freedom of SURE for the MPCA model and provide a statistical interpretation thereof. In Section 4, we compare the rank selection performance of AIC, BIC and SURE. We end this article with a brief discussion in Section 5. The usual AIC and BIC and the mode-decoupled AIC and BIC are introduced in Appendix A. The proof of Theorem 1 is relegated to Appendix B.",The generalized degrees of freedom of multilinear principal component analysis,https://www.sciencedirect.com/science/article/pii/S0047259X1730492X,4 February 2019,2019,Research Article,280.0
"Castañer Anna,Claramunt M. Mercè,Lefèvre Claude,Loisel Stéphane","Departament de Matemàtica Econòmica, Financera i Actuarial, Universitat de Barcelona, 690 Avinguda Diagonal, E-08034 Barcelona, Spain,Département de Mathématique, Université Libre de Bruxelles, Campus de la Plaine C.P. 210, B-1050 Bruxelles, Belgium,Univ Lyon, Université Lyon 1, ISFA, LSAF EA2429, 50, avenue Tony-Garnier, F-69007 Lyon, France","Received 7 February 2018, Available online 28 January 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.01.007,Cited by (3),"In this paper, we introduce a new multivariate dependence model that generalizes the standard Schur-constant model. The difference is that the random vector considered is partially exchangeable, instead of exchangeable, whence the term partially Schur-constant. Its advantage is to allow some heterogeneity of marginal distributions and a more flexible ","Schur-constant models describe random lifetimes with a particular dependence. They are used in a variety of areas, including reliability, survival analysis, marketing, insurance and finance. Traditionally, the lifetimes are absolutely continuous random variables evaluated in ====. Properties of continuous Schur-constant models have been studied, e.g., in [3], [4], [7], [18], [20], [24].====Discrete versions of Schur-constant models have received little attention so far. However, they may be more appropriate in a variety of applications, particularly in reliability and lifetime studies; see, e.g., [19]. Recently, Castañer et al. [6] introduced Schur-constant models for vectors valued in ====, where ====. Lefèvre et al. [15] showed for discrete vectors with different ranges that the associated partial sum process is a non-homogeneous Markov chain. Castañer and Claramunt [5] discussed the link with equilibrium distributions. Jones and Marchand [13] developed a generalized model based on a sum and share decomposition.====In the following, we will focus on the discrete framework for brevity and because the continuous case study is roughly similar. We recall that for any integer ====, a discrete random vector ==== is Schur-constant on ==== if its joint survival function can be expressed as ====for all ====. Here, ==== is some admissible function ====, called generator of the model. From (1), such a random vector ==== is exchangeable [10]. Thus, the variables ==== have the same distribution and the covariances between any pair of variables ==== are equal. Clearly, this property is very restrictive, if not unrealistic, for a number of real-life situations. This is the case for example in the survival study of a given population.====To overcome this difficulty, we follow de Finetti’s idea and propose to construct a more general model incorporating a property of partial Schur constancy. Specifically, we partition the vector ==== into ==== groups, namely ==== of size ====, ====, ==== of size ====, with ====. The model (1) is then generalized as follows.====We note that the partially Schur-constant model (2) keeps the property of indifference relative to aging satisfied by the model (1). This means that any two vectors of residual lifetimes in the ==== groups, ==== and ====, have the same conditional distributions even if they have different ages. Indeed, we see from (2) that ====By definition (2), the vector ==== is a particular case of partially exchangeable vectors [11]. Diaconis [9] and Aldous [1] present a complete analysis of the notion of partial exchangeability. Obviously, the model (2) constructed for a single group ==== gives the exchangeable Schur-constant model (1). The advantage of (2) is that the previous symmetry in ==== is now partially broken because ==== is divided into ==== different symmetric groups. Thus, the marginal distributions in each group are equal but vary from one group to another. The covariances within each group are identical but the dependencies between groups are allowed to vary. As a result, the proposed model gains flexibility and greatly increases the scope of potential applications.====It should be mentioned that discrete Schur-constant models can be considered as analogs to the continuous models underlying Archimedean copulas [20]. In this context, the extension to partial Schur-constancy recalls, to some extent, Hierarchical Archimedean Copulas (HAC). This similarity is close but different, however, since the notion of hierarchy is not explicitly taken into account in modeling by (2). Besides, correlation inside groups is usually stronger than between members of different groups in HAC models, which is not always the case in partially Schur-constant models.====The paper is organized as follows. In Section 2, we show that the generator ==== is an ====-dimensional function with a monotonicity degree of order ====. In Section 3, we derive different joint distributions and provide two partial Schur-constancy characterizations: the first by conditioning with respect to the sums of variables in the ==== groups and the second by using an ====-dimensional doubly mixed multinomial distribution. In Section 4, we prove that if a model is partially Schur-constant for any order ====, all its submodels have an ====-dimensional mixed geometric distribution. In Section 5, we determine the correlation coefficients within and between the ==== groups and illustrate with numerical examples. In Section 6, we use the second characterization of the model to discuss an application in the risk management of insurance networks.",Partially Schur-constant models,https://www.sciencedirect.com/science/article/pii/S0047259X18300812,28 January 2019,2019,Research Article,281.0
"Engel Janina,Pagano Andrea","European Commission, Joint Research Centre (JRC), Directorate Growth and Innovation, Finance and Economy Unit, Via Enrico Fermi 2749, 21027 Ispra (VA), Italy,Chair of Mathematical Finance, Technical University of Munich, Parkring 11, 85748 Garching, Germany","Received 14 February 2018, Available online 28 January 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.01.008,Cited by (7),". The relevance of the presented results stems from the urgent need of having realistic instances of possible ==== as input in technical studies on the stability of inter-banking networks. Various such studies exist, however, most of them rely on toy models for the analyzed ====.",None,Reconstructing the topology of financial networks from degree distributions and reciprocity,https://www.sciencedirect.com/science/article/pii/S0047259X18300903,28 January 2019,2019,Research Article,282.0
"Gandy Axel,Veraart Luitgard Anna Maria","Imperial College London, Department of Mathematics, London, SW7 2AZ, UK,London School of Economics and Political Science, Department of Mathematics, Houghton Street, London WC2A 2AE, UK","Received 25 October 2017, Available online 28 August 2018, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2018.08.011,Cited by (24),This paper is concerned with reconstructing weighted directed networks from the total in- and out-weight of each node. This problem arises for example in the analysis of ==== performs best.,"Financial institutions are linked in various ways. It is natural to use network models to assess the stability of financial systems [[3], [26], [28]]. A key hindrance for using such models is that the networks are often not directly observable. As mentioned by Haldane in a 2015 speech (see p. 14 in [27]): “Even among the world’s largest banks, data on their bilateral exposures to one another remains partial and patchy, especially for off balance sheet positions and securities holdings. That means large parts of the core of the international banking map remain, essentially, uncharted territory”. Furthermore, as mentioned on p. 31 of Bank of England’s document [5], there are “big gaps in the data on interlinkages between different parts of the financial system and common exposures across the financial system. The lack of data makes it difficult to build up a point-in-time picture of the interlinkages between different parts of the financial system and calibrate quantitative models”. This is despite efforts by regulators to collect more data [23]. We develop an empirical Bayesian approach to obtain a distribution of networks from partial information.====A financial network can be characterized in terms of a matrix where each entry corresponds to a financial link, e.g., a loan from one market participant to another one in the system or an exposure due to the sale of products such as a Credit Default Swap (CDS). We consider the classical network reconstruction problem, in which the row and column sums of the network matrix are the only available information. This is, e.g., the case for interbank assets and liabilities, where the aggregate information is available from balance sheet information. Sometimes additional entries of the matrix might be known to the regulator, since they correspond to (large) exposures within the regulator’s jurisdiction [[4], [33]].====The classical network reconstruction problem in which approximately ==== unknowns (the entries of the matrix) are estimated from around ==== observations seems to be ill-posed. Indeed, usually a large variety of matrices is consistent with the marginals. This is somewhat comparable to problems in high-dimensional statistics in which a large number ==== of parameters is estimated from a small number ==== of observations with ====, where useful results can be obtained by assuming a structure of the solution, such as sparsity. Similarly, in the matrix reconstruction problem, such assumptions will have to be made.====Recently, the performance of six different approaches to network reconstruction has been compared on 23 different financial networks [4]. A slightly extended and updated comparison can be found in [2]. We briefly outline these and some other approaches in Section 2. The study finds that there is no single method that is best for all types of networks. For example, the degree of sparsity influences which method works best. This then leads to recommendations as to which methods work best for different types of networks depending on their density/structure.====The most important conclusion from this study, which was not actually made, is that it is crucial for a network reconstruction method to be adjustable to topological characteristics such as the density of the network, which we define as the proportion of links existing divided by the total number of possible links. It is in general not possible to infer the density of the network from the marginals alone [24]. Hence, one can construct matrices of (almost) any desired density that are consistent with the observed marginals. Any network reconstruction technique that does not allow for calibration to additional information or target characteristics cannot be successful over a wide range of different types of networks.====Recently, Gandy and Veraart [24] proposed a Bayesian approach to network reconstruction that is very flexible in terms of the underlying structure of the network. The Bayesian approach assumes a prior distribution for the financial network which is then conditioned on the available information. This posterior distribution is approximated using a Markov Chain Monte Carlo (MCMC) sampler. The sampled matrices, i.e., the financial networks, are consistent with the row and column sums (and potentially some additional observed entries). Several well-known modeling assumptions can be included in the prior model, such as, e.g., the Erdős–Rényi random graph model, core–periphery structures or power laws for both the degree distribution and the distribution of the weights. Gandy and Veraart [24] do not give a method for adjusting the method to desirable topological properties. They also do not test their approach on empirical data in which the full network is actually observable.====Our paper makes three main contributions. First we develop an empirical Bayesian approach to network reconstruction by developing a calibration methodology of the Bayesian approach proposed in [24]. In particular, we show how networks can be reconstructed that have a specified target density and are consistent with the observed marginals, and possibly additional entries; see Section 4.3.====Second we introduce in Section 4.4 an empirical fitness model within the Bayesian framework and show how this can be calibrated to topological information about the network density. This new model accounts for heterogeneity in the nodes of the network while still being parsimonious in the number of parameters and hence easy to calibrate.====Third we evaluate in Section 5 the performance of the new methods as well as of existing methods on real data. For this, we use 89 fully observed networks of Credit Default Swaps (CDS) exposures described in Section 3, assume that we do not observe the full networks and attempt a reconstruction. Based on a wide range of measures, the (empirical) Bayesian approach performs very well in absolute terms and also compared to alternative reconstruction techniques. When looking at criteria that measure systemic risk or check whether the probability distribution from which the reconstructed networks are sampled has the true observed weights of the links within their support or not, we find that the (empirical) Bayesian approach performs significantly better than all competitor models considered in this paper. This is particularly remarkable because we included a competitor in the comparison that was considered the “winner” among the probabilistic approaches to network reconstruction considered in the comparison study by Anand et al. [2], which did not consider the Bayesian approach by Gandy and Veraart [24] and our extension. Furthermore, the (empirical) Bayesian approach can be calibrated quite easily to match important quantities of interest. We provide specific results for one example network in Section 5.3 and results for all 89 networks in Section 5.4.====We compare theoretical properties of different network reconstruction methods in Section 6, and find that the (empirical) Bayesian approach has more desirable properties than all the alternatives considered. We provide an implementation of the methods introduced in this paper as part of the ====-package ====.",Adjustable network reconstruction with applications to CDS exposures,https://www.sciencedirect.com/science/article/pii/S0047259X17306279,28 August 2018,2018,Research Article,283.0
Genest Christian,"Department of Mathematics and Statistics McGill University 805, rue Sherbrooke ouest Montréal (Québec), Canada H3A 0B9","Available online 21 June 2019, Version of Record 6 August 2019.",https://doi.org/10.1016/j.jmva.2019.06.001,Cited by (0),None,None,Editor’s final report,https://www.sciencedirect.com/science/article/pii/S0047259X19303288,21 June 2019,2019,Research Article,286.0
von Rosen Dietrich,"Department of Energy and Technology, Swedish University of Agricultural Sciences, Uppsala, Sweden","Available online 14 June 2019, Version of Record 6 August 2019.",https://doi.org/10.1016/j.jmva.2019.06.002,Cited by (0),None,None,Editorial,https://www.sciencedirect.com/science/article/pii/S0047259X1930329X,14 June 2019,2019,Research Article,287.0
"Nešlehová Johanna G.,Fougères Anne-Laure,McNeil Alexander J.","Department of Mathematics and Statistics, McGill University, 805, rue Sherbrooke ouest, Montréal (Québec), Canada H3A 0B9,Université de Lyon, CNRS UMR 5208, Université Lyon 1, Institut Camille-Jordan, 43, boul. du 11 novembre 1918, F-69622 Villeurbanne Cedex, France,The York Management School, Freboys Lane, University of York, Heslington, York, YO10 5GD, United Kingdom,Lehrstuhl für Finanzmathematik, Technische Universität München, Parkring 11, 85748 Garching, Germany","Received 23 March 2019, Accepted 23 March 2019, Available online 29 March 2019, Version of Record 23 May 2019.",https://doi.org/10.1016/j.jmva.2019.03.009,Cited by (0),None,None,Editorial for the Special Issue on dependence models,https://www.sciencedirect.com/science/article/pii/S0047259X19301629,29 March 2019,2019,Research Article,289.0
