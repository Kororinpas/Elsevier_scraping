name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Lévy-Leduc Céline,Bondon Pascal,Reisen Valdério","Université Paris-Saclay, AgroParisTech, INRAE, UMR MIA-Paris, 75005, Paris, France,Université Paris-Saclay, CNRS, CentraleSupélec, Laboratoire des Signaux et Systèmes, 91190, Gif-sur-Yvette, France,NuMEs - DEST/PGGEA - Federal University of Espírito Santo, Brazil","Received 8 July 2021, Revised 12 May 2022, Accepted 20 May 2022, Available online 30 May 2022, Version of Record 3 June 2022.",https://doi.org/10.1016/j.jspi.2022.05.005,Cited by (0),This paper proposes two novel alternative estimators for the ,"The autocovariance function of a stationary process plays a central role in time series analysis. However, it is well known that the classical sample autocovariance function is very sensitive to the presence of additive outliers in the data. A small fraction of additive outliers can indeed alter the performance of the classical autocovariance estimator, see for instance Deutsch et al., 1990, Chan, 1992, Chan, 1995, Maronna et al., 2006 and the references therein. Since additive outliers are common in practical situations, having an estimator of the autocovariance function which is robust to atypical data may be very helpful. Ma and Genton (2000) introduced such a robust estimator and discussed its performance on simulated and real data, and Lévy-Leduc et al. (2011) established its asymptotic properties. However, it should be pointed out that, although this estimator of the autocovariance function has interesting theoretical properties and displays reasonable performance for estimating the autocovariance of time series with additive outliers, Ma and Genton (2000) observed that their robust autocovariance estimator does not share the non-negative definiteness property with the classical sample autocovariance estimator. Due to the lack of robustness of classical sampling functions in the time domain, estimators in the frequency domain have recently emerged in the literature as alternative approaches to estimate time series models with different correlation structures see, for example, Fajardo et al., 2018, Reisen et al., 2017 and references therein.====Combining tools in the time and frequency domains, this paper proposes two alternative estimators which have the non-negative definiteness property, one of them is also not sensitive to the presence of atypical observations in the data. The theoretical roots of the estimation method discussed here are based on the diagonalization of a real symmetric circulant matrix which is discussed, for example, in Brockwell and Davis (1991, Section 4.5). Among many other approaches applying the circulant matrix theory in different data structures, this algebraic tool has been widely used to obtain fast simulation methods, usually called embedding circulant methods, to generate stationary univariate and multivariate Gaussian time series. This method has emerged in a wide variety of domains see, for example, Percival, 1992, Helgason et al., 2011, Coeurjolly and Porcu, 2018, among others.====Now, let ====, ====, be a real valued stationary process with autocovariance function ====where ====. In the following, we assume that ==== is absolutely summable, ==== ====Then, the spectral density of ==== is defined by ====The standard estimator of ==== from the ==== consecutive observations ==== is the sample autocovariance ==== defined by ====where ====. Note that ==== has the desirable property of non-negative definiteness. Following Brockwell and Davis (1991, Proposition 4.5.2), let ==== for ==== where ==== is the integer part of ==== and ==== is the ====th Fourier frequency. With ====, we define ==== by ====when ==== is odd, and we replace ==== by ==== in the definition of ==== and drop the last row of ==== when ==== is even. Let ==== be the ==== matrix defined by ====The alternative to ==== that we propose in this paper is defined by ====where ====, the value 1 being at the ====th position, ====and ==== is obtained by replacing ==== by an estimator ==== in (5). Many estimators can be proposed. Here, we consider the truncated window periodogram and the truncated window M-periodogram that will be defined below. Note that (7) implies that ==== is non-negative definite when the elements of ==== are non-negative.====The value ==== of the periodogram at frequency ==== is defined by ====Although ==== is a natural estimator of ====, it is well-known that ==== is not a consistent estimator of ==== and that the periodogram needs to be locally averaged to be consistent, see e.g. Priestley (1981, Chapter 6) and Brockwell and Davis (1991, Chapter 10). The truncated window periodogram at frequency ==== is given by ====where ==== and ==== as ====. Plugging (9) into (7) gives the estimator ====. Its consistency is established in Theorem 1.====In the following, another strategy for estimating the spectral density is proposed. It is based on the M-periodogram ==== defined in (12) which should thus lead to a robust estimator of the autocovariance function. As it is well known, M-estimation is indeed an alternative robust procedure to least-square and this estimation method has recently been the subject of intensive research for modeling linear models with absolutely summable autocovariance processes, such as ARMA models, see for example Wu (2007), Li (2010) and Li (2008) among others.====Following these ideas, Reisen et al. (2017) among others, proposed an alternative way to derive the periodogram function ==== by observing that the periodogram can be defined through the least-square estimates of a two-dimensional vector ==== in the linear regression model ====where ==== denotes the deviation of ==== from ====, ==== and ====. More precisely, we have ====where ==== denotes the Euclidean norm and ==== is the least-squares estimator of ==== defined by ====where ====. An ====-norm periodogram was proposed by Li, 2008, Li, 2010 by replacing the ====-norm in (11) with an ====-norm for ====. Here, we replace the ====-norm with the Huber loss function. The so-called M-periodogram is defined by ====where ==== is given by ====with ====, ====. The derivative of ==== is ====Replacing ==== by ==== in (9), we obtained the truncated window M-periodogram ==== defined by ====where ==== tends to infinity and is such that ====, as ==== tends to infinity.====Similarly as in (6), the corresponding autocovariance estimator ==== is defined as follows: ====with ====where ==== is defined in (4) and ==== is obtained by replacing ==== in (5) by ==== defined in (15). The asymptotic properties of ==== defined by (16) are established in Theorem 2.====The paper is organized as follows. Theorem 1, Theorem 2 are presented in Section 2. In Section 3, the finite sample size properties of ====, ==== and ==== are investigated in ARMA models with and without atypical observations. Concluding remarks are given in Section 4.",A spectral approach to estimate the autocovariance function,https://www.sciencedirect.com/science/article/pii/S037837582200043X,30 May 2022,2022,Research Article,0.0
"Lando Tommaso,Arab Idir,Oliveira Paulo Eduardo","Department of Economics, University of Bergamo, Italy,Department of Finance, VŠB-TU Ostrava, Czech Republic,CMUC, Department of Mathematics, University of Coimbra, Portugal","Received 3 December 2020, Revised 15 May 2022, Accepted 16 May 2022, Available online 27 May 2022, Version of Record 6 June 2022.",https://doi.org/10.1016/j.jspi.2022.05.004,Cited by (2),"We study the family of distributions characterised by an increasing odds rate (IOR), showing that this provides an alternative interpretation of the “adverse ageing” notion, comparable to the well known increasing hazard rate (IHR) model. We prove some preservation properties of this class under several transformations that are often considered in reliability and life testing problems, including formation of order statistics. Moreover, the IOR assumption enables the derivation of survival bounds and tolerance limits, extending the scope of applicability of some known results, which are based, for instance, on the IHR assumption. Finally, we propose a test for the IOR null hypothesis, establishing its consistency and providing a table of simulated ====-values.","Important models in survival analysis and reliability theory conform to the idea of “adverse ageing”, meaning that residual life is negatively affected by age, in some stochastic sense. Thanks to the well known “lack of memory” property of the exponential distribution, usually interpreted as “no ageing”, a distribution is said to exhibit “adverse ageing” if it is dominated by the exponential, in terms of some suitable stochastic order. Following this approach, the well known ==== (CTO) of van Zwet (1964) gives rise to the most widely-used ageing class, namely the ==== (IHR) family, corresponding to the increasing behaviour of the hazard rate (HR). This class has been used to derive many useful characterisations, inequalities and properties (Shaked and Shanthikumar, 2007, Marshall and Olkin, 2007), or to include shape-constrained properties in nonparametric estimates (Barlow et al., 1971, Robertson et al., 1988). However, the IHR family excludes several important models, such as bathtub or heavy tailed distributions. Therefore, it seems natural to search for alternative models of broader applicability, encompassing violations of the IHR condition. The typical solution to this problem has been choosing a ==== stochastic order, compared to the CTO, giving rise to wider classes. For example, the ==== order yields the IHRA class, whereas the ==== order yields the DMRL class (Marshall and Olkin, 2007).====In this paper, we propose a different approach: instead of choosing a weaker order, a wider family may be constructed by replacing the exponential with an alternative benchmark distribution, which dominates the exponential w.r.t. the CTO. Mathematically, this means choosing a benchmark which has a decreasing HR (DHR). Now, the key point is choosing a reference distribution that might provide an adequate reinterpretation of the “no ageing” concept. We propose the following location-scale family of distributions as a benchmark (remember that the CTO and the ageing classes subsequently defined are location and scale independent): ====As ==== is the cumulative distribution function (CDF) of the log–logistic distribution, with shape parameter equal to 1, we will refer to this class as the ====. The LL1 distributions are well-known to be DHR, so they exhibit “beneficial ageing” (opposed to “adverse ageing”) w.r.t. the HR criterion. However, if a random variable (RV) ==== has CDF in the LL1 family with ====, it is easily seen that ====that is, the probability of surviving ====, given survival up to time ====, coincides with the survival probability of ====, which is known as the ==== property (Galambos and Kotz, 2006). Moreover, the LL1 is the only family of distributions such that the ==== by time ====, that is, the ratio between probability of failure within time ==== over survival at ====, (see (3) below for the formal definition), has a constant growth rate w.r.t. time. In other words, the LL1 model keeps ageing with constant ==== (OR). Additionally, note that a life distribution that does not age, literally, is expected to last forever, in some stochastic sense, suggesting a benchmark with infinite expectation, such as the LL1 family.====The class of distributions that are dominated by the LL1 family, w.r.t. the CTO, will be referred to as the ==== (IOR) family, denoted ====. This family is characterised by the acceleration of the odds of failure w.r.t. time, which conforms to the idea that ageing has a negative effect. The HR and the OR are special cases of the ==== family (Barlow et al., 1971 p.242). By transitivity of the CTO, the IOR class contains the IHR class. Therefore, it has a wider range of applicability, including, for example, some bathtub and heavy tailed models, without requiring the existence of moments. On the other hand, the OR criterion places the exponential distribution in an “adverse ageing” class, differently from the usual interpretation, according to which the exponential represents “no ageing”. Consequently, the OR is less sensitive to “beneficial ageing” behaviours, compared to the HR, in that IOR distributions may show a decrease of the HR, at least in some interval.====The rest of the paper is organised as follows. In Section 2, we present the IOR class. In Section 3, we present several preservation properties of the IOR family, under transformations that are often considered in reliability and life testing problems. In Section 4, we provide bounds for the survival function and tolerance limits for IOR distributions. In Section 5, we propose a test of the IOR null hypothesis. Finally, in Section 6, we introduce a new stochastic order, based on the OR, aimed at establishing whether one random variable “ages faster” than another. All the proofs are deferred to Appendix.====Note that ageing properties only depend on distributions, not on random variables, so we will use them interchangeably, as more convenient. Moreover, throughout this paper, “increasing” (“decreasing”) is taken as equivalent to “non-decreasing” (“non-increasing”).",Properties of increasing odds rate distributions with a statistical application,https://www.sciencedirect.com/science/article/pii/S0378375822000428,27 May 2022,2022,Research Article,1.0
"Lin Yu-Xuan,Tang Yi-Han,Zhang Jia-Hua,Fang Kai-Tai","Division of Science and Technology, BNU-HKBU United International College, Jintong Road 2000, Zhuhai, 519000, Guangdong, China,Department of Biostatistics, University of North Carolina at Chapel Hill, United States of America,Department of Biostatistics, Bioinformatics and Biomathematics, Georgetown University, United States of America,The Key Lab of Random Complex Structures and Data Analysis, The Chinese Academy of Sciences, China","Received 18 September 2020, Revised 15 May 2022, Accepted 16 May 2022, Available online 24 May 2022, Version of Record 6 June 2022.",https://doi.org/10.1016/j.jspi.2022.05.003,Cited by (0),. Hence we adopt the projection Average Squared Correlation distribution as an amendment in the detection algorithm to improve the effectiveness.,"Experimental designs have been widely applied in various fields. There are many design methods in the existing literature, among which the orthogonal design (OD) and uniform design (UD) have been widely used. Let ==== denote the set of the orthogonal arrays having strength 2, ====, where ==== represents the number of runs (rows), ==== as the number of factors (columns), and ==== as the number of levels of each factor. A design is called a U-type design, denoted as ==== if all levels of each factor appear equally often. The set of all ==== is denoted by ====. A design is called an orthogonal design ==== if it satisfies (1) the design is U-type, and (2) for any two different factors, the ==== level combinations appear equally often. As for the orthogonal array with strength ====, it requires that for any ====-factor, the ==== level combinations appear equally often. A U-type design ==== can be expressed as a matrix ====, where ==== is the number of runs, ==== as the number of factors and each element belongs to a set of ==== levels, for example, ==== or ==== in prevalent cases.====If the number of levels of each factor is the same, then the orthogonal design ==== is also called symmetric orthogonal array with strength ====. A design is called a uniform design, proposed by Fang (1980) and Wang and Fang (1981), if it evenly spreads out the experimental points over the design space, leading to a minimum discrepancy. The discrepancy, that is a uniformity measure, has various definitions including the centered ====-discrepancy (CD), the wrap-around ====-discrepancy (WD) proposed by Hickernell (1998), and the mixture ====-discrepancy (MD) proposed by Zhou et al. (2013).====Isomorphism plays a significant role for classification of orthogonal experimental designs. Two ==== designs are called combinatorially isomorphic if one can be obtained from the other by the joint operations of relabeling factors, reordering the runs or switching the levels of the factors. Otherwise, they are combinatorially non-isomorphic, referred to Cheng and Ye (2004). There are ==== combinatorially isomorphic designs for each ==== design. If two factorial designs are isomorphic, then their performances in ANOVA model are at the same level. Hence, for the experimenter, one only needs to compare all non-isomorphic designs and choose one.====There exist various criteria including minimum aberration and the uniformity measure that are used for detecting or comparing non-isomorphic designs. Box et al. (1978) proposed resolution, and Fries and Hunter (1980) proposed minimum aberration (word-length pattern) for regular fractional factorial designs (FFD). For extending the concept to non-regular FFD, Ma and Fang (2001) and Xu and Wu (2001) proposed generalized word-length pattern (GWLP). Moreover, the Hamming distance pattern (HDP) has been proposed by Ma et al. (2001) and by Clark and Dean (2001). From the overall mean model, various discrepancies as uniformity measure, are used to evaluate the geometrical structures of design points and can be used detecting non-isomorphic ODs. Ma et al. (2001) also proposed the distance enumerator (DE) to detect non-isomorphism and provided the relationship among DE, discrepancies, HDP and GWLP. The DE shows its advantages in non-isomorphism detection. All the above criteria have analytic expressions related to HDP when the number of levels is two or three, that are reviewed in Section 2.1. Booth and Cox (1962) proposed an efficient criterion for supersaturated 2-level designs, that is the variance of the sum of products by each pair of columns, denoted as ====. Evangelaras et al. (2005) extended the concept to three-level designs, that is the so-called Average Squared Correlation (ASC). A lower ASC value among main effects indicates less confounding in modeling.====When detecting non-isomorphism by minimum aberration criteria such as GWLP and HDP, there are some cases in which we cannot distinguish all classes. For instance, Lam and Tonchev (1996) has proved that there are 68 non-isomorphic classes of ====. However, these 68 classes of designs have the same GWLP and HDP values. Evangelaras et al. (2007) proved that there exist only three classes for ====, but these three classes have the same HDP and GWLP values as well. Hence, Elsawah et al. (2021) explored the distribution of projection GWLP (PGWLP) and projection HDP (PHDP) to detect non-isomorphism among the 68 classes of ====. It turns out that there are 67 out of 68 combinatorially non-isomorphic classes that can be distinguished by either PGWLP or PHDP. Through the Average Squared Correlation proposed by Evangelaras et al. (2005), 67 out of 68 classes for ==== can be detected while all the three classes for ==== can be detected. In this paper, we propose the ====, denoted by ==== and defined in Definition 4, to detect non-isomorphism for orthogonal designs. We explore a non-saturated three-level design, ====, a saturated three-level design, ====, and a non-saturated four-level design, ====. It turns out that the ==== is able to detect all three classes of ====, 67 out of 68 classes of ====, and 18 out of 20 classes of ==== with competitive time consumption. Moreover, when detecting non-isomorphism by ASC, we cannot detect two classes in ==== and five classes in ====. Hence, we consider the distribution of projection ASC distribution of ==== and ====. It turns out that all 68 classes of ==== and all 20 classes of ==== can be distinguished.====The concept of entropy in thermodynamics was first used for quantifying the amount of information by Shannon (2001) in 1948.====Considering the signal as a discrete random variable, Shannon used the information entropy to measure the uncertainty. It inspires us that for any given discrete random variable, we can construct its information entropy. For any saturated or unsaturated, regular or irregular factorial design, HDP is suitable for evaluating designs as the generalized minimum aberration criterion. In practical applications, the experimenter may focus on a certain number of factors less than the number of columns of a given design table. Hence, evaluating the projection property of factorial designs plays a significant role as well. Suppose a design ==== with a given dimension ====, there are ====
 ====-factor projection designs of ====. After calculating the HDP values of these projection designs, there are ==== types of PHDP values each having the corresponding frequency out of ====. Hence, we derive the entropy based on a discrete distribution of projection HDP through Definition 1 that will be introduced in Section 2.2.====In this paper, to justify the new criterion, we provide a review to the relationships between HDP and other criteria including GWLP, discrepancies and distance enumerator in Section 2.1. The detection results and performance of ==== are included in Section 2 as well. In Section 3, we apply the ASC criterion proposed by Evangelaras et al. (2005) on two kinds of three-level orthogonal designs and further develop the projection ASC distribution with higher detection effectiveness. The calculation and comparison of ASC and projection ASC distribution are time consuming. The entropy based on projection HDP provides a new perspective of evaluating experimental designs. Moreover, its detection process saves a great amount of time. However, in some cases, ==== may fail to distinguish non-isomorphism due to the limit of HDP on detection task. Thus, we provide a detection flowchart in concluding remarks in which we adopt both ==== and projection ASC distribution to improve detection efficiency and effectiveness.",Detecting non-isomorphic orthogonal design,https://www.sciencedirect.com/science/article/pii/S0378375822000416,24 May 2022,2022,Research Article,2.0
Tan Zhiqiang,"Department of Statistics, Rutgers University, 110 Frelinghuysen Road, Piscataway, NJ, 08854, United States of America","Received 15 March 2021, Revised 9 February 2022, Accepted 8 May 2022, Available online 21 May 2022, Version of Record 28 May 2022.",https://doi.org/10.1016/j.jspi.2022.05.002,Cited by (1),Analysis of 2 by 2 tables and two-sample survival data has been widely used. Exact calculation is computational intractable for ,"Analysis of 2 × 2 tables and two-sample survival data has been widely used. The subjects are covered in numerous articles and books (e.g., Andersen et al., 1993, Breslow and Day, 1980, Cox and Oaks, 1984, Kalbfleisch and Prentice, 1980, McCullagh and Nelder, 1989). The dominant approach is to use odds ratio models and conditional likelihood inference for handling 2 × 2 tables, and Cox’s (1972) proportional hazards models and partial likelihood inference for analyzing censored survival data. The two methods are closely related or, to some extent, equivalent to each other. In fact, conditional logistic regression can be implemented by calling a computer routine for fitting Cox’s regression model, as seen in the popular R package ====
 (Therneau, 2015).====There are, however, open problems in the existing theory and methods. For Cox’s proportional hazards model with survival data, the partial likelihood is powerful and analytically simple in the absence of tied event times. Large sample theory has been developed using counting processes, where the event time is commonly assumed to be absolutely continuous, thereby excluding the possibility of tied event times. There are two ways in which ties may arise in survival data. One way is that the underlying survival outcome is continuous, but the measurements are made in intervals which are not small enough. Second way is that the survival outcome of interest may be more meaningful if considered being discrete instead of continuous. An example in the latter case can be found in Allison (1982), where the survival outcome is time to a change of employers among a sample of biochemists from the first year as assistant professors.====A possible approach for handling tied data is to use Cox’s (1972) discrete-time version of proportional hazards models, which amounts to modeling odds ratios of hazard probabilities. The associated partial likelihood is conceptually straightforward, but exact calculation is numerically difficult with a moderate or large number of ties. Alternatively, various ad hoc approximations to the exact partial likelihood have been proposed (Breslow, 1974, Efron, 1977, Peto, 1972). It is often said that such approximations could yield satisfactory results with a small number of ties, but this reasoning defeats the very purpose of using approximations to deal with a relatively large number of ties. There seems to be no formal theory to justify these approximate methods or study their operating characteristics. In fact, if the event time is truly discrete, then the estimators of Breslow (1974) and Efron (1977) would in general be inconsistent under Cox’s discrete-time propositional hazards model. For discrete-time survival analysis, it has also been proposed to use unconditional maximum likelihood estimation, either with pooled logistic regression, corresponding to Cox’s discrete-time model, or complementary log–log regression induced by grouping observations under Cox’s continuous-time model (Prentice and Gloeckler, 1978). See Allison (1982) for a review. But such methods seem problematic in the presence of a large number of event times or intervals, which lead to the same number of nuisance parameters.====There are similar issues in the existing theory and methods for analyzing 2 × 2 tables under odds ratio models (Zelen, 1971, Breslow, 1976). Although various methods were proposed in the early literature including Mantel–Haenszel estimation (Cochran, 1954, Mantel and Haenszel, 1959), conditional maximum likelihood estimation (Breslow, 1981, McCullagh and Nelder, 1989) has been regarded as the “gold standard” for various reasons including optimal asymptotic properties (Lindsay, 1980) and superior empirical performance (Hauck, 1984), as remarked by Breslow and Cologne (1986). In particular, conditional likelihood inference is well-behaved with a fixed number of large tables or a large number of sparse tables. On the other hand, exact calculation for conditional likelihood estimation is numerically intractable for tables with large marginals, similarly to partial likelihood estimation with a large number of ties. Approximate methods have been proposed, with supportive numerical evidence (Breslow and Cologne, 1986, McCullagh and Nelder, 1989). But there seems to be no formal analysis of statistical properties of these methods, while taking into account the approximation involved.====To address the foregoing issues, we develop new methods and theory for analyzing 2 × 2 tables and two-sample survival data. See Tan (2020) for an extension to regression models for survival analysis. In contrast with previous methods, our general approach is to carefully construct estimating functions, while leveraging knowledge from conditional or partial likelihood inference in these problems.====In particular, for two-sample survival analysis, our work not only clarifies differences between probability and odds ratio models (although they both reduce to Cox’s hazard ratio model in the continuous-time limit), but also shows that the Breslow–Peto point estimator along with our new model-based and model-robust variance estimators are valid in the probability ratio model in both discrete and continuous time.====There are two distinctive features in our approach methodologically. First is use of estimating functions for point estimation while exploiting likelihood-based ideas, to achieve desirable numerical and statistical properties. Our point estimators are M-estimators based on concave objective functions and can be readily solved by standard optimizers. Second is incorporation of model-robust variance estimation, as an important complement to model-based inference in quantifying sampling variation with possible model misspecification. See Freedman (2006) and Buja et al. (2019) for related discussions on model-robust variance estimation. Both our model-based and model-robust variance estimators are of closed form given the point estimators.====In the following, we highlight main ingredients in the construction and properties of our point and variance estimators. First, in an odds ratio model for 2 × 2 tables, we derive simple estimating functions for the weighted Mantel–Haenszel estimator from two related angles. One is to achieve a close approximation to optimal estimating functions in minimizing asymptotic variances when positive responses are rare, which are often satisfied in applications. The other is to mimic conditional likelihood estimation in the extreme case where the total number of successes is 1 in each 2 × 2 table. The weighted Mantel–Haenszel estimator can be shown to be consistent and asymptotically normal in two asymptotic settings with large tables or many sparse tables, provided that the odds ratio model is valid. Moreover, to complement model-based inference, we derive a model-robust variance estimator, which are consistent in both asymptotic settings, while allowing for possible misspecification of the odds ratio model.====Second, in a probability ratio model for 2 × 2 tables, we construct simple estimating functions, which not only provides a reasonable approximation to optimal estimating functions in minimizing asymptotic variances, but also coincides with the Brelow–Peto approximation of the conditional likelihood in the odds ratio model. The resulting estimator, called the Breslow–Peto estimator, can be shown to be consistent and asymptotically normal in two asymptotic settings with large tables or many sparse tables, provided that the probability ratio model is valid. Moreover, we derive a model-robust variance estimator and a model-based variance estimator. The new model-based variance estimator is shown to be no greater than the commonly reported model-based variance estimator for the Breslow–Peto estimator, although the two variance estimators are identical in the special case of a total of one success in each table.====Finally, for two-sample survival analysis, we directly adopt the weighted Mantel–Haenszel estimator in an odds ratio model, or the Breslow–Peto estimator in a probability ratio model, with 2 × 2 tables constructed as usual from risk sets over time. The model-based variance estimators from 2 × 2 tables can be seen to remain valid due to a martingale argument. We then derive model-robust variance estimators for the weighted Mantel–Haenszel estimator and the Breslow–Peto estimator respectively. The latter variance estimator can be shown to coincide with an extension of Lin and Wei’s (1989) variance estimator, which is originally proposed and studied for the partial likelihood estimator in Cox’s proportional hazards model in continuous time. As a result, the Breslow–Peto estimator and our new model-based and model-robust variance estimators are valid in the probability ratio model in both continuous and discrete time.====For convenience, Table 1 lists the models and the point and variance estimators which are discussed in the remaining sections.","Analysis of odds, probability, and hazard ratios: From 2 by 2 tables to two-sample survival data",https://www.sciencedirect.com/science/article/pii/S0378375822000404,21 May 2022,2022,Research Article,3.0
"Tyssedal John,Hamre Yngvild Hole","Department of Mathematical Sciences, NTNU, Norway,DNB, Norway","Received 20 August 2020, Revised 4 May 2022, Accepted 8 May 2022, Available online 16 May 2022, Version of Record 2 June 2022.",https://doi.org/10.1016/j.jspi.2022.05.001,Cited by (0),"Regular two-level designs are useful and popular screening designs, but if they need to be run in blocks, their projection properties can dramatically deteriorate. Interactions may be fully confounded with block defining contrast(s), causing uncertainty in the identification of active factors. In this paper, we demonstrate alternative ways of blocking two-level regular designs such that their projective properties can be preserved or just weakly affected at the expense of just a small decrease in efficiency. Thereby, we can estimate effects we are normally interested in even if the design is blocked. Common regular two-level designs with 16, 32 and 64 runs are considered.","Regular fractional factorial two-level designs, usually denoted as ==== designs, are among the most popular experimental plans used for screening. They are characterized by the property that any two effects can be estimated independently of each other or are fully aliased which makes their analysis rather straight forward. Their properties and usefulness are nicely explained in several textbooks like Box et al. (2005); Wu and Hamada (2009) and Montgomery (2019).====Regular two-level designs can be constructed by allocating ==== factors to the principal factor columns in a full factorial with ==== runs and the remaining ==== factor(s) to some higher order interaction column(s). This causes the full aliasing of effects. Resolution is an important concept to describe the aliasing. ==== (Box and Hunter, 1961). The aliasing can be derived from the defining relation which is the set of all columns that are equal to the identity column. For instance, if factor ==== is assigned to a three-factor interaction column, say ====, it creates a word in the defining relation given by ====. Any effect associated with one, two or three of these four letters will then be aliased with the effect associated with the remaining one(s). In general, the defining relation has ==== words and to each design a wordlength pattern can be constructed of the form ====, where ==== is the number of words of length ==== It is assumed that no main effects are aliased with each other. The higher the resolution, the better, but designs with the same resolution can have unequal numbers of fully aliased effects. For given ==== and ====, let ==== and ==== be two designs. If ==== is the smallest integer for which ==== and ====, ==== is said to have less aberration than ====. If no design has less aberration than ====, it is said to have minimum aberration (Fries and Hunter, 1980). This is a useful way to rank regular two-level designs.====A much cited rule for experimental work is: “Block what you can and randomize what you can’t”. Blocking is an effective way to improve the efficiency of a design when not all the experimental runs can be performed under homogeneous conditions. For regular two-level designs the general rule of blocking is to assign one or a set of higher order interaction column(s), named block defining contrast(s) as block factor(s), and then associate the distinct level combinations in the column(s) with different blocks. A good blocking scheme should have as few as possible lower order interactions confounded with the block effects, and an additional wordlength pattern ==== can be constructed, where ==== is the number of ====th order interactions confounded with the block effect(s).====In order to rank blocked regular two-level designs, a combined wordlength pattern may be constructed. Several ways of doing this have been proposed and discussed in the literature (Sitter and M, 1997, Chen and Cheng, 1999, Zhang and Park, 2000, Mukerjee and Wu, 2006, Cheng and Tsai, 2009, Xu and Mee, 2010, Zhao et al., 2013). Cheng and Mukerjee (2001) proposed and studied a criterion for blocking based on the alias pattern of the interactions with the purposes of maximizing the number of two-factor interactions that are neither aliased with main effects nor confounded with blocks and at the same time distributing the interactions over the alias sets as uniformly as possible.====Sun et al. (1997) used the two wordlength patterns in addition to two other criteria, the number of clear main effects and the number of clear two-factor interactions, to come up with good blocking schemes. A main effect was called clear if it was not aliased with any two-factor interaction and any block effect. Similarly, a two-factor interaction was called clear if it was not aliased with any main effect, any other two-factor interaction and any block effect. Based on these four criteria, the concept of admissibility of blocking schemes was introduced, as a way to rule out bad designs. This criterion was further explored in Mukerjee and Wu (1999).====In this paper we will mainly be concerned with using blocked regular two-level designs for screening purposes. Screening is about separating out the normally few factors, from potentially many, that can explain most of the variation in the response. Projection properties concern how good a design is when restricted to a subset of factors. It is therefore important for a screening design to have good projection properties. Box and Tyssedal (1996) defined projectivity of two-level designs as follows: ==== ==== ==== ==== ==== ==== ==== ==== ==== ==== ==== ==== ==== ==== For regular two-level designs the projectivity is always given by ====. For more on projectivity, factor sparsity and screening, we refer to Box and Tyssedal (2001) and Tyssedal (2008).====However, recommended schemes for blocking two level regular designs may cause the projection properties of the blocked designs to deteriorate. For instance, the two-level resolution ==== (projectivity ====) design for five factors in 16 runs, denoted as the ==== design, becomes a projectivity ==== design when blocked in two blocks using the recommended scheme.====We will in this paper present alternative ways of blocking regular designs, such that projection properties will be preserved either fully or to some extent. Reflected in our focus on screening, all the designs chosen to be blocked have good projections properties. Only blocks having an equal number of runs in each block will be considered. The designs to be blocked will have from 16 to 64 runs and will, given ==== and ====, be minimum aberration designs taken from Wu and Hamada (2009), pages 253–257. Also, whenever we write ==== way of blocking, we will from now on refer to the way of blocking given there on pages 260–263. With reference to the tables presented in Sun et al. (1997), the blocked designs found in Wu and Hamada (2009) are the ones that rank best according to the number of clear main effects for 16 run designs. For designs with more than 16 runs, the number of clear two-factor interactions is used as ranking criterion.====This paper is organized as follows. In Section 2, we explain what is meant by projectivity of blocked regular two-level designs and introduce the criterion used to discriminate between different ways of blocking. A motivational example is given in Section 3. Section 4 is devoted to strategies for finding good blocking schemes, and in Section 5 we present specific ways of blocking the designs and the projectivity that is possible to obtain in each case. Several ways of blocking regular two-level designs are discussed and compared in Section 6. Concluding remarks are given in Section 7.",Preserving projection properties when regular two-level designs are blocked,https://www.sciencedirect.com/science/article/pii/S0378375822000398,16 May 2022,2022,Research Article,4.0
"Medina-Aguayo Felipe J.,Christen J. Andrés","Centro de Investigación en Matemáticas (CIMAT), A.C. Jalisco S/N, Col. Valenciana, CP: 36023, Guanajuato, Gto, Mexico","Received 3 December 2020, Revised 8 December 2021, Accepted 29 April 2022, Available online 12 May 2022, Version of Record 21 May 2022.",https://doi.org/10.1016/j.jspi.2022.04.008,Cited by (0),. We acknowledge that the effectiveness of the new method will be problem dependent and might struggle in complex scenarios; for such cases we propose a post-processing technique based on pseudo-marginal theory for combining isolated samples.,"Performing Bayesian analysis has become routine in many areas of application and research and, in turn, this is usually carried out using Monte Carlo methodology (see e.g. Robert and Casella, 2004). A popular and effective method, belonging to this class of methods, is Markov Chain Monte Carlo (MCMC), which is based on the construction of a Markov chain that converges to the desired target or posterior distribution (see Migon et al., 2015 for example).====However, naive implementations of MCMC may result in chains that require a prohibitively large number of iterations in order to be of any use. For example, implementing the celebrated Metropolis–Hastings (MH) algorithm (Metropolis et al., 1953, Hastings, 1970) using highly local proposals may result in chains that do not explore the target fully in a reasonable amount of time. Thus, constructing informative proposals is essential for obtaining useful samplers. This may be achieved by incorporating gradient information from the target, as done by Langevin and Hamiltonian algorithms (Roberts and Tweedie, 1996, Neal, 2011). Another possibility, that avoids computing derivatives, is adaptive MCMC (Haario et al., 2001, Roberts and Rosenthal, 2009) aiming at learning the shape of the target in an online manner and modifying the proposal accordingly. Other approaches rely solely on extending the state space, and consequently extending the target distribution, for constructing good proposals (Foreman-Mackey et al., 2013). Another example of the latter is the t-walk algorithm (Christen and Fox, 2010), which will be our subject of study. It is based on 4 different moves that are invariant under affine transformations (the “traverse”, the “walk”, the “hop” and the “blow” moves, see Christen and Fox, 2010 for details). The twalk is implemented in an official R package (Rtwalk), in Python, Matlab, C and C++, ====.====Nevertheless, and similarly to other MCMC methods, in the presence of multimodality the t-walk may become trapped in a subset of modes, especially when two or more modes are separated by large regions (valleys) of very small probability. The chain becomes trapped since the proposals are not bold enough for jumping into regions that are far relative to where the chain currently resides. In this respect, and within the Bayesian perspective, one may naively argue that multimodal posteriors are merely the result of a bad modelling process or due to a poor choice of priors. However, multimodality is not an artificial phenomenon resulting from poor decisions by the user since it commonly arises and is unavoidable in many well studied Bayesian problems. In particular, inference involving non-linear regressors, as in Bayesian inverse problems, may result in complex multimodal posteriors. Some examples include the FitzHugh–Nagumo model (FitzHugh, 1961, Nagumo et al., 1962), a soil organic matter decomposition model from Sierra et al. (2012), or the epidemiological SIR model describing a black plague outbreak in 1666 (Massad et al., 2004, Jonoska Stojkova and Campbell, 2019). There is indeed a need to develop better MCMC methods to address multimodality in posterior distributions arising from modern Bayesian inference.====Currently, the state-of-the-art method for dealing with multimodality is parallel tempering (PT) (see Swendsen and Wang, 1986, Geyer, 1991 for example). PT is based on several interacting chains that converge to a product of tempered distributions, one of which is the desired posterior. The chains interact via a series of swap moves that in principle help exploring multiple modes. The challenge, however, is the computational cost involved in the algorithm as one usually requires a large number of tempered distributions and iterations (Woodard et al., 2009) for the method to work correctly. Hence, this and similar approaches might not deal well with multimodality as the computational expense will render the approach unfeasible.====Despite that PT is possibly the method of choice for sampling from multimodal posteriors, this is a very active area of research with recent interest contributions. An example is the MultiNest sampler from Feroz et al. (2009) which produces weighted samples for approximately describing the posterior; MultiNest is motivated by nested sampling (Skilling, 2006) and is quite popular in cosmology applications. Another example is the equi-energy sampler from Kou et al. (2006), which is very similar to PT in the sense that multiple chains are run on an extended target of tempered but also truncated distributions. The sampler solves the local trapping problem by permitting moves between regions that might be distant from each other but that share similar levels of energy. Other recent contributions deserving mention are the works from Pompe et al. (2018) and Tawn et al. (2020). The former deals with an optimisation-based adaptive sampler that is able to learn optimal tuning parameters; whereas the latter studies a modification of the PT algorithm in order to preserve mode weights in the tempered targets.====The t-walk can sample from posteriors with several modes with different scales (see Christen and Fox, 2010 fig. 2 and 3) but, as with most other methods, once these modes are separated by areas of very low density, then it may get trapped in one particular region of the sample space. Various starting points should always be used in the practice of MCMC, increasing the chance of finding how and where our chain got trapped in different modes. How to join these samples from different regions into one that has the correct target is not obvious.====The t-walk has proved its value and has been used in several studies (Capistrán et al., 2016, Ward and Fox, 2012, Aquino-López et al., 2018, Capistrán et al., 2012, Villa and Rubio, 2018, Rubio and Genton, 2016, Rubio and Yu, 2017) and is currently used routinely by several users of data analysis software in paleoecology (Blaauw and Christen, 2011), and more recently in the COVID19 epidemic modelling (Capistran et al., 2020, Acuña-Zegarra et al., 2020). Certainly, it is not redundant to improve the performance of the t-walk in the presence of multimodal targets. Moreover, the ideas presented here could in principle be imported to improve the performance of other MCMC algorithms.",Penalised t-walk MCMC,https://www.sciencedirect.com/science/article/pii/S0378375822000386,12 May 2022,2022,Research Article,5.0
"Bagkavos Dimitrios,Ioannides Dimitrios","Department of Mathematics, University of Ioannina, 45110, Greece,Department of Economics, University of Macedonia, 54006, Greece","Received 22 May 2019, Revised 27 April 2022, Accepted 29 April 2022, Available online 10 May 2022, Version of Record 18 May 2022.",https://doi.org/10.1016/j.jspi.2022.04.006,Cited by (0),The local polynomial modeling of the Kaplan–Meier estimate for random designs under the right ==== setting is investigated in detail. Two classes of boundary aware estimates are developed: estimates of the distribution function and its derivatives of any arbitrary order and estimates of integrated distribution function derivative products. Their statistical properties are quantified analytically and their implementation is facilitated by the development of corresponding data driven plug-in bandwidth selectors. The asymptotic rate of convergence of the plug-in rule for the estimates of the distribution function and its derivatives is quantified analytically. Numerical evidence is also provided on its finite sample performance. A real life data analysis illustrates how the methodological advances proposed herein help to generate additional insights in comparison to existing methods.,"Let the random variable ==== denote a continuous positive duration with cumulative distribution function (c.d.f.) ====. In the right censored data setting the classical nonparametric estimator of ==== is the Kaplan–Meier, introduced in Kaplan and Meier (1958). However, its discontinuity limits its use when the analysis additionally requires estimation of the underlying probability density function (p.d.f.) and its derivatives. A popular solution is the use of convolution kernel smoothers (see Kulasekera et al. (2001) and the references therein) which provide continuous estimates with theoretically smaller Mean Square Error (MSE) compared to the Kaplan–Meier. Nonetheless their intrinsic excessive bias at the boundaries, Müller and Wang (1994), diminishes their global performance and invalidates their estimation at the endpoints. This inefficiency is further exacerbated when estimating p.d.f. derivatives. Boundary correction methods are available, see Cheng et al. (1997) for an account of important contributions. However they usually require complicated adjustments or come with undesirable side effects, e.g. they produce negative c.d.f. or p.d.f. estimates, which discourage their use in practice.====The purpose of the present research is to rectify the above mentioned drawbacks by developing kernel-based estimates of ==== and its derivatives of any arbitrary order that automatically adjust at the boundaries. The development is based on extension of the local polynomial modeling technique to the right censored data setting. Local polynomial smoothing was discussed in detail for complete data in Fan and Gijbels (1996) for estimation of the ====th derivative of the nonparametric regression function ==== viz. ====. There it was shown that the resulting local polynomial estimate of ==== automatically addresses the endpoint effects of conventional kernel smoothers. Thus, in parallel to Fan and Gijbels (1996), assuming the existence of ====, the local polynomial smoothing of the Kaplan–Meier estimate for right censored data is expected to yield self-adjusting at the boundaries kernel estimates of ====, for any arbitrary derivative order ====. Apart from the obvious importance of valid c.d.f. (====) and p.d.f. (====) estimation at the boundaries, accurate endpoint estimation for higher order p.d.f. derivatives is useful on its own right as these quantities arise in various expressions of optimal bandwidths, mode and inflection point estimates. The special cases of c.d.f. and p.d.f. estimation stemming from the approach pursued here can be viewed as extensions to the right censored data setting of the corresponding complete data estimators discussed in Cheng and Peng (2002), Zhang and Karunamuni (1998) and Cattaneo et al. (2020). However when it comes specifically to right censored data, to our knowledge, the only related research in the literature is the work of Peng and Sun (2007) which quantifies the asymptotic MSE of the local linear c.d.f. estimate and assesses its finite sample performance based on numerical simulations. Thus in a manner similar to Fan and Gijbels (1996), the present research fills the gap in the literature of simultaneous local polynomial estimation of the first ==== derivatives of ==== for right censored data.====Another equally important contribution is the development of an automatic plug-in bandwidth rule for the proposed local polynomial estimates of ====. The rule is based on minimization of the estimate’s Mean Integrated Square Error (MISE) and is effective in both the boundary and the interior of the region of estimation. Assuming estimation of ====, its asymptotic rate of convergence is quantified analytically as a function of ====. It should be noted here that the literature is particularly thin on plug-in bandwidth selectors for right censored data even for conventional kernel estimates of ====; a plug-in bandwidth rule for local linear c.d.f. estimation is briefly discussed in Peng and Sun (2007) while Sánchez-Sellero et al. (1999) develop a plug-in rule for density estimation. However as both bandwidth rules utilize conventional kernel smoothers, they are expected to be less efficient in the boundary than the rule proposed here, which additionally is applicable to estimation of any derivative order of ====. Thus, the present research also fills the gap of MISE optimal plug-in bandwidth selection for kernel estimation of ====, for all ====, in the right censored data setting.====Yet another important byproduct of the present research is the development of boundary aware estimates of integrated c.d.f. derivative products of any arbitrary order. These quantities are essential, apart from their use in plug-in bandwidth selectors, in estimation of curve roughness, rank based statistics e.t.c. Estimates of integrated squared density derivatives based on conventional kernel smoothers have been studied in the complete data setting in Hall and Marron (1987) and Bickel and Ritov (1988). See also Cheng (1997a) for a local polynomial smoothing approach in the complete data setting. Here there is another gap in the literature and is filled in the present research by utilizing the proposed local polynomial estimates of ==== in the development of efficient estimates of integrated c.d.f. derivative products of any arbitrary order. A direct plug-in MSE optimal bandwidth selector, also developed herein, facilitates their implementation in practice.====From a mathematical perspective the reasoning for investigating the local polynomial smoothing of the Kaplan–Meier estimate together with precise formulation is provided in Section 2.2. The asymptotic unbiasedness of the Kaplan–Meier and its almost sure convergence to the underlying c.d.f. over the whole real line (Chen and Lo, 1997), prompt its use as the response in a nonparametric regression model for ==== with the sample points used as the design. The regression function is modeled locally by polynomials with their coefficients estimated from the data by solving a kernel weighted least squares optimization problem. Then, the correspondence between the local polynomial coefficients and the derivatives in a Taylor expansion of the regression function at a nearby point, naturally yields the proposed smooth estimates of ====, for any ====. As shown in Section 2.3, the novelty of the approach lies on that the local polynomial smoothing of randomly right censored data automatically reinstates the kernel mass falling outside the region of estimation back in. As a consequence, the resulting estimates automatically adjust at the boundary without the negative side effects caused by manual methods such as the use of boundary kernels. It should also be noted that employing the Kaplan–Meier as the response in the aforementioned regression model does not require pre-binning of the data nor it is subject to any parameter selection. Thus, in comparison to e.g. the binned data (fixed design) counterpart of this work explored in Bagkavos and Ioannides (2021), or the double smoothing approach explored in Spierdijk (2008) and Kim et al. (2010) and which uses an extra parameter to smooth the empirical response estimate of the regression model, the approach suggested here greatly simplifies the practical implementation of the estimation procedure. The derived estimates of ==== are employed in Section 2.4 in the construction of improved efficiency estimates – in comparison to using conventional kernel smoothers – for integrated distribution function derivative products.====The asymptotic properties of both types of estimates are established analytically in Section 3. There it is shown that the proposed local polynomial estimates of ====, for all ==== retain the familiar from non boundary cases MSE rate of convergence throughout the region of estimation. Also, their utilization in the place of conventional kernel estimates in the estimation of integrated distribution function derivative products, remedies the inefficiencies caused due to edge effects in non smooth boundaries in the support of the curve being estimated, thus ensuring efficient estimation of the functionals.====Bandwidth selection for both types of estimates is discussed in detail in Section 4. There it is first exhibited that a plug-in bandwidth rule for estimation of ====, depends on estimation and implementation of integrated squared c.d.f. derivatives of order ====. For this reason a fully automatic MSE optimal bandwidth selector for the general case of estimation of integrated c.d.f. derivative products, of any arbitrary order, is discussed first in Section 4.1. The rule is utilized in Section 4.2 for the development of a data driven MISE optimal plug-in bandwidth rule for the local polynomial estimates of ====. As expected the rule is of order ==== where ==== is the sample size. This indicates an increasing bandwidth with ====, the order of the estimated derivative. The analytic quantification of the rule’s asymptotic rate of convergence suggests stable behavior throughout the support of the curve under estimation, thus overcoming the boundary problems of plug-in rules based on classical kernel smoothers.====The finite sample performance of the proposed bandwidth selector specifically for c.d.f. estimation is illustrated in Section 5 with distributional data for various sample sizes and amounts of censoring. The outcome suggests a robust behavior even if the parametric reference distribution is not close to the true density. Finally, a real data analysis, also in Section 5, exhibits how the developed methodological advances help in detecting data patterns which cannot be discovered by existing techniques. All proofs are deferred for Section 6.",Local polynomial smoothing based on the Kaplan–Meier estimate,https://www.sciencedirect.com/science/article/pii/S0378375822000362,10 May 2022,2022,Research Article,6.0
"Chen Guanzhou,Tang Boxin","Simon Fraser University, Canada","Received 4 August 2021, Revised 28 April 2022, Accepted 29 April 2022, Available online 8 May 2022, Version of Record 16 May 2022.",https://doi.org/10.1016/j.jspi.2022.04.007,Cited by (1),Strong ,"In designing computer experiments, it is desirable to have design points scattered in the design region in some uniform fashion. Such designs are broadly referred to as space-filling designs (Santner et al., 2018, Fang et al., 2006). Use of space-filling designs in computer experiments is intuitively appealing as one would like to have every portion of a design region represented, and can also be theoretically justified in terms of their performances in the mean squared prediction error (Vazquez and Bect, 2011). Space-filling designs can be constructed by optimizing a uniformity criterion such as that of distance or discrepancy (Johnson et al., 1990, Li et al., 2021a, Fang et al., 2000). Orthogonality also plays a role in constructing space-filling designs (Ye, 1998, Bingham et al., 2009, Georgiou, 2011, Georgiou and Efthimiou, 2014, Georgiou et al., 2014).====We consider space-filling designs based on orthogonal arrays. Designs of this type are attractive because they enjoy some guaranteed low-dimensional projection properties. This line of research started with the introduction of Latin hypercubes by McKay et al. (1979), and went further with OA-based designs (Owen, 1992, Tang, 1993). Recently, He and Tang (2013) introduced a class of new designs, namely strong orthogonal arrays. These arrays, being more space-filling than comparable orthogonal arrays, have found applications in optimizing the braking performances for freight trains (Nikiforova et al., 2021).====Most economical strong orthogonal arrays (SOAs) are those of strength ==== that focus on two-dimensional projection properties. Construction of SOAs of strength ==== has been largely based on regular designs (He et al., 2018, Shi and Tang, 2019). This method puts a severe restriction on the run sizes of the resulting designs as they must be prime powers. Cheng et al. (2021) considered the use of two-level nonregular designs but their results are limited to designs of run sizes that are multiples of 16.====In this paper, we develop a general method of constructing space-filling designs using nonregular designs. Designs so constructed have very flexible run sizes compared to those constructed from regular designs. One challenging complication with using nonregular designs is that it is often impossible to obtain SOAs of strength ====. We meet this challenge by proposing two criteria for design evaluation under the new situation. Apart from some theoretical results, computer searches are conducted to find space-filling designs using two-level nonregular designs of up to 40 runs and three-level nonregular designs of 27 and 54 runs. One of the interesting findings is a strength ==== SOA of 54 runs for 12 factors.====Section 2 of the paper introduces notation and necessary background. Section 3 develops our method of constructing space-filling designs using two-level nonregular designs and presents corresponding theoretical and computational results. In Section 4, we show how the ideas of Section 3 can be generalized and used to construct some space-filling designs from three-level nonregular designs. The paper is then concluded by a discussion in Section 5.",Using nonregular designs to generate space-filling designs,https://www.sciencedirect.com/science/article/pii/S0378375822000374,8 May 2022,2022,Research Article,7.0
"Hu Jie,Liang Wei,Dai Hongsheng,Bao Yanchun","School of Mathematical Science, Xiamen University, China,Department of Mathematical Sciences, University of Essex, UK","Received 3 June 2020, Revised 5 April 2021, Accepted 26 April 2022, Available online 7 May 2022, Version of Record 12 May 2022.",https://doi.org/10.1016/j.jspi.2022.04.005,Cited by (1),"(EL-ratio) asymptotically follows a standard ==== distribution. This new method does not require any scale parameter adjustment for the log-likelihood ratio and thus does not suffer from the convergence problems involved in traditional EM-type algorithms. Finite sample simulation results show that this method provides much less biased estimate than existing methods, when censoring percentage is large. The application to COVID19 data will help researchers in other field to achieve better estimates and forecasting results.",". Another example is time from symptom onset to recovery for people who get COVID19. For COVID19 studies ====, the incubation rate and recovery rate are the key factors for us to understand the epidemiology. In particular, in the current COVID19 outbreak, better understanding of the recovery rate will help governments to take the right intervention strategy at the right time. However, many existing research for COVID19 are based on published information from government or ministry of health websites and media reports ==== that different model parameters will give very different forecasting results.====The dataset used in ==== is from====, ==== to study the recovery time, e.g. the time from symptom onset to recovery ====, and to study the sensitivity of recovery rate on the epidemiology forecasting. The recovery times are clearly observed under right censoring because when the data were reported, recovery may not have happened to many patients. Therefore the right censoring time ==== is the time from the symptom onset date to the reporting date. See ==== for scenarios when right censoring happens. Under right censoring, we will have no information about the left-censoring time ====, the recorded exposure ending time to recovery. On the other hand, as we know that the symptom usually occur after exposure to the virus, when symptom onset date is missing and also reporting date is missing but the date of exposure to virus is available, we can impose the reasonable condition of ==== on ====, which gives the left censoring time ====. So when left-censoring occurs the time ==== is from the date of exposure to the date of recovery. See ==== for details of left-censoring. Under left censoring, we will have no information about ====. When ====, we will observe ==== but cannot observe ==== and ====. This is shown in ====. In such cases we usually have that, ==== (symptoms immediately occur after exposure; events recorded on the same day) or ==== (the recorded exposure date means the ending time of an exposure period). We also have ==== which means that recovery occurs before reporting.====Denote ==== as the cumulative distribution function of ====. Suppose that we are interested in a parameter ====, defined by a functional ====. Many important parameters can be represented as this form, or sometimes we obtain ==== via the corresponding estimating equation ====. For example, if we are interested in the expectation of a known function ====, then ====, and the corresponding estimating equation is ====. Other examples include:====[1.] ==== is the cumulative hazard function at given time ====, i.e. ====, then the estimating equation is ====;====[2.] ====, i.e. ====where ====, then the estimating equation is ====.====To draw inference on the unknown parameter ====, a straightforward approach is to implement a distribution function estimation for ==== ====, ====, ====, ====. Using the distribution function estimation, the asymptotic-normality based confidence interval for the parameter of interest ====, ====, ==== and two-sample tests ====, which is a very useful tool for constructing confidence interval for ==== in nonparametric settings. Based on estimating equation ====, the original Empirical Likelihood (OEL) in ==== is defined as ====It can be proved that ====A very important work by ==== distribution, therefore the confidence interval for ==== can be constructed without estimating asymptotic variance.====However, applying OEL methods to incomplete data will lead to a scaled ==== result. When the data is right censored, ==== distribution. This limiting distribution can be used to construct the confidence interval for ====, if the scaled parameter is estimated. To avoid estimating the scaled parameter, ==== used the efficient influence function of the parameter under right censorship to define the log-likelihood ratio statistics and proved its asymptotic distribution is a ==== distribution. The confidence interval for ==== based on this method is much more accurate. Under doubly censoring, ==== distribution, the scaled parameter as an adjustment coefficient needs to be estimated in practice. Besides, the LBEL method demands that the parameter of interest should be the linear functional of ====.====Notice that the EL likelihood function ==== is not the real likelihood function for doubly censored data, ==== defined the likelihood function based on observations ====
 ====where DC is the abbreviation for Double Censoring, ==== and ====, ==== showed that this log-likelihood ratio subject to nonparametric moment constraints obeys the Wilks’ phenomenon under some assumptions. This method avoids the scaled parameter, but is computationally difficult to find the nonparametric maximum likelihood. To solve this problem, ==== proposed an EM algorithm to calculate this log-likelihood ratio statistics. However, EM algorithm may suffer from the problem of convergence to a local maximum point. Different from ====, we investigate another approach in this paper. Inspired by ====. This method is called Efficient-EL method in our paper. Under this new approach, we demonstrate that the log empirical likelihood ratio converges to the standard ==== distribution without using any scale parameter adjustment, which means the confidence interval for different kinds of parameters ==== can be obtained by a unified algorithm. In the mean time, it is computationally much more efficient than existing EL methods under doubly censoring.====The rest of the paper is organized as follows. The Efficient-EL inference for the differential functional parameter ==== under doubly-censored data is given in Section ====, including the large sample properties and the computing algorithm. Simulation studies of the Efficient-EL and the EM-EL method proposed by ==== are provided in Section ====. We find that our approach performs much better for longer tail distributions, which usually lead to higher censoring proportions. In the mean time, the new method still performs as good as existing methods for lighter tail distributions which lead to lower censoring proportions. An application on COVID-19 study based on our proposed methodology is presented in Section ====. The paper concludes with a discussion in Section ====.",Efficient empirical likelihood inference for recovery rate of COVID19 under double-censoring,https://www.sciencedirect.com/science/article/pii/S0378375822000350,7 May 2022,2022,Research Article,8.0
"Zhao Honghe,Yang Shu","Department of Statistics, North Carolina State University, 2311 Stinson Dr., Raleigh, NC 27695-8203, USA","Received 17 December 2021, Revised 6 March 2022, Accepted 21 April 2022, Available online 29 April 2022, Version of Record 13 May 2022.",https://doi.org/10.1016/j.jspi.2022.04.004,Cited by (2),"In this article, we propose the outcome-adjusted balance measure to perform model selection for the generalized propensity score (GPS), which serves as an essential component in estimation of the pairwise average treatment effects (ATEs) in observational studies with more than two treatment levels. The primary goal of the balance measure is to identify the GPS model specification such that the resulting ATE estimator is consistent and efficient. Following recent empirical and theoretical evidence, we establish that the optimal GPS model should only include ","Estimating the causal effects from observational data with more than two treatment levels has become an increasingly important goal in socioeconomic and biomedical research. Examples of observational studies with multiple treatment levels are ubiquitous. In public policy, the National Antidrug Media Campaign was implemented nationwide in the United States, with teens receiving varying degrees of exposure to the media campaign (Zanutto, Lu, and Hornik, 2005). In medicine, the REFLECTIONS was a 12-month prospective study that involved three fibromyalgia medication cohorts from 58 outpatient sites in the United States and Porto Rico (Yang et al., 2016). In health care, the Minimum Data Set is part of a federally mandated process for clinical assessment of multiple Medicare or Medicaid certified nursing homes (Scotina and Gutman, 2019). To deal with confounders, methods that weight (McCaffrey et al., 2013, Li and Li, 2019), stratify (Zanutto, Lu, and Hornik 2005; Yang et al. 2016), and match (Yang et al., 2016, Scotina and Gutman, 2019) the sample based on the generalized propensity score (GPS) (Imbens, 2000) have been proposed. While the popular AIPW estimator enjoys nice properties including semiparametric efficiency and double robustness, inversely weighting by extreme values of the GPS can still jeopardize its performance. In practice, it is common for the GPS to take on extreme values, and the instability issue with weighting methods can become more pronounced as the number of treatments increases. The matching estimator is an attractive alternative due to its resistance to those extreme probabilities (Frölich, 2004). The idea of matching is also intuitively appealing as it seeks to replicate the ideal randomized experiment (Rubin, 2006, Stuart, 2010). GPS matching has been widely implemented in practice (Bennett et al. 2020; Scotina, Beaudoin, and Gutman 2020; Brown et al. 2020). However, as with all GPS based methods, GPS matching requires carefully modeling the generalized propensity score, which is typically unknown in practice. Given multiple postulated models, little work has been done to develop GPS model selection strategies, especially for improving the efficiency of the matching estimator.====When treatment is binary, considerable progress has been made towards developing modeling strategies for the propensity score (PS) (Rosenbaum and Rubin, 1983). The essential purpose a PS model specification serves is in assisting the estimation and inference of the ATE(s) rather than in explaining or predicting treatment assignment. Standard diagnostics for model prediction performance should be avoided as they often fail to suggest PS models that provide unbiased and efficient estimation of the ATE (Westreich et al., 2011). This is supported by a large thread of empirical evidence, which surprisingly suggests that including instrumental variables (covariates that are part of the true PS model) in the PS model specification inflates variance of the resulting ATE estimators (Brookhart et al. 2006; Austin, Grootendorst, and Anderson 2007; Myers et al., 2011, Pearl, 2011; Yang, Kim, and Song 2020). Additionally, including precision variables (covariates only related to outcomes but not treatment) in the PS leads to improved efficiency for ATE estimation (Brookhart et al., 2006, Patrick et al., 2011). Building upon the work of Lunceford and Davidian (2004) and Hahn (2004), Tang et al. (2020a) theoretically justify that both excluding instrumental variables and including precision variables in the PS will help improve asymptotic efficiency in the Horvitz–Thompson estimator, the ratio estimator, and the doubly robust estimator. Rotnitzky and Smucler (2020) provide a graphical criterion for identifying the optimal covariate adjustment set for non-parametric efficient estimation of the ATE.====In the binary treatment case, when baseline covariates are high-dimensional, many regularized regression methods have been developed to perform variable selection for the PS model. Shortreed and Ertefaie (2017) propose the outcome-adaptive LASSO with a penalty function that takes into account association between covariates and outcome, and association between covariates and treatment. Ju et al. (2019) propose a collaborative-controlled LASSO that uses the C-TMLE algorithm based on LASSO to minimize a bias–variance tradeoff in the estimated treatment effect. Tang et al. (2020a) improve upon the outcome-adaptive LASSO by incorporating the ball covariance (Pan et al., 2020), which makes the method free of dependence on the outcome model specification. These methods are useful not only for screening out redundant, highly correlated variables but also for arriving at a propensity score model that is consistent and efficient in ATE estimation. However, all of these methods technically must presuppose a parametric logit model for the PS, which could be restrictive given the primary objective is no longer to explain or predict treatment assignment.====A reasonable diagnostic for the PS model is to assess the resulting covariate balance in the matched sample or balance within each stratum after stratifying on the quantiles of the PS. Austin et al. (2007) investigate the issue of variable selection by comparing the ability of different PS model specifications in balancing baseline covariates. Austin (2009), Belitser et al. (2011), and Ali et al. (2014) carry out simulations that compare the ability of different balance measures (standardized mean difference, KS distance, etc.) in assessing whether a PS model is adequate in reducing finite sample bias. However, most of these works (except Belitser and others) only focus on standard measures of balance, which assign equal weights to all covariates and hence do not make a distinction among types of covariates. Caruana et al. (2015) propose a weighted standardized mean difference for PS variable selection, where the weights are coefficients from regressing the observed outcome on the covariates.====We propose a recipe for GPS model selection for estimating pairwise ATEs in observational studies with multiple (====) treatment levels. Motivated by the aforementioned literature, the optimal GPS model is the one that includes only covariates that are predictors of the potential outcomes. Given a set of postulated GPS models, if the set happens to contain the optimal GPS model, then the proposed outcome-adjusted balance measure is able to consistently select the optimal model in large samples. The balance measure evaluates the discrepancies between two estimators of the average covariates, i.e., the GPSM estimator and the sample average of covariates, and imposes weights that penalize models with variable selection different from the optimal GPS model. The weights incorporate the association of covariates and outcome, which can be estimated either parametrically or nonparametrically. We show the selection consistency, i.e., the optimal model can be selected with probability one asymptotically, and that the resulting GPSM estimator of the ATEs based on this model selection criterion is not only consistent and asymptotically normal. Most importantly, this estimator will be efficient based on the empirical evidence.",Outcome-adjusted balance measure for generalized propensity score model selection,https://www.sciencedirect.com/science/article/pii/S0378375822000349,29 April 2022,2022,Research Article,9.0
"Gijbels Irène,Kika Vojtěch,Omelka Marek","Department of Mathematics and Leuven Statistics Research Center (LStat), KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium,Department of Probability and Mathematical Statistics, Faculty of Mathematics and Physics, Charles University, Sokolovská 83, 186 75 Praha 8, Czech Republic","Received 27 April 2021, Revised 13 December 2021, Accepted 9 April 2022, Available online 22 April 2022, Version of Record 11 May 2022.",https://doi.org/10.1016/j.jspi.2022.04.002,Cited by (0),Multivariate tail coefficients are of interest when one is trying to summarize dependence of extremes. The difficulty is that these coefficients are defined as limits and thus to estimate them from data one needs to choose a kind of smoothing parameter ==== are illustrated on a real data set.,"Assume that we observe a ====-variate random vector and we are interested in the tendency of the components to achieve extremely small or large values simultaneously. This interest dates back to studying bivariate extremes (Sibuya, 1960). Since then the research has been focused mainly on studying and describing tail dependence with the help of a stable tail dependence function introduced in Huang (1992) and a tail copula dependence function. For both concepts, which are equivalent in dimension ====, see also Beirlant et al. (2004), De Haan and Ferreira (2006) and the references therein.====Recently a considerable attention is also given to various tail coefficients that summarize the dependence of the extremes. The bivariate tail coefficients studied in Joe (1993) were later generalized to an arbitrary dimensions by Frahm (2006), Li (2009) or Schmid and Schmidt (2007). See also Gijbels et al. (2020) for a recent discussion of these generalizations.====Nevertheless, to be able to use the tail coefficients in practice, one needs to estimate them. For fully nonparametric estimation, this translates into selecting a parameter ====, which corresponds to the proportion of the area of the unit hypercube used to estimate the corresponding limit. This ==== can be also viewed as a kind of a smoothing parameter needed for estimating a tail coefficient.====The problem of choosing ==== was intensively studied in the context of estimating the extreme-value index in univariate data, see e.g. Draisma et al., 1999, Danielsson et al., 2001, Beirlant et al., 2004 and De Haan and Ferreira (2006) and references therein. But for multivariate extremes the problem has not yet been investigated in detail. To the best of our knowledge the method for choosing ==== has been investigated in the context of estimating a stable tail dependence function only in Peng (2010). In the context of tail coefficients we are aware only of the algorithm that searches for a plateau in the considered estimator as a function of ====. This algorithm was introduced in Frahm et al. (2005) and used also in Schmidt and Stadtmüller (2006) in the context of tail copulas. The aim of this paper is to present some alternative methods to choose ==== and compare them in terms of their performances.====In this text, we only focus on the extremal dependence coefficient introduced by Frahm (2006). This coefficient is one of the possible generalizations of the tail dependence parameters (see e.g. Chapter 5.4 in Nelsen, 2006) to more than two dimensions. As shown in Gijbels et al. (2020) this coefficient has many properties that are considered to be desirable when one aims at summarizing the tail dependence by one number. Nevertheless analogously one can handle also other multivariate tail coefficients. See also Section 2.2.====The organization of the paper is as follows. In Section 2 an asymptotic representation of the estimator of the extremal dependence coefficient is derived. In that section we also discuss the link with results in the extreme value theory literature. The asymptotic representation established in Section 2 underpins the practical data-driven methods for choosing ==== suggested in Section 3. The finite-sample performances of these methods are investigated via simulations in Section 4, which also includes a comparison with the plateau algorithm. A real data example is presented in Section 5. Finally, conclusive comments are given in Section 6. Proofs of all theoretical results are deferred to the Appendix.",Choice of smoothing parameter in multivariate copula-based tail coefficients,https://www.sciencedirect.com/science/article/pii/S0378375822000325,22 April 2022,2022,Research Article,10.0
"Kirch Claudia,Stoehr Christina","Otto-von-Guericke University Magdeburg, Institute for Mathematical Stochastics, Center for Behavioral Brain Sciences (CBBS), Magdeburg, Germany,Ruhr-University Bochum, Faculty of Mathematics, Bochum, Germany","Received 19 March 2020, Revised 7 April 2022, Accepted 8 April 2022, Available online 19 April 2022, Version of Record 5 May 2022.",https://doi.org/10.1016/j.jspi.2022.04.001,Cited by (0),Sequential change point tests aim at giving an alarm as soon as possible after a structural break has occurred while controlling the asymptotic ,"Monitoring time series for structural breaks has a long tradition in time series going back to Page, 1954, Page, 1955. In the seminal paper of Chu et al. (1996) sequential change point tests are introduced that allow to control the asymptotic false alarm rate (type-I-error) while guaranteeing that the procedure will asymptotically stop with probability one if a change occurs. Using the data from a stationary historic data set we can estimate the unknown parameters of the time series before a change occurs. This approach has been adopted for a variety of settings and a variety of monitoring statistics. For example, Aue et al., 2006, Horváth et al., 2004 and Hušková and Koubková (2005) derive procedures in linear models, Hlávka et al. (2012) for changes in the error distribution of autoregressive time series, Gut and Steinebach (2002) for renewal processes while Berkes et al. (2004) consider changes in GARCH models. Kirch and Tadjuidje Kamgaing (2015) derive a unified theory based on estimating functions, which has been extended to different monitoring schemes by Kirch and Weber (2018). Bootstrap methods designed for the particular needs of sequential procedures have also been proposed by Hlávka et al., 2016, Hušková and Kirch, 2008 and Kirch (2008). A-posteriori change point tests based on ====-statistics were first proposed in the i.i.d. setting (Csörgő and Horváth, 1988, Ferger, 1994, Gombay, 2001) and then generalized to weakly dependent data (Dehling et al., 2015). Li et al. (2019) propose both offline and online tests based on kernel maximum mean discrepancy, which are closely related to a degenerate ====-statistic.====In this paper, we revisit the sequential test based on ====-statistics that has been proposed in Kirch and Stoehr (2021+), which includes a difference-of-means (DOM) as well as a Wilcoxon sequential test. A similar setting for a-posteriori change point tests has been considered by Csörgő and Horváth (1988) for independent data and by Dehling et al. (2015) for time series data.====The corresponding stopping times have been derived in several papers starting with Aue and Horváth (2004) for a mean change model. For example Aue et al., 2008, Fremdt, 2014 consider a modified test statistic for changes in the mean, Aue et al. (2009) and Černíková et al. (2013) consider changes in a linear regression model, while Gut and Steinebach (2009) consider changes in renewal processes. All of these papers only obtain results for early (sublinear) change points, where the time of change relative to the length of the historic data set vanishes asymptotically. In practice such an approximation only sheds light on changes that occur almost instantly after monitoring starts, while the behavior we observe for moderately early changes is not explained very well by this asymptotic theory. This is why we are interested in deriving the corresponding results not only for such early change points but also for late (linear and even superlinear) changes relative to the length of the historic data set. This has been an open problem even for mean changes and the standard DOM statistic since the first paper (Aue and Horváth, 2004) of this type appeared in 2004.====As in the setting of Chu et al. (1996) we assume the existence of a stationary historic data set ====. Then, we monitor new incoming data by testing for a structural break after each new observation ====, based on a monitoring statistic ====. The procedure stops and detects a change as soon as ====. The weight function ==== is chosen such that ==== converges in distribution to some non-degenerate limit distribution (as the length of the historic data set ==== grows to infinity) if no change occurs. If the threshold ==== is chosen as the corresponding ====-quantile, the procedure has an asymptotic false alarm rate of ==== (while still having asymptotic power one under alternatives).====To motivate the monitoring statistics, consider the mean change model ====where ==== is a stationary time series with mean ====. The change in the mean is given by ==== and is allowed to depend on ====.====For this situation the classical ==== is given by ====The corresponding sequential procedure has already been investigated by several authors including (Aue et al., 2006, Chu et al., 1996, Horváth et al., 2004, Hušková and Koubková, 2005). Clearly, as a version of a two-sample ====-statistic, the corresponding sequential test cannot be expected to be robust. Given the good properties of the Wilcoxon/Mann–Whitney two-sample test it is promising to consider the ==== ====which was recently proposed by Kirch and Stoehr (2021+). Both statistics are sequential ====-statistics of the following type: ====where the kernel ==== is a measurable function and ==== with ==== is an independent copy of ====. In this framework, the DOM-kernel is given by ==== such that ====. The Wilcoxon-kernel is given by ====, such that ====.====The stopping time of the corresponding sequential procedure is given by ====where ====. The monitoring procedure stops as soon as the monitoring statistic ==== exceeds in absolute value a critical curve given by ====. If ==== is chosen as the ====-quantile of the limit distribution as given in Theorem 1, the asymptotic false alarm rate is given by ====. The above weight function is popular in the literature because the corresponding limit distribution as given in Theorem 1 has a nice well-known structure as the supremum in the limit distribution is only taken over ==== despite the infinite observation horizon. Kirch and Stoehr (2021+) consider a much larger class of weight functions including weight functions of the type ====that also have this nice property. However, it is well known that a choice of ==== only results in a quicker detection for early changes (see also Remark 4). Since the main focus of this paper is the analysis of the detection delay for late changes, where the critical curve for larger values of ==== lies above the critical curve for ==== (see e.g. Figure 6.1 in Stoehr (2019)), the results are derived for ==== only. Nevertheless, for early changes the corresponding results for all ==== have been considered in Section 5.1 in Stoehr (2019) and are summarized in Theorem 7.====This paper is organized as follows: In Section 2 we first state the model and the Hoeffding decomposition before discussing the threshold selection. This sets the stage for Section 3, where we discuss asymptotic results for the delay times. In Section 3.1 we obtain conditional results given the historic data. Consequences for the unconditional results are then obtained in Section 3.2. It turns out that the influence of the historic data set on the asymptotically expected stopping times is no longer negligible for late changes. We explain how to obtain the standardizing sequences for the asymptotic results in Section 3.3 giving a sketch of the main proof ideas along the way. In Section 3.4 we compare the DOM and the Wilcoxon procedure based on the obtained theoretic results. In Section 4 we report some simulation results indicating that the asymptotic theory reflects the behavior observed in simulations. In Section 4.1 we compare the asymptotic distribution of the delay times with the distribution obtained from some Monte-Carlo simulations. An empirical comparison between DOM and Wilcoxon kernel is given in Section 4.2. After some conclusions in Section 5 we detail the proofs in Section 6.",Asymptotic delay times of sequential tests based on ,https://www.sciencedirect.com/science/article/pii/S0378375822000313,19 April 2022,2022,Research Article,11.0
"Zhang Ruoyang,Yao Yisha,Ghosh Malay","Statistics Department, University of Florida, United States of America,Department of Statistics, Rutgers University, United States of America","Received 5 May 2021, Revised 19 November 2021, Accepted 30 March 2022, Available online 14 April 2022, Version of Record 11 May 2022.",https://doi.org/10.1016/j.jspi.2022.03.003,Cited by (0)," with continuous shrinkage priors. A very recent work (Sagar et al., 2021) studies the posterior concentration properties of the graphical horseshoe prior and graphical horseshoe-like priors under a full likelihood model. In this paper, we propose a new method that integrates some commonly used continuous shrinkage priors into a quasi-Bayesian framework featured by a pseudo-likelihood. Under mild conditions, we establish an optimal posterior contraction rate for the proposed method. Compared to existing approaches, our method has two main advantages. First, our method is computationally efficient while achieving similar error rate; second, our framework is amenable to theoretical analysis. Extensive simulation experiments and the analysis on a real data set are supportive of our theoretical results.","Precision matrices are critical in a wide range of disciplines, including social networks, biomedical sciences, and economics. The zero-nonzero structure of the precision matrix ==== corresponds to the edge pattern of a Gaussian graphical model where the nodes are distributed as ==== (Lauritzen, 1996). An element ==== of ==== represents the conditional covariance between nodes ==== and ==== given the remaining nodes. Thus ==== implies conditional independence of the two nodes given the rest. Bearing these significant implications, the problem of precision matrix estimation has attracted enormous attention in the past few decades, which can be described as follows. Suppose ==== is a ====-dimensional multivariate normal random vector, ====. Given a design matrix ==== whose rows are ==== copies of ====, one aims to obtain an estimator of ====.====In contemporary real-world applications, researchers frequently confront cases where the dimension ==== of the precision matrix is comparable or even larger than the sample size ==== and the corresponding precision matrix is sparse. Naturally, in such scenarios, one would like to exploit the sparsity of the underlying precision matrix so as to uncover the edge pattern of the graph and to measure the conditional dependence among the nodes. Yet seeking a sparse and accurate estimator can be rather difficult since one needs to address entry selection besides estimation.====The issue of entry selection in precision matrices was raised in Dempster (1972) which he referred to as “covariance selection”. Conventional approaches for covariance selection builds upon discrete optimization, including stepwise forward and backward search, as described in Whittaker, 1990, Lauritzen, 1996, Edwards, 2012. Entry estimation is then based on the selected model (Whittaker, 1990, Lauritzen, 1996, Edwards, 2012). However, these approaches become computationally infeasible even when the dimension ==== is moderately large. Besides, the discrete procedures may lead to selection instability (Breiman, 1996). There have been numerous methods emerging recently that alleviate the instability of the discrete selection procedure and apply to very large graphs. In the following paragraphs we shall selectively review the existing literature for precision matrix estimation.====First we briefly go through several frequentist approaches that emerged recently. They generally fall under four categories, Cholesky decomposition based method, penalized likelihood estimation, regularized regression, and reduced tuning estimation. Huang et al. (2006) proposed to reparameterize the precision matrix or covariance matrix via Cholesky decomposition, then estimate the Cholesky factor, and finally estimate the precision matrix through the Cholesky representation. Penalized likelihood estimators are proposed in Huang et al. (2006), Yuan and Lin (2007), Banerjee et al. (2008), Friedman et al. (2008), Zhang and Zou (2014), among others. Due to the nontrivial constraints, ====, positive definiteness, penalized likelihood methods are relatively computationally expensive. Efficient algorithms are essential for their successful implementation. The existing algorithms to maximize the penalized log likelihood function include the maxdet algorithm (Yuan and Lin, 2007), block coordinate descent (Banerjee et al., 2008), Nesterov’s first order method (Banerjee et al., 2008), and majorization–minimization algorithm (Lange et al., 2000, Friedman et al., 2008). The theoretical properties, ====, convergence rates, of these penalized likelihood estimators are detailed in Rothman et al., 2008, Lam and Fan, 2009, Ravikumar et al., 2011. Regularized-regression type methods are based on the fact that when regressing one node ==== against the remaining nodes ====, the theoretical regression coefficients are equal to ==== (Fan et al., 2016). Naturally Lasso (Tibshirani, 1996), Dantzig selector (Candes and Tao, 2007), and scaled Lasso (Sun and Zhang, 2012) have been employed for columnwise precision matrix estimation/selection, respectively in Meinshausen and Bühlmann (2006), Yuan (2010), and Sun and Zhang (2013). Most of the above methods rely on tuning parameters that are functions of the pre-perceived parameters. While reduced tuning estimation, such as TIGER (Tuning-Insensitive Graph Estimation and Regression) (Liu and Wang, 2017) and EPIC (Estimating Precision matrIx with Calibration) (Zhao and Liu, 2014), is asymptotically tuning-free or requires very few efforts on tuning parameters.====We then discuss some Bayesian approaches which are the primary focus of this paper. In addition to estimation, Bayesian model selection procedures can produce posterior distributions that quantify estimation and selection uncertainty. To promote sparsity, Bayesian approaches usually impose sparsity-inducing priors like spike-and-slab priors, which put a point mass at zero and continuous shrinkage priors including the double exponential prior (Bayesian Lasso) (Park and Casella, 2008, Hans, 2009), horseshoe prior (Carvalho et al., 2010), normal-gamma prior (Brown and Griffin, 2010), double-Pareto prior (Armagan et al., 2013a), Dirichlet–Laplace prior (Bhattacharya et al., 2015), horseshoe+ prior (Bhadra et al., 2017), and the continuous spike-and-slab prior (Ročková, 2018). Compared to the point mass prior, these continuous shrinkage priors ease the computation to a large extent. Moreover, they also enjoy reasonable concentration properties (Song and Liang, 2017, Wei and Ghosal, 2020). The asymptotic normality of posteriors is established in Ghosal, 1997, Ghosal, 1999, Ghosal, 2000 under the condition that ==== grows slower than ====, while the general theory of posterior contraction was established in Ghosal et al. (2000), which provides a broadly applicable technique for more specific settings. In Jiang (2007), it is shown that the posterior convergence rate of some special priors in terms of Hellinger distance can be close to ====. Castillo and van der Vaart (2012) studied the variable selection consistency and posterior contraction in sparse normal mean models with certain point-mass priors. Castillo et al. (2015) established the optimal posterior contraction rate and selection consistency in high-dimensional linear regression models for priors that are mixtures of point masses at zero and continuous distributions. A more recent paper proved the optimal posterior contraction rate for the empirical Bayes method in high-dimensional linear regression models (Belitser and Ghosal, 2020). Compared to point-mass priors, it is more challenging to study the posterior contraction properties under continuous shrinkage priors. Luckily there have been several milestone papers addressing this difficult problem. Armagan et al. (2013b) showed that the posteriors concentrate around the true parameters when ==== grows sufficiently slow (slower than ====) for shrinkage priors. Van Der Pas et al. (2014) and Bhattacharya et al. (2015) gave optimal posterior contraction rates for the horseshoe prior and Dirichlet–Laplace prior, respectively. Song and Liang (2017) showed that a wide class of continuous shrinkage priors would attain a posterior contraction rate similar to that of the spike-and-slab prior in high-dimensional linear regression models. There are also several papers that extend the aforementioned results to generalized linear models (Atchadé, 2017, Wei and Ghosal, 2020), or high-dimensional nonparametric additive models (Yang and Tokdar, 2015, Shen and Ghosal, 2016, Belitser and Ghosal, 2020).====Despite such huge amount of literature on Bayesian approaches in linear models, limited work exists on exploring the theoretical properties of shrinkage priors for sparse precision matrix estimation. The early work by Carvalho and Scott (2009) addresses model selection in Bayesian Gaussian graphical models by combining a multiplicity-correction prior and fractional Bayes factors. Banerjee and Ghosal (2014) imposed a conjugate graphical Wishart prior on ==== and established a contraction rate ==== when the true precision matrix has ====-banded structure, while Xiang et al. (2015) extended the results to general decomposable graphs. A rate of ==== is established in Lee and Lee (2021) for a special class of banded precision matrices using ====-banded Cholesky priors. For arbitrary sparsity structures, Banerjee and Ghosal (2015) used a prior that puts a mixture of a point mass at zero and certain absolutely continuous distribution on off-diagonal elements and established a contraction rate of ==== in terms of Frobenius norm, where ==== is the number of nonzero off-diagonal elements in the true precision matrix. These works either put mixture priors with point mass at zero (Banerjee and Ghosal, 2015) or imposed restrictive assumptions on the structure of true precision matrix, e.g., bandedness (Banerjee and Ghosal, 2014, Lee and Lee, 2021) or decomposable structures (Xiang et al., 2015). In contrast to restricting the structure of the true precision matrix, a very recent study by Sagar et al. (2021) (simultaneous with this paper) pioneeringly establishes a posterior contraction rate ==== under an arbitrary sparsity pattern of the true precision matrix with continuous horseshoe and horseshoe-like priors.====The Bayesian methods mentioned above for precision matrix estimation are based on the multivariate Gaussian likelihood. Two recent papers by Atchadé, 2017, Atchadé, 2019 came up with a quasi-Bayesian scheme where the spike-and-slab prior is combined with a pseudo-likelihood, and the resulting quasi-posterior distribution has nice contraction properties. Inspired by this pseudo-likelihood function in Atchadé, 2017, Atchadé, 2019, we propose to estimate large sparse precision matrices by integrating continuous shrinkage priors into a quasi-Bayesian scheme. Although we use a similar quasi-Bayesian framework as in Atchadé (2019), there is significant difference. Atchadé (2019) studied the spike-and-slab prior within the quasi-Bayesian scheme, while we explore the theoretical properties of some common continuous shrinkage priors. An optimal posterior contraction rate is established for our method by extending some techniques in Song and Liang, 2017, Wei and Ghosal, 2020, Song, 2020. To the best of our knowledge, this is the first effort to study the posterior contraction properties of a general class of continuous shrinkage priors in a quasi-Bayesian framework for precision matrix estimation.====The rest of this paper is organized as follows. Section 2 introduces some of the most relevant works and outlines our quasi-Bayesian framework. Section 3 states some assumptions and provides theoretical analysis on the posterior contraction rate. In Sections 4 Simulation study, 5 An illustration with real data, we illustrate the proposed method through extensive simulation studies and analyzing a real data set. Finally in Section 6, we conclude with some discussions and possible future research topics. Proofs are relegated to Appendix A.",Contraction of a quasi-Bayesian model with shrinkage priors in precision matrix estimation,https://www.sciencedirect.com/science/article/pii/S0378375822000301,14 April 2022,2022,Research Article,12.0
"Zhu Rong,Zhang Xinyu,Wan Alan T.K.,Zou Guohua","MRC Biostatistics Unit, School of Clinical Medicine, University of Cambridge, Cambridge CB2 0SR, UK,Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing, China,Beijing Academy of Artificial Intelligence, Beijing, China,Department of Management Sciences and School of Data Science, City University of Hong Kong, Kowloon, Hong Kong,School of Mathematical Sciences, Capital Normal University, Beijing, China","Received 12 December 2020, Revised 17 March 2022, Accepted 17 March 2022, Available online 28 March 2022, Version of Record 16 April 2022.",https://doi.org/10.1016/j.jspi.2022.03.002,Cited by (0),"Motivated by the inability of the ====-fold cross validation criterion for choosing model weights that reflect the support of the models by the data, and prove that the resultant model average estimator has an optimal ","The objective of this paper is to develop a frequentist model averaging (FMA) method for linear regression models in the face of linear inequality constraints on the unknown regression parameters. We adopt a ====-fold cross validation approach for selecting model weights which is shown to result in an estimator with an optimal asymptotic property and favourable finite sample performance. In the rest of the Introduction, we discuss the motivation for this work and provide a synopsis of the existing literature and an outline of the proposed approach.",Frequentist model averaging under inequality constraints,https://www.sciencedirect.com/science/article/pii/S0378375822000295,28 March 2022,2022,Research Article,13.0
"Bonsaglio Marta,Fortini Sandra,Ventz Steffen,Trippa Lorenzo","Department of Decision Sciences, Università Bocconi, Italy,Dana-Farber Cancer Institute, United States,Department of Biostatistics, Harvard T.H. Chan School of Public Health, United States","Received 1 July 2021, Revised 10 March 2022, Accepted 15 March 2022, Available online 23 March 2022, Version of Record 31 March 2022.",https://doi.org/10.1016/j.jspi.2022.03.001,Cited by (0)," at completion of the study.==== of the number of patients assigned to each arm and of the randomization probabilities. We use these results to approximate relevant operating characteristics such as the power of the BUD. We evaluate the accuracy of the approximations through simulations under several scenarios for binary, time-to-event and continuous outcome models.","Randomized clinical trials (RCTs) are essential to demonstrate the efficacy of novel experimental therapies (Council et al., 1948). The landscape of clinical studies has changed during the last decades, with an increasing number of trials that utilize adaptive designs, in some cases to evaluate several experimental treatments in biomarker-defined subpopulations (Barker et al., 2009, Ventz et al., 2017). Adaptive designs are attractive to reduce the duration of the study and to allocate efficiently limited resources (Berry et al., 2004). Most adaptive designs use data generated during the clinical trial for interim decisions (Berry et al., 2004), for example to vary the randomization probabilities during the study (Barker et al., 2009, Berry and Eick, 1995, Ventz et al., 2017, Zhou et al., 2008) or to discontinue the evaluation of an experimental treatment (Ventz et al., 2017). In multi-arm studies adaptive randomization algorithms unbalance the randomization probabilities, in most cases, towards the most promising treatments. This can increase power compared to balanced randomization, and it can reduce the overall sample size necessary to test experimental treatments (Wason and Trippa, 2014). Adaptive randomization procedures have been developed for several designs, including multi-arm studies (Berry and Eick, 1995, Berry et al., 2010), platform and basket studies (Barker et al., 2009, Ventz et al., 2017, Zhou et al., 2008).====The decision theoretic paradigm has been used to develop trial designs (Berry, 1978, Berry and Fristedt, 1985, Ding et al., 2008). The study aims and costs are represented by a utility function ==== of the data ==== generated during the trial and the study design ====. Using a Bayesian joint model for patient profiles, outcomes and other key variables, candidate designs ==== can be compared by computing their expected utility ====. The optimal design maximizes ==== among all candidate designs.====Several approximations of the described optimization have been proposed. For example, Ventz et al. (2018) discussed ==== (BUDs), a class of approximate decision theoretic designs. The utility function ==== in BUDs coincides with an information metric. In different words, the goal is to minimize uncertainty at completion of the study.====BUDs for dose-finding and basket trials have been discussed in Domenicano et al., 2019, Trippa and Alexander, 2016. Previous work, related to BUDs, proposed information-based sampling schemes (Berry et al., 2002, Müller et al., 2006, Russo and Van Roy, 2017).====There is a rich literature on large sample analyses of adaptive designs. For instance, (Bai and Hu, 1999, Wei et al., 1979) studied the behavior of sequential urn schemes. See also (Bai and Hu, 2005, Ghiglietti et al., 2017, Rosenberger, 2002, Zhang, 2016) for a recent summary on large sample results for urn schemes. The limiting behavior of adaptive biased coin designs have been investigated, among others, by Eisele and Woodroofe, 1995, Hu et al., 2004, Hu et al., 2009. Relevant work connecting stochastic approximation with response-adaptive clinical trials include (Bartroff et al., 2010, Laruelle and Pagès, 2013).====In this manuscript we focus on the asymptotic characteristics of BUDs. The design of adaptive clinical trials requires the estimation of pivotal operating characteristics, such as type I and II error rates and the distribution of patients randomized to each treatment arm. In most cases these estimates are based on time consuming Monte Carlo simulations, conducted for different candidate designs and varying key parameters, including sample sizes, enrollment rates, and outcome distributions. Approximations of the operating characteristics, beyond simulations, using asymptotic results, are crucial to compare designs across plausible scenarios.====The need for computationally efficient approximations of design-specific operating characteristics motivates our study. We show the almost sure convergence and asymptotic normality of the relative allocation of patients to treatment arms in BUDs. We first derive analytic results assuming that the treatment-specific outcome distributions belong to natural exponential family (Diaconis and Ylvisaker, 1979), and later relax this assumption. In our analysis, we represent BUD randomization procedures as stochastic approximations (SAs). We study the ordinary differential equations associated with the resulting SAs and the stability of the stationary points, following the framework developed in Benveniste et al. (2012) and using results of  (Kushner and Yin, 2003, Laruelle and Pagès, 2013). We illustrate through examples the accuracy of the asymptotic approximations by comparing asymptotic and Monte-Carlo estimates of operating characteristics of BUDs.====Our asymptotic results allow investigators to quickly approximate the distribution of the number of patients that will be assigned to each arm and the power of BUDs.",Approximating the operating characteristics of Bayesian Uncertainty directed trial Designs,https://www.sciencedirect.com/science/article/pii/S0378375822000283,23 March 2022,2022,Research Article,14.0
"Wang Xiaogang,Zhou Xiaoying,Li Bing,Zhang Feipeng,Zhou Xian","School of Mathematics and Information Science, North Minzu University, Yinchuan, 750021, China,School of Mathematics and Statistics, Hainan Normal University, Haikou, 571158, China,Key Laboratory of Data Science and Smart Education, Ministry of Education, Hainan Normal University, Haikou, 571158, China,School of Economics and Finance, Xi’an Jiaotong University, Xi’an, 710049, China,Department of Actuarial Studies and Business Analytics, Macquarie University, North Ryde, NSW 2109, Australia","Received 26 March 2020, Revised 14 January 2022, Accepted 25 February 2022, Available online 8 March 2022, Version of Record 18 March 2022.",https://doi.org/10.1016/j.jspi.2022.02.008,Cited by (2),"In this article, we consider a bent line ==== model with an unknown change point due to a ","In the literature of censored regression models, ==== is used to estimate linear relationships between dependent variable and explanatory variables when dependent variable is left censoring (Tobin, 1958, Amemiya, 1973, Powell, 1984, Richard et al., 2007, Ye et al., 2018), also refer to Gijbels (1994) and Linton (2002) for more details. In the traditional Tobit regression model, the response variable related to a covariate of interest is linear in different domains of the covariate. However, in many applications in biological, economic, financial and other studies, the response and a covariate of interest would show a piecewise linear relationship that has varying slopes over different domains of the covariate.====As a motivating example, the household financial assets are affected by the level of education. There is a nonlinear relationship in the sense the financial assets are lower when the education level is before undergraduate, and then are higher when the education level after undergraduate. It implies that the growth curve of financial assets may be described by two bent lines with different slopes intersecting at a change point. In Stanford heart transplant data, transplanted subjects whose ages are greater than 50 years old have a major risk with respect to younger subjects (Molinari et al., 2001). These examples demonstrate that the model may be described by two line segments with different slopes intersecting at an unknown change point. However, when the effect of a covariate of interest on the censored response variable is segmented, the standard Tobit regression model using linear structure may lead to inaccurate inferences. This issue motivates us to consider the bent line Tobit regression model.====Note that the regression function in the bent line regression model is constrained to be everywhere continuous with a kink and its slope is not always constant but varies at a certain point. The bent line regression model is used to analyze mouse leukaemia (Rigby and Stasinopoulos, 1992), time series of AIDS cases (Stasinopoulos and Rigby, 1992), the land mammal maximal running speeds (Li et al., 2011), and the credit or bank card fraud data (Zhou and Zhang, 2020). Also, the bent line regression model has been received much attention in the context of least squares regression (Quandt, 1958, Quandt, 1960, Hinkley, 1969, Feder, 1975, Chappell, 1989, Chan and Tsay, 1998, Chiu et al., 2006, Hansen, 2017), quantile regression (Li et al., 2011), rank-based regression (Zhang and Li, 2017b), and expectile regression (Zhang and Li, 2017a). However, no literature has investigated bent line models in the context of Tobit regression.====The primary purpose of this article is to propose a bent line Tobit regression model. It is worth emphasizing that the proposed bent line Tobit regression model provides a useful tool for studying the bent line effect of a specific covariate on the left censored response and serves as a useful alternative to the Cox model (Pons, 2003, Jensen and Lutkebohmert, 2008) or transformation models (Kosorok and Song, 2007) with a change point in a covariate threshold for right censored data. One main contribution of this article is to estimate the regression coefficients and to identify the change point location in the bent line Tobit regression model. Since the objective function is neither differentiable nor nonconvex with respect to the change point, we propose a grid search strategy to estimate the regression parameters and the unknown change point. We establish the limiting distribution of the proposed estimator and show that estimator of the change point location achieves ====-consistency. This is quite different from the ====-consistency of the change point estimator in the Tobit model with a structural change in Bhattacharjee et al. (2017). Another contribution of this article is to develop a formal test procedure for the existence of a change point, based on a weighted CUSUM type statistic, which was inspired by Qu (2008) and Zhang and Li (2017a). The limiting distributions of the proposed test statistic under both the null and local alternative hypotheses are derived by the empirical processes theory.====The rest of the article is organized as follows. In Section 2, we propose the grid search method to estimate all coefficients and the unknown change point in the bent line Tobit model, and derive their asymptotic properties. Simulation results and a real data analysis are presented in Section 3. Section 4 concludes the article with some discussions. All the proofs are presented in Appendix.",A bent line Tobit regression model with application to household financial assets,https://www.sciencedirect.com/science/article/pii/S0378375822000209,8 March 2022,2022,Research Article,15.0
Rezende Thiago,"Department of Statistics, Universidade Federal de Minas Gerais, Belo Horizonte, MG, 31270-901, Brazil","Received 6 November 2020, Revised 8 February 2022, Accepted 25 February 2022, Available online 8 March 2022, Version of Record 17 March 2022.",https://doi.org/10.1016/j.jspi.2022.02.009,Cited by (1),"A vast amount of methods have been developed to make inferences for volatility data, taking into account the stylized facts of rate/return data. However, the common problem is that the latent parameters of many volatility models are high-dimensional and analytically intractable. Therefore, their inference procedure requires ==== methods for making the inference of static parameters and perform a simulation exercise to study the estimators’ properties. Our results show that the proposed model can be reasonably estimated, and the approximation of our method is reasonable. Furthermore, we provide a case study of the pound/dollar and real/dollar exchange rate to illustrate our approach’s performance. For the real/dollar time series, the model captures a high volatility pattern due to the COVID-19 pandemic.","Since volatility is a latent/hidden component, its estimation calls for specific techniques and appropriate statistical inferences. A huge amount of volatility models have been proposed to estimate volatility in volatility data. Deterministic and stochastic volatility (SV) models are the two main classes. Some examples of them are the GARCH family (Tsay, 2010) (deterministic) and the SV model of Taylor (1994) (stochastic).====The SV model class includes the SV model of Taylor (1986) and generalizations of it, and dynamic volatility models with a proper filtering procedure under Bayesian and/or classical perspectives. Due to the difficulty in obtaining the marginal likelihood of the model according to Fridman and Harris (1998), the GARCH model has been an attractive option among the users even though it is not a stochastic volatility model. The problem of some SV models is the needed of using intensive computational methods to deal with high-dimensional volatility. This requires large computer-intensive simulations (Broto and Ruiz, 2004), like Monte Carlo Markov Chain (MCMC) (Jacquier et al., 2004, Omori et al., 2007, Nakajima and Omori, 2012, Kastner and Fruhwirth-Schnatter, 2014, Abanto-Valle et al., 2015), the particle filter (Pitt and Shephard, 1999, Lopes and Tsay, 2011, Malik and Pitt, 2011, Warty et al., 2018) and Laplace (Rue et al., 2009) methods.====The most common approach is to use a model linearization to avoid intensive numerical techniques for dealing with the high-dimensional of volatility. This approach consists of converting it into a linear state-space model by a transformation in the rate/return data and, hence, to apply the Kalman filter (Harvey et al., 1994, Kim et al., 1998 see). Another option is to derive a proper filtering procedure. SV models and methods using a model linearization comprehend the quasi-maximum likelihood (QML) Harvey et al. (1994), the Monte Carlo likelihood (MCL) Danielsson (1994) and Sandmann and Koopman (1998), Monte Carlo integration/importance sampling (Davis and Yam, 2005) methods.====Dynamic volatility models with a proper filtering procedure include the works of Shepard, 1994, Triantafyllopoulos, 2008, Deschamps, 2011, Triantafyllopoulos, 2012, Gamerman et al., 2013, Pinho and Santos, 2013, Pinho et al., 2016, Souza et al., 2018, Rego and dos Santos, 2020 and Aktekin et al. (2020). Gouriéroux and Jasiak, 2006, Vidoni, 2004, Varin and Vidoni, 2008, Creal, 2017, Bormetti et al., 2020 and King et al. (2021), which developed the filtering, forecasting, and smoothing procedures to estimate the latent states and to compute the likelihood function in state-space models, approximately/analytically. Ferrante and Vidoni (1998) presented a method to obtain finite-dimensional filters in discrete time, providing theoretical results and interpreting known outcomes in a more general setting. Although, it is not easy to build a filter, and, in general, the evolution equation is quite restrictive.====In face to these difficulties, another alternative is to build new filtering inference procedures with an approximated marginal/integrated likelihood and a smart strategy in Bayesian inference procedure for a dynamic volatility model with a flexible evolution equation without model linearization, inspired in the Dynamic Generalized Linear Models (DGLM) (West and Harrison, 1997, Souza et al., 2018).====This study’s primary objective/contribution is to develop a new filtering inference procedure with a marginal likelihood for a GED state-space volatility model (GED-SSV model), named the Conjugate Update Procedure 1 (CUP1) filter. The CUP1 provides a marginal likelihood that is a product of the generalized Student’s t-distributions evaluated directly. The inferential procedure is fast and easy to implement under the Bayesian approach. Also, we investigate the approximation quality of the CUP1 and introduce an approximated smoothing method in this work.====The paper is organized in the following manner. First, Section 2 presents the GED-SSV model. Then, Section 3 offers a simulation study, and Section 4 provides a case study of the proposed model using real currency exchange data. Finally, Section 5 concludes the paper, including an indication of potential areas for future research.",A new filtering inference procedure for a GED state-space volatility model,https://www.sciencedirect.com/science/article/pii/S0378375822000210,8 March 2022,2022,Research Article,16.0
"Noonan Jack,Zhigljavsky Anatoly","School of Mathematics, Cardiff University, Cardiff, CF24 4AG, UK","Received 18 January 2021, Revised 16 February 2022, Accepted 23 February 2022, Available online 4 March 2022, Version of Record 15 March 2022.",https://doi.org/10.1016/j.jspi.2022.02.006,Cited by (0), and discuss the situations when the resulting approximations achieve high accuracy.,"Assume that there are ==== items (units, elements, variables, factors, etc.) ==== with some of them ==== (significant, important, etc.). The problem of group testing (also known as “pooling” or “factor screening”) is to determine the defective items by testing a certain number of ==== ====. A design ==== is a collection of ==== test groups. We assume that all test groups ==== belong to some set ==== containing certain subsets of the set ====. The set ==== will be called ====The group testing problems differ in the following aspects:====The group testing problems considered in this paper are specified by the following properties.====Group testing is a well established area and has attracted significant attention of specialists in optimum design, combinatorics, information theory and discrete search. The origins of group testings can be traced back to the paper (Dorfman, 1943) devoted to adaptive procedures of blood testing for detection of syphilitic men. Since then, the field of group testing has seen significant developments with extensive literature and numerous books dedicated to the field. The textbooks (Du and Hwang, 2000, Du and Hwang, 2006) and lecture notes (D’yachkov, 2014) provide a background on group testing especially for zero-error non-adaptive problems. An excellent introduction and summary of recent developments in group testing and its connection to information theory can be found in Aldridge et al. (2019). The group testing problem in the binomial sample is especially popular in the group testing literature, see Aldridge et al., 2019, Sobel and Groll, 1959 and Torney et al. (1998).====Research in group testing often concentrates around the following important areas:====In this paper, we touch upon all the above areas. In particular:====Existence theorems for group testing problems were extensively studied in Russian literature by M.B. Malutov, A.G. Dyachkov, V.V. Rykov and other representatives of the Moscow probability school, see e.g. D’yachkov and Rykov (1983) and Tsybakov et al. (1983). The construction of upper bounds for the length of optimal zero-error designs in the binary group testing model has attracted significant attention; see Du and Hwang (2000) for a good survey. In the papers (Katona and Srivastava, 1983, Macula, 1997a, Macula and Reuter, 1998), the construction schemes of group testing designs in important specific cases, including the case of the binary model with two and, more generally, ==== defectives, are studied. Using probabilistic arguments, existence theorems for designs under the zero-error criterion for the additive model have been thoroughly studied in Zhigljavsky and Zabalkanskaya (1996). Motivated by the results of Zhigljavsky and Zabalkanskaya (1996), in Zhigljavsky (2003) expressions for the binary model were derived under the zero-error and small-error criterions. The results of Zhigljavsky (2003) provided the inspiration for this paper. Note that there is a limited number of results on construction of optimal algorithms for finding one, two or three defectives in search with lies, see e.g. De Bonis et al., 1997, Hill and Karim, 1992 and Macula (1997b). Some asymptotic expansions in existence theorems for general group testing problems have been derived in Zhigljavsky (2010).====In the majority of papers devoted to construction of designs for the non-adaptive binary group testing problem, the designs are built from the so-called disjunct matrices, these are defined in Section 4.5. Moreover, the COMP decoding procedure (according to COMP, all items in a negative test are identified as non-defective whereas all remaining items are identified as potentially defective, see Section 4.5) is often used for identification of the set of defective items; see e.g. a popular paper (Chan et al., 2014) and a survey on non-adaptive group testing algorithms through the point of view of decoding of test results (Chen and Hwang, 2008). Despite common claims, as explained in Sections 4.5 Comparison with designs constructed from the disjunct matrices, 4.6 Efficiency of the COMP decoding procedure for random designs, the designs based on the use of disjunct matrices are inefficient and the COMP decoding procedure alone leads to poor decoding.====In the asymptotic considerations, we assume that the number of defective items is small relative to the total number of items ====; that is, we consider a very sparse regime. Many results can be generalized to a sparse regime when ==== slowly increases with ==== but ==== as ====. There is a big difference between the asymptotic results in the sparse regime and results in the case when ==== as ====. In particular, in view of Cantor and Mills, 1966, Erdős and Rényi, 1963, Lindström, 1964 and Lindström (1975), where the non-adaptive group testing problem for the additive model is considered with no constraints on both the test groups and the number of defective items, ====, ====, for the minimal length of the non-adaptive strategies that guarantee detection of all defective items. For fixed ====, the best known explicit constructions of designs come from number theory (Bose and Chowla, 1962, Lindström, 1969) and are closely related to the concept of Bose–Chaudhuri–Hocquenghem codes. For these constructions it is shown that ==== tests are required. For ====, the best currently known construction is with ==== and can be obtained from results of D’yachkov and Rykov (1981) and Poltyrev (1987). This result is constructed using random coding and is shown to be order-optimal.====In the very sparse regime with ==== constant and ====, the best known upper bound for the length of zero-error designs in the binary group testing problem has been derived in Dyachkov et al. (1989), see also Theorem 7.2.15 in Du and Hwang (2000): ====, where ====and ====. Asymptotically, when both ==== and ==== are large, this is a marginally better bound than the asymptotic bound ==== which has been derived in D’yachkov and Rykov (1983) by the probabilistic method based on the use of the Bernoulli design. Exactly the same upper bound can be obtained using random constant-weight designs, see Corollary 5.2 in Zhigljavsky (2003). Development of existential (upper) bounds for group testing designs for binary group testing has been complemented by establishing various lower bounds; for comparison of the lower and upper bounds, see the well-written Section 7.2 of Du and Hwang (2000).====Primarily for the binary model, notable contributions in recent years are as follows. In Aldridge et al. (2014), the authors consider the problem of nonadaptive noiseless group testing problem using Bernoulli designs and describe a number of algorithms used to locate the defective set after the design has been constructed; one of these is the COMP procedure which will be discussed in Section 4.5. For bounds on the number of tests when using Bernoulli designs, also see Scarlett and Cevher, 2016a, Scarlett and Cevher, 2016b. In Aldridge et al. (2016), instead of Bernoulli designs the authors consider designs where each item is placed in a constant number of tests. The tests are chosen uniformly at random with replacement so the test matrix has (almost) constant column weights, these terms will be fully explained in Section 4.3. The authors show that application of the COMP detection algorithm with these constant column-weight-designs significantly increases detection of the defective items in all sparsity regimes. This (almost) constant-column-weight property will be discussed further in Section 4.3 where it will be combined with a Hamming distance constraint to improve the probability of separation. In Coja-Oghlan et al. (2020a), for the randomized design construction discussed in Aldridge et al. (2016), the authors provide a sharp bound on the number of tests required to locate the defective items. In Coja-Oghlan et al. (2020b), the authors consider existence bounds for both a test design and an efficient algorithm that solve the group testing problem with high probability. In Mézard and Toninelli (2011), the authors consider the binomial sample group testing problem where each item is defective with probability ====. The authors construct a class of two-stage algorithms that reach the asymptotically optimal value of ====. The asymptotic bounds for the one-stage (nonadaptive) setting for the binomial sample problem are studied in Mézard et al. (2008).====This paper differs from the aforementioned papers in the following aspects: (a) the majority of known theoretical results require large ==== and only numerical evidence is presented when ==== is small; this paper, however, provides rigorous results for any ==== where many asymptotic results do not apply; (b) the asymptotic expansions in this paper provide constants that have crucial significance when ==== is only moderately large (this additional constant term is not present in many asymptotic results for group testing); (c) many of the previously cited papers use decoding procedures that do not guarantee identification of the defective set even if it is possible to locate it. Procedures like COMP are fast to execute, and as previously mentioned, with certain design constructions can in a large number of cases locate the defective set. However, in this paper we will use decoding procedures that will guarantee the location of the defective set if this is possible given the design.====By requiring a given design to satisfy the constraint of being able to find the defective items, we are considering an example of a (random) constraint satisfaction problem (CSP). Many of the main advances of this paper can be viewed as the careful counting of satisfying assignments for a CSP, where the satisfying assignments can correspond to tests that are able to differentiate between different subsets of ====. The techniques used in this paper are related to approaches used in the random CSPs literature, see for instance Zdeborová and Krzakala (2016). However group testing problems are very specific and cannot be simply considered as specific application of the general CSP methodology.====The rest of the paper is organized as follows. In Section 2 we develop a general methodology of derivation the lower bounds for ====, the probability that all defective items are uniquely identifiable from test results taken according to constant-weight random designs and establish several important auxiliary results. In Section 3 we derive lower bounds for ==== in a general group testing problem and consider the case of additive model for discussing examples and numerical results. The more practically important case of the binary model is treated in Section 4. Section 2 is devoted to asymptotic existence bounds and construction of accurate approximations. In Appendix A we provide some proofs and in Appendix B we formally describe the algorithm of Section 4.3. Let us consider the content of Sections 2 General discrete search problem, random designs and the probability of solving the problem, 3 Derivation of an upper bound for, 4 Group testing for the binary model, 5 Asymptotic results in more detail.====In Section 2.1 we discuss general discrete search problems. In Section 2.2 we develop the general framework for derivation of the upper bounds ==== for ====, the probability that for a random design all defective items cannot be recovered; the main result is formulated as Theorem 1. Theorem 2 of Section 2.3 extends Theorem 1 to the case when some of ==== test results are allowed to be wrong (the case of lies). In Section 2.4 we show how many of our results can be reformulated in terms of existence bounds in the cases of weak and exact recovery. In Section 2.5 we consider different assumptions on the occurrence of defective items and the randomization schemes used for the construction of the randomized designs. In Sections 2.6 and  2.7 we formulate two important combinatorial results, Lemma 2, Lemma 3.====In Section 3.1 we derive upper bounds ==== for ==== for a general test function (1.1) in the most important case ====; that is, when all ==== have exactly ==== items (see (2.14) for the formal definition of ====). In Section 3.2 we specialize the general results of Section 3.1 to a relatively easy case of the additive model and consider special instances of the information about the defective items including the case of the binomial sample case, see Corollary 2. In Section 3.3 we provide some results of simulation studies for the additive model. In Section 3.4 we show how to extend the results established for the case ==== to cover other randomization schemes for choosing the groups of items ==== including the case of Bernoulli designs.====In Section 4.1 we provide a collection of upper bounds ==== for ==== for different instances of the binary model. All results formulated in this section follow from general results and specific considerations of Sections 3.1 General test function, 3.4 Extension for. In Section 4.2, we illustrate some of the theoretical results formulated in Section 4.1 by results of simulation studies. In Section 4.3 we develop a procedure for construction of a sequence of nested nearly doubly regular designs ==== which, for all ====, have large Hamming distances between all pairs ====
 ====. With the help of numerical studies we also demonstrate excellent separability properties of the resulting designs. In Section 4.4 we apply the technique of Section 4.3 and numerically demonstrate that indeed the resulting designs provide a superior separability relative to random designs. In Section 4.5 we numerically compare random, improved random of Section 4.3 and the very popular designs constructed from the disjunct matrices. In particular, we find that improved random designs have a better separability than the designs constructed from the disjunct matrices, see Table 10, Table 11. In Section 4.6 we discuss the (in)efficiency of the COMP decoding procedure. In Section 4.7 some specific upper bounds are specialized for the binary group testing with lies; simulation results are provided to illustrate theoretical bounds.====In Section 5.1 we describe the technique used to transform finite-==== results into the asymptotic expansions. A very important feature of the developed expansions is that in the very-sparse regime we have explicit expressions for the constant term, additionally to the main term involving ====. In Sections 5.4 Binary model, weak recovery, 5.2 Additive model, 5.3 Binary model, exact recovery we apply results of Section 5.1 respectively to the cases of additive model (both exact and weak recoveries), binary model with exact recovery and the binary model with weak recovery. Results of these sections clearly demonstrate the following: (a) weak recovery is much simpler than exact recovery, (b) the constant terms in the asymptotic expansions play an absolutely crucial role if these expansions are used as approximations, and (c) the resulting approximations have rather simple form and are very accurate already for moderate values of ====. Finally, in Section 5.5 we discuss a technique of transforming the asymptotic upper bounds for ==== for noise-free group testing problems into upper bounds for ==== in the same model when up to ==== lies are allowed.",Random and quasi-random designs in group testing,https://www.sciencedirect.com/science/article/pii/S0378375822000180,4 March 2022,2022,Research Article,17.0
"Dasgupta Subhadra,Mukhopadhyay Siuli,Keith Jonathan","IITB-Monash Research Academy, India,Department of Mathematics, Indian Institute of Technology Bombay, India,School of Mathematics, Monash University, Australia","Received 10 April 2021, Revised 24 January 2022, Accepted 16 February 2022, Available online 25 February 2022, Version of Record 10 March 2022.",https://doi.org/10.1016/j.jspi.2022.02.004,Cited by (3),This article focuses on the estimation and design aspects of a ,"Kriging is a method for estimating a variable of interest, known as the ====, at unknown input sites. When multiple responses are collected, multivariate kriging, also known as cokriging, is a related method for estimating the variable of interest at a specific location using measurements of this variable at other input sites along with the measurements of ====, which may provide useful information about the primary variable (Myers, 1983, Myers, 1991, Wackernagel, 2003, Chiles and Delfiner, 2009). For example, consider a water quality study in which a geologist is interested in estimating pH levels (primary response) at several unsampled locations along a river, but auxiliary information such as phosphate concentration or amount of dissolved oxygen may facilitate more accurate estimates of pH levels. We may also consider a computer experiment, where the engineering code produces the primary response and its partial derivatives. The partial derivatives (secondary variables) provide valuable information about the response (Santner et al., 2010). This scenario is typical when the responses measured are correlated, both non-spatially (at the same input sites) and spatially (over different sites, particularly those close to each other).====Very little is known about designs for such cokriging models. Li and Zimmerman (2015), Madani and Emery (2019), Bueso et al. (1999), Le and Zidek (1994), Caselton and Zidek (1984) developed optimal designs for multivariate kriging models or multivariate spatial processes, however the designs were all based on numerical simulations. The key difficulty in using such multivariate models is specifying the cross-covariance between the different random processes. Unlike direct covariance matrices, cross-covariance matrices need not be symmetric; indeed, these matrices must be chosen in such a way that the second-order structure always yields a non-negative definite covariance matrix (Genton and Kleiber, 2015, Subramanyam and Pandalai, 2004). A broad list of valid covariance structures for multivariate kriging models has been proposed by Li and Zimmerman (2015).====In this article, we address two issues for bivariate cokriging experiments, (i) estimation of the primary variable and (ii) determining optimal designs by minimizing the mean squared error of the estimation. In the first couple of sections, we discuss simple and ordinary bivariate collocated cokriging models, the various covariance functions available in the literature for such models, and their estimation aspects. Specifically, we consider two stationary and isotropic random functions, ==== and ==== over ====, where ==== is the primary variable and ==== is the secondary/auxiliary variable. Our main interest is in the prediction of ====, at a single location, say ====, in the region of interest. For defining covariance matrices for the bivariate responses, we mainly utilize two families of stationary covariances, namely the generalized Markov-type and the proportional covariance functions. The generalized Markov-type covariance, an extended version of Markov-type covariance, is a new function proposed in this article. Along with the generalized Markov-type and proportional covariances, the other covariance types mentioned by Li and Zimmerman (2015) are also studied. We prove a linear dependency condition under which the best linear unbiased predictor (BLUP) of ==== in a bivariate cokriging model is shown to be equivalent to the BLUP in a univariate kriging setup. A wide class of covariance functions is identified which allows this reduction.====In the later part of the article, we determine optimal designs for some cokriging models, particularly those for which the reduction holds true. We consider the maximum and the integrated cokriging variance of ==== as the two design criterion functions. The primary variable is assumed to have an isotropic exponential covariance, that is, it satisfies ==== with marginal variance ==== and the exponential parameter ====. Note, ==== is also called an Ornstein–Uhlenbeck process (Antognini and Zagoraiou, 2010). For known covariance parameters in simple and ordinary cokriging models, we prove that the equispaced design minimizes the maximum and integrated prediction variance, that is, it is both G-optimal and I-optimal. In real life, however, covariance parameters are most likely unknown. To address the dependency of the design selection criterion on the unknown covariance parameters, we assume prior distributions on the parameter vector and instead determine pseudo-Bayesian optimal designs. The equispaced design is also proved to be the Bayesian I- and G-optimal design.====The original contributions of this article include (i) a linear dependency condition for reduction of collocated bivariate kriging estimators to a kriging estimator, (ii) the generalized Markov-type covariance, (iii) G-optimal designs for known covariance parameters and G-optimal Bayesian designs, for such simple and ordinary reduced bivariate cokriging models and (iv) I-optimal Bayesian designs.====We stress that our sole objective is to find theoretical, exact optimal designs, not numerical designs, for bivariate cokriging models. For this reason, we consider only the exponential covariance structure for the primary variable ====. Note no theoretical exact optimal designs for covariance structures other than the exponential covariance are currently available in the statistical literature.====Many researchers have studied D- and I-optimal designs for univariate kriging experiments with an exponential covariance structure. For single responses and one-dimensional inputs, Kisel’ák and Stehlík (2008), Zagoraiou and Antognini (2009) and Antognini and Zagoraiou (2010) proved that equispaced designs are optimal for trend parameter estimation with respect to average prediction error minimization and the D-optimality criterion. For the information gain (entropy criterion) also, the equispaced design was proved to be optimal by Antognini and Zagoraiou (2010). Zimmerman (2006) studied designs for universal kriging models and showed how the optimal design differs depending on whether covariance parameters are known or estimated using numerical simulations on a two-dimensional grid. Diggle and Lophaven (2006) proposed Bayesian geostatistical designs focusing on efficient spatial prediction while allowing the parameters to be unknown. Exact optimal designs for linear and quadratic regression models with one-dimensional inputs and error structure of the autoregressive of order one form were determined by Dette et al. (2008). This work was further extended by Dette et al. (2013) to a broader class of covariance kernels, where they also showed that the arcsine distribution is universally optimal for the polynomial regression model with correlation structure defined by the logarithmic potential. Baran et al. (2013) and Baran and Stehlík (2015) investigated optimal designs for parameters of shifted Ornstein–Uhlenbeck sheets for two input variables. More recently, Sikolya and Baran (2020) worked with the prediction of a complex Ornstein–Uhlenbeck process and derived the optimal design with respect to the entropy maximization criterion.====In Sections 2 Cokriging models and their estimation, 3 Bivariate covariance functions we introduce bivariate cokriging models and the related functions, respectively. The linear dependency condition which allows the BLUP of a cokriging model to reduce to the BLUP of a kriging model is discussed in Section 4. In Section 5, we discuss optimal designs for some cokriging models with known and unknown parameters. An illustration using a water quality data set is provided in Section 6. Concluding remarks are given in Section 7.",Optimal designs for some bivariate cokriging models,https://www.sciencedirect.com/science/article/pii/S0378375822000167,25 February 2022,2022,Research Article,18.0
"Dupuis Xavier,Tardivel Patrick J.C.","Institut de Mathématiques de Bourgogne, UMR 5584 CNRS, Université Bourgogne Franche-Comté, F-21000 Dijon, France","Received 14 April 2021, Revised 27 December 2021, Accepted 16 February 2022, Available online 25 February 2022, Version of Record 7 March 2022.",https://doi.org/10.1016/j.jspi.2022.02.005,Cited by (4),"A decade ago OSCAR was introduced as a penalized estimator where the penalty term, the sorted ==== norm, allows to perform clustering selection. More recently, SLOPE was introduced as a penalized estimator controlling the False Discovery Rate (FDR) as soon as the hyper-parameter of the sorted ==== norm is properly selected. For both, OSCAR and SLOPE, numerical schemes to compute these estimators are based on the proximal operator of the sorted ==== norm. The main goal of this note is to provide a short and simple formula for this operator. Based on this formula one may observe that the output of the proximal operator has some components equal and thus this formula corroborates that SLOPE, as well as OSCAR, perform clustering selection. Moreover, our geometric approach to prove the formula for the proximal operator provides insights to show that testing procedures based on SLOPE are more powerful than step-down testing procedures but less powerful than step-up testing procedures.","Octagonal Shrinkage and Clustering Algorithm for Regression (OSCAR) (Bondell and Reich, 2008) and Sorted L-One Penalized Estimation (SLOPE) (Bogdan et al., 2015, Zeng and Figueiredo, 2014) are both penalized estimators based on the sorted ==== norm. First introduced in the particular case where the loss function is the residual sum of squares, these estimators are defined as follows: ====For OSCAR the hyper-parameter ==== has arithmetically decreasing components. SLOPE is both an extension of OSCAR where the hyper-parameter satisfies ==== and ==== and an extension of the Least Absolute Shrinkage and Selection Operator (LASSO) (Chen and Donoho, 1994, Tibshirani, 1996). Indeed, when ==== then the penalty ==== coincides with the ==== norm; the well known penalty term for the LASSO. With respect to other penalized estimators, when ====, SLOPE (and a fortiori OSCAR) has the particularity to perform clustering selection, namely some components of the SLOPE estimator are equal in absolute value (Bondell and Reich, 2008, Schneider and Tardivel, 2020). Moreover, in the linear Gaussian model, by taking ==== (where ==== is the standard normal cumulative distribution function) SLOPE estimator allows to control the False Discovery Rate at level ==== in the particular case where ==== is an orthogonal matrix (Bogdan et al., 2015). Note that the above hyper-parameter also called BH sequence coincides with thresholds given in the seminal article introducing the Benjamini–Hochberg’s procedure and controlling the FDR (Benjamini and Hochberg, 1995).====Numerically, like many penalized estimators where the loss function is smooth and the penalty term is non-smooth, one may solve SLOPE or OSCAR with a Forward–Backward proximal gradient algorithm (see e.g. Combettes and Wajs, 2005, Parikh and Boyd, 2014). This method relies on the computation of the proximal operator for the sorted ==== norm. This operator is also a very important tool for the development of the approximate message passing theory (Bu et al., 2020, Zhang and Bu, 2021). A statistical motivation for this operator relies on the fact that, when ==== is an orthogonal matrix (==== ====), the SLOPE estimator is the image of the ordinary least squares estimator ==== by the proximal operator of the sorted ==== norm. Indeed, since ==== is orthogonal to the vector space ====, one may deduce the following identity: ====which gives the claimed formula when ==== is an isometry (==== ====).====There is a particular case under which the proximal operator of the sorted ==== norm is explicit; when ==== and ==== then the proximal operator is simply given by ====. Otherwise, when components of ==== are non-increasing and non-negative, an algorithm computing the proximal operator for the sorted ==== norm is given in Bogdan et al. (2015) (see algorithm 3). This algorithm suggests to first identify a sub-sequence ==== non-decreasing and non-constant (where ====) and then to substitute (1) ==== by ==== and (2) ==== by ====. Whereas correct, we believe that this algorithm is difficult to implement because it is not easy to identify iteratively non-decreasing and non-constant sub-sequence.====The main motivation for this note is to provide a short and simple formula for the proximal operator of the sorted ==== norm. The proof for this formula is based on recent advances on sub-differential calculus for the sorted ==== norm and on the description of the signed permutahedron polytope (the signed permutahedron is the sub-differential of the sorted ==== norm at ====) (Schneider and Tardivel, 2020). We also illustrate that some sub-differential calculus rules give geometrical insights for testing procedures based on SLOPE. In particular there is a geometrical way to understand why the testing procedure based on SLOPE with the BH sequence is more conservative than the seminal Benjamini–Hochberg’s procedure.",Proximal operator for the sorted ,https://www.sciencedirect.com/science/article/pii/S0378375822000179,25 February 2022,2022,Research Article,19.0
"Jiang Yuanyuan,Xu Xingzhong","School of Mathematics and Statistics, Beijing Institute of Technology, Beijing, 100081, China,Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing, 100081, China","Received 26 January 2021, Revised 3 February 2022, Accepted 7 February 2022, Available online 15 February 2022, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jspi.2022.02.002,Cited by (0),None,"In statistical hypotheses testing, likelihood ratio test is the primary approach owing to its good behavior. However, likelihood ratio tests are intractable for the irregular models. The family of skew-normal distributions introduced by Azzalini (1985) is just such one. For skew-normal distributions, the likelihood ratio test is not well defined since the maximum likelihood estimator is infinite with positive probability. The classical likelihood ratio test statistic is the ratio of the maximum likelihood functions. When the maximum likelihood estimator is unobtainable, we could replace the maximum likelihood ratio with the integrated likelihood ratio. Integrating the likelihood function with respect to a probability distribution of the parameter over the parameter space, the integrated likelihood ratio is the Bayes factor proposed in Jeffreys (1931) and the densities of the parameter are the priors. In this paper, we use Bayes factors to construct test statistics, and propose significant tests for testing skewness of the skew-normal distributions.====Suppose that a random variable ==== is distributed as the skew-normal distribution with density function ====written as ====, where ==== and ==== are the location, scale and skew parameters respectively, ==== is the standard normal probability density function and ==== is the standard normal cumulative distribution function. Specially when ==== and ====, the distribution is called the standard skew-normal distribution with skew parameter ====, written as ====.====The pioneering works of the skew-normal distribution are given by Azzalini, 1985, Azzalini, 1986. Then Henze (1986) provided a probabilistic representation of the skew-normal distribution family in terms of a normal random variable and a truncated normal random variable. Azzalini and Valle (1996) generated it to multivariate case and Azzalini and Capitanio (1999) examined further probabilistic properties of the distribution.====Because of the particular and attractive characteristics of skew-normal distribution, inferences for the parameters have also been considered in the literature. It has been proposed by Pewsey (2000) that the profile-likelihood function for ==== has a stationary point at ====, independently of the observed sample, which causes no unique solution to the likelihood equations. Liseo and Loperfido (2004) showed the maximum likelihood estimate for ==== could be infinite with high probability for small ==== and large ====.====Due to the behavior of the likelihood function of the skew-normal distribution, Chiogna (2005) considered likelihood based inference for the parameter of a skew-normal distribution, derived the rate of convergence to the asymptotic distribution of the maximum likelihood estimator and studied an alternative parameterization. Sartori (2006) proposed the bias prevention of maximum likelihood estimates for scalar skew-normal distribution using a modified score function as an estimating equation for the shape parameter. Bayes and Branco (2007) presented a discussion about the Bayesian inference for the skew parameter of the scalar skew-normal distribution, considered hypotheses tests using Bayes Factor but not as a significant test. Sanqui et al. (2012) extended the locally optimal test which was proposed by Salvan (1986) to the skew parameter test of the skew-normal distribution. Genetic algorithm is used in Yalcinkaya Abdullah and Birdal (2018) to get the maximum likelihood estimation for the parameters of skew-normal distribution. Penalized maximum likelihood estimators are proposed in Azzalini and Arellano-Valle (2013) and Jin et al. (2016). Thiuthad et al. (2018) tested the hypothesis on the location parameter of skew-normal distribution. Thiuthad and Pal (2019) dealt with estimation of the location parameter of a skew-normal distribution.====As seen before, few works are concerned with the hypothesis testing for the parameters of skew-normal distribution. Therefore, we will pay attention to the testing of skewness with Bayes factors as significant test statistics. For the skew-normal distribution ====, the hypotheses to be tested are ====where ====, ====, ==== is the set of all real numbers, ==== is the set of all positive real numbers, and ==== is the subspace of ==== defined as ====where ==== is a preassigned constant. We denote ==== the true parameter which generates the data when the null hypothesis is true.====Let ==== be an independently and identically distributed sample with size ==== from ====. The Bayes factor for testing hypotheses (1.2) is ====where ==== and ==== are the likelihood function and the prior density on ====, ==== and ==== respectively. Bayes factor is not only the integrated likelihood ratio but also the ratio of the two prior predictive densities for the density of data under the null and alternative hypotheses respectively, which inherits the likelihood ratio statistic in Neyman–Pearson fundamental lemma.====Starting from the standard skew-normal distribution, we use Bayes factors as test statistics from a frequentist perspective. First, we show that the likelihood functions are quadratic mean differentiable and their Taylor series expansions at true parameter are locally uniform. Then, the local normality of the posterior distribution (i.e., Bernstein–von Mises Theorem) is committed, which leads to the ====-consistency of the posterior. It follows that the Bayes factors can be rewritten in a way such that the asymptotic distributions of the linear functions of the logarithms of Bayes factors are chi-square distribution with 1 degree of freedom. Finally, we construct the rejection region with appropriate priors. Through Le Cam’s First and Third Lemma, the local power of the test is represented by the quantile of a non-central chi-square distribution.====When both location and scale parameters are unknown, we also obtain corresponding results as the standard skew-normal distributions except ====. It should be noted that the Fisher information matrix is singular if ====. Hence, the asymptotic chi-square distribution cannot be derived by our method. Further, if ==== is near to zero, the relative variable converges slowly to chi-square distribution. Therefore, we give the asymptotic chi-square distribution of Bayes factor with centered parameters in Azzalini (1985) except ====. In addition, for specific prior, we show that the distribution of Bayes factor under the null hypothesis is independent of the nuisance parameters. Then an exact test based on the Bayes factor can be constructed, which includes the case of ====.====The rest of this article is organized as follows. Section 2 is concerned with the standard skew-normal distribution ====. Section 3 extends the results in Section 2 to the case that nuisance parameters are unknown and provides an exact test based on the Bayes factor with specific prior. Section 4 reports simulation results and Section 5 presents conclusions. Technical proofs are deferred to the Appendix.",Testing the skewness of skew-normal distribution by Bayes factors,https://www.sciencedirect.com/science/article/pii/S0378375822000143,15 February 2022,2022,Research Article,20.0
Haines Linda M.,"Department of Statistical Sciences, University of Cape Town, Private Bag X3, Rondebosch, 7701, South Africa","Received 23 June 2021, Revised 27 January 2022, Accepted 7 February 2022, Available online 14 February 2022, Version of Record 23 February 2022.",https://doi.org/10.1016/j.jspi.2022.02.003,Cited by (0),This paper is concerned with approximate ====-optimal designs with their ====-optimal counterparts are made and a Pareto approach to obtaining a design which is a compromise between ====- and ====-efficiency is introduced. Examples which reinforce the findings are presented throughout.,"Polynomial regression models are used extensively in the fitting of response surfaces to data (Myers et al., 2016). In formulating the model, the independent variables are usually taken to be factors, with levels of ==== and ====, or to be continuous variables, scaled to lie between ==== and ====. The design spaces so defined correspond to the unit cube or the unit ball, with the cube being, arguably, the most highly researched and the most used in practice. Many criteria for constructing designs which are in some sense optimal for this setting have been proposed and a blend of such criteria is generally adopted in practice. At the same time, ====-optimal designs, that is designs which minimize the generalized variance of the parameter estimates of the polynomial model, are invariably of particular interest. In addition, recent and more broad-based work in response surface methodology has also focussed on ====-optimal designs, that is designs which minimize the integrated prediction variance over the design space (Goos and Jones, 2011, Jones and Goos, 2012, de Oliveira et al., 2019). These designs display many attractive properties and compare favourably with their ====-optimal counterparts.====The present study is concerned with designs for the full polynomial model over the unit ball with independent variables which are continuous. The unit ball is an attractive design space in that it can readily accommodate rotatable designs, that is designs for which the prediction variance depends only on the distance of the response from the origin, and, in addition, is often appropriate in practice. Kiefer, in a seminal paper in 1961, addressed the problem of constructing optimal designs for this setting and, specifically, demonstrated that the approximate ====-optimal designs allocate weights to uniform distributions placed on spheres centred at the origin and thus concentric or coincident with the boundary of the unit ball. The result was further developed by Farrell et al. (1967) and Galil and Kiefer, 1977, Galil and Kiefer, 1979 and has found relevance more latterly within the context of Euclidean design theory (Sawa et al., 2019). Kiefer’s design provides a benchmark for ====-optimal or near-====-optimal designs comprising a finite set of support points over the unit ball and is particularly valuable in this regard. However, the structure of the approximate ====-optimal design does not appear to extend to other criteria, and in particular to the alphabetic optimality criteria detailed in Atkinson et al. (2007), with the exception of a generic criterion which includes ====-optimality. This special case, and the proof thereof, was noted in a brief remark by Galil and Kiefer (1977, p.30) but has not been pursued further.====The aim of the present study is to explore the nature and properties of approximate ====-optimal designs for full polynomial regression models over the unit ball and, thereby, to fill a gap in the optimal design literature. The paper is organized as follows. Some necessary notation and detail is given in Section 2 and the main theorem relating to the designs of interest is presented in Section 3. An efficient construction of the designs is detailed in Section 4, together with an illustrative example and tables reflecting the scope of the optimal designs so constructed. In Section 5, the properties of the approximate ====-optimal designs are compared with their ====-optimal counterparts and, in addition, a Pareto front approach to the construction of a design which represents a compromise between ====- and ====-efficiency is included. A broad-based discussion, including some pointers for further research, is given in Section 6.",Approximate ,https://www.sciencedirect.com/science/article/pii/S0378375822000155,14 February 2022,2022,Research Article,21.0
"Tang Yinfen,Peng Ye,Zhang Zhiyuan","School of Statistics and Mathematics, Shanghai Lixin University of Accounting and Finance, China,School of Statistics and Management, Shanghai University of Finance and Economics, China","Received 8 February 2021, Accepted 2 February 2022, Available online 11 February 2022, Version of Record 21 February 2022.",https://doi.org/10.1016/j.jspi.2022.02.001,Cited by (0),This paper investigates the challenging problem of volatility estimation using high-frequency financial data when both price discreteness and ,None,"Trading information, price discreteness, and volatility estimation",https://www.sciencedirect.com/science/article/pii/S0378375822000118,11 February 2022,2022,Research Article,22.0
"Dao Cecilia,Jiang Jiming,Paul Debashis,Zhao Hongyu","Yale University School of Medicine, New Haven, CT, USA,Department of Statistics, University of California, Davis, Davis, CA, USA,Yale University School of Public Health, New Haven, CT, USA","Received 6 February 2021, Revised 4 October 2021, Accepted 9 January 2022, Available online 25 January 2022, Version of Record 14 February 2022.",https://doi.org/10.1016/j.jspi.2022.01.003,Cited by (0)," is complicated and not ready to be implemented for practical use. In this paper, we develop practical and computationally convenient methods for estimating such asymptotic variances and constructing the associated confidence intervals. Performance of the proposed methods is evaluated empirically based on Monte-Carlo simulations and real-data application.","Genome-wide association studies (GWAS) have proved successful by scanning the genome for genetic variations, e.g., single nucleotide polymorphisms (SNPs), that are associated with disease status and traits across study subjects. Tens of thousands of SNPs have been identified to be associated with various diseases and traits. For a review of the remarkable discoveries through GWAS, see Visscher et al. (2017). Researchers can use GWAS results to further medical research, such as to determine a person’s risk of developing a disease or treat/prevent the disease. Genetic factors may account substantially for disease risk or various traits, and heritability estimates the proportion of variation in a phenotype due to genetic (and environmental) differences between individuals in a population. Historically, heritability was inferred from resemblance among different degrees of related individuals (e.g., twin studies) without studying specific genetic variations, but today there is an emerging interest in quantifying how much variation can be accounted for from GWAS data due to the recent development of efficient genotyping and sequencing technology and the success of the GWAS strategy. However, when GWAS significant variants were considered, they only explained a small fraction of the genetic component of the phenotypes. The gap between the phenotypic variance explained by significant GWAS results and that estimated from classical heritability methods is known as the “missing heritability problem”.====More precisely, the problem refers to the concept that SNPs that are significant in GWASs cannot fully account for heritability of many diseases and traits. One explanation for missing heritability is that many SNPs jointly affect the phenotype, and SNPs with smaller effects that have not been identified may contribute to heritability as well. To address this issue, Yang et al. (2010) used an approach involving linear mixed models (LMMs) to show that a large proportion of heritability is not missing but rather captured by SNPs with weak effects that do not reach genome-wide significance level. The general idea is to use an LMM to treat the effects of all SNPs as random effects rather than relying on single-SNP association analysis. This approach has been widely used for heritability estimation in the genetics community via the genome-based restricted maximum likelihood (GREML) method. A popular implementation of GREML, with assumptions regarding the variance of the effect size prior distributions, is the genome-wide complex trait analysis (GCTA) software in Yang et al. (2011a).====In an attempt to make the modeling more accurate, others have proposed extensions of this LMM approach. For instance, Heckerman et al. (2016) proposed to add an environmental random effect (along with a genomic random effect) in the LMM to reduce heritability inflation, and Zhou et al. (2013) proposed to use a hybrid of LMM and regression models to learn the true genetic architecture from the data to estimate heritability. To improve heritability estimation compared to GCTA, Speed et al. (2017) developed the LDAK model to factor in minor allele frequency (MAF), linkage disequilibrium (LD), and genotype certainty. Speed et al. (2020) extended their LDAK model to handle more complex heritability models by proposing an approximate model likelihood to be computed by GWAS summary statistics. Comprehensive comparisons of heritability estimation methods [Yang et al., 2011b, Yang et al., 2015, Speed et al., 2017, Speed et al., 2012, Zaitlen et al., 2013, Bulik-Sullivan et al., 2015] can be found in Evans et al. (2018). Zhu and Zhou (2020) also provide a review of statistical methods for heritability estimation.====Consider an LMM which can be expressed as ====where ==== is an ==== vector of observations; ==== is an ==== matrix of known covariates; ==== is a ==== vector of unknown regression coefficients (the fixed effects); and ====, where ==== is an ==== matrix whose entries are random variables. Furthermore, ==== is a ==== vector of random effects that are distributed as ====, ==== is an ==== vector of errors that is distributed as ====, and ====, ==== and ==== are independent. The heritability parameter is defined as ==== when the average trace of the genetic relationship matrix is one (i.e., under the GCTA model); for the general equation, see equation B1 in Speed et al. (2012).====The LMM (1) is the model used by Yang et al. (2010) where it is assumed that the effects of all the SNPs (random effects) are nonzero. The restricted maximum likelihood (REML) estimator of the heritability is given by ====, where the estimates of the variance components ==== and ==== are based on the REML method [e.g., Jiang, 2007, Section 1.3.2]. In reality, however, only a subset of the SNPs are potentially nonzero. Let ==== be the vector of effect sizes across the whole genome, where non-causal SNPs have a zero effect. Without loss of generality, we can assume that ====, where ==== is the vector of the first ==== components of ====
 ====, and ==== is the ==== vector of zeros. Correspondingly, we have ====, where ==== is ====, and ==== is ====. Therefore, the true LMM can be expressed as ====With respect to the true model (2), the assumed model (1) is misspecified. We call the latter a misspecified LMM, or mis-LMM.====Jiang et al. (2016) showed that even under a mis-LMM, ==== and ==== are consistent by investigating the asymptotic behavior of the estimators as the sample size and the number of SNPs increase to infinity, such that their ratio converges to a finite, nonzero constant. However, the asymptotic variances of the REML estimators have complex forms that are not ready to be implemented for practical use. This issue is important, from a practical point of view, because the asymptotic variance is used to obtain the standard error of the estimator, and confidence interval for the associated parameter, in applications. The main goal of the current paper is to propose accurate estimators of the variance of ==== and ==== along with confidence intervals that are robust even under the mis-LMM. The proposed variance estimators are derived based on asymptotic approximation; they have analytic expressions and are simple to use. Using the variance estimators and Jiang et al. (2016), we construct approximate ==== confidence intervals for the associated parameters.====In this paper, we first derive the variance estimators and associated confidence intervals, providing technical details in the Appendix. Then, we compare the performance of our method with that of GREML under the GCTA model through simulation studies and a real data example using the UK Biobank data. We end with a discussion of the results.",Variance estimation and confidence intervals from genome-wide association studies through high-dimensional misspecified mixed model analysis,https://www.sciencedirect.com/science/article/pii/S0378375822000039,25 January 2022,2022,Research Article,23.0
"Zhang Rui,Wang Dehui,Li Cong","School of Mathematics, Jilin University, Changchun, Jilin 130012, China,School of Economics, Liaoning University, Shenyang 110036, China","Received 3 March 2021, Revised 9 November 2021, Accepted 6 January 2022, Available online 17 January 2022, Version of Record 1 February 2022.",https://doi.org/10.1016/j.jspi.2022.01.002,Cited by (2),". We also investigate the finite-sample performance of the proposed method in simulation studies. Finally, a real data example is provided to illustrate the model.","Over the last few years, time series with a finite range from ====, ==== have been widely reported. For example, the number of occupied habitat patches in a metapopulation of size ====
 (Weiß and Pollett, 2012), the number of ==== districts in Germany which have at least one new case of measles (Yang et al., 2018), the number of access to the websites of ==== staff members from a department (Kim et al., 2020). To model this kind of data, McKenzie (1985) first proposed the binomial AR(1) (BAR(1)) model: ====where ====, ==== with ==== and ====. Here, the thinning operator ====’ is proposed by Steutel and Van Harn (1979): ====, where ====s are identically and independent distributed (i.i.d.) Bernoulli random variables with success probability ====. All thinnings are performed independently of one another and the thinnings at time ==== are independent of ====. Model (1) was generalized. Brännäs and Nordström (2006) considered replacing ==== with ==== to present an econometric model to account for the tourism accommodation impact of arranging festivals or special events in many cities. Möller et al. (2016) proposed a class of self-exciting threshold binomial autoregressive (SET-BAR) processes. Gouveia et al. (2018) introduced the bvAR(p) model using the binomial variation operator.====In some cases, the parameters ==== and ==== in model (1) may vary with time and may be random with dependence structures. For example, in a certain hotel of ==== rooms. Let ==== represent the number of occupied rooms at time ====. Then, ==== denotes the number of still occupied rooms at time ==== and ==== denotes the number of newly occupied rooms at time ====. Then the coefficients ==== and ==== may be affected by a lot of same factors, such as the current season, the service level and advertisement, and could vary randomly over time with a dependence structure. As further example, consider the securities business in a certain financial market, which has ==== companies. Let ==== denote the number of traded companies at time ====. Then, ==== is the number of still traded companies at time ==== and ==== represents the newly traded companies at time ====. Again ==== and ==== may dependent and vary randomly since ==== and ==== may be affected by various same factors, such as the fluctuate of the financial market, the national policies, etc. These examples lead researchers to select feasible distributions that can capture the distributions and dependence structure between two random coefficients. Weißand Kim (2014) proposed the BBAR(1) model using the beta-binomial thinnings. Weißand Pollett (2014) considered a class of density-dependent BAR(1) process. Kang et al. (2019) extended the binomial AR(1) model based on the generalized binomial thinning operator. However, the dependence structure is limited and do not offer enough flexibility for dependence modeling. So we consider some other alternative forms to construct bivariate distribution for the coefficients.====A popular alternative is the construction of models based on copulas. They provide a general way of introducing dependence among several series with given distributions. Copulas have gained much attention in many different fields, such as asset pricing, portfolio management, and risk management applications (Jung et al., 2008). Karlis and Pedeli (2013) used the Frank and normal copulas to model the dependence of the innovations of the BINAR(1) model. Buteikis and Leipus (2019) analyzed additional copula families and different estimation methods for BINAR(1) model. Føkianos et al. (2020) proposed a joint distribution of counts with copula structure on waiting time of the Poisson process.====In this paper, we propose a new class of binomial AR(1) models, in which we use the Farlie–Gumbel–Morgenstern (FGM) copula and let the coefficients follow beta distributions, respectively. The motivation of such approach is that: (i) the beta distribution has support on ====, and (ii) the FGM copula accommodates both positive and negative correlations between two coefficients and has simple form: ====where ==== is the dependence parameter.====The outline of the paper is as follows. In Section 2, we propose a first-order flexible binomial autoregressive (FBAR(1)) process using copulas and discuss its basic probabilistic and statistical properties. In Section 3, the estimation problem of the unknown parameters is considered. The related finite sample performance for the estimation methods are studied in Section 4. In Section 5, the proposed method is applied to real examples, which demonstrate the usefulness and flexibility of our model. Some concluding remarks are given in Section 6. Some details of proofs and lemmas are reported in Appendix.",Flexible binomial AR(1) processes using copulas,https://www.sciencedirect.com/science/article/pii/S0378375822000027,17 January 2022,2022,Research Article,24.0
Jeong Seonghyun,"Department of Statistics and Data Science, Department of Applied Statistics, Yonsei University, Seoul, Republic of Korea","Received 22 November 2020, Revised 30 May 2021, Accepted 3 January 2022, Available online 11 January 2022, Version of Record 29 January 2022.",https://doi.org/10.1016/j.jspi.2022.01.001,Cited by (0),This paper studies posterior contraction rates in multi-category logit models with priors incorporating group sparse structures. We consider a general class of logit models that includes the well-known ,"The theory of high-dimensional sparse regression has recently received a great deal of attention in the Bayesian community. Most existing studies on Bayesian sparse regression have examined continuous response variables (e.g., Castillo et al., 2015, Martin et al., 2017, Gao et al., 2020, Belitser and Ghosal, 2020, Jeong and Ghosal, 2021b). However, discrete response variables are also very useful and essential in many areas of application; thus, they deserve far more attention than they have received. In particular, the theory of Bayesian high-dimensional regression for multi-categorical (nominal) responses has not yet been investigated in the literature.====In this paper, we aim to fill this gap by considering high-dimensional logit models for categorical responses under group sparsity. For every ====, with the sample size ====, let the response variable be ====, where ==== represents the number of categories. Let ==== be the total number of parameters, ==== be a design matrix for the ====th observation, and ==== be a vector of regression coefficients. We can then write a general logit model for the categorical response ==== as ====where ==== is the ====th row of ==== and ==== is the probability operator. The covariate vector ==== quantifies characteristics of category ==== against the reference category ====. It is obvious that the model subsumes logistic regression models for binary response variables. More precisely, model (1) is reduced to a standard logistic regression model when ====. Form (1) is general in the sense that the covariates can vary with ====, but it is often assumed that these covariates are not category-specific. We present the following two examples to elaborate upon this point.====In view of Example 1, group sparse modeling is extremely useful for variable selection in the multinomial logit models. However, Example 2 suggests that a specific treatment of the multinomial logit models may not be sufficient and indicates that considering the general framework itself in (1) could be highly beneficial. We refer the reader to Hoffman and Duncan (1988) for further discussion on the multinomial and conditional logit models.====We study the posterior contraction rates of model (1) under group sparsity, possibly with unequal group sizes. We are primarily interested in the high-dimensional setting for which ====, where ==== is the number of groups. Clearly, ====. Note that ==== if sparsity is imposed at the individual level only. Using a lasso-type penalty, the idea of group sparse estimation was first considered for linear models in Yuan and Lin (2006) and extended to logistic regression in Meier et al. (2008). A group lasso for multinomial logit models was considered in Vincent and Hansen (2014). However, even when taking the frequentist perspective, theoretical studies on high-dimensional group sparse estimation are mostly directed at linear models (Nardi and Rinaldo, 2008, Huang and Zhang, 2010, Lounici et al., 2011), and few extensions have been attempted; see Blazère et al. (2014) for some findings for the generalized linear model setting. Within the Bayesian framework, the estimation properties for group sparse modeling have only recently been studied, even in the case of linear regression (Ning et al., 2020, Bai et al., 2020, Gao et al., 2020). To the best of our knowledge, the estimation properties for model (1) with group sparsity have not been examined previously, not even in the frequentist literature.====Although model (1) has not been scrutinized under group sparsity conditions, some Bayesian works on binary logistic regression, which is subsumed by our setup, do exist. Under the high-dimensional generalized linear model framework, Jiang (2007) established contraction rates relative to the Hellinger metric with sparsity-inducing priors. More recently, Jeong and Ghosal (2021a) obtained ====-type posterior contraction results directly on regression coefficients under relaxed assumptions. Wei and Ghosal (2020) examined posterior contraction in logistic regression using continuous shrinkage priors. Model selection consistency of high-dimensional logistic regression was considered by Narisetty et al. (2019) under individual sparsity and by Lee and Cao (2021) under group sparsity, respectively. All these works, however, require some size restrictions on the true regression coefficients. Such a requirement is often undesirable in high-dimensional scenarios (Castillo et al., 2015). To the best of our knowledge, Atchadé (2017) is the only available Bayesian work that makes no direct restriction on size. He obtained a lasso-type ====-contraction rate in high-dimensional logistic regression under certain compatibility conditions. However, we find that his results can be refined under our framework, as will be seen in Section 3. As such, this study improves the findings of Atchadé (2017) and goes beyond it by studying posterior contraction for model (1) under group sparsity without any direct size restrictions on the coefficients.====The rest of this paper is organized as follows. Section 2 describes the notation and specifies the prior distribution. Section 3 provides our main results on the posterior contraction rates of high-dimensional logit models under group sparsity. The technical proofs are provided in Section 4. Lastly, Section 5 concludes with a discussion. Auxiliary results are presented in Appendix.",Posterior contraction in group sparse logit models for categorical responses,https://www.sciencedirect.com/science/article/pii/S0378375822000015,11 January 2022,2022,Research Article,25.0
"Banerjee Sayantan,Shen Weining","Operations Management & Quantitative Techniques Area, Indian Institute of Management Indore, Madhya Pradesh, India,Department of Statistics, University of California, Irvine, CA, USA","Received 26 December 2020, Revised 21 November 2021, Accepted 29 December 2021, Available online 5 January 2022, Version of Record 29 January 2022.",https://doi.org/10.1016/j.jspi.2021.12.012,Cited by (1),"-shrinkage prior as a special case, and it is shown that this rate is optimal up to a logarithmic factor, besides automatically adapting to the unknown edge sparsity level of the graph. The excellent empirical performance of the proposed method is demonstrated via extensive simulation studies and applications to stock market data and signal denoising over large real-world networks.","Let ==== be an undirected graph with node (or, vertex) set ==== and edge set ====. The main focus of this paper is the estimation of a piecewise constant signal ==== defined over ====. In particular, we consider the following model, ====where ==== is the signal associated with the node ==== for every ====, ==== are independent and identically distributed (i.i.d) random Gaussian noises with an unknown variance parameter ====, and ==== are the observed data points. By assuming that ==== has a ==== structure, we mean that for any pair of indexes ====, there is a chance that the associated signals satisfy ====.====Model (1) is often referred to as a graph denoising model, and it is closely connected to the vast literature on change point detection (Tartakovsky et al., 2014, Brodsky and Darkhovsky, 2013) and shape constrained regression (Guntuboyina and Sen, 2018), especially when ==== is a chain graph. In general, the graph denoising model has been widely studied in various areas such as image segmentation and denoising (Besag, 1986), signal processing (Gavish et al., 2010), and network analysis (Crovella and Kolaczyk, 2003, Shuman et al., 2013). In the literature, regularization has become a popular strategy to solve the graph denoising problem (Barron et al., 1999, Birgé and Massart, 2007). In particular, Fan and Guan (2018) proposed an ====-edge denoising method by considering the following minimization problem, ====where ==== is the observed data, and ==== is the tuning parameter. Also, throughout the paper, we use the notation ==== to denote the ====-norm of a vector ====, and ==== is the indicator function. Due to the combinatorial nature of the objective function in (2), it is usually very difficult to find the exact solution to the minimization for general graphs. Fan and Guan (2018) hence developed an efficient approximation algorithm for solving (2) within a polynomial time and showed that the resulting solution achieved the same risk bound (up to a constant factor) with that of the exact minimizer of (2). Alternatively, Padilla et al. (2017) proposed a fused Lasso approach by considering an ==== convex relaxation of (2), i.e., ====The key idea of that paper is to first perform a depth-first search (DFS) (Tarjan, 1972) that converts an arbitrary graph to a linear chain, and then apply the fused Lasso (Rudin et al., 1992, Tibshirani et al., 2005) for denoising purpose (also known as total variation denoising). The authors also provided a theoretical justification for using DFS, that is, they showed that the total variation in the signal defined over the chain graph obtained from DFS is at most twice as its total variation over the original graph. Based on this result, the authors obtained an upper bound for the mean squared error (MSE) of their proposed signal estimators and established minimax optimality for the obtained bound with respect to tree-structured graphs.====Despite their success in computation and theory, it still remains unclear how the aforementioned regularization methods can be used to conduct statistical inference for graph signal denoising problems; and this is the main focus of our paper. In particular, we propose a simple-yet-useful Bayesian solution to graph denoising problem. The main idea is to first perform DFS that converts a general graph to a chain graph, and then conduct Bayesian fusion estimation over the DFS-induced chain graph by shrinking the difference between consecutive pairs of signals towards ====. One advantage of adopting a full Bayesian framework is that it automatically accounts for the uncertainty from both DFS and fusion estimation. Moreover, the inference can be conveniently conducted based on posterior samples. For Bayesian fusion estimation, we choose to use the recently studied ====-shrinkage prior (Song and Liang, 2017, Song and Cheng, 2019), which has been shown to have a better theoretical (e.g., in terms of posterior contraction rate) and empirical performance (Bhattacharya et al., 2015, Castillo et al., 2015) than the conventional Laplace prior in Bayesian fused Lasso (Kyung et al., 2010). We show that the posterior computation can be conveniently implemented using a Gibbs sampler thanks to the conjugacy structure in our model. We also derive a posterior contraction rate for the estimated signals with respect to the class of “edge sparse” graphs. To be precise, we show that when ==== satisfies ====, then with posterior probability tending to one, the mean squared error for ==== is of order ====, which only differs with the oracle rate ==== (assuming that the true graph sparsity structure is known) by a logarithmic factor in ====. This result provides a useful theoretical justification for our proposed method, and also reveals a desirable adaptive estimation property since we do not require the knowledge of the graph edge sparsity parameter ==== when constructing the prior while the posterior can automatically achieve the optimal rate of convergence with respect to different values of ====.====Despite the fast growing literature on change point detection and shape-constrained regression for linear chains, the exact minimax rate for estimating a piecewise constant or monotonic signal over the linear chain is only obtained recently by Gao et al. (2020). In the Bayesian domain, several approaches have been proposed to fit such models, such as the recursion method (Hutter et al., 2007), Bayesian fused Lasso (Kyung et al., 2010), and Bayesian trend filtering (Roualdes, 2015). The posterior contraction property for those methods, however, remains largely unknown until recently. For example, Song and Cheng (2019) proposed to fit a Bayesian piecewise constant model using a heavy-tailed shrinkage prior and obtained the posterior contraction rate for the estimated signals. Liu et al. (2020) considered a Bayesian piecewise polynomial model using an empirical Bayesian approach and obtained some posterior contraction rate and structure recovery results. Bayesian piecewise constant models on general graphs, to the best of our knowledge, still remain unsolved to date; and our theoretical contribution in this paper is to fill this gap.====Another paper that is worth mentioning is Li and Sang (2019), where the authors study spatial regression problem and propose a new clustering approach for the regression coefficients at different locations. One key step in their approach is to construct a spatial penalty function over an edge set (where each edge connects a pair of locations), which is obtained by the minimal spanning tree. As pointed out by one anonymous referee, the idea of converting spatial information to path information in that paper is actually shared in our work since our method uses DFS to translate the graph structure information into path information over a linear chain.====The rest of the paper is organized as follows. We describe the proposed methodology in Section 2 and discuss an efficient posterior computational algorithm in Section 3. We then present the theoretical results in Section 4, and demonstrate the excellent empirical performance of our method via simulations and real data examples in Sections 5 Simulation studies, 6 Real data application, respectively. We discuss several future work directions in Section 7. Proof of the main theorem and associated lemmas, and additional numerical results are given in Appendix.",Graph signal denoising using ,https://www.sciencedirect.com/science/article/pii/S037837582100135X,5 January 2022,2022,Research Article,26.0
"Fasen-Hartmann Vicky,Mayer Celeste","Institute of Stochastics, Englerstraße 2, D-76131 Karlsruhe, Germany","Received 11 May 2021, Revised 13 November 2021, Accepted 29 December 2021, Available online 4 January 2022, Version of Record 20 January 2022.",https://doi.org/10.1016/j.jspi.2021.12.011,Cited by (0),"In this paper, we investigate estimators for ====-stable CARMA processes sampled at low frequencies is not consistent and highlight why simulation studies suggest something else. Thus, in contrast to the light-tailed setting the properties of the Whittle estimator for heavy-tailed ARMA processes cannot be transferred to heavy-tailed CARMA processes. We elaborate as well that the estimator presented in Garcia et al. (2011) faces the same problems. However, the Whittle estimator for stable CAR(1) processes is consistent.","Continuous-time ARMA (CARMA) processes are the continuous-time versions of the well-known ARMA processes in discrete time. Since CARMA processes with finite second moments sampled equidistantly are weak ARMA processes, several methods to estimate the parameters of ARMA processes as, e.g., quasi-maximum likelihood estimators or Whittle estimators, can be utilized to estimate the parameters of CARMA processes. A challenge is that the distribution of the white noise of the weak ARMA representation is highly dependent on the model parameters of the CARMA process and it is only a weak white noise instead of a strong white noise. Moreover, some attention has to be paid to guarantee the model identifiability. These generalizations are technical and take some effort.  Schlemm and Stelzer (2012) derived a rigorous theory for quasi maximum-likelihood estimation for equidistantly sampled multivariate CARMA processes with finite second moments; see as well  Brockwell and Lindner (2019) and Brockwell et al. (2011) for CARMA processes. The method itself is much older and applied in several disciplines, in particular, in economics, even though the statistical inference of that estimator was as far as we know not investigated earlier. Since this procedure essentially depends on the variance of the sampled white noise, it is unsuitable for estimating the parameters of heavy-tailed CARMA processes. The consistency and the asymptotic normality of the Whittle estimator for multivariate CARMA processes with finite second moments were recently derived in Fasen-Hartmann and Mayer (2021a). The limit covariance matrices of these estimators are slightly different to those of the corresponding ARMA estimators. Apart from that, the structure of the estimator is the same taking the identifiability assumptions into account.====The topic of this paper is the estimation of ====-stable CARMA processes with infinite second moments sampled at low frequencies. To the best of our knowledge there exists no rigorous theory for estimators for heavy-tailed CARMA processes in the literature yet. Only stable Ornstein–Uhlenbeck processes, which correspond to the class of CARMA(1,0) processes, are investigated, e.g., in Hu and Long, 2007, Hu and Long, 2009, Fasen (2013a) and Ljungdahl and Podolskij (2021).  García et al. (2011) proposed an indirect quasi-maximum likelihood method for stable CARMA processes and used it for the estimation of electricity spot prices; this attempt was further explored and applied in Müller and Seibert (2019) and Benth et al. (2014) to futures pricing in electricity markets. Their simulation study suggests that the estimator is converging when the number of observations tends to infinity, however they do not present a mathematical analysis of their estimator. In this paper, we mainly investigate the Whittle estimator in more detail. The Whittle estimator was first introduced by Whittle (1953) to estimate the parameters of Gaussian ARMA processes and further explored in Hannan (1973), see the monograph of Brockwell and Davis (1991). The more general case of Whittle estimation of multivariate ARMA processes with finite second moments was topic of Dunsmuir and Hannan (1976). Mikosch et al. (1995) observed that the same method also works for symmetric ====-stable ARMA processes with infinite second moments. This is not apparent since the spectral density is not defined for processes with infinite second moments and the Whittle estimator is based on its empirical version, the periodogram.  Mikosch et al. (1995) derived the consistency and the convergence of the properly normalized and standardized Whittle estimator to a functional of stable random variables. Therefore, Whittle estimation is an estimation method for both heavy-tailed and light-tailed ARMA processes. The estimator is consistent in both settings which is a very important property. Indeed, the convergence rate of the Whittle estimator is ==== in the stable case which is even faster than in the case with finite second moments, where it is ====. It would thus be desirable to have the same for heavy-tailed and light-tailed CARMA processes.====Similarly, for heavy-tailed fractional ARIMA processes, which exhibit long range dependence, the Whittle estimator is consistent and the asymptotic behaviour is known (see Kokoszka and Taqqu (1996)). The Whittle estimator was also used for parameter estimation of GARCH processes in Giraitis and Robinson (2001) and Mikosch and Straumann (2002). For GARCH(1,1) processes Mikosch and Straumann (2002) pointed out that the Whittle estimator is consistent as long as the 4th moment is finite and inconsistent when the 4th moment is infinite. This is a very interesting statement to which we come back later.====Therefore, the behaviour of the Whittle estimator for heavy-tailed ARMA models and the fact that the Whittle estimator also works for light-tailed CARMA processes (see Fasen-Hartmann and Mayer (2021a)) might suggest that the Whittle estimator is suitable for parameter estimation of heavy-tailed CARMA processes. In particular, there exist simulation studies which confirm this idea. However, the main statement of the present paper is that the Whittle estimator for ====-stable CARMA processes is in general not a consistent estimator, even though there are simulation experiments which suggest something else. There is also evidence that the estimator of García et al. (2011) is not consistent as well. We elaborate that in more detail and present several arguments for our conjecture.====The paper is structured in the following way. We start with an introduction about ====-stable CARMA processes in Section 2 and present some basic facts on Whittle estimation for CARMA processes with finite second moments in Section 3 which motivate our approach. Then, in Section 4, we show the convergence of the Whittle function for ====-stable CARMA processes and deduce that the Whittle estimator is not consistent for general ====-stable CARMA processes. However, we show that the Whittle estimator is a consistent estimator for ====-stable CAR(1) processes. Finally, in Section 5, we demonstrate the theoretical results through a simulation study where we compare the performance of our estimator with the estimator proposed in García et al. (2011). The behaviour of the Whittle estimator and the behaviour of the estimator of García et al. (2011) are very similar in our simulation study. In particular, in the simulation setup of García et al. (2011) the Whittle estimator performs excellent and leads to the presumption that the Whittle estimator is converging. Conclusions are given in Section 6. Some auxiliary results on the asymptotic behaviour of the sample autocovariance function of ====-stable CARMA processes are postponed to the Appendix.",A note on estimation of ,https://www.sciencedirect.com/science/article/pii/S0378375821001361,4 January 2022,2022,Research Article,27.0
"She Rui,Mi Zichuan,Ling Shiqing","Center of Statistical Research, School of Statistics, Southwestern University of Finance and Economics, Chengdu, China,Shanxi University of Finance and Economics, China,Department of Mathematics, Hong Kong University of Science and Technology, HongKong, China","Received 13 August 2020, Revised 4 October 2021, Accepted 3 December 2021, Available online 24 December 2021, Version of Record 1 January 2022.",https://doi.org/10.1016/j.jspi.2021.12.003,Cited by (0),"This paper studies the Whittle estimation for vector autoregressive and moving average (ARMA) models with heavy-tailed noises. It is shown that the Whittle estimator is consistent with the rate of convergence ==== and its limiting distribution is a function of two stable random vectors, where ==== is a slowly varying function and ","The heavy-tailed phenomena have been well observed in financial market, engineering, network system and other areas, see Resnick (1997) for a very nice review.  Davis and Resnick, 1985, Davis and Resnick, 1986 for the first time showed that the limiting distribution of the least squares estimator (LSE) of parameters in heavy-tailed autoregressive (AR) models is the functional of two stable random variables. Davis et al. (1992) established the asymptotic theory of the least absolutely deviation (LAD) estimator and M-estimator for heavy-tailed AR models, see Andrews et al. (2009) for its MLE.  Mikosch et al. (1995) developed the asymptotic theory of the Whittle estimator for one dimensional heavy-tailed ARMA models and Kokoszka and Taqqu (1996) extended this theory for heavy-tailed ARFIMA models.  Davis and Mikosch (1998) established the asymptotic theory of the sample autocorrelations of heavy-tailed ARCH processes.  Andrews and Davis (2013) considered the model identification for infinite variance AR models. Zhang and Ling (2015) established the limiting distribution of the LSE in AR models with the heavy-tailed GARCH noises.  Davis et al. (2016) studied the sample covariance matrix of a vector heavy-tailed time series. Akashi et al. (2015) studied the empirical likelihood ratio statistic for ====-stable linear processes. She and Ling (2020) studied the LSE of the heavy-tailed vector error correction models.====However, the asymptotic theory of the estimated parameters in vector ARMA models with heavy-tailed noises is not available, yet. This paper studies the Whittle estimation for vector ARMA models with heavy-tailed noises. It is shown that the Whittle estimator is consistent with the rate of convergence ==== and its limiting distribution is a function of two stable random vectors, where ==== is a slowly varying function and ==== is the tail index . A simulation study is carried out to assess the performance of this estimator in finite samples and a real example is given. This paper includes several limiting theorems for the general heavy-tailed vector processes, which is independent of interest.====The rest of this paper is arranged as follows. Section 2 gives several limiting theorems of vector heavy-tailed time series. Section 3 establishes the asymptotic theory of the Whittle estimation. Simulation results are reported in Section 4 and a real example is given in Section 5. The proofs of the results in Section 3 and Section 2 are given in Section 6 and Appendix, respectively.",Whittle parameter estimation for vector ARMA models with heavy-tailed noises,https://www.sciencedirect.com/science/article/pii/S0378375821001257,24 December 2021,2021,Research Article,28.0
"Angkunsit Annop,Suntornchost Jiraphan","Department of Mathematics and Computer Science, Faculty of Science, Chulalongkorn University, Bangkok 10330, Thailand","Received 15 August 2020, Revised 14 December 2021, Accepted 14 December 2021, Available online 24 December 2021, Version of Record 11 January 2022.",https://doi.org/10.1016/j.jspi.2021.12.010,Cited by (1), and perform numerical results to investigate performances of the proposed estimates.,"Small area estimation is one of well-known statistical methods to estimate parameters when sample size is not large enough to provide reliable direct survey estimates. The concept of the small area estimation is to borrow strength from available information through models, called model-based approach (Rao and Molina, 2015). In small area estimation context, the Fay–Herriot model (Fay and Herriot, 1979) is widely used model. The Fay–Herriot model consists of two models: sampling model and linking model. For the sampling model, ==== are independently and normally distributed with mean ==== and known variance ====. The sampling model is used to account for the sampling distribution of the direct estimate ====. For the linking model, ==== are independently and normally distributed with mean ==== and variance ====. The linking model links the true small area means ==== to a vector of ==== known auxiliary variables ====, obtained from administrative and census data. The parameters ==== and ==== are unknown and are estimated from the available data. The Fay–Herriot model can be viewed as the following model: ====where ==== and ==== are mutually independent, ==== and ====.====Under model (1), the traditional estimators of the small area means ==== is the best linear unbiased prediction (BLUP) estimator of ====. The BLUP estimator ====, which is defined by minimizing mean squared error (MSE), is given by ====where ==== and ==== is the weighted least square estimator of ==== with ====.====The parameter ==== is usually unknown but is estimated by an estimator ====. Substituting ==== into (2), we obtain the empirical best linear unbiased prediction (EBLUP) estimator as ====where ==== and ==== with ====.====The mean squared error (MSE) is the measurement of the uncertainty of the EBLUP estimator. The MSE of EBLUP estimator ==== is given by ====where ====, ====, ====
 ====, and ==== is the asymptotic variance of ====  (Yoshimori and Lahiri, 2014). Note that, the MSE of EBLUP involves unknown ====. In practice, we need the estimation of MSE of EBLUP, which is denoted as ====. Then we obtain the following second-order unbiased MSE estimator of EBLUP: ====where ==== is a second-order unbiased estimator of ====.====The consistent estimator ==== denotes an even-translation-invariant estimator for all ==== and ==== that achieve unbiasedness in the EBLUP, ==== and ====, as in Kackar and Harville (1981). Such conditions are satisfied by common methods of the variance parameter ====: Prasad–Rao simple method-of-moments (Prasad and Rao, 1990), Fay–Herriot method-of-moments (Fay and Herriot, 1979), profile maximum likelihood method (Hartley and Rao, 1967) and residual maximum likelihood method (Patterson and Thompson, 1971). Even though, the four methods have been widely used in applications, there is a common problem among them where they produce zero estimate of ====, when ==== is small relative to the sampling variance ==== and the sample size ==== is small. This causes zero estimates of the weight of the direct estimates and then EBLUP reduces to regression-synthetic estimator which is undesirable.====Therefore, parameter estimation methods have been introduced in literature to avoid the zero estimate of variance component ====. The concept was first introduced in Lahiri and Li (2009) where they produced a strictly positively consistent estimator of variance component for a general linear mixed model. Li and Lahiri (2010) proposed the adjusted maximum likelihood (AML.LL) method, including adjusted profile maximum likelihood method and adjusted residual maximum likelihood method. These adjustments prevent the zero estimate of variance component ====. Later on, Yoshimori and Lahiri (2014) developed an adjusted maximum likelihood method, which produces strictly positive estimators which are closer to the standard maximum likelihood estimators (profile or residual maximum likelihood estimators).====Extensions of the Fay–Herriot model to multivariate Fay–Herriot models (Datta et al., 1991, Fay, 1987) have brought interests from researchers worldwide due to availability of data and technologies and the flexibility in allowing correlations between different variables of interest. For example, Datta et al. (1996) apply a multivariate Fay–Herriot model to obtain hierarchical Bayes estimates of the median income of four-person families for the US states; Gonzàlez-Menteiga et al. (2008) studied a class of multivariate Fay–Herriot model with a common random effect for all the components of the vector of the variable of interest; and Benavent and Morales (2016) studied a class of multivariate Fay–Herriot model with one random effect per components of the vector of the variable of interest for different covariance patterns between the components of the vector of random effects.====For the multivariate Fay–Herriot model, when some components of the covariance matrix of random effects are zeros, the EBLUP reduces to regression-synthetic estimator for such component. To avoid zero estimate of variance components in bivariate Fay–Herriot model, Angkunsit and Suntornchost (2021) extended the adjusted maximum likelihood method for bivariate Fay–Herriot models to produce a positive valued estimator of variance component. However, they considered the bivariate Fay–Herriot model when the variances of random effects for the two components are the same. In this study, we will extend the methods proposed in Li and Lahiri (2010) and Yoshimori and Lahiri (2014) to obtain an adjusted maximum likelihood method for a more general multivariate Fay–Herriot model when the variances of random effects for different components could be different.====The organization of this paper is as follows. Section 2 gives a short review of the multivariate Fay–Herriot model. Section 3 discusses the proposed adjusted maximum likelihood method for multivariate Fay–Herriot models. Section 4 presents properties of the adjusted maximum likelihood estimates. Section 5 shows simulation results. Section 6 shows results from Thai Household Socio-Economic Survey data and Section 7 gives conclusions.",Adjusted maximum likelihood method for multivariate Fay–Herriot model,https://www.sciencedirect.com/science/article/pii/S0378375821001348,24 December 2021,2021,Research Article,29.0
"Geng Pei,Koul Hira L.","Department of Mathematics, Illinois State University, Normal, IL, 61790, United States of America,Department of Statistics and Probability, Michigan State University, East Lansing, MI, 48824, United States of America","Received 10 September 2020, Revised 7 December 2021, Accepted 9 December 2021, Available online 17 December 2021, Version of Record 31 December 2021.",https://doi.org/10.1016/j.jspi.2021.12.007,Cited by (0),"We develop analogs of a class of weighted empirical minimum distance estimators of the underlying parameters in errors-in-variables linear regression models, when the ==== distribution and the conditional distribution of conditionally centered measurement error, given the surrogate, are symmetric around the origin. This class of estimators is defined as the minimizers of integrals of the square of a certain symmetrized weighted empirical process of the residuals. It includes the least absolute deviation (LAD) and an analog of the Hodges–Lehmann (H–L) estimators. In this paper we first develop this class of estimators when the distributions of the true ","Donoho and Liu, 1988a, Donoho and Liu, 1988b argue that in the one and two sample location models the minimum distance estimators based on ==== distances involving residual empirical distribution functions have some desirable finite sample properties and tend to be automatically robust against some contaminated models. In regression models without measurement error in the covariates, analogs of these estimators of the underlying regression parameters based on certain weighted residual empirical processes were developed in Koul, 1979, Koul, 1985, Koul, 1996. These estimators include least absolute deviation (LAD), analogs of Hodges–Lehmann (H–L) estimators and several other estimators that are robust against outliers in regression errors and asymptotically efficient at some error distributions.====There are numerous practical situations where covariates are not accurately observed. Instead one observes their surrogates with additive errors. The regression models with such covariates are known as the errors-in-variables (EIVs) regression models. Fuller (1987), Cheng and Van Ness (1999), and Carroll et al. (2006) discuss numerous practical examples of these models. In the linear EIVs models, Stefanski (1985) developed the bias corrected least square estimator based on M-estimation, given the measurement error variance is known. Cook and Stefanski (1994) constructed a simulation-based estimation method for parametric measurement error models which is asymptotically equivalent to method-of-moments estimation in linear EIVs modeling. Buonaccorsi (2010) summarizes the moment-based bias corrected estimation for different settings in linear EIVs models. When the covariate is univariate, Al-Sharadqah (2018) proposed an adjusted maximum likelihood estimator for Gaussian measurement error when the variance ratio of regression error and measurement error is known.====Given the importance of the EIVs regression models and the above mentioned properties of the above minimum distance (m.d.) estimators, it is desirable to develop their analogs for the EIVs regression models. Section 2 describes the m.d.==== ==== ====estimators of interest in the EIVs linear regression model and the assumptions needed for their asymptotic normality when distributions of the true covariate vector ==== and the measurement error vector ==== are known. It also includes a discussion of these assumptions for some important cases, an example of distributions of ==== and ==== that satisfy the needed assumptions, and a discussion of the Pitman’s asymptotic relative efficiency (ARE) of some members of the proposed class of estimators. The ARE of the LAD and the analog of H–L estimators, relative to the bias corrected least squares (BCLS) estimator, is seen to increase to infinity as the measurement error variances of the components of ==== tend to infinity, when ==== and the regression error are Gaussian r.v.’s. See Remark 2.3 for the case when the distribution of ==== is known but that of ==== is unknown.====Section 3 discusses the m.d.==== ==== ====estimators under the setting that the distributions of ==== are unknown but validation data is available. A real data application with validation data is included in Section 5. Some proofs are deferred to Appendix.====Section 4 presents the findings of a simulation study that assesses the performance of the empirical bias and root mean squared error (RMSE) of the two members of the proposed class of m.d.==== ==== ====estimators, viz, the analog of H–L and LAD estimators, calibrated least squares (LS) and BCLS estimators. In these simulations, ==== is one dimensional, the regression error distributions are Gaussian(0,1), Laplace(0,1) and the ====-distribution with 2 degree of freedom denoted by ====. To assess the effect of the measurement error ==== on these estimators, we used several values of the measurement error variance ====. Table 1, Table 2, Table 3 and Table 4, Table 5, Table 6, Table 7, Table 8, Table 9 report the findings when the distributions of ==== and ==== are known and unknown but validation data is available, respectively.====The analog of H–L and LAD estimators is seen to be relatively much more stable with controlled empirical bias and much smaller RMSE for the larger values of ==== and for all chosen regression error distributions. The analog of the H–L estimator is relatively more stable than the other three estimators when the regression error distribution is ====. In comparison, RMSE of BCLS estimator is seen to be much larger than that of the other three estimators for the chosen larger values of ==== and error distributions. For more on simulations see Section 4.",Weighted empirical minimum distance estimators in linear errors-in-variables regression models,https://www.sciencedirect.com/science/article/pii/S0378375821001312,17 December 2021,2021,Research Article,30.0
"Liu Shu,You Jinhong,Hu Lixia","School of Statistics and Information, Shanghai University of International Business and Economics, Shanghai, China,School of Statistics and Mathematics, Interdisciplinary Research Institute of Data Science, Shanghai Lixin University of Accounting and Finance, Shanghai, China","Received 24 November 2020, Revised 25 July 2021, Accepted 10 December 2021, Available online 17 December 2021, Version of Record 31 December 2021.",https://doi.org/10.1016/j.jspi.2021.12.009,Cited by (0),"Studying massive functional/longitudinal data, we adopt a flexible nonlinear dynamic regression method named the Semi-Varying Coefficient ====, in which the response can be a functional/longitudinal variable, and the ==== can be a mixture of functional/longitudinal and scalar variables. With the aid of an initial B-spline approximation, a local linear smoothing is proposed to estimate the unknown functional effects in the model. Existing methods of ","Due to advances in modern computational technology, data are increasingly recorded continuously over an interval of time (or spatial location, wavelength, etc.) or intermittently at several discrete points in time. As a result, datasets in which each individual has multiple observations are becoming more and more common in almost all scientific, societal and economic fields. An obvious recent example of this kind is COVID-19 data: Daily confirmed diagnoses, death tolls and suspected cases for different countries are recorded and made publicly available. When a variable is measured or observed at different times, it is usually treated as a function of time. As a result, the variable is called a functional variable, the data for the variable are called functional data and the related statistical analysis is called functional data analysis (FDA) (Wang et al., 2016). Functional data and corresponding FDA have been successfully used to explore interactions and co-movements within groups of temporally evolving subjects. Several studies provide comprehensive discussions on the methods and applications, for example Ramsay and Silverman, 2007, Ferraty and Vieu, 2006. More recent work about FDA includes Wang et al. (2016).====According to Wang et al. (2016), functional data can typically be divided into two cases: sparse and dense. Sparse functional data usually occurs in longitudinal studies where subjects are measured at different time points and the number of measurements for each subject is often bounded away from infinity. Conversely, in dense functional data the number of measurements of each subject tends towards infinity. In theory, the difference between sparse (longitudinal) and dense functional data is clear. However, due to the human limitations, the observations in real datasets cannot be infinite and are always finite. Therefore, in practice the boundary between sparse (longitudinal) and dense functional data is vague in some scenarios, especially when the number of measurements of each subject is of an intermediate level or different subjects have different numbers of measurements.====In many functional/longitudinal studies, repeated measurements within each subject are possibly correlated with each other, but different subjects can be supposed to be independent. One approach to take into account intra-subject variation is the mixed-effects model developed by Wu and Zhang (2002), which decomposes a regression function into a fixed population mean and a subject-specific random trajectory with zero mean. For sparse and dense functional data, Kim and Zhao (2013) considered a mixed-effects nonparametric regression model without covariates, and showed that the asymptotic distributions of kernel estimators are different for the two data types. Therefore, a subjective choice between sparse and dense cases may lead to erroneous conclusions. To avoid this problem, they proposed a self-normalized method, able to deal with sparse and dense longitudinal data in a unified framework. Furthermore, Chen and Yao (2017) generalized the results of Kim and Zhao (2013) to a mixed-effects varying-coefficient model (VCM) that incorporates covariates with sparse or dense longitudinal data. Recently, Zhang and Wang (2016) have provided a comprehensive perspective that deals with a general weighing scheme on a unified platform for all types of sampling plan, including sparse, dense and ultra dense data. Motivated by a monotonic relationship between age and volume of gray matter in the older population, Chen et al. (2019) considered sparse and dense cases on a unified framework under monotone constraint of the mean function. The research work of Zhang and Wang (2016) and Chen et al. (2019) has focused on statistical inferences about the mean function of the underlying process. To the best of our knowledge, there exists no further development of unified inference comparable to Zhang and Wang (2016) for nonparametric regression models with the presence of covariates, despite the fact that this is a common case in practice.====In the analysis of longitudinal data, varying-coefficient models (VCM), which enjoy flexibility, parsimony and interpretability, are widely used as a method of nonparametric regression (Hastie and Tibshirani, 1993, Fan and Zhang, 2000, Huang et al., 2002, Morris and Carroll, 2006, Şentürk and Nguyen, 2011). Another popular nonparametric regression method is the additive model (AM) (Breiman and Friedman, 1985, Qi and Luo, 2018, Scheipl et al., 2015, Xue et al., 2010). Recently, Zhang and Wang (2015) and Hu et al., 2019, Hu et al., 2021 have investigated a novel nonparametric regression method named the varying-coefficient additive model (VCAM), which can be viewed as a generalization of VCM and AM, exemplified as follows. Let ==== be the observation time when the ====th measurement of subject ==== is made, let ==== and ==== be the response and ====-covariates for subject ==== at time ====, respectively. Then ==== constitutes a longitudinal/functional sample from ==== randomly selected subjects with ==== repeated measurements of the ====th subject. The VCAM for longitudinal/functional data proposed by Hu et al. (2021) is as follows: ====with abuse of notations. Here ==== is the subject-specific random trajectory at observation time ====, and ==== are independent and identically distributed (i.i.d.) random measurement errors.====The multiplicative factors ====
 (====) and ====
 (====) are a varying-coefficient component function and additive component function, respectively, and ==== is a trend term. Obviously, the VCAM (1) reduces to an AM provided that each ====
 (====) is time-invariant, and it becomes a VCM if each ====
 (====) has a simple linear form. Therefore, it can be said that the VCAM is a kind of hybrid of an AM and a VCM, enjoying more flexibility than either, and which can greatly decrease the bias caused by model misspecification. It is hard to address how to choose between an AM and a VCM in practice, and the general type of VCAM provides a data-driven method to decide which model may be more suitable for the real-life data at hand.====However, in many practical applications, there may exist a dynamic linear relationship for some covariates and dynamic nonlinear relationship for other predictors. Taking a parsimonious approach, in this paper we consider a mixed-effects semi-varying coefficient additive model (Semi-VCAM) to analyze longitudinal/functional data. Let ==== be a ====-vector of covariates observed at time ====, and the vector of varying-coefficient functions for ==== is denoted as ==== be the vector of varying-coefficient functions for ==== that is i.i.d. with ====, where ==== denotes the trend function. Then, a Semi-VCAM is given as follows: ====where the subject-specific random trajectory ==== satisfies ==== and covariance function ====, ==== are random errors such that ==== and ====, and ==== is a smooth standard deviation function of process ====. Note that (2) allows a mixture of functional/longitudinal predictors and scalar covariates, and that it reduces to a partial linear additive model (PLAM) if each varying-coefficient function is time-invariant, and to a VCM if each additive function has linear form. Compared with model (1), studied by Hu et al. (2021), Semi-VCAM (2) allows heteroscedasticity in the time elapsed and takes into account intra-subject correlation, which was merged into random errors in Hu et al. (2021).====In function-on-function regression, an interesting and popular modeling strategy is to use concurrent models, in which the response at a specific point is modeled as a function of the value of the covariate only at that specific point. Ramsay and Silverman (2005) provide a detailed exposition of the linear functional concurrent model and its fitting procedures, Maity (2017) considers an additive functional concurrent model to seek more flexibility in modeling and prediction. The newly-proposed Semi-VCAM (2) can be viewed as a generalization of a linear functional concurrent model in the sense that (2) allows nonlinear covariates effects.====As a global smoothing technique, the spline method is widely used to fit smooth nonparametric functions because of its cost saving benefits. However, it usually has no asymptotic distribution due to the absence of decomposition of the bias part and variance part, unless the asymptotic bias is of lower order than the asymptotic variance. All existing research literature on VCAM is based on spline methods. Zhang and Wang (2015) provides no asymptotic distributions of estimators, whereas Hu et al. (2019) and Hu et al. (2021) obtain asymptotic distributions under the condition that the asymptotic bias can be ignored. Alternatively, the kernel method is a local smoothing tool, based on which we can construct the involved asymptotic distribution with presence of asymptotic bias, and make statistical inferences on certain interested functions. In particular, local linear smoothing is popular due to its properties such as design adaption, good boundary performance and statistical efficiency in an asymptotic minimax sense. See Fan and Gijbels (1996) for more details.====In this paper, we build a pilot estimation based local linear estimator (PEBLLE) for varying-coefficient component functions and additive component functions. The proposed estimation method has wide applicability, including sparse data and dense data, and the presence of functional/longitudinal covariates and scalar variables. We have shown the consistency of PEBLLE, and as a major contribution of this paper, we construct the asymptotic distributions on a unified framework for sparse, dense and ultra dense data. Most importantly, we obtain the oracle property of the proposed iterated estimation methodology as if we have additional information about the component functions. For convenience and concise presentation, we assign the same weight to each subject (SUBJ), and our theoretical results can be viewed as a generalization of the model of Zhang and Wang (2016) to a nonparametric regression model with presence of covariates and SUBJ weight scheme.====In the empirical studies, we consider the novel coronavirus disease (COVID-19) that broke out in December 2019, and apply our method to analyze the growth rate of cumulative confirmed cases (GRCCC) in China (excluding Hubei Province, Tibet, Macao, Taiwan and Hong Kong). We collect data from ====, and take as our sample period January 22nd, 2020 to April 8th, 2020. To model GRCCC, four functional covariates and one scalar covariate (population size) are chosen.====The remainder of this paper is organized as follows. Section 2 proposes a pilot estimation based local linear smoothing method and Section 3 presents a series of asymptotic theories. Section 4 discusses the implementation of the proposed method. Section 5 considers extensive simulation studies investigating the finite-sample performance and real data applications of our methodologies. Section 6 concludes the paper. In Appendix A, we present the requirements for validity of the asymptotic theories. The technical proofs and some complementary numerical studies can be found in the supplementary materials.",Unified statistical inference for a nonlinear dynamic functional/longitudinal data model,https://www.sciencedirect.com/science/article/pii/S0378375821001336,17 December 2021,2021,Research Article,31.0
Cavicchioli Maddalena,"Department of Economics, University of Modena and Reggio E., Italy","Received 11 February 2020, Revised 9 December 2021, Accepted 10 December 2021, Available online 17 December 2021, Version of Record 31 December 2021.",https://doi.org/10.1016/j.jspi.2021.12.008,Cited by (4),"We consider a vector autoregressive (VAR) model subject to Markov Switching and present results for testing the fit of such a model and its capability to parametrize appropriately the ==== of the observed multivariate process. The proposed method is based on a linear VARMA representation of the Markov Switching VAR model, from which we derive a close-form formula for its ====, the goodness-of-fit tests and the feasibility of the approach.","In the present paper we consider multivariate autoregressive models whose parameters can change as a result of a regime-shift variable, described as the outcome of an unobserved Markov chain. Since the seminal papers by Hamilton, 1989, Hamilton, 1990, Markov switching (MS) models have attracted much interest in the literature since they play an important role in many economic, statistical and financial studies, and constitute a useful method to model uncertainty preserving the tractability of linear framework. As general references see, for example, Hamilton, 1994, Hamilton, 2005, Krolzig (1997), and Guidolin (2012).====The achievements of the Markov switching models in fitting empirical data have been confirmed in many economic studies. For this reason, it is important to characterize the theoretical properties of these models. For information concerning maximum likelihood (ML) estimation, consistency, statistical inference and model selection of Markov switching (MS) vector autoregressive (VAR) models see Hamilton, 1989, Hamilton, 1990, Hamilton, 1994, Krolzig (1997), Yang (2000), Stelzer (2009) and Cavicchioli (2014a). Explicit matrix expressions for the ML estimator of the parameters in MS VAR models have been derived in Cavicchioli (2014b). Higher moments for a range of univariate MS autoregressive models were studied by Timmermann (2000). Higher-order moments and asymptotic Fisher information matrix of MS VARMA models are provided in Cavicchioli, 2017a, Cavicchioli, 2017b, respectively. Stationarity conditions and the autocovariance structure for MS VARMA models were derived by Yang (2000), Francq and Zakoïan (2001), and Zhang and Stine (2001). The autocovariance structure of powers of MS ARMA models was characterized by Francq and Zakoïan (2002). Guidolin and Ria (2011) examined how the presence of regimes in means, variances, and correlations of asset returns translates into explicit dynamics of the Markowitz mean–variance frontier.====Another interesting question arising in this context is to investigate the state dimension of the MS process. The current methods for determining the number of regimes are based either on complexity-penalized likelihood criteria (see, for example, Psaradakis and Spagnolo, 2003, Olteanu and Rynkiewicz, 2007, Ríos and Rodríguez, 2008, Awirothananon and Cheung, 2009) or on finite order stable VARMA representations of the initial switching models (see, for example Krolzig, 1997, Zhang and Stine, 2001, Francq and Zakoïan, 2001, Cavicchioli, 2014a). The parameters of the VARMA representations can be determined by evaluating the autocovariance function of the MS models. It turns out that the above parameters are elementary functions of the dimension of the dynamic process, the number of regimes and the orders of the switching autoregressive moving-average model. As the sample autocovariances are more easily calculated than maximum (penalized) likelihood estimates of the model parameters, the bounds arising from the above-mentioned elementary functions are very useful for selecting simultaneously the number of regimes and the orders of the switching moving-average autoregression. Some bounds are previously determined by Krolzig (1997), Zhang and Stine (2001), Francq and Zakoïan (2001), and Cavicchioli (2014a) for some Markov regime switching models of different type.====Furthermore, in time series analysis, the primary interest is often to study the periodic behavior of the data and a useful tool is the Fourier transform. The frequency content of a time series can be analyzed through the spectral density function, which results from its autocovariance function values. Tractable methods to derive the spectral representations of general classes of MS VARMA models have been proposed by Pataracchia (2011), Cavicchioli (2013), and Cheng (2016).====In the present paper, we derive a new spectral representation of a MS VAR process, and give a close-form formula for its spectral density matrix. The procedure is based on a linear VARMA representation of the given MS VAR model, which extends and completes some results obtained in Krolzig (1997). Then we evaluate the closeness of the sample spectral density matrix of the observed process to the spectral density matrix of the fitted model. This allows us to derive goodness-of-fit tests for MS VAR models based on spectral analysis, which generalize in some cases and complete several results known in the literature for linear VARMA models. Here we mention some of them. Ioannidis (2007) provides a characterization of the range of spectral matrices, which are feasible for linear bivariate VAR models. This, in turn, is used to construct a visual goodness-of-fit criterion for fitting such models, yielding also indications on the necessary autoregressive orders. Velilla (1994) proposes a goodness-of-fit test for univariate autoregressive moving-average (ARMA) models based on the standardized sample spectral distribution of the residuals. A goodness-of-fit test for linear VARMA models based on a modified residual correlation matrix sequence has been presented in Velilla and Thu (2018). This generalizes to a multivariate setting the approach of Ubierna and Velilla (2007) for univariate ARMA models. A different procedure for testing the fit of a linear VARMA model has been proposed by Paparoditis (2005) (for the univariate case see Paparoditis, 2000). The method evaluates in a certain way the closeness of the sample spectral density matrix of the observed process to the spectral density matrix of the parametric model, and uses for this purpose nonparametric estimation techniques. We take advantage from these nice papers to construct our goodness-of-fit diagnostics for the class of MS VAR models.====The rest of the paper is organized as follows. In Section 2 we introduce the model and present different state space representations of it. In Section 3 we derive a well-specified linear VARMA representation of the considered MS VAR model, and show that the autoregressive and moving-average orders of such a linear VARMA are elementary functions of the number of regimes and the autoregressive order of the initial MS VAR. This is useful for model selection. In Section 4 we obtain a new spectral representation of a MS VAR process, and give a close-form formula for its spectral density matrix. The procedure is based on the obtained linear VARMA representation of MS VAR models. Under standard assumptions, we show that the matrix parameters of the VARMA representation one-to-one correspond to those of the initial MS VAR model. Section 5 introduces the proposed goodness-of-fit tests for MS VAR models. Some useful visual goodness-of-fit diagnostics supporting the interpretability of the test results are developed in Section 6. Some practical aspects and applications to real-life data are discussed in Section 7. Section 8 concludes. All technical proofs are deferred to Appendix.",Goodness-of-fit tests for Markov Switching VAR models using spectral analysis,https://www.sciencedirect.com/science/article/pii/S0378375821001324,17 December 2021,2021,Research Article,32.0
"Wang Chunyan,Mee Robert W.","School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China,Department of Business Analytics and Statistics, University of Tennessee, Knoxville, TN 37996, USA","Received 11 June 2021, Accepted 7 December 2021, Available online 17 December 2021, Version of Record 31 December 2021.",https://doi.org/10.1016/j.jspi.2021.12.006,Cited by (4),"Van Nostrand’s pairwise order model has been the most popular model for analyzing order-of-addition experiments. Here we construct designs of minimal size for the pairwise order model, considering both ====-optimality, as well as the prediction-based ====- and ====-optimal designs, which may have fewer runs than the number of parameters for the pairwise order model. These supersaturated designs may be used as screening designs for applications where the number of components is large.","Five years ago, literature on order-of-addition experimentation was limited to Van Nostrand (1995), who introduced the pairwise order (PWO) model and explored design construction based on using subsets of treatment combinations from regular, two-level, fractional factorial designs. Recent improvements to design construction for the PWO model include order-of-addition orthogonal arrays (OofA-OA) (Voelkel, 2019) and a bubble-sort algorithm for constructing ====-efficient designs of any size (Lin and Peng (2019)). Peng et al. (2019) proved that OofA-OAs are optimal under numerous optimality criteria, including ====-, ====-, ====-, and any other criterion that is concave and signed permutation invariant. Schoen and Mee (2021) proved OofA-OAs are also ====-optimal for the PWO model. Peng et al. (2019) also furnished a class of OofA-OAs for any even number of components ====, with run size ====. Chen et al. (2020) proposed a method for constructing an OofA-OA for ==== components based on OofA-OAs for ==== and ==== components, respectively, that have the same run size. For 7 and 8 (9 and 10) components, the run sizes of their OofA-OAs are ====
 ==== 168 (432). These are the only known closed-form constructions for OofA-OAs, but they produce designs that are impractically large. Schoen and Mee (2021) carefully defined isomorphism classes for order-of-addition designs and enumerated all nonisomorphic OofA-OAs of size ====
 ==== 12 (====) and ====
 ==== 24 (====); previous work had been unable to find any 24-run OofA-OA at ====. The Lin and Peng (2019) bubblesort-exchange algorithm can be used to generate designs for very large ====, since it does not require storing a candidate set of points; they illustrated the success of their algorithm for ====. However, they admit that a standard Federov exchange algorithm will return better results than their bubblesort-exchange algorithm for small ====. Winker et al. (2020) proposed an alternative search algorithm, but the D-efficiencies they report are inferior to those in Lin and Peng (2019) for ====. Finally, Anderson-Cook and Lu (2019) recommended the use of ==== of size ==== (or ====) based on Latin squares (Graeco-Latin squares) not to estimate a model but as a preliminary design to assess the impact treatment order can have on the response.====Generalizations to the PWO model include PWO models with interactions, e.g., the triplets model (Mee, 2020) and the PWO model with tapering (Peng et al., 2019). The recently proposed component-position (CP) model (Yang et al., 2020) is an entirely different approach to modeling order-of-addition responses, with categorical effects based on the position of each component. Their model requires ====. Anderson-Cook and Lu (2019) suggested a more parsimonious version of this CP model, where polynomial contrasts for the positions are used rather than nominal effects. Recently, Stokes and Xu (2022) proposed three parsimonious CP models based on such polynomial contrasts: the first-order (FO), quadratic (Q) and second-order (SO) models. Their FO model with just ==== parameters sometimes suffices. Schoen and Mee (2021) proved that OofA-OAs are also ====- and ====-optimal for the (Stokes and Xu, 2022) FO model. Whether a PWO-type model or a CP model is more appropriate will depend on the application. For example, the position effect of search engine ads might be more effectively modeled with a CP model than a pairwise order model. Case studies and examples of order-of-addition experiments are provided by Voelkel and Gallagher (2019), Lin and Peng (2019), and Mee (2020), among others.====In this article, we make three contributions. First, we generate improved minimal saturated designs for estimating the standard PWO model, selecting these designs according to several optimality criteria. Section 2 summarizes these designs; the designs themselves are listed in the Appendix, Table A.1. We also provide the code we used to create these designs. Second, we show that maximizing ====-efficiency often does not minimize the maximum prediction variance. Thus, in Section 2, we present designs that have a smaller maximum prediction variance than the designs that maximize ====-efficiency. In Section 3 we examine the potential bias from using one of these saturated designs, due to misspecification of the model, and discuss how this can be mitigated by using a larger, foldover design. Third, we propose Bayesian ====-optimal designs, which can provide designs of size ====. When the number of components ==== is large, Bayesian D-optimal designs are recommended for screening. Section 4 presents two methods for constructing Bayesian ====-optimal designs, and evaluates these in terms of their projection information capacity. In Section 5 we illustrate the performance of these recommended designs. Concluding remarks are provided in Section 6.",Saturated and supersaturated order-of-addition designs,https://www.sciencedirect.com/science/article/pii/S0378375821001300,17 December 2021,2021,Research Article,33.0
"Yuan Mingao,Feng Yang,Shang Zuofeng","Department of Statistics, North Dakota State University, 1340 Administration Ave, Fargo, ND 58102, United States of America,College of Global Public Health, New York University, 665 Broadway, New York, NY 10003, United States of America,Department of Mathematical Sciences, New Jersey Institute of Technology, 323 Dr M.L.K. Jr. Blvd, Newark, NJ 07102, United States of America","Received 12 February 2020, Revised 6 December 2021, Accepted 7 December 2021, Available online 17 December 2021, Version of Record 22 December 2021.",https://doi.org/10.1016/j.jspi.2021.12.005,Cited by (2),"A fundamental problem in network data analysis is to test Erdös–Rényi model ==== versus a bisection stochastic block model ====, where ==== are constants that represent the expected degrees of the graphs and ==== denotes the number of nodes. This problem serves as the foundation of many other problems such as testing-based methods for determining the number of communities (Bickel and Sarkar, 2016; Lei, 2016) and community detection (Montanari and Sen, 2016). Existing work has been focusing on growing-degree regime ","In recent years, stochastic block model (SBM) has attracted increasing attention in statistics and machine learning. It provides the researchers a ground to study many important problems that arise in network data such as community detection or clustering (Amini et al., 2013, Amini and Levina, 2018, Neeman and Netrapalli, 2014, Sarkar and Bickel, 2015, Bickel and Chen, 2009, Zhao et al., 2012), goodness-of-fit of SBMs (Bickel and Sarkar, 2016, Lei, 2016, Montanari and Sen, 2016, Banerjee and Ma, 2017, Banerjee, 2018, Gao and Lafferty, 2017a, Gao and Lafferty, 2017b) or various phase transition phenomena (Mossel et al., 2015, Mossel et al., 2017, Abbe and Sandon, 2018). See Abbe (2018) for a comprehensive review about recent development in this field. A key assumption in most of the literature is that the expected degree of every node tends to infinity along with the number of nodes ====. For instance, in community detection (Bickel and Chen, 2009, Zhao et al., 2012), such a condition is needed for proving weak consistency of the detection methods; to prove strong consistency, the expected degree is further assumed to grow faster than ====. For goodness-of-fit test, the growing-degree condition is needed to derive various asymptotic distributions for the test statistics (Bickel and Sarkar, 2016, Lei, 2016, Banerjee and Ma, 2017, Banerjee, 2018, Gao and Lafferty, 2017a, Gao and Lafferty, 2017b).====Many real-world network data sets are highly sparse. For instance, the LinkedIn network, the real-world coauthorship networks, power transmission networks and web link networks all have small average degrees (see Leskovec et al., 2008, Strogatz, 2001). Therefore, it is reasonable to assume bounded degrees in such networks. There is a breakthrough recently made by Mossel et al., 2015, Mossel et al., 2017 and Abbe and Sandon (2018) about the possibility of successfully detecting the community structures when the expected degree of SBM is bounded. Specifically, the signal-to-noise ratio (SNR) of the multi-community SBM is used in these work as a phase transition parameter to indicate the possibility of successful detection. Motivated by such a groundbreaking result, it is natural to ask whether one can propose successful testing methods for SBMs with bounded degrees. Progress in this field may help researchers better understand the roles played by the expected degrees of SBMs in hypothesis testing, as well as provide a substantially broader scope of network models in which a successful test is possible.====In this paper, we address this problem in the bisection SBM scenario. We propose a likelihood-ratio (LR) type test statistic to distinguish an Erdös-Rényi model versus a bisection SBM whose expected degrees are finite constants, and investigate its asymptotic properties. In what follows, we describe the models and our contributions more explicitly.",A likelihood-ratio type test for stochastic block models with bounded degrees,https://www.sciencedirect.com/science/article/pii/S0378375821001294,17 December 2021,2021,Research Article,34.0
"Su Bing,Zhu Fukang","School of Mathematics, Jilin University, Changchun 130012, China","Received 24 March 2021, Revised 9 November 2021, Accepted 3 December 2021, Available online 14 December 2021, Version of Record 28 December 2021.",https://doi.org/10.1016/j.jspi.2021.12.002,Cited by (2),The temporal aggregation (TA) and ,"Time series are usually available in different frequencies, e.g., annually and quarterly. The temporal aggregation (TA) and systematic sampling (SS) are two approaches to handle these data. The TA generates the aggregated process (low-frequency, e.g., annually) by summing every ==== consecutive observations of the original process (high-frequency, e.g., quarterly), while the SS is about sampling every ====th observations. The TA or SS analysis focuses on inferring the aggregated or sampled process from the original one. For example, Amemiya and Wu (1972) showed that if the original process follows an AR(====) model, corresponding aggregated process follows an ARMA==== model, whose parameters connect with parameters in that AR(====) model.====The TA and SS had been studied in the literature for continuous-valued time series for the last decades. For example, Lütkepohl, 1987, Drost and Nijman, 1993 and Hafner (2008) discussed the TA and SS in the vector ARMA, univariate GARCH and multivariate GARCH processes, respectively. The predictors in the aggregated or sampled processes were also focused, e.g., Abraham (1982) and Kourentzes et al. (2017). A survey about the TA and SS was presented by Silvestrini and Veredas (2008). Recently, Rostami-Tabar et al. (2019) showed that the TA can improve the accuracy of the demand forecasting. Chan (2020) introduced the TA to some nonlinear time-series models.====The literature about the TA or SS in integer-valued time series is little (discussed later), while the integer-valued data are quite common in practice, e.g., counts of individuals and daily number of road accidents (see Weiß (2018)). To model these data, the integer-valued generalized autoregressive conditional heteroskedasticity (INGARCH), introduced by Ferland et al. (2006), is widely used. A Poisson INGARCH==== model for ==== is defined by ====where ==== is the ====-field generated by ====, ==== and ====. If ====, this model is referred to as the INARCH(====) model. The assumption of Poisson distribution can be generalized to those of negative binomial distribution or exponential family distributions, see Zhu (2011) and Davis and Liu (2016). Plenty of literature focused on generalizations and statistical inferences of INGARCH models, see, e.g., Fokianos and Tjostheim, 2011, Wang et al., 2014, Cui et al., 2020, Weiß et al., 2021 and a survey in Weiß (2018). In this paper, we will discuss the TA and SS for the INGARCH process.====Another widely used integer-valued model is the integer-valued autoregressive (INAR) model introduced by Mckenzie (1985), it was also surveyed in Weiß (2018). To the best of our knowledge, only two papers studied the aggregation for integer-valued time series, and both focused on the INAR model. Brannas et al. (2002) only showed that the temporal-aggregation INAR(1) model is an INARMA(1,1) model. Mohammadipour and Boylan (2012) focused on another type of aggregation in the INAR model, instead of the TA or SS. The operator in the INAR model makes related discussions difficult, which is one reason why we focus on the TA and SS for the INGARCH process.====As mentioned in the first paragraph, the TA or SS provides an approach to estimate the low-frequency data by the high-frequency data, and we will also show that the high-frequency process can be estimated by the low-frequency data. In other words, we can provide an approach to estimate integer-valued processes with different frequencies.====The aggregated INGARCH process will be shown to be a weak INGARCH process, which is a novel definition and connects to the weak ARMA process. The weak ARMA process with its statistical inferences and applications were studied by Francq and Zakoian, 2005, Zhua and Li, 2015 and Dufour and Pelletier (2021) and so on. The weak INGARCH process shares a similar structure with the weak GARCH process, whose studies can be found in Drost and Nijman, 1993, Francq and Zakoian, 2000 and Alexander and Lazar (2021). In this paper, we also discuss the estimation methods for the weak INGARCH process. Due to its flexibility and generality, studies for the weak INGARCH process will be an interesting topic.====The rest is organized as follows. In Section 2, some definitions and notations are presented. In Section 3, the TA and SS for INGARCH processes and corresponding predictors are discussed. Two approaches to estimate the INGARCH processes with different frequencies are illustrated. In Section 4, the quasi maximum likelihood (QML) and nonlinear least squares (NLS) estimation methods are considered for the weak INGARCH processes. In Section 5, using Monte Carlo simulations, these estimation methods are evaluated. These approaches are also discussed. In Section 6, an empirical example illustrates the applicability of our results. Section 7 concludes. Appendix A, Appendix B contain some useful proofs and expressions, respectively.",Temporal aggregation and systematic sampling for INGARCH processes,https://www.sciencedirect.com/science/article/pii/S0378375821001270,14 December 2021,2021,Research Article,35.0
"Yi Si-Yu,Zhou Yong-Dao,Zheng Wei","School of Statistics and Data Science LPMC & KLMDASR, Nankai University, Tianjin 300071, China,College of Mathematics, Sichuan University, Chengdu 610064, China,Haslam College of Business, University of Tennessee, Knoxville, TN 37996, United States","Received 28 May 2020, Revised 24 November 2021, Accepted 5 December 2021, Available online 14 December 2021, Version of Record 17 December 2021.",https://doi.org/10.1016/j.jspi.2021.12.004,Cited by (0),This paper focuses on the optimal design of ,"Longitudinal data arise frequently in the biomedical, epidemiological, social, and economical fields. A salient feature of longitudinal studies is that subjects are measured repeatedly over time. Thus, the responses between subjects may be independent but the repeated measurements within subjects are very likely to be correlated. Ignoring such correlation could result in invalid statistical inferences. Based on the modified Cholesky decomposition, Pourahmadi (1999) and Pourahmadi (2000) reparameterized the marginal covariance matrix by a mean–covariance model where the mean and the covariance matrix were modeled jointly and estimated by the maximum likelihood estimation (MLE) method. Afterwards, much literature work has devoted to extending the approach. Ye and Pan (2006) estimated the parameters in mean–covariance models within the framework of generalized estimating equations. Leng et al. (2010) proposed a data-driven approach based on semiparametric regression models for the mean and the covariance simultaneously, motivated by the modified Cholesky decomposition. Xu et al. (2019) developed a maximum ====-likelihood estimation for the mean–covariance model, which could yield robust and consistent estimators of the mean regression coefficients.====In practical longitudinal studies, the measurements of interest can be missing due to subjects’ non-response, dropout, or other reasons. In fact, it is rare to have complete data. There is a rich statistical literature on the analysis of missing data (Rubin, 1976, Little and Rubin, 1994). Two types of missing patterns are generally considered. One is called ‘nonmonotone missing’, where a subject may miss particular visits during the course of study and return at later scheduled visits. The other is ‘monotone missing’, where a subject may leave the study at some point and never return. For both missing patterns, the missing mechanisms can be classified into three categories. If the missingness is independent of both observed and unobserved data, it is missing completely at random (MCAR). Given the observed data, if the missingness is independent of the unobserved data, it is missing at random (MAR). If the missing probability depends on the unobserved data, it is not missing at random (NMAR). Many studies also have been done to handle incomplete longitudinal data based on the mean–covariance models. Pan and MacKenzie (2006) presented new computational algorithms which can handle unbalanced longitudinal data with missingness, thereby extending existing methods. Huang et al. (2012) embedded the covariance matrix of the observed data in a larger covariance matrix and employed the EM algorithm for both monotone and nonmonotone missingness. Garcia et al. (2012) adopted data-based and graphical methods to handle missing data.====From the viewpoint of experimental designs, it is well known that a carefully designed experiment can substantially improve statistical inferences. Many criteria have been proposed in optimal experimental designs. For example, D-optimality criterion maximizes the determinant of the information matrix of the estimates for the parameters and thus minimized the volume of the confidence region of the parameters of interest. However, in previous studies of longitudinal data, most focused on the estimation of the mean or the fixed effects and few on the covariance matrix, e.g. Ouwens et al., 2002, Tekle et al., 2008, Zhou et al., 2021. Instead, we hope to focus on the estimation of mean and covariance concurrently. Moreover, we also try to address multiple issues simultaneously. (a) The designs which satisfy the optimality criterion without considering the missing observations may be not optimal when there exists missingness indeed. Hence the optimality criteria need to be modified to accommodate for the missingness. Herzberg and Andrews (1976) and Andrews and Herzberg (1979) added the random variables to characterize the potential missingness in the information matrix and maximized its expected determinant. Imhof et al. (2004) introduced a known probability function to depict the probability of valid observation at a trial and put it into the original D-optimality criterion. Alrweili et al. (2019) considered the minimax loss response surface designs which is robust to one missing design point. (b) For a nonlinear model including the one that we will use, the information matrix and hence the optimal design depends on the true value of the parameter. However, the latter is unknown during the planning stage of experiment. One mainstream approach is to derive the optimal design for a particular guessed value of the parameter, and hence the local optimal design. Here, we would adopt the Bayesian optimal design, which is to find a design that is a good compromise over a distribution of the parameter as a prior information. (c) In longitudinal studies, accurate timing for taking measurements is difficult or even impossible. For example, the precisely prearranged times may be not likely to be adhered to in clinical experiments where patients have to attend a clinic for treatment. To allow an experimenter some flexibility in timing the data collection and assure a required design efficiency for parameter estimation, a sampling window approach has been used previously in pharmacokinetic studies and other clinical trials. Graham and Aarons (2006) proposed an approach to pharmacokinetic study design which determined the optimal time windows around the D-optimal pharmacokinetic sampling times. Bogacka et al. (2008) calculated the sampling windows based on the equivalence theorem for D-optimality which makes the widths of the windows related to the parameter sensitivities.====This paper is organized as follows. Section 2 gives a brief introduction of the mean–covariance models and the technique to characterize missingness. Section 3 proposes the optimality criterion and shows the superiority about the symmetrized design. Section 4 discusses the sampling windows and gives the algorithm to search it. Section 5 considers different missing probabilities and shows some simulation studies to compare different kinds of designs. The optimal window widths for certain target efficiencies are also obtained. Section 6 applies the proposed criterion to a real dataset. Some conclusions and discussions are given in Section 7. The proofs of the propositions and theorems are all given in Appendix A.",Optimal designs for mean–covariance models with missing observations,https://www.sciencedirect.com/science/article/pii/S0378375821001282,14 December 2021,2021,Research Article,36.0
"Sattler Paavo,Bathke Arne C.,Pauly Markus","Institute for Mathematical Statistics and Industrial Applications, Faculty of Statistics, Technical University of Dortmund, Joseph-von-Fraunhofer-Straße 2-4, 44221 Dortmund, Germany,Paris-Lodron-University of Salzburg, Department of Mathematics, Hellbrunner Str. 34, 5020 Salzburg, Austria","Received 28 January 2021, Revised 30 November 2021, Accepted 3 December 2021, Available online 11 December 2021, Version of Record 31 December 2021.",https://doi.org/10.1016/j.jspi.2021.12.001,Cited by (2),"We introduce a unified approach for testing a variety of rather general null hypotheses that can be formulated in terms of ====. These include as special cases, for example, testing for equal variances, equal traces, or for elements of the covariance matrix taking certain values. The proposed method only requires very few assumptions and thus promises to be of broad practical use. Different test statistics are defined, and their asymptotic or approximate sampling distributions are derived. In order to particularly improve the small-sample behaviour of the resulting tests, two bootstrap-based methods are developed and theoretically justified. Several simulations shed light on the performance of the proposed tests. The analysis of a real data set illustrates the application of the procedures.",None,Testing hypotheses about covariance matrices in general MANOVA designs,https://www.sciencedirect.com/science/article/pii/S0378375821001269,11 December 2021,2021,Research Article,37.0
"Kapelner Adam,Krieger Abba M.,Sklar Michael,Azriel David","Queens College, CUNY, Department of Mathematics, Kiely Hall Room 604, 65-30 Kissena Boulevard, Queens, NY, 11367, USA,The Wharton School of the University of Pennsylvania, Department of Statistics, Huntsman Hall Room 442, 3730 Walnut Street, Philadelphia, PA, 19104, USA,Stanford University, Department of Statistics, Sequoia Hall Room 105, 390 Serra Mall, Stanford, CA, 94305, USA,The Technion, Faculty of Industrial Engineering and Management, Bloomfield Building Room 301, Technion City, Haifa 32000, Israel","Received 16 June 2020, Revised 30 August 2021, Accepted 23 November 2021, Available online 1 December 2021, Version of Record 11 December 2021.",https://doi.org/10.1016/j.jspi.2021.11.005,Cited by (3), package available on ==== named ==== which generates designs according to our algorithm.,None,Optimal rerandomization designs via a criterion that provides insurance against failed experiments,https://www.sciencedirect.com/science/article/pii/S0378375821001130,1 December 2021,2021,Research Article,38.0
"Li Wenlong,Liu Min-Qian,Tang Boxin","School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 100081, China,School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China,Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, British Columbia V5A 1S6, Canada","Received 2 September 2020, Revised 19 November 2021, Accepted 19 November 2021, Available online 26 November 2021, Version of Record 7 December 2021.",https://doi.org/10.1016/j.jspi.2021.11.004,Cited by (0),"Karunanayaka and Tang (2017) introduced a class of compromise designs for estimating main effects under the baseline parameterization. Their approach is to add some runs to the basic one-factor-at-a-time design, and its implementation requires computer search except for the case of adding one run where a theoretical result is available. The reliance on computer search only allowed them to find the limited results from adding up to four runs to the basic one-factor-at-a-time design. In this paper, we provide a systematic construction of compromise designs, without computer search and without restriction on the run size and the number of factors. Closed-form expressions of the bias and efficiency criteria are obtained for the new designs. Both theoretical and empirical studies show that our designs outperform those of Karunanayaka and Tang (2017) saving small-sized problems.","Two-level factorial designs are arguably most useful designs for screening experiments and their studies are generally carried out under the orthogonal parameterization, which defines the factorial effects using a set of orthogonal contrasts (Mee, 2009, Wu and Hamada, 2009, Cheng, 2014). Recently, Banerjee and Mukerjee (2008) and Stallings and Morgan (2015) considered a quite natural, though less common, non-orthogonal baseline parameterization, motivated by practical situations as those in Glonek and Solomon (2004), Kerr (2006) and Yang and Speed (2002). The baseline parameterization defines the factorial effects using the baseline levels of the factors. For the relationship between the orthogonal and baseline parameterizations, we refer to Sun and Tang (2022). Throughout the paper, we consider two-level factorial designs under the baseline parameterization.====The baseline parameterization is useful in experiments whose factors possess a null state or baseline level. For example, in a toxicological study, each binary factor represents the absence or the presence of a particular toxin, where the states of absence and presence can be regarded as the natural baseline and test levels, respectively (Kerr, 2006). Another real case is an industrial experiment whose objective is to improve the production process via changing several factors’ current settings. The current and new settings can be interpreted as the baseline and test levels, respectively.====Mukerjee and Tang (2012) showed that orthogonal arrays of strength two are universally optimal for the estimation of main effects. As non-negligible interactions can lead to the bias for the main-effect model, they further proposed to select minimum aberration designs within the class of orthogonal arrays. Subsequent work along this line includes Li et al., 2014, Miller and Tang, 2016 and Mukerjee and Tang (2016). Also considered in Mukerjee and Tang (2012) are one-factor-at-a-time (OFAT) designs and they showed that such designs allow unbiased estimation of main effects even though all interactions are present. To achieve a bias–variance tradeoff when estimating main effects, Karunanayaka and Tang (2017) introduced a class of compromise designs by adding some runs to the basic OFAT design. They derived a theoretical result for the case of adding one run. Their approach requires computer search in general, and this reliance on computers only allowed them to obtain designs by adding up to four runs to the basic OFAT design.====We present in this paper a systematic construction of compromise designs, which does not require computer search and has no restrictions on the run size and the number of factors. Closed-form expressions for the bias and efficiency criteria are obtained for the new designs. We then conduct some theoretical and empirical investigations to compare our designs with those of Karunanayaka and Tang (2017). The results of the investigations show that our designs outperform theirs under both criteria of bias and variance except for small-sized problems. Along the way, a theoretical result is also obtained for the case of adding two runs to the basic OFAT design in the method of Karunanayaka and Tang (2017).====The rest of this paper is organized as follows. Section 2 provides background and preliminaries. Section 3 introduces a systematic construction of compromise designs, examines its theoretical properties, and discusses some comparisons with the existing designs. A generalization is presented in Section 4. To further demonstrate the usefulness of our designs, in Section 5 we compare them with minimum aberration designs and OFAT designs in terms of the mean squared error. Section 6 concludes the paper.",A systematic construction of compromise designs under baseline parameterization,https://www.sciencedirect.com/science/article/pii/S0378375821001129,26 November 2021,2021,Research Article,39.0
"Patriota Alexandre Galvão,Alves Jônatas de Oliveira","Departamento de Estatística, IME, Universidade de São Paulo, Rua do Matão, 1010, São Paulo/SP, 05508-090, Brasil","Received 25 March 2021, Revised 19 October 2021, Accepted 18 November 2021, Available online 25 November 2021, Version of Record 9 December 2021.",https://doi.org/10.1016/j.jspi.2021.11.002,Cited by (2),This paper studies the ====-value discussed. It is shown that the ====-value satisfies logical consequences which are not satisfied by typical ====-values and also that the test based on the ====-value has significance level ====-value not exceeding a fixed threshold ==== does not surpass ==== for each fixed ,"Mixed models are typically applied in the analysis of repeated measures and, in particular, in longitudinal studies, since they naturally incorporate the positive dependence among the repeated observations of the same subject (sample or experimental unit). This dependence is modeled by means of random effects which have zero means and non-negative variances (Laird and Ware, 1982). It is common to test whether some of these variances are zero in order to simplify the model by removing the random effects with zero variances (at a fixed significance level). Since this is a nonregular problem (the parameter is on the boundary of the parameter space), the usual chisquared asymptotic distribution of the test statistics (e.g., likelihood ratio statistics) is not valid and a special treatment is required (Chernoff, 1954, Self and Liang, 1987, Drton, 2009).====Vu and Zhou (1997) derived a general asymptotic theory of likelihood ratio tests under nonstandard conditions where the parameters can be on the boundary of the parameter space. Their asymptotic theory requires eight regularity conditions on model quantities and on the null parameter space. The authors applied the proposed theory to a two-way nested random variance components model with covariates. Giampaoli and Singer (2009) applied this general asymptotic theory to a general linear mixed model which included the illustration presented in Vu and Zhou (1997) as a particular instance. From those results, asymptotic ====-values can be computed to test whether some variance components are zero.====For the sake of illustration, consider the following random effect model ====where ==== is the ====th observation from the ====th experimental unity, ==== is the populational mean, ==== is the random effect independent of the error term ====, where ==== is the random effect variance, ==== is the random error variance and “====” denotes “independent and identically distributed”. For this model, the parameter space is ==== and the parameter vector is ====, where ==== is the set of real numbers, ==== and ====. Notice that the parameter space allows the random-effect variance to be zero, namely, ====. That is, the null random-effect variance hypothesis ====, under which the model (1.1) reduces to the one mean model with independent response variables, lists parameter values on the boundary of the parameter space. For this reason, the standard asymptotic theory cannot be straightforwardly employed. The asymptotic distribution of the likelihood ratio statistics under nonstandard conditions derived by Vu and Zhou (1997), and its specialization studied by Giampaoli and Singer (2009), can be employed to test this hypothesis by means of (asymptotic) ====-values based on likelihood ratio statistics.====In practice, however, practitioners may not be interested only in testing null random-effect variances but also in other types of hypotheses such as ==== and ====. Both hypotheses list elements on the boundary of ==== and, for this reason, a nonstandard asymptotic theory should be applied such as the one derived by Vu and Zhou (1997). Observe that there is a logical relationship among the aforementioned three hypotheses, namely, ==== and ====. That is, the null hypothesis ==== is the most restricted one among the threes and, therefore, we would expect more evidence against this null than against either ==== and ==== from the same observed data. In general, let ==== and ==== be two hypotheses on ==== such that ==== (i.e., if ==== is true, then ==== must be true), then we expect finding more evidence against the more restricted one, namely, ====, than ====. A measure of evidence (or support) that satisfies this feature is called to be ====.====Schervish (1996) and Patriota (2013) presented examples for which ====-values violate this monotonicity. The authors considered models with normal distributions and defined two (or more) hypotheses ==== and ==== on ==== such that ==== but the corresponding ====-values satisfy a non-expected relation ====for the same observed data ==== rather than the one expected by the logical consequence ====We present some numerical examples of this non-expected relation for the random effect model (1.1) by considering the hypotheses ====, ==== and ====, see Example 4.1 in Section 4.====In order to overcome this problem, Patriota (2013) defined an alternative measure called (asymptotic) ====-value that is based on (asymptotic) confidence regions built with likelihood ratio statistics. The smaller is the ====-value, the larger is the distance of the null hypothesis from the observed data. Also, the ====-value is a monotonic measure. In this paper, we extend the standard ====-value definition to be also employed in cases where the parameter vector is on the boundary of the parameter space under a linear mixed model. In this context, the challenge is to find the asymptotic distribution of the likelihood ratio statistics for a simple hypothesis (not for the composite one as was developed by Giampaoli and Singer, 2009). Then, based on this nonstandard asymptotic distribution, we build a ====-value version which is valid for testing also null variances for the random effects.====The rest of this paper is organized as follows. Section 2 introduces the linear mixed model. Section 3 presents the asymptotic distribution of the likelihood statistic for some very general hypotheses where the parameter vector is on the boundary of parameter space. Section 4 offers a numerical example to show that the ====-value based on the asymptotic distribution is not monotonic. Section 5 defines the asymptotic ====-value under model defined in Section 2 and presents some of its properties. Section 6 illustrates, through Monte Carlo simulations, some of the theoretical results attained in the previous sections. Section 7 applies the theory in the famous Orthodontic growth data (Pinheiro and Bates, 2000) considering only male subjects. Finally, Section 8 ends the paper with concluding remarks. For the sake of completeness, the reader can consult the eight nonstandard regularity conditions (Vu and Zhou, 1997) in Appendix A. The proofs and regularity conditions for the general linear mixed model are deferred to Appendix B.",A monotone frequentist measure of evidence for testing variance components in linear mixed models,https://www.sciencedirect.com/science/article/pii/S0378375821001105,25 November 2021,2021,Research Article,40.0
"Sun Yifan,Wang Qihua","Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences, Beijing 100190, China","Received 28 August 2020, Revised 21 October 2021, Accepted 18 November 2021, Available online 25 November 2021, Version of Record 6 December 2021.",https://doi.org/10.1016/j.jspi.2021.11.003,Cited by (2)," are also supported by extensive simulation studies. The resulting estimator performs better than direct adaptive LASSO estimators and some existing functional generalized linear models estimators, which are obtained without considering the domain selection. A real data application reveals the effectiveness of the proposed method.","In the past decade, functional data have been commonly encountered in diverse research fields because of the development of modern technology. Studies of functional data analysis are abundant. Readers can refer to some well-known monographs, for example, Ramsay and Silverman (2005), Ferraty and Vieu (2006), Horváth and Kokoszka (2012) and Hsing and Eubank (2015), for a general view. The functional generalized linear model is a popular and effective tool for establishing the relationship between a category response and functional predictors. James (2002) first proposed functional generalized linear models mathematically, where covariates are parameterized by natural cubic splines and no theoretical results are provided. Functional principle component analysis (FPCA) was adopted in the prominent work of Müller and Stadtmüller (2005), where asymptotic inference was also developed. Further results about convergence rate of FPCA approach was investigated in Dou et al. (2012) under regular conditions on covariates and slope functions. Cardot and Sarda (2005) added smoothness penalty in criterions and derived convergence rate of the coefficient function under B-spline framework. Du and Wang (2014) considered similar penalized likelihood problem in some reproducing kernel Hilbert spaces and verify the minimax optimality. Other extensions include but not limited to Li et al. (2010) for more complicated model with interactions between functional and scalar covariates, Crainiceanu et al. (2009) for multilevel functional data and Wang et al. (2017) for scalar-on-image problems.====Comparing to the substantial focus on slope function estimation, little attention is paid on domain selection. Suppose that the canonical parameter in functional generalized linear model is ====, where ==== is a functional predictor and ==== is the slope function. If ==== is null on some regions of ====, then ==== on the corresponding subdomains has no contribution to response. Locating null regions of slope may reduce the variability and improve interpretability. It is also significant in the sense of potential cost saving in practice. The literature on domain selection mainly concentrates on functional linear models. James et al. (2009) considered ==== penalty of ==== on a sequence of grid points, where no theories about null region identification are given. Zhou et al. (2013) developed a two-stage procedure with LASSO-type penalties on B-spline approximation coefficients and verified its oracle property. Lin et al. (2017) extended the idea of SCAD (Fan and Li, 2001) to functional linear models and proved its effectiveness. Recently Huang and Wang (2022) developed a functional information criterion to select useful regions. Note that there is also another paradigm aiming at detecting impact points. See McKeague and Sen, 2010, Aneiros and Vieu, 2014, Kneip et al., 2016 and Berrendero et al. (2018) and the references therein. Unlike the traditional setting of functional models, they assumed that response was mainly effected by some isolated points ====. Since points detection problem is different from region detection, we will not discuss this further in this paper.====To the best of our knowledge, there is no research work for domain selection in the case of category response. Kraus and Stefanucci (2019) investigated functional discriminant problem and simply searched the best subset in a series of nested candidates. They provided no selection consistency results. This motivates us to consider the functional generalized linear models, which include some category response models as special examples. We develop methods to achieve null regions detection and obtain estimates of the slope function in functional generalized linear models simultaneously. Adopting B-spline approximation framework, some consecutive sequences of B-splines coefficients need to be shrunk to 0 jointly in order to stimulate the structural sparsity. We propose an adaptive group LASSO penalty by combining ideas of group LASSO (Yuan and Lin, 2006) and adaptive LASSO (Zou, 2006). Note that this penalty term is different from previous works (Zhou et al., 2013, Lin et al., 2017). Therefore corresponding establishments of theories are dissimilar to the existing approaches. The convergence rate of the proposed estimator is computed. The consistency of domain selection is then established, which is not straightforward since the groups to be penalized are overlapping.====The rest of the paper is organized as follows. In Section 2, we develop the adaptive group LASSO penalty and introduce an algorithm to optimize the penalized likelihood criterion. All assumptions and theoretical properties are stated in Section 3. Section 4 and Section 5 present simulations and real data applications respectively. The technical proofs and some auxiliary simulated examples are postponed to Appendix A.",An adaptive group LASSO approach for domain selection in functional generalized linear models,https://www.sciencedirect.com/science/article/pii/S0378375821001117,25 November 2021,2021,Research Article,41.0
"Bhattacharya Indrabati,Ghosal Subhashis","Department of Biostatistics and Computational Biology, University of Rochester Medical Center, United States of America,Department of Statistics, North Carolina State University, United States of America","Received 2 December 2020, Revised 24 September 2021, Accepted 23 October 2021, Available online 15 November 2021, Version of Record 25 November 2021.",https://doi.org/10.1016/j.jspi.2021.10.005,Cited by (0), and non-parametric tests.,"Several frequentist testing procedures for multivariate locations are available in the literature, both parametric and non-parametric. The most well-known parametric procedure is the Hotelling’s ====-test, which is based on the multivariate mean vector and the covariance matrix, and it relies on the assumption of multivariate normality. This technique performs well if the assumption of multivariate normality is nearly correct, but suffers heavily otherwise, or in the presence of outliers. Non-parametric and robust alternatives based on signs and ranks have been quite popular over the years (Oja and Randles, 2004).====The notions of signs and ranks are based on the “ordering” of the data points, but in the multivariate setting, there is no objective basis of ordering. The notions are generalized to higher dimensions using ====-objective functions (see Section 2). The existing one-sample location problem has the following set up. Suppose that, we have ==== independent and identically distributed (i.i.d.) observations ==== taking values in ==== from a distribution ====, a ====-variate continuous distribution centered at ====. Our objective is to test the hypothesis ====The existing score-based non-parametric test procedures are based on the multivariate spatial sign vector ====, multivariate spatial rank ====, and multivariate spatial signed rank ====, which are defined respectively as ==== The estimator of the location associated with spatial sign vector in (2) is the spatial median ====where ==== is the empirical measure. The score functions (3), (4) give rise to multivariate Hodges–Lehmann estimators (Oja and Randles, 2004). One drawback of these multivariate sign and rank-based tests is that their ====-values rely on a limiting chi-square distribution of the test statistics, provided the underlying distribution is elliptically symmetric (defined in Section 2). In this paper, we construct Bayesian non-parametric testing procedures for multivariate locations using the spatial median. In other words, here we focus on the score functions of type (2) and propose a non-parametric Bayesian testing procedure. Such a procedure is more attractive because it directly provides a credible set for the spatial median through quick posterior sampling, hence a testing criterion can be formulated without depending on asymptotics. We assume that the observations are drawn from a random distribution ====, and we put a Dirichlet process (details given in Section 3) prior on it. From ====, we can infer about its spatial median functional ====where ====. The exact posterior distribution (modulo the Monte Carlo error) of ==== can be obtained easily by posterior simulation. Thus, we can form a credible region for ==== and our decision is based on whether the null value ==== falls into this credible set. For elliptically symmetric distributions, this testing procedure effectively studies the one-sample location problem described above, but our testing procedure can be used to study a wider range of distributions ====, where we study the null hypothesis ====. We show that our testing procedure is asymptotically non-parametric, i.e., the limiting type I error does not depend on the true distribution, and we further compute the asymptotic power function under Pitman (contiguous) alternatives along possible directions. The two-sample test has be formulated in a similar way, and its properties have been explored in a similar fashion.====The development of the asymptotic theory for the testing procedures relies on a strengthening of the theory developed in Bhattacharya and Ghosal (2022), which studies the asymptotic properties of a multivariate median, denoted by ====, in a non-parametric Bayesian framework. Precisely, they put a Dirichlet process prior on ==== and proved a Bernstein–von Mises theorem for ==== (Theorem 3.1 of Bhattacharya and Ghosal, 2022), which we use for the derivation of the theorems in Sections 3 Bayesian non-parametric tests, 4 Asymptotic power under contiguous alternatives. The rest of this paper is organized as follows. In Section 2, we give an overview of the existing multivariate testing procedures. In Section 3, we describe our proposed Bayesian non-parametric test procedures. Section 4 gives the local asymptotic power under contiguous alternatives and Section 5 presents a simulation study. All the proofs are given in Section 6, and we close the paper with a brief discussion in Section 7.",Bayesian nonparametric tests for multivariate locations,https://www.sciencedirect.com/science/article/pii/S0378375821001075,15 November 2021,2021,Research Article,42.0
Overstall Antony M.,"Southampton Statistical Sciences Research Institute (S3RI), University of Southampton, Southampton, SO17 1BJ, United Kingdom","Received 6 January 2021, Revised 23 September 2021, Accepted 23 October 2021, Available online 4 November 2021, Version of Record 16 November 2021.",https://doi.org/10.1016/j.jspi.2021.10.006,Cited by (2),The Bayesian decision-theoretic approach to ,"Suppose an experiment is to be conducted to learn the relationship between a series of controllable variables and a measurable response for some physical system. The experiment consists of a number of runs where each run involves specification of a series of controllable variables and subsequent observation of a response. Typically, on completion of the experiment, it is assumed that the observed responses are realisations of a random variable with a fully specified probability distribution apart from a vector of unknown parameters. That is, a statistical model is assumed for the responses. The observed responses are used to estimate the parameters thus allowing estimation of the relationship between controllable variables and response.====A question of interest is how should the controllable variables for all runs (i.e. the design) be specified to facilitate as precise as possible estimation of the unknown parameters. The Bayesian decision-theoretic approach (see, for example, Chaloner and Verdinelli, 1995) is to select the design to maximise the expectation of a utility function. Expectation is taken with respect to the joint distribution of all unknown quantities, i.e. the responses and the parameters, with this joint distribution following from the specification of the statistical model and a prior distribution for the parameters. The advantages of this approach are as follows. (1) The choice of utility allows bespoke experimental aims to be incorporated (for example, experimental aims of prediction and/or model selection can be considered instead of estimation although these are not the focus of this paper). (2) By taking expectation with respect to unknown quantities, all known sources of uncertainty are incorporated. (3) The framework fits into the iterative nature in which knowledge is accumulated in science, i.e. the distribution of unknown quantities for the current iteration is based on the results of the previous iteration. With regards to point (3) above, the approach can even be extended to account for the fact that the expected utility will be maximised in subsequent iterations (see, for example, Huan and Marzouk, 2016), i.e. so-called Bayesian sequential or adaptive design. However this extension is not considered in this paper.====The choice of utility allows different ways of specifying estimation precision. Ideally, the choice of utility should be uniquely tailored to the experiment. However, a more pragmatic choice is often made from certain default utility functions. Two common default utilities for estimation precision are Shannon information gain (Lindley, 1956) and negative squared error loss (see, for example, Robert, 2007 pages 77–79). In either case, typically, the utility function depends on the responses through the posterior distribution of the parameters. Even for relatively simple statistical models, the posterior distribution is not of a known form meaning that the utility and expected utility are not available in closed form and require approximation.====Recently, Walker (2016) instead proposed the Fisher information gain utility. This is the difference between the squared Euclidean length of the gradient of the log posterior and log prior densities of the parameters. The appealing feature of Fisher information gain is that the resulting expected utility is the prior expectation of the trace of the Fisher information matrix. For many models, the Fisher information matrix is available in closed form, significantly simplifying approximation of the expected utility and subsequent identification of an optimal design.====However, in this paper it is shown that, under a broad class of models, a design that maximises the expected Fisher information gain can lead to a non-identifiable model. Under the classical approach to statistical inference, this means the parameters are not uniquely estimable (see, for example, Catchpole and Morgan, 1997). Under the Bayesian approach, it means that the posterior and prior distributions for a subset of parameters (conditional on the complement of the subset) are identical (see, for example, Gelfand and Sahu, 1999). These are undesirable consequences for a default utility function.====The paper is organised as follows. Section 2 provides a background to Bayesian decision-theoretic design of experiments, introduces the Fisher information gain utility, describes the exponential family of models and discusses the notion of non-identifiability. Section 3 explores the consequences of designing experiments under Fisher information gain with examples provided in Section 4.",Properties of Fisher information gain for Bayesian design of experiments,https://www.sciencedirect.com/science/article/pii/S0378375821001087,4 November 2021,2021,Research Article,43.0
"Scholtus Sander,Shlomo Natalie,de Waal Ton","Process Development and Methodology Department, Statistics Netherlands, PO Box 24500, 2490 HA The Hague, The Netherlands,School of Social Sciences, University of Manchester, Humanities Bridgeford Street, Manchester M13 9PL, United Kingdom,Department of Methodology and Statistics, Tilburg University, Tilburg, The Netherlands","Received 2 April 2020, Revised 11 October 2021, Accepted 14 October 2021, Available online 26 October 2021, Version of Record 11 November 2021.",https://doi.org/10.1016/j.jspi.2021.10.004,Cited by (2),"Record linkage aims to bring records together from two or more files that belong to the same statistical entity. Naïvely treating a linked file as if there are no linkage errors may lead to biased inference. We present two general approaches for compensating for linkage error when calculating and analysing a two-way ==== for ====, and study the following question: under what conditions can a compensation approach improve on the naïve approach, where linkage error is not compensated for? To this end, we compare estimation errors, bias, variance and ==== for the naïve approach and two compensation approaches by means of an analytical study as well as a simulation study.","Record linkage aims to bring records together from two or more files that belong to the same statistical entity such as an individual or a business. The seminal paper by Fellegi and Sunter (1969) provides a framework for probabilistic record linkage and resulted in decades of research on linking datasets. In official statistics, the first large-scale record linkage exercises were between census and coverage surveys. In practice, with large real-life datasets, the record linkage is carried out within “blocks” defined by the values of certain background variables (see, e.g., Herzog et al., 2007) and this approach scales the problem into manageable sub-groups.====Given the increasing use of administrative and new forms of data in statistical systems there is an increasing need to continue to develop record linkage approaches and, in particular, there is a growing emphasis on accounting for linkage errors in statistical analysis. In a linked dataset there are two types of errors: incorrectly linked pairs which may potentially incur bias in statistical analyses and missed links which impact on coverage and potential bias if those missed links differ in their characteristics from the found links. In this paper, we focus on incorrectly linked pairs which result from records in two or more datasets being linked incorrectly due to errors, missing values or changes over time in the variables that are used in the matching procedure. Throughout the paper, we will assume that both datasets contain the same units and the aim is to link all of them (a one-to-one linkage). Although this assumption is necessary to develop the theoretical framework of this study, this scenario is relevant at National Statistical Institutes (NSIs) where there is a move towards administrative-based censuses which link multiple administrative data sources, for example, administrative data from patient registers, income tax, council tax and social security authorities. In this scenario, not all data subjects will be linked but there is an expectation of a one-to-one linkage.====Naïvely treating a probabilistically linked file as if there are no linkage errors may lead to biased inference. There has been early work on compensating for linkage errors in regression models, see Scheuren and Winkler, 1993, Scheuren and Winkler, 1997 and Lahiri and Larsen (2005). Chambers (2009) proposed bias-corrected linear regression coefficients and Kim and Chambers, 2012a, Kim and Chambers, 2012b also looked at more general models using estimating equations. Chipperfield et al. (2011) and Chipperfield and Chambers (2015) focused on categorical data and proposed an unbiased method for compensating for linkage errors. Given the emerging focus on administrative-based censuses at NSIs, more research is needed to understand the impact of linkage error on categorical data and census tables. NSIs aim to publish high-quality and accurate descriptive statistics on the population. Therefore, the estimation of such contingency tables is important as they are mainly used to inform policies by government agencies and other stake-holders. On the other hand, the statistical analyses of such tables, for example tests for independence and the estimation of regression coefficients where the independent and dependent variables are from different linked datasets, are important for research purposes.====The aim of this paper is to study the effect of linkage errors in its “purest” form and we assume that there are no other errors besides linkage errors. We focus on a basic problem, namely the estimation of contingency tables where the two target variables are from different linked datasets, and research two fundamental correction methods. We note that when these fundamental correction methods do not lead to accurate results for the basic problem of estimating contingency tables, they are unlikely to lead to accurate results for more complicated estimation problems.====In this paper, we will follow the Chambers (2009) assumption of the exchangeable linkage error model to develop the theoretical framework and this is described in Section 2. The error rates for the exchangeable linkage error model can be estimated by sub-sampling linked pairs and manually checking for errors in these linked pairs. In addition, bootstrapping approaches have been proposed, see Winglee et al. (2005) and Chipperfield and Chambers (2015). Here, we assume that the linkage error rate is known under the exchangeable linkage error model.====We present two approaches for compensating for linkage error when analysing a two-way contingency table where one variable is coming from one file and the second variable is coming from the other file, and the files are linked through a probabilistic record linkage process. One approach is an unbiased correction and we show that this generally equates to the approach used by Chipperfield and Chambers (2015) under the exchangeable linkage error model. The other approach is a biased correction, but which often leads to lower mean square error compared to the unbiased approach. Under the exchangeable linkage error model, we will study the following fundamental questions: can a compensation approach for linkage error improve on the naïve approach, where linkage error is not compensated for, and, if so, under what conditions? To this end, we will compare the two compensation approaches to the naïve approach. We will examine the three approaches in detail, both by means of an analytical study as well as by means of a simulation study. In particular, we examine estimation errors, bias, variance and mean square error of the three approaches.====Our paper tells a cautionary tale in two aspects. The first aspect is that aiming for unbiasedness, which is usually a very desirable property for an estimator to have, is not always the best option as this may come at the expense of higher variance and higher mean square error compared to a biased estimator. The second aspect is that even though two approaches may look quite different, such as the approach used by Chipperfield and Chambers (2015) and our unbiased correction approach, they can still be essentially the same.====Results in this paper show that the compensation approach to take for a given situation depends on the characteristics of the table and whether the table shows dependent or independent attributes, and more specifically whether the particular cell of the table has a positive, negative or no association. Section 2 provides the motivation for correcting for linkage errors in contingency tables with a small toy example and defines the two compensation approaches based on the biased and unbiased correction techniques. In Section 3, we present theoretical results on which approach outperforms the naïve approach in terms of the estimation error in individual cell values, depending on the type of association of the cell value. Section 4 shows theoretical results of the bias and variance of the cell values under the naïve and compensation approaches. All theory is tested in Section 5 in a simulation study based on a set of tables with dependent and independent attributes and varying cell associations and, in particular, we show the impact of the naïve and compensation approaches on statistical tests of independence. We conclude in Section 6 with a discussion and recommendations.",Correcting for linkage errors in contingency tables—A cautionary tale,https://www.sciencedirect.com/science/article/pii/S0378375821001063,26 October 2021,2021,Research Article,44.0
"Bhattacharya Indrabati,Martin Ryan","Department of Biostatistics and Computational Biology, University of Rochester Medical Center, United States of America,Department of Statistics, North Carolina State University, United States of America","Received 24 October 2020, Revised 21 September 2021, Accepted 7 October 2021, Available online 21 October 2021, Version of Record 2 November 2021.",https://doi.org/10.1016/j.jspi.2021.10.003,Cited by (8)," convergence rate and a Bernstein–von Mises property, i.e., for large ","In multivariate analysis, often the quantity of interest is the multivariate mean vector. However, there are situations when the mean is not a very good measure of location, for example, when the data is skewed, has outliers, etc. In such situations, a multivariate median would be a much more robust measure of the distribution’s center. Unfortunately, there is no universally accepted definition of a multivariate median, because there is no objective basis of ordering the data points in higher dimensions. Over the years, various definitions of multivariate medians and, more generally, multivariate quantiles have been proposed; see Small (1990) for a comprehensive review.====The most common version of a multivariate median is called the ====-median, which is characterized through an ====-optimization problem. Define ====where ==== is the usual ====-norm of a ====-dimensional vector ==== in ====, for ==== a fixed constant. Following Small (1990), the ====-median of the random vector ====, taking values in ====, is ====where we use the notation ==== to denote the expected value of a random variable ==== with respect to the distribution ====. The special case ==== is called the spatial median and was studied in, e.g., Brown (1983). Additional details about ====-medians and, more generally, about multivariate quantiles are given in Section 2.1.====In statistical applications, the distribution ==== is unknown, but an independent and identically distributed (iid) sample ==== of random vectors in ==== are available from ====. Based on the above formulation, an immediate strategy for estimating the ====-median is to follow the definition of ==== but replace the distribution ==== with the empirical distribution, i.e., with ====, where ==== denotes a point-mass distribution at the point ====. That is, the standard point estimate of ==== is ====The spatial sample median is a highly robust estimator of location, in particular, its breakdown point is ====. Also, Möttönen et al. (2010) investigated the asymptotic properties of spatial median and proved its asymptotic normality.====Beyond estimation, if the goal is probabilistic inference on a multivariate median or quantile, i.e., via a “posterior distribution,” then the chief difficulty is that these are not naturally described as parameters in a statistical model. That is, no standard or otherwise “reasonable” model for multivariate data will include a quantile in its parametrization, so some potentially dangerous non-linear marginalization (e.g., Fraser, 2011, Martin, 2019) would typically be required. More importantly, with specification of a model comes the risk of model misspecification bias, and, since our quantity of interest is well-defined without a model, it is not clear what can be gained by working in a model-based framework to balance out the risk of misspecification bias. However, there are advantages to having a posterior probability distribution on which to base inferences; for example, a posterior density provides a nice visual summary of uncertainty, credible regions can be immediately read off from it without asymptotic approximations, and prior information about the quantity of interest can be incorporated whenever it is available. A Bayesian approach that both achieves these desirable features (more or less) and avoids the risk of model misspecification bias must be nonparametric. That is, assign a prior distribution—say, a Dirichlet process (Ferguson, 1973)—to the infinite-dimensional ====, get the corresponding posterior, and then do the non-trivial marginalization to ====. We call this an ==== approach. Aside from computational challenges, a downside of the indirect approach is that incorporating available prior information about the quantile is difficult; probably the best option is to choose a Dirichlet process base measure to have quantile equal to a prior guess, but it is not clear how this (and other features of the specified base measure) affects the marginal posterior for the quantile.====Is it possible to develop a posterior for the multivariate quantile in a more ==== way, without marginalization, etc.? Here we investigate the construction of a ==== for a multivariate quantile. On one hand, like M-estimation, this approach uses a suitable loss function, rather than a likelihood, to connect the quantity of interest to the observed data, which eliminates the risk of model misspecification bias. On the other hand, like Bayesian inference, it produces a genuine posterior distribution and allows for the direct incorporation of prior information. After some background about multivariate quantiles and Gibbs posteriors in Section 2, we define our object of interest, namely, the Gibbs posterior distribution for a multivariate quantile, and investigate its properties. In particular, in Section 3.2, we first establish that the Gibbs posterior concentrates around the true quantile at the usual root-==== rate and, second, that it has an asymptotic Gaussian approximation in the Bernstein–von Mises sense. Unfortunately, the covariance matrix in this Gaussian approximation is “wrong” in the sense that it does not match that of the M-estimator around which it is centered. Fortunately, the Gibbs posterior depends on a user-specified ==== (e.g., Bissiri et al., 2016, Grünwald and van Ommen, 2017, Syring and Martin, 2019) which can be tuned to at least partially correct for the covariance matrix mismatch. We use a bootstrap-based calibration algorithm proposed by Syring and Martin (2019) for choosing the learning rate, which we describe in Section 3.3. In Section 4.1, we compare the finite-sample performance of our proposed Gibbs posterior inference to that based on existing Bayesian approaches. The two key take-aways are: (a) the Gibbs posterior outperforms the model-based parametric Bayesian approach in misspecified situations, and (b) the Gibbs posterior credible sets are at least as good as the nonparametric Bayes credible sets in terms of coverage but they are more efficient in some cases. We also apply the Gibbs posterior approach to infer the spatial median based on a real data set in Section 4.2, for which the assumption of normality is debatable. We show that the Gibbs posterior outperforms a normality-based Bayesian solution in terms of out-of-sample risk, implying that our Gibbs solution avoids the inherent bias coming from the normality assumption. Some concluding remarks are given in Section 5 and proofs of the two main theorems are presented in Appendix.",Gibbs posterior inference on multivariate quantiles,https://www.sciencedirect.com/science/article/pii/S0378375821001051,21 October 2021,2021,Research Article,45.0
"Xu Wenchao,Lin Hongmei,Zhang Riquan,Liang Hua","Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China,School of Statistics and Information, Shanghai University of International Business and Economics, Shanghai 201620, China,Key Laboratory of Advanced Theory and Application in Statistics and Data Science, MOE, and School of Statistics, East China Normal University, Shanghai 200241, China,Department of Statistics, George Washington University, Washington, D.C. 20052, USA","Received 19 July 2020, Revised 20 September 2021, Accepted 1 October 2021, Available online 12 October 2021, Version of Record 28 October 2021.",https://doi.org/10.1016/j.jspi.2021.10.001,Cited by (0),"This paper studies two-sample functional linear regressions with functional responses, where the ==== are assumed to have a scaling transformation. We estimate the intercept function, slope function, and parameter components based on the least squares method and functional principal component analysis. The proposed estimators of the parameter components are shown to be consistent and asymptotically normal. We also establish a rate of convergence for the estimator of the slope function. In addition, we propose asymptotically more efficient estimators for the parameter components. The numerical performance of the proposed methods is evaluated via simulation studies and an analysis of an AIDS data set.","Functional data are realizations of random elements that take values in a function space. Statistical methodology dealing with such data is called functional data analysis (FDA). FDA has many useful applications in chemometrics, econometrics, medical research, etc., and has received great attention over the past two decades. See, e.g., the monographs by Ramsay and Silverman, 2005, Ferraty and Vieu, 2006, Horváth and Kokoszka, 2012, and Hsing and Eubank (2015), and articles by Yao et al., 2005a, Yao et al., 2005b, Müller, 2005, Li and Hsing, 2010, Morris, 2015, Wang et al., 2016, and Zhang and Wang (2016).====The purpose of this paper is to extend two-sample functional linear models with scalar responses proposed by Xu et al. (2019) to functional responses. To be more specific, assume that ==== and ==== are two stochastic processes defined on a compact interval ==== of ====, ==== is a ====-dimensional vector of stochastic process on the same domain ====, and ==== is a Bernoulli random variable taking values in ==== with ====. We consider a regression model of the form ====where ==== is an unknown parameter, ==== is an unknown parameter vector of dimension ====, ==== is a random error process with zero mean and a finite second moment that is independent of the covariates ====, and ==== is an unknown function from ==== to ====, where ==== is the set of square-integrable functions defined on ====. Furthermore, we assume that ==== and ==== are independent.====Note that ==== can be regarded as a dummy variable and the model (1.1) is designed to account for two groups for the regression analysis. In the first group (====), the relationship between ==== and ==== is described by ====. In the second group (====), this relationship changes to ====. The model (1.1) combines two functional models with similar features. For the AIDS data set analyzed in Section 6, the patterns of CD4 and the viral load of the two groups show similarities, so we may combine two groups, which may benefit patients because they and doctors can choose a cheaper and lower side-effect treatment. Such a combination may also increase power because the sample size of the combined group becomes larger. For independent data, the similar topic in semiparametric comparison regression has been studied by Schick (1993). In this paper, we assume that there is a scaling transformation between the two regression functions, and a linear relationship between ==== and ====. The proposed methods may be extended to the case that ==== and ==== also depend on ==== with extra efforts and assumptions.====Since ==== lies in an infinite-dimensional space, nonparametrically estimating ==== may suffer from the curse of dimensionality. Therefore, we impose structural constraints on ====; i.e., we impose the following linear relationship between ==== with ====: ====with an unknown function ==== and an unknown bivariate function ====. As a result, the two-sample functional linear models with functional responses can be formulated as ====The model in (1.2) is an extension of the model studied by Xu et al. (2019). If ====, then (1.2) reduces a partial functional linear model: ====To the best of our knowledge, there is limited literature to study model (1.3) like Scheipl et al. (2015). Mostly related to (1.3) are partial functional linear models with scalar responses that characterize the linear relationship between a scalar response and mixed-type covariates, including both a vector-valued and function-valued random variables, and it has been widely studied recently, see, e.g., Shin, 2009, Shin and Lee, 2012, Kong et al., 2016a, Kong et al., 2016b, and Xu et al. (2020). Kong et al. (2018) studied the partial functional linear Cox regression for censored outcome.====Furthermore, if ====, then (1.3) reduces to functional linear models with functional responses: ====, which can be treated as an extension of traditional multivariate linear regressions. This model has been studied in the literature (He et al., 2000, Ramsay and Silverman, 2005, Yao et al., 2005b, He et al., 2010, Lian, 2015, Sun et al., 2018, Imaizumi and Kato, 2018). Ramsay and Silverman (2005, Chapter 16) used penalized least squares method and B-spline to fit this model, and applied it to analyze the Canadian weather data. He et al. (2000) showed that estimating the slope function ==== involves solving an ill-posed inverse problem. Yao et al. (2005b) studied this model for sparse and irregular sampled functional data by functional principal component analysis (FPCA). He et al. (2010) proposed an estimator of ==== based on the functional canonical correlation analysis. Lian (2015) and Sun et al. (2018) assumed that ==== lies in a reproducing kernel Hilbert space and established a minimax rate of convergence for prediction. Recently, Imaizumi and Kato (2018) proposed two approaches to estimate ==== based on FPCA, and derived the rates of convergence and studied their minimax optimality.====Our focus is on the estimation of the parameters ==== and ====, and the functions ==== and ====. The idea we adopt is the least squares method and FPCA (Dauxois et al., 1982, Yao et al., 2005a, Hall and Hosseini-Nasab, 2006, Hall et al., 2006, Li and Hsing, 2010, Li et al., 2013, Goldsmith et al., 2013, Yan et al., 2020), a popular technique in the FDA literature. For example, Cai and Hall, 2006, Hall and Horowitz, 2007, and Lei (2014) studied the prediction, estimation, and hypothesis testing for scalar-on-function linear models based on FPCA, respectively. Yao and Müller (2010) used FPCA to study functional quadratic regression models for both densely and sparsely sampled longitudinal predictor data. Li and Guan (2014) used FPCA to model the latent spatiotemporal point processes. Note that a fixed and known basis (e.g., spline or Fourier basis) with a roughness penalty approach was also been examined recently by Goldsmith et al. (2011) and Scheipl et al. (2015). However, it is more difficult to develop estimation properties and to make inference for our model than the FPCA method.====In this paper, we first solve a functional population normal equation (He et al., 2010) to obtain explicit representations of ==== and ==== in terms of functional principal component basis of ==== by using a profile least squares method, and establish a population least squares criterion for ==== and ====. As a result, the estimators of ==== and ==== are obtained by minimizing the estimated least squares criterion and an iterative algorithm is proposed for this optimization. The estimators of ==== and ==== are obtained by substituting the unknown quantities with their empirical versions along with a double spectral truncation. Under some regularity conditions, we prove the consistency of the resulting estimators of ==== and ====, and further derive their an asymptotically linear representation and a joint asymptotic distribution. Motivated by the asymptotically linear representation, we further construct new estimators of parametric components, and show they are asymptotically more efficient. We also establish a rate of convergence for the estimator of ====.====The rest of this paper is organized as follows. Section 2 proposes estimation procedures for the parametric and nonparametric components based on FPCA, and the computation of the resulting optimization problem. Section 3 investigates asymptotic properties for the resulting estimators, including consistency and asymptotic normality of the estimators of the parametric component, and the rates of convergence of the estimator of the slope function. Section 4 proposes an asymptotically more efficient estimator for parametric component. In Section 5, we conduct some simulation studies to examine the finite sample performance of the proposed estimation procedures. Section 6 analyzes a data set from an AIDS study. The proofs of the main results are relegated to the appendices. We present several lemmas and their detailed proofs in the online supplemental materials.",Two-sample functional linear models with functional responses,https://www.sciencedirect.com/science/article/pii/S0378375821001038,12 October 2021,2021,Research Article,46.0
"Tung Hung-Ping,Lee I-Chen,Tseng Sheng-Tsaing","Institute of Statistics, National Tsing-Hua University, Hsinchu, 30013, Taiwan, ROC,Department of Statistics, National Cheng Kung University, Tainan, 70101, Taiwan, ROC","Received 26 May 2020, Revised 17 September 2021, Accepted 1 October 2021, Available online 12 October 2021, Version of Record 26 October 2021.",https://doi.org/10.1016/j.jspi.2021.10.002,Cited by (2),"To provide timely lifetime information of manufactured products to potential customers, designing an efficient accelerated degradation test (ADT) is an important task for reliability analysts. In the literature, several papers have addressed a general ====-level ADT design problem (including the determinations of the number of stress levels (====), testing stresses, allocation of test units and termination times) when the underlying degradation path follows an exponential dispersion model. The results are practical and interesting. However, most of those studies only addressed the case of ====. Generally, if a direct proof (such as using the Karush–Kuhn–Tucker conditions) for the case of ==== is intractable, we can consider an indirect proof via the general equivalence theorem.====In this paper, we first propose a conjecture optimal design for a ====-level ADT design problem and then apply the general equivalence theorem to show that this conjecture design turns out to be the global V-optimal design. In addition, an example is used to illustrate the proposed procedure. The main contribution of this work is that this analytical approach can provide reliability analysts with better insight for designing an efficient ADT plan.","Currently, the global market is very competitive. To attract potential customers, timely assessment of a product’s lifetime information is an important task for manufacturers. For highly reliable products, it is difficult to assess their lifetime distributions by using traditional life tests. In this situation, accelerated life tests (ALTs) and accelerated degradation tests (ADTs) are widely used for this purpose. They essentially use higher stress levels (including elevated temperatures or voltages) to extrapolate the lifetime information during normal use. General references for ALTs and ADTs can be found in Meeker and Escobar (2014) and Nelson (2009).====Several authors such as Chernoff (1962), Escobar and Meeker (1995), Meeker and Hahn, 1977, Meeker and Hahn, 1985 have studied how to conduct an efficient (optimal) ALT plan and have outlined practical guidelines for planning ALTs. Similar to designing an efficient ALT plan, the optimal ADT plan includes the determination of the stress levels, total testing (duration) times, and allocation of test units that must be addressed very carefully. When the underlying degradation path follows a Wiener process and prefixes two testing stress levels, Tseng et al. (2014) proposed the optimal allocation of the test units in an ADT design by using three well-known criteria: D-optimality, A-optimality, and V-optimality (Myers, Montgomery, and Anderson–Cook, 2009). Among them, the V-optimality criterion is widely used in ADTs which considers the minimization of variance of the estimated ====th quantile of product’s lifetime distribution under the normal use condition. Lim and Yum (2011) used the V-optimality criterion to address the problem of how to simultaneously determine the optimal settings of the 2-level and 3-level stresses and the allocation of the test units. Furthermore, Tung and Tseng (2019) and Ye et al. (2014) also address the ADT design problem when the underlying degradation path follows a monotonic pattern (such as a gamma or inverse Gaussian process).====It is well-known that Wiener, gamma and inverse Gaussian processes are special cases of an exponential dispersion model (ED; Jørgensen, 1997). Recently, assuming the termination times and total test units are prefixed, Tseng and Lee (2016) and Lee et al. (2020) proposed a unified approach to simultaneously determine the optimal proportion of the sample-size allocation and optimal stress levels of a ====-level ADT plan when the underlying degradation path follows an ED model. When ====, they showed that ==== is the optimal strategy and the corresponding optimal proportion of the sample-size allocation and stress levels are analytically derived by minimizing the asymptotic variance of the estimated ==== quantile (====) of the product’s lifetime. However, in their proof, the Karush–Kuhn–Tucker (KKT) conditions become very complicated and intractable when ====. Therefore, without clearly addressing the optimal setting of ====, the results cannot theoretically be guaranteed to be the global optimal solution. In addition, their proposed procedure can be applied only to the case of an equal number of measurement times for all testing stresses, which may be very restricted for practical applications.====In this paper, we release the assumption of an equal number of measurement times to allow an unequal number of measurement times. Hence, to design a general ====-level ADT plan, we need to determine several decision variables, such as the number of stress levels (====), the stress levels, the allocation of the test units and their termination times, simultaneously. First, when the total number of measurement times is given, we propose a conjecture optimal design for a general ====-level ADT problem and then apply the general equivalence theorem (GET) to analytically show that this conjecture design turns out to be the global V-optimal design. In addition, it demonstrates that two decision variables (the number of measurement times and the allocation of the test units) are nonidentifiable in the proposed procedure. Therefore, we may have infinitely many optimal solutions for an ADT design problem. From the viewpoint of practical applications, we introduce a cost model in such a way that those two variables can be reasonably separated.====The rest of this study is organized as follows. Section 2 introduces the problem formulation. Section 3 provides an explicit expression of the asymptotic variance of ====. Section 4 proposes a conjecture of a V-optimal design. Section 5 briefly introduces the GET and uses the GET to theoretically show that the conjecture design is the global optimal design. Section 6 introduces a cost function to separate the number of measurement times and allocation of test units. Section 7 uses an example to demonstrate the proposed procedure. Finally, some concluding remarks are addressed in Section 8.",Analytical approach for designing accelerated degradation tests under an exponential dispersion model,https://www.sciencedirect.com/science/article/pii/S037837582100104X,12 October 2021,2021,Research Article,47.0
"Wang Xiaodi,Chen Xueping,Lin Dennis K.J.","School of Statistics and Mathematics, Central University of Finance and Economics, Beijing 102206, China,Department of Statistics, Jiangsu University of Technology, Changzhou, 213001, China,Department of Statistics, The Pennsylvania State University, USA","Received 8 November 2020, Revised 5 August 2021, Accepted 22 September 2021, Available online 7 October 2021, Version of Record 25 October 2021.",https://doi.org/10.1016/j.jspi.2021.09.004,Cited by (0)," the design structure inherits efficient variance reduction ability for the estimation from the sliced Latin Hypercubes, and ==== each slice of the design is flexible in run size. Finally, numerical illustrations are provided to corroborate the theoretical results.","Experiments with deterministic computer models are becoming ubiquitous in sciences, engineering, and services for studying complex phenomena (Santner et al., 2003). Here, the phrase ==== refers to representations of physical or other systems of interest that are first expressed mathematically and then implemented in the form of computer programs. Meteorological phenomena, and heat transfer in engineered structures, as examples, are modeled often in the form of large differential equations, and computer programs are written to evaluate the outputs of interest for specified inputs. In practice, the behavior of the model is investigated empirically through a computer experiment, in which output values are collected at different values of inputs. A main goal in computer experiments is to estimate the expected output (an integral) of the computer model given a distribution of inputs. To address this issue, the Latin hypercube design (LHD) (McKay et al., 1979) is frequently used, due to its one-dimensional projection property, simpleness, and flexibility. For some cases, it is desirable to run a computer model in batches, or run multiple computer models based on similar mathematics (Williams et al., 2009). Qian (2012) introduced the sliced Latin hypercube design (SLHD) to address this problem. This design can be divided into multiply slices of smaller LHDs, and each slice corresponds to one batch of a computer model or one model of a multi-model experiment. Other studies for sliced designs can be found in Qian and Wu (2009), Yang et al. (2013), Huang et al. (2014), Xie et al. (2014), Yang et al. (2016), Kong et al. (2018) and Guo et al. (2020).====For computer experiments that each simulation may require several hours or days, the computational cost is generally not affordable if accuracy estimation results are required. Improving the space filling property of present designs is one way to reduce the computational cost (Morris and Mitchell, 1995). However, it can hardly decrease the evaluations exponentially. This study is to address such a challenge by detecting symmetry properties of the model(s). For a multivariate model, symmetry is typically defined as a immutability under any permutation of inputs (Artin, 2010). Then it is generalized to a group symmetry by Zhang et al. (1998) which describes a broader relationship (more than strict equality) of output under given permutations of inputs. For a permutation group ====, the model ==== is called ==== if ====, where ==== is the order of ==== and ==== is a given function on ====. Different choices of ==== can lead to different types of symmetry. For example, let ====, ====, ==== and ====, then it is ====. Such symmetry widely exists in physics, chemistry and engineering fields. For instance, in the analysis of isotropic material structure, the geometry (mesh), boundary constraint or load of the structure is usually found to be invariable about certain coordinate permutations (Newnham, 2005). In order-of-addition experiments, the experimental results have symmetry about the orders that ingredients are added (Peng et al., 2019, Lin and Peng, 2019). If a group symmetry of model is identified, the integral cost can be reduced by shrinking the experimental region. Recently, a symmetrical design (Wang and Chen, 2017) is shown to be effective for detecting the group symmetry of model, which has the group enclosure structure that if ==== belongs to the design, then ==== belongs to the design for ====.====Motivated by the discussions above, we introduce a new class of design, named sliced symmetrical Latin hypercube design (SSLHD) for collective evaluations of computer models. Such a design imposes the group enclosure structure in slices of a SLHD, and thus combines the advantages of the SLHD and symmetrical design. The symmetrical structure of design is used to identify the group symmetry of model, which leads to a decline in the experimental cost. The SLHD-structure of design ensures the sampling efficiency for the integration of the output function. Under our construction algorithm, the proposed SSLHs simultaneously possess three appealing features: ==== maximum flexibility in run sizes of all the slices, ==== optimal one-dimensional projection property in each slice and the whole design, and ==== group enclosure in some slices, which can be used to identify the symmetrical feature of the model(s). We believe that they are suitable for multi-batch computer experiments with the model(s) having symmetries in practice.====The rest of this paper is organized as follows. Section 2 introduces basic notations and definitions for SSLHD. Section 3 derives sampling properties of the resulting designs. Section 4 provides the procedure to construct flexible SSLHDs. Section 5 presents numerical illustrations for the derived theoretical results. We end this paper with some concluding remarks in Section 6.",Sliced symmetrical Latin hypercube designs,https://www.sciencedirect.com/science/article/pii/S0378375821000963,7 October 2021,2021,Research Article,48.0
"Zhou Mo,Ma Yaolan,Zhang Rongmao","School of Mathematical Science, Zhejiang University, Hangzhou, Zhejiang 310007, China,School of Mathematics and Information Science, North Minzu University, Yinchuan, Ningxia 750021, China","Received 19 February 2021, Revised 12 August 2021, Accepted 14 September 2021, Available online 27 September 2021, Version of Record 11 October 2021.",https://doi.org/10.1016/j.jspi.2021.09.003,Cited by (0), and ==== is a sequence of i.i.d. innovations belonging to the domain of attraction of an ====-stable distribution for some ,"Consider the first order autoregressive model (AR(1)): ====Testing for whether ==== has been one of the most intensely studied problems since the pioneer DF statistic was developed by Dickey and Fuller (1979). The classic DF test based on the least squares suffers from low power in finite samples when ==== is close to 1 and ==== is serially correlated. Therefore, several improvements have been proposed to deal with this issue, see augmented Dickey–Fuller (ADF) procedure for autoregressive-moving average (ARMA) errors by Said and Dickey (1984) and PP test by Phillips and Perron (1988) which is a nonparametric correction of DF test. More recently, Jansson and Moreira (2006) and Jansson (2008) used Gaussian power envelopes for point-optimal tests of a unit-root under various trend specifications. Frequency domain methods like the Von Neumann variance ratio (VN) test (see Sargan and Bhargava (1983) and Cai and Shintani (2006)) and wavelet spectral approach by Fan and Gençay (2010) were developed as well. Recently, Zhang and Chan (2018) proposed a portmanteau-type statistics by combining several lags of the autocorrelations to improve the power for serial correlations cases, which is robust to model specification of the difference process and easy to use. To derive the asymptotic distribution of the portmanteau-type statistics, Zhang and Chan (2018) required a fourth moment condition for the noise. However, it is known that heavy-tailed phenomena exist frequently in many economical and financial series.====Compared to the finite-variance case, unit-root test for heavy-tailed noise is less addressed. Some results can also be found in the literature. For example, Chan and Tran (1989) considered the unit-root test for i.i.d. errors, which belong to the domain of attraction of an ====-stable law for ====. They established the strong consistency of the ordinary least-squares (OLS) estimator for ==== and showed that the limiting distribution of the DF test is a functional of ====-stable process. Knight, 1989, Knight, 1991 considered the asymptotic behaviour of the OLS and M-estimates of the autoregressive parameter with infinite-variance innovations. They show that M-estimates have a faster rate of convergence. Ahn et al. (2001) developed limit distributions for the DF test, the Lagrange multiplier, the Durbin–Watson and Phillips-type modified test statistics when the noise ==== is in the domain of attraction of a symmetric ====-stable law. Recently, Samarakoon and Knight (2009) extended the results in Knight, 1989, Knight, 1991 by considering DF tests for an autoregressive model of order ==== (AR(p)) with infinite-variance innovations based on M-estimates. Chan and Zhang (2010) derived the limit distribution of DF test when ==== is a heavy-tailed GARCH(1,1) noise with infinite variances. Arvanitis (2017) developed a functional limit theorem for the DF test when the unit root process has innovations constructed as a pointwise product between an i.i.d. sequence in the domain of attraction of an ====-stable distribution and a positive scaling sequence that has a slowly varying at infinity truncated ==== moment. However, they did not give a procedure for approximating the derived null distribution of the test statistics in practice. Georgiev et al. (2017) analysed theoretical and practical impact of heavy-tailed innovations on popular unit tests including ADF and PP tests in the context of a near-integrated series driven by linear process shocks, which is a linear combination of a thin-tailed distribution and a heavy-tailed distribution. Moreover, they proposed a variant of the ADF test based on the use of Eicker–White standard errors to improve power of the conventional unit root test. The asymptotic distribution of ADF statistic was studied by Cavaliere et al. (2018) when ==== is a linear process driven by i.i.d. infinite-variance innovations, and Zhang and Chan (2021) when ==== is a linear process driven by heavy-tailed GARCH(1,1) errors. However, DF test is usually not powerful for dependent noise, while ADF test requires a parametric regression for the difference process. It is natural to ask whether there exists an easy and direct method for testing the existence of unit-root under infinite-variance noise.====Motivated by the above question, this paper extends the portmanteau-type statistics of Zhang and Chan (2018) to heavy-tailed case. More precisely, we consider the case when ==== is a heavy-tailed linear process, i.e., ==== and ==== are i.i.d. innovations belonging to the domain of attraction of an ====-stable distribution for some ====. Under certain mild assumptions, we show that the limit distribution of the portmanteau-type statistics is a functional of stable processes. Our method not only preserves the easy-to-implemented property of the test in Zhang and Chan (2018), but also improves the size and power of the test for heavy-tailed case. A simulation study confirms the good finite sample performance compared with PP, DF and ADF tests. Applications to the daily world crude oil price and 3-month AA financial commercial paper rate are also given to illustrate the performance of the new test.====The rest of the paper is organized as follows. The proposed methods and its asymptotic properties are presented in Section 2. In Section 3 we outline how the wild bootstrap algorithm can be applied to the proposed testing problem. Simulations are reported in Section 4 and Section 5 illustrates the application to the real data. Section 6 gives a conclusion. Proofs are contained in Appendix.",Portmanteau-type test for unit root with heavy-tailed noise,https://www.sciencedirect.com/science/article/pii/S0378375821000951,27 September 2021,2021,Research Article,49.0
"Wang Jing,Yue Rong-Xian","College of Mathematics and Information Science, Henan Normal University, Xinxiang, 453007, China,Department of Mathematics, Shanghai Normal University, Shanghai, 200234, China","Received 13 January 2021, Revised 15 August 2021, Accepted 29 August 2021, Available online 20 September 2021, Version of Record 1 October 2021.",https://doi.org/10.1016/j.jspi.2021.08.005,Cited by (0),"
 ====. According to these matrices, necessary conditions for the existence of tight asymmetric orthogonal arrays of strengths 2 and 4 described by Mukerjee and Wu (1995) are generalized to the existence of tight asymmetric orthogonal arrays of strength ==== run design is ====-optimal.","An orthogonal array (OA) of strength ====, denoted by ====, is an array of size ====, where ==== is the total number of factors, the first ==== columns have symbols from ====, the next ==== columns have symbols from ====, and so on, with the property in any ==== subarray that every possible ====-tuple occurs an equal number of times as a row (see Hedayat et al. (1999)). If ====, the OA is said to be of high strength. If ====, the OA is said to be a fixed or symmetric OA, denoted by ====; otherwise, it is a mixed or asymmetric OA.====It is known that for an ==== with ==== and ====, these parameters satisfy the following Rao’s inequalities (see Hedayat et al. (1999)):====If ====
 (====) then ====and if ====
 (====) then ====where ====
 ==== denotes a sum over all ====-tuples ==== in ====.====An OA of strength ====, where the size ==== attains Rao’s bound, is called ====. Rao’s inequalities provide only a necessary but not a sufficient condition for the existence of an OA. It is worth remarking that tight OAs are of great importance in design of experiments, since they are optimal fractional factorial plans with the least number of runs. Hence studying such OAs is always of great interest.====OAs play important roles in fields of communication, coding theory, cryptography, computer science, and statistics, especially in the design of experiments. Because of the wide applicability of OAs, many mathematicians and statisticians have devoted themselves to studying the aspects of OAs and have obtained many valuable results (Hedayat et al. (1999), Bierbrauer, 2005, Colbourn and Dinitz, 2007, and the references therein). To check if the construction of an OA is possible, it is required to determine whether a set of the parameters satisfies the necessary condition. Hence it is of theoretical interest to discuss in some detail the issue of existence of OAs. The existence of OAs with strength 2 and symmetric OAs with high strength have been studied extensively. Chen et al. (2014) studied the existence of asymmetric OAs of strength 2 with four and five factors. Subsequently, the existence of such arrays with seven factors was exploited by Pang et al. (2016). Boyvalenkov et al. (2017) proved the nonexistence of a few binary OAs of strengths 4 and 5. However, relatively less work is available on the existence of asymmetric OAs of high strength, especially on the tight asymmetric OAs. Brouwer et al. (2006) illustrated the nonexistence of several asymmetric OAs of strength 3. Mukerjee and Wu (1995) provided the necessary conditions for the existence of tight asymmetric OAs of strengths 2 and 4 and discussed the nonexistence of a class of tight asymmetric OAs of strength 4. Although several useful methods have been proposed for the existence of tight OAs with high strength, it is evident that greater study is needed.====We heuristically append some runs to OAs to construct augmented designs when the number of runs available is only slightly larger than the runs in the OA, and the performance of these designs, is studied under a wide range of optimality criteria. The augmentation of a set of additional runs to a saturated ====-optimal two-level factorial design was explored in Hedayat and Zhu (2003). Chatzopoulos et al. (2011) proved the optimality of designs constructed by OA plus ==== run designs under Type 1 criterion. Tsai and Liao (2011) gave sufficient conditions for a partially replicated ==== mixed factorial design to be ====-, ====- and ====-optimal, which is a generalization of Liao and Chai (2009). Bird and Street (2016) identified conditions for sets of ====
 ==== runs which are adjoined to OA of strength 2, such that the resulting augmented design is ====-optimal for the estimation of the main effects only model, and proposed a question if there exist sets of additional runs from within the OA with the best distance properties.====In this paper, in view of Rao’s inequalities, we first define some extended matrices for OAs of even and odd strengths, respectively, which play a key role in our study results. The contribution of this work is as follows:====(1) Based on the extended matrices of OAs, the necessary conditions for the existence of tight asymmetric OAs of strength ==== are established. The nonexistence of some tight asymmetric OAs of high strength is presented for illustration.====(2) It is shown that an OA of strength ==== plus ==== run design is ====-optimal when the additional ==== runs come from a tight OA of strength ====. Specifically, when ====, the result yields a positive answer to the question proposed by Bird and Street (2016). When the additional ==== runs are not from a tight OA of strength ====, the inner product expression of the corresponding extended row vectors of any pair of the additional ==== runs is given, which is useful to verify ====-optimality.====The remainder of the paper is organized as follows. In Section 2, we provide some preliminaries and notations. Sections 3 Results on tight asymmetric OAs of even strength, 4 Results on tight asymmetric OAs of odd strength present, respectively, the ====-optimality of augmented designs and the necessary conditions for the existence of tight asymmetric OAs of even and odd strengths. We close with a brief discussion in Section 5. For clarity of exposition, all the proofs of the theorems are documented in the supplementary material.",D-optimal augmented designs and the existence of tight orthogonal arrays with high strength,https://www.sciencedirect.com/science/article/pii/S0378375821000926,20 September 2021,2021,Research Article,50.0
"Kawai Tetsuya,Uchida Masayuki","Graduate School of Engineering Science, Osaka University, Toyonaka, Osaka 560-8531, Japan,Center for Mathematical Modeling and Data Science (MMDS), Osaka University, Japan,Japan Science and Technology Agency, CREST, Japan","Received 15 November 2020, Revised 19 August 2021, Accepted 22 August 2021, Available online 3 September 2021, Version of Record 25 September 2021.",https://doi.org/10.1016/j.jspi.2021.08.004,Cited by (1),We consider ,"We consider a ====-dimensional diffusion process satisfying the following stochastic differential equation: ====where ==== is an ====-dimensional standard Wiener process, ====, ====, ==== being compact and convex parameter space, ==== and ==== are known except for the parameter ====. We assume the true parameter ==== belongs to ====. The data are discrete observations ====, where ==== for ====, and the discretization step ==== satisfies ==== and ==== as ====. Let ==== be the law of the solution of (1) and ==== be the restriction of ==== to ====.====Diffusion processes are used as the mathematical models to describe the random development of the phenomena depending on time in many fields such as physics, neuroscience, meteorology, epidemiology and finance. For these models, the data are discretely observed. The statistical inference for ergodic diffusion processes based on discrete observations has been studied by many researchers; see Florens-Zmirou, 1989, Yoshida, 1992, Yoshida, 2011, Genon-Catalot and Jacod, 1993, Kessler, 1995, Kessler, 1997, Uchida and Yoshida, 2012 and reference therein. In particular, Kessler, 1995, Kessler, 1997 proposed the adaptive maximum likelihood (ML) type estimator and joint ML type estimator which has asymptotic efficiency under ====, where ==== is an arbitrary integer with ====. Uchida and Yoshida (2012) presented the polynomial type large deviation of adaptive statistical random fields under ==== and moment convergence of the adaptive ML type estimator with asymptotic efficiency.====The parametric testing problem for ergodic diffusions has been studied as the following joint test: ====
 Kitagawa and Uchida (2014) proposed three kinds of test statistics (likelihood ratio type test, Wald type test and Rao’s score type test) and proved their asymptotic properties. De Gregorio and Iacus, 2013, De Gregorio and Iacus, 2019 constructed the test statistics by means of ====-divergence measure and the empirical ====-distance when ==== and ====.====In this paper, we consider the following set of tests instead of (2): ====The set of tests (3) give more information about parameters than the test (2). The test (2) provides only two interpretations: ==== is rejected or ==== is not rejected. On the other hand, the tests (3) give the following four conclusions: (i) ==== is rejected and ==== is rejected;  (ii) ==== is rejected and ==== is not rejected;  (iii) ==== is not rejected and ==== is rejected;  (iv) ==== is not rejected and ==== is not rejected. We utilize the adaptive ML type estimator of Uchida and Yoshida (2012) and construct three types of test statistics. Furthermore, we prove that these test statistics converge in distribution to the chi-squared distribution under null hypothesis, have consistency of the tests under alternatives and converge in distribution to the non-central chi-squared distribution under local alternatives. For the asymptotic null distribution of adaptive test statistics based on local means for noisy ergodic diffusion processes and consistency of the tests under alternatives, see Nakakita and Uchida (2019).====The paper is organized as follows. In Section 2, notation and assumptions are introduced. In Section 3, we state main results. Two quasi log likelihood functions are constructed and three kinds of adaptive test statistics are proposed. Moreover, their asymptotic properties are shown. In Section 4, we give some examples and simulation results of the asymptotic performance for three types of test statistics for 1-dimensional ergodic diffusion processes. Section 5 is devoted to the proofs of the results presented in Section 3.",Adaptive testing method for ergodic diffusion processes based on high frequency data,https://www.sciencedirect.com/science/article/pii/S0378375821000914,3 September 2021,2021,Research Article,51.0
"Bai Yang,Jiang Rongjie,Zhang Mengli","School of Statistics and Management, Shanghai University of Finance and Economics, Shanghai, China","Received 13 March 2020, Revised 4 August 2021, Accepted 5 August 2021, Available online 26 August 2021, Version of Record 14 September 2021.",https://doi.org/10.1016/j.jspi.2021.08.003,Cited by (1),"In this paper, we consider model averaging for expectile regressions, which are common in many fields. The ","Quantiles and expectiles are informative location measures of probability distributions for a random variable ====. Expectiles are natural extensions of the usual mean, just like quantiles generalize the median. Unlike quantiles, which are percentiles of the cumulative distribution of ====, expectiles incorporate information about the tail expectations of ==== (Efron, 1991). Specially, for ====, the ====th quantile ==== of ==== is ====. Moreover, the ====th expectile ==== of ==== is the solution to the minimization of asymmetrically weighted mean squared errors. That is ====where ==== and ==== is an indicator function. It satisfies ====from which it follows that ==== where ====Therefore, the ====th expectile ==== provides information about the expectation of ====, conditional on ==== in a tail of its distribution. In addition, expectiles, using only iteratively re-weighted least squares, lie in the computational expedience of a sample relative to quantiles (Newey and Powell, 1987). Furthermore, making statistical inference on expectiles is much easier than that on quantiles, and estimating expectiles is more efficient than estimating quantiles since weighted least squares depend on the distance to data points, while empirical quantiles use whether an observation is below or above the variable (Abdous and Remillard, 1995, Daouia et al., 2018).====In risk management, value at risk (VaR) and expected shortfall (ES) are the two most popular risk measures. A risk measure is an estimated amount of capital to be reserved at a given risk level to avoid substantial losses. VaR at ==== can be explained as the maximum potential loss at a given ==== level. However, VaR reports only a quantile and disregards losses beyond the quantile. In addition, VaR is not sub-additive, which contradicts the diversification principle that combining portfolios together can reduce the risk (Acerbi and Tasche, 2002). ES is defined as the conditional expectation of the loss given that the loss exceeds the VaR (Artzner et al., 1999), which can avoid these two drawbacks. However, ES fails to satisfy elicitability (Gneiting and Ranjan, 2011), which is a useful property of risk measures because it makes valid point predictions and forecast performance comparisons possible. Unlike VaR and ES, the expectile is the only risk measure that is both sub-additive and elicitable (Ziegel, 2016). Moreover, expectiles are based on a quadratic loss function, making them more sensitive to the extreme values of a distribution than VaR, which is based on absolute errors. This can be beneficial when measuring potential losses, since investors prefer that a risk measure is sensitive to extreme tail losses. Moreover, expectiles have a clear financial meaning. Following (1), we have ====
 (Bellini and Di Bernardino, 2017) provide a transparent financial meaning for it: the ====th expectile ==== is the amount of money that should be added to a position to produce a pre-specified gain/loss ratio.====An expectile regression estimates the conditional expectiles of a response variable based on a set of regressors. Expectile regressions are widely used in finance, demography, and education; see Taylor (2008), Schnabel and Eilers (2009), and Sobotka et al. (2013). Qualifying the utility of covariates is an essential aspect of an expectile regression. The traditional approach is to first choose one model from a number of candidate models, each containing a different combination of regressors, and make statistical inferences under the selected model. This ignores the uncertainty introduced by a model selection process, and thus, it often underreports the variance in the inferences, as discussed in Hjort and Claeskens (2003) and Wang et al. (2009). Spiegel et al. (2017) discussed several methods for choosing covariates for a semiparametric expectile model. In addition, when the expectile index ==== is very high or very low, expectile estimators tend to be unstable. Thus, this article contributes to the existing literature by providing a model averaging method for estimating expectile regression estimators. It is an integrated progress that avoids ignoring the uncertainty introduced by a model selection process and relying on a single estimator. Furthermore, model averaging frequently yields more accurate predictions of the target variable than model selection.====For linear models, the Mallows-type model averaging can be used after one obtains an unbiased estimator of the squared prediction risk. There is no analytical solution for an expectile regression and the corresponding model averaging estimator is not a linear function of the response variables, therefore we focus on developing a ====-fold cross-validation model averaging criterion for expectile regressions. Such an extension faces several challenges. First, we must establish the theory for the consistency of estimators in a misspecified model with a diverging number of parameters. This is needed to build the asymptotic optimality of our ====-fold cross-validation model averaging estimator since all candidate models can be misspecified. Second, the conditional expected expectile prediction error does not have the usual bias–variance decomposition such that the corresponding expectile loss has a complicated form, and asymptotic optimality based on expectile loss of our proposed method is not well established. Third, we also discuss the situation when the true model is one of these candidate models. Then, we prove the consistency of the resulting model averaging estimators. It is not trivial because of the randomness of the selected weight vector.====Cross-validation criteria are widely used to select weights in model averaging. Hansen and Racine (2012) proposed a jackknife model averaging (JMA) estimation that selects the weights by minimizing a cross-validation criterion for linear models. Their finite-sample results suggest that the estimator is preferable to several other model selection and averaging criteria, especially when the errors are heteroscedastic. Zhang et al. (2013) applied JMA in a linear model with a nondiagonal error covariance structure and lagged dependent variables. Lu and Su (2015) and Ando and Li (2017) extended JMA to quantile regressions and high-dimensional generalized linear models, respectively. Cheng and Hansen (2015) considered forecast combination with factor-augmented regressions based on the leave-====-out cross-validation criterion. Gao et al. (2016) implemented a frequentist model averaging method based on the leave-subject-out cross-validation for longitudinal data models. Liu et al. (2019) and Zhang et al. (2018) developed ====-fold cross-validation model averaging for copula models and functional linear regression models, respectively.====The remainder of this paper is organized as follows. Section 2 describes the expectile regression and the model averaging estimation. The ====-fold cross-validation model averaging criterion for expectile regressions is also proposed. The asymptotic optimality of the proposed method and the estimation consistency are discussed in Section 3. In Section 4, we compare the finite sample performance of the ====-fold cross-validation model averaging estimator with the ====-fold cross-validation model selection estimator, several information criterion-based model selection and averaging estimators. Section 5 presents real data examples. Section 6 concludes. Technical proofs of the main results are given in the appendix.",Optimal model averaging estimator for expectile regressions,https://www.sciencedirect.com/science/article/pii/S0378375821000902,26 August 2021,2021,Research Article,52.0
"Zhao Fanrong,Lin Nan,Hu Wenjuan,Zhang Baoxue","School of Statistics, Capital University of Economics and Business, Beijing, 10070, China,Department of Mathematics and Statistics, Washington University in St. Louis, St. Louis, MO 63130, USA","Received 11 June 2020, Revised 25 July 2021, Accepted 6 August 2021, Available online 21 August 2021, Version of Record 5 September 2021.",https://doi.org/10.1016/j.jspi.2021.08.002,Cited by (0),"Testing the dependence between the response and the functional predictor in a functional linear model is of fundamental importance. In this paper, based on a U-statistic of order two, we develop a computationally more efficient test for lacking of dependence in functional linear regression model. By the martingale ","Functional data, such as curves and images, are ubiquitous in many scientific fields, such as medicine, finance, biology, and chemistry (see Crainiceanu et al. (2009); Gabrys et al., 2010, Leng and Müller, 2006, Yao and Müller, 2010). In this paper, we consider regression problems in which the response variable is either scalar or functional and the predictor is functional. More specifically, we focus on testing if there is a linear relationship between the predictor variable and the response variable. For example, the linear relationship between a functional response and a functional predictor is described by the following functional linear regression model. ====where ==== is the response variable, ==== is a functional predictor with mean function ==== and covariance operator ====. The error term ==== has mean 0 and variance function ====, and is independent of the predictor variable ====. ==== is an unknown intercept, and ==== is an unknown coefficient function in ====. The functional linear model for a scalar response ==== is ====, where both the intercept ==== and the random error ==== are real values. All functions are in the Hilbert space ====. Hereafter, we mainly focus on the fully functional linear model (1.1), and the proposed method can be applied to the scalar response case in a straightforward way.====Denote the regression operator as ====: ====. The aim of this paper is then to develop a test for testing the following hypotheses, ====where ==== is an assigned covariance operator.====For testing in functional linear models, a major idea in the literature is to construct test statistics after dimension reduction on the functional predictor ==== using functional principal component analysis (FPCA), such as Cardot et al. (2003), Cardot et al. (2004), Kokoszka et al. (2008), and Lei (2014). However, performance of these approaches depend on the number of principal components retained, which is decided by some pre-specified threshold. To circumvent the sensitivity to the choice of such thresholds, Su et al. (2017) constructed a test statistic which took into account both the association with the response and the variation along each eigenfunction. For functional responses, Patilea et al. (2016) proposed a general nonparametric test for dependence with a U-statistic being the test statistic. In the context of functional linear model, the FLUTE test by Hu et al. (2020) aims for testing linear dependence specifically. The FLUTE test statistic is a U-statistic of order four. It avoids explicitly estimating the covariance operator of the functional predictor, and achieves better performance especially for small/moderate data sets. However, the FLUTE test can be computationally very costly due to use an order-four U-statistic.====In this paper, we develop a test much faster than the FLUTE test. Our proposed test statistic is still a U-statistic, but has only order-two, and is constructed using a different idea from Patilea et al. (2016). We are partly motivated by the U-statistic-based test in Cui et al. (2018) for high-dimensional linear regression and extend it to functional linear models with increasing number of projections. We further develop the asymptotic theory for our new test, and show that its power is similar to the FLUTE test.====The rest of the paper is organized as follows. In Section 2, we construct a faster U-statistic of order two for testing in the fully functional linear regression model, and prove its asymptotic theory under some regularity conditions. Then the proposed test is applied to the scalar response functional linear model in Section 3. Simulation study is conducted in Section 4 to assess the finite sample performance of the proposed test procedure. Section 5 reports the testing results for the diffusion tensor imaging (DTI) data and Canadian weather data. Section 6 concludes the paper. All the proofs of the main theoretical results are relegated to Appendix.",A faster U-statistic for testing independence in the functional linear models,https://www.sciencedirect.com/science/article/pii/S0378375821000896,21 August 2021,2021,Research Article,53.0
Stein Michael L.,"Department of Statistics, Rutgers University, 501 Hill Center, Busch Campus, 110 Frelinghuysen Rd Piscataway, NJ 08854, United States of America","Received 16 October 2020, Revised 23 July 2021, Accepted 1 August 2021, Available online 10 August 2021, Version of Record 18 August 2021.",https://doi.org/10.1016/j.jspi.2021.08.001,Cited by (0),Empirical distributions in a range of fields are often substantially non-Gaussian but smooth enough to suggest that the ==== has at least several derivatives. ==== of many random variables often have smooth densities even if the random variables are not independent. The main result here generalizes the elementary result that a ,"Anyone who has looked at the empirical distributions of many large univariate datasets has likely noticed how smooth some of these distributions appear to be. An example of this smoothness that I recently encountered and that motivated me to undertake this research is shown in the left panel of Fig. 1, which displays a normal quantile plot of average daily temperature in July for Chicago from 1850–1949 over 50 runs of a climate model that differ only in their initial conditions, yielding ==== observations. The availability of 50 runs of the model provides a dataset size that would be unattainable with observational data. The plot shows that the distribution is modestly but distinctly not normal and appears to be quite smooth, although it is not clear how one might determine how many derivatives the corresponding density actually has. The climate model used is the Community Earth System Model, or CESM, which is a fully coupled deterministic model that simulates the atmosphere, the oceans, land surfaces and other components of the climate system (Hurrell et al., 2013). The specific runs used here are described in Sriver et al. (2015). Because the model is purely deterministic, the smoothness in this distribution is not due to any stochastic element in the model formulation. Furthermore, since there is no measurement error in the model output, the smoothness in the empirical distribution cannot be attributed to smoothness in the distribution of measurement error, as might be the case for some observational data. The climate model runs described in Sriver et al. (2015) cover all months for the years 1850–2100; here I restrict to July to reduce seasonality and restrict to 1850–1949 to reduce effects due to climate change, with the result that the selected temperature values might be thought of as nearly coming from a common distribution. Similar results (not shown) hold for other locations.====To give an example based on observational data rather than model output, consider (nominally) hourly surface pressure measurements taken in Central Park, New York City from 1989-2018 (Vose et al., 2014), available from the National Centers for Environmental Information (station ID GHCND:USW00094728). Surface pressure has weak diurnal and seasonal cycles, so all 163,630 available measurements were used, even though the measurements are closer to daily in the early years and there are sometimes several measurements taken in a given hour. Measurements taken within an hour of each other are strongly correlated, but they should be nearly identically distributed. The right panel of Fig. 1 shows, once again, that the distribution is very smooth but is now distinctly non-Gaussian, with a substantially fatter left tail than a normal distribution.====Thus, it is worthwhile to consider scenarios that would naturally generate smooth but non-normal distributions. The state vector at any given time for the climate system as represented in the model runs used here has over ==== components and the true climate system is effectively infinite-dimensional. The temperature or atmospheric pressure at a given location can then be viewed as a complex function of some past state of the system. To the extent that it makes sense to view the climate system as effectively random due to its sensitivity to initial conditions, we might try to justify the smoothness of these distributions through results on the smoothness of distributions of scalar functions of high-dimensional random vectors. Under quite broad conditions on the distribution for a random vector ==== in high dimensions whose distribution has limited smoothness, nearly all linear projections of ==== have distributions much smoother than the distribution of ==== itself (Peres and Schlag, 2000). However, it is also the case that under different but still broad conditions, most projections of ==== are nearly Gaussian (von Weizsäcker, 1997, Reeves, 2017). In addition, for the climatological quantities considered here, arguments based on linear combinations of random variables are problematic because of the substantial nonlinearities that are present in climate dynamics. Therefore, it is natural to consider nonlinear functions of high-dimensional random vectors. Such functions will not generally be nearly Gaussian. Indeed, Peres and Schlag (2000) proves general results on the smoothness of nearly all projections of measures under families of both linear and nonlinear mappings, but these results do not explicitly identify the exceptional sets and thus do not imply the main result in this work even for linear mappings.====Rather than consider families of projections for which some smoothness property holds for almost every projection in the family, the goal here is to prove that certain specific functions of random vectors have densities with some number of derivatives. In particular, this work generalizes the elementary result that sums of independent random variables have, under appropriate regularity conditions, densities with as many derivatives as the sum of the number of derivatives of each component of the sum. Specifically, here the random variables may be dependent and the function of the random variables may be nonlinear, although it cannot have any stationary points. Obtaining such an explicit result comes at the cost of a correspondingly explicit condition on the density of ====, which is required to have any mixed partial derivative given the existence of the corresponding derivatives along each axis that make up the partial derivative.====Section 2 states the main result; its proof is given in Appendix. This section also considers explicit examples of joint densities and nonlinear functions that satisfy the conditions of the main result as well as some ways to weaken these conditions. Section 3 briefly addresses the use of Fourier methods to obtain results like the one obtained here and considers what might be achievable if the nonlinear function has stationary points. Section 3 also discusses some other possible approaches to justifying the smoothness of densities in practice.",A note on the smoothness of densities,https://www.sciencedirect.com/science/article/pii/S0378375821000884,10 August 2021,2021,Research Article,54.0
Lloyd Chris J.,"University of Melbourne, Carlton, 3053, Australia","Received 9 October 2019, Revised 23 July 2021, Accepted 23 July 2021, Available online 6 August 2021, Version of Record 18 August 2021.",https://doi.org/10.1016/j.jspi.2021.07.014,Cited by (0),"Sequential (or adaptive) designs are common in acceptance sampling and pharmaceutical trials. This is because they can achieve the same type 1 and type 2 error rate with fewer subjects on average than fixed sample trials. After the trial is completed and the test result decided, we need full inference on the main parameter ====. In this paper, we are interested in exact one-sided lower and upper limits.====Unlike standard trials, for sequential trials there need not be an explicit test statistic, nor even ====-value. This motivates the more general approach of defining an ordering on the sample space and using the construction of Buehler (1957). This is guaranteed to produce exact limits, however, there is no guarantee that the limits will agree with the test. For instance, we might reject ==== at level ==== but have a lower ==== limit being less then ====. This paper gives a very simple condition to ensure that this unfortunate feature does not occur. When the condition fails, the ordering is easily modified to ensure compatibility.","Let ==== be a parameter of interest and consider one-sided tests of the form ====where ====. With a group sequential design, data is collected in stages and we may decide, on the basis of all data collected up to the current stage, either ==== or ==== or reserve judgment to a later stage. At the last stage ====, a decision is forced. The rationale for and advantages of group sequential designs are well known (Simon, 1989). They are common in acceptance sampling of industrial processes as well as in clinical trials where there are ethical arguments for early termination.====After the test decision, there is still the issue of summarising the inference about ==== with an estimate and confidence interval. Since the likelihood function is unaffected by the sequential boundaries, the maximum likelihood estimator of ==== is also unaffected though its frequentist performance will be different in a sequential trial (Kunzmann and Kieser, 2017). Approximate likelihood based confidence limits can also be constructed without worrying about the sequential design, though coverage can break down for moderate sample sizes especially when the endpoint is binary. Thus exact methods which guarantee coverage are sought.====There is a well established theory for exact one-sided confidence limits which does not rely on any of the familiar notions of ====-value or test statistic. Such theory is necessary for group sequential trials since they are not expressed in such terms. All that is required is a function ==== that ranks all elements of the sample space. However, there is no guarantee that exact limits generated in this way will respect the result of the test. For instance, the lower limit may be lower than ==== when we have just rejected the null hypothesis ====.====The plan of the paper is as follows. In Section 2 we give a brief generic overview of adaptive designs and then establish the theory of exact upper and lower confidence limits. In Section 4 we prove the main result. While this theory is general, we are mainly interested in binary data where approximate confidence limits can have poor accuracy. Section 5 contains some generic and specific examples.",Exact confidence limits compatible with the result of a sequential trial,https://www.sciencedirect.com/science/article/pii/S0378375821000811,6 August 2021,2021,Research Article,55.0
Prus Maryna,"Otto-von-Guericke University Magdeburg, Institute for Mathematical Stochastics, PF 4120, D-39016 Magdeburg, Germany","Received 8 November 2020, Revised 6 July 2021, Accepted 17 July 2021, Available online 5 August 2021, Version of Record 17 August 2021.",https://doi.org/10.1016/j.jspi.2021.07.012,Cited by (2), regression models and illustrate the results by simple examples.,"The subject of this work is multiple-design problems — optimization problems with optimality criteria depending on several designs simultaneously. Such optimality criteria can be, for example, commonly used design criteria for estimation of unknown model parameters in cases when the covariance matrix depends on several designs (see e.==== ====g. Fedorov and Jones (2005), Schmelter (2007a)). For such criteria the general equivalence theorem proposed in Kiefer (1974) cannot be used directly. In Fedorov and Jones (2005) optimal designs were obtained for specific regression functions. In Schmelter (2007a) particular group-wise identical designs have been discussed.====In this paper we formulate equivalence theorems for two kinds of multiple-design problems: (1) problems on finite experimental regions and (2) problems with optimality criteria depending on designs via information matrices. For both cases we assume the optimality criteria to be convex and differentiable in the designs themselves or in the information matrices, respectively. In case (1) we formulate optimality conditions with respect to the designs directly (as proposed in Whittle (1973) for one-design problems). These results can be useful in situations when design criteria cannot be presented as functions of information matrices (see e.==== ====g. Bose and Mukerjee (2015)). In case (2) optimality conditions are formulated with respect to the information matrices. Therefore, no additional restrictions of the experimental regions are needed.====We apply the equivalence theorems to multiple-group random coefficient regression (RCR) models. In these models observational units (individuals) are assigned to groups. Within one group a single design (group-design) for all individuals is assumed. Group-designs for individuals from different groups are in general not the same. The assumption of the same design for all individuals within one group may be not feasible for some applications. This simplification is needed for analytical purposes. Most of the commonly used design criteria in multiple-group RCR models are functions of several group-designs. The particular case of these models with one observation per individual has been considered in Graßhoff et al. (2012). Mentré et al. (1997) and Dumont et al. (2018) proposed computational methods for determining ====-optimal designs based on the normality assumption. In Prus (2015), ch. 6, models with group-specific mean parameters were briefly discussed. Bludowsky et al. (2015), Kunert et al. (2010), Lemme et al. (2015) and Prus (2020) considered models with particular regression functions and specific covariance structures of random effects. In Entholzner et al. (2005) and Prus and Schwabe (2016) a single design was assumed for all observational units. Other particular cases of multiple-design problems have been considered in Wierich (1989) and Schwabe (1996).====In this work, we propose an analytical approach for determining optimal designs with respect to all convex and differentiable criteria in multiple-design problems. In particular, linear and ====-criteria in the multiple-group models are considered in detail. The proposed approach is based on moment assumptions only (no distributional assumptions are needed).====The paper has the following structure: Section 2 provides equivalence theorems for the multiple-design problems. In Section 3 we apply the obtained optimality conditions to the multiple-group RCR models. In Section 4 we illustrate the results by a simple example. The paper is concluded by a short discussion in Section 5.",Equivalence theorems for multiple-design problems with application in mixed models,https://www.sciencedirect.com/science/article/pii/S0378375821000793,5 August 2021,2021,Research Article,56.0
"Chauvet Guillaume,Goga Camelia","Ensai (Irmar), Campus de Ker Lann, 35170, Bruz, France,Laboratoire de Mathématiques de Besançon, Université de Bourgogne Franche-Comté, 16 Route de Gray, 25000, Besançon, France","Received 9 July 2020, Revised 10 July 2021, Accepted 17 July 2021, Available online 29 July 2021, Version of Record 5 September 2021.",https://doi.org/10.1016/j.jspi.2021.07.011,Cited by (5),"In a finite population sampling survey, auxiliary information is commonly used to improve the Horvitz–Thompson estimators and calibration has been extensively used by national statistical agencies over the last decades for that purpose. This method enables to make ==== with known totals of auxiliary variables and to reduce variance if the calibration variables are explanatory for the variable of interest. Nowadays, it is not unusual anymore to have high-dimensional auxiliary data sets and adding too much additional calibration variables may increase the variance of calibration estimators. We study in this paper the ==== of the calibration estimator with high-dimensional auxiliary data sets and we prove that it may suffer from an additional variability that may not be neglected in certain conditions. We suggest a ==== criterion in the choice of calibration variables. A short simulation study shows that the proposed method may lead to a more parsimonious number of calibration variables with associated weights of smaller variation and no variance inflation.","During the 90s, Deville and Särndal (1992) introduced calibration as a unified way to build weighted estimators for finite population totals ==== of survey variables ==== accounting for auxiliary information ==== with known population totals ====. Under mild assumptions, Deville and Särndal (1992) derive also the asymptotic variance of the calibration estimator and they show that the calibration estimator is highly efficient, namely small asymptotic variance, if the auxiliary information explains well the survey variable ====. Särndal (2007) gives a comprehensive review of the calibration and he states that the calibration should bring “the extra benefit of improved accuracy (lower variance and/or reduced nonresponse bias)”.====Nowadays, with the spread of automatic process for data collection as well as increasing storage capacities, it is not unusual anymore to have very large sets of auxiliary variables. When the number ==== of auxiliary variables used for calibration is large with respect to the sample size ====, we have over-calibration (Guggemos and Tillé, 2010) and the efficiency of the calibration estimator may be deteriorated as remarked by Silva and Skinner (1997) in their simulation study; however, Silva and Skinner (1997) do not give any theoretical justification of this fact. Calibrating on a too large number of auxiliary variables may lead to estimators whose performances are worse than the simple Horvitz–Thompson estimator, even if this latter estimator does not account for additional auxiliary information at the estimation stage. To cope with this issue, several methods have been suggested in the literature such as the penalized calibration (Rao and Singh, 2009, Beaumont and Bocci, 2008) or calibration on principal components of the auxiliary information (Cardot et al., 2017).====We study in this paper the asymptotic efficiency of the calibrated estimator when the sample size ==== as well as the number ==== of auxiliary variables is growing to infinity. We intend in particular deriving the asymptotic variance of the calibration estimator for ==== large and getting more insights into the asymptotic behavior of it in this high-dimensional data-setting. A preliminary work was realized in Chauvet and Goga (2013). We briefly recall in Section 2 the principles of calibration as introduced by Deville and Särndal (1992) with a fixed number ==== of auxiliary variables and in Section 3, we study the asymptotic behavior of the calibration estimator when the number ==== of auxiliary variables is growing to infinity. Under mild assumptions on the sampling design and the survey variable ==== as well as on the auxiliary variables and if ==== with ====, we obtain that the asymptotic variance of the calibration estimator derived for ==== fixed is still valid in this high-dimensional setting. We also obtain that the calibration estimator is ====-consistent for the true total ====. However, if the number ==== is going to infinity with a different rate, then the asymptotic variance of the traditional calibration estimator is no longer valid and an additional term should be considered in the computation of the asymptotic variance of the calibration estimator. This result gives the theoretical justification of the increase of the variance of the calibration estimator as remarked through simulation by Silva and Skinner (1997). A bootstrap criterion for the choice of calibration variables is also suggested, and empirically evaluated in Section 4 through a small simulation study on real data set and with two sampling designs: simple random sampling without replacement and stratified Poisson sampling. Our empirical results suggest that the proposed method leads to a more parsimonious number of calibration variables, with associated weights of smaller variation and no variance inflation. The proofs of results are given in the Appendix.",Asymptotic efficiency of the calibration estimator in a high-dimensional data setting,https://www.sciencedirect.com/science/article/pii/S0378375821000781,29 July 2021,2021,Research Article,57.0
"Leach Justin M.,Aban Inmaculada,Yi Nengjun","Department of Biostatistics, University of Alabama at Birmingham, School of Public Health, 1665 University Blvd, Birmingham, AL 35233, United States of America","Received 5 November 2020, Revised 15 July 2021, Accepted 17 July 2021, Available online 29 July 2021, Version of Record 7 August 2021.",https://doi.org/10.1016/j.jspi.2021.07.010,Cited by (2),"Spike-and-slab priors model predictors as arising from a mixture of distributions: those that should (slab) or should not (spike) remain in the model. The spike-and-slab lasso (SSL) is a mixture of double exponentials, extending the single lasso penalty by imposing different penalties on parameters based on their ====. The SSL was extended to ==== (GLM) for application in genetics/genomics, and can handle many highly ","Variable selection is a long-standing statistical problem in both classical and Bayesian paradigms, and aims to determine which variables/predictors are associated with some outcome(s). Classical statistics often relies on hypothesis testing to select predictors in a (generalized) linear model (GLM), but variability of the resulting final models can result in unacceptable generalizability, despite removing many extraneous variables. Such issues partly motivated the lasso model, which is a penalized model that implicitly performs variable selection by setting many parameter estimates to zero, and decreases the variability of the selected model compared to other common classical approaches (Tibshirani, 1996). Additionally, while traditional GLMs are not identifiable when the number of predictors exceeds the number of the observations, the lasso model is identifiable in most cases. Furthermore, while the lasso was born within a classical framework, its Bayesian interpretation is realized by placing double exponential priors on the “effect” parameters (Park and Casella, 2008).====Bayesian variable selection often employs the spike-and-slab prior framework, which models the distribution of parameters as a mixture: a wide “slab” distribution models “relevant” parameters and a narrow “spike” distribution models “irrelevant” parameters (Mitchell and Beauchamp, 1988, George and McCulloch, 1993). While initial work focused on mixtures of normal priors, other distribution choices are possible. In particular, Roc̆ková and George (2018) introduce the spike-and-slab lasso, a mixture of double exponential priors that trades the single lasso penalty for an adaptive penalty based on the probabilities of inclusion. While the initial spike-and-slab lasso was proposed and described for normal linear regression, Tang et al. (2017) demonstrated a novel computational approach based on the EM algorithm that fits the model for GLMs. One of the primary benefits of the algorithm from Tang et al. (2017) is that it is much faster than traditional Markov Chain Monte Carlo (MCMC) methods, whose computational time can be prohibitive for large-scale data sets, like those found in genomics or neuroimaging studies.====Unlike classical GLM’s, penalized or Bayesian models are usually identifiable when predictors are highly correlated, but require suitably structured priors to use dependence structure in modeling and/or variable selection. This issue has been approached from multiple angles within genomics research. Li and Li (2008) and Pan et al. (2010) extend the lasso to use networks to describe relationships among predictors, while Li and Zhang (2010) take a graph theoretic approach while employing an Ising prior on the model space to handle dependence structures. More recently, Roc̆ková and George (2014) discuss an EM variable selection approach based on spike-and-slab normal priors, which in part explores independent logistic regression priors and Markov random field priors to model dependence in variable selection, also inspired by genetics. Importantly, the above models use structured priors as an avenue for incorporating relevant biological information into models, making them more plausible and improving prediction accuracy.====In this work, we focus specifically on spatially structured priors in situations where it is reasonable to expect “relevant” and “irrelevant” parameters will exhibit spatial clustering, which implies the probability that a parameter should remain in the model will be similar to the respective probabilities of spatially adjacent parameters. Other works have addressed spatially structured variable selection, particularly within neuroscience and functional magnetic resonance imaging (Smith and Fahrmeir, 2007, Quirós et al., 2010, Brown et al., 2014). However, we find that these works tend to focus on using images as the outcomes of interest, e.g., activation across many voxels in the brain, whereas we want to address situations where the outcome of interest for each subject is a scalar value, while treating images/spatially structured data as the predictors rather than outcomes. For example, we may use images to predict or model whether a subject has, or will develop, dementia, which is more naturally achieved in a GLM framework. This subtle shift in focus increases the attractiveness of using penalized models like the lasso for variable selection.====The lasso has two primary downsides: when the number of predictors far exceeds the number of subjects, the number of non-zero parameters cannot exceed the number of subjects, and when predictors are correlated it tends to select one predictor and discard the rest. In part, these issues inspired the elastic net, which compromises between ridge and lasso penalties (Zou and Hastie, 2005). Unlike the ridge penalty, the elastic net solution is sparse, but unlike the lasso penalty it can include more non-zero parameters. This flexibility may be desirable when images are used as predictors, since there are often more (highly correlated) spatial measurements than subjects, and the lasso penalty may be too severe. In addition, while there are circumstances where the lasso is not uniquely identifiable, the elastic net is strictly convex and is always uniquely identifiable (Zou and Hastie, 2005).====In what follows we extend the spike-and-slab lasso to a spike-and-slab elastic net, explicitly incorporate spatial information into variable selection, and fit the model with an adaptation of the computationally efficient EM algorithm from Tang et al. (2017). Section 2 reviews the spike-and-slab lasso GLM and outlines the EM-algorithm used to fit the model. Section 3 introduces an extension of the spike-and-slab lasso GLM that generalizes the model to the elastic net and uses intrinsic autoregressions to incorporate spatial information into variable selection. Section 4 presents a simulation study to demonstrate the potential of the proposed method and examine its properties. Section 5 applies the methodology to Alzheimer’s Disease (AD) classification using data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI). Finally, Section 6 summarizes the findings and discuss their implications.",Incorporating spatial structure into inclusion probabilities for Bayesian variable selection in generalized linear models with the spike-and-slab elastic net,https://www.sciencedirect.com/science/article/pii/S037837582100077X,29 July 2021,2021,Research Article,58.0
Hamura Yasuyuki,"Graduate School of Economics, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan,Faculty of Economics, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan","Received 28 August 2020, Revised 6 July 2021, Accepted 7 July 2021, Available online 18 July 2021, Version of Record 27 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.004,Cited by (1),"In this paper, we consider the problem of estimating the density function of a Chi-squared variable on the basis of observations of another Chi-squared variable and a normal variable under the Kullback–Leibler divergence. We assume that these variables have a common unknown scale parameter and that the mean of the normal variable is also unknown. We compare the ","Bayesian predictive densities have been widely studied in the literature since Aitchison (1975) showed their superiority to plug-in predictive densities under the Kullback–Leibler (KL) divergence. In particular, since Komaki (2001) proved for a normal model with unknown mean that the Bayesian predictive density against the uniform prior is dominated by that against a shrinkage prior as in estimation problems, many researchers have considered Bayesian procedures and Stein’s phenomenon in the predictive density estimation setting. Parallels between estimation and prediction were investigated by George et al., 2006, George et al., 2012 and Brown et al. (2008) in terms of minimaxity and admissibility. The case of unknown mean and variance was treated by Kato (2009) and Boisbunon and Maruyama (2014). Prediction for a 2 × 2 Wishart model was considered by Komaki (2009). Recently, Matsuda and Strawderman (2021) considered the ==== Wasserstein loss instead of the KL divergence for location and location-scale families.====In the point estimation literature, Stein’s phenomenon has also been considered in another setting since Stein (1964) showed that the usual estimator of an unknown variance can be improved upon by using additional information from a normal observation with unknown mean and variance. While Stein (1964) considered a truncated estimator, it was shown by Brewster and Zidek (1974) that the usual estimator is dominated by a smooth generalized Bayes estimator also. Kubokawa (1994) showed that improved estimators can be derived through the unified method of Integral Expression of Risk Difference (IERD). Maruyama (1998) gave a class of priors including that of Brewster and Zidek (1974) to improve on the usual estimator when the mean of the normal distribution is equal to zero. Related hierarchical priors have been shown to be useful in estimating location parameters in the presence of an unknown scale parameter (Maruyama and Strawderman, 2005, Maruyama and Strawderman, 2020a, Maruyama and Strawderman, 2020b).====In this paper, we treat a prediction problem which is analogous to the estimation problem considered by Stein (1964). In particular, predictive density estimation for the Chi-squared distribution is considered. Such a distribution has not been considered as widely as the normal distribution, aside from Komaki (2009) and L’Moudden et al. (2017), who considered the Wishart distribution in the two-dimensional case and the gamma distribution in the case where the parameter space is restricted.====To specify the problem we address, let ==== and ==== be independent normal and Chi-squared variables with densities ==== respectively, for known ==== and ==== and unknown ==== and ====. This is a canonical form of a linear regression model with ==== regressors. Let ==== and ==== be independent future variables with densities ==== for known ==== and the same unknown parameters ====. Let the joint densities of ==== and of ==== be denoted by ====respectively. Then there are three prediction problems of estimating three different predictive densities under the KL divergence.====In general, under the KL divergence, Bayesian predictive densities for future observations are obtained by calculating posterior means of their densities (Aitchison, 1975). Therefore, for the problems (P1), (P2), and (P3), the Bayesian predictive densities ====, ====, and ==== associated with a prior ==== for ==== are given by ==== In particular, when the noninformative prior ==== is assumed for ====, the resulting Bayesian predictive densities are denoted by ====, ====, and ====.====The problem (P1) arises, for example, in estimating the predictive densities of future estimators of regression coefficients. Kato (2009) and Boisbunon and Maruyama (2014) showed that if ====, then ==== is dominated by ==== for ==== suitably chosen. This corresponds to the so-called Stein problem in point estimation of the mean vector with unknown variance. The problem (P3) is a generalized version of the problem (P1) and corresponds to the simultaneous estimation of ==== and ====; an analogous problem under the alpha-divergence was considered by Maruyama and Strawderman (2012). On the other hand, less attention has been paid to the problem (P2), which corresponds to the problem of estimating an unknown variance considered by Stein (1964). In particular, no dominance results specialized to the prediction problem (P2) have been established.====In this paper, we consider the problem (P2). In the context of linear regression, this problem corresponds to prediction for the distribution of the residual sum of squares. In this problem, since the distribution of ==== depends only on ==== with ==== being a nuisance parameter, the Bayesian predictive density ==== with respect to a prior ==== is given by ====In particular, the Bayesian predictive density against the noninformative prior ==== is given by ====and does not depend on the normal observation ====, whose distribution depends on both ==== and ====. We write ==== and use it as a benchmark. Later we prove that ==== is dominated by ==== for some suitable prior ====, which means that ==== can be improved by using additional information about ==== from ====. This is a parallel phenomenon in predictive density estimation to Stein’s phenomenon for the point estimation of a normal variance with unknown mean: Stein (1964) showed that when estimating the variance ==== under the standardized squared error loss, the unbiased estimator ==== can be improved upon by using the information in ====.====In Section 2, after giving some additional examples, we specify the hierarchical shrinkage prior we use for the set of the unknown location and scale parameters and we derive explicit expressions for the Bayesian predictive densities against the noninformative prior and the hierarchical shrinkage prior. In Section 3, we present sufficient conditions for the hierarchical Bayesian predictive density to dominate the Bayesian predictive density based on the noninformative prior under the KL divergence. Their performance is compared by simulation in Section 4. Some concluding remarks are given in Section 5. Proofs of the main results and a supplemental discussion are in the Appendix.",Bayesian predictive density estimation for a Chi-squared model using information from a normal observation with unknown mean and variance,https://www.sciencedirect.com/science/article/pii/S0378375821000719,18 July 2021,2021,Research Article,59.0
"Liu Xialu,Zhang Ting","Management Information Systems Department, San Diego State University, San Diego, CA 92182, United States of America,Department of Statistics, University of Georgia, Athens, GA 30602, United States of America","Received 19 October 2019, Revised 1 July 2021, Accepted 7 July 2021, Available online 17 July 2021, Version of Record 28 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.006,Cited by (2),We consider estimating a factor model for high-dimensional time series that contains structural breaks in the factor loading space at unknown ==== and a real data application are presented to illustrate the proposed estimators perform well.,"High-dimensional time series has been emerging as a common and important data type in applications from a number of disciplines, including climate science, economics, finance, medical science, and telecommunication engineering among others. Although numerous statistical methods and their associated theory have been developed for the modeling and inference of time series data, existing results mostly focused on the univariate or finite-dimensional multivariate case. The problem of extending existing results developed under low-dimensional settings to handle high-dimensional time series, however, is typically nontrivial and requires significant innovations. For example, when the dimension is larger than the length of the observed time series, the commonly used autoregressive moving-average (ARMA) model in its conventional form may face a serious identification problem as commented by Lam et al. (2011). To handle the phenomenon of high dimensionality, one typically resorts to certain sparsity-type conditions for the purpose of dimension reduction. For example, when considering vector autoregressive (VAR) models in the high-dimensional setting, one typically needs to assume that the coefficient matrices are sparse in a suitable sense in order to obtain their meaningful estimators; see for example Basu and Michailidis (2015), Davis et al. (2016) and references therein for research results in this direction.====Unlike the aforementioned sparse VAR approach that aims at extending existing parametric time series models to their sparse high-dimensional counterparts, a popular approach in the literature for modeling high-dimensional time series is through the use of a factor model; see for example Chamberlain and Rothschild (1983), Stock and Watson (1998), Bai and Ng, 2002, Bai, 2003 and Forni et al. (2004) among others. The approximate factor model is one of the most widely used models discussed by Bai and Ng, 2002, Bai, 2003, and it assumes that most of the variation in high-dimensional time series data can be explained by a few of factors. Serial dependence is allowed to exist in both the factor and noises but the cross-sectional dependence in the noise has to satisfy the condition that ==== for any ====, where ==== is the ====-th entry in the covariance matrix of the noise process at time ====, ==== is a positive constant, ==== is the dimension of time series, and ==== is the time length. The common component in such factor models is asymptotically identifiable when the number of time series goes to infinity. On the other hand, Lam et al. (2011) proposed an alternative way to define the factor model for time series data. In their model, the common factors are now viewed as the force that drives all the dynamics and is used to explain the serial dependence in the data. The noise process in this setting can exhibit a strong notion of cross-sectional dependence with ==== for ==== and ====, and the common component in the resulting factor model becomes identifiable no matter whether the number of time series grows to infinity with the time length. Therefore, both classes of factor models have been proven to be useful in different scenarios.====Change-point detection in the approximate factor model has been well investigated; see for example Breitung and Eickmeier (2011), Chen et al. (2014), Han and Inoue (2015), Barigozzi et al. (2018), Ma and Su (2018) and references therein. However, the model proposed by Lam et al. (2011) in the change-point setting has not been much explored. Related works in this direction include (Liu and Chen, 2016), which modeled change points as regime shifts between different states of a hidden Markov chain, and Liu and Chen (2020), which discussed a threshold variable approach to modeling the change-point mechanism. The major goal of the current paper is to consider estimating the recently proposed factor model of Lam et al. (2011) in the change-point setting, while not imposing additional structural assumption on the underlying change-point mechanism.====Change-point detection for high-dimensional time series become popular recently. Cho and Fryzlewicz (2012) used the nonparametric locally stationary wavelet model to estimate the number and locations of change points. Xie et al. (2013) described a new approach and introduced the multi-scale model to detect breaks for data with missing values. Cho and Fryzlewicz (2015) proposed the sparsified binary segmentation algorithm to segment the second-order structure of a time series. Cho (2016) used the double CUSUM statistic combined with the binary segmentation algorithm to examine the breakpoints. The existing methods aiming at identifying abrupt changes for high-dimensional time series with/without a factor structure, require the data to be stationary or ‘close’ to a stationary process within regimes, which is rather restrictive. The algorithm we proposed in this paper focuses exclusively on the changes of the factor loading space and can be applied to the case that the factor and noise processes are non-stationary.====In Section 2, we consider the factor model of Lam et al. (2011) with a single change point, and propose a projection-based change-point estimator whose convergence rate shown in Section 3 depends on an interplay between the dimension of the observed time series and the strength of the underlying factor structure. Furthermore, our results reveal that its asymptotic behavior can be asymmetric in the sense that a larger estimation error can occur toward the regime with weaker factor strength. Based on the proposed estimator for the structural break location, we also consider the problem of estimating the factor loading spaces before and after the structural break. We show that the proposed estimators for change-point location and loading spaces are still consistent when the numbers of factors are correctly estimated or overestimated. Section 4 describes the algorithm to identify and locate multiple change points when the number of change points is unknown. Compared with existing results on change-point detection of high-dimensional time series, one advantage of the current paper is that the stationarity assumption for the factor or noise processes is not necessary and as a result our method performs well when the observed data are non-stationary within regimes. It can be seen from our simulation results in Section 5 that existing results on multiple change-point detection developed for high-dimensional time series without a factor structure may struggle in detecting and locating the change points when the observed process is non-stationary within regimes, while the proposed algorithm works reasonably well. The performance for the proposed methods are further illustrated in Section 6 using real data, and Section 7 concludes the paper. Technical proofs are deferred to Appendix.",Estimating change-point latent factor models for high-dimensional time series,https://www.sciencedirect.com/science/article/pii/S0378375821000732,17 July 2021,2021,Research Article,60.0
"Lyu Ziyang,Welsh A.H.","School of Mathematics and Statistics, University of New South Wales, NSW 2052, Australia,Research School of Finance, Actuarial Studies and Statistics, Australian National University, ACT 2601, Australia","Received 30 June 2020, Revised 19 April 2021, Accepted 9 July 2021, Available online 17 July 2021, Version of Record 28 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.009,Cited by (1),"This paper establishes asymptotic results for the maximum likelihood and restricted maximum likelihood (REML) estimators of the parameters in the nested error regression model for clustered data when both of the number of independent clusters and the cluster sizes (the number of observations in each cluster) go to infinity. Under very mild conditions, the estimators are shown to be asymptotically normal with an elegantly structured ====. There are no restrictions on the rate at which the cluster size tends to infinity but it turns out that we need to treat within cluster parameters (i.e. coefficients of unit-level covariates that vary within clusters and the within cluster variance) differently from between cluster parameters (i.e. coefficients of cluster-level covariates that are constant within clusters and the between cluster variance) because they require different normalisations and are asymptotically independent.","Regression models with nested errors (also called random intercept or homogeneous correlation models) are widely used in applied statistics to model relationships in clustered data; they were introduced for survey data, by Scott and Holt (1982) and Battese et al. (1988), and, for longitudinal data, by Laird and Ware (1982). The models are usually fitted (see Harville (1977)) by assuming normality and computing maximum likelihood or restricted maximum likelihood (REML) estimators. As these estimators are nonlinear, asymptotic results provide an important way to understand their properties and then to construct approximate inferences about the unknown parameters. The usual asymptotic results applied to these estimators from Hartley and Rao, 1967, Anderson, 1969, Miller, 1977, Das, 1979, Cressie and Lahiri, 1993, and Richardson and Welsh (1994) increase the number of clusters while keeping the size of each cluster fixed or bounded. However, there are theoretical problems (e.g. in estimating and predicting functions of the random intercepts, see Jiang (1998)) for which both the number of clusters and the cluster sizes need to increase. In addition, there are many applications, particularly with survey data, with large cluster sizes; for example, Arora and Lahiri (1997) give an example with ==== clusters and cluster sizes ranging from ==== to ==== and such examples are common in analysing poverty data (Pratesi, 2016). We analyse an example in Section 4 with ==== clusters ranging in size from ==== to ====. Therefore, in this paper, we study the asymptotic properties of normal-theory maximum likelihood and REML estimators of the parameters in the nested error regression model as both the number of clusters and the cluster sizes tend to infinity.====Suppose that we observe on the ====th unit in the ====th cluster the vector ====, where ==== is a scalar response variable and ==== is a vector of explanatory variables or covariates, ====, ====. The nested error regression model specifies that ====where ==== is the intercept, ==== is the slope parameter, ==== is a random effect representing a random cluster effect and ==== is an error term. We assume that the ==== and ==== are all mutually independent with mean zero and variances (called the variance components) ==== and ====, respectively; we do not assume normality. This regression model treats clusters as independent with constant (i.e. homogeneous) correlation within clusters. It is a particular, simple linear mixed model that is widely used in fields such as small area estimation (see Rao and Molina (2015)) to model and make predictions from clustered data, so our results are immediately useful. In addition, its simplicity allows us to use elementary methods to gain insight into exactly what is going on and obtain explicit, highly interpretable results as the cluster sizes increase. These arguments and results form the basis for how to proceed to more complicated cases, with multiple variance components.====When the random effects and errors are normally distributed, the likelihood for the parameters and the REML criterion can be obtained analytically. Irrespective of whether normality holds or not, we refer to these functions as the likelihood and the REML criterion for the model (1) and the values of the parameters that maximise them as maximum likelihood and REML estimators, respectively. For our results, we make very simple assumptions: essentially finite “====” moments for the random effects and errors (instead of normality) and, allowing the explanatory variables to be fixed or random, conditions analogous to finite “====” moments for the explanatory variables. We allow ==== and ==== without any restriction on the rates. We obtain asymptotic representations for both the maximum likelihood and REML estimators that give the influence functions of these estimators, are very useful for deriving results when we combine these estimators with other estimators, and lead to central limit theorems for these estimators and asymptotic inferences for the unknown parameters. The normalisation is by a diagonal matrix which is easy to interpret. These results provide new and striking insights. First, we need to separate and treat within cluster parameters (i.e. coefficients of unit-level covariates that vary within clusters and the within cluster variance ====) differently from between cluster parameters (i.e. coefficients of cluster-level covariates that are constant within clusters and the between cluster variance ====). We make explicit the fact that the information for within cluster parameters grows with ==== and the information for between cluster parameters grows with ==== so they require different normalisations. The asymptotic variance matrix which we obtain explicitly has a very tidy and easy to interpret block diagonal structure. Second, there are good reasons for centring the within cluster covariates about their cluster means and then including the cluster means as contextual effect variables in the between cluster covariates (see for example Yoon and Welsh, 2020 for references) but our asymptotic results (which include both cases) show that increasing cluster size has asymptotically the same effect as the centring (although without increasing the number of between cluster parameters) and also asymptotically orthogonalises the variance components. These apparently simple insights are new and not available from the existing literature.====The few results in the literature that allow both the number of clusters and the cluster size to go to infinity do not give the same insights as our results. Jiang (1996) proved consistency and asymptotic normality of the maximum likelihood and REML estimators for a wide class of linear mixed models allowing increasing cluster sizes. Xie and Yang (2003) obtained results for generalised estimating equation regression parameter estimators with increasing cluster size which potentially relate to our estimators, but their estimators do not include the variance components so the results do not apply to our estimators. The difficulties with trying to apply general results to particular models like (1) are that it can be difficult to understand the conditions and interpret the main result. To illustrate, increasing cluster size in Jiang (1996) is a part of other complicated assumptions and, for particular examples, needed further conditions on the way the cluster size increases, making it difficult to see whether there is any restriction on the relationship between the cluster size and the number of clusters and leaving open questions of whether the conditions are minimal or not. Also, although Jiang did give some nested model examples which satisfy his main invariant class ==== condition, this condition is quite complicated. In terms of their main results, both Jiang (1996) and Xie and Yang (2003) normalise the estimators by the product of general (nondiagonal) matrices, producing results which are difficult to interpret and do not provide the insights our results provide.====The proofs of our results follow standard arguments, something we regard as positive because it helps to compare them with other asymptotic results. Nonetheless, we did not find the results easy to prove. Initially, we struggled with conceptual issues, the complexity of terms involving the cluster size, and trying to bound norms of the matrices that appear in the expansions. We had to consider very simple special cases and then build up to the current case. Key insights came from (i) expressing the response ==== in terms of ==== and ==== to simplify handling the within cluster dependence, (ii) realising that we could simplify the estimating equations (Lemma 2), and (iii) considering the centred case in an interim step that simplified controlling the off-diagonal elements and hence showed how to normalise the second derivative matrix.====We introduce notation to describe the maximum likelihood and REML estimators for the parameters in (1)–(2), specify the conditions and state our main results in Section 2. We discuss the results in Section 3, present numerical results to illustrate our theoretical results in Section 4 and discuss future research in Section 5. Finally, we give the proofs in Section 6.",Increasing cluster size asymptotics for nested error regression models,https://www.sciencedirect.com/science/article/pii/S0378375821000768,17 July 2021,2021,Research Article,61.0
Su Yingcai,"Missouri State University, USA","Received 21 May 2020, Revised 10 May 2021, Accepted 9 July 2021, Available online 17 July 2021, Version of Record 29 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.008,Cited by (0),"A mean zero spatial process ==== on the ====-dimensional Euclidean space ====, where ====, ====, ====
 ====, and ==== is an admissible function on ====. An isotropic spatial process ==== has a bounded range of dependence if ====
 ====. Here we consider a class of isotropic c.f.’s ====, ==== with bounded ranges of dependence, among which there are ====, the classical triangular c.f. on the real line (====), and ====, the spherical c.f. in dimension three (====). For each dimension ==== as a c.f. in higher dimensions is studied. While it is well known that for each ====, ==== is a legitimate c.f. on ==== for all ==== but it is shown that the considered ==== is not a legitimate c.f. on ==== when ====. Thus the spherical c.f. ==== cannot be a c.f. on ==== when ====. The issue of recognition of an isotropic c.f. on ==== is discussed, and simple procedures of constructing isotropic c.f.’s on ==== for every ==== are given. This article serves as one more reminder that caution must be taken concerning the legitimacy of a selected c.f. in the corresponding spatial dimensions.","For an integer ====, let ==== be the ==== dimensional Euclidean space and ==== the Euclidean norm of ====
 ====. Let ==== be a second order mean zero isotropic spatial process on ==== with covariance function (c.f.) ==== defined by ====where ====, for all ====. Here ==== also holds since ==== is assumed to have mean zero. The variance function of ==== is a nonnegative finite constant: ====, for all ====
 ====, since ==== is of second order. The ==== as in (1.1) is real and symmetric: ====, ====. It follows from the Cauchy–Schwarz inequality that ====, for all ====
 ====
 ====. If ====, then ====, ====., for all ====
 ====, which is a trivial case with a degenerate c.f. ====, ====. When ====, as will be assumed so throughout, ==== is the correlation function of ====. The spatial process ==== is continuous in quadratic mean if and only if ==== is continuous everywhere, equivalently, continuous at the coordinate origin ==== of ====. The indicator c.f.: ==== if ====, and ====, otherwise, corresponds to the white noise spatial process on ====. The indicator c.f. is discontinuous at ====, thus, the white noise random field is nowhere continuous in quadratic mean. We refer readers to Adler, 1981, Christakos, 1992, Cramér and Leadbetter, 1967, Lamperti, 1977, Parzen, 1999 and Yaglom, 1987a, Yaglom, 1987b for more properties of c.f.’s of stationary random processes and isotropic spatial processes, and to Christakos, 1992, Porcu et al., 2009 for discussions of c.f.’s in spatial-time models. This article is not intended to discuss spatial inferences and their related sampling design problems such as spatial extreme search, spatial modeling, and spatial prediction as in the respective references Christakos, 1992, Smith, 1994, Stein et al., 1999 and Su (1997). Rather, it serves as one more reminder that caution must be taken concerning the legitimacy of a selected c.f. in the corresponding spatial dimensions.====For each ====, denote by ==== the class of all functions ==== on ==== such that (1.1) holds. Thus, ==== if and only if ==== is the c.f. of a mean zero spatial process ==== on ====. When ====, ==== stands for the class of all functions ==== such that ==== for all ====. The class ==== is closed under operations of time scaling, linear combinations with non-negative coefficients, products, pointwise limits, and integration with respect to (w.r.t.) a bounded non-decreasing function, provided the respective limits and integration exist. If ==== is the c.f. of an isotropic spatial process ==== on ====, then the restriction ==== of ==== to the ====-dimensional hyper-plane: ==== or ==== is a spatial process with the c.f. ==== on ====. This fact implies ==== for all ====. Thus ====.====The indicator c.f. is a member of ==== for all ====. When ====, ==== coincides with the class of all real c.f.’s of mean zero stationary random processes. The classical triangular c.f.: ====, if ====, and ====, otherwise, is a member of ====. A large subclass of ==== is the class of c.f.’s that have a finite range of dependence. A mean zero isotropic spatial process ==== on ==== has a finite range ==== of dependence if ====
 ====. That is, the values of ==== at locations with a distance less than the range ==== are correlated while observations at points more than ==== units apart are uncorrelated, i.e. ==== for all ====. One of the most useful c.f.’s is ====, defined by ====which has a range ==== of dependence. The ubiquitous c.f. ==== is called the spherical c.f. in literature. For the white noise spatial process, ====. The value ==== corresponds to a random field with unbounded range of dependence. The exponential type c.f. such as ====, ====, where ==== and ====, belongs to ====, according to Schoenberg (1938), and is a common example of c.f.’s with ====. Here, the limit of the exponential c.f. as ==== is the c.f. of the white noise spatial process.====If ====, then for every ====, all real numbers ==== and all ====, ====
 ====
 ====
 ====. Thus ==== is a positive definite function on ====. The converse is also true: for each positive definite function ==== on ==== there is a Gaussian spatial process whose correlation function is ==== as explained in many literatures such as Lamperti (1977). Thus ==== is the class of positive definite radial functions on ====. This is the most general characterization of the class ====. Recall that the characteristic function (ch.f.) ==== of an ====-variate random vector is a continuous positive definite function satisfying ====, according to Bochner’s Theorem. Thus the ch.f. of any ====-variate random vector with a spherically symmetric distribution is a member of ==== since the ch.f. of a spherically symmetric distribution is also radial.====But it is worth noting that a class of isotropic c.f.’s is more complex than a class of radial ch.f.’s as those investigated in Cambanis et al. (1983) and Gneiting (1998). A ch.f. is continuous everywhere but a c.f. such as that of a white noise spatial process, needs not be so. Throughout, the spatial process ==== is assumed to be continuous in quadratic mean, that is, the c.f. ==== is continuous at the origin, and ====. Thus ==== can be viewed as a ====-symmetric multivariate ch.f. as termed in Cambanis et al. (1983) and Gneiting (1998) and also as a spherically symmetric multivariate ch.f. We refer readers to Fang et al. (1990) for an account on spherically symmetric multivariate distributions.====In Section 2, we study a useful class of isotropic c.f.’s with a finite range of dependence. The class constructed includes the well known spherical c.f. ==== as in (1.2), and the classical triangular c.f. ==== as well. From this fact and also because of the geometric feature of the constructions, the constructed c.f.’s will also be naturally called spherical c.f.’s. The class can also be viewed as an extension of the spherical c.f. ==== from dimension three to all other dimensions. It will be shown that ==== for all ====, but ==== for all ====. Can a known covariance function ==== in ==== be used to model the dependence of spatial process in dimension ====? For instance, is ==== as in (1.2) still a c.f. in dimensions ==== or ====? The answer to this question is yes if ==== since ====’s are monotonically decreasing, but the answer is no when ====. In Section 3, we address the issue of recognizing a function that belongs to ====, and in Section 4, we discuss constructions of more isotropic c.f.’s of spatial processes on ==== for ====.",On the admissibility of spherical spatial covariance functions in higher dimensions,https://www.sciencedirect.com/science/article/pii/S0378375821000756,17 July 2021,2021,Research Article,62.0
Ryabko Boris,"Federal Research Center for Information and Computational Technologies of Siberian Branch of Russian Academy of Science, Russian Federation,Novosibirsk State University, Russian Federation","Received 12 February 2020, Revised 3 July 2021, Accepted 9 July 2021, Available online 17 July 2021, Version of Record 24 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.007,Cited by (2),"The problem of constructing the most powerful test for ====-value of an optimal test in the case where the alternative hypothesis is a known stationary ergodic source, and then describe a family of tests each of which has the same ==== of the ====-value for any (unknown) stationary ergodic source. This model appears to be acceptable for ==== generated by physical devices that are used in cryptographic data protection systems.","Random number generators (RNG) are widely used in many applications, including cryptographic information protection systems, modelling and simulation systems and computer games. The goal of RNG is to generate sequences of binary digits, which are distributed as a result of throwing an fair coin or, more precisely, obey the Bernoulli distribution with parameters ====. As a rule, for practically used RNG this property is verified experimentally with the help of statistical tests intended for this purpose, see for a review L’Ecuyer (2017) and L’Ecuyer and Simard (2007). Nowadays, there are more than a hundred applicable statistical tests and some of them are a mandatory part of cryptographic information protection systems  (Rukhin et al., 2010). Besides, there are dozens of RNGs based on different algorithms and different physical processes (Herrero-Collantes and Garcia-Escartin, 2017); see for review L’Ecuyer (2017). In such a situation, the natural question is how to compare different tests. Currently, the main method of such a comparison is numerical experiments in which different tests are practically applied to different RNGs (L’Ecuyer, 2017, L’Ecuyer and Simard, 2007, L’Ecuyer and Simard, 2013, Rukhin et al., 2010).====Here we consider the problem of finding optimal tests in the case when the RNG is modelled by a stationary ergodic source. This model appears to be acceptable for binary sequences generated by physical devices that are used in cryptographic data protection systems. We propose the following asymptotic solution to this problem: we first describe the asymptotic behaviour of the ====-value of the optimal test for the case where the probability distribution of the RNG is a priori known, and then describe a family of statistical tests that have the same asymptotic estimates of the ====-value for any distribution (which is not known in advance). More precisely, we show that in both cases, with probability 1, ====, where ==== is the sample generated by a stationary ergodic ====, ==== is the test statistic, ==== is the ====-value, and ==== is the Shannon entropy of the ====.====It turns out that asymptotically optimal tests with the required properties are known (Ryabko and Astola, 2006, Ryabko et al., 2016, Ryabko and Monarev, 2005), and are deeply connected with so-called universal codes. Note that nowadays there are many universal codes which are based on different ideas and approaches, among which we note the PPM universal code (Cleary and Witten, 1984) which is used along with the arithmetic code (Rissanen and Langdon, 1979), the Lempel–Ziv (LZ) codes (Ziv and Lempel, 1977), the Burrows–Wheeler transform (Burrows and Wheeler, 1994) which is used along with the book-stack (or MTF) code (Ryabko, 1980, Bentley et al., 1986, Ryabko et al., 1987), the class of grammar-based codes (Kieffer and Yang, 2000, Yang and Kieffer, 2000) and some others (Drmota et al., 2010, Ryabko, 1984, Louchard and Szpankowski, 1995). All these codes are universal. This means that, asymptotically, the length of the compressed file goes to the smallest possible value, i.e. the Shannon entropy (====) per letter.====The main idea of randomness tests based on universal codes is rather natural: try to “compress” a test sequence by a universal code. If the sequence is significantly compressed, then it is not random, see Ryabko and Astola, 2006, Ryabko et al., 2016, Ryabko and Monarev, 2005 and a short description below.====The rest of the paper is organized as follows. The next section contains the necessary definitions and some basic facts used in the following. Sections 3 Asymptotic behaviour of a, 4 Asymptotically optimal tests for randomness are devoted to the investigation of the Neyman–Pearson test and tests based on universal codes, correspondingly. We see that universal codes play an important role in testing of randomness, so we give a brief description of one of them in Appendix A; the proofs are given in Appendix B.",Asymptotically most powerful tests for random number generators,https://www.sciencedirect.com/science/article/pii/S0378375821000744,17 July 2021,2021,Research Article,63.0
Frolov Andrei N.,"Department of Mathematics and Mechanics, St. Petersburg State University, St. Petersburg, Russia","Received 4 February 2019, Revised 7 July 2021, Accepted 8 July 2021, Available online 16 July 2021, Version of Record 24 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.002,Cited by (1),We investigate an ==== of ==== of large deviations for normalized combinatorial sums. We find a zone in which these ==== are equivalent to the tail of the standard normal law. Our conditions are similar to the classical Bernstein conditions. The range of the zone of the normal convergence can be of a power order.,"Let ==== be a sequence of matrices of independent random variables and ====, be a sequence of random permutations of numbers ====. Assume that ==== has the uniform distribution on the set of permutations of ==== and it is independent of ==== for all ====. Define the combinatorial sum ==== by relation ====Under certain conditions, a sequence of distributions of normalized combinatorial sums converges weakly to the standard normal law. Every such result is called a combinatorial central limit theorem (CLT).====Investigations in this direction have a long history. One can find results on combinatorial CLT in Wald and Wolfowitz (1944), Noether (1949), Hoeffding (1951), Motoo (1957) and Kolchin and Chistyakov (1973). Further, non-asymptotic Esseen type bounds have been derived for accuracy of normal approximation of distributions of combinatorial sums. Such results have been obtained in Bolthausen (1984), von Bahr (1976), Ho and Chen (1978), Goldstein (2005), Neammanee and Suntornchost (2005), Neammanee and Rattanawong (2009), Chen et al. (2011), Chen and Fang (2015), Frolov, 2014, Frolov, 2015a, and in Frolov (2015b) for random combinatorial sums.====Note that if ==== are identically distributed for all ==== and ====, then the combinatorial sum has the same distribution as that of the sum of i.i.d. random variables. This case is well investigated, but one has to take it into account for estimation of optimality of derived results.====Besides some partial cases, combinatorial sums have not independent increments. Hence, it is difficult to use classical methods of proofs for Esseen type inequalities those are based on bounds for differences of characteristic functions (c.f.). One usually applies the Stein method. For combinatorial sums, it yields Esseen type inequalities when random variables ==== have finite third moments. Applying of the truncation techniques, Frolov, 2014, Frolov, 2015a derived generalizations of these results to the case of finite moments of order ==== and infinite second moments. In there, one can also find a variant of combinatorial CLT for ==== without second moments.====Every bound in CLT similar to the Esseen inequality yields results on asymptotic behaviour for large deviations coinciding with that for tail of the normal law in a logarithmic zone. Such results are usually called moderate deviations. Moderate deviations for combinatorial sums have been investigated in Frolov (2017).====In this paper, we derive new results on the asymptotic behaviour for large deviations of combinatorial sums in power zones. Note that ranges of power zones are powers from some characteristic similar to the Lyapunov ratio. Indeed, we deal with non-identically distributed random variables. Even for sums of independent random variables, ranges of zones of the normal convergence depend on the Lyapunov ratios. For identically distributed random variables, this yields that the ranges are powers from the number of summands. But the last case corresponds to the classical theory for sums of independent random variables and it is not new therefore.====In our proofs, we will use the method of conjugate distributions. Note that von Bahr (1976) developed a method to bound distances between c.f.’s of normalized combinatorial sums and normal law. Assuming that random variables are bounded or satisfy certain analogue of the classical Bernstein condition, we conclude that moment generating functions (m.g.f.) of normalized combinatorial sums are analytic in a circle of the complex plane. Adopting Bahr’s method, we will bound the difference between m.g.f.’s in some circle. In view of the analytic property, this will also give bounds for derivatives of m.g.f.’s. Hence, we will arrive at desired asymptotics for m.g.f.’s and their first and second logarithmic derivatives which are means and variations of random variables being conjugate for normalized combinatorial sums. Then we will estimate a closeness of distributions of conjugate random variables and the standard normal law. Using relationship between distributions and conjugate ones, we will derive the asymptotics of large deviations under consideration.",On large deviations for combinatorial sums,https://www.sciencedirect.com/science/article/pii/S0378375821000690,16 July 2021,2021,Research Article,64.0
"Lee Eun Ryung,Cho Jinwoo,Park Seyoung","Department of Statistics, Sungkyunkwan University, South Korea","Received 28 September 2020, Revised 22 June 2021, Accepted 7 July 2021, Available online 16 July 2021, Version of Record 24 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.003,Cited by (0), of the varying coefficient model even when the number of ==== is allowed to increase with a sample size. We develop an efficient algorithm using the ,"As a useful nonparametric modeling method for exploring dynamic changes in data, varying coefficient regression has been used in various applications. There are extensive references in literature, for example, the studies of Hastie and Tibshirani (1993), Chen and Tsay (1993), Fan and Zhang (1999), Cai et al. (2000b), Fan and Zhang (2000), Park et al. (2017), and Park and He (2017), where various settings, including longitudinal and time series data, and extensions to generalized and quantile regression models were considered. An overview of the literature is presented in the following references: Fan and Zhang (2008) and Park et al. (2015). One popular estimation approach for varying coefficient models is kernel smoothing, which facilitates the use of intuition and insight directly from linear regression. In particular, it is well developed in the context of inferring smooth nonparametric coefficient changes (e.g., Fan and Zhang, 2000, Cai et al., 2000a, Fan and Huang, 2005, Xia et al., 2004, Wang and Zhu, 2009, Zhou and Liang, 2009, Lee et al., 2012b, Lee et al., 2012a, Chen and Hong, 2012, Cheng et al., 2013, Wu and Zhou, 2017, Lee et al., 2018).====Utilizing penalization functions, such as the least absolute shrinkage and selection operator (LASSO, Tibshirani, 1996) and smoothly clipped absolute deviation (SCAD, Fan and Li, 2001) have become effective tools for variable selection in the regression analysis. For an overview of variable selection via penalizations, see Fan and Lv (2010), Lee et al. (2019), and the references therein. In recent decades, the success of the penalization methods has been well investigated in linear regression, and the extensions to semiparametric and nonparametric models have recently become the focal point of studies. There is considerable literature on the estimation and variable selection for semiparametric and nonparametric models. Among these, numerous penalized sieve estimations have been well established (e.g., Wang et al., 2008, Ravikumar et al., 2009, Meier et al., 2009, Huang et al., 2010, Raskutti et al., 2012, Wei et al., 2011, Xue and Qu, 2012, Cheng et al., 2014, Noh and Lee, 2014, Klopp and Pensky, 2015, Honda et al., 2019).====In contrast, there is a paucity of literature on penalized kernel smoothing: for finite dimensional models, for example, Wang and Xia (2009a), Kai et al. (2011), Hu and Xia (2012), and Wang and Kulasekera (2012); and for high-dimensional models, only Lee and Mammen (2016) was found, to the best of our knowledge. The limited development in penalized kernel smoothing might be due to computational challenges in localization in kernel smoothing as also discussed by Lee and Mammen (2016), as well as the inherent methodological and technical issues involved. Specifically, in penalized kernel smoothing, the dimension of the optimization problem can be large due to numerous grids selected over the interval ====, even when the number of covariates is moderate. Therefore, it is very desirable to provide a computationally efficient penalized algorithm working in combination with kernel smoothing; in particular, for high-dimensional settings.====In this paper, we develop penalized kernel smoothing techniques that work theoretically and computationally for varying coefficient (VC) quantile models with a fixed or diverging number of variables. In VC quantile regression, some coefficient functions can be zero or invariant, i.e., a nonzero constant function. In this case the direct application of the VC model results in a loss of estimation efficiency for such invariant coefficients. It is an important statistical task to identify the underlying semi-parametric model structure for more accurate estimation. See Section 2.1 for a review on VC quantile model.====A main contribution of this paper is to develop a novel penalized local linear quantile regression that can simultaneously identify zeros, invariant coefficients, and varying coefficient functions for joint structural identification and selection. Notably, the proposed penalization scheme has similar complexities as the other penalized methods for nonparametric models. Note that every nonparametric penalization, including the one proposed in this study, has two tuning parameters, i.e., the penalty parameter and the additional nonparametric tuning parameter; specifically, the bandwidth for kernel methods and the number of basis for sieve methods. In this work, we develop a tuning parameter selection in a computationally efficient manner. Specifically, we derive a plug-in bandwidth selector for the proposed method from high-dimensional kernel theory. We also propose Bayesian Information Criterion for selection of the penalty parameter. Therefore, our algorithm requires only one tuning parameter ==== without sacrificing the computational costs. For the statistical properties, we demonstrate that the proposed method identifies the underlying partially linear VC model with probability tending to one, which asymptotically provides the same estimators for the nonzero coefficients as the estimator using the knowledge from the true model, referred to as “oracle regularized estimator”.====For this asymptotic oracle property, we consider that the number of covariates ==== is allowed to increase with ====, i.e., ==== approaches infinity, which requires different technical approaches. To the best of our knowledge, there exists no penalized kernel quantile regression for such simultaneous model identification and estimation even for fixed dimension. Further, the proposed theoretical development is novel in that it uses a technical argument that is different from other traditional kernel approaches, such as the sub-gradient from the convex optimization literature. Specifically, the high-dimensional kernel quantile regression theory in this paper is among the first and it requires a novel empirical process analysis on a space of function tuples of increasing dimension—refer to the Supplementary materials.====Another main contribution of this paper is to develop a new efficient computation method to implement the proposed method. We develop the penalized kernel smoothing algorithm using the alternating direction method of multipliers (ADMM, Boyd et al., 2011) algorithm with computational convergence guarantee—refer to Theorem 4. The ADMM approaches have been popularly used in efficient computations of numerous penalized methods. To the best of our knowledge, computation using ADMM with convergence guarantee is yet to be considered in the area of penalized kernel smoothing. Each step in the proposed ADMM has a closed-form expression and can be efficiently computed via a parallel implementation. This leads to a significant reduction in the computational time—refer to Section 5.3 in the Supplementary Material, which is desirable, especially for large dimensions. The proposed ADMM idea can be also easily extended to other kernel settings (e.g., Wang and Xia, 2009a, Kai et al., 2011, Zhang and Wu, 2014, Wu and Zhou, 2017, Park et al., 2021).====The remainder of this paper is organized as follows. In Section 2, we introduce the VC quantile regression model and propose penalized kernel smoothing for simultaneous structural identification and estimation in the VC quantile model. In Section 3, we highlight the theoretical properties of the proposed method. In Section 4, we present details on the proposed ADMM algorithm with a computational convergence property and on the tuning parameter selection. In Section 5, we present the simulation studies to evaluate the finite sample properties of the proposed method. In Section 6, we also apply the proposed method to real data sets to illustrate its usefulness. Section 7 concludes the paper. Technical proofs and some additional tables are presented in the Supplementary material.",Penalized kernel quantile regression for varying coefficient models,https://www.sciencedirect.com/science/article/pii/S0378375821000707,16 July 2021,2021,Research Article,65.0
"Al-Kandari Noriah,El Barmi Hammou","Department of Statistics and Operations Research, Kuwait University, Faculty of Science, P.O. Box 5969, Safat 13060, Kuwait,Paul Chook Department of Information Systems and Statistics, Baruch College, City University of New York, New York, NY 10010, United States of America","Received 6 February 2020, Revised 3 July 2021, Accepted 7 July 2021, Available online 16 July 2021, Version of Record 31 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.005,Cited by (1),"The cumulative incidence function (CIF) plays an important role in the comparison of competing risks in a competing risks model. Its value at time ==== from a particular type of risk in the presence of other risks. In this paper we consider the estimation of two CIFs, ==== and ====, corresponding to two competing risks when the ratio ==== is nondecreasing in ","Consider a competing risks model in which a unit or a subject is exposed to two risks at the same time but the actual failure (or death) is attributed to exactly one of them. Suppose that the notional (or latent) lifetimes of a unit or a subject under these two risks are ==== and ====. We do not assume that these two variables are independent and we only observe ====, where ==== is the time of failure and ==== is the cause of failure with ==== if ==== and ==== otherwise. Throughout we assume that ====. Thus the observed data is of the form ====. Let ==== be the distribution function (DF) of ==== which is assumed to be continuous and let ==== be its survival function (SF).====It is often of interest to know whether these two risks are equal or one is more serious than the other on the basis of the competing risks data. Such comparisons are usually made using the CIFs or the cause specific hazard rates (CSHR) corresponding to these risks. The CIF due to risk ==== is a sub-distribution function (SDF) defined as ====with ====. Its CSHR is defined as ====and the overall hazard rate is ====. In the continuous case, the ====th CIF can be expressed in terms of the CSHR by the following relation: ====It is well documented in the literature of order restricted inference that, when the parameters are known to satisfy order constraints, the utilization of these constraints increases the efficiency of the estimation procedures (for more on this, see, for example, the comprehensive monographs of Silvapulle and Sen (2005) and Robertson et al. (1988)). For this reason, when it is reasonable to assume that the CIFs satisfy a given constraint, it is desirable to have estimators that satisfy the same constraint.====Estimation of CIFs under order restrictions and comparisons of competing risks based on their CIFs have been considered by several authors. El Barmi et al. (2004) developed estimators for ==== and ==== when ==== and studied the properties of these estimators. Their work was extended to the ====-competing risks case in El Barmi et al. (2006). The estimation of ==== and ==== when ==== was also developed in El Barmi and Mukerjee (2004).====Several tests are also available in the literature to test equality of CIFs corresponding to competing risks against the alternative that they are linearly ordered. For the case of two risks, Aly et al. (1994) and El Barmi et al. (2004) used very closely related Kolmogorov–Smirnov type statistics for this situation in the continuous case while El Barmi and Kochar (2002) developed a likelihood ratio test for the same problem in the discrete or the grouped data situation. Extensions of these tests to the ====-sample case have been considered in ElBarmi and Mukerjee (2006) and in El Barmi et al. (2006) in the general and in the discrete/grouped data case, respectively.====Most of the methods developed for two risks compare them on the basis of the difference ====. However, and as pointed out in Dauxois and Kirmani (2003), ==== is not a suitable measure for their comparison if interest is in the conditional distribution of ==== given ==== and its conditional distribution given ====. In this case, interest should be in the temporal function ==== since ==== is proportional to ==== and ==== is nondecreasing if and only if ==== is nondecreasing. We note that ==== is nondecreasing if and only if the conditional distribution of ==== given ==== is larger than the conditional distribution of ==== given ==== in the reversed hazard rate ordering. When this is the case, the former distribution is stochastically larger than the latter in particular. For more on the reversed hazard rate ordering and its properties, see Shaked and Shanthikumar (2006).====While there is an extensive literature on the problem of estimation of CIFs under linear ordering, to our knowledge, their estimation when ==== is nondecreasing has not been considered before. Our aim here is to derive the generalized NPMLEs of ==== and ==== under this constraint and show that they are inconsistent. We then develop projection type estimators that are consistent under the same restriction. These new estimators resemble estimators employed by Rojo and Samaniego (1993) and Mukerjee (1996) for estimating two distributions under uniform stochastic ordering where the NPMLE fails also to be consistent as shown in Mukerjee (1996). We also study the properties of the resulting processes, provide a test for ==== against ==== where ====and extend these results to the censoring case. It is worth noting that, even though the estimators we propose resemble the estimators in Mukerjee (1996), his estimators are based on two independent samples from the two distributions while in our case, we use one sample to estimate the two CIFs with the additional constraint ====. For this reason, the results of the weak convergence of his estimators, as shown in Arcones and Samaniego (2000), do not extend to our situation.====The rest of the paper is organized as follows. In Section 2 we derive the NPMLEs of ==== and ==== under (1.3) and show that they are inconsistent. In Section 3 we provide new estimators, prove their strong uniform consistency and study their weak convergence. We show in particular that, when the order restriction is strict, these estimators and the unrestricted estimators converge to the same Gaussian processes. In addition we show that our estimators are obtained by tweaking the NPMLEs and provide an algorithm to compute them. We also provide a test for equality of these CIFs against the alternative that they satisfy (1.3). In Section 4 we extend the results in Section 3 to the situation where, in addition to the two risks, there are other risks serving as a censoring mechanism. In Section 5 we discuss a real life example to illustrate the applicability of the theoretical results developed in the previous sections and compare via simulations the finite performance of the new estimators and the NPMLEs. Throughout we use ==== and ==== to indicate equality in distribution, convergence almost surely and weak convergence, respectively. We will also use ==== to denote the sup-norm in an appropriate space and take ====. The proofs are relegated to the Appendix.",Restricted estimation of the cumulative incidence functions of two competing risks,https://www.sciencedirect.com/science/article/pii/S0378375821000720,16 July 2021,2021,Research Article,66.0
"Fang Junhan,Yi Grace Y.","Department of Statistics and Actuarial Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1,Department Statistical and Actuarial Sciences and Department of Computer Science, Western University, London, Ontario, Canada N6A 3K7","Received 15 May 2020, Revised 17 June 2021, Accepted 2 July 2021, Available online 10 July 2021, Version of Record 30 July 2021.",https://doi.org/10.1016/j.jspi.2021.07.001,Cited by (1),Matrix-variate ====. It is imperative to account for the misclassification effects and select active ,"Logistic regression is an important tool for describing the relationship between the binary response and a vector of covariates. As the advent of new technologies for data acquisition, measurements in a matrix form are constantly available from the bio-medical research. Recently, matrix-variate logistic regression emerges as a useful method to handle such data which captures the row and column effects of the matrix-variate data on the binary response (e.g., Zhou et al. 2013; Hung and Wang 2013; Zhou and Li 2014).====While matrix-variate logistic regression has proven to be useful, its utility has been restricted to handle data with good quality. In applications, however, responses can be subject to misclassification and some covariates with no predictive value may be included in the data, and the ignorance of these features in inferential procedures may yield invalid results (e.g., Neuhaus 1999; Chen et al. 2011; Fang and Yi 2021a). In regression settings with vector-covariates, it has been well understood that ignoring the misclassification effects can seriously bias the inference results. However, to the best of our knowledge, there has been no work on variable selection under the matrix-variate logistic regression model with response misclassification as well as spurious covariates.====In this paper, we address this problem and develop inference methods that simultaneously do variable selection and parameter estimation, in addition to accounting for the misclassification effects. We develop two sets of regularized methods which originate from the likelihood or estimating equation perspective, where the smoothly clipped absolute deviation (SCAD) penalty function is employed. Our development begins with the case where the misclassification probabilities are assumed to be known. This development is useful for conducting sensitivity analyses. In applications, we often encounter misclassification-prone data but lack information to characterize the misclassification process. In such instances, sensitivity analyses enable us to understand the impact of misclassification on inference results, where representative misclassification models together with parameter values are specified. Furthermore, we extend the development to the case where misclassification probabilities are unknown and estimated from a validation sample, a case that is constantly considered in the literature of measurement error models (e.g., Spiegelman et al. 2000; Carroll et al. 2006; Yi 2017). Theoretical results for the proposed methods are established.====The remainder is organized as follows. In Section 2, we introduce the matrix-variate logistic regression model and the misclassification process for the binary response. In Section 3, we propose the first set of methods based on regularized unbiased estimating functions, and establish the asymptotic results for the resulting estimators. In Section 4, we develop the second set of methods which employ regularized observed likelihood functions. In Section 5, we conduct simulation studies to assess the performance of the proposed methods. In Section 6, we present an application to a breast cancer data set. The article is concluded with a discussion presented in the last section.",Regularized matrix-variate logistic regression with response subject to misclassification,https://www.sciencedirect.com/science/article/pii/S0378375821000689,10 July 2021,2021,Research Article,67.0
"Zhang Feipeng,Yang Jiejing,Liu Lei,Yu Yuan","School of Finance and Economics, Xi’an Jiaotong University, Xi’an, 710061, China,Information Technology Department, Bank of Changsha, Changsha, 410005, China,Division of Biostatistics, Washington University in St. Louis, St. Louis, 63110, USA,School of Statistics, Shandong University of Finance and Economics, Jinan, 250014, China","Received 8 November 2020, Revised 10 May 2021, Accepted 26 May 2021, Available online 3 July 2021, Version of Record 24 July 2021.",https://doi.org/10.1016/j.jspi.2021.05.012,Cited by (0),"In this article, we develop a generalized linear–quadratic model with a change point due to a ","Generalized linear models (GLM) assume linear covariate effects on the response variable (McCullagh and Nelder, 1989). However, the association between the response variable and the covariates could be non-linear in many practical applications. In particular, the regression function may take different forms before and after an underlying threshold of some covariates. Such models, known as change point models or segmented models, are applied frequently to many fields in biomedicine, economics, sociology, industry, among others. We illustrate the models in the following two empirical datasets, which motivates us to this article.====The common feature of these examples is that the regression function has a linear–quadratic pattern with an underlying change point due to a covariate threshold. There is a large literature on the change point model under the conditional mean regression, for the structural change regression model  (Andrews, 1993, Andrews, 1996, Bai, 1996, Bai and Perron, 1998, Hall and Sen, 1999, Hansen, 2000, Bai and Perron, 2003, Perron, 2006, Seo and Linton, 2007, Kato, 2009, Lee et al., 2011), and the continuous threshold regression model (Quandt, 1958, Quandt, 1960, Sprent, 1961, Feder, 1975, Gallant and Fuller, 1973, Chappell, 1989, Zhang and Li, 2017b, Zhang and Li, 2017a, Hansen, 2017), among others. Although a wide variety of change point models are available in linear regression framework, only limited works focus on the change point model in the GLM framework. Muggeo (2003) developed a linear approximation estimating algorithm for the segmented regression model in a generalized regression model framework.  Zhou and Liang (2008) proposed estimation method by smoothing techniques for the bent line model under GLM.  Fong et al. (2017) developed a model-robust method for constructing confidence intervals for coefficients in continuous threshold logistic model. Tapsoba et al. (2020) proposed a derivative-free spectral estimating algorithm for the bent line regression with multiple change points for independent and correlated data. However, the aforementioned papers only consider the change point model under GLM with linear–linear pattern, which is not suitable for the linear–quadratic form in our motivated examples.  Pastor and Eliseo (1998) proposed a linear–quadratic logistic regression. Nonetheless, the authors neither consider the other generalized models with linear–quadratic regression function nor give the statistical inference.====In this article, we propose a generalized linear–quadratic model with a change point due to a covariate threshold. To estimate the regression coefficients and the unknown change point, we propose a two-step estimating procedure. The limiting distribution of the proposed estimator is derived by modern empirical process theory, and the estimator of the change point achieves ====-consistency. Moreover, we propose a sup-likelihood ratio test statistic to test for the existence of the change point. The limiting distributions of the test statistic are established. Our test has a higher power in a wide range of scenarios via simulation studies.====The rest of this article is organized as follows. In Section 2, we introduce the generalized linear–quadratic model with a change point due to a covariate threshold, and develop a two-step procedure for estimating the regression coefficients and the unknown change point. A sup-likelihood ratio test procedure for the existence of the change point is also proposed in this section. The simulation studies are conducted in Section 3 and two applications to empirical datasets are presented in Section 4. Section 5 provides the conclusions and the discussions with possible future extensions. Technical proofs are provided in Appendix.",Generalized linear–quadratic model with a change point due to a covariate threshold,https://www.sciencedirect.com/science/article/pii/S037837582100063X,3 July 2021,2021,Research Article,68.0
"Hoberman Steven,Ivanova Anastasia","Department of Biostatistics, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599-7420, USA","Received 19 October 2019, Revised 31 January 2021, Accepted 27 May 2021, Available online 25 June 2021, Version of Record 14 July 2021.",https://doi.org/10.1016/j.jspi.2021.05.009,Cited by (2),", which complements the random allocation proportion. We then use ==== to formulate and prove a statement about conditions for the asymptotic mean entropy of a randomization design to achieve its optimum value. We compare both response-adaptive and non response adaptive randomization designs with respect to the asymptotic mean entropy, connecting the variance of ==== to the asymptotic mean entropy under normality. Then we explore whether both ","The interpretation of between-group comparisons in a clinical trial is facilitated by the creation of treatment groups that are similar to each other in baseline composition. Selection bias is a major impediment to baseline similarity of treatment groups, especially in unmasked clinical trials. Such bias occurs when expected responders are enrolled or denied enrollment based on both the knowledge of treatment to be allocated next (Blackwell and Hodges, 1957), and the baseline attributes of the patients. Thus, one approach to minimizing selection bias is to minimize the predictability of a treatment assignment based on earlier knowledge of the clinical trial. One way to measure predictability is to define a reasonable guessing strategy and then compute the expected number of correctly predicted treatment assignments. Allocation procedures that minimize selection bias against various guessing strategies have been identified (Blackwell and Hodges, 1957, Stigler, 1969). One drawback to using the expected number of correct guesses is that the expected value is just one measure of a distribution. Also, that approach requires assumptions about the distribution of healthiness in the patient population, which are generally difficult to evaluate. Finally, in a sequential clinical trial where a response adaptive randomization design is used, it is not clear what the best guessing strategy for the investigator is, and thus it is not clear how to calculate bias.====A canonical way to measure predictability is to quantify the amount of information the investigator has about the next treatment assignment, given the knowledge of previous treatment assignments and outcomes in the trial. This approach relies on information theory, specifically, Claude Shannon’s definition of entropy (Shannon 1959), which quantifies the amount of information an observer has about the values a random variable may take. The greater the entropy, the less information the observer has. The formula for entropy of a discrete probability distribution taking on ==== values is ====where there are ==== different possible treatments, and ==== is the probability of the ==== treatment being assigned given some prior knowledge about the trial. Entropy can range from zero, when the next treatment assignment is certain, to log(====), when each of the ==== treatments is equally likely. The definition of information is very well established in the statistical literature because it has certain desirable properties. Clearly, we want the entropy of an allocation scheme to be larger, because that would mean that the next treatment assignment is less predictable.====Entropy has appeared in the clinical trials literature before, but has not been used to quantify the overall randomness of the trial. Klotz (1978) used entropy to minimize imbalance in a trial where there were several stratification factors. He proposed assigning the next treatment to minimize the entropy of each treatment assignment with respect to a constraint that is a function of the imbalance. However, the average entropy over the whole trial and how to maximize it were not discussed. Ball et al. (1993) measured treatment assignment uncertainty by entropy and used entropy as a penalty in an objective function that combined the precision of estimation of parameters of interest in a clinical trial. In the objective function, the entropy was multiplied by a penalty factor that was essentially arbitrary. Atkinson (2002) used the idea of Ball et al. (1993) to create allocation design that balances randomness and inference. A more recent paper (Piantadosi, 2005) used entropy as a way of quantifying uncertainty about the treatment effect in a small trial, but it did not explore the concept of using entropy to quantify prior knowledge about upcoming treatments as related to bias. This paper also did not consider trials of arbitrary size.====In Section 2 we develop notation and discuss conceptual foundations. In Section 3 we state the main results of our paper. In Section 4 we consider several randomization designs and response adaptive randomization designs to illustrate theoretical results from Section 3. Section 5 is the discussion section. In Section 6 we consider future research in optimal entropy designs.",The properties of entropy as a measure of randomness in a clinical trial,https://www.sciencedirect.com/science/article/pii/S0378375821000604,25 June 2021,2021,Research Article,69.0
"Chen E,Tang Yu","School of Mathematical Sciences, Soochow University, Suzhou 215006, China","Received 3 April 2020, Revised 30 May 2021, Accepted 16 June 2021, Available online 23 June 2021, Version of Record 9 July 2021.",https://doi.org/10.1016/j.jspi.2021.06.002,Cited by (1),"-discrepancy will be investigated. Theoretical result shows that the wrap-around ====-discrepancies of the saturated orthogonal arrays constructed using the revised method are less than those of the original ones, and asymptotically attain the lower bounds. A series of numerical examples also confirms the effectiveness of the proposed method.","Orthogonal array has been extensively applied in experimental design, combinatorial mathematics, computer science, coding and cryptography, since it was introduced and systematically studied in Rao (1947). Generally speaking, many existing manuscripts, especially those in combinatorial design theory, paid much attention to the existence results of orthogonal arrays or their equivalent designs, for example, Stinson (2003) and Colbourn and Dinitz (2007). Although Hedayat et al. (1999) used one chapter to discuss statistical application of orthogonal arrays, more effort of that book was devoted to construction methods. However, the essential idea about orthogonality could date back to Fisher (1935). In that book, Fisher spent a lot of effort in elaborating the necessity of using mutually orthogonal Latin squares in agricultural experiments. This fact told us that we should investigate the statistical properties of designs in advance before implementing an experiment based on some special configurations. Tang (1993) and Ai et al. (2016) employed the inner structures of orthogonal arrays to construct Latin Hypercube designs with better space-filling properties. However, the resultant arrays are no longer orthogonal. Other researchers have noticed that orthogonal arrays may have different geometrical structures and statistical properties, even though they share the same generalized word-length pattern (Cheng and Wu, 2001) and (Fang and Ma, 2001). Fang and Ma (2001) also emphasized the use of uniformity to evaluate the statistical performance of orthogonal arrays. Later on, Tang et al. (2012) proposed the concept of uniform fractional factorial design to systematically analyze the uniformity properties for three-level regular fractional factorial designs. They established a relationship between the average centered ====-discrepancy and the word-length pattern and employed the level permutation technique to search for three-level uniform orthogonal arrays. The computational load of such a procedure would of course be heavy, even for an orthogonal array with moderate scale. Thus it would be infeasible to utilize such an exhaustive approach, when the size of arrays becomes larger. In this paper, we will directly investigate some uniformity properties for saturated orthogonal arrays and provide a revised construction method to make the resultant orthogonal arrays with less discrepancies. This paper is organized as follows. Section 2 introduces some concepts and notations. Section 3 proposes a revised construction method for saturated orthogonal arrays and compares the wrap-around ====-discrepancies of the original arrays and the revised ones. Section 4 investigates the asymptotical properties of the original and revised saturated orthogonal arrays, respectively. Some numerical results are listed in Section 5. Section 6 includes some conclusion and discussion and proofs of theoretical results are provided in the Appendix.",Uniformity of saturated orthogonal arrays,https://www.sciencedirect.com/science/article/pii/S0378375821000677,23 June 2021,2021,Research Article,70.0
"Kevei Péter,Oluoch Lillian,Viharos László","Bolyai Institute, University of Szeged, Aradi vértanúk tere 1, 6720, Szeged, Hungary","Received 27 April 2020, Revised 31 May 2021, Accepted 16 June 2021, Available online 22 June 2021, Version of Record 2 July 2021.",https://doi.org/10.1016/j.jspi.2021.06.001,Cited by (1),"Denote ====, where ====, ==== is a sequence of integers such that ==== and ====, and ==== are the order statistics of iid random variables ==== with regularly varying upper tail of index ====. The estimator ==== and ==== both for fixed ==== and for ====. We prove consistency for ==== under appropriate assumptions. We obtain both Gaussian and non-Gaussian (stable) limit depending on the growth rate of the power sequence ====. Applied to real data we find that for larger ==== the estimator is less sensitive to the change in ==== than the Hill estimator.","Let ==== be independent identically distributed (iid) random variables with common distribution function ====, ====. For each ====, let ==== denote the order statistics of the sample ====. Assume that ====where ==== is a slowly varying function at infinity and ====. This is equivalent to the condition ====where ====, ====, stands for the quantile function, and ==== is a slowly varying function at 0. For ==== introduce the notation ====The main object of the present paper is the estimate ====of the tail index, where ==== is the usual gamma function. In what follows we always assume that ==== is a sequence of integers such that ==== and ====.====As a special case for ==== we obtain the well-known Hill estimator of the tail index ==== introduced by Hill (1975). For ==== the estimator was suggested by Dekkers et al. (1989), where they proved that ==== a.s. or in probability, depending on the assumptions on ====, and they proved asymptotic normality of the estimator as well. For general ==== the properties of the estimator ==== in (3) were investigated by Gomes and Martins (2001). Under second-order regular variation assumption they proved weak consistency and asymptotic normality of the estimator ====. Segers (2001) considered more general estimators of the form ====for a nice class of functions ====, called ====. Segers proved weak consistency and asymptotic normality under general conditions. More recently, Ciuperca and Mercadier (2010) investigated weighted version of (2). The residual estimator of Segers was further analyzed for special function classes. Paulauskas and Vaičiulis (2017) considered estimators of the form (4) with ====. The classical Hill estimator can be considered as the logarithm of the geometric mean of the variables ====. Based on this interpretation, Brilhante et al. (2013) introduced the ==== ==== ====, Beran et al. (2014) introduced the ====, while very recently Penalva et al. (2020) introduced the ==== ====. For a general overview on the generalizations of the Hill estimator we refer to Penalva et al. (2020).====To the best of our knowledge the possibility ==== in (3) was not considered before, which is the main focus of our paper. The estimate ==== can be considered as ==== as the limit law for the norm of the extremal sample. In this direction Schlather (2001), Bogachev (2006), and Janßen (2010) proved limit theorems for norms of iid samples.====In the present paper we investigate the asymptotic properties of ==== and ==== both for ==== fixed and for ====. Although the focus of the paper is to obtain asymptotics for large ====, in the course we obtain new results for ==== fixed. In Section 2 in Theorem 2.1 we prove strong consistency of the estimator for ==== fixed. Strong consistency was only obtained by Dekkers et al. (1989) for ==== and ====, thus our result is new for general ====. Asymptotic normality was obtained in several papers for different generalizations of the Hill estimator, see e.g. Gomes and Martins (2001), Segers (2001), Paulauskas and Vaičiulis (2017), and Penalva et al. (2020) for more general estimators. In all these results second-order regular variation is assumed. In Theorem 2.4 our assumptions on the slowly varying function ==== are weaker, therefore the asymptotic normality in this generality is new. Our main results are contained in Section 3, where we obtain weak consistency and asymptotic normality when ====. Under appropriate assumptions on the power sequence ==== we prove non-Gaussian stable limit theorems. Section 4 contains a small simulation study and data analysis. Here we show that for larger values of ==== the estimator is not so sensitive to the choice of ====, which is a critical property in applications. The use of larger ==== values was already suggested in Gomes and Martins (2001) for ==== fixed. We illustrate this property on the well-known dataset of Danish fire insurance claims, see Resnick (1997) and Embrechts et al. (1997, Example 6.2.9). Some auxiliary results and the proofs for fixed ==== are contained in Section 5. In Section 6 we analyze the asymptotic behavior of the power sums. These results are extensions of Bogachev’s results in Bogachev (2006), and are needed to prove the limit properties of ==== as ====. Finally, the proofs of the large ==== asymptotics are in Section 7.",Limit laws for the norms of extremal samples,https://www.sciencedirect.com/science/article/pii/S0378375821000665,22 June 2021,2021,Research Article,71.0
"Milhaud Xavier,Pommeret Denys,Salhi Yahia,Vandekerkhove Pierre","Univ Lyon, UCBL, ISFA LSAF EA2429, F-69007, Lyon, France,Aix Marseille Univ, CNRS, Centrale Marseille, I2M, 13288 Marseille cedex 9, France,Université Gustave Eiffel, LAMA (UMR 8050), 77420 Champs-sur-Marne, France","Received 16 April 2020, Revised 6 May 2021, Accepted 26 May 2021, Available online 7 June 2021, Version of Record 22 June 2021.",https://doi.org/10.1016/j.jspi.2021.05.010,Cited by (0),"In this paper, we consider admixture models which are two-component mixture distributions having one known component. This is the case when a gold standard reference component is well known, and when a population contains such a component plus another one with different features. When two populations are drawn from such models, we propose a penalized ","Let us consider the two-component mixture model with probability density function (pdf) ==== defined by ====where ==== is a known pdf, and the unknown parameters are the mixture proportion ==== and the pdf ====. This model, sometimes so-called ==== or ==== model, has been widely investigated in the last decades, see for instance Bordes and Vandekerkhove (2010), Nguyen and Matias (2014), Cai and Jin (2010) or Celisse and Robin (2010) among others. Numerous applications of model (1) can be found in topics such as: (i) genetics regarding the analysis of gene expressions from microarray experiments as in Broët et al. (2004); (ii) the false discovery rate problem (used to assess and control multiple error rates as in Efron and Tibshirani (2002)), see McLachlan et al. (2006); (iii) astronomy, in which this model arises when observing variables such as metallicity and radial velocity of stars as considered in Walker et al. (2009); (iv) biology to model trees diameters, see Podlaski and Roesch (2014); (v) kinetics to model plasma data, see Klingenberg et al. (2017).====In this paper, the data of interest is made of two i.i.d. samples ==== and ==== of size ==== and ==== with respective probability density functions: ====where ==== are the unknown mixture proportions and ==== are the unknown component densities with respect to a given reference measure ====. Given the above model, our goal is now to answer the following statistical problem: ====without assigning any specific parametric family to the unknown components ====’s. The main shape constraint used throughout this paper is, similarly to Bordes and Vandekerkhove (2010), the fact that the ====’s are symmetric with respect to (w.r.t.) a non-null location parameter, ==== there exists ==== such that ====, ====, for all ====.====This problem is a natural extension of a recent work by Pommeret and Vandekerkhove (2019) to the two sample case. Basically our test procedure consists in expanding the two unknown densities in an orthogonal polynomial basis, and then in comparing, with an ==== method, their coefficients up to a parsimonious rank selected according to a data-driven technique detailed later on in the paper.====Our method can be used in many areas as soon as the unknown densities are supposed to be symmetric w.r.t. a location parameter, including the Gaussian case, but also Uniform and Laplace, among others. As a practical illustration of our work, we analyze kinematic datasets from two Milky Way dwarf spheroidal (dSph) satellites: Carina and Sextans, see for instance Walker et al. (2009). More specifically, we consider the heliocentric velocities (HV) of stars in these satellites, which are the velocities defined with respect to the solar system. These measurements are mixed with the HV of stars in the Milky Way. Since the Milky Way is largely observed, see Robin et al. (2003), we can assume that, as required in our model (2), its velocity distribution is perfectly known. One interesting problem is then to compare the HV distributions of both satellites Carina and Sextans through such mixture models with a common Milky Way known component. We are therefore left with a two sample admixture components comparison problem with a common known and well documented component ====, ==== ==== in (2). Moreover, since the ====’s distributions are generally considered as Gaussian in the astronomical literature, we can therefore also reasonably assume that their (technically required) symmetry with respect to a location parameter holds.====The remainder of the paper is organized as follows. In Section 2, we introduce the testing problem and describe our methodology. In Section 3, we state the assumptions and asymptotic results under the null hypothesis, along with the test divergence under the alternative. Section 4 provides details about the adequate polynomial decomposition depending on the nature of the distributions support. In Section 5, we implement a simulation-based study to evaluate the empirical level and power of the test. Finally, Section 6 is devoted to a real-world application based on a kinematic dataset with galactical heliocentric velocities comparisons. A discussion closes the paper, when proofs and additional exploratory simulations involving the Patra and Sen (2016) estimator approach are relegated in Appendix A Proofs, Appendix B Estimation of the asymptotic variance of, Appendix C Additional tables on Monte Carlo experiments.",Semiparametric two-sample admixture components comparison test: The symmetric case,https://www.sciencedirect.com/science/article/pii/S0378375821000616,7 June 2021,2021,Research Article,72.0
"Telschow Fabian J.E.,Schwartzman Armin","Institute of Mathematics, Humboldt-Universität zu Berlin, Germany,Division of Biostatistics, University of California, San Diego, USA,Halıcıoğlu Data Science Institute, University of California, San Diego, USA","Received 11 March 2020, Revised 25 May 2021, Accepted 26 May 2021, Available online 5 June 2021, Version of Record 9 June 2021.",https://doi.org/10.1016/j.jspi.2021.05.008,Cited by (14),We propose a construction of simultaneous confidence bands (SCBs) for functional parameters over arbitrary dimensional compact domains using the Gaussian Kinematic formula of ,"In the past three decades functional data analysis has received increasing interest due to the possibility of recording and storing data collected with high frequency and/or high resolution in time and space. Many methods have been developed to study these complex data objects; for overviews of some recent developments in this fast growing field we refer the reader to the review articles Cuevas (2014) and Wang et al. (2016) and books among others Ferraty and Vieu (2006) and Ramsay and Silverman (2007).====Despite the success of functional data analysis, only recently quantification of uncertainty with simultaneous confidence bands has received increasing attention. The existing methods for construction of simultaneous confidence bands (SCBs) split into three groups. The first group is based on functional central limit theorems (fCLTs) in the Banach space of continuous functions endowed with the maximum metric and evaluation of the maximum of the limiting Gaussian field often using Monte-Carlo simulations with an estimate of the limiting covariance structure, cf. Bunea et al., 2011, Degras, 2011, Degras, 2017, Cao et al., 2012 and Cao et al. (2014). The second group is based on the bootstrap, among others (Cuevas et al., 2006, Chang et al., 2017, Wang et al., 2019) and Belloni et al. (2018). Thirdly, recently the use of a generalized Kac Rice formula for fast construction of SCBs for one dimensional functional data has been proposed in Liebl and Reimherr (2019), which is similar to our proposal, but limited to one dimensional domains.====Except for Liebl and Reimherr (2019) the mentioned methods are computationally expensive, since they either simulate from an estimated limiting field or require to draw many bootstrap samples. This hinders their use for domains of dimension greater than one. Moreover, they often perform poorly on small samples.====In order to construct precise and efficiently computable SCBs for functional parameters over arbitrarily dimensional domains, we propose to use random field theory (RFT). RFT was studied extensively in Adler (1981) and Adler and Taylor (2009), and has been successfully used in the neuroimaging community to control the FWER of statistical 3D images, among others (Worsley et al., 1996, Worsley et al., 2004). More precisely we use the so called Gaussian kinematic formula (GKF) for pointwise ====-distributed random fields (Taylor, 2006, Taylor and Worsley, 2007).====In a nutshell GKFs express the expected Euler characteristic (EEC) of the excursion set of a Gaussian related random field ====, ==== in terms of a finite linear combination of ==== known functions, called Euler characteristic (EC) densities. Here ==== are i.i.d. zero-mean, unit-variance Gaussian fields with twice differentiable sample paths over a nice compact subset of ====. The linear coefficients in this formula are called ==== (LKCs) and depend solely on the domain and the covariance structure of the derivative of ====. Remarkably, the only difference between GKFs for different Gaussian related fields is that the EC densities change, see Adler and Taylor (2009, p.315, (12.4.2)). Takemura and Kuriki (2002) have shown that the GKF for Gaussian fields is closely related to the ==== dating all the way back to Working and Hotelling (1929). The latter has been applied for SCBs in nonlinear regression analysis, e.g., Johansen and Johnstone, 1990, Krivobokova et al., 2010 and Lu and Kuriki (2017). In this sense the GKFs of Taylor (2006) can be interpreted as a generalization of the volume of tube formula for repeated observations of functional data.====Our main contributions are the following. In Theorem 2 we show based on the main result in Taylor et al. (2005) that, asymptotically, the error in the covering rate of SCBs for a function-valued population parameter based on the tGKF can be bounded and is small, if the targeted covering probability of the SCB is sufficiently high. This requires neither Gaussianity nor stationarity of the observed fields. It only requires that the estimator of the targeted function-valued parameter fulfills an fCLT in the Banach space of continuous functions with a sufficiently regular Gaussian limit field. Moreover, it requires consistent estimators for the LKCs. The latter have been studied in Taylor and Worsley (2007) and Telschow et al. (2020). We illustrate the general approach for the special case of SCBs of the population mean curve and the difference of population means for functional signal-plus-noise models, where we allow the error fields to be non-Gaussian. Especially we derive for such models defined over sufficiently regular domains ====, ====, consistent estimators for the LKCs and derive CLTs for them. In order to deal with observation noise we discuss SCBs for scale spaces. In Theorem 8 we give sufficient conditions to have weak convergence of a scale space field to a Gaussian limit extending the results from Chaudhuri and Marron (2000) from regression analysis to repeated observations of functional data. Additionally, we prove that the LKCs of this limit field can be consistently estimated and therefore Theorem 2 can be used to bound the error in the covering rate for SCBs of the population mean of a scale space field.====Scale spaces are not the only way to deal with the observation noise and discrete sampling. For example local polynomial estimators can be used to estimate the population mean (Zhang et al., 2007, Degras, 2011, Zhang et al., 2016). Our proposed construction of SCBs is applicable in these cases provided that the LKCs can be consistently estimated and a bias correction is introduced. Note that in the ultra dense sampling case (Zhang et al., 2016) our developed theory for the SCBs of signal-plus-noise models can be directly used, since the bias introduced by the smoother is negligible. For less dense sampling schemes a bias correction and consistent estimates of the LKCs need to be derived, yet Theorem 2 can then still be applied.====The theory is accompanied by a simulation study using the Rpackage ==== (====patial ====nference for ====andom ====ields), which can be found on ====, and demonstrate the use of SCBs on two different data applications. In the simulation study we compare the performance of the tGKF approach to SCBs for different error fields mainly with bootstrap approaches and conclude that the tGKF approach does not only often give better coverings for small sample sizes, but outperforms bootstrap approaches computationally. Moreover, the average width of the tGKF confidence bands is lower for large sample sizes. As a first application we demonstrate the use of SCBs for scale spaces on a diffusion tensor imaging (DTI) experiment to detect differences in population means between healthy subjects and patients. The second application constructs simultaneous confidence bands for the expected increase in mean summer and winter temperatures over North America obtained from NARCAP simulations (Mearns et al., 2013).",Simultaneous confidence bands for functional data using the Gaussian Kinematic formula,https://www.sciencedirect.com/science/article/pii/S0378375821000598,5 June 2021,2021,Research Article,73.0
"Binette Olivier,Guillotte Simon","Université du Québec à Montréal, 201 Avenue du Président-Kennedy, Montréal (Québec), H2X 3Y7, Canada","Received 12 March 2018, Revised 17 May 2021, Accepted 26 May 2021, Available online 4 June 2021, Version of Record 14 June 2021.",https://doi.org/10.1016/j.jspi.2021.05.007,Cited by (2)," may provide gains over comparable circular density estimators previously suggested in the literature.====From a theoretical point of view, we propose a general prior specification framework for density estimation on ","There is increasing interest in the statistical analysis of non-euclidean data, such as data lying on a circle, on a sphere or on a more complex manifold or metric space. Applications range from the analysis of seasonal and angular measurements to the statistics of shapes and configurations (Jammalamadaka and SenGupta, 2001, Bhattacharya and Bhattacharya, 2012). In bioinformatics, for instance, an important problem is that of using the chemical composition of a protein to predict the conformational angles of its backbone (Al-Lazikani et al., 2001). Bayesian nonparametric methods, accounting for the wrapping of angular data, have been successfully applied in this context (Lennox et al., 2009, Lennox et al., 2010).====Directional statistics deals in particular with univariate angular data and provides basic building blocks for more complex models. Among the most commonly used model for the probability density function of a circular random variable is the von Mises density defined by ====where ==== is the circular mean, ==== is a shape parameter and ==== is the modified Bessel function of the first kind and order ====. This function is nonnegative, ====-periodic and integrates to one on the interval ====. It can be regarded a circular analogue to normal distribution (Jammalamadaka and SenGupta, 2001) (see also Coeurjolly and Le Bihan (2012) for a comparison with the geodesic normal distribution). Mixtures of von Mises densities and other log-trigonometric densities are also frequently used (Kent, 1983). Another natural approach is to model circular densities using trigonometric polynomials ====These densities have tractable normalizing constants, but the coefficients ==== and ==== must be constrained as to ensure nonnegativity (Fejér, 1916, Fernández-Durán, 2004).====For a review of common circular distributions, see Mardia and Jupp (2000) and Jammalamadaka and SenGupta (2001). Notable Bayesian approaches to circular and directional statistics include Ghosh and Ramamoorthi, 2003, McVinish and Mengersen, 2008, Ravindran and Ghosh, 2011 and Hernandez-Stumpfhauser et al. (2017).====In this paper, we introduce a basis of the trigonometric polynomials (1.1) consisting only of probability density functions. Properties shown in Section 2, such as its shape-preserving properties, suggest it as a circular analogue to the Bernstein polynomial densities and we argue that it is particularly well suited to mixture modelling. In Section 3, we use this basis to devise nonparametric priors on the space of bounded circular densities. We compare their posterior mean estimates to other density estimation methods based on the usual trigonometric representation (1.1) in Section 4.====An important aspect of nonparametric prior specification is the posterior consistency property, which entails almost sure convergence (in an appropriate topology) of the posterior mean estimate. In Section 3.2, we thus develop a general prior specification framework that immediately provides consistency of a class of sieve priors for density estimation on compact metric spaces. Particular instances of this framework appeared previously in the literature. For instance, Petrone and Wasserman (2002) obtained consistency of the Bernstein–Dirichlet prior on the set of continuous densities on the interval ====. More recently Xing and Ranneby (2009) (see also Walker (2004) and Lijoi et al. (2005)) have obtained a simple condition for models of this kind ensuring consistency on the Kullback–Leibler support of the prior. As an application, they quickly revisit the problem of Petrone and Wasserman (2002) but without discussing what contains the Kullback–Leibler support. Our main contribution here is the proof that the Kullback–Leibler support of the priors specified in our framework contains every bounded density, without continuity assumptions. Furthermore, we show in Section 3.4 how our framework may be used to obtain posterior contraction rates. The results are related to those of Ghosal (2001) and Kruijer and van der Vaart (2008) in the case of the Bernstein–Dirichlet prior but are stated with more generality. They express posterior contraction rates over classes of smooth densities in terms of a balance between the dimension of the sieves and their approximation properties, as they are accounted for by a sequence of positive linear approximation operators.",Bayesian nonparametrics for directional statistics,https://www.sciencedirect.com/science/article/pii/S0378375821000586,4 June 2021,2021,Research Article,74.0
"Díaz-García José A.,Caro-Lopera Francisco J.","Universidad de Medellín, Faculty of Basic Sciences, Carrera 87 No.30-65, of. 5-103, Medellín, Colombia","Received 9 May 2019, Revised 24 May 2021, Accepted 26 May 2021, Available online 4 June 2021, Version of Record 14 June 2021.",https://doi.org/10.1016/j.jspi.2021.05.011,Cited by (0),A new family of matrix variate distributions indexed by elliptical models is proposed in this work. The termed ,"In recent decades (since the early 80s) it has become increasingly important to consider the dependence of random variables beyond a simple antithesis of the concept of independence, the latter being a fundamental concept in probability theory. In response, a number of methods have been proposed to consider certain types of dependency on random variables with given marginal distributions. Most of these methods have been based on a well-known result of Hoeffding (1940) and Fréchet (1951). A concrete solution to this problem arose as a result of the collaboration between Abe Sklar and Berthold Schweizer. Sklar (1959) demonstrated through the now celebrated theorem that bears his surname, that there is a C function ====; which he termed copula, which establishes the functional relationship between the joint distribution and their one-dimensional marginal distributions.====Alternatively, another group of researchers has answered this problem based on the following procedure: starting from a set of independent random variables with known marginal distributions and proposes a variable change, after which the transformed variables are probabilistically dependent and their corresponding marginal distributions are known. In the univariate case, several bivariate type distributions have been proposed, the list includes the pairs: Gamma–beta, Gamma–Gamma, beta–beta among others. They have been applied in hydrology, finance and other areas, see for example Libby and Novick, 1982, Chen and Novick, 1984, Olkin and Liu, 2003, Nadarajah, 2007, Nadarajah, 2013 and Sarabia et al. (2014) and the references therein. In a general setting, the bimatrix variate type distribution has been studied by several authors, see Olkin and Rubin, 1964, Díaz-García and Gutiérrez-Jáimez, 2010a, Díaz-García and Gutiérrez-Jáimez, 2010b, Díaz-García and Gutiérrez-Jáimez, 2011, and Bekker et al. (2011), and the references therein. In particular, Ehlers (2011) presents a complete exposition of the advances in this difficult topic. As usual the univariate and multivariate cases are the most common in literature, but the matrix variate case is considered in few works.====This article examines the joint density function, ====, of dependent random matrices ====, such that their corresponding marginal densities are not equal, in particular you get ==== when the marginal distribution of ==== has a matrix variate Gamma distribution and ==== can have a matrix distribution ====, Pearson type II, beta type I or beta type II. It is important to emphasise that the joint distribution ==== defines a family of distributions that is specified when defining the particular kernel ==== of the specific elliptical distribution. In addition, the joint matrix density ==== of probabilistically dependent random matrices ====, ====, and ==== is studied when ==== has a matrix variate Gamma distribution, and ==== and ==== have a marginal distribution not necessarily equal, with matrix distributions ====, Pearson type II, beta type I or beta type II, respectively. Other results that consider the inverse of a matrix are also obtained. These shall be termed ====. In Section 2 new results on special Jacobians are derived. The central results are presented in Section 3. Some properties of multimatricvariate distributions are obtained in Section 4. Finally an application of DNA is shown in Section 5.",Multimatricvariate distribution under elliptical models,https://www.sciencedirect.com/science/article/pii/S0378375821000628,4 June 2021,2021,Research Article,75.0
"Liao Yijie,Butler Ronald W.","Alector, Inc., 131 Oyster Point Blvd Suite 600, South, San Francisco, CA 94158, USA,Department of Statistical Sciences, Southern Methodist University, Dallas, TX 75275, USA","Received 4 April 2019, Revised 15 May 2021, Accepted 21 May 2021, Available online 28 May 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.jspi.2021.05.006,Cited by (0)," using expansions as ====. Using Dirichlet process priors, the ==== which accommodates the most general setting with arbitrary ties and right censoring. In the previously considered case of gamma process priors, the approach extends the results of Kalbfleisch (1978) to deal with arbitrary arrangements of ties and also provides a rigorous justification for his posterior expressions. As in Kalbfleisch (1978), we show that the leading terms of both expansions approximate Cox’s partial likelihood when there are no ties and the process priors are diffuse. The ====-grid likelihood approach is similar in concept to the grouped data likelihood approach of Sinha et al. (2003) but differs in the likelihood approximation. Whereas our expansions are Poincaré expansions (with relative error ==== as ====), those of Sinha et al. (2003) are stochastic expansions and not proper Poincaré expansion (as we show), so that they lack relative error ====. Using a flat improper prior for ==== are shown to be integrable for general arrangements of ties and censoring under weak conditions on the design and failures.","Cox (1972) introduced the proportional hazard (PH) model and derived partial likelihood as a means for making marginal inference about the regression parameter ====. The earliest Bayesian approach to marginal posterior inference for ==== was Kalbfleisch (1978) who marginalized over a gamma process prior on the cumulative baseline hazard function ====. His method leads to an explicit unnormalized posterior on ==== when the data lack ties. In the event of tied data, however, his approach was for the most part intractable mathematically.====Using a gamma process prior on ====, this paper extends the results of Kalbfleisch (1978) to accommodate arbitrary arrangements of tied and censored data and provides new explicit expressions for the marginal posterior distribution of ====. In the same context with arbitrary ties and censoring, we also develop a parallel approach which uses a Dirichlet process prior on the baseline survival function ==== and determine an explicit marginal posterior expression for ====. Both of these explicit expressions have not previously appeared in the literature and are the leading terms of proper Poincaré expansions (of the sort for which big-==== notation like ==== as ==== is used).====Sinha et al. (2003) also addressed the marginal posterior of ==== using a gamma process prior and computed stochastic expansions rather than the Poincaré expansions we derive. We show that their stochastic expansions, as described in Section 3.3, are not proper Poincaré expansions.",Marginal posterior distributions for regression parameters in the Cox model using Dirichlet and gamma process priors,https://www.sciencedirect.com/science/article/pii/S0378375821000513,28 May 2021,2021,Research Article,76.0
"Ye Keying,Han Zifei,Duan Yuyan,Bai Tianyu","Department of Management Science and Statistics, The University of Texas at San Antonio, San Antonio, TX, USA,School of Statistics, University of International Business and Economics, Beijing, China,Novartis Institutes for BioMedical Research, Cambridge, MA, USA,U.S. Food and Drug Administration, Silver Spring, MD, USA","Received 13 September 2019, Revised 16 March 2021, Accepted 15 May 2021, Available online 20 May 2021, Version of Record 31 May 2021.",https://doi.org/10.1016/j.jspi.2021.05.005,Cited by (10),"The elicitation of power priors, based on the availability of historical data, is realized by raising the likelihood function of the historical data to a fractional power ====, which quantifies the degree of discounting of the historical information in making inference with the current data. When ==== with three data examples, and provide an implementation with an R package NPP.","In applying statistics to real experiments, it is common that the sample size in the current study is inadequate to provide enough precision for parameter estimation, while plenty of the historical data or data from similar research settings are available. For example, when designing a clinical study, historical data of the standard care might be available from other clinical studies or a patient registry. Due to the nature of sequential information updating, it is natural to use a Bayesian approach with an informative prior on the model parameters to incorporate these historical data. Though the current and historical data are usually assumed to follow distributions from the same family, the population parameters may change somewhat over different time and/or experimental settings. How to adaptively incorporate the historical data considering the data heterogeneity becomes a major concern for the informative prior elicitation.====To address this issue, Ibrahim and Chen (1998), and thereafter Chen et al. (2000), Ibrahim and Chen (2000), and Ibrahim et al. (2003) proposed the concept of ====, based on the availability of historical data. The basic idea is to raise the likelihood function based on the historical data to a ==== ====
 ==== that controls the influence of the historical data. Its relationship with hierarchical models is also shown by Chen and Ibrahim (2006). For a comprehensive review of the power prior, we refer the readers to the seminar article (Ibrahim et al., 2015). The power parameter ==== can be prefixed according to external information. It is also possible to search for a reasonable level of information borrowing from the prior-data conflict via sensitivity analysis according to certain criteria. For example, Ibrahim et al. (2012a) suggested the use of deviance information criterion (Spiegelhalter et al., 2002) or the logarithm of pseudo-marginal likelihood. The choice of ==== would depend on the criterion of interest.====Ibrahim and Chen (2000) and Chen et al. (2000) generalized the power prior with a fixed ==== to a random ==== by introducing the ====. They specified a joint prior distribution directly for both ==== and ====, the parameters in consideration, in which an independent proper prior for ==== was considered in addition to the original form of the power prior. Hypothetically, when the initial prior for ==== is vague, the magnitude of borrowing would be mostly determined by the heterogeneity between the historical and the current data. However, under the joint power priors, the posterior distributions vary with the constants before the historical likelihood functions, which violates the likelihood principle (Birnbaum, 1962). It raises a critical question regarding which likelihood function should be used in practice. For example, the likelihood function based on the raw data and the likelihood function based on the sufficient statistics could differ by a multiplicative constant. This would likely yield different posteriors. Therefore, it may not be appropriate (Neuenschwander et al., 2009). Furthermore, the power parameter has a tendency to be close to zero empirically, which suggests that much of a historical data may not be used in decision making (Neelon and O’Malley, 2010).====In this article, we investigate a modified power prior which was initially proposed by Duan et al. (2006) for a random ====. It is named as the ==== since it includes a scale factor. The normalized power prior obeys the likelihood principle. As a result, the posteriors can quantify the compatibility between the current and historical data automatically, and hence control the influence of historical data on the current study in a more sensible way.====The goals of this work are threefold. First, we review the joint power prior and the normalized power prior that have been proposed in literature. We aim to show that the joint power prior may not be appropriate for a random ====. Second, we carry out a comprehensive study on properties of the normalized power prior both theoretically and numerically, shed light on the posterior behavior in response to the data compatibility. Finally, we design efficient computational algorithms and provide practical implementations along with three data examples.",Normalized power prior Bayesian analysis,https://www.sciencedirect.com/science/article/pii/S0378375821000501,20 May 2021,2021,Research Article,77.0
"Chagny Gaëlle,Channarond Antoine,Hoang Van Hà,Roche Angelina","LMRS, UMR CNRS 6085, Université de Rouen Normandie, France,Université Paris-Dauphine, CNRS, UMR 7534, CEREMADE, 75016 Paris, France","Received 30 July 2020, Revised 15 January 2021, Accepted 4 May 2021, Available online 20 May 2021, Version of Record 7 June 2021.",https://doi.org/10.1016/j.jspi.2021.05.004,Cited by (0),"A two-class mixture model, where the density of one of the components is known, is considered. We address the issue of the nonparametric adaptive estimation of the unknown ","The following mixture model with two components: ====where the mixing proportion ==== and the probability density function ==== on ==== are unknown, is considered in this article. It is assumed that ==== independent and identically distributed (==== in the sequel) random variables ==== drawn from density ==== are observed. The main goal is to construct an adaptive estimator of the nonparametric component ==== and to provide non-asymptotic upper bounds of the pointwise risk: the resulting estimator should automatically adapt to the unknown smoothness of the target function. The challenge arises from the fact that there is no direct observation coming from ====. As an intermediate step, the estimation of the parametric component ==== is addressed as well.====Model (1) appears in some statistical settings: robust estimation and multiple testing among others. The one chosen in the present article, as described above, comes from the multiple testing framework, where a large number ==== of independent hypotheses tests are performed simultaneously. ====-values ==== generated by these tests can be modelled by (1). Indeed these are uniformly distributed on ==== under null hypotheses while their distribution under alternative hypotheses, corresponding to ====, is unknown. The unknown parameter ==== is the asymptotic proportion of true null hypotheses. It can be needed to estimate ====, especially to evaluate and control different types of expected errors of the testing procedure, which is a major issue in this context. See for instance Genovese and Wasserman (2002), Storey (2002), Langaas et al. (2005), Robin et al. (2007), Strimmer (2008), Nguyen and Matias (2014a), and more fundamentally, Benjamini and Hochberg (1995) and Efron et al. (2001).====In the setting of robust estimation, different from the multiple testing one, model (1) can be thought of as a contamination model, where the unknown distribution of interest ==== is contaminated by the uniform distribution on ====, with the proportion ====. This is a very specific case of the Huber contamination model (Huber, 1965). The statistical task considered consists in robustly estimating ==== from contaminated observations ====. But unlike our setting, the contamination distribution is not necessarily known while the contamination proportion ==== is assumed to be known, and the theoretical investigations aim at providing minimax rates as functions of both ==== and ====. See for instance the preprint of Liu and Gao (2019), which addresses pointwise estimation in this framework.====Back to the setting of multiple testing, the estimation of ==== in model (1) has been addressed in several works. Langaas et al. (2005) proposed a Grenander density estimator for ====, based on a nonparametric maximum likelihood approach, under the assumption that ==== belongs to the set of decreasing densities on ====. Following a similar approach, Strimmer (2008) also proposed a modified Grenander strategy to estimate ====. However, the two aforementioned papers do not investigate theoretical features of the proposed estimators. Robin et al. (2007) and Nguyen and Matias (2014a) proposed a randomly weighted kernel estimator of ====, where the weights are estimators of the posterior probabilities of the mixture model, that is, the probabilities of each individual ==== being in the nonparametric component given the observation ====. Robin et al. (2007) propose an EM-like algorithm, and prove the convergence to a unique solution of the iterative procedure, but they do not provide any asymptotic property of the estimator. Note that their model ====, where ==== is a known density, is slightly more general, but our procedure is also suitable for this model under some assumptions on ====. Besides, Nguyen and Matias (2014a) achieve a nonparametric rate of convergence ==== for their estimator, where ==== is the smoothness of the unknown density ====. However, their estimation procedure is not adaptive since the choice of their optimal bandwidth still depends on ====.====In the present work, a complete inference strategy for both ==== and ==== is proposed. For the nonparametric component ====, a new randomly weighted kernel estimator is provided with a data-driven bandwidth selection rule. Theoretical results on the whole estimation procedure, especially adaptivity of the selection rule to unknown smoothness of ====, are proved under a given identifiability class of the model, which is an original contribution in this framework. Major results derived in this paper are the oracle-type inequality in Theorem 1, and the rates of convergence over Hölder classes, which are adapted to the control of pointwise risk of kernel estimators, in Corollary 1.====Unlike the usual approach in mixture models, the weights of the proposed estimator are not estimates of the posterior probabilities. The proposed alternative principle is simple and consists in using weights based on a density change, from the target distribution ====, which is not directly reachable, to the distribution of observed variables ====. A function ==== is thus derived such that ====, for all ====. This type of link between one of the conditional distribution given hidden variables, ====, to the distribution of observed variables ====, is quite remarkable in the framework of mixture models. It is a key idea of our approach, since it implies a crucial equation for controlling the bias term of the risk, see Section 2.1 for more details. This is necessary to investigate adaptivity using the Goldenshluger and Lespki (GL) approach (Goldenshluger and Lepski, 2011), which is known in other various contexts, see for instance, Comte et al. (2013), Comte and Lacour (2013), Doumic et al. (2012), Reynaud-Bouret et al. (2014) who apply GL method in kernel density estimation, and Bertin et al. (2016), Chagny (2013), Chichignoud et al. (2017) or Comte and Rebafka (2016).====Thus oracle weights are defined by ====, ====, but ==== and ==== are unknown. These oracle weights are estimated by plug-in, using preliminary estimators of ==== and ====, based on an additional sample ====. Some assumptions on these estimators are needed to prove the results on the estimator of ====; this paper also provides estimators of ==== and ==== which satisfy these assumptions. Note that procedures of Nguyen and Matias (2014a) and Robin et al. (2007) actually require preliminary estimates of ==== and ==== as well, but they do not deal with additional uncertainty caused by the multiple use of the same observations in the estimates of ====, ==== and ====.====Identifiability issues are reviewed in Section 1.1 in Nguyen and Matias (2014b). In the present work, ==== is assumed to be vanishing at a neighbourhood of ==== to ensure identifiability. Under this assumption, ==== can be recovered as the infimum of ====. Moreover, as shown above by the equation linking ==== to ==== and ====, ==== is actually uniquely determined by giving ==== and ====, even though the latter is not the infimum of ====. Note that the theoretical results on the estimator of the nonparametric component ==== do not depend on the chosen identifiability class, and can be transposed to other cases. For that reason, the discussion on identifiability is postponed to Section 4.2, after results on the estimator of ====.====The paper is organized as follows. Our randomly weighted estimator of ==== is constructed in Section 2.1. Assumptions on ==== and on preliminary estimators of ==== and ==== required for proving the theoretical results are in this section too. In Section 2, a bias–variance decomposition for the pointwise risk of the estimator of ==== is given as well as the convergence rate of the kernel estimator with a fixed bandwidth. In Section 3, an oracle inequality is given, which justifies our adaptive estimation procedure. Construction of the preliminary estimators of ==== and ==== is to be found in Section 4. Numerical results illustrate the theoretical results in Section 5. Proofs of theorems, propositions and technical lemmas are postponed to Section 6.",Adaptive nonparametric estimation of a component density in a two-class mixture model,https://www.sciencedirect.com/science/article/pii/S0378375821000495,20 May 2021,2021,Research Article,78.0
Saegusa Takumi,"Department of Mathematics, University of Maryland, College Park, MD 20742, USA","Received 13 October 2019, Revised 28 April 2021, Accepted 4 May 2021, Available online 12 May 2021, Version of Record 14 May 2021.",https://doi.org/10.1016/j.jspi.2021.05.002,Cited by (0),"We consider general semiparametric inference when data are merged from multiple overlapping sources. Merged data exhibit several characteristics including heterogeneity across multiple data sources, potential unidentified duplicated records, and dependence due to sampling without replacement within each data source. In this paper, we establish a large sample theory for the weighted semiparametric ","Various data sets have become rapidly available thanks to the proliferation of information technology. These data sets, when combined, provide unprecedented opportunities to enhance the quality of inference and accelerate scientific discovery. For instance, reliable analysis on rare scientific phenomena (e.g. rare genetic disease) can be achieved by increasing sample size through incorporating small data sets each of which contains a little information on rare phenomena. If public health data are collected from heterogeneous sources (e.g. clinical trials, disease registries, insurance claims), merging data will significantly reduce selection bias and ensure the generalizability of scientific findings to the broader population. Despite these potential benefits, statistical methodology for data integration has not yet been fully developed in many areas of statistical research.====In this paper, we study semiparametric inference for merged data from multiple sources. A semiparametric model ==== is a collection of probability measures ==== dominated by some measure ==== and indexed by a finite dimensional parameter ==== and an infinite-dimensional parameter ==== where ==== is a Banach space. When data are independent and identically distributed (i.i.d.), various semiparametric models have been studied including censored regression models (Huang, 1996, Murphy, 1995, Murphy et al., 1997, Parner, 1998), the missing data models (Nan et al., 2009), and the measurement error model (Murphy and van der Vaart, 1996) to name a few (see Bickel et al., 1998, Kosorok, 2008 for more applications). Large sample theory for these models heavily counts on the i.i.d. @assumption but merged data treated in this paper is characterized by (1) heterogeneous data sources, (2) unidentified duplicated records across data sets and (3) dependence due to sampling without replacement. The purpose of this paper is to provide a general inferential procedure to give a basis for studying important semiparametric models in the context of data integration.====The basic setting considered in this paper is as follows (see also Fig. 1).==== The variables of interest for data integration are a random vector ==== taking values in a probability space ====. The probability measure ==== belongs to a statistical model ==== with a true parameter ====.==== Let ==== where ==== is a coarsening of ==== and ==== is a vector of auxiliary variables. The variables ==== do not involve the model ==== but help to create data sources. The space ==== is composed of ==== overlapping population data sources ==== with ==== and ==== for some ====. Values of ==== determine membership of data sources.==== Data collection is carried out in a two-stage framework. First, a large i.i.d.sample of ==== is collected from a population. The unit ==== is assigned to data source ==== if ====. Because data sources overlap, the unit ==== may belong to multiple sources. The sample size of data source ==== is denoted as ====.==== Next, a random sample of size ==== is selected without replacement from data source ====. The selection probability for this data source is ==== where ==== is the indicator function. For selected items, variables ==== are observed.==== The procedure described above is repeated for all data sources. Data sets from each data source are combined and statistical analysis is conducted. If the unit ==== is selected multiple times, its duplication is not identified.====This two-stage formulation for data collection serves for describing duplicated records in multiple data sets. Duplication naturally occurs in many applications such as public health data research. Clinical and epidemiological studies identify target populations by the inclusion and exclusion criteria. When national disease registries are combined with these studies, diseased subjects must be in a national database as well. Duplicated records are difficult to identify in practice because key identifiers such as names and addresses are often not disclosed for privacy protection in public health data.====Examples covered by our framework include opinion polls (Brick et al., 2006), public health surveillance (Hu et al., 2011), health interview surveys (Cervantes et al., 2006), and the synthesis of existing clinical and epidemiological studies with surveys, disease registries, and healthcare databases (Chatterjee et al., 2016, Keiding and Louis, 2016, Metcalf and Scott, 2009). For the ease of better understanding of our mathematical setting, consider combining a cohort study and a disease registry as a hypothetical example. Variables of interest are disease status ====, age ====, and a biomarker ==== which form ====. The statistical model ==== of interest is the logistic regression model with outcome ==== and covariates ==== and ====. The cohort study targets a high risk group defined by age older than 40 and a positive test result ==== of an error-prone inexpensive medical test to measure ==== in the state in the United States. Let ==== be an indicator of living in the target state. The national disease registry collects information on diseased subjects in the United States. Because disease status, the age category, and potentially mismeasured biomarker are partial information needed for logistic regression, we can write ====. Since a living address is not considered as a risk factor, the auxiliary variable is ====. Based on ====, data sources for the cohort study and the disease registry are ==== and ====, respectively. The cohort study collects ==== by sampling without replacement and then ascertain the disease status ==== and measure biomarker ==== with a more precise medical test. At the diagnosis of disease, information on risk factors such as a biomarker is sent to the disease registry for all diseased subjects.====Merged data is considered to be a biased and dependent sample with duplication. Bias in merged data arises in two ways. Certain data sources are over/under-represented due to biased sampling with different selection probabilities ==== which yields heterogeneity in integrated data. Duplicated records enter the final merged data without identification. There are two types of dependence in merged data. Dependence between data sources is induced through duplicated records from overlapping data sources. Dependence within data sources is generated from sampling without replacement from each data source. These characteristics grossly differentiate our data integration problem from the analysis of an i.i.d.sample. Estimation and the corresponding asymptotic theory require different theory and methods that specialize in data integration in order to address challenging issues of bias, dependence, and duplication.====Data integration problems described above were first studied by our previous work (Saegusa, 2019). We developed a weighting procedure to study the infinite-dimensional ====-estimator. The estimator is computable without identifying duplicated records but corrects bias due to duplication and biased sampling. For its asymptotic theory, several specialized probabilistic tools were developed including the uniform law of large numbers and uniform central limit theorem for data integration. Semiparametric estimation was briefly mentioned as an example (Example 5.2 of Saegusa (2019)) but its asymptotic properties were presented without proof. For asymptotic variance of the ====-estimator, a plug-in estimator was proposed and examined in simulation studies.====In this paper, we study weighted semiparametric likelihood estimators proposed by our previous work (Saegusa, 2019). We provide a rigorous large sample theory to establish asymptotic distributions of our semiparametric estimators. The main contribution of this paper is a novel computational procedure to estimate asymptotic variance. The previously proposed plug-in variance estimator has limited uses because asymptotic variance in many semiparametric models do not have a closed form or contain expectations of unknown functions even in the i.i.d.setting (Murphy and van der Vaart, 1999). A popular alternative is resampling methods such as bootstrap (Efron, 1979) and jackknife (Quenouille, 1949, Tukey, 1958). Various kinds of resampling methods cover different data generating mechanisms (see e.g. Shao and Tu, 1995) but existing methods do not address heterogeneity of data sets and duplicated selection. Another approach in the i.i.d.setting is to estimate an efficient information matrix (Murphy and van der Vaart, 1999, Zhang et al., 2010). This approach focuses on an efficient estimator whose asymptotic variance is the inverse of the efficient information. In our setting, the loss in efficiency is expected compared to the i.i.d.setting, and hence estimation of the efficient information does not lead to consistent variance estimation.====Applications of semiparametric inference to data integration problems are largely hampered by the lack of valid variance estimators. Our proposed method is the first to address the challenging issue of estimating complicated asymptotic variance due to heterogeneity and duplicated selection arising from data integration problems. To address the lack of a simple asymptotic variance formula, we adopt two computational methods to estimate different parts of variance. The proposed methodology covers many semiparametric models and is easy to apply without computing complicated asymptotic variance.====The rest of the paper is organized as follows. In Section 2, we introduce our estimator and present its asymptotic properties. Section 3 concerns a novel variance estimation method. Consistency of the proposed estimator is provided. The finite sample performance of our estimator is presented in Section 4. Data example from the national Wilms tumor study is discussed in Section 4. All proofs for theorems in Section 3 and auxiliary results are collected in Section 6. Section 7 concludes with discussion on the future research.",Semiparametric inference for merged data from multiple data sources,https://www.sciencedirect.com/science/article/pii/S0378375821000471,12 May 2021,2021,Research Article,79.0
"Chen Baojiang,Qin Jing,Yuan Ao","Department of Biostatistics and Data Sciences, University of Texas Health Science Center at Houston, School of Public Health in Austin, Austin, 78701, USA,National Institute of Allergy and Infectious Diseases, National Institute of Health, Bethesda, MD, 20892, USA,Department of Biostatistics, Bioinformatics and Biomathematics, Georgetown University, Washington DC, 20057, USA","Received 14 July 2020, Revised 14 April 2021, Accepted 4 May 2021, Available online 12 May 2021, Version of Record 18 May 2021.",https://doi.org/10.1016/j.jspi.2021.05.003,Cited by (1)," of this estimate. Simulation studies demonstrate that the proposed method can yield higher sensitivity, while the naive method that without doing transformation can lead to lower sensitivity. We apply the proposed method to a gene expression study.","High dimensional data analysis has become a fertile area of research in many fields, such as genomics, health sciences, economics, social sciences, etc. However, in the application, only a small number of covariates may be associated with the outcome. As such, variable selection in the high-dimensional setting has drawn great attention in recent years. Many methods for variable selection in high dimensional data analysis have been proposed, including the widely-used penalized likelihood approach which can simultaneously select variables and estimate their associated regression coefficients. Examples of this approach include Tibshirani’s (1996) least absolute shrinkage and selection operator (LASSO), which can be viewed as the solution of penalized least squares with the ==== penalty. However, critics of LASSO have noted that the technique might not have the oracle property (Fan and Li, 2001, Zou, 2006). Fan and Li (2001) proposed the smoothly clipped absolute deviation (SCAD) penalty, which has the oracle property. More recently, Zou (2006) proposed the adaptive LASSO estimate by introducing a weighted ==== penalty, which also enjoys the oracle property. Similar to Fan and Li (2001), Zhang (2010) proposed the minimax concave penalty (MCP). Further review of penalty functions and high dimensional variable selection is provided in Fan and Lv (2010).====Estimation of the regression coefficients is, in general, obtained by maximizing the penalized likelihood. Fan and Lv (2011) gave conditions that the penalized likelihood estimator exists and is unique. When the penalty function is convex (e.g., the ==== penalty), a convex optimization algorithm can be applied to maximize the penalized likelihood. However, specific techniques have varied in the literature. Fan and Lv (2010) showed that the penalized likelihood can be solved using a sequence of reweighted ====-regression problems via local linear approximation (LLA) (e.g., Zou and Li, 2008). Fan and Li (2001) proposed a local quadratic approximation algorithm (LQA) for optimizing the non-concave penalized likelihood, while Zou and Li (2008) stated that a better approximation can be achieved using an LLA. Osborne et al. (2000) proposed a quadratic programming algorithm to maximize the penalized likelihood. Efron et al. (2004) derived a fast and efficient least angle regression (LARS) algorithm. For a comprehensive review of the estimation methods, please see Fan and Lv (2010).====In the literature, few studies have been focusing on variable selection for skewed outcome data. Simulation studies demonstrate that ignoring an appropriate transformation for the outcome can lead to biased inferences (e.g., missing important covariates). For example, consider a simple regression model ====The power to test the significance of the covariate ==== is 0.842 using the log transformation of the outcome based on 2000 simulations. However, if we ignore the transformation of the outcome and fit a linear regression model ====the power to test the significance of the covariate ==== is 0.668 based on 2000 simulations. The power loss may be because ignoring the transformation leads to a wrong model assumption. We expect similar conclusions as in the high dimensional case. See Section 3 for further examples and simulation results of different transformations for high dimensional data. To eliminate this bias and to improve the precision of inference, it is desirable to make an appropriate transformation of the outcome before doing a variable selection. Klaassen et al. (2017) considered an estimation problem for a transformation model using the LASSO penalty only. Zhang and Yang (2017) considered a Box–Cox transformation model in big data where the sample size is large, while the number of predictors is small.====In this paper, we propose a variable selection procedure under the Box–Cox power transformation model for handling skewed data. The Box–Cox power transformation model (Box and Cox, 1964, Bickel and Doksum, 1981, Hinkley and Runger, 1984, Carroll and Ruppert, 1985, Taylor, 1985a, Taylor, 1985b, Taylor, 1987, Sakia, 1992) is a widely used technique for handling skewed data. Compared to the nonparametric transformation, the Box–Cox transformation is simple and easy to implement. After transformation, the resulted response yields a linear regression model on its covariates with a normal error and constant variance. Under this parametric assumption, with high-dimensional data, a penalized likelihood can be easily constructed. We develop a penalized maximum likelihood estimate and derive the consistency, oracle property and the asymptotic distribution of this estimate for different penalties. Extensive simulation studies demonstrate that using the Box–Cox transformation can lead to larger sensitivity and smaller false discovery rate compared to the naive method without transformation. Furthermore, simulation studies also demonstrate that the proposed method can be robust to the misspecification of the Box–Cox transformation (see Section 3). Our study is different from that of Klaassen et al. (2017) since their study focused on estimation and inference for the regression parameter, while our study focuses on variable selection in high dimensional spaces. As the research goals are different, Klaassen et al. (2017) only considered the LASSO penalty, while our study further considers SCAD/MCP penalties. Furthermore, we study the oracle property of the estimators under the SCAD/MCP penalties, while Klaassen, Kueck, and Spindler’s estimator under the LASSO penalty does not have the oracle property.====The paper is organized as follows. In Section 2, we describe the proposed variable selection procedure under the Box–Cox power transformation model and study the asymptotic properties of the estimate. In Section 3, we evaluate the performance of our method using simulation studies. In Section 4, we apply our method to a gene expression study, and Section 5 provides concluding remarks. The proofs of the main theoretical results are relegated to Appendix A.",Variable selection in the Box–Cox power transformation model,https://www.sciencedirect.com/science/article/pii/S0378375821000483,12 May 2021,2021,Research Article,80.0
Saegusa Takumi,"Department of Mathematics, University of Maryland, College Park, MD 20742, USA","Received 14 October 2019, Revised 28 April 2021, Accepted 4 May 2021, Available online 7 May 2021, Version of Record 10 May 2021.",https://doi.org/10.1016/j.jspi.2021.05.001,Cited by (0), of the proposed estimator and develop a simulation-based method to estimate the limiting process. The finite sample performance is evaluated through a simulation study. A Wilms tumor example is provided.,"We consider nonparametric estimation of a distribution function ==== of a random variable ==== when data are collected from two-phase stratified sampling in biomedical studies. Two-phase stratified sampling design is a cost-effective design used for collecting expensive variables to measure such as rare disease and rare exposure. Examples of this design include stratified case-control design (White, 1986) and stratified case cohort designs (Prentice, 1986, Borgan et al., 2000). At the first phase, the independent and identically distributed (i.i.d.) sample is collected from an infinite population. Variables available at the first phase are incomplete for statistical inference. In order to collect missing variables ====, the i.i.d.sample is stratified and subsamples are sampled from each stratum without replacement. The resultant final sample is a heterogeneous and dependent sample because of finite population sampling with different selection probabilities from strata. In this paper, we address bias arising from both heterogeneity across strata and dependence within strata, and develop a rigorous large sample theory for inference of the distribution function.====Statistical inference with two-phase stratified sampling has focused on censored regression modeling (see e.g.accelerated failure time model (Nan et al., 2006, Nan et al., 2009), the additive hazards model (Kulich and Lin, 2000), the Cox proportional hazards model (Prentice, 1986, Self and Prentice, 1988), and the transformation model (Lu and Tsiatis, 2006, Kong et al., 2006, Zeng and Lin, 2014)). The standard assumption adopted in these literature is Bernoulli sampling from each stratum rather than sampling without replacement (see e.g. Breslow and Wellner, 2007, Saegusa and Wellner, 2013 for some exceptions). Although the number of selected observations is random, Bernoulli sampling ensures the independence among observations so that theoretical analysis of statistical methods becomes simple. The approximation of sampling without replacement by Bernoulli sampling fortunately leads to a statistically conservative conclusion in practice because asymptotic variance in Bernoulli sampling is larger than in sampling without replacement (Breslow and Wellner, 2007, Saegusa and Wellner, 2013). Along the same line of reasoning, the confidence band based on Bernoulli sampling, if successfully constructed, would be wider and the corresponding coverage probability would be inflated in sampling without replacement although the methods for confidence bands has not been studied in Bernoulli sampling to the best of our knowledge. To achieve the correct coverage probability, we do not adopt the standard practice of assuming Bernoulli sampling, but rigorously quantify uncertainty arising from sampling without replacement.====Unlike biostatistical literature, dependence has been the main research focus in sampling theory. Variance that accounts for dependence has been studied in various sampling designs including two-phase sampling design (see e.g. Särndal et al., 1992). Note, however, that two-phase sampling in sampling theory (e.g. Särndal et al., 1992, Chen and Rao, 2007) is different from two-phase sampling in the biostatistical literature because sampling at the first phase is from the finite population in sampling theory in addition to subsequent sampling at the second phase. In contrast, our two-phase stratified sampling obtains the i.i.d. @sample from the infinite population at the first phase and collects subsamples from each stratum after stratifying the i.i.d.sample at the second phase. The closest design to our setting in sampling theory is thus stratified sampling without replacement where stratified samples are obtained from the finite population. This is because our sampling design changes into stratified sampling in sampling theory when one treats the i.i.d.sample at the first phase as the fixed finite population by ignoring randomness in sampling from the infinite population.====In stratified design in sampling theory, Bickel and Krieger (1989) adopted two types of bootstrap methods which are designed for survey sampling to construct confidence bands for a distribution function. This method, however, does not produce valid confidence bands in our setting due to the key difference in probabilistic frameworks between biostatistics and sampling theory. In the finite population framework which standard sampling theory adopts, the random variable ==== is treated as a non-random fixed variable so that statistical methods in sampling theory disregard additional randomness due to sampling ==== from the infinite population. Accordingly, the confidence band of Bickel and Krieger (1989) would be narrower and the corresponding coverage probability would be deflated in our setting which we show in a simulation study below. In biostatistical applications, scientific phenomena such as disease progression are naturally considered random so that the distribution function ==== is a more meaningful parameter of interest compared to the framework in sampling theory. To account for randomness in ==== for biostatistical applications, a novel asymptotic theory is required for precise quantification of uncertainty due to both sampling from the infinite population and subsequent sampling from strata.====In this paper, we study the inverse probability weighted empirical distribution as the proposed nonparametric estimator of the distribution function. The inverse probability weighting is a natural technique and well adopted in sampling theory and missing data literature. The key difference from those literature comes from our probabilistic framework where the variable ==== is random and sampling from stratum is without replacement as described above. To rigorously evaluate the uncertainty of the proposed estimator in our setting, we apply empirical process theory developed for two-phase stratified sampling by Saegusa and Wellner (2013). We show the uniform consistency of the proposed estimator over the real line and its weak convergence to a Gaussian process. The precise limiting distribution obtained from this approach is a basis for achieving the correct coverage probability for the proposed confidence band of the distribution function.====The main contribution of this paper is the novel computational method to construct a confidence band of the distribution function whether or not ==== is a continuous or discrete variable. The proposed approach for a confidence band is to estimate quantiles of the supremum of the absolute difference between our estimator and the true function ==== over the real line. In the i.i.d.setting, this approach is applied to the supremum of the Kolmogorov–Smirnov statistic for a continuous random variable, and its quantiles are analytically obtained from the Kolmogorov–Smirnov distribution (Kolmogorov, 1933, Smirnov, 1944). For non-continuous random variables, the Dvoretzky–Kiefer–Wolfowitz inequality (Dvoretzky et al., 1956) yields an upper bound of quantiles of interest resulting in an inflated coverage probability. An alternative approach is to estimate quantiles by bootstrapping the supremum of (weighted) Kolmogorov–Smirnov statistic. This approach was explored in the i.i.d. @setting by Bickel and Freedman (1981) and in stratified sampling from a finite population by Bickel and Krieger (1989). These bootstrap methods are valid for both continuous and non-continuous random variables. In our setting, the proposed estimator is a sum of dependent variables and its limiting process is a linear combination of multiple Gaussian processes. Accordingly, the Kolmogorov–Smirnov distribution and the Dvoretzky–Kiefer–Wolfowitz inequality which assume the i.i.d.sample do not produce a reasonable estimate of quantiles. A valid bootstrap method is not available either in our setting because existing bootstrap methods reproduce randomness either due to sampling from the infinite population or due to sampling from strata, but not both at the same time.====Besides analytical or bootstrap computation of quantiles, various methods for confidence bands have been proposed. For parametric models, confidence bands are considered for normal distributions (Kanofsky and Srinivasan, 1972), Weibull distributions (Schafer and Angus, 1979), and the location scale parameter model (Cheng and Iles, 1983). For nonparametric models, Bayesian approach with the Dirichlet prior was explored by Breth (1978). Inversion of a nonparametric likelihood test of Berk and Jones (1978) was considered by Owen (1995). For continuous random variables, the optimal confidence band was proposed by Frey (2008) based on the narrowness criterion, and the kernel smoothed estimator of a distribution function was adopted by Wang et al. (2013).====The rest of the paper is organized as follows. In Section 2, we formally introduce two-phase stratified sampling, and the limit behavior of the proposed estimator of ==== is derived. We present the algorithm to compute the confidence band and study its large sample property in Section 3. Our method is extended to conditional distribution functions in Section 4. The performance of the proposed methodology is evaluated through a simulation study in Section 5. A data example from the national Wilms tumor study is presented in Section 6. All proofs are deferred to the Appendix.",Nonparametric inference for distribution functions with stratified samples,https://www.sciencedirect.com/science/article/pii/S037837582100046X,7 May 2021,2021,Research Article,87.0
"Malcherczyk Dennis,Leckey Kevin,Müller Christine H.","Faculty of Statistics, TU Dortmund University, D-44227 Dortmund, Germany","Received 8 May 2020, Revised 3 March 2021, Accepted 19 April 2021, Available online 28 April 2021, Version of Record 6 May 2021.",https://doi.org/10.1016/j.jspi.2021.04.006,Cited by (2),"The ====-sign depth (====-depth) of a model parameter ==== in a data set is the relative number of ====-depth test based on ====-depth, recently proposed by Leckey et al. (2020), is equivalent to the classical residual-based sign test for ====, but is much more powerful for ====. This test has two major drawbacks. First, the computation of the ====-depth which can be computed efficiently. For ====-statistics. We provide here a much shorter proof based on Donsker’s theorem and extend it to any ====. As part of the proof, we derive an asymptotically equivalent form of the K-depth which can be computed in linear time. This alternative and the original implementation of the K-depth are compared with respect to their runtimes and absolute difference.","Let ==== be real-valued residuals for a model with model parameter ==== satisfying ==== for the true underlying parameter. For ====, the ====-sign depth (====-depth) of ==== is defined as ==== where ==== denotes the indicator function. Hence, the ====-depth is the relative number of ====-tuples of the residuals with alternating signs. In several models for time series and several generalized linear models, it can be assumed that the residuals ==== are independent with median Med==== if ==== is the true underlying parameter. Under such assumptions, the ====-depth can be used for testing hypotheses of the form ==== where ==== is an arbitrary subset of the parameter space: the so-called ====-depth test rejects ==== if the maximum ====-depth of ==== with ==== is smaller than a critical value, i.e. there are not enough sign changes in the residuals of the null hypothesis. Leckey et al. (2020) showed that this ====-depth test is equivalent to the classical residual-based sign test for ==== and is much more powerful for ====. Since it is based on signs, it is outlier robust.====Moreover, a two-sided version can be used to test simultaneously the independence of the residuals and Med==== since a too large ====-depth, i.e. too many sign changes in the residuals, indicates negatively correlated residuals, whereas a too small ====-depth indicates not only Med==== but also positively correlated residuals. In particular, Leckey et al. (2020) noted that a modification of the two-sided version can be considered as a generalization of the runs test given by Wald and Wolfowitz (1940), see e.g. Gibbons and Chakraborti (2003), pp. 78–86.====An important condition for a powerful ====-depth test is an appropriate ordering of the residuals ====. For time series or for generalized linear models with one quantitative explanatory variable, the natural ordering is appropriate. If no natural ordering exists, appropriate orderings can be defined as (Horn and Müller, 2020) demonstrated for multiple regression.====There have been two open problems: One is the determination of the critical values of the test for large sample sizes ==== and the other problem is the ==== time complexity when calculating the ====-depth, i.e. its runtime lies within ==== for some constants ====. Both are solved in this paper.====At first note that ====-depth for residuals belongs to the big class of depth notions developed after Tukey (1975) introduced the half space depth. Many of these depth notions concern the deepness of data points in ==== like Oja depth (Oja, 1983, Chen et al., 2013), simplicial depth (Liu, 1988, Liu, 1990), Mahalanobis depth (Liu and Singh, 1993, Hu et al., 2011), projection depth  (Zuo, 2003, Zuo, 2006), or zonoid depth (Mosler, 2002, Liu et al., 2019). Other depth notions concern the depth of distributions as considered by Dong and Lee (2014) or of functional data as considered by López-Pintado and Romo, 2007, López-Pintado and Romo, 2009, Claeskens et al., 2014, López-Pintado et al., 2014, Nagy and Ferraty, 2019.====However, the ====-depth for residuals belongs to the depth notions for model parameters first introduced by Rousseeuw and Hubert (1999). They defined the regression depth of a regression parameter ==== via the residuals given by ====. Instead of residuals, other quality measures like general likelihood functions can be used as proposed by Mizera, 2002, Mizera and Müller, 2004, Müller, 2005, Denecke and Müller, 2011, Denecke and Müller, 2012. Unfortunately, the complicated computation of the regression depth and their extensions is a crucial drawback.====The ====-depth first appeared as a simplification of the so-called simplicial regression depth. This depth notion is a combination of the regression depth of Rousseeuw and Hubert (1999) and the simplicial depth of Liu, 1988, Liu, 1990. It defines the depth of a ====-dimensional parameter ==== as the relative number of ====-tuples of residuals that have a positive regression depth. Under certain conditions given in Kustosz et al. (2016b), a positive regression depth of a ====-tuple of residuals is equivalent to alternating signs. Hence the simplicial regression depth coincides with the ====-depth given by (1) in many applications. Note that the ====-depth in its general form also works for other choices than ==== when considering a ==== dimensional parameter ====.====When using the ====-depth for testing, a critical value is needed. For small sample sizes ====, the critical value of the ====-depth test can be calculated by determining the ====-depth for all of the ==== possible sign constellations. For larger sample sizes, it is way more efficient to use the quantiles of an asymptotic distribution of the ====-depth. The derivation of asymptotic distributions of depth notions and depth estimators is not easy, see e.g. Bai and He, 1999, Wang, 2019. However, almost all simplicial depth notions have the advantage that the depth statistic is a U-statistic which was used in Dümbgen, 1992, Arcones and Gine, 1993, Arcones et al., 1994. Unfortunately, the U-statistic is degenerated in several cases which makes it harder to derive limit theorems, see Müller, 2005, Wellmann et al., 2009, Wellmann and Müller, 2010b, Wellmann and Müller, 2010a. Some simplicial depth notions based on likelihood depth are not degenerated U-statistics but with the price of providing biased estimators, see Denecke and Müller (2012). The statistic given by the ====-depth has the additional disadvantage that it is not a classical U-statistic since it is influenced by the order of the residuals and thus its kernel is not symmetric for ====. In the particular case of an explosive AR(1) regression with two unknown parameters, Kustosz et al. (2016a) provide a limit theorem for the ====-depth by mimicking the classical proof for U-statistics. The resulting asymptotic distribution is given as an integrated two-dimensional Gaussian process. Note that the limit theorem provided by Kustosz et al. (2016a) is not restricted to AR(1) models since the proof only relies on having independent residuals with signs that are uniformly distributed on ====. However, the proof is restricted to the ====-depth, i.e. ====-depths with ==== are not covered.====Here, we provide a much shorter proof by using Donsker’s theorem (see e.g. Billingsley, 1999 Theorem 14.1). This also yields a slightly different representation of the asymptotic distribution based on a standard Brownian motion rather than the two-dimensional Gaussian process in Kustosz et al. (2016a). Moreover, we extend this proof to any ====. Finally, our proof yields an asymptotically equivalent form of the ====-depth which can be computed in linear time for all ==== instead of the ==== time needed for any simplicial depth based on subsets with ==== observations.====The paper is organized as follows. Section 2 provides the derivation of this asymptotic distribution. In Section 3, the asymptotically equivalent form, which can be computed in linear time, is derived and the exact ====-depth is compared to the asymptotically equivalent form. The comparison is based on the absolute differences between the two statistics and their runtimes when considering randomly generated signs as an input. Detailed versions for some proofs and simulations of the asymptotic distribution and its quantiles are given in the supplementary material.==== ==== denotes the indicator function, i.e. ==== if ==== and ==== if ====. For real numbers ====, let ==== and ====. Moreover, the sign of a real number ==== is denoted by ====. We use the standard Bachmann–Landau symbols: For sequences ==== and ====, ==== denotes the existence of a constant ==== and an integer ==== such that ==== for all ====. Moreover, ==== denotes that ==== as ====. Finally, ==== denotes that both ==== and ====. For ====, ==== denotes the floor function.",K-sign depth: From asymptotics to efficient implementation,https://www.sciencedirect.com/science/article/pii/S0378375821000458,28 April 2021,2021,Research Article,88.0
"Gadrich Tamar,Marmor Yariv N.","Department of Industrial Engineering and Management, ORT Braude College, Karmiel, Israel","Received 28 September 2019, Revised 7 September 2020, Accepted 19 April 2021, Available online 28 April 2021, Version of Record 6 May 2021.",https://doi.org/10.1016/j.jspi.2021.04.005,Cited by (4)," ====(within) component and ‘inter’==== ====(between) component for any scale. Building on this, we provide a way for decomposing the ‘inter’==== ","Stevens (1946) proposed four measurement scales: nominal and ordinal scales (i.e., categorical scales) and interval and ratio scales (i.e., numerical scales). Each scale imposes its respective legitimate operations between data. The focus of the present paper is on an ordinal response variable that is evaluated according to a predetermined set of ordered exhaustive and disjoint categories. Some examples of such kinds of variables are academic degrees, customer satisfaction levels, Likert scales, item quality levels, the Mohs scale of mineral hardness, countries’==== ====credit ratings, the Gleason score of prostatic carcinomas, etc. The only legitimate operations between ordinal variables are equal, unequal, greater than or less than ====, so all calculations must be based on these limitations. To test the null hypothesis of the homogeneity of samples, first we need an appropriate variability measure. Variance (VAR), a classic measure, and the Gini mean difference (GMD) (Gini, 1912) are the most popular variability measures that fit a numerical scale. While VAR is based on dispersion around some predetermined ‘midpoint’==== ====(expectation or average), the GMD is based only on differences between data pairs, as is expected from a dispersion measure. We refer the reader to Gadrich et al. (2015) for an extended review of variability measures. Therein, Gadrich et al. (2015) proposed a population (or sample) total-variation measure based on an appropriate Loss-of-Similarity (LoS) function depending on the scale used for measuring the data (see below, Section 3). Moreover, Gadrich and Bashkansky (2012) proposed an equivalent expression, normalized to the ==== interval, for the total-variation measure of ordinal scale-based data, based on the cumulative probability (or proportion), following previous work by Blair and Lacy, 1996, Blair and Lacy, 2000 (see Section 4).====Hypothesis testing of group differences can be distinguished by the number of independent main factors used to explain the response (dependent) variable. When considering a numerical scale, analyzing variability to compare means is related to Fisher. His famous one-way or two-way ANOVA (analysis of variance) method is based on using one or two main factors and their possible interaction. For data measured on a nominal scale, Light and Margolin (1971) proposed the one-way CATANOVA (categorical analysis of variance) method for one response variable and only one explanatory factor when comparing more than two samples (for only two samples, the chi-squared test or Fisher’s exact test can be used).====An extension of the CATANOVA method to multidimensional contingency tables (with the focus on two factors) was proposed in Anderson and Landis (1980) by Anderson and Landis for a nominal scale and for an ordinal scale in Anderson and Landis (1982). The CATANOVA method is calculated based on the proportion of categories in the sample. D’Ambra et al. (2005) proposed an extension of CATANOVA that decomposes the between-group sum of squares (defined in Light and Margolin (1971) using orthogonal polynomials while Lombardo and Camminatiello (2010) proposed another extension of CATANOVA that uses orthogonal polynomials to decompose the index of a multiple association.====For data measured on an ordinal scale with one response variable and only one factor, we can use one of the following a-parametric tests: the Mann–Whitney test if comparing only two groups or the Kruskal–Wallis test if comparing more than two groups. These a-parametric tests convert ordinal data by transforming ratings into rankings so that the data can be measured on an interval scale.====Gadrich and Bashkansky (2012) proposed the one-way ORDANOVA (ordinal data analysis of variation) method for ordinal response variable and only one factor.====In this paper, we propose an extension of the one-way ORDANOVA method to the two-way cross-balanced ORDANOVA method. Both ORDANOVA methods are based on the cumulative proportions and we propose the decomposition of the sample total-variation into an ‘intra’==== ====(within) component and the ‘inter’==== ====(between) components. Building on this, in the two-way model considered in this article, we decompose the ‘inter’==== ====component into two variations contributed by each of the two factors and the interaction component, based on ==== replications in each cross-design (see Section 4) or without the interaction component when considering only one replicated test (see Section 5). Under the null hypothesis of homogeneity – that all samples were drawn from the same infinite population characterized by a given probability vector – we present relations between the expected values of the sample total-variation components. Based on these relations, we suggest segregation power indices to check the null hypothesis of homogeneity.====The paper is organized as follows. In Section 2, we present the layout of categorical data. In Section 3, we establish the total-variation decomposition for any scale. In Section 4, we focus on an ordinal scale and formulate the two-way ORDANOVA model with replications. In addition, we present the two-way ORDANOVA calculator (constructed using VBA in EXCEL) for processing empirical data and for decision rules for null hypotheses at a given confidence level. In Section 5, we study the two-way ORDANOVA model without replications. Some examples are given to demonstrate the proposed method.",Two-way ORDANOVA: Analyzing ordinal variation in a cross-balanced design,https://www.sciencedirect.com/science/article/pii/S0378375821000446,28 April 2021,2021,Research Article,89.0
"Gaucher Solenne,Klopp Olga","Département de Mathématiques d’Orsay, Université Paris-Saclay, Bâtiment 307 Rue Michel Magat, 91400 Orsay, France,ESSEC Business School, 3 Avenue Bernard Hirsch, 95000 Cergy, France,CREST, ENSAE, 5 Avenue Le Chatelier, 91120 Palaiseau, France","Received 23 May 2020, Revised 8 April 2021, Accepted 17 April 2021, Available online 24 April 2021, Version of Record 4 May 2021.",https://doi.org/10.1016/j.jspi.2021.04.003,Cited by (4),"Estimating the matrix of connections ==== proxy, we bound the risk of the ","In the past two decades, networks have attracted considerable attention, as many scientific fields are concerned about the advances made in the understanding of these complex systems. In social sciences (Wasserman and Faust, 1994) as in physics (Albert and Barabási, 2002) and biology (Yamanishi et al., 2004), networks are used to represent a great variety of systems of interactions between social agents, particles, proteins or neurons. These networks are often modeled as an observation drawn from a random graph.====Missing observations are a common problem when studying real life networks. In social sciences, data coming from sample surveys are likely to be incomplete, especially, when dealing with large or hard-to-find populations. While biologists often use graphs to model interactions between proteins, experimental discovery of these interactions can require substantial time and investment from the scientific community (Bleakley et al., 2007). In many cases, collecting complete information on relations between actors can be difficult, expensive and time-consuming (Kshirsagar et al., 2012, Yan and Gregory, 2012, Handcock and Gile, 2010, Guimerà and Sales-Pardo, 2009). On the other hand, the emergence of detailed data sets coming, for example, from social networks or genome sequencing has fostered new challenges, as their large size makes using the full data computationally unattractive. This has led scientists to consider only sub-samples of the available data (Benyahia et al., 2017). However, incomplete observation of the network structure may considerably affect the accuracy of inference methods (Kossinets, 2006).====Our work focuses on the study of the inhomogeneous random graph model with ====. In this setting, the problem of estimating the matrix of connections probabilities is of primary interest. Minimax optimal convergence rates for this problem have been shown to be attained by the least square estimator under full observation of the network for dense graphs in Gao et al. (2015) and for sparse graphs in Klopp et al. (2017). In Gao et al. (2016), the authors extended these results to the setting in which observations about the presence or absence of an edge are missing independently at random with the same probability ====. However their estimator requires the knowledge of ====, and cannot be extended to non-uniform sampling schemes. Unfortunately, least square estimation is too costly to be used in practice. Many other approaches have been proposed, for example, spectral clustering (McSherry, 2001, Hagen and Kahng, 1992, Rohe et al., 2011), modularity maximization (Newman, 2006, Bickel and Chen, 2009), belief propagation (Decelle et al., 2011), neighborhood smoothing (Zhang et al., 2017), convex relaxation of k-means clustering (Giraud and Verzelen, 2018) and of likelihood maximization (Amini and Levina, 2018), and universal singular value thresholding (Chatterjee, 2015, Klopp and Verzelen, 2017, Xu, 2018). It was conjectured the existence of possible computational gap when no polynomial time algorithm can achieve minimax optimal rate of convergence. The present work shows that this is not the case.====In this work, we consider the maximum likelihood estimator. This estimator is also NP-hard but its computationally efficient approximations (under some additional conditions) have been proposed in the literature (see, e.g., Matias and Robin (2014) for a detailed review of these methods). For example, the authors of Amini et al. (2013) suggest to use pseudo-likelihood methods, as it leads to computationally tractable estimators. Alternatively, in Daudin et al. (2008) a tractable variational approximation of the maximum likelihood estimator is proposed. This method has been applied successfully to study biological networks, political blogsphere networks and seeds exchange networks (Picard et al., 2009, Tabouy et al., 2020, Latouche et al., 2011). The authors of Bickel et al. (2013) show asymptotic normality of the maximum likelihood estimate and of its variational approximation for sparse graphs generated by stochastic block models when the connections probabilities of the different communities are well separated. In Tabouy et al. (2020), these results are extended to the case of missing observations. These methods suffer from a lack of theoretical guarantees when the model is misspecified or non-identifiable. On the other hand, to the best of our knowledge, no non-asymptotic bound has been established for the risk of the maximum likelihood estimator. In this work, we close this gap and show that the maximum likelihood estimator is minimax optimal in a number of scenarii, and adaptative to non-uniform missing data schemes. Moreover we show that it can be efficiently approximated using tractable variational methods.====Our results find a natural application in predicting the existence of non-observed edges, a commonly encountered problem called ==== (Lü and Zhou, 2011, Zhao et al., 2017). Interaction networks are often incomplete, as detecting interactions can require significant experimental effort. Instead of exhaustively testing for every connection, one might be interested in deducing the pairs of agents which are most likely to interact based on the relations already recorded and on available covariates. If these estimations are precise enough, testing for these interactions would enable scientists to establish the network topology while substantially reducing the costs (Clauset et al., 2008). In this context, estimating the probabilities of connections through likelihood maximization enables to accordingly rank unobserved pairs of nodes. Link prediction also finds applications in recommender systems for social networks. The missing observation scheme studied in this work is motivated by the above examples, and generalizes the model described in Gao et al. (2016).",Maximum likelihood estimation of sparse networks with missing observations,https://www.sciencedirect.com/science/article/pii/S0378375821000422,24 April 2021,2021,Research Article,90.0
"Gong Yuan,Chen Zehua","National University of Singapore, Singapore","Received 28 May 2020, Revised 16 April 2021, Accepted 19 April 2021, Available online 24 April 2021, Version of Record 30 April 2021.",https://doi.org/10.1016/j.jspi.2021.04.004,Cited by (1), using simulated data and a real data demonstrate that the sgsAM method has an advantage over the existing methods.,"High-dimensional data arise in many important scientific fields such as genetic research, medicine, and financial studies. By high-dimensionality, it refers to the situation that the number of variables, ====, is much larger than the number of observations, ====. In an asymptotic mathematical setting, it is usually assumed that ==== or ==== for some small constants ==== and ====. The latter is called the ultra-high dimensionality by Fan and Lv (2008). A typical problem in high-dimensional data analysis is feature selection in the general regression model: ====, where ==== is a response variable, ==== is a ====-dimensional vector of covariates. It is usually assumed that ==== is linear, i.e., ====. However, there is no convincing reason to stick to this assumption in practical problems. For example, Huang et al. (2010) considered a gene expression data which demonstrates non-linearity. The general form of ==== is intractable due to the curse of dimensionality. A flexible and tractable form is the additive model: ====, where ==== is the ====th component of ====. In this article, we deal with the problem of feature selection for high-dimensional additive models.====The problem of feature selection has been comprehensively studied in the literature. The methodologies can be roughly classified into two major categories. The first category consists of penalized likelihood methods, including LASSO (Tibshirani, 1996), adaptive LASSO (Zou, 2006), SCAD (Fan and Li, 2001), MCP (Zhang, 2010), group LASSO (Yuan and Lin, 2006), sparse-group LASSO (Simon et al., 2013), and so on. The second category consists of sequential methods such as least angle regression (LAR) (Efron et al., 2004), boosting (Buehlmann, 2006), orthogonal matching pursuit (OMP) (Cai and Wang, 2011), forward stepwise regression (FSR) (Wang, 2009), and sequential Lasso (Luo and Chen, 2014). These methods are originally developed for univariate linear regression models, but later are extended to other models including multivariate regression models and additive models, see, e.g., Turlach et al., 2005, Obozinski et al., 2011, Peng et al., 2010, Lutz and Bühlmann, 2006, Li et al., 2015, Luo and Chen, 2020a, Ravikumar et al., 2009, Meier et al., 2009, Huang et al., 2010, and Fan et al. (2011).====In the case of ultra-high dimensionality, the strategy of screening-selection is usually adopted. In this strategy, the features are first screened to reduce the dimensionality, then an appropriate method is applied to the reduced data for final feature selection. The most popular screening procedures are the sure independence screening (SIS) and the iterated SIS proposed in Fan and Lv (2008) for linear models. Other versions of the SIS are also considered in the literature. Screening using Kendall’s ==== is proposed in Li et al. (2012a). A procedure called sure independent ranking screening is proposed in Zhu et al. (2011). Screening using distance correlation measure is considered in Li et al. (2012b) and Zhong and Zhu (2015).====The following are a few methods for additive models. A method dubbed as SpAM is proposed in Ravikumar et al. (2009) which imposes a penalty on each additive component in a standard back-fitting algorithm. A penalized likelihood approach with a sparsity-smoothness penalty, named penGAM, is considered in Meier et al. (2009). An adaptive group Lasso method is used by Huang et al. (2010). An iterative non-parametric independence screening (INIS) approach is adopted in Fan et al. (2011).====In this article, we propose a sequential group selection approach for additive models, dubbed as sgsAM. Like the adaptive group Lasso method of Huang et al. (2010), the additive functions are estimated by B-spline method and are treated as groups of features. Then, the groups are selected sequentially by a correlation search procedure. The sgsAM method is shown to be selection consistent under mild conditions. It is compared with the methods mentioned above by simulation studies. The simulation studies demonstrate that the sgsAM method has an advantage over those methods in terms of selection accuracy and computation time.====The remainder of the article is arranged as follows. In Section 2, the sgsAM method and its theoretical properties are presented. In Section 3, the simulation studies are reported and analyzed. In Section 4, a real data analysis is presented. Technical proofs are given in an Appendix.",A sequential approach to feature selection in high-dimensional additive models,https://www.sciencedirect.com/science/article/pii/S0378375821000434,24 April 2021,2021,Research Article,91.0
"Boukehil Djamila,Fourdrinier Dominique,Mezoued Fatiha","École Nationale Supérieure de Statistique et d’Économie Appliquée, LAMOPS, Tipaza, Algeria,Université de Normandie, UNIROUEN, UNIHAVRE, INSA Rouen, LITIS; avenue de l’Université, BP 12, 76801 Saint-Étienne-du-Rouvray, France,International Laboratory SSP & QF, National Research Tomsk State University, Russia,Rutgers University, Department of Statistics, 561 Hill Center, Busch Campus Piscataway NJ 08854, USA","Received 17 July 2020, Revised 8 April 2021, Accepted 10 April 2021, Available online 21 April 2021, Version of Record 17 May 2021.",https://doi.org/10.1016/j.jspi.2021.04.001,Cited by (0),"We consider estimation of the inverse scatter matrix ==== for a scale mixture of Wishart matrices under various Efron–Morris type losses, ==== for ====, where ====, where ==== denotes the Moore–Penrose inverse of ==== and ==== is invertible (====) and where ==== is singular.","The estimation of precision matrices has recently received much attention. Most of the literature is devoted to the case where the observation has a multivariate normal distribution so that the sample covariance matrix ==== has a Wishart distribution. In that case, ====, the inverse of the covariance matrix ====, is often called the precision matrix. This classical multivariate setting has been studied by Efron and Morris (1976), Haff (1977), Dey (1987), Krishnamoorthy and Gupta (1989), Dey et al. (1990), Zhou et al. (2001), and Tsukuma and Konno (2006). Note that, in these papers, ==== is assumed to be invertible. However, in the setting where the dimension of ==== is larger than the sample size, its inverse does not exist. Then estimation of ==== is based on the Moore–Penrose generalized inverse ====. This approach was, for instance, developed in  Kubokawa and Srivastava (2008).====Estimation of ==== has been extended to the distributional framework of elliptically contoured distributions. Thus Fourdrinier et al. (2016) considered a large subclass of the elliptically contoured distributions and improvements over the usual estimators (proportional to ==== or ==== according to the invertibility or singularity of ====) were provided under quadratic loss. Note that the form of the underlying distributions was such that no unbiased estimator of the risk was available.====In this article, we focus on the subclass of scale mixture of Wishart distributions for ====. Mixtures of Wishart are more and more popular in modeling. Thus Jian et al. (2009) use mixtures of Wishart distributions for modeling diffusion weighted magnetic resonance imaging while Yang et al. (2016) consider such distributions for the change of detection of polarimetric synthetic aperture radar images. Also,  Nielsen et al. (2016), in a Bayesian context, deal with a Wishart mixture model for modeling dynamic functional connectivity. In a more theoretical paper, Haff et al. (2011) consider the problem of estimating the mixing density of a continuous mixture of Wishart distributions, the parameter being a covariance matrix; they also provide an application to finance.====Here, as a first approach, we consider a real valued mixing variable, that is, we observe ==== from the mixture model ====where ==== denotes the Wishart distribution with ==== degrees of freedom and positive-definite covariance matrix ==== and where ==== is a known distribution on ====(and hence, is not estimated). This is a natural extension of the Wishart distribution as the variance mixture of normals extends the normal distribution, a case for which a wide literature exists when estimating a location parameter. Note that we may view the distribution of ==== as that of ====, where ==== has the Wishart distribution ==== and is independent of ====. Thus, for any function ==== such that its expectation with respect to (1.1) exists, we have ====where ==== denotes the expectation with respect to the model in (1.1) and ==== the expectation with respect to the mixing distribution ====. We will often use this form in the calculation below. Note also that, under the mixture model in (1.1), the population covariance matrix is proportional to ==== (the expectation of ==== is ====). For convenience, we refer to ==== as a scatter matrix and to ==== as a precision matrix.====Estimating ====, we consider a wide class of losses defined by ====for ====. and the associated risks ====Note that, except for ====, the ====’s are data-based losses in the sense that they are function of ====. They are reminiscent of the losses used by Efron and Morris (1976). In the Wishart case, loss ==== has been mostly considered for ==== and ====. For example, for the non singular Wishart distribution, loss ==== was considered by Efron and Morris (1976), while both ==== and ==== were considered by Haff (1979a) (actually, as for ====, the identity matrix ==== was replaced by an arbitrary positive definite matrix ====). Kubokawa and Srivastava (2008) considered ====, ====
 ==== for the singular case. The consideration of the case where ==== is natural and we will see that, in the cases where ==== and ====, optimal estimators in (1.4) exist while, the case ==== is similar to the case ====, where there are no such estimators.====Among the authors who consider non-Wishart distributions for ====, Sarr and Gupta (2009) dealt with a Kotz type model (when ==== in (1.2)). However this distribution is not in the framework of (1.1). Here, we provide a unified approach to the settings where ==== is invertible and ==== is singular. To this end, the notation ==== for the Moore–Penrose inverse of ==== when ==== is non invertible is also used when ==== is invertible (====). For Model (1.1), the usual, or reference, estimators of ==== are of the form ====where ==== is a positive constant. It is well known that such estimators may be inappropriate (see the references above). The alternative estimators that we consider are of the form ====where ==== is a constant and the matrix function ==== is homogeneous in the sense that ====The paper is organized as follows. In Section 2, we develop an unbiased estimator of the risk difference between ==== in (1.5) and ==== in (1.4), relying on a Stein–Haff type identity given in Haddouche et al. (2021) and valid for the two cases where ==== is invertible and ==== is singular. Note that this unbiased estimator of risk difference holds for all the losses in (1.2) and implies directly conditions under which ==== improves over ====. In Section 3, we consider the possibility of the existence of an optimal estimator in the class (1.4) of reference estimators, for each loss in (1.2). When such an optimal estimator does not exist, we propose an alternative estimator. In Section 4, when the order of homogeneity in (1.6) is -2, we provide explicit conditions for improvement of estimators of the form (1.5) over the reference estimators in (1.4). In addition, we give analogous results for estimators of the form ==== when ==== is orthogonally invariant and ==== is a real valued function. In Section 5, we illustrate the theory with examples reminiscent of those studied by Haff (1977) and Dey (1987), for each loss in (1.2). Also, in Section 6, we present a numerical study which complements these illustrations. Some concluding remarks are given in Section 7. Finally, we provide an Appendix which gathers most of the proofs of the results presented in this article.",Estimation of the inverse scatter matrix for a scale mixture of Wishart matrices under Efron–Morris type losses,https://www.sciencedirect.com/science/article/pii/S0378375821000409,21 April 2021,2021,Research Article,92.0
"Trong Dang Duc,Thanh Nguyen Hoang,Minh Nguyen Dang,Lan Nguyen Nhu","Faculty of Maths and Computer Science, University of Science, Ho Chi Minh City, Viet Nam,Vietnam National University, Ho Chi Minh City, Viet Nam,Centre for Mathematical Science, University of Science, Ho Chi Minh City, Viet Nam,Faculty of Basic Science, VanLang University, Viet Nam,Department of Fundamental Studies, Ho Chi Minh City Open University, Viet Nam","Received 21 December 2019, Revised 5 April 2021, Accepted 10 April 2021, Available online 18 April 2021, Version of Record 23 April 2021.",https://doi.org/10.1016/j.jspi.2021.04.002,Cited by (0),"We consider the model ==== where ==== is observable, ==== is a noise random variable with density ====, ==== has an unknown mixed density such that ====, ==== with ==== being continuous and ====, ====. Typically, in the last decade, the model has been widely considered in a number of papers for the case of fully known quantities ==== with an unknown ====. From i.i.d. copies ==== of ==== we will estimate ==== where ==== is the density of ","In applications, we often have the problem of finding the probability distribution of a random variable ==== from measurements contaminated with a noise ====. Simplicity, we can assume the additive model ==== with ==== observed. Estimation of the target density ==== of ==== from observations of the random variable ==== is called the ====. The literature of the specific problem has grown very rapidly in last decades (see Devroye, 1989, Meister, 2009 and references therein). Precisely, in the present paper, we assume the data model ====where the ==== are i.i.d. observable copies of the random variable ====, the ==== and the ====, ====, are mutually independent and have the same distribution as ====, ==== respectively. As is well known, any estimation tool of deconvolution problem is based on a combination of presumptions about the target density function and the density function of error.====We first discuss shortly the presumption on the target density function. In the deconvolution literature, most of the research is focused on continuous distributions. However, in some applications, the target random variable ==== is not continuous. We will illustrate this situation by a model in evolutionary biology where gene mutations play an important role. Mutations provide the raw material for plant breeding and genetic research and the study of mutation distribution is necessary to master the evolutionary dynamics. As presented in Gugushvili et al., 2011, Lee et al., 2010, there are two types of mutations: silent mutations without effect on virus fitness, and deleterious mutations that reduce virus fitness. The first mutation is detected by a random variable having a point-mass distribution and the second one is detected by a continuous random variable. Hence, the distribution of fitness effects of the mutations is a mixture of the point-mass and the continuous distribution. Studying this kind of distributions can be found in some papers, such as Gugushvili et al., 2011, Lee et al., 2010, Lee and Hall, 2013, Lee et al., 2015. Mathematically, we have ====where ==== and ====. In the model, the target random variable ==== has the density function ====Here ==== and the function ==== are the Dirac mass and the density of the continuous random variable ==== respectively. Because of scanty investigation of mixture target distributions, studying the non-continuous deconvolution problem for the distribution (2) is reasonable.====Next, we confer about the presumption on errors. Almost deconvolution papers are limited to the case of perfectly known errors. The estimability and consistency in the deconvolution papers were based heavily on that assumption. However, in many real-world problems, we often do not know fully the error distribution and we can only guess about a set of possible error distributions. In this case, we obtain a ==== problem. In Butucea and Matias (2005), the authors studied a semi-parametric problem with ==== and ==== unknown. We would try to expand these results to the models (1)–(2). Among many error distribution classes chosen in application, the normal distribution class is often mentioned. Hence, in the present paper, we assume as in Matias, 2002, Meister, 2006 that the errors ====, ==== are mutually independent and ====which have the density ====. In short, we have==== ==== ==== ==== ==== ==== (1)==== (2), (3)==== ==== ==== ==== ==== ==== ==== ====Here, ==== is the density of ====.====Let us provide some special aspects of our problem. The deconvolution problem of the models (1)–(2) was studied in papers (Gugushvili et al., 2011, Lee et al., 2010, Lee and Hall, 2013, Lee et al., 2015). Especially, the authors of Gugushvili et al. (2011) derived in detail the convergence rate of estimators when ==== is ordinary smooth, i.e., ====and when ==== is supersmooth, i.e., ====In the supersmooth case, using the presumption that ==== is in the Sobolev space of order ==== (see Condition 3 in Gugushvili et al. (2011)), the authors of Gugushvili et al. (2011) estimated ====with the minimax rates ==== respectively. If ==== then ==== and these minimax rates are ====In Gugushvili et al. (2011), the density function ==== and the location ==== of the point-mass are assumed to be perfectly known. However, as above-mentioned it is difficult to determine experimentally the exact value of the location ==== and the density ====. In the present paper, we partially relax these presumptions of Gugushvili et al. (2011). Since the quantities ==== are assumed to be unknown, the algorithm in Gugushvili et al. (2011) cannot be used to estimate ====. Therefore, in the present paper, new estimators for ==== are constructed. Moreover, the estimator errors for ==== have the minimax rates as in (5). We also prove the minimax property of the estimator for ====.====The rest of the present paper is divided into three sections. In order to have a panoramic view, we condense all main results of our papers in Section 2. For the convenience of readers, we postpone the presentation of proofs to the end of the papers. In Section 3, some numerical simulations are provided. Finally, the proofs will be presented in Section 4.",Density estimation of a mixture distribution with unknown point-mass and normal error,https://www.sciencedirect.com/science/article/pii/S0378375821000410,18 April 2021,2021,Research Article,93.0
Narukawa Masaki,"Faculty of Economics, Okayama University, 3-1-1 Tsushima-naka, Kita-ku, Okayama, 700-8530, Japan","Received 30 September 2019, Revised 12 March 2021, Accepted 23 March 2021, Available online 2 April 2021, Version of Record 20 April 2021.",https://doi.org/10.1016/j.jspi.2021.03.005,Cited by (0),"The semiparametric estimation of multivariate fractional processes based on the tapered ==== of the differenced series is considered in this paper. We construct multivariate local Whittle estimators by incorporating the maximal efficient taper developed by Chen (2010). The proposed estimation method allows a wide range of potentially nonstationary long-range dependent series, being invariant to the presence of deterministic trends with the same extent of the differencing order, without a two-step procedure. We establish the consistency and ==== of the proposed estimators, which have no discontinuities, and show that the ==== is the same as that of the nontapered local Whittle estimation by increasing the order of a taper to infinity with a moderately slow rate. We examine the finite sample behavior of the proposed estimators through a simulation experiment.","The statistical analysis of multivariate fractional processes, which not only covers strong persistence or long-range dependence in autocorrelation but also captures the interdependence between different series, has been studied theoretically and extensively over the past few decades. In the context of fractional time series, statistical inference on the fractionally integrated order that describes long-run behavior, usually called the memory parameter, is often a central concern. This paper focuses on the semiparametric estimation of the memory parameters in multivariate fractional processes to obtain estimators robust to the model misspecification of short-run behavior and avoid invalid inference, especially the inconsistency in the estimated memory parameters caused by incorrect modeling.====One of the most popular semiparametric methods for analyzing fractional time series is the local Whittle (LW) estimation (also called the Gaussian semiparametric estimation), which was introduced by Künsch (1987) and theoretically established by Robinson (1995) within a univariate stationary and invertible framework. Lobato (1999) extended the LW estimation to a stationary and invertible multivariate setting, with Lobato and Velasco (2000) generalizing it further to nonstationary processes by tapering the periodogram. However, using the tapered periodogram inflates the asymptotic variance of the estimated memory parameters; thus, this approach comes at the cost of the asymptotic efficiency of the estimators. Moreover, since it is based on a two-step estimation method for alleviating the computational burden associated with a multidimensional minimization problem, in which the stationary or nonstationary LW estimation of Robinson (1995) or Velasco (1999b) is employed for each of the univariate elements in the multivariate time series as the first step, the consistency of the memory parameters in the multivariate setting is not considered. Shimotsu (2007) refined Lobato’s approach by examining the behavior of the spectral density around the origin in detail and proposed the more efficient multivariate LW estimation, in which the two-step estimation method is not adopted, and the consistency of the estimators is given as well as the asymptotic normality for the stationary and invertible fractional processes.====There are two other approaches to a nonstationary generalization of the LW estimation. The first is to invoke the extended discrete Fourier transform and periodogram for nonstationary fractional processes, called the fully extended LW (FELW) estimation, which was considered by Abadir et al. (2007) within a univariate framework. Their approach generates no extra loss of efficiency in the estimated memory parameter unlike the tapered periodogram approach, thus having the same limiting distribution as in the stationary and invertible case of Robinson (1995). Naielsen (2011) extended this approach to a multivariate setting based on the local specification of a spectral density around the origin proposed by Shimotsu (2007), showing the consistency and asymptotic normality of the estimated memory parameters. To allow for deterministic trend components, including cyclical behavior, in the univariate process, Abadir et al. (2011) proposed a detrending method for the FELW estimation using ordinary least squares, revealing that the asymptotic normality is unaffected; however, it may be required to utilize prior information on the structure of deterministic components, with their approach developing into a two-step procedure. Nevertheless, there seem to be some disadvantages of this approach, as discussed by Faÿ et al. (2009) for example. The estimators employing the extended periodogram do not cover the case where the memory parameter takes the values ====, ====, implying that their asymptotic properties have discontinuities at these points. Moreover, Cheung and Hassler (2020) pointed out the discontinuity of the objective function for the univariate FELW estimator at ==== by using the simulation experiments which can cause serious size distortions of the studentized test statistics in finite samples for some range of memory parameters.====The second approach is the exact local Whittle (ELW) estimation developed by Shimotsu and Phillips (2005) based on the so-called type II definition of a fractional process, whereas the tapered and extended periodogram approaches presume a type I fractional process (for details on the two definitions, see e.g., Marinucci and Robinson, 1999, Robinson, 2005), and covers the memory parameter included in any interval of length less than ====. They investigated the asymptotic properties, showing that the limiting distribution is the same as that of Robinson (1995) in which no discontinuities exist. Although the ELW estimation seems to be more general than the above, it requires the condition that a zero mean process is postulated, or the mean of a process is given. Shimotsu (2010) addressed this limitation by proposing the two-step ELW estimation to deal with the unknown mean and the deterministic polynomial trends, in which the tapered periodogram approach is employed as the first step estimator. However, at the cost of the demeaning or the detrending procedure, the same asymptotic normality as above holds for ==== or ====, respectively. The multivariate extension has not been explicitly proposed and established, but suggested by Shimotsu (2012) as the two-step procedure in the bivariate fractional cointegrated system. Hence, from a practical viewpoint, it seems that the ELW estimation cannot avoid invoking the tapered periodogram approach at the first step; moreover, it is likely that its estimation performance will be affected by the first step estimator.====This paper proposes a multivariate LW estimation of potentially nonstationary fractional processes using the maximal efficient tapered periodogram of the differenced series, which was developed by Chen (2010) to study univariate fractional time series. Thus, we provide an extension to the multivariate setup including the relevant asymptotic results. The approach of differencing the series before constructing the tapered periodogram was first introduced by Hurvich and Chen (2000) as an alternative to Velasco’s (1999b) method of directly applying a taper to the original data. To our knowledge, that class of tapers appears to be the most efficient because the variance inflation factor accompanying tapering tends to ==== under certain conditions. A similar class of tapers was considered by Hurvich et al. (2005), with these classes being a generalization of those proposed by Hurvich and Chen (2000). Therefore, we expect that the efficient tapered multivariate LW (ETMLW) estimators constructed in this paper have a much smaller asymptotic variance than the tapered ones of Lobato and Velasco (2000), with no discontinuities in the limiting distribution. Moreover, we establish the consistency and asymptotic normality of the estimators. If a taper order tends to infinity with an increasing rate slower than and depending on the so-called bandwidth necessary for the local semiparametric methods, the variance inflation factor of the estimators is asymptotically close to 1 due to efficient tapering, similar to the univariate case of Chen (2010). Hence, we show that the proposed estimation method can achieve the same asymptotic variance as the nontapered multivariate LW estimation in the stationary and invertible case; thus, the efficiency loss is asymptotically recovered. Moreover, although the proposed estimators are invariant to the additive deterministic polynomial trends in each of the series because of the differencing manipulation, this property hinges on the differencing order or, at most, holds to the same degree as it. Furthermore, it would enable the utilization of the proposed estimators as the first step of the ELW estimation, which might produce a more refined approach; however, the tractability is clearly reduced and the theoretical gap due to the different types of fractional processes should be bridged. Thus, the proposed estimation method will provide a potential alternative to the existing approaches for obviating the discontinuities in the asymptotic properties and the two-step procedure.====The remainder of this paper is organized as follows. Section 2 describes the multivariate fractionally integrated model for the semiparametric estimation considered in this paper. Section 3 introduces the maximal efficient tapered periodogram and constructs the multivariate LW estimation based on it. Section 4 provides the asymptotic properties of the estimator and presents a set of conditions for deriving them. In Section 5, we perform a Monte Carlo experiment for the finite sample behavior of the proposed estimation method. Section 6 summarizes the conclusions, with the appendix providing the proofs of the theorems stated in Section 4.",Efficient tapered local Whittle estimation of multivariate fractional processes,https://www.sciencedirect.com/science/article/pii/S0378375821000380,2 April 2021,2021,Research Article,94.0
Ouimet Frédéric,"California Institute of Technology, Pasadena, USA","Received 23 January 2020, Revised 24 March 2021, Accepted 24 March 2021, Available online 1 April 2021, Version of Record 19 April 2021.",https://doi.org/10.1016/j.jspi.2021.03.006,Cited by (20)," on most subsets of ====. Furthermore, we discuss a recent application of the result to obtain ","Given a set of probability weights ==== that satisfies ====, the ==== probability mass function is defined by ====where ==== and ====. The covariance matrix of the multinomial distribution is well-known to be ====, where ====, see e.g. Severini (2005, p. 377). From Theorem 1 in Tanabe and Sagae (1992), we also know that ====. The purpose of this paper is to establish an asymptotic expansion for (1.1) in terms of the multivariate normal density with the same covariance profile, namely: ====This kind of expansion can be useful in all sorts of estimation problems; we give three examples in Section 3. For a general presentation on local limit theorems, see e.g. Kolassa (1994).",A precise local limit theorem for the multinomial distribution and some applications,https://www.sciencedirect.com/science/article/pii/S0378375821000392,1 April 2021,2021,Research Article,95.0
Sabbaghi Arman,"Department of Statistics, Purdue University, 150 North University Street, West Lafayette, IN 47907, USA","Received 18 August 2019, Revised 28 February 2021, Accepted 8 March 2021, Available online 29 March 2021, Version of Record 20 April 2021.",https://doi.org/10.1016/j.jspi.2021.03.003,Cited by (0),Geometric and hidden projection are two important properties to consider for three-level fractional ,None,An integrative framework for geometric and hidden projections in three-level fractional factorial designs,https://www.sciencedirect.com/science/article/pii/S0378375821000367,29 March 2021,2021,Research Article,96.0
"Jiang Dandan,Hou Zhiqiang,Hu Jiang","School of Mathematics and Statistics, Xi’an Jiaotong University, No.28, Xianning West Road, Xi’an 710049, China,School of Statistics, Shandong University of Finance and Economics, No.7366, East Erhuan Road, Jinan 250014, China,KLASMOE, School of Mathematics and Statistics, Northeast Normal University, No.5268 Renmin Street, Changchun 130024, China","Received 17 December 2019, Revised 16 January 2021, Accepted 16 March 2021, Available online 26 March 2021, Version of Record 16 April 2021.",https://doi.org/10.1016/j.jspi.2021.03.004,Cited by (1),A generalized spiked Fisher matrix is considered in this paper. We establish a criterion for the description of the support of the limiting ,"Consider the spiked model involved with two sample covariance matrices, ====where ==== and ==== are general covariance matrices and ==== is a symmetric matrix with a finite rank. This two-sample spiked model has wide applications to many fields, including signal processing, regression analysis in Wang and Yao (2017) and Johnstone and Nadler (2017), etc. In a signal detection with noise system, the signal observation ==== is a ====-dimensional vector with the form ====where ==== is a ==== signal with a unit covariance matrix, ==== is a mixing matrix with size ====, and ==== is an i.i.d. noise with covariance matrix ====. Obviously, the covariance matrix ==== of ==== can be denoted as the form in (1). In fact, testing the presence of signals and determining the number of signals in signal processing can be transformed to test ==== or ==== and determine the rank of ==== or ====, respectively. Furthermore, it sets up the corresponding relationship between the number of the spiked eigenvalues of the Fisher matrix ==== and the number of signals, where ==== and ==== are the sample covariance matrices corresponding to ==== and ====, respectively.====Additionally, James (1964) provided a remarkable five-way classification of the distribution theory associated with some classical multivariate problems. For example, in testing the linear hypothesis, there are four classical statistics, the likelihood ratio criterion, the Lawley–Hotelling trace criterion, the Bartlett–Nanda–Pillai trace criterion and the Roy Maximum root criterion. For more details, see Anderson, 2003, Muirhead, 1982 and Bodnar et al. (2019). These four statistics are expressed as the form of spectral of a Fisher matrix under the null hypothesis. Under the alternative hypothesis, these tests are based on the sample spiked eigenvalues of the Fisher matrix ====. However, the sample spiked eigenvalues do not converge to their corresponding population spiked eigenvalues if the dimensionality ==== goes to infinity. Therefore, traditional testing methods and their asymptotic laws lose efficiency in such a case. Thus, a study of the limits of sample spiked eigenvalues is necessary.====There are many works that investigate the spiked model in a high-dimensional setting. As is well known, the spiked model, first proposed by Johnstone (2001), can be seen as a special case of (1) with ====, which has the same approach as that of principal component analysis (PCA). Then, some relevant works are devoted to improving the study of the one-sample spiked model, such as Bai and Ng, 2002, Baik et al., 2005, Baik and Silverstein, 2006, Paul, 2007, Bai and Yao, 2008, Bai and Yao, 2012, Onatski, 2009, Onatski, 2012, Fan and Wang, 2017 and Cai et al. (2020). Some related studies are also devoted to investigations of PCA or factor analysis (FA), which can be seen as another way of understanding the spiked model, including Hoyle and Rattray, 2004, Nadler, 2008, Jung and Marron, 2009, Shen et al., Berthet and Rigollet, 2013 and Birnbaum et al. (2013), etc. Recently, Jiang and Bai (2021) extended the work to a general case and gave the limits and CLT for the sample spiked eigenvalues of a generalized covariance matrix.====In contrast, there are only a few studies related to the two-sample spiked model. Wang and Yao (2017) assumed that ==== is an identity matrix with a rank M perturbation or diagonal block independent and presented the limits of the extreme eigenvalues of a high-dimensional spiked Fisher matrix. In addition, Johnstone and Onatski (2020) described the relationship between the two-sample spiked models with some classical statistical problems that lead to each of James’ five-way classification mentioned above in James (1964). In the alternative hypothesis, they focused on the two-sample spiked model with ==== being a rank-one matrix that is used to derive the asymptotic power for testing the presence of a spike. The spiked model they considered is based on a noncentral Fisher matrix, which is different from ours. In addition, the Gaussian assumption is required in Johnstone and Onatski (2020). The main contributions of this paper include: we relax the unrealistic conditions posed in the previous works, such as the covariance matrices are assumed to be diagonal or diagonal block-wise structure. The proposed conclusions are obtained without the Gaussian constraint. In addition, we also give a consistent estimator of the population spiked eigenvalues.====The paper is organized as follows. In Section 2, we present the almost sure limits of the sample spiked eigenvalues for a high-dimensional generalized Fisher matrix and establish a criterion for the description of the support of the limiting spectral distribution of high-dimensional generalized Fisher matrix, which are the main results of the paper. Section 3 gives estimators of the population distant spiked eigenvalues for the generalized Fisher matrix. In Section 4, we conduct simulations that support the theoretical results and illustrate the accuracy of the estimators of the population distant spiked eigenvalues. Technical lemmas and proofs are postponed to the Appendix.",The limits of the sample spiked eigenvalues for a high-dimensional generalized Fisher matrix and its applications,https://www.sciencedirect.com/science/article/pii/S0378375821000379,26 March 2021,2021,Research Article,97.0
"Hsu Hsiang-Ling,Huang Mong-Na Lo","Institute of Statistics, National University of Kaohsiung, Kaohsiung, Taiwan, ROC,Department of Applied Mathematics, National Sun Yat-sen University, Kaohsiung, Taiwan, ROC","Received 21 March 2020, Revised 13 February 2021, Accepted 5 March 2021, Available online 22 March 2021, Version of Record 10 April 2021.",https://doi.org/10.1016/j.jspi.2021.03.002,Cited by (0),"In this work, optimal design problems for estimation of unknown parameters for a flexible class of non-normal distributions useful for describing various data types are considered. A particular model, designated the simplex dispersion model, can be applied to model proportional (or compositional) outcomes confined within the (0, 1) interval. The main interest here is to determine the optimal experimental settings to be able to estimate the unknown model parameters more accurately and efficiently. Locally ====-optimal designs for accurate estimation of parameters in the simplex dispersion model are characterized through the corresponding equivalence theorem and under certain cases with some given prior information, optimal design results are presented for illustration. Examples including a water purification experiment and a dose study are used to demonstrate the efficiencies of the corresponding ====-optimal designs.","Continuous proportional outcomes where responses are confined within the interval (0, 1) are observed and collected from many practical studies. This type of data, also named as percentage or ratio data, is acquired quite often in public health or environmental sciences. In environmental study or drug analysis, we may be concerned with the percentage of reduction on observed response values such as the water purification rate or cure rate etc. The main purpose of this paper is to investigate related optimal design problems for estimation of parameters of models suitable for proportional data. Wu et al. (2005) and Latif and Zafar Yab (2015) had considered design problems for modeling proportional response data based on beta regression with one variable ==== restricted to [0, 1].====For analyzing binary response data, the generalized linear model (GLM) with binomial or logistic distributions are often used to model the corresponding response probability by distribution functions taking values within (0, 1). Here instead, a special type of distribution in the class of dispersion models (DM), defined by Jørgensen (1997) and introduced later in Section 2 will be used, which is a very appealing family of distributions to model the proportional type of data considered here. The distribution in this class, denoted as the “simplex dispersion model (SD model, ====)”, has support within the (0, 1) interval, rendering it be a possible choice for characterizing continuous proportional data. Some works on longitudinal studies have employed this type of model; see Song and Tan (2000) and Qiu et al. (2008). Song and Tan (2000) demonstrated that the variance–covariance matrix of the estimator of the unknown parameters in the SD model is formed by a function containing the model parameters. In what follows, we consider the locally optimal design problems and omit the word “locally” for simplicity.====In the development of optimal design theory, weighted polynomial regression models with variance functions depending on the explanatory variable have played an important role. In the literature on optimal design theory about estimation of linear models, the differential equation methodology for weighted polynomial regression models was utilized initially in Karlin and Studden (1966), Huang et al. (1995), Chang and Lin (1997), Imhof et al. (1988), and Dette et al. (1999) thereafter. For the polynomial regression model without an intercept, Huang et al. (1995) treated the design problem as that for a special type of weighted polynomial model, and derived appropriate differential equations and solved corresponding eigenvalue problems for obtaining the optimal solution. See Sitter and Torsney, 1995a, Sitter and Torsney, 1995b, and Torsney and Musrati (1993) among others for the discussions on ====-optimal designs under the weighted models. Using geometric and other arguments, Musrati (1992) reviewed and augmented the methods of optimal designs for non-linear problems with a single variable. Dette and Trampisch (2010) unified different types of ====-optimal design problems in weighted, univariate polynomial regression models for a broad class of efficiency functions. Here the construction of ====-optimal designs for simplex dispersion models can also be viewed as that for weighted polynomial regression model with a special type of variance which has not been studied before. The corresponding weight function for the SD model has an interesting feature which can be expressed as the sum of two functions where one is proportional to the inverse of the other.====The rest of the paper is organized as follows. Preliminaries about dispersion models and optimal designs for nonlinear models are presented in Section 2. Section 3 centers around the characteristics of the weight function of the corresponding information matrix and the ====-optimal designs for continuous proportional data with linear link functions, respectively, in the simplex dispersion model. Examples including a water purification experiment and a dose study are illustrated in Section 4. Finally, discussion and conclusions are presented.",-optimal designs for estimation of parameters in a simplex dispersion model with proportional data,https://www.sciencedirect.com/science/article/pii/S0378375821000355,22 March 2021,2021,Research Article,98.0
"Li Wenlong,Liu Min-Qian,Yang Jian-Feng","School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China","Received 11 August 2019, Revised 23 September 2020, Accepted 28 December 2020, Available online 19 March 2021, Version of Record 31 March 2021.",https://doi.org/10.1016/j.jspi.2020.12.005,Cited by (5),", inherit the attractive two-dimensional space-filling property of strong orthogonal arrays, and can accommodate twice or more number of factors than the existing strong orthogonal arrays. In addition, the proposed designs with four levels enjoy an attractive space-filling property under the maximin distance criterion. The construction methods are convenient and flexible, and the resulting designs are useful in computer experiments.","Computer experiment is a popular and powerful tool to investigate the complex phenomena and systems in engineering and sciences. The implementation of computer experiments needs space-filling designs (Santner et al., 2013, Fang et al., 2006). A commonly used approach for constructing space-filling designs is to adopt orthogonal arrays and similar structures. Randomized orthogonal arrays (Owen, 1992) and orthogonal array-based Latin hypercubes (Tang, 1993) employ orthogonal arrays of strength ==== to realize ====-dimensional space-filling property. Motivated by (====-nets from quasi-Monte Carlo (Niederreiter, 1992 Chap. 4), He and Tang (2013) introduced the concept of strong orthogonal arrays (SOAs) and found their attractive space-filling property for computer experiments. These arrays of strength ==== are more space-filling than ordinary orthogonal arrays in less than ==== dimensions and have the same space-filling property as the latter ones in ==== dimensions. However, SOAs, to enjoy more attractive space-filling property than orthogonal arrays, must have strength three or higher. He and Tang (2014) examined the characterizations of SOAs of strength 3. Given the number of runs, the number of factors for an SOA of strength 3 is very small because such arrays are based on orthogonal arrays of strength 3. In order to increase the number of factors and retain the two-dimensional space-filling property of SOAs of strength 3, He et al. (2018) proposed a new class of arrays, called SOAs of strength ====.====Obviously, both the SOAs of strength 3 in He and Tang (2014) and SOAs of strength ==== in He et al. (2018) have no column orthogonality. Nevertheless, the column orthogonality is of great importance. Joseph and Hung (2008) argued that minimizing the correlations among the columns will help in estimating the linear trends efficiently when the universal kriging model with linear trends is considered. Furthermore, the column orthogonality, viewed as a stepping stone, helps finding space-filling designs when Gaussian process models are considered (Bingham et al., 2009). Many researchers have discussed the column orthogonality of Latin hypercube designs including Ye, 1998, Steinberg and Lin, 2006, Sun et al., 2009, Lin et al., 2009, and so on. Liu and Liu (2015) constructed column-orthogonal strong orthogonal arrays (OSOAs) of strength ==== based on orthogonal arrays of strength ==== while the numbers of factors are still very small. Zhou and Tang (2019) further examined OSOAs of strength ==== with relatively more factors.====In this paper, we propose column-orthogonal nearly strong orthogonal arrays (ONSOAs) to accommodate much more factors than the existing ones. The resulting designs have the column orthogonality, attractive stratifications in two dimensions and flexible run sizes with very high factor-to-run ratios, almost equal to that of the orthogonal arrays. Such stratifications, as shown in Definition 1, are a type of space-filling property. In addition, the proposed ONSOAs with four levels enjoy an attractive space-filling property under the maximin distance criterion. The construction methods involve two key ideas. The first is that the specific three-column submatrices form orthogonal arrays of strength three. The second, due to Steinberg and Lin (2006) and Lin et al. (2009), is that rotating the points in an orthogonal array will preserve the column orthogonality of the original orthogonal array.====Compared to stratifications in Definition 1, any two columns of an orthogonal Latin hypercube design from Pang et al. (2009) only achieve a stratification on an ==== grid, and their designs have a severe restriction of run sizes. Many pairwise columns of orthogonal Latin hypercube designs from Lin et al. (2009) cannot achieve such stratifications. Thus the above two works can be regarded as a motivation for constructing column-orthogonal designs with good stratifications. The proposed ONSOAs with many levels, not requiring the number of levels for each factor to be the same as the number of runs, enjoy good stratifications as shown in Definition 1. In addition, Bingham et al. (2009) discussed the rationale and usefulness for constructing exactly or nearly column-orthogonal designs that are not Latin hypercube designs but still have many levels.====The remainder of this paper is organized as follows. Section 2 provides some preliminaries and defines the ONSOA. Section 3 investigates two construction methods for the ONSOAs. Section 4 is devoted to some comparisons between the ONSOAs and other types of SOAs. Section 5 examines the space-filling property of the constructed ONSOAs under the maximin distance criterion. Section 6 contains some concluding remarks.",Column-orthogonal nearly strong orthogonal arrays,https://www.sciencedirect.com/science/article/pii/S0378375821000343,19 March 2021,2021,Research Article,99.0
"Laha Nilanjana,Miao Zhen,Wellner Jon A.","Department of Biostatistics, Harvard University, 677 Huntington Ave, Boston, MA 02115, United States of America,Statistics, Box 354322, University of Washington, Seattle, WA 98195-4322, United States of America","Received 24 May 2020, Revised 6 October 2020, Accepted 1 March 2021, Available online 13 March 2021, Version of Record 23 March 2021.",https://doi.org/10.1016/j.jspi.2021.03.001,Cited by (0),"We introduce new shape-constrained classes of distribution functions on ====, the bi-====-concave classes. In parallel to results of Dümbgen et al. (2017) for what they called the class of bi-log-concave distribution functions, we show that every ====-concave density ==== has a bi-====-concave distribution function ==== for ====.====Confidence bands building on existing nonparametric confidence bands, but accounting for the shape constraint of bi-====-concavity, are also considered. The new bands extend those developed by Dümbgen et al. (2017) for the constraint of bi-log-concavity. We also make connections between bi-==== which plays an important role in the theory of quantile processes.","Statistical methods based on shape constraints have been developing rapidly during the past 15–20 years. From the classical univariate methods based on monotonicity going back to the work of Grenander (1956) and van Eeden (1956) in the 1950s and 1960s, research has progressed to consideration of convexity type constraints in a variety of problems including estimation of density functions, regression functions, and other “nonparametric” functions such as hazard (rate) functions. See Samworth and Sen (2018) for a summary and overview of some of this recent activity.====One very appealing shape constraint is log-concavity: a (density) function ==== is ==== if ==== is concave (with ====). See Samworth (2018) for a recent review of the properties of log-concave densities and their relevance for statistical applications. While much of the current literature has focused on point estimation, our main focus here will be on inference for one-dimensional distribution functions and especially on (honest, exact) confidence bands for distribution functions which take advantage of shape constraints.====To this end, Dümbgen et al. (2017) introduced the class of ==== distribution functions defined as follows: a distribution function ==== on ==== is bi-log-concave if both ==== and ==== are log-concave. They provided several different equivalent characterizations of this property, and noted (the previously known fact) that if ==== is a log-concave density, then the corresponding distribution function ==== and survival function ==== are both log-concave. But the converse is false: there are many bi-log-concave distribution functions ==== with density ==== which fail to be log-concave; see Section 2 for an explicit example.  Dümbgen et al. (2017) also showed how to construct confidence bands which exploit the bi-log-concave shape constraint and thereby obtain narrower bands, especially in the tails, with correct coverage when the bi-log-concave assumption holds.====However, a difficulty with the assumption of bi-log-concavity is that the corresponding density functions inherit the requirement of exponentially decaying tails of the class of log-concave densities, and this rules out distribution functions ==== with tails decaying more slowly than exponentially. Here we introduce new shape-constrained families of distribution functions ====, which we call the ====, with tails possibly decaying more slowly (or more rapidly) than exponentially. As the name indicates, these families involve a parameter ==== which allows heavier than exponential tails when ====, lighter than exponential tails when ====, and which correspond to exactly the bi-log-concave class introduced by Dümbgen et al. (2017) when ====.====Here is an outline of the rest of the paper. In Section 2 we give careful definitions of the new classes of ====-====. We also present several helpful examples and discuss some basic properties of the new classes and their connections to the classes of ====-concave densities studied by Borell, 1975, Brascamp and Lieb, 1976, and Rinott (1976). (See also  Dharmadhikari and Joag-Dev (1988), and Gardner (2002).) Section 3 contains the main theoretical results of the paper. The connection between the bi-====-concave class and a key condition in the theory of quantile processes, the Csörgő–Révész condition, is discussed in Corollary 4. Finally, we give two tail bounds for distribution functions ====, see Corollary 5.====In Section 4 we first introduce the new confidence bands for a distribution function ==== assuming ==== is known. We also discuss some of their theoretical properties: the consistency of confidence bands is discussed in Theorem 7, and Theorem 9 provides a rate of convergence for linear functionals of bi-====-distribution functions contained in the bands. This extends Theorem 5 of Dümbgen et al. (2017). We then briefly discuss the algorithms used to compute the new bands, and illustrate the new bands with real and artificial data. Section 5 gives a brief summary and statements of further problems. An especially important remaining problem concerns construction of confidence bands when ==== is unknown. The proofs for all the results in Sections 2,  3, and  4 are given in Sections 6 and Appendix.====We conclude this section with some notation which will be used throughout the rest of the paper. The supremum norm of a function ==== is denoted by ====, and for ==== we write ====. For a function ====, ==== assuming that the indicated limits exist. In general, we use ==== and ==== to denote a distribution function and the corresponding density function with respect to Lebesgue measure, and we set ====.",Bi-,https://www.sciencedirect.com/science/article/pii/S0378375821000331,13 March 2021,2021,Research Article,100.0
Zhang Shibin,"Department of Mathematics, Shanghai Normal University, 100 Guilin Rd., Shanghai 200234, China","Received 11 June 2019, Revised 28 December 2020, Accepted 27 February 2021, Available online 9 March 2021, Version of Record 20 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.010,Cited by (4),This paper is concerned with testing the second-order ,"The assumption of second-order stationarity is often made when analyzing time series since it usually yields some well-established statistical methods. But the model may be misspecified if we use an inappropriate test approach. Therefore, it is of importance to test the second-order stationarity of a time series by a simple, powerful and easily implementable test approach.====Over the past several years, different types of statistics have been proposed to test the second-order stationarity of a time series. One is the spectrum-comparison-based statistics. Paparoditis (2009) developed a nonparametric test for stationarity against the alternative of a smoothly time-varying spectral structure. The test is based on a comparison between the sample spectral density calculated locally on a moving window of data and a global spectral density estimator based on the whole stretch of observations. von Sachs and Neumann (1999) proposed a test for stationarity of a time series against the alternative of a time-varying covariance structure. They constructed the test statistic based on the estimators of Haar wavelet series expansion coefficients of time-varying periodogram. Based on a comparison of the estimated spectral distribution on different segments of the observed time series, Preuss et al. (2015) proposed a bootstrap procedure to detect the existence of structural breaks. When computing the test statistic, they fitted an ==== model to the observed data. The second type is the DFT-uncorrelation-based statistics. Dwivedi and Subba Rao (2011) found that the discrete Fourier transform (DFT) at the canonical frequencies is asymptotically uncorrelated if and only if the time series is second-order stationarity. Using this property, they constructed a Portmanteau type test for testing stationarity of the time series. Lately, Jentsch and Subba Rao (2015) and Bandyopadhyay and Subba Rao (2017) extended the idea to test stationarity for multivariate time series and irregularly spaced spatial data, respectively. The third type is the max-type test statistics. Hušková et al. (2007) discussed various max-type test statistics for detecting changes in the parameters of an autoregressive (AR) time series. Kirch et al. (2015) extended their statistics to the vector autoregressive (VAR) model and the epidemic change alternative. The construction of this type of statistics depends on several parametric estimators of the AR (VAR) model. The fourth type is the CUMSUM-based statistics. Andreou and Ghysels (2008) made a wide review of the literature on this type of statistics detecting structural breaks in financial time series. The last two types of statistics are specifically designed for the detection of change points in a time series, which is a particular example of nonstationarity. Recently, there also exist some papers that are concerned about testing stationarity of functional time series (e.g. Horváth et al., 2014) and measuring stationarity of locally stationary processes (Dette et al., 2011).====Almost all the afore-mentioned first two types of statistics for testing stationarity are built up from some spectral density estimators, which require the specification of smoothing parameters, while the last two types of statistics focus mostly on change-point detection and are constructed by some estimators of model parameters. In this paper, we provide a statistic for testing the second-order stationarity of a time series, which is independent of any choice of smoothing parameter and parametric estimator, but applicable to an extensive type of time series regardless of whether the alternatives are smoothly- or abruptly-varying. Like the test statistics constructed from local periodograms (e.g. Paparoditis, 2009, Preuß et al., 2013, Dette et al., 2011), our proposed statistic also relies on the blockwise scheme of the time series. We provide some recommendations for reliable and practical segmentation of the time series in Section 4.2.====Let ==== be a time series. In this paper, we test the null hypothesis ====In the proposed test procedure, we first divide the time series into equally-spaced segments and then transform the testing problem (1.1) to comparing all the local time-averaged spectral densities at different segments. The consistency of the local time-averaged spectral densities at each of the two different segments is gauged by an Anderson–Darling-like (A–D-like) statistic. Maximizing the A–D-like statistics, each of which compares the local time-averaged spectral densities at two different segments, gives the proposed statistic. By transforming the problem to the goodness-of-fit (gof) test that the periodogram-ratios of two univariate stationary time series at different frequencies are sampled from ====, Zhang and Tu (2018) recently proposed a Pearson-like statistic to test the equality of two time-invariant spectra, where ==== denotes an ==== distribution with 2 and 2 degrees of freedom. To avoid the choice of partition sets, Zhang and Tu (2019) proposed an A–D-like statistic to compare spectra. They also noted that the A–D-like statistic has some advantages such as exchangeability with respect to the numerator and denominator of the periodogram-ratio as well as powerfulness of the test. Therefore, when constructing the proposed test statistic, we use the A–D-like statistic to evaluate to what extent the empirical distribution of the periodogram-ratios of two different segments at the same frequencies is close to the ====. Moreover, the proposed statistic is very convenient to use as its distribution can be approximated by a pseudo-bootstrap scheme not relying on any specification of the time series.====We believe this paper makes several significant contributions to test stationarity. First and foremost, the proposed statistic relies only on the local periodograms, which are not only independent of any nonparametric spectral estimator that requires specification of smoothing parameters but also any parametric estimator. Second, the proposed approach is applicable to detect the non-stationarity regardless of whether the alternative time series has abrupt or smooth changes. Third, the proposed test is easy to use and more computationally efficient than most of other existing alternative tests in this field. Finally, the proposed test achieves good performance for a very broad class of time series, as evidenced by our simulation results.====The rest of this article is organized as follows. Section 2 presents the proposed statistic with the motivation to construct it. Section 3 presents the sampling properties of the test statistic under the null hypothesis and under an alternative hypothesis. Section 4 explains how to use the test statistic to test the second-order stationarity of a time series. Section 5 reports results for examining performance of the proposed test. The use of the proposed procedure in analyzing the stationarity of the El Niño-Southern Oscillation is illustrated in Section 6. Section 7 contains our concluding remarks. All technical proofs are deferred in the Appendix.====Supplementary materials related to this article, including some R programs and a guide for using them, are available online.",A test for second-order stationarity of a time series based on the maximum of Anderson–Darling statistics,https://www.sciencedirect.com/science/article/pii/S0378375821000264,9 March 2021,2021,Research Article,101.0
"Masuda Hiroki,Uehara Yuma","Faculty of Mathematics, Kyushu University, Motooka 744 Nishi-ku, Fukuoka 819-0395, Japan,Faculty of Engineering Science, Kansai University, 3-3-35 Yamate-cho Suita-shi, Osaka 564-8680, Japan","Received 6 March 2020, Revised 14 November 2020, Accepted 16 February 2021, Available online 5 March 2021, Version of Record 24 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.008,Cited by (1),"We consider ==== estimation of the continuous part of a class of ergodic diffusions with jumps based on high-frequency samples. Various papers previously proposed threshold based methods, which enable us to distinguish whether observed increments have jumps or not at each small-time interval, hence to estimate the unknown parameters separately. However, a data-adapted and quantitative choice of the threshold parameter is known to be a subtle and sensitive problem. In this paper, we present a simple alternative based on the Jarque–Bera normality test for the Euler residuals. Different from the threshold based method, the proposed method does not require any sensitive fine tuning, hence is of practical value. It is shown that under suitable conditions the proposed estimator is asymptotically equivalent to an estimator constructed by the unobserved fluctuation of the continuous part of the solution process, hence is asymptotically efficient. Some numerical experiments are conducted to observe finite-sample performance of the proposed method.","Consider the following one-dimensional stochastic differential equation with jumps: ====defined on a complete filtered probability space ====. The ingredients are as follows:====Throughout this paper, we assume that there exists a true value ====, and that the intensity parameter ==== is positive. We want to estimate ==== based on a discrete-time but high-frequency observation ==== from a solution to (1.1), where the sampling times are supposed to be equally spaced: ====for a positive sequence ==== such that ==== and the terminal sampling time ====.====For diffusion models whose coefficients are possibly nonlinear with respect to unknown parameters, many estimators of ==== have been proposed and theoretically analyzed, such as Gaussian quasi-likelihood estimator (Kessler, 1997), adaptive estimator (Uchida and Yoshida, 2012), multi-step estimator (Kamatani and Uchida, 2015), to mention few. Hence the special of the coefficients in (1.1) may seem restrictive. However, we are particularly interested in models which can be estimated without heavy computational effort. As will be mentioned in Section 4, we do not need any numerical search of a maximizer to estimate ==== as good as virtual situation where we know every jump instances over ====. A typical example expressed by (1.1) are mean-reverting processes with jumps: ====with ====. Especially when the diffusion coefficient is ====, the above process corresponds to Ornstein–Uhlenbeck processes with jumps. Another example is Langevin diffusions with jumps: ====where ==== is a potential function.====In the presence of the jump component, elimination of the effect of ==== is crucial for a reliable estimation of ====. A well-known approach for it is the threshold based method independently proposed in Mancini (2004) and Shimizu and Yoshida (2006); see also Ogihara and Yoshida (2011) for subsequent developments. In the method, we look at sizes of the increments ====for ==== in absolute value: we assume that one jump has occurred over ==== if ==== for a pre-specified ==== ====, and then estimate ==== after removing such increments. For a suitably chosen ====, it is shown that the estimator of ==== is asymptotically normally distributed at the same rate as diffusion models, while finite-sample performance of the threshold method strongly depends on the value of ====. A data-adaptive quantitative choice of ==== is a subtle and sensitive problem in practice; see Shimizu, 2008, Shimizu, 2009, as well as the references therein. Obviously, if the model may have “small” jumps with positive probability, joint estimation of diffusion and jump components can exhibit a rather bad finite-sample performance; for example, some increments may simultaneously contain small jumps and large fluctuation caused by continuous component. This practical issue can also be seen in other jump detection methods such as Aït-Sahalia and Jacod (2009).====Recently, for estimating the volatility parameter in the non-ergodic framework, i.e., for a fixed ====, ==== and ====, Inatsugu and Yoshida (2021) proposed an alternative estimation procedure called a global jump-detection filter based on the theory of order statistics constructed from the whole increments; there, it is shown that the global filtering can work both theoretically and numerically better than the previously studied local one (Mancini, 2004, Shimizu and Yoshida, 2006, and Ogihara and Yoshida, 2011). Nevertheless, as will be seen later, required conditions on the distribution of jump sizes and decaying rate of ==== may be more stringent in the case where ====. Hence it is not quite clear whether or not and how the global filtering of Inatsugu and Yoshida (2021) is directly applicable to our ergodic setting.====The primary objective of this paper is to formulate an intuitively easy-to-understand strategy, which can simultaneously estimate ==== and detect jumps without any precise calibration of a jump-detection threshold. For this purpose, we utilize the approximate self-normalized residuals (Masuda, 2013a), which makes the classical Jarque–Bera test (Jarque and Bera, 1987) adapted to our model. More specifically, the hypothesis test whose significance level is ==== is constructed by the following manner: let the null hypothesis be of “no jump component” against the alternative hypothesis of “non-trivial jump component”: ====Then, if the Jarque–Bera type statistic introduced later is larger than a given percentile of the chi-square distribution with ==== degrees of freedom, we reject the null hypothesis ====; and otherwise, we accept ====. For such a test, we can intuitively regard that the largest increment contains at least one jump when the null hypothesis is rejected. Following this intuition, our proposed method will go as follows: we iteratively conduct the test with removing the largest increments in the retained samples until rejection of ==== is stopped; after that, we construct the modified estimator of ==== by the remaining samples. Our method enables us not only just to make a “pre-cleaning” of diffusion-like data sequence by removing large jumps which breaks the approximate Gaussianity of the self-normalized residuals, but also to approximately quantify jumps relative to continuous fluctuations in a natural way; see Remark 3.4.====This paper is organized as follows: in Section 2, we give a brief summary of the approximate self-normalized residuals, and the Jarque–Bera type test for general jump diffusion models. Section 3 provides our strategy and some remarks on its practical use. In Section 4, we will propose a least-squares type estimator and its one-step version for (1.1). In the calculation of our estimator we can sidestep optimization, and thus it is numerically tractable, with retaining high representational power of the nonlinearity in the state variable. Moreover, we will prove that our estimator is asymptotically equivalent to the “oracle” estimator which is constructed as if we observe the unobserved continuous part of ====. We show some numerical experiments results in Section 5. Finally, Appendix presents the proofs of the results given in Section 4.====Here are some notations and conventions used throughout this paper. We largely abbreviate “====” from the notation like ==== and ====. For any vector variable ====, we write ====. For any process ====, ==== denotes the ====th increment ====. ==== denotes a universal positive constant which may vary at each appearance. ==== stands for the transpose operator, and ==== for any matrix ====. The convergences in probability and in distribution are denoted by ==== and ====, respectively. All limits appearing below are taken for ==== unless otherwise mentioned. For two nonnegative real sequences ==== and ====, we write ==== if ====. For any ====, ==== denotes the maximum integer which does not exceed ====.",Estimating diffusion with compound Poisson jumps based on self-normalized residuals,https://www.sciencedirect.com/science/article/pii/S0378375821000240,5 March 2021,2021,Research Article,102.0
"Nandi Shinjini,Sarkar Sanat K.,Chen Xiongzhi","Department of Mathematical Sciences, Montana State University, Bozeman, MT 59717, USA,Department of Statistical Science, Temple University, Philadelphia, PA 19122, USA,Department of Mathematics and Statistics, Washington State University, Pullman, WA 99164, USA","Received 10 May 2020, Revised 24 November 2020, Accepted 16 February 2021, Available online 27 February 2021, Version of Record 15 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.006,Cited by (4),There is ample research on ,"Large-scale multiple testing often involves classifying hypotheses into several groups. The groups might be formed naturally due to characteristics of the underlying scientific experiment, or a certain feature attributable to each hypothesis might serve as the basis of partition. For example, hypotheses from gene expression data can be naturally classified into three ontologies provided by Gene Ontology (Hu et al., 2010, Ashburner et al., 2000). In spatial data analysis, hypotheses from local contiguous regions may be clustered together to form groups (Benjamini and Heller, 2007). We refer to the setting where hypotheses are classified into groups via a single criterion as ‘one-way classification’ of hypotheses. Multiple testing procedures adapted to such arrangement of hypotheses are able to incorporate the structural information and consequently can have more power and better control over false discoveries than their counterparts that ignore such information. Considerable amount of research has taken place on developing FDR controlling multiple testing procedures under such setting (Pacifico et al., 2004, Hu et al., 2010, Ignatiadis et al., 2016, Liu et al., 2016).====However, there are multiple testing situations where hypotheses might be classified simultaneously according to more than one norm of classification determined by multiple interesting features or nature of the experiment. For instance, in a microbial abundance study (to be discussed in Section 6), hypotheses on the presence or absence of a microbe can be regarded as two-way classified hypotheses, since they can be classified into two sets of groups by two different criteria - which family a microbe belongs to and the environment in which the microbe is commonly present. Other examples of two-way classified hypotheses can be obtained from voxelwise genome-wise association studies investigating relationship between SNPs and voxels (Stein et al., 2010), experiments having an underlying spatio-temporal structure, such as fMRI studies where hypotheses can be classified by brain location and time of scan (Foygel Barber and Ramdas, 2015), or studies of vegetation fluctuations (Clements et al., 2014), where classification criteria are field location and season of the year.====For two-way classified hypotheses, researchers are most interested in the hypotheses that emerge as significant when effects due to both classifications are factored in. Unfortunately, the scope of existing multiple testing procedures is limited to one-way classified data, and are hence incapable to gauge the simultaneous effect of two-way classification. There seems to be very few methods that were specifically designed for multiple testing of two-way classified hypotheses. Even though the methods in Foygel Barber and Ramdas (2015) and Ramdas et al. (2017) have been applied to such hypotheses, they address a generic multi-layer classification setting, where each layer consists of a set of groups of hypotheses, and apply a BH-type procedure to p-values in each layer. Though these methods control group-level and overall FDR, they may not be able to efficiently capture or account for the simultaneous two-way structure among the hypotheses.====The goal of this article is to broaden the scope of BH-type FDR controlling procedures from unstructured to one- and two-way classified structures of hypotheses, and propose new data-adaptive weights for testing multiple hypotheses exhibiting such structures using weighted ====-value based FDR controlling procedures developed in a frequentist framework. Contingent on nature of the study and specific research goals, different weighing schemes can be incorporated in the multiple testing procedures. In order to improve control on FDR, for a set of hypotheses, a function of the proportion of true nulls (or its estimate) is used to construct oracle (or data-adaptive) weight which is assigned to all p-values, (see Benjamini and Hochberg, 2000, Storey et al., 2004, Sarkar, 2008 and  Blanchard and Roquain (2009)). Benjamini and Hochberg (1997) proposed a weighted version of the BH procedure in order to control weighted FDR.  Genovese et al. (2006) showed that in a BH-type FDR controlling procedure, incorporating a set of non-negative weights bearing additional information about the hypotheses more often than not improves power. Literature contains several instances of using prior data, expert knowledge and scientific insights to formulate weights (see Roeder et al., 2006, Roeder and Wasserman, 2009, Benjamini and Cohen, 2017, etc.). Though some of these weighted multiple testing procedures incorporate structural information, none of them address two-way classification structures. Our proposed multiple testing procedure is more generalized than its predecessors and adaptable to hypotheses that are (or can be) partitioned into groups, one- or two-way.====Our main contributions in this article are two-fold. First, we propose a new set of weights for one-way classified hypotheses. They share the same oracle form as those in Hu et al. (2010), but their data-adaptive forms are different. These new data-adaptive weights ensure, unlike Hu et al. (2010), that the resultant data-adaptive BH-type procedure is a valid FDR controlling procedure asymptotically under weak dependence as well as non-asymptotically under independence. Furthermore, they capture the underlying structure of hypotheses more efficiently than their competitors, as demonstrated through simulations. Secondly, we propose weights for two-way classified hypotheses by extending those for one-way classified hypotheses, and thus provide a novel approach to adapting the BH procedure to such hypotheses. For the resultant BH-type procedures, we prove that they control FDR under certain positive dependence when the weights are in their oracle forms and under independence when the weights are in their data-adaptive forms. The much improved performances of these resultant procedures, as compared to their relevant competitors, are demonstrated via extensive simulation studies and an application to multiple testing of two-way classified hypotheses in the microbial abundance study mentioned above.====The paper is organized as follows: Section 2 gives preliminaries for developing our proposed methods in Sections 3 One-Way grouped BH procedure: Adapting the BH procedure to One-Way classified hypotheses, 4 Two-Way grouped BH procedure: Adapting the BH procedure to Two-Way classified hypotheses; Section 5 contains core findings of simulation studies on all these methods; Section 6 illustrates real application of one of our proposed two-way adaptive methods; and Section 7 concludes the paper with final remarks and further discussions. Proof of the main lemma supporting key arguments in proving the theorems on FDR control under independence for the various data-adaptive methods and a proof of a part of one of these theorems showing how this lemma is being applied are given in Appendix A. Supplementary Materials (SM) contains (i) proofs of all other theorems, (ii) detailed descriptions of the data generating processes for all simulations studies and findings, (iii) a toy example providing an insight into how ====-value weighting works in adapting the BH method to one- or two-way structured hypotheses, and (iv) additional simulation results and plots associated with the aforementioned real data application.",Adapting to one- and two-way classified structures of hypotheses while controlling the false discovery rate,https://www.sciencedirect.com/science/article/pii/S0378375821000227,27 February 2021,2021,Research Article,103.0
"Yang Guijun,Yang Jingbo,Yang Xue,Wang Weizhen","School of Statistics, Tianjin University of Finance and Economics, Tianjin 300222, China,School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China,Department of Mathematics and Statistics, Wright State University, OH 45435, USA","Received 26 June 2020, Revised 3 November 2020, Accepted 16 February 2021, Available online 27 February 2021, Version of Record 15 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.007,Cited by (0),"Phase II clinical trials in oncology are used to initially evaluate the therapeutic efficacy of a new treatment. In the past, the total response was a frequently used endpoint to access the effectiveness of the treatment. When the total response is modest, clinicians may also be interested in disease control (defined as the total response or stable disease) since it may better predict clinical outcomes. Thus, formally testing both the total response and disease control in a phase II trial has an important clinical implication. In this paper, we propose a new method to construct optimal subset designs for one-stage and two-stage phase II clinical trials with two binary endpoints, i.e. the total response and disease control. A new set of hypotheses under the framework of intersection-union tests is provided in which the treatment is considered promising if both the total response and disease control are good. We show that the power function for each test in a large family of tests is nondecreasing in both the total response rate and disease control rate; identify the parameter configurations at which the maximum Type I error rate and minimum power are achieved and derive level-==== tests. We also provide optimal one-stage designs with the minimum sample size and optimal two-stage designs with the least expected total sample size.","The main objective of phase II clinical trials is to provide preliminary information on a treatment activity. In phase II clinical trials of antitumor treatment, a single-arm trial without a control group is often used. If a new therapy shows adequate safety and efficacy, then it will be compared with the “standard” treatment in a phase III trial. Traditionally, phase II antitumor clinical trials are often assessed by the efficacy information based on four mutually exclusive categories: Complete Response, Partial Response, Progressive Disease or Stable Disease, which are defined by Response Evaluation Criteria in Solid Tumors (Eisenhauer et al., 2009). The criteria have been adapted from the original (WHO handbook, 1979), taking into account the measurement of the longest diameter only for all target lesions: Complete Response (CR) — the disappearance of all target lesions; Partial Response (PR) — at least a 30% decrease in the sum of the longest diameter of target lesions, taking as reference the baseline sum longest diameter; Progressive Disease (PD) — at least a 20% increase in the sum of the longest diameter of target lesions, taking as reference the smallest sum longest diameter recorded since the treatment started or the appearance of one or more new lesions, or Stable Disease (SD) — neither sufficient shrinkage to qualify for partial response nor sufficient increase to qualify for progressive disease, taking as reference the smallest sum longest diameter since the treatment started. The relationships among CR, PR, SD, and PD are given in Table 1 (Ivanova et al., 2012).====Phase II clinical trials should have enough power to estimate the response rate when the data are collected from a fixed number of patients. Due to cost and ethics, if the results clearly indicate that the treatment is ineffective or is worthy of further investigation, the trial should be terminated as early as possible. Therefore, multi-stage or group-sequential clinical trial designs are especially useful for these purposes. Among them, the two-stage design is the most commonly used one in practice. Simon (1989) proposed optimal two-stage designs for a single endpoint (for example, Total Response (TR) that is defined as either PR or CR) to minimize the expected total sample size. Shan et al. (2016) proved that Simon’s designs controlled the maximum Type I error rate. However, there exist interesting cases of multiple endpoints. Several authors have proposed alternative strategies to consider either response and toxicity (Yin et al., 2019) or response and early progression (Yang et al., 2020). TR consists of two types of responses, CR and PR, whose effects on subsequent patients are different. Thus, the designs assessing the treatment effectiveness through multiple endpoints are needed. Lin and Chen (2000) and Panageas et al. (2002) proposed optimal two-stage designs based on CR and PR. Lu et al. (2005) provided a general approach to consider TR and CR simultaneously.====For some tumors, however, only a minority of patients with advanced cancer experience tumor shrinkage (or response) after standard platinum-based chemotherapy, such as non-small-cell lung cancer. More patients experience either SD or PD (Lara et al., 2008). Recently, disease control (DC) defined as CR, PR or SD, has been used as a primary outcome (Abrey et al., 2001, Cohen et al., 2003). Lara et al. (2008) demonstrated that the DC rate is a stronger predictor of survival than the objective response rate. Thus, both TR and DC have clinical value in the development of novel agents as shown in Ivanova et al. (2012). In their designs, the treatment is considered effective when the TR or DC rate is large. SD may partially reflect the natural history of the disease, so it is inappropriate to access efficacy only based on DC without considering TR.====In this paper, we propose a new method to construct one-stage and two-stage optimal subset designs when considering two endpoints TR and DC. The design optimality criterion is the minimum expected sample size under the null hypothesis. We derive exact tests which strictly control both the maximum Type I and Type II error rates under a realistic underlying distribution, the trinomial distribution. The structure of the article is as follows. Section 2 describes the formulation of the hypotheses and the corresponding rejection region. Optimal one-stage and two-stage designs with the minimum expected sample size are given in Sections 3 Optimal one-stage clinical trial designs, 4 Optimal two-stage clinical trial design, respectively. We give a discussion in Section 5.",On optimal subset designs for phase II clinical trials with both total response and disease control,https://www.sciencedirect.com/science/article/pii/S0378375821000239,27 February 2021,2021,Research Article,104.0
Khmaladze Estate V.,"Victoria University of Wellington, PO Box 600, Wellington, New Zealand","Received 27 February 2020, Revised 17 December 2020, Accepted 16 February 2021, Available online 24 February 2021, Version of Record 13 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.009,Cited by (1),We propose a method for ==== on ==== family of ," In 1900 Karl Pearson published a paper, Pearson (1900), where he introduced what he called ==== ==== and test. Since long ago the statistic became so important and so widely used, that little else can be compared to it in the statistical literature. The ==== statistic became almost a symbol of statistics, or at least, of the goodness of fit testing theory.====The heuristics behind this statistic is simple. Indeed, consider a sequence of ==== independent experiments, where one observes frequencies of ==== disjoint events with the same probabilities ==== in every experiment, and all of them are positive. If ==== denotes the frequency of the ====th event in these ==== experiments, then consider the vector ====The ==== statistic is the square of the norm of this vector, ====Denote by ==== the vector with independent, standard normal coordinates ====. The limit distribution of ==== is that of ====where ==== is the vector of unit norm. Thus, ==== is projection of ==== on the subspace orthogonal to ====. From geometric considerations it is clear that if we project such a homogeneous object as ==== on any ====-dimensional subspace, the distribution of the norm of this projection will not change from subspace to subspace. If ==== is the group of rotations in ====, then the distribution of ==== is invariant under this group. It implies that the limit distribution of the ==== statistic does not depend on the probabilities ====, given only that they are all positive. In statistical terms, the ==== statistic is ====. It is, basically, the ==== distribution free statistic for testing discrete distributions. This absolute dominance is in sharp contrast to the situation with testing for samples from continuous distributions, where the whole class of distribution free test statistics was introduced and used for decades. Most of them are constructed as functionals of the underlying distribution free version of empirical process.====Working on distribution free empirical processes for Markov chains, which we will introduce in the next sections, and in earlier work too, we found ourselves in immediate neighborhood of the phenomenon, which led to distribution free-ness of K. Pearson’s famous statistic. Finding invariants under groups of transformations was always very important and led to major discoveries. We are glad to refer here to the beautiful introduction in the paper Higgins et al. (2018). However, although the distribution of K. Pearson’s statistic is invariant under the group of rotation, it is not the maximal invariant. We will distinguish here between these two objects. We will use maximal invariant. It is strange that this step has not been taken in all these many decades, and only was made in Khmaladze (2013). But it helped to develop a full-scale distribution free approach to testing for discrete distributions, and now it lies in the background of the distribution free theory for Markov sequences.==== The reader would note that, in comparison to the situation with a sample, as in Khmaladze (2013), the situation with Markov sequences involves more complex structure of observations and of hypothetical distribution, or rather family of conditional distributions, with essential dependence structure. The empirical process needs to be defined, as there are now several possibilities, see, for example, the version of empirical process connected with the notion of occupation measure and local time, as in Davydov (2001) and Kutoyants (2004). The one we consider in this paper, see (1), is not universally accepted form of this process, but we hope it may be recognized as a useful one.====One of the first statistical questions for Markov processes would be the estimation of parameters of their distributions; that is, given a family of transition probabilities ====, which depend on the finite-dimensional parameter ====, estimate the value of this parameter from the observed trajectory of the Markov process. The question was investigated on relatively early stage, in Billingsley, 1961a, Billingsley, 1961b. Modern theory of estimation of parameters of Markov processes is rich and several well known monographs exist. We refer here to Basawa and Scott (1983), Prakasa Rao (1999) and Kutoyants (2004), especially to Section 2 along with the very extensive historic comments there. Important advance in estimation theory for the case of Markov chains was made in well-known papers Greenwood and Wefelmeyer, 1995, Greenwood and Wefelmeyer, 1999.====Another fundamental statistical question is how to test the hypothesis that our parametric model for a Markov process is correct, that is, the transition probabilities of an observed Markov process belongs to a given parametric family. Here large amount of work has been done on the likelihood ratio type tests, and the reader will be well served by reference to the three monographs cited above, to which we add reference to Lin’kov (1993). Very different kind of tests are those called goodness-of-fit tests. Their tests statistics usually have null distributions, which are not easy to calculate. Therefore, it is practically important that we have distribution-free versions of such tests. This task also was addressed, for example, in Kleptsyna and Kutoyants (2014) and Masuda et al. (2010). These papers carried over the “martingale approach” of Khmaladze (1981) into domain of Markov processes.====In the present paper we describe new and heuristically simpler approach to construction of classes of distribution-free statistics. To do this we introduce a version of empirical process, along with parametric empirical process, or empirical process with estimated parameters, and then transform this process into its distribution-free version. More than by practical advantage in gaining distribution-free property, we have been driven by theoretical interest of establishing equivalence between testing problems for different models. These are the testing problems which can be mapped into each other. In this way one needs to establish distribution of test statistics for only one member of the equivalence class. The mapping is described below, in Sections 4 Unitary transformation in the case of fixed transition probabilities, 5 Parametric models for transition probabilities, 6 Unitary transformation in the parametric case, and examples demonstrate the equivalence classes can be surprisingly broad. Sections 2 The first facts, 3 Test statistics and the difficulty expand statistical motivation and build necessary set-up.",Testing hypothesis on transition distributions of a Markov sequence,https://www.sciencedirect.com/science/article/pii/S0378375821000252,24 February 2021,2021,Research Article,105.0
"Xu Xiaoming,Meyer Mary C.,Opsomer Jean D.","Department of Statistics, Colorado State University, United States of America,Westat, United States of America","Received 10 May 2020, Revised 26 January 2021, Accepted 10 February 2021, Available online 23 February 2021, Version of Record 13 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.004,Cited by (0),"In survey domain estimation, ",None,Improved variance estimation for inequality-constrained domain mean estimators using survey data,https://www.sciencedirect.com/science/article/pii/S0378375821000203,23 February 2021,2021,Research Article,106.0
Dussap Florian,"Université de Paris, CNRS, MAP5 UMR 8145, F-75006 Paris, France","Received 6 January 2020, Revised 10 July 2020, Accepted 13 February 2021, Available online 22 February 2021, Version of Record 15 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.005,Cited by (7),", where ==== and ==== are independent ====-dimensional random vectors with non-negative coordinates. Our goal is to recover the density of ==== from independent observations of ====, assuming the density of ==== is known. In the ====, we improve the previous projection procedure). We illustrate these procedures on simulated data, and in dimension ==== we compare our procedure with the previous adaptive projection procedure.",None,Anisotropic multivariate deconvolution using projection on the Laguerre basis,https://www.sciencedirect.com/science/article/pii/S0378375821000215,22 February 2021,2021,Research Article,107.0
Zhou Niwen,"School of Statistics, Beijing Normal University, Beijing, China,Center for Statistics and Data Science, Beijing Normal University, Zhuhai, China,Department of Mathematics, Hong Kong Baptist University, Hong Kong, China","Received 9 January 2020, Revised 30 January 2021, Accepted 10 February 2021, Available online 18 February 2021, Version of Record 12 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.003,Cited by (3)," in the set of arguments of the propensity score, the bandwidth and kernel function selections. The results show an essential difference from the IPW-based estimator of the unconditional average treatment effects(ATE). The ","Treatment effects have been widely analyzed by economists and statisticians in diverse fields. In this paper, we focus on estimating treatment effects under the potential outcomes framework and the unconfoundedness assumption with binary treatment. Let ==== mean that the individual does not receive or receives treatment, and the response ==== be the corresponding potential outcome as ==== or ====. To conveniently identify the quantities measuring treatment effects, the unconfoundedness assumption (Rosenbaum and Rubin, 1983) is generally considered, that is, the assignment to treatment is independent of the potential outcomes given a ====-dimensional vector ==== of covariates with ====, i.e. ==== Further, we in this paper consider the dimension of ==== to be fixed throughout this paper, but in some cases, it can be high.==== As ==== and ==== cannot be simultaneously observed for any individual, the observed outcome can be written as ====. Since estimating the ====th individual treatment effect ==== is unrealistic, a reasonable way in the literature is to estimate average treatment effects (====): ====. See for instance  Rosenbaum and Rubin (1983) and Hirano et al. (2003).====Recently, there is an increasing interest in estimating conditional (or heterogeneous) average treatment effects: ====, which are designed to reflect how treatment effects vary across different subpopulations. Note that even though receiving treatment may have no effect on outcomes for the overall population, i.e. ====, the treatment can still be effective for some subpopulation defined by specific observable characteristics, i.e. for some ==== such that ====. Thus heterogeneous treatment effects are more informative than ==== and can play important roles in personalized medicine and policy intervention. Most of the existing estimation methods for the heterogeneous treatment effects are conditional on the full set of variables, ====, see e.g. Crump et al. (2008), Wager and Athey (2018), where the variables ==== are designed to make the unconfoundedness assumption plausible. After 2015, researchers consider estimating more general conditional/heterogeneous treatment effects, in which the conditioning covariates are ==== with ==== being a subset of covariates, i.e. ====See e.g. Abrevaya et al. (2015) and Lee et al. (2017). Note that treatment effects conditioning on a subset of ====, rather than the high dimensional covariates ====, can provide desirable flexibility and can help policy decision-making.====Based on the assumption (1), Abrevaya et al. (2015) used the inverse probability weighting (====)-based method, which is popularly used in the literature (see, e.g. Robins et al., 1994), to estimate ====when the propensity score function is estimated parametrically and nonparametrically, denoting the corresponding CATE estimators as ====-==== and ====-==== respectively. Abrevaya et al. (2015) gave a deep investigation on the asymptotic properties of the estimators. There are two main conclusions in Abrevaya et al. (2015): one is ====-==== can be asymptotically more efficient than ====-==== in the sense that the asymptotic variance function of ====-==== can be uniformly smaller than that of ====-====, the another is the asymptotic variance function of ====-==== equal to that of ====-==== which is defined as the oracle estimator with the true propensity score. It is noted that the last conclusion is different from that of IPW-type ATE estimators, because the IPW-type ATE estimator based on parametrically estimated propensity score can be more efficient than the one based on the true propensity score.====As is known, to make the unconfoundedness assumption plausible, it is often the case that we need to include many covariates in the analysis. Thus we say ==== is of high dimension with ====. In this case, on one hand, it is often not easy to choose a parametric specification of propensity score that can successfully capture all the important nonlinear and interaction effects to have ====-====. On the other hand, any nonparametric estimation of propensity score clearly suffers from the curse of dimensionality and then ====-==== does not work anymore.====Therefore, in this paper, we suggest a semiparametric IPW-based ==== estimation procedure to simultaneously alleviate the propensity score misspecification problem and particularly the curse of dimensionality. To this end, we consider a semiparametric dimension reduction structure of the propensity score and the unconfoundedness assumption (1) can have a dimension reduction version. It is worth pointing out that the general nonparametric structure can be regarded as a special case of the dimension reduction structure we consider with an orthonormal projection matrix of full rank. We will call the estimator ====-==== and give the details about the model setting and the estimation procedure in the next section.====For theoretical development, we will give the asymptotically linear representation and asymptotic normality of ====-====. We will also give some further properties of the ====-==== proposed in Abrevaya et al. (2015). Based on the theoretical studies, we give a systematic comparison of the asymptotic efficiency amongst ====-====, ====-====, ====-==== and ====-====.====Combining the results in Abrevaya et al. (2015) and the further properties of ====-==== we derive in this paper, the comparison reveals some very interesting and important phenomena. Specifically, letting ==== mean that the asymptotic variance of estimator ==== is not greater than that of estimator ==== and ==== stand for that ==== has the same asymptotic variance function as ====, we have the following observations in theory.====First, ==== in general.====Second, the affiliation of ==== to the set of arguments of the propensity score plays an important role in the asymptotic efficiency of ====-==== and ====-====. That is, when ==== is a subset of the arguments of the propensity score, ==== and ====, otherwise, ====. Note that this newly found phenomenon provides a deep insight into the performance of ====-==== and ====-====, which is also useful in practice.====Third, when the propensity score function is smooth enough, then even in general cases we can also have the asymptotic equivalence: ====, by carefully choosing the bandwidths and using high order kernel functions. This also gives us a better understanding for the asymptotic performances of different estimators. Of course, this part mainly serves as a theoretical exploration. For practical use, we would have no interest to willfully choose those kernel functions and bandwidths, which are very difficult to implement and make the estimator with worse performance. But it reminds the researchers that a “good” estimator of the propensity score would not be helpful for the performance of a ==== estimator.====Fourth, owing to the dimension reduction structure of ====, the requirements for bandwidths and the order of kernel function for ====-==== are much milder than those for ====-====. Thus when the dimension is high, even though ====-==== has the superior efficiency in theory, ====-==== may still be preferable. The rest of the paper is organized as follows. In Section 2, we first introduce an estimation procedure for ====-====. Also, we investigate its asymptotic properties and the theoretical comparisons between the four ==== estimators. Section 3 contains some numerical studies to examine the performance of the ==== estimators. In Section 4, we apply the CATE estimators to analyze a real data set for illustration. Section 5 contains some conclusions and further discussion. The regularity conditions are listed in the Appendix A and all the technical proofs are relegated to Supplementary Materials to save space.",On IPW-based estimation of conditional average treatment effects,https://www.sciencedirect.com/science/article/pii/S0378375821000197,18 February 2021,2021,Research Article,108.0
"Ninomiya Yoshiyuki,Kuriki Satoshi,Shiroishi Toshihiko,Takada Toyoyuki","Department of Mathematical Analysis and Statistical Inference, The Institute of Statistical Mathematics, 10-3 Midori-cho, Tachikawa-shi, Tokyo 190-8562, Japan,BioResource Research Center, Riken, 3-1-1 Koyadai, Tsukuba-shi, Ibaraki 305-0074, Japan","Received 17 August 2020, Revised 2 February 2021, Accepted 6 February 2021, Available online 17 February 2021, Version of Record 1 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.001,Cited by (1),"-statistic is considered, and we determine its rejection region using a common rejection limit. When there are unknown correlations among test statistics, the multiplicity adjusted ","We consider a simple multiple testing problem that compares the expectations of two-dimensional independent data from control and case groups. Setting the sample size as ==== in each group, we denote the data in the control group by ==== and the data in the case group by ====. In addition, their expectations and variances are denoted by ====
 (====), and we assume that they are unknown. For this model, we consider testing ==== against ==== and ==== against ====, simultaneously. As test statistics, we use conventional ====-statistics, ==== and ====, and a rejection region is determined using a common rejection limit ====. If the values of ==== are known, then as a rejection limit, we have to only obtain the value of ==== such that the family-wise error rate ====is controlled; that is, the family-wise error rate is equal to ====, where ==== refers to a probability under a hypothesis ==== and ==== is the significance level for this multiple testing. Note that this yields a strong control of the family-wise error rate because this satisfies a subset pivotality condition introduced in Westfall and Young (1993). It is also to be noted that the higher the value of the correlation ==== is, the higher the correlation between ==== and ==== is. This would result that the magnitude of the necessary multiplicity correction decreases and more number of null hypotheses are rejected. In this problem, the value of ==== is unknown, and we intend to asymptotically control the family-wise error rate.====A natural choice would be to replace ==== with its reasonable estimator such as an unbiased estimator ====
 (====) in the asymptotic null distribution of ====, that is a two-dimensional Gaussian distribution with a mean of ====, a variance of ====, and a correlation of ====. We call this the MaxT method (see Section 2.6 of Dudoit and van der Laan 2007). Because ==== is consistent without relation to what hypothesis is true, this method asymptotically controls the family-wise error rate.====On the other hand, to improve statistical power, we evaluate the correlation by assuming that the expectations are the same in both groups. This means that we use ====an unbiased estimator of ==== when ==== is true, in place of ====. The reason for this is the fact that ====, the so-called spurious correlation, tends to be larger than ==== when an alternative hypothesis is true. In general, using such a spurious correlation does not assure any control of the family-wise error rate because it becomes a meaningless value under certain hypotheses; however, this spurious correlation does assure that the family-wise error rate is asymptotically controlled in this problem and can be seen in the next paragraph.====We will verify the asymptotic control in the following four cases: (a) when ==== is true, (b) when ==== is true, (c) when ==== is true, and (d) when ==== is true. In this method, we use ====, such that ====, as a rejection limit for each test, where ==== is a two-dimensional Gaussian random vector with the mean being ====, the variance being ==== and the correlation being ====. For case (a), since the spurious correlation is a consistent estimator, the family-wise error rate is evaluated as ====For case (b), although the spurious correlation is not consistent, it does not appear in the expression of the family-wise error rate. The family-wise error rate is evaluated as ====For case (c), the spurious correlation does not appear in the expression of the family-wise error rate, which is similar to case (b). For case (d), the family-wise error rate is always zero. Therefore, we have verified the control.====Fig. 1 plots synthetic data under ==== in the setting above. Under ====, the first and second variables in the case group are larger than those in the control group. Consequently, the spurious correlation becomes larger than the correlations in the two groups. For this data, we consider a multiple test consisting of the above-mentioned two tests, testing ==== against ==== and testing ==== against ====, with a significance level of 5%. While the MaxT method does not reject either of the null hypotheses, both null hypotheses are rejected when the spurious correlation is used.====It is not true that we can always use the spurious correlation. We assume one more case group which consists of independent data ==== satisfying (1) with ====. We consider testing ==== against ==== and testing ==== against ==== in addition to the above-mentioned two tests, and we denote their test statistics by ==== and ====. We then consider a four-dimensional Gaussian distribution which is the asymptotic null distribution of ====. Let ==== be a random vector distributed according to a distribution made by replacing ==== with ====which is an unbiased estimator under ==== in the four-dimensional Gaussian distribution. We assume the value of ==== such that ==== is the rejection limit of each test. Under this condition, we consider the family-wise error rate when ==== is true. If it holds for similar type of evaluations as in the case of the two groups, the family-wise error rate is expressed as ====however, this approximation does not necessarily hold. This is because of the fact that the approximation replaces ==== with ==== despite ==== not being a consistent estimator of ==== under this hypothesis.====In general, using a spurious correlation does not assure any asymptotic control of the family-wise error rate; however, for the correlation between two test statistics, using an estimator that is consistent under the null hypotheses in the two tests assures asymptotic control of the family-wise error rate. In (3), if the correlation between ==== and ==== is replaced with an estimator under ====, the approximation holds. This theory is given in its general form in Section 2.====For correlated multiple tests without any pre-specified hypothesis ordering such as the above example, the MaxT method is conventional, and Westfall and Young (1993) showed that it asymptotically controls the family-wise error rate under a subset pivotality condition. It was also shown by Pollard and van der Laan (2004) and Dudoit et al. (2004) that the condition is relaxed by an easy algorithm. Moreover, the MaxT method can also be used in a step-down procedure (van der Laan et al. 2004). In this paper, under the same situation, we consider a different method that enhances statistical power.====In recent years, multiple testing procedures have been developed due to a rising demand from applications in the fields of medicine, bioinformatics, genomics, and brain imaging (see, e.g., Farcomeni 2008). A well-developed approach that does not consider the use of correlations is known as the “oracle approach”. This approach constructs an optimal test function by assuming that the true values of the parameters or their prior distributions are known. When the subject of what we want to control is the false discovery rate, the oracle approach works well if we substitute simple estimators for the true values of the parameters, or even if the prior distributions are slightly misspecified (Genovese et al. 2006, Storey 2007, Sun and Cai 2007, Guindani et al. 2009). On the other hand, it is difficult to control the family-wise error rate if we only use a simple estimator, and as shown in Roeder and Wasserman (2009), a natural choice would be to use a two-stage method with a sample-splitting procedure (Rubin et al. 2006, Wasserman and Roeder 2006, Habiger and Peña 2014). In this method, the parameters are estimated from one split sample and testing is implemented by the other split samples. Although this assures the asymptotic control of the family-wise error rate, we have no appropriate theory on the splitting of the samples, and so an arbitrariness for the splitting exists.  Roeder and Wasserman (2009) avoided this two-stage method and roughly estimated the parameters on purpose in order to approximately control the family-wise error rate; however, it is still difficult to construct a theory on how to roughly estimate the parameters. The approach for improving statistical power using such methods is different from the one proposed in this paper, and it is an attractive future theme that combines the two approaches.",A modification of MaxT procedure using spurious correlations,https://www.sciencedirect.com/science/article/pii/S0378375821000173,17 February 2021,2021,Research Article,109.0
"Flournoy Nancy,May Caterina,Tommasi Chiara","University of Missouri, Columbia, USA,Università degli Studi del Piemonte Orientale, Italy,Università degli Studi di Milano, Italy","Received 22 January 2020, Revised 9 December 2020, Accepted 7 February 2021, Available online 13 February 2021, Version of Record 1 March 2021.",https://doi.org/10.1016/j.jspi.2021.02.002,Cited by (1)," with additive Gaussian errors. The observations are collected in a two-stage experimental design and are dependent because the second stage design is determined by the observations at the first stage. The MLE maximizes the total likelihood.====Unlike most theory in the literature, the approximation made to the distribution of the MLE only involves taking the second stage sample size to infinity, as the resulting approximate model retains the dependency between stages, and therefore, more closely reflects the actual two-stage experiment.====It is proved that the MLE is consistent and that its ==== is a specific Gaussian mixture, via stable convergence. Finally, the efficiency of the adaptive procedure relative to the fixed procedure is illustrated by a simulation study under three parameter dose–response Emax and Exponential models.","This paper deals with problems related to the estimation of a nonlinear multi-parameter model with additive Gaussian errors. We take an optimal experimental design approach to improving the efficiency of the maximum likelihood estimate (MLE). As is well-known in the literature, when an optimal experimental design is used to estimate the parameter of a nonlinear model, the optimal design depends on the unknown parameter [for instance, see Atkinson et al. (2007)]. One way to tackle this problem is to use a locally optimal design, which uses a guessed value for the parameter. If this guessed value is poorly chosen, however, the locally optimal design may be poor too. Adaptive optimal designs were suggested as solutions to this problem by Box and Hunter (1965), Fedorov, 1972, White, 1975, Silvey, 1980 and others, but they were not computationally feasible until recently. The two-stage adaptive optimal design solution is now increasingly popular.====At the first stage an initial design is applied to collect the first-stage responses which are used to estimate the unknown parameter. This is the so called interim analysis. To collect the second stage responses, a locally optimal design is determined using the estimated parameter from the interim analysis. Finally, the maximum likelihood method is applied to estimate the vector parameter, employing the whole sample of data. It is increasingly popular to obtain the MLE using asymptotic approximations in which the sample sizes at both stages go to infinity [See, for instance  Dette et al., 2012, Pronzato and Pázman, 2013, Hyun, 2013, Zhang and Hyun, 2016, Pierrillas et al., 2018 and Hill and Lewicki (2018); also see Dragalin et al. (2008) and Kim and Flournoy (2015) for applications that are fleshed out in considerable detail.]. This classical approach, which eliminates the dependency between stages, justifies using the normal distribution to approximate the finite distribution of the MLE. However, the first and the second stage observations are dependent. If one takes only the first stage sample size to infinity, a second stage is of no value in the approximating model. We approximate the distribution of the MLE by only taking the second stage sample size to infinity. Lane and Flournoy (2012) and Lane et al. (2014) observed in simulations that this approach provides a better approximation than is obtained by taking both sample sizes to infinity for small or moderately sized first stages. This approach has also been taken by Lin et al. (2020). These authors all considered unidimensional parametric models which motivates the non-trival extension to multiparameter models and indicates the importance of making this approach applicable to more complex experimental situations.====In this non-standard setting, we prove the consistency of the MLE. In addition, we prove that the asymptotic distribution of the MLE is a specific normal mixture which is obtained via stable convergence [Stable convergence is stronger than convergence in distribution, but not as strong as convergence in probability. For an overview on stable convergence theory see Häusler and Luschgy (2015)]. In this context of dependent data, the inverse of the Fisher information matrix is not the covariance matrix of the MLE [see, for instance, Crowder (1976)]. However, we provide an analytical relation between these two matrices, which justifies the idea of using a function of the information matrix as an optimality criterion.====Finally, assuming three parameter Emax and exponential models, the proposed two-stage adaptive design is compared with one-stage locally optimal designs based on different guessed initial parameter values, through a simulation study. These simulations suggest that unless a researcher does an excellent job of guessing parameters that are very close to the true ones for the stage 1 design, it will be beneficial to adapt. The size of the stage 1 sample relative to the total is also important. Because the optimal proportion to assign to stage 1 is a function of the unknown model parameters, if the designer has discretion in choosing this proportion, we recommend calculating a locally optimal stage 1 sample size as proposed by Lane et al. (2014).====The paper is organized as follows. Section 2 recalls the basic concepts and introduces the model and the notation. Section 3 describes the two-stage adaptive experimental procedure and provides the structure of the likelihood in this particular case. Section 4 contains the main theoretical results. Section 5 reports the results of simulations studies that illustrate the efficiency of adapting as proposed under ==== and ====. In Section 6 a summary with a few comments conclude the paper.",The effects of adaptation on maximum likelihood inference for nonlinear models with normal errors,https://www.sciencedirect.com/science/article/pii/S0378375821000185,13 February 2021,2021,Research Article,110.0
"Lee Shen-Ming,Pho Kim-Hung,Li Chin-Shang","Department of Statistics, Feng Chia University, Taiwan, ROC,Faculty of Mathematics and Statistics, Ton Duc Thang University, Ho Chi Minh City, Viet Nam,School of Nursing, The State University of New York, University at Buffalo, Buffalo, NY 14214, USA","Received 18 June 2020, Revised 20 December 2020, Accepted 16 January 2021, Available online 11 February 2021, Version of Record 21 February 2021.",https://doi.org/10.1016/j.jspi.2021.01.005,Cited by (5),A zero-inflated Bernoulli (ZIBer) regression model is an alternative model in order to improve fitting the binary outcome data that have many more zeros than expected under a regular ====. Because some ,"Logistic regression has turned into a primary implement to study the relationship between a binary outcome variable and covariates. However, when analyzing binary outcome data, it is usual to see the situation where the binary outcome data have many more zeros than expected under regular logistic regression. Although fitting a logistic regression model to this type of data may yield good estimates of the parameters, their standard errors and related confidence intervals may be underestimated. Therefore, a zero-inflated Bernoulli (ZIBer) regression model is an alternative to the logistic regression model in order to improve fitting this type of data. Diop et al. (2011) first investigated the identifiability problem in a ZIBer regression model and provided the maximum likelihood (ML) estimation of the parameters of the model with logit links to the zero-inflated probability. Recently, Diop et al. (2016) made simulation-based inferences in a ZIBer regression model. The ZIBer regression model is a special model for analyzing binary outcome data with excess zeros. For literature on zero-inflated (ZI) models, see, e.g., Hall (2000), Bohning et al. (1999), Cheung (2002), Ridout et al. (2001), Moghimbeigi et al. (2008), Kelley and Anderson (2008) and Diop et al., 2011, Diop et al., 2016.====In many applications, one often faces that a data set contains missing data. Missing data is a widespread issue in numerous disciplines including, e.g., economics, sociology, political science, transportation and communication, and other fields. This issue occurs for several reasons such as respondents who do not respond to item questions, provide confusing responses, etc. (Schafer and Graham, 2002). Thus handling missing data properly is a very crucial issue. Rubin (1976) proposed a framework for incomplete data to address missing data problems. He classified three missing-data mechanisms that consist of missing completely at random (MCAR), missing at random (MAR), and not missing at random (NMAR).====The combination of a regression model and missing data to become the problem of estimating the parameters of the regression model with missing data plays a very important role in the literature. The issues were investigated by, e.g., Wang et al. (2002) who proposed the joint conditional likelihood (JCL) estimation method for logistic regression with missing covariate data by using both the validation and non-validation data sets. Hsieh et al. (2009) extended the JCL approach to propose a semiparametric method to analyze randomized response data with missing covariates in logistic regression. Lee et al. (2012) also extended the JCL method to propose a semiparametric approach for a logistic regression model with missing outcome and covariates. In addition, several approaches have been developed to deal with missing covariate problems in ZI models when data are MAR; for example, Lukusa et al. (2016) proposed a semiparametric inverse probability weighting (IPW) estimation method for a zero-inflated Poisson (ZIP) regression model with missing covariates. Diallo et al. (2019) proposed an IPW estimation method for a zero-inflated binomial (ZIB) regression model with covariates MAR. See Lukusa et al. (2017) for a comprehensive review of the ZI models with missing data.====Although there are many studies of the ZI models with missing data, to the best of our knowledge, the problem of estimating the parameters of a ZIBer regression model with missing covariate values has not yet been investigated. Thus we are motivated to aim to estimate the parameters of a ZIBer regression model with covariates MAR (Rubin, 1976) by employing the validation likelihood (VL) estimation method. The motivation to use the VL estimation method is that the ML estimation method is the most efficient compared to other estimation methods. The VL method is also based on the maximum VL to estimate parameters by using the validation data set. We demonstrate that this VL method is preferred to the IPW and multiple imputation (MI) methods via simulations in Section 6. The reason for restricting attention to the ZIBer regression model is that we are also motivated by the real survey data set of violations of traffic rules of motorcyclist respondents in Taiwan. In this real survey data set, the outcome of interest is whether a motorcyclist respondent violated one speed regulation or more in a year, which is binary. Among the 7386 respondents, 6740 (91%) did not have any violations of speed regulations, and distance covered in kilometers per year, which is a covariate of interest, had missing values. Hence we only focus on the ZIBer regression model in this work. In Section 3, we propose the VL method to estimate the parameters of the ZIBer regression model with covariates MAR. Asymptotic results are provided in Section 4. Section 5 introduces some other estimation methods including the IPW and MI methods. Simulation studies are conducted in Section 6. In Section 7, the real survey data set of violations of traffic rules of motorcyclist respondents in Taiwan is employed to annotate the practical application of the proposed methodology. Finally, some conclusions and discussions are presented in Section 8.",Validation likelihood estimation method for a zero-inflated Bernoulli regression model with missing covariates,https://www.sciencedirect.com/science/article/pii/S0378375821000069,11 February 2021,2021,Research Article,111.0
"Bartenschlager Christina C.,Brunner Jens O.","Chair of Health Care Operations/Health Information Management, Faculty of Business and Economics, University of Augsburg, Universitätsstraße 16, 86159 Augsburg, Germany","Received 11 October 2019, Revised 3 November 2020, Accepted 14 January 2021, Available online 27 January 2021, Version of Record 4 February 2021.",https://doi.org/10.1016/j.jspi.2021.01.004,Cited by (2)," (FDR) and unadjusted conservatism at the same time. Thus, our new method transforms the two-stage decision into a one-stage decision on the type I error definition. In addition, our method is one of the few that controls the FDR by a single step concept. Extensive simulation studies show that our new method outperforms existing procedures.","Multiple hypotheses testing problems are highly relevant whenever data is evaluated statistically. This is getting more important due to the big data era where decision makers analyse huge data sets to make better decisions. In line with this development, data driven decision making is the focus of many business researchers in various disciplines like health care, accounting, strategy and information systems as well. Looking at recent literature, it is obvious that the development of user specific multiple testing methods is crucial for business research findings and will gain much more importance in the future. For instance, 2018 management publications face multiple testing problems in finance (e.g. Crane and Koch, 2018, Lan and Du, 2017), marketing (e.g. Kouchaki and Jami, 2018, Wang et al., 2018), operations management (e.g. Ing et al., 2017, Özer et al., 2018, Shunko et al., 2018) and behavioural economics (e.g. Agnew et al., 2018, Hooshangi and Loewenstein, 2018). From a methodological point of view, testing coefficients in regression analyses and selecting linear models define multiple testing problems in management publications.====Since Fisher (1935) introduced the margin based Bonferroni correction, the literature on multiple testing has grown continuously. After the research on multiple testing problems in the Analysis of Variance (Scheffe, 1953, Tukey, 1953, Dunnet, 1955, Steel, 1960), Holm (1979) and Hochberg (1988) come back to margin based tests. The authors suggest stepwise extensions to the Bonferroni method based on the closure principle (Marcus et al., 1976). In addition, Holm (1979) includes weights. Benjamini and Hochberg (1997) categorize such weighted procedures. According to their systematization, methods either reflect the importance/logical dependency of hypotheses or use ‘procedural weights’. The latter, like Holm’s procedure, directly modify multiple tests. Recently, weighted graphical approaches on the Bonferroni correction (Bretz et al., 2011, Lu, 2016) and weighted False Discovery Rate (FDR) control (Genovese et al., 2006, Basu et al., 2017, Benjamini and Cohen, 2017) are proposed. The introduction of the FDR as a liberal alternative to the Familywise Error Rate (FWER) revolutionized multiple testing in the nineties (Benjamini and Hochberg, 1995). FDR methods (e.g. Benjamini and Hochberg, 1995, Benjamini and Liu, 1999, Yekutieli and Benjamini, 1999, Benjamini et al., 2006) enable testing thousands of hypotheses and cause recent multiple testing literature to be in medicine, biometrics and psychology as well (Dudoit et al., 2003, Blakesley et al., 2009, Dickhaus, 2014, Goeman and Solari, 2014). Among others, Bonferroni’s, Holm’s and Hochberg’s procedures secure time-honoured FWER conservatism. Multiple methods are thus defined for distinct conservatism regarding type I errors. Consequently, the decision on a multiple method presupposes the decision on a suitable conservatism level (Bartenschlager and Krapp, 2015, Benjamini, 2010).====Besides stepwise extensions and ‘procedural weights’, mixtures of multiple methods modify multiple testing outcomes. The literature on the mixture of multiple methods goes back to van der Laan et al. (2004). The Augmentation test first employs a FWER method. Second, the authors use a FDR procedure for retained null hypotheses. This proceeding guarantees a compromise conservatism level of FWER and FDR. Dmitrienko and Tamhane (2011) suggest another approach for clinical trials. The authors focus on mixtures of FWER multiple testing procedures for problems with logical dependency. Benjamini et al. (2006) introduce a mixture of FDR methods. In two steps, the Benjamini and Hochberg (1995) procedure is employed sequentially with different adjusted significance levels.====An important concept that drives the development of multiple hypothesis testing is the dependence among the tested variables. While the closure principle (Marcus et al., 1976, Westfall and Tobias, 2007) and its generalization, the partitioning principle (Finner and Strassburger, 2002), are based on logical dependencies of the hypotheses (Gabriel, 1969), e.g. Romano and Wolf (2005), Cheverud (2001) and Machado, 2007, Machado, 2015 study stochastical dependencies of the variables. Thereby, the latter study focuses Bonferroni based spectral graph theory methods with important applications in medical imaging.====In this paper, we introduce a Bonferroni modification for medium size hypotheses families as, for example, in business applications, called single step margin based error flexible (SiMaFlex) procedure. Our method flexibly outlines FWER, FDR and unadjusted inference by mixing/weighing the Bonferroni method and single tests. From an application perspective, the two-stage decision on first type I error definition and second multiple test is transformed to a one-stage decision on the type I error. A basic contribution of the paper is this flexibility of the multiple type I error measures FWER and FDR integrated in our method. Furthermore, the method is one of the few single step FDR controlling procedures. We give analytical proofs of our results and employ business related simulation studies. The simulation studies show that our method outperforms existing recommendable tests for management applications. The method can be implemented easily in any statistical software because it is based on their standard output of ====-values. Please contact the corresponding author regarding an R routine for the application of SiMaFlex.====The paper is structured as follows: Section 2 reviews relevant multiple testing concepts for this work. Section 3 introduces the SiMaFlex method. The simulation study in Section 4 evaluates and compares the new method with existing tests. Summary and outlook conclude the work in Section 5.",A new user specific multiple testing method for business applications: The SiMaFlex procedure,https://www.sciencedirect.com/science/article/pii/S0378375821000057,27 January 2021,2021,Research Article,112.0
"Feng Sanying,Tian Ping,Hu Yuping,Li Gaorong","School of Mathematics and Statistics, Zhengzhou University, Zhengzhou 450001, China,School of Mathematics and Statistics, Xuchang University, Xuchang 461000, China,School of Statistics, Beijing Normal University, Beijing 100875, China","Received 2 July 2020, Revised 2 January 2021, Accepted 10 January 2021, Available online 27 January 2021, Version of Record 8 February 2021.",https://doi.org/10.1016/j.jspi.2021.01.003,Cited by (3),". At last, we illustrate the finite sample performance of our proposed methods with some simulation studies and a real data application.","Functional data analysis has received considerable attentions in various fields, for example, environmental science, biology, medicine, finance and system engineering. The basic idea behind functional data analysis is to express each individual in repeatedly measured data as a smooth function and then draw information from the collection of functional data (Ramsay and Silverman, 2005, Horváth and Kokoszka, 2012). There was significant amount of recent work devoted to regression models with functional predictors, for example, functional linear model (Cardot et al., 2003, Yao et al., 2005, Hall and Horowitz, 2007, Crambes et al., 2009, Hall and Hooker, 2016), functional nonparametric model (Ferraty and Vieu, 2002), functional single index model (Chen et al., 2011, Jiang and Wang, 2011, Ma, 2016), functional additive model (Müller and Yao, 2008, Fan et al., 2015).====All of the aforementioned functional regression models are very useful when the response is scalar and the predictor is a random curve. However, in practice, in addition to a function-valued random variable as predictor variable, there are other scalar predictors often involved in explaining the variation of the response variable. For example, in our real data analysis, we are interested in establishing the association between the fat content of a meat sample and the spectra curve as well as two scalar predictors—contents of moisture and protein. The modeling challenge one faces is that the functional predictor may interact with the scalar predictors. Typical methods developed so far for the aforementioned functional regression models cannot capture the interaction effect, which can often arise in real-world applications and might be of interest. Thus, in this paper we introduce a new functional single-index varying coefficient model (FSIVCM) with the following form: ====where ==== is a scalar response variable, ==== is a scalar predictor vector, ==== is a zero mean random function defined on a compact interval ====, ==== is an unknown slope function, ==== is a ==== vector of unknown coefficient functions, and ==== is the random error with ==== and ====. Generally, ==== may be taken as ==== so that the model has an intercept function term. For the sake of identifiability, we assume that ==== and the value of ==== at one fixed point ==== is positive.====To the best of our knowledge, the above functional single-index varying coefficient model has not been studied in the scientific literature. The model is flexible in practice and can deal with more complicated data structures, which motivates us to investigate it. We also note that there are many papers focusing on the partial functional regression models in which the predictors include function predictors and several scalar predictors. For example, Aneiros-Pérez and Vieu (2006) introduced a semi-functional partial linear model and showed that the additional information of the scalar predictors can be used to improve the fitting accuracy. Shin (2009) proposed a partial functional linear model and studied the large sample properties of the estimators. Kong et al. (2016) and Yu et al. (2019) studied the regularized estimation and variable selection for the partial functional linear model with high-dimensional scalar predictors. Zhou and Chen (2012) introduced a semi-functional linear model, in which the random function is the linear component and the scalar predictor is the nonparametric component. Feng and Xue (2016) proposed a partially functional linear varying coefficient model. Wang et al. (2016) introduced a functional partial linear single-index model, in which the scalar predictors are treated as linear component and the random function is treated as functional single-index component. All of the aforementioned partial functional regression models have been proved useful or effective for specific purposes. However, they cannot capture the interaction effects between the functional predictor and the scalar predictors.====The main advantage of FSIVCM (1) is that it includes the nonlinear interaction effects between the functional predictor ==== and the scalar predictor ====. In addition, model (1) is flexible and it includes a class of important statistical models. For example, if ==== and ====, model (1) reduces to the functional single-index model in Chen et al. (2011). If ==== and ==== are identically equal to some constants, model (1) becomes the functional partial linear single-index model in Wang et al. (2016). If ====, ==== is the identity function and ==== are identically equal to some constants, model (1) becomes the partial functional linear model in Shin (2009). Moreover, model (1) can also be regarded as a novel extension of single-index varying coefficient model for scalar and vector observations (see Xue and Wang, 2012, Xue and Pang, 2013, Feng and Xue, 2015).====In this paper, our purpose is to develop the theory and method for estimating the unknown slope function ==== and the unknown coefficient functions ==== of model (1). Based on the functional principal components analysis (FPCA), for given ====, we first construct the estimators of ==== and their derivatives ==== using B-spline basis expansion. Then, we develop a profile least squares estimation (PLSE) method to estimate the slope function ====. An important methodological merit of our approach is the ease of simultaneously approximating multiple nonparametric functions to create a single objective function for ====, so that the PLSE can be established in a straightforward manner. Under some regularity conditions, the rates of convergence of the estimators are established. Moreover, we also investigate the convergence rates of the mean squared prediction error for a predictor.====To motivate this research, we consider the Tecator data that has been analyzed by Aneiros-Pérez and Vieu (2006). It is of interest to predict the fat contents of meat samples based on their protein contents, moisture contents and their spectrometric curves. Similar to Aneiros-Pérez and Vieu (2006), we regard the percentage of fat content as the response variable ====, the spectrometric curve as the functional predictor ==== and the percentage of protein content and moisture content as the scalar predictors ====. The proposed FSIVCM and the newly developed estimation method are utilized to reanalyze the data set. The lower prediction error demonstrates the rationality of our model and the efficiency of the novel estimation procedure.====The paper is organized as follows. In Section 2, we introduce the estimation method, and investigate the asymptotic properties of the estimators. In Section 3, some simulation studies are carried out to assess the performance of the proposed estimate method. An application to the Tecator data is given in Section 4. Lastly, we conclude the paper in Section 5 with some future work, and present the technical results in Section 6.",Estimation in functional single-index varying coefficient model,https://www.sciencedirect.com/science/article/pii/S0378375821000045,27 January 2021,2021,Research Article,113.0
"Liu Guanfu,Fan Yan,Liu Yang,Liu Yukun","School of Statistics and Information, Shanghai University of International Business and Economics, Shanghai 201620, China,KLATASDS-MOE, School of Statistics, East China Normal University, Shanghai 200062, China","Received 12 August 2020, Accepted 28 December 2020, Available online 26 January 2021, Version of Record 4 February 2021.",https://doi.org/10.1016/j.jspi.2020.12.004,Cited by (0),"In medical researches such as case-control studies with contaminated controls, frequently encountered is a particular two-sample testing problem in which one sample has a mixture structure. It is a very common case that the exposure in a case-control study may have a positive (or negative) effect on the response variable if the effect exists. This is often ignored by existing tests, which would lead to potentially power loss. Meanwhile, it is of much practical importance to determine a minimal sample size to reach a target power. Based on empirical likelihood and density ratio model, we develop a new EM-test by incorporating the inequality information in the alternative. We show that the proposed EM-test has a mixture of zero and ","This paper is concerned with the homogeneity testing problem based on a particular two-sample data, in which one sample comes from population one, and the other comes from a mixture of population one and some other population with unknown proportions. This problem arises naturally in many areas, such as genetic studies for gene mutation, case-control studies with contaminated controls, and treatment effect testing in the presence of non-responders in biological experiments, and so on. See, for example, Good, 1979, Boos and Brownie, 1991, Lancaster and Imbens, 1996, and Qin and Liang (2011). Specifically suppose we have the following two independent samples: ====where ==== and ==== are probability distribution functions, and ====
 ==== is the unknown mixing proportion. The problem of interest is to test the hypothesis problem ====If both nonparametric models are imposed on ==== and ====, the mixture structure in the data is not identifiable. Although commonly-used two-sample goodness-of-fit tests can still be used to test the homogeneity in data, they may lose power since the mixture structure is ignored in these tests. Given that parametric models are risky to model mis-specification, semi-parametric models are preferable due to their balance in efficiency and robustness. A commonly-used semi-parametric model to link two distributions is the density ratio model or exponential titling model (Anderson, 1979), ====where ==== and ==== are both unknown parameters. This model includes many commonly-used parametric distribution families as its special cases. For example, the exponential, binomial and Poisson distribution families, the normal distributions with a common variance, the gamma distributions with a common scale parameter and so on.====There are a large number of researches on the hypothesis problem in (2) under model (3). Qin and Liang (2011) proposed a score test and showed that the score test after scaled has a central chi-square limiting distribution. Liu et al. (2012) used the empirical likelihood (Owen, 1990) to profile the baseline distribution out and constructed an EM-test, which is shown to follow a chi-square distribution under ====. Ning and Chen (2015) eliminated the unknown nuisance baseline function by a pairwise pseudolikelihood approach. However the corresponding pseudolikelihood ratio test has a very complicated limiting distribution. To maintain the appeal of simplicity for conventional likelihood ratio tests, they proposed two alternatives, which were shown to have a simple asymptotic distribution of ==== under the null. Borrowing the ideas of pairwise pseudolikelihood and EM-test, Hong et al. (2018) proposed another EM-test under an expanded exponential tilt model assumption.====In practice, it is very common the case that the existence of a particular allele in genetic studies, the exposure in a case-control study or the treatment in biological experiments may have a positive effect on the response variable if the effect exists. The two real applications in Section 4 are two such examples. Taking these information into account, we consider the following hypotheses testing problem under model (3) ====However all the aforementioned tests are not designed for this one-sided testing problem but for the alternative ====. In other words, they did not take the inequality constraint information into consideration, which would potentially lose power.====This motivates us to investigate the existing tests for the hypothesis problem in (4). Due to the flexibility and efficiency of empirical likelihood, we take the empirical-likelihood-based EM-test of Liu et al. (2012) as a representative and construct a new EM-test by incorporating the inequality constraint in (4). We show that under the null, it converges to a chi-square mixture distribution with known proportion. Meanwhile, since the two-sample problem studied in this paper is very commonly encountered in many fields such as genetics and epidemiology, it is therefore important to know the sample size needed to ensure a reasonable testing power. This motivates us to study the local power of the proposed EM-test. Based on the local power function, we establish an explicit sample size formula for the proposed EM-test to reach a target power.====The rest of this paper is organized as follows. In Section 2, we present the proposed EM-test and establish its limiting null distribution after introducing the empirical likelihood for model (1) under (3). Its finite sample performance is then investigated by simulations. Section 3 studies the local power of the new test and provides an original and calibrated sample size formulae for it to achieve a target power. Further simulations are conducted to study the performance of these sample size formulae. Two data-sets are analyzed in Section 4 for illustration.",Semi-parametric homogeneity test and sample size calculation for a two-sample problem under an inequality constraint,https://www.sciencedirect.com/science/article/pii/S037837582100001X,26 January 2021,2021,Research Article,114.0
"Dumitrescu Laura,Schiopu-Kratina Ioana","School of Mathematics and Statistics, Victoria University of Wellington, Wellington, 6140, New Zealand,Department of Mathematics and Statistics, University of Ottawa, Ottawa, Ontario Canada K1N 6N5","Received 31 October 2020, Accepted 19 January 2021, Available online 26 January 2021, Version of Record 8 February 2021.",https://doi.org/10.1016/j.jspi.2021.01.006,Cited by (1),"We study the existence, strong consistency and ==== of estimators obtained from estimating functions, that are ","Regression models with stochastic covariates are useful in many applications such as the analysis of time series, stochastic recursive approximations, dosage adjustment problems, as well as in learning algorithms and artificial neural networks. A recent application of the latter, which is relevant to our approach, is included in Solares et al. (2017) and is based on a nonlinear autoregressive exogenous (NARX) model. The NARX model incorporates past values as well as exogenous inputs ====where ==== is a nonlinear mapping, ==== and ==== are, respectively, the output and the external variable, determined at moment ====. Approximating ==== by ====, with given functions ==== and ====, for binary data ====, in Solares et al. (2017) the authors propose an algorithm based on the following probability model ====This method, which combines the logistic function with the NARX representation, is used as a classifier for dynamic binary classification and was shown in simulation studies to perform better than other classification techniques, such as ====-nearest neighbours or random forest. We refer to Duflo (1997) for more applications of stochastic regression, including estimation methods and inference for random iterative models.====In the present work we consider a general approach for the study of an evolutionary cluster, with new data (having a distribution that belongs to the exponential family) arriving at each time step. Given a collection of past observations, the objective is to “explain” their variation in terms of an array of exogenous variables. This approach allows the decomposition of the overall variability of the vector of interest into a time trend and a component that can be explained by exogenous variables and which may give rise to a nonstationary time series. In the context of time series, for non-Gaussian data, two alternative modelling approaches, “====” and “====”, were discussed in Cox (1981). In the latter approach, the autocorrelation is induced via a latent process and was used for modelling time series of counts in e.g. Zeger (1988) and Davis et al. (1999). A comprehensive review of count time series models is included in Fokianos (2012).====We consider the ==== approach for multivariate data and study the asymptotic behaviour of a sequence of estimators obtained from an estimating equation based on conditional nonlinear models, for each individual time series. More precisely, at each moment ====, we assume that, given the past, the conditional distribution of the ====th response component, ====, is a function of a linear combination ====. Here, the vector ==== may depend on past observations as well as exogenous variables and the objective is to estimate ====.====Our approach represents an extension of the stochastic models considered in Lai and Wei (1982) and Chen et al. (1999), where an asymptotic theory was established for uni-dimensional linear and generalized linear regression models, respectively. As a function of time, the response is a vector-valued time series with correlated components, whose conditional correlation, given the evolution of time, need not be specified. Our work extends some of the elegant asymptotic results presented in Kaufmann (1987b) to accommodate possible correlations among components of the response vector series.====We remark that, in the related case of longitudinal data, the observations among clusters are assumed to be independent, as they correspond to the situation when several correlated measurements are taken on different subjects. A popular approach is to assume a generalized linear model for the marginals, whereas the correlation among the responses from the same subject is acknowledged, but not modelled. Then, it is well known that estimators obtained from associated estimating equations are consistent regardless of the working correlation structure used and the question of asymptotic efficiency was discussed in Jiang et al. (2007) and Balan et al. (2010). This article shows that a similar property holds when clusters are not independent and are assumed to evolve in time according to a specified pattern, by making inference based on estimating functions which, in this case, are martingale transforms.====The article is organized as follows. We introduce our model assumption in Section 2 and discuss the special case of independent clusters in Section 3. In Section 4 we give sufficient conditions for strong consistency in Theorem 4.3, whose proof is based on a fixed point theorem and a strong law of large numbers for martingales. In Theorem 4.6 we simplify the hypotheses of Theorem 4.3 to obtain conditions which are easier to verify.====The study of the limiting distributions is presented in Section 5. Theorem 5.2 gives a central limit theorem for our estimating functions, which then leads to the main result of the section, Theorem 5.5. In the latter we give sufficient conditions for the asymptotic normality of estimators obtained as implicit solutions of the associated estimating equations.====In Section 6 we give a characterization of asymptotic optimal sequences of functions (in the sense defined in Heyde, 1997), which lead to asymptotic confidence region of minimal size. The class of estimating functions that we consider consists of square integrable martingale transforms and we show that not only it includes estimating functions with random coefficients, but it also allows for slight misspecification of the regressors (see Corollary 6.3). We expand on this particular type of sensitivity analysis in Section 6.====To evaluate the performance of the proposed method, we present results from a limited simulation study in Section 7. Moreover, we illustrate its applicability on a dataset with average daily wind speed in 2018, taken on several buoys located on the shores of lake Michigan.====For ease of exposition, the proofs are deferred to the Appendix.",Asymptotic results with estimating equations for time-evolving clustered data,https://www.sciencedirect.com/science/article/pii/S0378375821000070,26 January 2021,2021,Research Article,115.0
"Liu Zhan,Yau Chun Yip","Hubei Key Laboratory of Applied Mathematics, School of Mathematics and Statistics, Hubei University, Wuhan, Hubei 430062, China,Department of Statistics, Chinese University of Hong Kong, Hong Kong, China","Received 18 May 2019, Revised 7 January 2021, Accepted 11 January 2021, Available online 20 January 2021, Version of Record 3 February 2021.",https://doi.org/10.1016/j.jspi.2021.01.001,Cited by (3),"In this paper, we develop a method for handling nonignorable missing data in fitting ","Longitudinal studies are surveys that examine repeatedly the characteristics of a target population over time. Longitudinal surveys are very popular in many fields including medicine, business, social dynamics and education. For example, British Cohort Survey (BCS70) is a multi-disciplinary longitudinal study which follows a birth cohort born in a particular week in April 1970; Panel Study of Income Dynamics (PSID), which began in 1968, follows a nationally representative sample of over 18,000 individuals from 5000 families in the United States, covering employment, income, wealth, expenditures, health, marriage, childbearing, child development, philanthropy, education and related topics. In the first round of longitudinal surveys, a sample is often selected from the population based on some sampling design. Then, the same sample is measured repeatedly at different time points. For each sample unit, the sequence of observations can be treated as a time series (Eideh and Nathan, 2006). Therefore, a time series model can be fitted to analyze the population characteristics based on the measurements for all sample units.====A common problem in longitudinal surveys is missing data. Little and Rubin (2002) have discussed this problem and classified the missing data mechanism as missing completely at random, missing at random and missing not at random. Specifically, when missingness does not depend on the observed and unobserved values, the mechanism is known as missing completely at random. When missingness depends only on the observed data, the mechanism is known as missing at random. Finally, the mechanism is called missing not at random if the missingness depends on both the observed and unobserved data. The data of missing completely at random and missing at random are also called ignorable missing data because inferences can be conducted by analyzing only the observed data, without modeling the missing mechanisms (Tseng et al., 2016). However, under missing not at random, the data is called nonignorable missing data, since standard statistical methods which ignores missingness may yield seriously biased parameter estimates (Little and Rubin, 2002, Groves et al., 2004).====Existing works on longitudinal surveys with nonignorable missing data mainly involve generalized linear mixed models (Molenberghs et al., 1997, Ibrahim et al., 2001, Stubbendick and Ibrahim, 2006), quantile regression models (Yuan and Yin, 2010), and latent random effects models (Tseng et al., 2016). Most of these studies assume that the data are time independent, which may not be reasonable. Less literature, however, discusses parameter estimation for time series models in longitudinal surveys. Eideh and Nathan (2006) fitted directly time series models for longitudinal survey data under informative sampling. However, they did not discuss the nonignorable missing data problem in time series models. In fact, for the informative sampling, the sample selection probabilities depend on the values of the outcome variable (both the observed and unobserved data), whereas for the nonignorable missing mechanism it is the response probabilities that depend on the values of the outcome variable. They are different problems which may occur in sample selection phase and data collection phase of surveys, respectively.====Most of the previous works about parameter estimation in nonignorable missing longitudinal data focus on likelihood-based approaches. Ibrahim et al. (2001) developed a complete-data likelihood and a Monte Carlo EM algorithm to estimate the parameters in generalized linear mixed models with nonignorable missing data. Similar approaches are adopted by Ibrahim et al., 2005, Ibrahim and Molenberghs, 2009, Sinha (2012) and Yaseen et al. (2016). However, these approaches require the expectation of the likelihood function which is often difficult to compute. To avoid computing the expectation, we propose an observed likelihood approach for this problem. Specifically, the observed likelihood is obtained based on the sample distribution, which is defined as the distribution of the sample measurements given the observed sample, and can be derived by combining the assumed population distribution of the data and the nonignorable missing mechanism. This approach permits the use of standard methods on likelihood inference and is thus simple to use. In fact, the idea of sample distribution has been used by Pfeffermann et al. (1998) under the context of informative sampling in complex surveys, and is later extended to generalized linear models (Pfeffermann and Sverchkov, 2003), analysis of longitudinal surveys under informative sampling (Eideh and Nathan, 2006), small area estimation (Pfeffermann and Sverchkov, 2007), as well as imputation and estimation under nonignorable missingness (Pfeffermann and Sikov, 2011). None of the above studies, however, considers using the sample distribution to analyze nonignorable missing data in longitudinal surveys.====This paper aims to fit time series models for longitudinal surveys with nonignorable missing data. Under nonignorable missingness, we derive the observed likelihood function based on the population distribution of observations and the response probability with nonignorable missingness. In computing the observed likelihood function, the conditional probability of the observed data given its non-missingness will induce some terms that involve integration. For computational efficiency, we derive approximation to the integrals by series expansions. Simulation study and a real data application show that the proposed method has much better performance than the naive method that ignores the nonignorable missingness in parameter estimation.====The rest of the paper is organized as follows. In Section 2, the autoregressive model for longitudinal surveys with nonignorable missing data is introduced. In Section 3, we develop the proposed method, including the procedure for parameter estimation and variance estimation. In Section 4, simulation studies are presented to demonstrate the effectiveness of the proposed method. In Section 5, we illustrate a real data application. Concluding remarks are provided in Section 6.",Fitting time series models for longitudinal surveys with nonignorable missing data,https://www.sciencedirect.com/science/article/pii/S0378375821000021,20 January 2021,2021,Research Article,116.0
"Bailey R.A.,Fernandes Célia,Ramos Paulo","School of Mathematics and Statistics, University of St Andrews, North Haugh, St Andrews, Fife, KY16 9SS, UK,School of Mathematical Sciences, Queen Mary University of London, Mile End Road, London, E1 4NS, UK,Área Departamental de Matemática, Instituto Superior de Engenharia de Lisboa, Lisboa, Portugal,Centro de Matemática e Aplicações, Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa, Caparica, Portugal","Received 16 December 2020, Accepted 10 January 2021, Available online 20 January 2021, Version of Record 8 February 2021.",https://doi.org/10.1016/j.jspi.2021.01.002,Cited by (0),"A new class of designs is introduced for both estimating the variance components of nested factors and ==== about those variance components. These designs are flexible, and can be chosen so that the degrees of freedom are more evenly spread among the factors than they are in balanced nested designs. The variances of the estimators are smaller than those in stair nested designs of comparable size. The mean squares used in the estimation process are mutually independent, which avoids some of the problems with staggered nested designs.","In many practical situations, measurements are taken on observational units which can be identified by the levels of a sequence ====, …, ==== of factors, each nested in the one before, and each having random effects, where ====. Examples include calves nested in pens nested in farms (====), strength tests nested in preparations of material nested in boxes nested in lots (====) in a polymerization process reported by Mason et al. (2003), and small leaves nested in nodes nested in branches nested in stalks nested in plants (====), in a modification of the data reported by Trout (1985). Applications range from biometry (Searle, 1971) and agriculture to industry (Khattree, 2003).====Let ==== be the number of observational units. The vector ==== of observations has length ====. We assume that the distribution of ==== is multivariate normal, with expectation ==== for some unknown constant ====, where ==== denotes a column vector of length ==== with all entries equal to ====. Denote by ==== the variance–covariance matrix of ====.====For ====, …, ====, it is assumed that each level of factor ==== gives a random variable with zero mean and variance ====. Furthermore, all these random variables are independent. Let ==== and ==== be two observational units (possibly with ====). The observations ==== and ==== are given by ==== and ====, where, for ====, ==== if ==== and ==== have the same level of factor ==== and otherwise the random variables ====, …, ====, ====, …, ==== are independent. Because the factors are sequentially nested, there is a largest value of ==== such that ==== and ==== have the same level of factor ==== if and only if ====. Then the covariance of the observations ==== and ==== is ====, which is zero if ====. Our aim is to design an experiment to estimate the variance components ====, …, ====, and also to test if they are non-zero. There may be little interest in the value of ====.====All of our designs can be thought of as rooted trees: see Fig. 1, Fig. 2, Fig. 3, Fig. 4, Fig. 5. The root is at depth ====. For ====, …, ====, each vertex at depth ==== represents a level of factor ==== which occurs with the level of factor ==== represented by the vertex at depth ==== to which it is joined. The vertices at depth ==== are called ====: these represent the observational units in the experiment. The pattern of entries in ==== depends on the combinatorial properties of the tree.====In addition to any limits on the value of ==== in the experiment, there may be practical constraints on the number of levels of each factor. Let ==== be the maximum number of levels of factor ==== that it is feasible to use, and, for ====, …, ====, let ==== be the maximum number of levels of factor ==== that it is feasible to use within each level of factor ====. In industrial settings ==== may be large, but there are many other practical situations in which ==== for ====.====In Sections 2 Balanced nested designs, 3 Staggered and generalized staggered nested designs, 4 Stair nested designs we review three known families of designs for such experiments: balanced nested designs, staggered nested designs and stair nested designs. These are already in the literature: we include them here within a single approach to aid comparison between them. We show each design as a rooted tree, summarize properties of the estimators of the variance components, and comment on any difficulties in testing a hypothesis that a variance component is zero. Section 5 introduces our new designs, and develops the equivalent results for them.====Data analysis for the designs in Sections 2 Balanced nested designs, 3 Staggered and generalized staggered nested designs, 4 Stair nested designs, 5 Sparse component designs always uses ANOVA-type sums of squares ==== with ==== degrees of freedom, for ====, …, ====; the mean square ==== is denoted by ==== and the expectation of ==== is ====, which is a known linear combination of ====, …, ====. However, the precise meaning of ====, ====, ==== and ==== is different for the different designs.====Section 6 summarizes all the designs being considered for ==== and ====, and compares the variances of their estimators. Finally, Section 7 gives a more wide-ranging comparison of the designs, including flexibility and degrees of freedom.",Sparse designs for estimating variance components of nested factors with random effects,https://www.sciencedirect.com/science/article/pii/S0378375821000033,20 January 2021,2021,Research Article,117.0
"Bailey R.A.,Soicher Leonard H.","School of Mathematics and Statistics, University of St Andrews, North Haugh, St Andrews, Fife KY16 9SS, UK,School of Mathematical Sciences, Queen Mary University of London, Mile End Road, London E1 4NS, UK","Received 19 August 2020, Revised 16 November 2020, Accepted 11 December 2020, Available online 19 December 2020, Version of Record 12 January 2021.",https://doi.org/10.1016/j.jspi.2020.12.003,Cited by (1),"For integers ==== and ====, an ==== ==== is an ==== array of ====-subsets (called ====) of an ====-set (of ====), such that each treatment occurs once in each row and once in each column of the array. A semi-Latin square is ==== if every pair of blocks, not in the same row or column, intersect in the same positive number of treatments. It is known that a uniform ==== semi-Latin square is Schur optimal in the class of all ==== semi-Latin squares, and here we show that when a uniform ==== semi-Latin square exists, the Schur optimal ==== semi-Latin squares are precisely the uniform ones. We then compare uniform semi-Latin squares using the criterion of pairwise-variance (PV) aberration, introduced by J. P. Morgan for affine resolvable designs, and determine the uniform ==== semi-Latin squares with minimum PV aberration when there exist ====. These do not exist when ====, and the smallest uniform semi-Latin squares in this case have size ====. We present a complete classification of the uniform ==== semi-Latin squares, and display the one with least PV aberration. We give a construction producing a uniform ==== semi-Latin square when there exist ====, and determine the PV aberration of such a uniform semi-Latin square. Finally, we describe how certain affine resolvable designs and balanced incomplete-block designs can be constructed from uniform semi-Latin squares. From the uniform ====.","For integers ==== and ====, an ==== ==== is an ==== array of ====-subsets (called ====) of an ====-set (of ====), such that each treatment occurs once in each row and once in each column of the array. Note that an ==== semi-Latin square is the same thing as a Latin square of order ====. We consider two ==== semi-Latin squares to be ==== if one can be obtained from the other by applying an ====, which is a sequence of zero or more of: permuting the rows, permuting the columns, transposing the array, and renaming the treatments. An ==== of a semi-Latin square ==== is an isomorphism mapping ==== onto itself. The applications of semi-Latin squares include the design of agricultural experiments, consumer testing, and via their duals, human–machine interaction (see Bailey, 1992, Bailey, 2011).====A ====-==== is a binary block design for ==== treatments in ==== blocks of size ==== (considered as ====-subsets of the set of treatments), such that each treatment is in exactly ==== blocks. If we ignore the block structure of an ==== semi-Latin square ==== then we obtain an ====-design called the ==== of ====. A ====-design with ==== and ==== is ==== if its collection of blocks can be partitioned into ==== partitions of the treatments (called ====), and such a resolvable design is ==== if every pair of blocks in distinct parallel classes intersect in the same positive number ==== of treatments. A ====-design is a ====-==== (====) if ==== and every pair of distinct treatments occur together in exactly ==== blocks. Two ====-designs are ==== (as block designs) if there is a bijection from the treatments of the first to those of the second which maps the list of blocks of the first onto that of the second in some order. Such a bijection is called a (block design) ====, and an ==== of a ====-design is an isomorphism from that block design to itself.====An ==== of ==== ==== with ==== rows, ==== columns (====), and based on ==== symbols (====), here taken to be ====, or an ==== ====, is an ==== array whose entries are symbols, such that for every ==== subarray, each of the possible ====-tuples of symbols occurs as a row equally often (which must be ==== times). As Hedayat et al. (1999) point out, there are many trivial constructions of orthogonal arrays of strength one, so we ignore this case. Two orthogonal arrays ==== are ==== if one can be obtained from the other by permuting the rows, permuting the columns, and permuting the symbols separately within each column. It is known that orthogonal arrays of strength ==== and affine resolvable designs are equivalent combinatorial objects. In particular, Bailey et al. (1995) describe how to construct an equivalent ==== from an affine resolvable ====-design (see also Morgan, 2010), such that two affine resolvable ====-designs are isomorphic if and only if their equivalent orthogonal arrays are isomorphic. We remark that affine resolvable designs were introduced by Bose (1942) (in the context of BIBDs), and orthogonal arrays were introduced by Rao (1947).====An ==== semi-Latin square ==== is ==== if every pair of blocks, not in the same row or column, intersect in the same positive number ==== of treatments (in which case ====). For example, here is a ==== uniform semi-Latin square with ====: ====Uniform semi-Latin squares were introduced, constructed, and studied by Soicher (2012), where it was shown that a uniform ==== semi-Latin square is Schur optimal (defined later) in the class of all ==== semi-Latin squares.====In this paper, we further the study of uniform semi-Latin squares. We show that, if a uniform ==== semi-Latin square exists, then the Schur optimal ==== semi-Latin squares are precisely the uniform ones.====We then compare uniform ==== semi-Latin squares using the criterion of pairwise-variance (PV) aberration, introduced by Morgan (2010) for affine resolvable designs, and determine the uniform ==== semi-Latin squares with minimum PV aberration when there exist ==== mutually orthogonal Latin squares (MOLS) of order ====. These do not exist when ====, and the smallest uniform semi-Latin squares in this case have size ====. We describe a complete classification of the uniform ==== semi-Latin squares, and find that, up to isomorphism, there are exactly 8615 such designs. We compare their PV aberrations, and display the one with least PV aberration.====We give a construction producing a uniform ==== semi-Latin square when there exist ==== MOLS of order ====, and determine the PV aberration of such a uniform semi-Latin square.====Finally, we describe how a uniform ==== semi-Latin square can be used to construct two (possibly isomorphic) affine resolvable ====-designs and an ====-BIBD. From the uniform ==== semi-Latin squares we classified, we obtain (up to block design isomorphism) exactly 16875 affine resolvable ====-designs and 8615 ====-BIBDs. In particular, this shows that there are at least 16875 pairwise non-isomorphic orthogonal arrays ====.",Uniform semi-Latin squares and their pairwise-variance aberrations,https://www.sciencedirect.com/science/article/pii/S0378375820301373,19 December 2020,2020,Research Article,118.0
"Li Ning,Peng Xiaoling,Kawaguchi Eric,Suchard Marc A.,Li Gang","Department of Medicine Statistics Core, University of California - Los Angeles, CA, USA,Division of Science and Technology, Beijing Normal University - Hong Kong Baptist University United International College, Zhuhai, China,Division of Biostatistics and Epidemiology, Keck School of Medicine, University of Southern California, CA, USA,Departments of Computational Medicine, Biostatistics and Human Genetics, University of California - Los Angeles, CA, USA,Department of Biostatistics, University of California - Los Angeles, CA, USA","Received 6 February 2020, Revised 1 December 2020, Accepted 4 December 2020, Available online 17 December 2020, Version of Record 7 January 2021.",https://doi.org/10.1016/j.jspi.2020.12.001,Cited by (2),"This paper rigorously studies large sample properties of a surrogate ==== penalization method via iteratively performing reweighted ====parse ====igh ====imensional ====assive ====ample ====-penalized generalized linear models, the proposed BAR method can be conveniently implemented for sHDMSS data. An illustration is given using a large sHDMSS data from the Truven MarketScan Medicare (MDCR) database to investigate the safety of dabigatran versus warfarin for treatment of nonvalvular atrial filbrillation in elder patients.","-penalized regression has been popularly used in the classical variable selection methods through the well-known information criteria such as Mallow’s ==== (Mallows, 1973), Akaike’s information criterion (AIC) (Akaike, 1974), the Bayesian information criterion (BIC) (Schwarz et al., 1978, Chen and Chen, 2008), and risk inflation criteria (RIC) (Foster and George, 1994). It directly penalizes the cardinality of a model and has been shown to possess some optimal properties for variable selection and parameter estimation (Shen et al., 2012). On the other hand, ====-penalization is computationally NP-hard and thus not scalable to high dimensional covariates. It can also be unstable for variable selection (Breiman et al., 1996). The broken adaptive ridge (BAR) method has been recently studied as a scalable surrogate to ==== penalization for simultaneous variable selection and parameter estimation (Dai et al., 2018a, Dai et al., 2018b, Frommlet and Nuel, 2016, Liu and Li, 2016). Defined as the limit of an iteratively reweighted ==== penalization algorithm, the BAR estimator has been shown to enjoy the best of both ==== and ==== penalizations (Dai et al., 2018a, Dai et al., 2018b) while avoiding their shortcomings. For instance, it is easily scalable to high dimensional covariates and has been shown to be consistent for variable selection, oracle for parameter estimation, and has a grouping property for highly correlated covariates for the linear model (Dai et al., 2018b). As a surrogate to ==== penalized regression, BAR tends to yield more parsimonuous models as compared to an ====-type penalization method with comparable prediction performance in empirical studies (Dai et al., 2018b).====The purpose of this paper is to fill some critical gaps in the theoretical and computational development of the BAR methodology for the generalized linear model and extend its application to large scale data. First of all, although the asymptotic properties of the BAR approach have been established for the linear model, it has yet to be fully investigated for the generalized linear model. One of the contributions of this paper is to rigorously establish the asymptotic statistical guarantees of the BAR estimator for the generalized linear model. In particular, we establish its consistency for variable selection and parameter estimation and its grouping property for highly correlated covariates for the generalized linear model. Secondly, as discussed later in Remark 1 of Section 2, for the generalized linear model, current BAR algorithm and implementation will become practically infeasible for large scale data when both ==== and ==== are large, because (1) data stored in the standard dense format will exceed a computer’s memory and (2) the computational algorithms will become prohibitively costly. Another key contribution of this paper is to develop a scalable implementation of the BAR for the generalized linear model for sparse high-dimensional and massive sample-size (sHDMSS) data that has the following characteristics: (1) high-dimensional with thousands of baseline covariates, (2) massive in sample-size with up to millions or even hundreds of millions of patients records, and (3) sparse with only a small portion of covariates being nonzero for each subject. sHDMSS data are commonly encountered in large scale health studies using massive electronic health record (EHR) databases. An example of sHDMSS data is given in Section 4 from the Truven MarketScan Medicare (MDCR) database, which includes 73,206 patients with 17,032 baseline covariates for studying the safety of dabigatran versus warfarin for treatment of nonvalvular atrial filbrillation in elder patients. Our scalable implementation of the BAR method for sHDMSS data exploits the data sparsity and is conveniently implemented by taking advantage of a recently developed ==== package by Suchard et al. (2013) for fitting massive ==== penalized regression for the generalized linear model. Lastly, as a byproduct, our developed asymptotic theory implies that coupling the BAR method with an appropriate sure screening method will lead to an oracle sparse regression method for ultrahigh dimensional settings when the number of predictors far exceeds the sample size. We have developed an R package ==== and made it available to readers at ====.====The paper is organized as follows. In Section 2, we describe the BAR estimator, state our main results on its asymptotic statistical guarantees, discuss how to adapt it to sHDMSS data by taking advantage of existing efficient computation techniques for massive ====-penalized generalized linear models, and combine it with some dimension reduction methods for analysis of ultrahigh dimensional data where the number of predictors far exceeds the sample size. Section 3 presents simulation studies to examine the performance of the BAR estimator for both small and massive sample sizes. We illustrate the proposed approach using a real world sHDMSS data in Section 4. Discussion and concluding remarks are given in Section 5. Technical proofs are provided in the Appendix A.",A scalable surrogate ,https://www.sciencedirect.com/science/article/pii/S037837582030135X,17 December 2020,2020,Research Article,119.0
"Aboukhamseen Suja,Huda Shahariar,Bose Mausumi","Department of Statistics and O.R., Kuwait University, P.O. Box 5969, Safat 13060, Kuwait,Applied Statistics Division, Indian Statistical Institute, Kolkata 700108, India","Received 16 November 2019, Revised 28 November 2020, Accepted 8 December 2020, Available online 14 December 2020, Version of Record 28 December 2020.",https://doi.org/10.1016/j.jspi.2020.12.002,Cited by (0)," of the direct and carryover effects of the same treatment, that is, the ","Crossover designs, also known as repeated measurements designs, are used in experiments where a sequence of treatments is applied to each experimental subject over a number of time periods, and responses are recorded at each period. This not only leads to several practical advantages but also poses challenges in the design and analysis since there are two types of treatment effects, a direct effect of a treatment in the period in which it is applied, together with a carryover effect of this treatment in the following period. There is a long history on the use of these designs in a variety of experimental areas, and in particular, these designs have been widely used in clinical trials.====Crossover designs with two periods are well established in the literature. These designs are practically useful as they keep the duration of the experiment as short as possible, two being the minimum number of periods required for the estimation of direct and carryover effects. For example, these may be used in experiments where one dose is given in the morning and another dose given in the evening, morning and evening being the two periods. Moreover, as will be shown later in Section 2, using two periods has the additional advantage that we can allow the presence of within-subject correlation in the model and still obtain optimal designs under this model without the extra complications which arise for designs with more than two periods. Details of the importance and application of these two-period designs in clinical trials can be found in Grizzle, 1965, O’Neill, 1977, Armitage and Hills, 1982, Barker et al., 1982, Willan and Pater, 1986, Ratkowsky et al., 1993, Hills and Armitage, 1979, Senn, 1994 and many others.====The study of optimal crossover designs was initiated by Hedayat and Afsarinejad (1978), and this has been followed by a surge of results. Some of these optimality results for different situations are by Cheng and Wu, 1980, Sen and Mukerjee, 1987, Stufken, 1991, Kushner, 1997a, Shah et al., 2002, Bose and Stufken, 2007, Majumdar et al., 2008, Hedayat and Yang, 2006, Kunert and Stufken, 2008, Zheng, 2013, and many others. For the special case of two periods, optimal designs were obtained by Hedayat and Zhao, 1990, Carriere and Reinsel, 1993, Afsarinejad and Hedayat, 2002. Many more references may be found in Bose and Dey (2009) and Lui (2016).====The available optimality results are almost always for the separate inference on direct or carryover effects. However, as has been pointed out in Bailey and Druilhet (2004), an aim of a designed experiment is to recommend one single treatment for use over longer time periods than those used in the experiment, and so when this recommended treatment is used, it will be followed by only itself. Consequently, an effect of utmost importance is the sum of the direct and carryover effects of the same treatment. The importance of this sum has been recognized and studied since the initial days of study of crossover designs and can be seen from the many examples cited in Bailey and Druilhet (2004). For instance, Patterson, 1950, Patterson, 1951, Matthews, 1988 and Bailey and Druilhet (2004) called this the ‘total effect’ of treatment, Kempton (1991) called it ‘the permanent effect’ while Kempton (1997) called it the ‘pure stand’ effect. We shall henceforth use the term ‘total effect’.====The pioneering work on optimality for total effects is by Bailey and Druilhet (2004) who studied this in the context of block designs with neighbour effects under a circular model with uncorrelated errors. They showed that the circular neighbour balanced block design is universally optimal for total effects among all designs with no self neighbour. Their result implies that under a circular model with uncorrelated errors, balanced crossover designs are universally optimal for total effects within the class of all designs where no treatment precedes itself. However, there seems to be no available optimality result for total effects for crossover designs under a non-circular model within the general class of all designs.====To fill this gap, in this article, we consider two-period crossover designs under a non-circular model and obtain universally optimal designs for inferring on total effects in the general class of all designs. Moreover, we allow two observations from the same subject to be correlated. For some situations we give highly efficient designs.",Optimal crossover designs for inference on total effects,https://www.sciencedirect.com/science/article/pii/S0378375820301361,14 December 2020,2020,Research Article,120.0
"De Oliveira Melaine C.,Castro Luis M.,Dey Dipak K.,Sinha Debajyoti","Department of Statistics, Florida State University, P.O. Box 3064330, Tallahassee, FL, USA,Department of Statistics, Pontificia Universidad Católica de Chile, Casilla 306, Correo 22, Santiago, Chile,ANID - Millennium Science Initiative Program - Millennium Nucleus Center for the Discovery of Structures in Complex Data, Santiago, Chile,Centro de Riesgos y Seguros UC, Pontificia Universidad Católica de, Chile, Santiago, Chile,Department of Statistics, University of Connecticut, 215 Glenbrook Road, Storrs, CT 06269-4120, USA","Received 20 April 2019, Revised 6 August 2020, Accepted 23 November 2020, Available online 8 December 2020, Version of Record 22 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.010,Cited by (1),"For existing ==== (MCMC) samples from a single posterior based on the full data. Using a Bayesian meta-analysis of clinical trials, we illustrate how our new measures of influence of observations have more useful practical roles for data analysis than popular Bayesian residual analysis tools.","For implementing many statistical procedures including multiple regression, discriminant analysis, and principal components analysis, an influential diagnostic is an essential tool to identify the set of observations that profoundly affect the statistical inference of the parameter of interest and subsequent conclusion of the statistical analysis. Contrary to a suspected outlier (Gelfand et al., 1992) that has a small probability of occurrence under the fitted model and may not substantially affect the final inference, an influential observation may not necessarily have a low probability of occurrence under the fitted model. For practical and thorough statistical analysis, identification of the influential observations helps to further investigate their common features, characteristics, and congruity of the key modeling assumptions with them. For example, in a meta-analysis of multiple clinical trials, we would like to identify the trials that are influential in making inference about overall treatment effect to further investigate common features and characteristics (say, sample size, design, population under study) of these influential trials. One way to deal with influential observations is to develop “robust” methods (Mosteller and Tukey, 1977, Huber, 1977, Hogg, 1979) that are relatively less influenced by these observations. However, even robust methods may be influenced by a few observations (Krasker and Welsch, 1979), and, hence, even for them it is important to identify the influential observations.====Since Tukey (1977), Cook (1977), Hoaglin and Welsch (1978) and Chatterjee and Hadi (2009), “cross-validation tools” have become the most popular tools to measure the influence of an observation. These methods, alternatively called “case-influence assessment”, are based on the effect of deletion of the ====th observation ==== on inference. One such popular Bayesian method to identify influential observations uses Kullback–Leibler (KL) divergence between the posteriors ==== and ==== (e.g. Johnson and Geisser, 1982, Pettit and Smith, 1985, Weiss, 1996, Bradlow and Zaslavsky, 1997, Weiss and Cho, 1998), where ==== is the full data and ==== is the cross-validated data with observation ==== removed. Weiss and Cho (1998) establish the relationship between this influence measure and popular cross-validated Bayesian residuals, such as conditional predictive ordinate (CPO) (Gelfand et al., 1992, Dey et al., 1997). Our paper tries to investigate whether any further extension of these Bayesian influence measures can avoid the major impediments to the widespread use of the influence measures in practical data analysis. We would also address the challenge of proper and easy to interpret calibration of our influence measure.====In Section 2, we present our new cross-validated Bayesian influence measures based on Bregman Divergence (BD) (Banerjee et al., 2005) and show that the existing KL based influence measure is a special case of our influence measure. We show that these new BD based measures are more informative and have more desirable properties than their special case, existing KL based measures. We demonstrate the method of calibrating these new BD based influence measures when we are interested in the influence of each observation on various aspects, such as mean, variance and shape, of the posterior density. In Section 3, we show that, similar to existing KL based Bayesian influence measures, our BD based influence measure can be computed using Markov chain Monte Carlo (MCMC) samples from the single full posterior ====. In Section 4, using a Bayesian meta-analysis example, we illustrate how these new BD based measures of influence can detect the influential clinical trials and their effects on different aspects of the Bayesian meta-analysis. We demonstrate that these BD based influence measures are more useful and practical for model diagnostic analysis than popular Bayesian cross-validated residual analysis tools.",Bregman divergence to generalize Bayesian influence measures for data analysis,https://www.sciencedirect.com/science/article/pii/S0378375820301257,8 December 2020,2020,Research Article,121.0
"Pronzato Luc,Sagnol Guillaume","Université Côte d’Azur, CNRS, Laboratoire I3S, 2000 route des lucioles, 06900 Sophia Antipolis, France,Institut für Mathematik, Sekr. MA 5-2, Technische Universität Berlin, Straße des 17. Juni 136, 10623 Berlin, Germany","Received 24 June 2020, Revised 17 November 2020, Accepted 24 November 2020, Available online 8 December 2020, Version of Record 22 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.011,Cited by (1),"A design point is inessential when it does not contribute to an optimal design, and can therefore be safely discarded from the design space. We derive three inequalities for the detection of such inessential points in ====-optimal design: the first two are direct consequences of the equivalence theorem for ====-optimality; the third one is derived from a second-order cone programming formulation of ====-optimal design. Elimination rules for ====-optimal design are obtained as a byproduct. When implemented within an optimization algorithm, each inequality gives a screening test that may provide a substantial acceleration by reducing the size of the problem online. Several examples are presented with a multiplicative algorithm to illustrate the effectiveness of the approach.","Let ==== denote a set of ==== symmetric non-negative definite matrices and denote by ==== its convex hull. We assume that the linear span of ==== contains a nonsingular matrix. A design corresponds to a probability measure on ====, that is, to a vector of weights ==== in the probability simplex ====. The extension to designs ==== defined as probability measures on an arbitrary compact set of matrices ==== is straightforward, but for the sake of simplicity in the rest of the paper we shall restrict the presentation to the discrete case. A ====-optimal design, associated with a nonzero vector ====, is defined by a vector ==== that minimizes ====with respect to ====, with ==== the (information) matrix ====and ==== a pseudo-inverse of ====. We set ==== when ==== does not belong to the column space of ====; the value of ==== does not depend on the choice of the pseudo-inverse ====.====A typical example is when ==== for all ====, with ==== the vector of regressors in the model with observations ====parameters ==== and i.i.d. errors ==== having zero mean and variance ====. Suppose that ==== observations are collected according to the design ====, with ==== such that ==== for all ====. Then, ==== is the variance of ====, with ==== the Best Linear Unbiased Estimator (BLUE) of ====.====When the observations ==== are multivariate, with ==== in ==== where the errors ==== have the ==== covariance matrix ====, ==== is still equal to ====, with now ====in ====, and each ==== may have rank larger than 1; see for instance Harman and Trnovská (2009) for the case of ====-optimal design that maximizes ====. When setting a normal prior ==== on ==== in the regression model (1.3), with ==== having full rank, and when the errors ==== are normal ====, the posterior variance of ==== after ==== observations is ====, with ==== given by (1.1) and ====in (1.2); one may refer in particular to Pilz (1983) for a thorough exposition on optimal design for Bayesian estimation (Bayesian optimal design). When ==== is given by (1.5), ====-optimal design is equivalent to the minimization of ==== where ====, ====, and ==== for all ====, so that we can assume that ====, the ====-dimensional identity matrix, without any loss of generality. Other cases of interest correspond to ====- and ====-optimality, they will be considered in Section 4.====In general, not all ==== contribute to an optimal design ==== as some of the weights ==== equal zero. We shall say that such an ====, which does not support an optimal design, is inessential. In Section 2 we show that any nonsingular design ==== yields two simple inequalities that must be satisfied by an ==== that contributes to an optimal design. Any ==== that does not satisfy these screening inequalities is therefore inessential and can be safely eliminated from ==== — we shall sometimes speak of (design-) point elimination instead of matrix elimination, which is all the more appropriate when ==== has the form (1.5) and the ==== are seen as points in ====. The idea originated from Pronzato (2003) and Harman and Pronzato (2007) for ====-optimal design (and for the construction of the minimum-volume ellipsoid containing a set of points); it was further extended to Kiefer’s ==== criteria (Pronzato, 2013), to ====-optimal design (Harman and Rosa, 2019), and to the elimination of inessential points in the smallest enclosing ball problem (Pronzato, 2019). In Section 3, we use the Second-Order Cone Programming formulation of ====-optimal design of Sagnol (2011) to derive a third screening inequality. Since ====-optimal design can be seen as ====-optimal design in a higher dimensional space, see Sagnol (2011), the same screening inequalities can be used to eliminate inessential points in ====-optimal design. This is considered in Section 4, where a comparison is made with the elimination method of Pronzato (2013). The inequalities proposed in the paper become more stringent when ==== approaches an optimal design ====, which can be used to accelerate the construction of an optimal design. The examples of Section 5 provide an illustration for the particular case of a “multiplicative algorithm”, classical in optimal design, but elimination of inessential could benefit other algorithms, such as for instance those proposed in Yang et al. (2013) and Pronzato and Zhigljavsky (2014), which rely on finite sets of candidates.",Removing inessential points in ,https://www.sciencedirect.com/science/article/pii/S0378375820301324,8 December 2020,2020,Research Article,122.0
"Pinheiro Eliane C.,Ferrari Silvia L.P.,Medeiros Francisco M.C.","Department of Statistics, University of São Paulo, Brazil,Department of Statistics, Federal University of Rio Grande do Norte, Brazil","Received 2 May 2019, Revised 24 November 2020, Accepted 30 November 2020, Available online 5 December 2020, Version of Record 18 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.013,Cited by (0), while the approximation error of confidence intervals based on the first-order ====. ==== confirm the theoretical findings. An implementation for regression models and real data applications is provided.,"In applied statistics it is often the case that the practitioner wishes to estimate the parameters of a statistical model fitted to a data set. A point estimate is a single realization from the distribution of the possible values of the chosen estimator and does not carry information on its uncertainty. An uncertainty description is provided by confidence intervals.====Confidence intervals can, in principle, be constructed from the distribution of the estimator, but the exact distribution is unknown except in particular situations. The most common approach for estimating parameters of a parametric family of distributions is the use of Wald-type confidence intervals, i.e. maximum likelihood estimates (MLEs) coupled with the corresponding first-order asymptotic normal approximation. For large sample sizes, this approach is reasonably reliable and leads to virtually unbiased estimates and confidence intervals with approximately correct coverage. In small and moderate sized samples, the distribution of the MLEs may be far from normal, exhibiting bias, skewness, and also kurtosis that are not compatible with the normal distribution. Other standard methods for constructing confidence intervals are based on the score statistic and the signed likelihood ratio statistic, but both rely on first-order asymptotics.====A vast literature on bias adjustments for MLEs is available; see for instance Firth (1993), Ferrari and Cribari-Neto (1998), Cordeiro and Cribari-Neto (2014), Kenne Pagui et al. (2017), and references therein. For a review paper, see Kosmidis (2014). Additionally, effort has been made to correct the coverage of approximate confidence intervals in limited samples. Bartlett (1953) proposed an approximate confidence interval based on a skewness correction to the score function. An alternative possible approach is the use of computer intensive methods such as the bootstrap procedure (DiCiccio and Efron, 1996).====This paper proposes accurate approximate confidence intervals for a scalar parameter of interest possibly in the presence of a vector of nuisance parameters in general parametric families. We construct the proposed confidence intervals from a third-order approximation to the quantiles of the score function. Some features of the proposed confidence intervals are: first, the coverage approximation error is ==== while the approximation error of confidence intervals based on the first-order asymptotic distribution of the Wald, score, and signed likelihood ratio statistics is ====; second, they are equivariant under interest-respecting reparameterizations; third, they account for skewness and kurtosis of the distribution of the score function; fourth, they do not require computer-intensive resampling mechanisms; fifth, the confidence limits are simply computed from modified score equations. For a review on different routes for achieving third-order accuracy in confidence intervals for a scalar parameter of interest, including small-sample likelihood asymptotics, bootstrapping and the objective Bayes approach, the reader is referred to Young (2009).====In Section 2, we deal with the one parameter case. In Section 3, we extend the results to the case of a single parameter of interest and a fixed number of nuisance parameters. An implementation of accurate approximate confidence intervals for regression models is presented in Section 4. Monte Carlo evidence on the performance of the modified confidence intervals is presented in Section 5. Applications are presented and discussed in Section 6. The paper ends with concluding remarks in Section 7. Some technical details are left for Appendix A Proof of, Appendix B Symmetric regression: cumulants, Appendix C Beta regression: cumulants.",Higher-order approximate confidence intervals,https://www.sciencedirect.com/science/article/pii/S0378375820301348,5 December 2020,2020,Research Article,123.0
"Li Xiuqi,Ghosal Subhashis","Department of Statistics, North Carolina State University, United States of America","Received 14 February 2020, Revised 23 November 2020, Accepted 25 November 2020, Available online 3 December 2020, Version of Record 14 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.012,Cited by (3),We propose a ,"Change point detection is an important aspect of data analysis. In recent years, detecting change points in functional data has received considerable attention. Berkes et al. (2009) developed a method by projecting the difference of mean functions on the principal components for the data. Aue et al. (2009) derived the asymptotic distribution of a change point in the mean of a functional observation. Zhang et al. (2011) developed a self-normalization based test to identify the potential change points. Aston and Kirch (2012) introduced an estimator of a change point in a model for an epidemic, where a change in the state of the epidemic occurs, and then the observations return to baseline at a later time. Sharipov et al. (2016) developed a test for structural changes in functional data taking values in a Hilbert space and determined critical values by bootstrap. Aue et al. (2018) proposed a method to uncover structural breaks in functional data without using dimension reduction techniques.====In this paper, we propose a Bayesian method to detect change points in functional data. We consider the setup where functional observations appear in a sequence, and can be thought of as unknown signal functions observed with a noise process independent for each observation in the sequence. Each function is viewed as a collection of its features, typically described by its coefficients in a wavelet basis expansion. Our approach fundamentally differs from others, in that we do not assume that the true signals suddenly change completely at a change point and are identical for all before the change point and also identical for all after the change point. Instead, we let the signals evolve steadily with some, but not all, features of the signal function possibly changing at a given time, mimicking the process of evolution of species in nature. These changes cumulate and at a stage, their overall effect becomes substantial. Representing functions in a wavelet series, the features in a function can be quantified through their wavelet coefficients when expanding on a wavelet basis. While any orthogonal basis can be used to expand a function, wavelets are particularly effective in picking up local features. Analyzing functional data by treating wavelet coefficients was adopted by Hyndman and Shang (2009), Antoniadis et al. (2013) and Suarez and Ghosal (2016). Hyndman and Shang (2009) represented wavelet coefficients of functional observations as independent variables in forecasting functional time series by functional principal component regression and weighted functional partial least squares regression. Their approach allows to assign higher weights to more recent data, and provide a modeling scheme that is easily adapted to allow for constraints and other information. Representation of functions through wavelet coefficients is a very common and useful approach for putting a prior on an unknown function (Abramovich et al., 1998, Abramovich et al., 2004, Lian, 2011, Suarez and Ghosal, 2016).====We consider two settings of functional observations — either observed over a regular grid, or observed continuously over time. We extract the features of functional data by the discrete wavelet transform (DWT) in the former case, and using inner products with wavelet functions in the latter case. The signal-plus-noise model for functional data then reduces to the problem of detecting a change in the value of the mean vector in a multivariate (or an infinite-dimensional) normal model. Since there are no functional dependencies between the coefficients, we can treat each sequence of feature separately independently, and hence put independent priors on these coefficients. The prior for each coefficient allows for a change in the value of a wavelet coefficient before and after some time point, but this change point need not be the same or related across different wavelet coefficients. Therefore, the problem of the change point for functional data is reduced to the problem of multiple (or infinitely many) one-dimensional change points. Each of the latter can be solved very efficiently by a Bayesian method based on conjugate priors, allowing explicit posterior computation without needing to sample from the posterior distribution. Finally, the change points for individual wavelet coefficients need to be synthesized to a single value that represents the most prominent change point for the entire signal. To this end, we introduce a similarity matrix whose ====th entry measures the fraction of wavelet coefficients of ==== and ==== are tied together when the coefficients are sampled from their posterior distributions. The posterior expected value of this similarity can be obtained in terms of the posterior distributions of the change points for each wavelet coefficients, which are analytically computed in our setting. We define a synthetic change point for the overall change as the value that minimizes the ratio of the mean similarities between groups and within groups, where one group is formed by time points before and the other by time points after the given value. This process can also be repeated more than once to detect multiple change points. The number of change points to be found may also be adaptively obtained from the data using some stopping criterion involving these similarity measures.====The paper is organized as follows. In the next section, we formally introduce the model and describe the prior. Posterior computational formulas are presented in Section 3. Under a small noise regime, a posterior consistency result on estimation and identification of the change point is presented in Section 4. A simulation study demonstrating the accuracy of the proposed method, and an application to a dataset on climate change over a 250 years period are respectively presented in Sections 5 Simulation, 6 Application to a climate change dataset . Proofs of the results are given in Section 7.",Bayesian change point detection for functional data,https://www.sciencedirect.com/science/article/pii/S0378375820301336,3 December 2020,2020,Research Article,124.0
Hashimoto Shintaro,"Department of Mathematics, Hiroshima University, Hiroshima, Japan","Received 9 July 2020, Revised 2 November 2020, Accepted 19 November 2020, Available online 29 November 2020, Version of Record 11 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.007,Cited by (3),This paper presents reference priors for non-regular model whose support depends on an unknown parameter. A multi-parameter family which includes both regular and non-regular structures is considered. The resulting priors are obtained by asymptotically maximizing the expected ,"In Bayesian inference, when there is no prior information, we often begin inference by using objective priors such as non-informative or default priors. Then we are often faced with a problem of the selection of objective prior in a given context. One of the most widely used objective priors is the Jeffreys prior proposed by Jeffreys (1961). The Jeffreys prior is proportional to the positive square root of the Fisher information function in one-dimensional case. On the other hand, the reference prior was proposed by Bernardo (1979) and was extended by Berger and Bernardo (1989) in the presence of nuisance parameters. The reference prior is defined by maximizing the Kullback–Leibler (KL) divergence between the prior and the posterior under some regularity conditions. This prior maximizes the expected posterior information to the prior, i.e., the prior is the ‘least informative’ prior in some aspects. In the context of the reference priors, Ghosh et al. (2011) derives the priors which asymptotically maximize a more general divergence measure (called the ====-divergence) between the prior and the corresponding posterior under some regularity conditions. We note that the ====-divergence smoothly connected the KL divergence (====), the reverse KL divergence (====), the squared Bhattacharyya–Hellinger divergence (====) and the chi-square divergence (====) (see e.g. Amari (1982) and Cressie and Read (1984)). Recently, Liu et al. (2014) extend the result of Ghosh et al. (2011) to a multi-parameter model with or without nuisance parameters for regular parametric family. Beside the prior selection problem, statistical inference and prediction based on the ====-divergence have been also developed in recent years (see e.g. Corcuera and Giummolè, 1999, Ghosh et al., 2008, Ghosh and Mergel, 2009, Maruyama et al., 2019).====However, Ghosh et al. (2011) and Liu et al. (2014) deal with the regular parametric models and these results are not applied for non-regular cases whose supports of the density depend on unknown parameter. For example, the uniform and shifted exponential distributions have the parameter dependent supports and such non-regular distributions are also important in applications. For examples, the auction and search models in structural econometric models have a jump in the density and the jump is very informative about the parameters (e.g. Chernozhukov and Hong (2004)). In such non-regular cases, for example, the asymptotic normality of the posterior distribution does not hold (Ghosal and Samanta, 1995). Ghosal and Samanta (1997) show the prior which maximizes the KL divergence for a non-regular one-parameter family of distributions. In non-regular case, the prior which is different from the Jeffreys prior is derived. For a multi-parameter case, Ghosal (1997) gives the reference prior based on the KL divergence for multi-parameter non-regular model from the perspective of information theory. As related results, Ghosal (1999) also derives probability matching priors and Hashimoto (2019) derives moment matching priors for the same non-regular model as that of Ghosal (1997).====In this paper, we consider a certain multi-parameter family of distributions ==== which includes both regular and non-regular structures. In other words, this model is regular with respect to ==== for fixed ====, and is non-regular with respect to ==== for fixed ====. In this paper, we call ==== and ====, respectively. For example, the shifted exponential distribution with the density function ====
 (====, ====) belongs to this family of distributions. For such model, the reference priors based on the expected ====-divergence for ==== are derived by using the higher order asymptotic expansion for the posterior distribution. The results in this paper are a kind of generalizations of the result in Ghosal (1997) which is used the expected KL divergence. The resulting reference priors are different forms from Ghosal (1997) except for ====. However, in the location-scale family (see Example 1), if ==== is the parameter of interest, the reference prior for ==== is the same as Ghosal (1997)’s one which is the right invariant Haar measure when ==== and ====. In other words, in this case, our prior has loss-robustness for ====. On the other hand, if ==== is the parameter of interest, the resulting prior for ==== is not same as that of Ghosal (1997), that is, our prior does not have loss-robustness for ==== in such case. This is very interesting phenomenon. Furthermore, for ====, that is, the chi-square divergence, we also derive a new reference prior when ==== is the parameter of interest.====This paper organizes as follows: in Section 2, we introduce the higher order asymptotic representations for posterior density in non-regular case and the definition of the maximum ====-divergence prior in the presence of a nuisance parameter. In Section 3, we derive the marginal reference priors for the non-regular parameter ==== in the presence of the regular nuisance parameter ==== for ====. It is also shown that there is generally no reference prior for ====. In a similar way, we derive the marginal reference prior for the regular parameter ==== in the presence of the non-regular nuisance parameter ==== for ====. It is also shown that there is generally no reference prior for ====. Further, we give the explicit form of the marginal reference prior for ==== in the case of ====. Overall reference priors for ==== are calculated by using Berger and Bernardo (1989)’s algorithm (for details, see Section 2.2). As examples, we show the reference priors in the case of the (non-regular) location-scale family of distributions and the truncated Weibull distribution with known shape parameter.",Reference priors via ,https://www.sciencedirect.com/science/article/pii/S0378375820301221,29 November 2020,2020,Research Article,125.0
"Wang Chunyan,Yang Jinyu,Liu Min-Qian","School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China","Received 19 June 2020, Revised 20 November 2020, Accepted 20 November 2020, Available online 27 November 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.009,Cited by (5),"For designs of computer experiments, column-orthogonality and space-filling property are two ====. In this paper, we develop methods for constructing a new class of designs that include orthogonal ==== play a key role in the construction.","Computer experiments are being widely used in many fields recently, and space-filling designs have been appropriate designs for computer experiments (Fang et al., 2006). A space-filling design is a design that spreads its points in the design region uniformly, where the uniformity can be evaluated by some distance or discrepancy criteria. Based on the effect sparsity principle (Wu and Hamada, 2009), only a few factors are expected to be active in a computer experiment. Therefore, for a high-dimensional design region, it may be more reasonable to consider the space-filling properties in low-dimensional projections (Joseph et al., 2015) and (Sun et al., 2019). There have been fruitful approaches for constructing space-filling designs with good low-dimensional stratifications. Latin hypercube designs (LHDs), proposed by McKay et al. (1979), are of utmost popularity among the space-filling designs. An LHD with ==== runs possesses ==== equally spaced levels, which guarantees the maximum stratification in one dimension. Owen (1992) proposed randomized orthogonal arrays (OAs), which can be used to generalize LHDs and lattice samplings. Tang (1993) constructed LHDs based on OAs for getting designs with attractive ====-dimensional space-filling property for any ==== no more than ====, where ==== is the strength of the corresponding OAs. Recently, He and Tang (2013) introduced strong orthogonal arrays (SOAs) and Mukerjee et al. (2014) proposed mappable nearly orthogonal arrays (MNOAs), both can be used to construct space-filling designs better than those based on ordinary OAs.====In addition to the space-filling property, the column-orthogonality is also a desirable aspect for designs of computer experiments since it allows uncorrelated estimations of the main effects in linear models and effective factor screening in Gaussian process models. A number of methods have been proposed to construct orthogonal designs (ODs). Some are on the construction of orthogonal LHDs (OLHDs), see e.g., Steinberg and Lin (2006), Bingham et al. (2009), Pang et al. (2009), Sun et al., 2009, Sun et al., 2010, Lin et al., 2009, Lin et al., 2010, Georgiou (2009), Georgiou and Stylianou (2011), Yang and Liu (2012), Yin and Liu (2013), Ai et al. (2012), Georgiou and Efthimiou (2014), Sun and Tang (2017a), Wang et al. (2018) and the references therein. Besides, Liu and Liu (2015) and Zhou and Tang (2019) studied the constructions of column-orthogonal SOAs. Sun and Tang (2017b) provided constructions of ODs with two-dimensional space-filling properties.====In this paper, we introduce a new class of ODs with both two- and three-dimensional space-filling properties, which include OLHDs as special cases. To see the benefits of using such a design, we now provide an illustrative example. For a given run size ====, an OA of ==== levels (with strength 2) can accommodate 10 factors, which can achieve stratifications on 9 × 9 grids in all two dimensions, an OA of 3 levels (with strength 2) can accommodate 40 factors, which can achieve stratifications on 3 × 3 grids in all two dimensions, while an OA of strength ==== with ==== levels can accommodate 10 factors, which can achieve stratifications on 3 × 3 × 3 grids in all three dimensions. The column-orthogonal SOA with ==== runs of strength ==== constructed in Zhou and Tang (2019) can accommodate ==== factors each of 9 levels, which achieves stratifications on 3 × 9 and 9 × 3 grids in any two dimensions. Now consider the OD constructed in this paper for run size 81, it can accommodate ==== factors each of 9 levels, and guarantee stratifications on 3 × 9 and 9 × 3 grids in ==== out of all ==== (i.e., 93.03%) two dimensions and 3 × 3 grids for all two dimensions. Meanwhile it achieves stratifications on 3 × 3 × 3 grids in ==== out of all ==== (i.e., 94.74%) three dimensions. We can see that the OD has competitive space-filling properties while can accommodate more factors, making it a good choice for computer experiments.====This paper is organized as follows. Section 2 introduces some preliminaries used in this paper. In Section 3, we provide the construction methods and some theoretical results. Section 4 considers the mixed-level case of this kind of ODs when the factors have different numbers of levels. Concluding remarks are provided in Section 5. All proofs are deferred to Appendix A.",Construction of space-filling orthogonal designs,https://www.sciencedirect.com/science/article/pii/S0378375820301245,27 November 2020,2020,Research Article,126.0
"Zhang Jin-Ting,Zhou Bu,Guo Jia,Zhu Tianming","Department of Statistics and Applied Probability, National University of Singapore, Singapore,School of Statistics and Mathematics, Zhejiang Gongshang University, Hangzhou, China,School of Management, Zhejiang University of Technology, Hangzhou, China","Received 11 March 2020, Revised 29 October 2020, Accepted 19 November 2020, Available online 27 November 2020, Version of Record 7 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.008,Cited by (12),-approximation with the approximation parameters consistently estimated from the data. The asymptotic power of the proposed test is established. Good performance of the proposed test against several existing competitors is demonstrated via several simulation studies and illustrated by a real data example.,"High-dimensional data are frequently collected in various research and industrial areas in recent years. A common feature of high-dimensional data is that the data dimension ==== is close to or even much larger than the total sample size. For example, in microarray gene expression studies, each subject is measured usually by thousands of gene expressions but we only have a few subjects. Therefore, traditional methods may not be always applicable and are usually subject to instability. Methods for statistical inferences for high-dimensional data have recently been paid much attention.====In this paper, we are interested in the two-sample Behrens–Fisher (BF) problems for high-dimensional data which tests the equality of the mean vectors of two high-dimensional samples without assuming the equality of the two covariance matrices. It is motivated by the well-known leukemia microarray data set, available at ====. The leukemia data set contains two groups of high-dimensional observations: ==== patients with acute lymphoblastic leukemia (ALL) and ==== patients with acute myeloid leukemia (AML), each having ==== gene expression levels. Of interest is to check if the two groups of leukemia have the same mean expression levels. Since the data dimension is very high, it is difficult to check if the two covariance matrices are equal. This problem is then a high-dimensional two-sample BF problem. Mathematically, a two-sample BF problem for high-dimensional data can be described as follows. Suppose we have two independent high-dimensional samples: ====where the dimension ==== is very large, and may be much larger than the total sample size ====. We want to test whether the two mean vectors are equal: ====without assuming ====.====When ==== is fixed and much smaller than the total sample size ====, the above problem is known as a multivariate two-sample BF problem. It can be tested using the following classical Wald-type test statistic: ====where ==== and ==== are the group sample mean vectors and ====where ==== are the usual sample covariance matrices of the two samples. The statistic ====
 (3) is obtained via replacing the pooled sample covariance matrix in Hotelling (1931)’s ==== statistic with ====. Unlike Hotelling (1931)’s ==== statistic whose null distribution is a scaled F-distribution, the exact null distribution of ==== is usually intractable. However, the null distribution of ==== can be well approximated by a scaled F-distribution with parameters estimated using several methods, see, for example, Yao (1965), Johansen (1980), Krishnamoorthy and Yu (2004), Zhang, 2011, Zhang, 2012 among others. But in the high-dimensional scenario, the test statistic ==== is undefined because the matrix ====
 (4) is singular when the group sample sizes ==== and ==== are smaller than the data dimension ====. Even when ==== is smaller than the total sample size ==== but close to the group sample sizes ==== and ====, as will be seen from the simulation results presented in Table 1, Table 2 of Section 3, the tests based on ==== may not have a good size control and are less powerful. A possible reason is that in these cases, ==== is nearly singular. This shows that the classical BF tests are less useful in the high-dimensional settings.====Recently, much attention has been paid to this important two-sample BF problem for high-dimensional data. For example, Chen and Qin (2010) studied a U-statistics-based approach. They proposed the following test statistic ====Note that ====, where and throughout ==== denotes the ====-norm of a vector ==== and ==== is the trace of a matrix ====. ==== is constructed as the unbiased estimator of ====, resulting in a linear combination of the U-statistics-based estimators of the three terms in the expansion of ==== as shown in (5), which is a rather complicated expression and requires considerable efforts in programming and computing. In addition, ==== uses a normal approximation to its null distribution. However, when the required regularity conditions of Chen and Qin (2010) are not satisfied, ==== may not be asymptotically normal. Therefore, a normal approximation to the null distribution of ==== is not always applicable.====To overcome this difficulty, following Zhang et al. (2020), our ====-norm based test statistic for the BF problem (2) is constructed as ====which is just a scaled squared ====-norm of the sample mean difference vector ====. It is seen that ==== is always nonnegative while ==== can take both negative or positive values. Note that under some regularity conditions, as both ==== and ==== tend to infinity, ==== can have a normal limiting distribution; see Theorem 1(b) for details. However, under some other regularity conditions, as both ==== and ==== tend to infinity, ==== can also have a non-normal limiting distribution; see Theorem 1(a) for details. The current literature mainly focuses on the asymptotic normality of a test statistic for high-dimensional data via imposing strong regularity conditions which guarantee the asymptotic normality of the associated test statistic without considering if the normal approximation used is adequate for approximating the underlying null distribution of the test statistic in real data analysis. Examples include Bai and Saranadasa (1996), Chen and Qin (2010), Nishiyama et al. (2013), Feng et al. (2015), Hu et al. (2017) and  Yamada and Himeno (2015), and among others. In this paper, one of our aims is to show that our test based on the test statistic (6) can work well without imposing some strong assumptions which guarantee the asymptotic normality of the test statistic as in the aforementioned references. Therefore, comparing with the existing tests, it is expected that our new test will have a better size control.====It is worthwhile to highlight the differences between Zhang et al. (2020) and this paper since both articles use ====
 (6) as their test statistic. First of all, Zhang et al. (2020) study ==== under the assumption that the underlying two covariance matrices of the two samples (1) are equal, i.e., ==== where ==== is the common covariance matrix. This covariance homogeneity assumption is a very strong assumption and is hard to be verified for high-dimensional data especially when the data dimension is much larger than the total sample size and hence it limits the applications of the test developed by Zhang et al. (2020). In this paper, on the other hand, we study ==== without imposing this covariance homogeneity assumption and hence our test has a wider range of applications. Secondly, both Zhang et al. (2020) and this paper show that ==== and a ====-type mixture have the same normal and non-normal limiting distributions, but the ====-type mixture in Zhang et al. (2020) just depends on the eigenvalues of ==== while the ====-type mixture (9) obtained in this paper depends on ==== and ==== and ==== and ====. This means that it will be much more involved to derive the asymptotic null limiting distributions of ==== in this paper than in Zhang et al. (2020). Thirdly, both Zhang et al. (2020) and this paper employ the well-known Welch–Satterthwaite (W–S) ====-approximation to the null distribution of ==== with the approximation parameters ==== and ==== determined via the moment-matching approach, but ==== and ==== in Zhang et al. (2020) depend on the eigenvalues of ==== only while in this paper they depend on ==== and ==== and ==== and ==== in a complicated way; see (17) for details. To derive the ratio-consistent estimators ==== and ==== of ==== and ====, respectively, it is much more involved in this paper than in Zhang et al. (2020). For example, unlike in Zhang et al. (2020), to show the ratio-consistency of ==== and ====, in this paper, we have to show the ratio-consistency of ==== for ====. Fourthly, in this paper, we show theoretically and with some numerical results that the covariance-heterogeneity-based test proposed in this paper can still work well even when the two underlying covariance matrices are actually equal while the covariance-homogeneity-based test proposed in Zhang et al. (2020) may not work well when the two underlying covariance matrices are actually unequal unless the two sample sizes ==== and ==== are equal. Fifthly, Zhang et al. (2020) derive the upper density approximation error bound of the W–S ====-approximation while in this paper, we derive the upper distribution approximation error bound of the Welch–Satterthwaite ====-approximation. We believe that the upper distribution approximation error bound of the W–S ====-approximation is more appropriate to measure the approximation accuracy of the W–S ====-approximation since the tail probability of the test statistic is mainly of our concern in hypothesis testing and it provides a useful theoretical justification for the use of W–S ====-approximation. Sixthly, Zhang et al. (2020) derive the asymptotic power of ==== when its null distribution is asymptotically normal while in this paper, we derive the asymptotic powers of ==== when its null distribution is asymptotically normal or non-normal. Finally, we show that via some intensive simulation studies that our test outperforms several existing competitors, including three traditional Behrens–Fisher approximate solutions (Yao 1965, Johansen 1980, and Krishnamoorthy and Yu 2004), two recently proposed Behrens–Fisher tests (Chen and Qin 2010 and Feng et al. 2015) and the test by Zhang et al. (2020).====The rest of the paper is organized as follows. The methods and main results are described in Section 2. Methods for approximating the null distribution of the test statistic are proposed. Simulation studies are given in Section 3. Applications to the leukemia data are presented in Section 4. Some concluding remarks are given in Section 5. Technical proofs of the main theorems are outlined in Appendix.",Two-sample Behrens–Fisher problems for high-dimensional data: A normal reference approach,https://www.sciencedirect.com/science/article/pii/S0378375820301233,27 November 2020,2020,Research Article,127.0
"Amorino Chiara,Gloter Arnaud","Laboratoire de Mathématiques et Modélisation d’Evry, CNRS, Univ Evry, Université Paris-Saclay, 91037, Evry, France","Received 16 January 2020, Revised 13 November 2020, Accepted 19 November 2020, Available online 27 November 2020, Version of Record 8 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.006,Cited by (14),"We consider the solution ==== of a multivariate stochastic differential equation with Levy-type jumps and with unique invariant probability measure with density ====. We assume that a continuous record of observations ==== is available.====In the case without jumps, Dalalyan and Reiss (2007) and Strauch (2018) have found convergence rates of invariant density estimators, under respectively isotropic and anisotropic Hölder smoothness constraints, which are considerably faster than those known from standard multivariate density estimation.====We extend the previous works by obtaining, in presence of jumps, some estimators which have the same convergence rates they had in the case without jumps for ==== and a rate which depends on the degree of the jumps in the one-dimensional setting. We propose moreover a data driven bandwidth selection procedure based on the Goldenshluger and Lepski method (Goldenshluger and Lepski, 2011) which leads us to an adaptive non-parametric kernel estimator of the stationary density ==== of the jump–diffusion ====.","Diffusion phenomena arise from a Markovian stochastic modeling and as a solution of SDEs with or without jumps in many areas of applied mathematics. Their investigation concerns different mathematical branches and therefore research interest in questions such as existence and regularity of solutions of stochastic differential equations has constantly grown over the past years.====The study of the statistical properties of diffusion models has emerged since such models are widely used for applications in finance and biology. Diffusion processes with jumps, in particular, have been used in neuroscience for instance in Ditlevsen and Greenwood (2013) while in finance they have been introduced to model the dynamic of asset prices (Kou, 2002, Merton, 1976), exchange rates (Bates, 1996), or volatility processes (Barndorff-Nielsen and Shephard, 2001).====In this work, we aim at estimating adaptively the invariant density ==== associated to the process ====, solution of the following multivariate stochastic differential equation with Levy-type jumps: ====where ==== is a ====-dimensional Brownian motion and ==== a compensated Poisson random measure with a possible infinite jump activity. We assume that a continuous record of observations ==== is available.====Practical concerns raise new questions such as the dependence of statistical features on the observation scheme: it is, for the applications, a subject of interest to consider basic questions in different observation scenarios. From a theoretical point of view, it is however also of substantial interest to work under the assumption that a continuous record of the diffusion considered is available.====In this framework, it belongs to the folklore of the statistics for stochastic processes without jumps that the invariant density can be estimated under standard nonparametric assumptions with a parametric rate (cfr Chapter 4.2 in Kutoyants, 2013). The proof relies on the existence of diffusion local time and its properties and so such a result is restricted to the one-dimensional setting.====In the mono-dimensional case Schmisser considers, in Schmisser (2013), a strictly stationary and ==== mixing process ==== observed at discrete times ====. The author estimates the successive derivatives ==== of the stationary density either on a compact set or on ==== thanks to a penalized least square method. If the derivative ==== belongs to the Besov space ====, then the ==== risk of the estimator converges with rate ====, and the procedure does not require the knowledge of ====. When ====, the invariant density associated to the process ==== is estimated with a convergence rate which is the same found by Comte and Merlevède in Comte and Merlevède (2005) and Comte and Merlevède (2002).====Regarding the literature on statistical properties of multidimensional diffusion processes in the continuous case, an important reference is given by Dalalyan and Reiss in Dalalyan and Reiss (2007), where they show an asymptotic statistical equivalence for inference on the drift in the multidimensional diffusion case. As a by-product of the study they prove, under isotropic Hölder smoothness constraints, convergence rates of invariant density estimators for pointwise estimation which are faster than those known from standard multivariate density estimation. Their result relies on upper bounds on the variance of additive diffusion functionals, proven by an application of the spectral gap inequality in combination with a bound on the transition density of the process.====Still in the continuous case, in a recent paper, Strauch (2018) has extended their work by building adaptive estimators in the multidimensional diffusion case which achieve fast rates of convergence over anisotropic Hölder balls.====The notion of anisotropy plays an important role. Indeed, the smoothness properties of elements of a function space may depend on the chosen direction of ====.====The Russian school considered anisotropic spaces from the beginning of the theory of function spaces in 1950–1960s (in Nikolskii, 1975 the author takes account of the developments). However, results on minimax rates of convergence in classical statistical models were rare for a lot of time.====The question of optimal bandwidth selection based on i.i.d. observations for density estimation with respect to sup-norm risk was not completely solved until the pretty recent developments gathered in Lepski (2013). The methodology detailed in Goldenshluger and Lepski (2011) inspired the data-driven selection procedure of the bandwidth of the kernel estimator proposed by many authors such as Strauch in Strauch (2018) and Comte, Prieur and Samson in Comte et al. (2017) and provides the starting point for the study of our adaptive procedure as well.====In presence of jumps, we are only aware of a few works which takes place in the non parametric framework. For example, Schmisser investigates in Schmisser (2019) the non parametric adaptive estimation of the coefficients of a jumps–diffusion process and in Funke and Schmisser (2018), together with Funke, the non parametric adaptive estimation of the drift of an integrated jump–diffusion process.====In this paper, we provide a non-parametric estimator of the invariant density ==== with a fully data-driven procedure of the bandwidth. We propose to estimate the invariant density ==== by means of a kernel estimator, we therefore introduce some kernel function ====. A natural estimator of ==== at ==== in the anisotropic context is given by ====where ==== is a multi-index bandwidth, which will be chosen through the data-driven selection procedure. We first prove some bounds on the transition semigroup and on the transition density that will be useful to find sharp upper bounds on the variance of integral functionals of the diffusion ====. Through them, we find the following convergence rates for the pointwise estimation of the invariant density of our diffusion with jumps: ====where ==== is the degree of jumps activity of the Lévy process and ==== is the harmonic mean smoothness of the invariant density over the ==== different dimensions.====We remark that the rate we find for ==== is the same Strauch found in Strauch (2018) in absence of jumps, which is also the rate gathered in Dalalyan and Reiss (2007) up to replacing the mean smoothness with ====, the common smoothness over the ==== dimensions.====The case ==== evidences the main difference between what happens with and without jumps. Indeed, if in the continuous case the optimal convergence rate was ====, now the rate we found is between ==== and ====. It is worth noting here that such a convergence rate is not necessarily the optimal one in the jumps framework. As a matter of fact in the continuous case different approaches, as the diffusion local time, have been used to get the rate ====; we do not exclude the possibility that also in presence of jumps the implementation of other methods could lead to a convergence rate faster than the one presented here above for the mono-dimensional setting.====To complete the comparison to the continuous framework, we recall that in both Dalalyan and Reiss (2007) and Strauch (2018) the convergence rate found in the case ==== was ==== and so the convergence of the estimator seems being faster in presence of jumps than without them. The reason why it happens is that, to find the convergence rate, the transition density ==== is needed to be upper bounded. If in Dalalyan and Reiss (2007) the authors assume to have ==== and in Strauch (2018) Nash and Poincaré inequalities lead Strauch to a bound analogous to the one presented in Dalalyan and Reiss (2007); Lemma 1 below provides us a different bound which guides us to a different rate. However, in absence of the term ==== in the assumption before, which is the case for example considering a bounded drift, also in the continuous setting the convergence rate turns out being, as in the jump–diffusion case, equal to ====.====It is moreover worth noting here that, if in Dalalyan and Reiss (2007) and Strauch (2018) they needed to assume the existence of the transition density and a bound on it, we derive them through Lemma 1: all the assumptions we need are directly on the model (1).====We no longer need to assume that the drift is of the form ==== (where ==== is referred to as potential) as it was in both Dalalyan and Reiss (2007) and Strauch (2018).====After having provided the rates of convergence of the estimators we finally propose, in the case ====, a fully data-driven selection procedure of the bandwidth of the kernel estimator, inspired by the methodology detailed in Goldenshluger and Lepski (2011). The method has the decisive advantage of being anisotropic: the bandwidths selected in each direction are in general different, which is coherent with the possibly different regularities with respect to each variable. Finally, we prove that for the selected optimal bandwidth the following estimation holds: ====where we have denoted as ==== the ==== norm on ====, a compact subset of ==== and as ==== the set of candidate bandwidths; ==== is a bias term and ==== an estimate of the variance bound. We remark that the estimator leads to an automatic trade off between the bias and the variance: the second term on the right hand side of (2) is indeed negligible compared to the first one.====Moreover, as the rate optimal choice ==== belongs to the set of candidate bandwidths ====, (2) turns out being ====where ==== is the mean smoothness of the invariant density.====The paper is organized as follows. We give in Section 2 the assumptions on the process ====. In Section 3 we define the anisotropic Hölder balls and we construct our estimator. Section 4 is devoted to the statements of our main results; which will be proven in the two following sections. In particular, we show how we get the convergence rates for the invariant density estimation in Section 5 while in Section 6 we prove the estimator we find through our bandwidth selection procedure achieves the previously introduced convergence rate. Some technical results are moreover presented in Appendix.",Invariant density adaptive estimation for ergodic jump–diffusion processes over anisotropic classes,https://www.sciencedirect.com/science/article/pii/S037837582030121X,27 November 2020,2020,Research Article,128.0
"Kim Kun Ho,Chao Shih-Kang,Härdle Wolfgang K.","Katz School, Yeshiva University, New York, NY 10016, USA,Department of Statistics, University of Missouri, MO 65211, USA,Ladislaus von Bortkiewicz Chair of Statistics, C.A.S.E. - Center for applied Statistics and Economics, Humboldt-Universität zu Berlin, Unter den Linden 6, 10099 Berlin, Germany,Sim Kee Boon Institute for Financial Economics, Singapore Management University, 50 Stamford Road, Singapore 178899, Singapore","Received 20 November 2019, Revised 6 May 2020, Accepted 12 October 2020, Available online 21 November 2020, Version of Record 4 December 2020.",https://doi.org/10.1016/j.jspi.2020.10.007,Cited by (1),"In this paper, we conduct simultaneous inference of the non-parametric part of a partially linear model when the non-parametric component is a multivariate unknown function. Based on semi-parametric estimates of the model, we construct a simultaneous confidence region of the multivariate function for simultaneous inference. The developed methodology is applied to perform simultaneous inference for the U.S. gasoline demand where the income and price variables are contaminated by Berkson errors. The empirical results strongly suggest that the linearity of the ==== gasoline demand is rejected. The results are also used to propose an alternative form for the demand.","Partially linear models are welcome compromise between a pure nonparametric and a sometimes too restricted parametric specification. The semi-parametric structure not only makes it possible to include discrete predictors, but also to estimate part of the model with high precision. These and other reasons have made this model class very successful, e.g. Härdle et al. (2000). A typical assumption in the existing partially linear model literature is that the non-parametric part is univariate. In several applications though, one has a data structure described by: ====where ==== is a scalar random variable, ==== is a ==== random vector, and ==== is a ==== random vector for ====, respectively. In addition, ==== is a mean zero IID random error that is uncorrelated with ==== and ====. Here ==== and ==== are a ==== vector of unknown parameters and an unknown smooth function, respectively. Inference of the unknown function ==== can be conducted even when the covariate terms on the RHS of (1.1) are not fully observed, as illustrated in Section 3. The model (1.1) is widely used due to its flexibility to combine the parametric linear part ==== and the non-parametric non-linear component ====. See Härdle et al. (2000) for more on the partially linear model framework.====The primary contribution of this paper is to introduce a methodology for simultaneous inference of the ==== function ==== in (1.1) when ====. The majority of the literature on (1.1) and on its variants has focused on simultaneous inference for a “univariate” function (i.e. ====): Johnston (1982) conducts simultaneous inference for an univariate mean regression function. Härdle (1989) derives simultaneous confidence bands (SCB) for one-dimensional kernel M-estimators. Fan and Zhang (2000) and Zhou and Wu (2010) show how to perform simultaneous inference of linear models with varying coefficients. Wu and Zhao (2007) and Kim (2016) work on inference of univariate time trend in mean regression. Zhao and Wu (2008) and Liu and Wu (2010) conduct simultaneous inference of the univariate mean and univariate volatility functions of a discretized version of the stochastic diffusion model. Moreover, Härdle and Song (2010) and Guo and Härdle (2012) construct uniform confidence bands for conditional quantile and expectile functions, respectively. Song et al. (2012) employ bootstrap procedures for local constant quantile estimators to overcome the slow convergence of asymptotic confidence bands. Although they contribute to the literature, all of the mentioned papers deal with the univariate function case. Chao et al. (2017) extend it to the case of multivariate quantile regression functions. However, to the best of our knowledge, no paper has worked on simultaneous inference of ==== with ==== in the partially linear model (1.1). This paper attempts to undertake the task. The main results that we obtain in this paper have some resemblance to those in Chao et al. (2017). However, due to the partially linear structure in (1.1), the main assumptions of this paper are based on the conditional distribution of the filtered response ==== on ====, rather than on the conditional distribution of ==== on all the covariate terms, which is technically more challenging to handle than in Chao et al. (2017).====Simultaneous inference of (1.1) is conducted through the construction of simultaneous confidence region (====). Consider testing the following hypotheses for (1.1): ====where ==== is a multivariate “parametric” function suggested by related economic theory. To test the hypothesis in (1.2), we construct the ==== of ==== and observe whether the ==== contains the parametric specification “entirely”. The construction of the ==== with confidence level ====, requires us to find two functions ==== and ==== based on data, such that: ====where ==== is a ==== vector in a compact set ====. Given the ==== of ====, one can test (1.2) by checking whether or not the condition ==== holds for ==== ====. If the condition does not hold for ==== ====, then we ==== the null hypothesis at level ====. That is, even if the condition holds for all ==== except for only one, the null hypothesis still gets rejected by the test.====The relative advantage of the ====-based inference over other standard inferential procedures utilizing some integrated-squared-difference type statistic and its associated ====-value such as those in Härdle and Mammen (1993) is in its effectiveness in suggesting the right function form of ==== in (1.1). In case the null hypothesis in (1.2) is somehow rejected, it would be rather difficult to figure out the reason for rejection if the inference is based on such standard test statistic and its associated ====-value. Thus, it would not be straightforward to suggest an alternative to the parametric function under the null hypothesis in such as case. However, if the inference is based on the proposed ====, one can easily figure out the reason for the rejection by “locating graphically” where the ==== is violated by the parametric null ====. Hence it would be relatively straightforward to propose an alternative parametric form. For more on this, see the last paragraph of Section 3.3.====The proposed methodology is applied to perform simultaneous inference of the U.S. gasoline demand. The gasoline demand is of interest to many, including policy makers, due to its environmental consequences and the role as an economic indicator. With little guidance on the form of demand function provided by economic theory (Blundell et al., 2012), however, we refer to the semi-parametric demand structure (Schmalensee and Stoker, 1999, Yatchew and No, 2001, Blundell et al., 2012), which is a specification of the general one provided by (1.1). A popular candidate for the function ==== in (1.1) is the ==== structure. The linear structure is widely used because it provides a simple but useful analytical framework and because the coefficients under the framework represent important structural parameters such as income/price elasticities. Despite these advantages, any parametric forms including the linear one are essentially arbitrary and may be misspecified in ways that produce seriously erroneous results. Hence we propose to test its validity as ==== in (1.1) through the simultaneous inference proposed in this work.====The organization of the paper is the following: Section 2 introduces the methodology proposed to perform simultaneous inference of the partially linear model (1.1) with ====. We estimate the partially linear model and carry out the construction of simultaneous confidence region (====) based on the estimate. Both the asymptotic-based and simulation-based constructions of ==== are introduced. Section 3 handles an application of the proposed methodology. We estimate and perform simultaneous inference of a semi-parametric and partially linear U.S. gasoline demand under the Berkson errors. The data are explained and the empirical results are discussed in detail as well. Section 4 concludes the paper and discusses related future research. The mathematical proofs regarding the simultaneous inference for a multivariate function are relegated to the Appendix, provided as online Supplementary Material.==== For any vector ====, we let ====. For any random vector ====, we write ====
 (====) if ====. In particular, ====. In addition, we write ==== if ==== is bounded away from ==== and ==== for all large ====. For brevity, we sometimes write ==== for ====.",Simultaneous inference of the partially linear model with a multivariate unknown function,https://www.sciencedirect.com/science/article/pii/S0378375820301208,21 November 2020,2020,Research Article,129.0
"Zeng JingJing,Duan Yuze,Wang Desheng,Zou Bin,Yin Yue,Xu Jie","Faculty of Mathematics and Statistics, Hubei Key Laboratory of Applied Mathematics, Hubei University, Wuhan 430062, China,Wuhan Huaxia University of Technology, Wuhan 430223, China,School of Electronic Information and Communication, Huazhong University of Science and Technology, Wuhan 430074, China,Faculty of Computer Science and Information Engineering, Hubei University, Wuhan 430062, China","Received 16 September 2019, Revised 11 May 2020, Accepted 19 September 2020, Available online 21 November 2020, Version of Record 19 February 2021.",https://doi.org/10.1016/j.jspi.2020.09.001,Cited by (3),"MS) and show the learning performance of LSVM====MS for UCI datasets. The experimental results show that the LSVM====MS can improve obviously the learning performance of the classical LSVM algorithm. If the sampling and training total time is a main concern, the LSVM====MS algorithm is the preferred method compared the known SVM algorithm based on Markov sampling.","Support Vector Machine (SVM) is a very effective and convenient algorithm for data classification, see Vapnik (1998). It not only has good practical application performance, but also has good theoretical properties in consistency (see Zhang, 2004, Steinwart, 2005 and Steinwart and Christmann (2008)) and learning rate (see Steinwart and Christmann (2008) and Chen et al. (2004)). Mangasarian and Musicant (2001) introduced a new algorithm named Lagrangian Support Vector Machine (LSVM) by two simple changes on the standard SVM algorithm introduced by Vapnik (1998). The first one is that the margin or distance between the parallel bounding planes was maximized with respect to both orientation and location relative to the origin. The second one is that the error in the soft margin was minimized using the ====-norm squared instead of the conventional ====-norm. After the two changes above, Mangasarian and Musicant (2001) proposed a fast simple algorithm based on implicit Lagrangian formulation of the duel of a simple reformulation of the standard quadratic program of a linear SVM. This results in the minimization of an unconstrained differentiable convex function. Mangasarian and Musicant (2001) asserted that the introduced LSVM can easier to code than ==== and can classify the datasets with millions of points even by using MATLAB code. The main reason is that successive overrelaxation (SOR) for symmetric linear complementarity problems and quadratic programs were introduced to train the LSVM (see Algorithm III.1 in Mangasarian and Musicant (1999)) and the SOR method has a linear convergence rate (see Theorem III.3 in Mangasarian and Musicant (1999)). In addition, Mangasarian and Musicant (2001) proved that the LSVM algorithm frequently has the same solution as the standard SVM algorithm. Later, Fung and Mangasarian (2003) studied finite Newton method for LSVM algorithm. Yang et al. (2004) presented an extended LSVM (ELSVM) algorithm and proved the convergence of the ELSVM algorithm. Hwang et al. (2013) studied multi-classification problem based on LSVM algorithm. Wang et al. (2016) introduced an online learning LSVM (OLLSVM) algorithm to detect anomaly and obtained better experimental results. But there is no theoretical analysis for the generalization bounds of LSVM up to now.====One of main results of this paper is to establish the generalization bounds of the LSVM introduced by Mangasarian and Musicant (2001). We firstly establish the generalization bound of the LSVM for uniformly ergodic Markov chain (u.e.M.c.) samples. As the application of the established generalization bound for LSVM with u.e.M.c. samples, we also obtain the generalization bounds of the LSVM for strongly mixing sequence, independent and identically distributed (i.i.d.) samples, respectively. In addition, inspired by the idea from Xu et al. (2015), we also propose a new LSVM algorithm based on Markov sampling to improve the learning performance of the LSVM introduced in Mangasarian and Musicant (2001). Now, we highlight some features of this paper.==== The generalization bounds of LSVC algorithm are established.==== Experiments show that LSVM====MS has better performance than the known methods.====The main structure of our paper is as follows: We first introduce some definitions and notations in Section 2. Section 3 gives the main theoretical results of LSVM with u.e.M.c., strongly mixing sequence and i.i.d. samples. In Section 4, we propose the LSVM====MS. In Section 5, we show the experimental results of LSVM====MS and compare it with the classical LSVM and the SVM====MS, respectively. This paper is summarized in Section 6.",Generalization performance of Lagrangian support vector machine based on Markov sampling,https://www.sciencedirect.com/science/article/pii/S0378375820300987,21 November 2020,2020,Research Article,130.0
"Lu Xiao-Nan,Mishima Miwako,Miyamoto Nobuko,Jimbo Masakazu","Department of Computer Science and Engineering, University of Yamanashi, 4-4-37 Takeda, Kofu, Yamanashi 400-8510, Japan,Department of Electrical, Electronic and Computer Engineering, Gifu University, 1-1 Yanagido, Gifu, Gifu 501-1193, Japan,Department of Information Sciences, Tokyo University of Science, 2641 Yamazaki, Noda, Chiba 278-8510, Japan,Department of Contemporary Education, Chubu University, 1200 Matsumoto-cho, Kasugai, Aichi 487-8501, Japan","Received 22 January 2020, Revised 9 November 2020, Accepted 12 November 2020, Available online 21 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.005,Cited by (0),"In this paper, we investigate a class of optimal circulant ====. We show that for ==== the largest possible value of ==== for statistically optimal CAOA==== cannot exceed ====. We also clarify that CAOA==== with high D-efficiency and ==== greater than ","Functional magnetic resonance imaging (fMRI) is a way to study neural correlates of consciousness involving perception, memory, learning, thinking, and affection by measuring hemodynamic response to mental stimuli. In an fMRI experiment, the experimental subject is asked to participate in mental tasks in response to the stimuli, while the subject’s brain is scanned by a magnetic resonance (MR) scanner at regular time intervals to collect observation data. For event-related (ER) fMRI experiments, mental stimuli as brief as several milliseconds can be detected by high-speed MR scanners so that transient brain activity can be explored. Each observation at a constant time interval is supposed to be affected by not only the current stimulus but also the preceding stimuli. In that sense, observations from an ER-fMRI experiment may be viewed as time series data and an experiment is designed based on a multiple linear regression model to estimate hemodynamic response functions (HRFs) for plural types of stimuli. In an ER-fMRI experiment, the number of stimuli in a brief duration can be huge and this gives rise to challenging research problems for statistical designs. In this paper, we focus on designs of ER-fMRI experiments (fMRI experiments for short). We refer the reader to Lazar (2008) for an overview of statistical methods in fMRI experiments, Kao and Stufken (2015) for a review of some previous works on designs for fMRI experiments, and Cheng and Kao, 2015, Cheng et al., 2017 for a general theory of guiding the selection of fMRI designs to estimate the HRF. A detailed description of experimental settings and the statistical model for an fMRI experiment with a single type of stimulus is given in the next section.====M-sequences can provide fMRI designs (see Buračas and Boynton, 2002). In the binary case, such designs are shown to be optimal under certain optimality criterion in estimating the HRF (see Kao, 2014). However, for such fMRI designs, only few stimulus effects can be estimated. Lin et al. (2017b) tried to find fMRI designs overcoming this problem by introducing the concept of a circulant almost orthogonal array (CAOA) as a comprehensive framework of fMRI designs.====In an fMRI experiment, the parameter ==== of a CAOA is considered as the number of time points taken into account to estimate HRFs for the respective types of stimuli. In this sense, ==== is ideal to be at least the duration of an HRF from the onset of a stimulus to the HRF’s complete return to baseline. However, even for ====, the maximum values of ==== for ‘optimal’ CAOAs with small ==== have not been determined for all ====, and the values of ==== for previously known CAOA==== with ==== are bounded above by ====. That is why our purpose is to find ‘efficient’ CAOAs with larger ==== as well as ‘optimal’ CAOAs.====As a general theory of selecting ‘optimal’ CAOA====, we can refer to Cheng and Kao (2015) for ====, and Lin et al. (2017b) for ====. In Lin et al. (2017b), we can also find, for ==== and ====, the generating vectors of optimal CAOA==== and the values of ==== searched by computers as well as the previously known values of ====. For ====, Lin et al. (2017b) classified CAOA==== into three classes called ====, ====, and ====, where ====-CAOAs are ‘optimal’. Through computer search, Lin et al. (2017b) also found examples of ‘optimal’ CAOA==== other than ====-CAOAs, but they did not go deep into it. In this paper, restricting to the case ====, we characterize the class of CAOA==== of which Lin et al. (2017b) did not make a deeper investigation.====In the next section, we review the statistical model for estimating HRFs in an fMRI experiment with a single type of stimulus, and a class of optimality criteria, called “type 1”, for selection of designs. In Section 3, we propose the notion ====-CAOA==== as a subclass of ====-CAOAs and show their optimality under type 1 criteria. In Section 4, we discuss the asymptotic optimality of CAOA====. In Section 5, we demonstrate that algebraic constructions for perfect binary sequences can be applied to get efficient CAOA==== with large ====. Concluding remarks are given in the last section together with observations by computer search and some further problems.",Optimal and efficient designs for fMRI experiments via two-level circulant almost orthogonal arrays,https://www.sciencedirect.com/science/article/pii/S0378375820301191,21 November 2020,2020,Research Article,131.0
"Soale Abdul-Nasah,Dong Yuexiao","Department of Statistical Science, Temple University, Philadelphia, PA, 19122, United States of America","Received 18 November 2019, Revised 23 June 2020, Accepted 11 November 2020, Available online 20 November 2020, Version of Record 3 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.004,Cited by (2), and an analysis of the Big Mac data.,"Since its inception about three decades ago, sufficient dimension reduction (Li, 1991, Cook, 1998a) has become a very important tool for modern multivariate analysis. For predictor ==== and response ====, the goal of sufficient dimension reduction is to find ==== with ==== such that ====where ====
 means statistical independence. The column space of ==== satisfying (1) is known as a dimension reduction space. Under mild conditions, Yin et al. (2008) showed that the intersection of all dimension reduction spaces is still a dimension reduction space, and it is referred to as the central space for the regression ==== on ====. We denote the central space by ====. The dimension of the central space is known as the structural dimension.====There are many sufficient dimension reduction methods in the literature. Moment-based estimators include sliced inverse regression (SIR) (Li, 1991), sliced average variance estimation (SAVE) (Cook and Weisberg, 1991), principal Hessian directions (Li, 1992, Cook, 1998b), sliced average third-moment estimation (Yin and Cook, 2003), and SIR-====
 (Saracco, 2005). Semiparametric estimators include minimum average variance estimation (MAVE) (Xia et al., 2002), and semiparametric dimension reduction (Ma and Zhu, 2012, Luo et al., 2014). Sparse dimension reduction estimators include sparse SIR (Li, 2007, Tan et al., 2020), sparse MAVE (Wang and Yin, 2008), coordinate-independent sparse estimation (Chen et al., 2010), and sparse semiparametric estimation (Yu et al., 2013). Other sufficient dimension reduction methods include ensemble sufficient dimension reduction (Yin and Li, 2011), nonlinear sufficient dimension reduction (Li et al., 2011, Lee et al., 2013), groupwise sufficient dimension reduction (Li et al., 2010, Guo et al., 2015), post dimension reduction inference (Kim et al., 2020), and online sufficient dimension reduction (Chavent et al., 2014, Cai et al., 2020). For general reviews, one can refer to Cook (2007), Ma and Zhu (2013), and Dong (2021). An excellent reference is the recent book by Li (2018).====Due to their ease of implementation, SIR and SAVE are two of the most popular sufficient dimension reduction methods. One well-known limitation of SIR and SAVE is that they are not very efficient in the presence of heteroscedasticity. Quantile-based methods are proposed by Wang et al. (2018) and Kim et al. (2019) to address this limitation, and their proposals work better than SIR or SAVE with heteroscedastic error. However, another well-known limitation of SIR and SAVE is that they may be sensitive to specific link functions between the response and the predictor. In particular, SIR does not work well when the link function is symmetric, and SAVE is not efficient with monotone link functions. Since the quantile-based methods are extensions of SIR and SAVE, they inherit the limitation of their moment-based counterparts and may still have uneven performances with various link functions.====We propose expectile-assisted inverse regression in this paper. Our contribution is two-fold. First, we provide a general framework to extend moment-based dimension reduction methods to their expectile-based counterparts, such as expectile-assisted SIR, expectile-assisted SAVE, and expectile-assisted directional regression. Similar to the quantile-based methods, our expectile-based proposals utilize the information across different levels of the conditional distribution of ==== given ====, and perform better than the corresponding moment-based methods in the presence of heteroscedasticity. Since directional regression (Li and Wang, 2007) is known to perform well for a wide range of link functions, the expectile-assisted directional regression enjoys the additional benefit that it is no longer sensitive to the specific forms of the unknown link functions. Furthermore, to combine the information across different quantile levels, existing quantile-based methods such as quantile-slicing mean estimation and quantile-slicing variance estimation (Kim et al., 2019) rely on intricate weights, and it is not clear how the choice of different weights may affect the final estimation. We propose to combine the information across different expectile levels through random projection, which has roots in the projected resampling approach for multiple response sufficient dimension reduction (Li et al., 2008). Our proposed expectile-assisted estimators outperform existing methods in both simulation studies and a real data analysis.====The rest of the paper is organized as follows. In Sections 2 Population level development of expectile-assisted SIR, 3 Sample level algorithm of expectile-assisted SIR, we provide the population level and the sample level development of expectile-assisted SIR, respectively. Further extensions to expectile-assisted SAVE and expectile-assisted directional regression are described in Section 4. Some practical issues such as tuning parameter selection are discussed in Section 5. Extensive simulation studies are provided in Section 6 and we conclude the paper with a real data analysis in Section 7. All proofs and additional simulation results are relegated to Appendix A Proofs, Appendix B Additional simulation results, Appendix C Order determination.",On expectile-assisted inverse regression estimation for sufficient dimension reduction,https://www.sciencedirect.com/science/article/pii/S037837582030118X,20 November 2020,2020,Research Article,132.0
"Cai Tony,Kim Donggyu,Song Xinyu,Wang Yazhen","Department of Statistics, The Wharton School, University of Pennsylvania, United States of America,College of Business, Korea Advanced Institute of Science and Technology, Republic of Korea,School of Statistics and Management, Shanghai University of Finance and Economics, China,Department of Statistics, University of Wisconsin-Madison, United States of America","Received 4 November 2019, Revised 21 September 2020, Accepted 1 November 2020, Available online 17 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.002,Cited by (2), is carried out to investigate the finite sample performance of the proposed estimators.,"In quantum science and quantum technology, we often need to learn and engineer quantum systems. A prominent example is quantum information science (Nielsen and Chuang, 2010, Wang, 2011, Wang, 2012, Wang and Song, 2020). A quantum system is described by its state, therefore for its study, we need to reconstruct the quantum state. In the literature, researchers often characterize the quantum state by a complex matrix that is the so-called density matrix and refer to the reconstruction of the quantum state by quantum state tomography. Traditionally, quantum state tomography employs classical statistical models and methods to deduce quantum states from quantum measurements that are observations obtained from measuring identically prepared quantum systems. Because of the exponential complexity of the quantum system and the exponential growth of its corresponding density matrix, quantum state tomography often needs to reconstruct the density matrix of high-dimension. It is known that classical statistical methods are neither efficient nor effective in recovering the large density matrix. In this paper, we employ modern high-dimensional statistics to investigate the reconstruction of large density matrix.====Cai et al. (2016) studied the estimation of large sparse density matrix represented by Pauli matrices and established the optimal convergence rate of its estimator. However, the sparsity condition assumed in Cai et al. (2016) may not be very reasonable. For example, a low-rank density matrix does not satisfy the condition under the Pauli representation. Koltchinskii and Xia (2015) studied the reconstruction of low-rank density matrix and investigated its optimal estimation. We note that the estimation of eigenspace also plays an important role in the reconstruction of the low-rank density matrix, and modern high-dimensional statistics suggest that the optimal estimation of the eigenspace depends on the sparse structure of the eigenvectors. See Birgé, 2001, Cai et al., 2013, Cai et al., 2015, Johnstone and Lu, 2009, Ma, 2013 and Vu and Lei (2013) for related research works. Koltchinskii and Xia (2015) considered a broad class of low-rank density matrices that may not be suitable for density matrices with sparse eigenvectors, and as a result, their optimal rate is not sharp for density matrices with sparse eigenvectors.====This paper considers the eigenspace estimation problem for a quantum spin system based on Pauli measurements. As all Pauli matrices have ==== eigenvalues, Pauli measurements also take binary values ==== and ==== while their distributions correspond to shifted and rescaled binomial distributions (Cai et al., 2016, Wang, 2013). Thus, the eigenspace estimation problem studied in this paper is a high-dimensional statistics problem with binomial distributions, where both the matrix size and sample size are allowed to go infinity. To be specific, we analyze the asymptotic behaviors of the principal component analysis (PCA) estimators and establish their convergence rates under both dense and sparse eigenvector settings. Under the sparse eigenvector condition, we derive the minimax lower bound for the eigenspace estimation procedure and demonstrate that the iterative thresholding sparse PCA (ITSPCA) proposed by Ma (2013) can achieve the minimax lower bound, and therefore the ITSPCA is rate-optimal. The convergence rates and minimax lower bound in this paper are obtained by asymptotic analysis with binomial distributions instead of usual normal distributions. With the ITSPCA eigenspace estimator, we can estimate the corresponding eigenvalues and reconstruct the large density matrix. We show that the constructed low-rank density matrix is also rate-optimal.====The rest of this paper proceeds as follows: Section 2 briefly reviews the quantum state and the density matrix that is represented through Pauli matrices with its estimation. Section 3 describes the iterative thresholding estimation algorithm and defines a sparsity condition for eigenvectors. Given the sparsity condition, Section 4 establishes the asymptotic theory for the iterative thresholding estimator and derives the minimax lower bound for eigenspace estimation under spectral and Frobenius norms, where both the matrix size and sample size are allowed to go to infinity. Section 5 proposes the eigenvalue and low-rank density matrix estimators and derives their convergence rates. Section 6 features numerical studies to illustrate the finite sample performances of the proposed estimators. All proofs are collected in Section 7 and further technical details are presented in the Appendix.",Optimal sparse eigenspace and low-rank density matrix estimation for quantum systems,https://www.sciencedirect.com/science/article/pii/S0378375820301166,17 November 2020,2020,Research Article,133.0
"Móri Tamás F.,Székely Gábor J.,Rizzo Maria L.","Rényi Institute of Mathematics, Budapest, Hungary,National Science Foundation, Alexandria, VA, USA,Department of Mathematics and Statistics, Bowling Green State University, Bowling Green, OH 43403, USA","Received 28 June 2019, Revised 29 October 2020, Accepted 1 November 2020, Available online 17 November 2020, Version of Record 27 November 2020.",https://doi.org/10.1016/j.jspi.2020.11.001,Cited by (7),"The energy test of multivariate normality is an affine invariant test based on a characterization of equal distributions by energy distance. The test ==== of normality and solve them by a variation of Nyström’s method. For the simple hypothesis, we also obtain the eigenvalues by an empirical approach which we call the sample ","Testing for normality is a classical statistical problem that sparked the interest of Fisher (1930) and Pearson (1930). Some of the well-known tests for univariate normality are Anderson–Darling (Anderson and Darling, 1952), Shapiro–Wilk (Shapiro and Wilk, 1965), Shapiro–Francia (Shapiro and Francia, 1972), and Kolmogorov–Smirnov. The third and fourth standardized moments (skewness and kurtosis) were applied by Mardia and others as some of the earliest tests of multivariate normality; see, for example, Baringhaus and Henze, 1992, Malkovich and Afifi, 1973 and Mardia, 1970, Mardia, 1974. Other tests include Bowman and Foster (1993) and Sarkadi and Tusnády (1977). More recent tests of normality are based on the entropy maximum property of normal distributions (Vasicek, 1976). Tests based on properties of the empirical characteristic function include Epps and Pulley (1983), Csörgő (1986) and Henze and Zirkler (1990), the BHEP test (Baringhaus and Henze, 1988, Henze and Wagner, 1997) and the energy test (Szekely and Rizzo, 2005). Testing normality and especially multivariate normality continues to attract the interest of researchers. See, for example, recent work of Henze and Visagie (2019), Tenreiro (2017), Thas and Ottoy (2003), or Zhu et al. (1995). For application to feature screening in ultrahigh dimension see Xue and Liang (2017). For a review of several tests of normality see Henze (2002) or Das and Imon (2016).====While several types of tests of normality and multivariate normality are available, it is useful to filter them somewhat by understanding some key properties. Desirable properties of a test of normality include the following.====Some of the tests in the literature mentioned above are not consistent against all alternatives. For example, Mardia’s skewness tests are not consistent against symmetric non-normal alternatives, and Mardia’s kurtosis test of ====-variate normality is consistent if and only if the kurtosis of the sampled population is not ==== (Baringhaus and Henze, 1992, Henze, 1994). Some other tests fail to be affine invariant (see e.g. Csörgő, 1986) because standardization of the sample does not automatically make tests affine invariant. The norm of the standardized sample, however, is always affine invariant, so tests based on distances, like the energy tests, have this property. The affine-invariant energy goodness-of-fit test (Szekely and Rizzo, 2005) is consistent and practical to apply for arbitrary dimension, and the energy test for univariate and multivariate normality is a powerful competitor of the Henze–Zirkler (HZ) and BHEP (Baringhaus and Henze, 1988, Henze and Wagner, 1997, Henze and Zirkler, 1990) tests. The HZ and BHEP tests for normality are affine invariant, and also consistent against all alternatives. The energy test, which is the subject of this paper, applies a different kernel than BHEP. Both tests are affine invariant, consistent tests for dimension ====.====In this paper we will focus on the energy test of normality and multivariate normality, with the aim of computing the parameters of the asymptotic distribution of the test statistics. As energy statistics are degenerate kernel V-statistics with first order degeneracy, the form of the null limit distribution of the test statistic is ====a quadratic form of independent and identically distributed (iid) standard normal variables, where the eigenvalues and associated eigenvectors are solutions to the corresponding integral equation. Here the exact form of the integral equation is derived for four cases, according to whether the population parameters are known or estimated. After solving the integral equations for the eigenvalues, one can obtain the probability distribution and compute p-values for new test implementations. New alternative test implementations based on applying (1), and computational methods to evaluate or estimate the eigenvalues are discussed, validated, and benchmarked.====In Section 2 the energy goodness-of-fit test for the simple and composite hypotheses of normality and multivariate normality, the null limit distribution, and five methods of implementing the test are summarized. Section 3 contains detailed derivations of the exact eigenvalue equations derived from the Schrödinger equation for eigenvalues. An alternate derivation of the Hilbert–Schmidt eigenvalue equation follows in Section 4. Section 5 covers methods of solving the eigenvalue equations. Section 6 contains solutions and estimates of eigenvalues for four types of hypotheses.====Many interesting comparisons of the power of the energy test exist in the literature, including the work where the test was introduced (Rizzo, 2002, Szekely and Rizzo, 2005), so our empirical results in Section 6 are not on power, but on estimation and computation of the eigenvalues that determine the limit distribution. For recent power comparisons including energy, BHEP and other tests of multivariate normality, interested readers may review e.g. Joenssen and Vogel (2014) or Tenreiro (2011). Related work on a similar problem for the BHEP test was published by Baringhaus (1996); he derived the sequences of eigenvalues analytically for the BHEP test, Case 1 (known parameters) for dimensions ====.",On energy tests of normality,https://www.sciencedirect.com/science/article/pii/S0378375820301154,17 November 2020,2020,Research Article,134.0
"Banerjee Samprit,Monni Stefano","Division of Biostatistics, Weill Medical College of Cornell University, United States of America,Department of Mathematics, American University of Beirut, Lebanon","Received 27 November 2018, Revised 26 October 2020, Accepted 28 October 2020, Available online 16 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.jspi.2020.10.006,Cited by (0),", is larger than the sample size ","Many multivariate methods require an estimate of the covariance matrix. In this paper, we are interested in the problem of estimating the covariance matrix of a multivariate normal distribution, ====, using a sample of mutually independent draws ====, from it, when ==== is less than the dimension ==== of ====. This problem has received much attention in the recent past because of an increasing number of applications where measurements are collected on a large number of variables, often greater than the available experimental units. The sample covariance matrix is not a good estimator in this case. In the general framework where both ==== and ==== go to infinity in such a way that their ratio ==== converges to a positive finite constant (often referred to as the large-dimensional asymptotic regime), the sample covariance matrix, its eigenvalues and its eigenvectors cease to be consistent. Some alternative estimators have thus been proposed in the literature. Ledoit and Wolf (2015) propose estimators of the eigenvalues of the covariance matrix in the large-dimensional framework that are consistent, in the sense that the mean squared deviation between the estimated eigenvalues and the population eigenvalues converges to zero almost surely. Their method is based on a particular discretization of a version of the Marčenko–Pastur equation that links the limiting spectral distribution of the sample eigenvalues and that of the population eigenvalues (Ledoit and Wolf, 2017). This method is then used to derive estimators of the covariance matrix itself that are asymptotically optimal with respect to a given loss function in the space of orthogonally equivariant estimators (Ledoit and Wolf, 2018). Additional results by these same authors have very recently appeared when our paper was under review (Ledoit and Wolf, 2020). Estimators that are derived in the large-dimensional asymptotic regime are also proposed by El Karoui (2008), Mestre (2008) and  Yao et al. (2012), among others. Estimators that deal with the case ==== and are derived in a decision-theoretic framework are those of Konno (2009), and, more recently, Tsukuma (2016). There is a vast literature on estimation of ==== where structural assumptions on ==== are made such as ordering or sparsity, for example Bickel and Levina, 2008, Bien and Tibshirani, 2011, Naul and Taylor, 2017 and Won et al. (2013).====In this paper, we propose an estimator for the covariance matrix that is equivariant under orthogonal transformations. In particular, these transformations include rotations of the variable axes. Equivariant estimation of the covariance matrix under the orthogonal group has been studied extensively (e.g., Dey and Srinivasan, 1985, Ledoit and Wolf, 2015, Takemura, 1984) since the pioneering work of Stein, 1975, Stein, 1986. In this study, we follow our previous work (Banerjee et al., 2016), where we describe estimators that are valid when ====, and extend it to the case when ====. Because of the property of equivariance, the eigenvectors of the covariance matrix are estimated by those of ====, to which we refer as the sample covariance matrix in this paper. Thus, the estimation problem is completed by providing estimates of the eigenvalues. These estimates are obtained from an adjusted profile likelihood function of the population eigenvalues, which is derived by approximating the integral of the density function of ==== over its eigenvectors (corresponding to the non-zero eigenvalues). This approximation is however not the large-==== (Laplace) approximation of such integral, which results in the modified profile likelihood of Barndorff-Nielsen (1983), but it is an approximation suggested in Hikami and Brézin (2006) useful for large ====. Our estimates are a mixture ==== of an exact critical point ==== of such a likelihood function, which is in fact a maximum when some conditions are satisfied, and an approximate critical point ==== whose components are a modification of the non-zero sample eigenvalues by terms of order ====. The tuning parameter ==== is determined from the data and controls the shrinkage of the eigenvalues. We will describe two algorithms to determine ====: one based on bootstrap and one based on cross-validation. High-dimensional estimators are generally derived under an asymptotic regime in which both ==== and ==== increase in such a way that their ratio tends to a constant. In our case, ==== is kept fixed, and the high-dimensionality of the estimator comes into play because we consider a large-==== approximation of the marginal density of the eigenvalues of ====.====In a variety of finite-sample simulation scenarios, we compare our estimator to two Ledoit–Wolf estimators, which are also orthogonally equivariant and have previously been shown to better many other estimators under some loss functions. Fig. 3, Fig. 4 summarize the results of our comparison, in terms of risk evaluated with respect to nine loss functions. To assess how our covariance matrix estimator estimates the eigenvalues of the population covariance matrix, we also compare the eigenvalues of our estimator with Ledoit–Wolf consistent estimator of the population eigenvalues themselves, under loss functions that depend only on the eigenvalues. Fig. 5 displays such a comparison. Finally, comparison of covariance matrix estimators is also carried out using two real data applications of breast cancer data and leukemia data: in a linear discriminant analysis of these data, we use plugged-in estimates of the covariance matrix in the classifier and demonstrate that our estimator leads to lower misclassification errors in the breast cancer data and similar misclassification errors in the leukemia data. The two Ledoit–Wolf covariance matrix estimators are optimal asymptotically under two loss functions, but we show that finite sample improvements and improvements under other loss functions are indeed possible. Since the tuning parameter ==== of our estimator can be chosen by minimizing risk estimates with respect to any loss function, our estimator can be used with any loss function appropriate to a statistical application.====This paper is organized as follows. In Section 2, we introduce the adjusted profile likelihood that is used to obtain our estimator, which is introduced and discussed in Section 3. Section 4 and Section 5 present some numeric assessment of the performance of our estimator in simulated and real data respectively.",An orthogonally equivariant estimator of the covariance matrix in high dimensions and for small sample sizes,https://www.sciencedirect.com/science/article/pii/S0378375820301142,16 November 2020,2020,Research Article,135.0
Huang Hengzhen,"College of Mathematics and Statistics, Guangxi Normal University, Guilin 541004, China","Received 20 July 2020, Revised 30 October 2020, Accepted 7 November 2020, Available online 14 November 2020, Version of Record 2 December 2020.",https://doi.org/10.1016/j.jspi.2020.11.003,Cited by (9),Component ,"An ==== (OofA) experiment is a kind of experiment in which the addition order of experimental factors has definite influence on the response. The first OofA experiment in the literature is perhaps the famous experiment of a lady tasting tea (Fisher, 1937), who claimed she could distinguish whether the tea or the milk was first added to her cup. In this experiment, two orders, “milk ==== tea” and “tea ==== milk”, yield different responses (the taste). In general, an OofA experiment involves ==== experimental factors and we may call each as a ====. Considering ==== components, denoted by ====, every permutation on the ==== components defines an addition order, i.e., a treatment. Obviously, there are in total ==== possible addition orders and performing the full ==== design for an OofA experiment is usually unaffordable even for moderately large ==== (e.g., if ====, there will be ==== orders). For economic reasons, it is necessary to choose a subset from the full design to perform the experiment.====During the past decades, the OofA experiments are frequently mentioned in a number of scientific investigations. For example, Yang et al. (2020) studied an ==== drug combination experiment using three anti-tumor drugs, coded as A, B and C, respectively. Tumor cells were treated by one of the three drugs every six hours in a sequence of drug administration and by all three drugs administered simultaneously. The percentage of tumor inhibitions, a larger-the-better response, was measured at 24 h after the first drug administered. Table 1 displays the experimental results, which are extracted from their paper. The last column of Table 1 shows the average response of three replicates. The data suggest that (i) the order of drug administration affects the endpoint efficacy, and (ii) simultaneous administration of all three drugs does not necessarily yield the most potent therapy in contrast to adding drugs in certain orders. The authors confirmed these conclusions by using a one-way layout ANOVA procedure.====Regarding another drug combination study, Wang et al. (2020) considered a combination experiment with five drugs. A 40-run design, which was selected from the full ====-run design, was used for the experiment to represent different sequences of drug administration. The experimental results provided strong evidence that the order of drug administration affects the endpoint efficacy. Besides drug combination study, the OofA experiments have also appeared in pharmaceutical science (Rajaonarivony et al., 1993), bio-chemistry (Shinohara and Ogawa, 1998), nutritional science (Karim et al., 2000), food science (Jourdain et al., 2009), engineering (Wilson et al., 2018) and job scheduling study (Zhao et al., 2020).====Given the usefulness of OofA experiments in certain areas, it is somewhat surprising that research on statistical design and modeling for such experiments is yet in an early stage. The most commonly used model for OofA experiments is the pair-wise ordering (PWO) model proposed by Van Nostrand (1995). Basically speaking, Van Nostrand’s PWO model defines ====PWO effects and assumes the response depends additively on these effects. Under the PWO model, Voelkel (2019) proposed to use ==== (OofA-OAs) which have the same correlation structures as the full designs. Voelkel (2019) found some OofA-OAs with small number of components and tabulated these designs in his paper. Peng et al. (2019) theoretically proved that OofA-OAs are ====-optimal designs and they proposed a closed-form construction method for ====-optimal designs with any number of components. Chen et al. (2020) employed block designs to systematically construct optimal PWO designs. Algorithmic constructions for nearly-optimal PWO designs with small number of runs are possible (Voelkel, 2019, Lin and Peng, 2019). To save the experimental costs in the maximum limit, Zhao et al. (2020) studied the construction of minimal-point PWO designs (i.e., the design has ====runs, which just equals the number of parameters to be estimated in the PWO model). Despite the predominant use of Van Nostrand’s PWO model for OofA experiments, there are cases where other models may be needed. Mee (2020) pointed out that Van Nostrand’s PWO model may be mis-specified and higher-order terms may be needed. Yang et al. (2020) argued that if the conditions of the experimental units are time-varying, an ====-way ANOVA model may be more appropriate. Both Mee (2020) and Yang et al. (2020) showed that the existing PWO designs are not necessarily efficient beyond Van Nostrand’s PWO model. Lin and Peng (2019) introduced some new thoughts about various potential OofA models. For these reasons, model independent designs that are robust to different OofA models are worth studying.====The recent work of Yang et al. (2020) proposed a class of model independent designs, called ==== for OofA experiments. A component orthogonal array, denoted by ====, is a ==== matrix where (i) each row is a permutation of ====, and (ii) for any two columns, each of the level combinations (====) with ==== and ==== appears ==== times as a row. Because of the pairwise balance between any two positions (columns) in the orders, the COAs have broad generality for OofA modeling. For example, Yang et al. (2020) has shown that the COAs are highly efficient under the PWO model, an ====-way ANOVA model, and even perform well under an intuitive graphical approach for finding the optimal orders. Although some researchers may have also mentioned about robust OofA experiments (for examples, see, Peng et al., 2019, Mee, 2020, Chen et al., 2020), most of the existing designs still depend heavily on the PWO effects defined by Van Nostrand (1995), with the COAs as a notable exception. In the paper of Yang et al. (2020), two construction methods for COAs are presented. Their methods, however, need to borrow strength from two classes of combinatorial designs, namely ==== and ==== (Hedayat et al., 1999), and the resulting COAs are restricted to the case where the number of components is prime power.====In this paper, we focus on systematic constructions of COAs. Section 2 presents a new construction method for COAs with prime power components, which is more direct than existing methods in the sense that it does not need to borrow any strength from other designs. In Section 3, we present a recursive construction method which can be used to generate COAs with any number of components. As a result, the existence of COAs with any number of components is addressed. Further, some properties of the generated designs are investigated and derived. A design selection strategy is provided in Section 4 to determine COAs with reasonable run sizes. It turns out that the proposed COAs not only possess reasonable run sizes, but also have high efficiencies under the PWO model. We conclude with a discussion in Section 5.",Construction of component orthogonal arrays with any number of components,https://www.sciencedirect.com/science/article/pii/S0378375820301178,14 November 2020,2020,Research Article,136.0
"El Barmi Hammou,Malla Ganesh,Mukerjee Hari","Paul Chook Department of Information Systems and Statistics, Baruch College, City University of New York, New York, NY 10010, United States of America,Department of MCGP, University of Cincinnati-Clermont, UC Clermont College, 4200 Clermont College Drive Batavia, OH 45103, United States of America,Mathematics and Statistics Wichita State University, 1845 Fairmount Street Wichita, KS 67260-0033, United States of America","Received 8 March 2019, Revised 1 September 2020, Accepted 4 September 2020, Available online 12 November 2020, Version of Record 11 December 2020.",https://doi.org/10.1016/j.jspi.2020.09.002,Cited by (5),A life distribution function ==== is nondecreasing where ,"Let ==== be a life distribution with distribution function (DF) ==== and survival function (SF) ====. The cumulative hazard (or failure) function (CHF) of ==== is defined by ====. If ==== is continuous with a density ====, the hazard rate is defined by ====.====There is a variety of ways to model the aging process of a system. One of the most popular one is the increasing failure rate (IFR) model that assumes ==== is increasing; throughout we use increasing (decreasing) to mean nondecreasing (nonincreasing). IFR distributions have been studied extensively. In applications, the model has been found to be too restrictive. The increasing failure rate average (IFRA) relaxes the assumption to require only that ==== is increasing, i.e., ==== is star-shaped. Of course, IFR implies IFRA. Whereas the IFR distributions are continuous except possibly at the endpoint, an IFRA distribution may have jumps; however, it has a density except at a countably many points. The IFRA class of distributions have the important property that it is the smallest class containing the exponential distributions that is closed under the formation of coherent systems and under limits of distributions. They also arise in a natural way in shock models where shocks occur according to a Poisson process, each independently causing damage to a system, and the system fails when the sum of the damages exceeds a threshold. The time of failure then will have an IFRA distribution. See Barlow and Proschan (1975) for these and other properties of IFRA distributions.====Since the empirical distributions cannot be IFRA, there have been several attempts at estimating an IFRA DF based on the empirical that is IFRA with desirable properties. Let ==== be a random sample from an IFRA DF ====. Let ==== be the empirical DF (SF) and let ==== denote an estimator of ====. The aim is always an estimation of ==== since the linear order restriction is on ====. If a restricted estimator is ====, then the estimator of ==== will be given by ====.====For any function ====, we use the following notation ====The nonparametric maximum likelihood estimator (NPMLE) of an IFR distribution by Marshall and Proschan (1965) is known to be consistent. However, the NPMLE is inconsistent for some IFRA distributions (Boyles et al., 1985); the NPMLE of the SF of the ==== random variable (RV) converges a.s. to ==== rather than to ====.====Since ==== is increasing, Barlow and Scheuer (1971) proposed an isotonic regression estimator ==== of ====. Since the empirical ==== may not be increasing in ====, we could “isotonize” them by the unique solution ==== on ==== that minimizes ====where ==== is any positive vector. These authors chose equal weights to get ==== as the estimate of ==== and extended them as a right continuous step function that is constant between successive order statistics. They claimed that they had proven strong uniform consistency of their estimator ====, but it was based on the false assumption that ==== are strongly uniformly continuous. Boyles (1981) showed that for the ==== distribution, ====Since ==== is star-shaped, Wang (1987b) used the greatest star-shaped minorant (GSM) of ==== for some ==== with ==== to obtain the estimate ====, and then extending ==== as a right continuous step function to ====. She did not address the problem of consistency. Using ====, she proved ==== under some strong analytic conditions. This implies uniform ==== consistency on compact sets. Since it is difficult to choose ==== in practice (==== is unknown), she showed that the same convergence in probability holds for all ==== with ==== if the GSM is taken over the entire set of observations, but under further assumptions. A fundamental assumption in all of these results is the existence of a density ==== with ==== for all ==== where ==== is the left endpoint of the support of ====. The bounds at both ends fail for the CHF ====Even for the IFR Weibull distributions with shape parameter ====, the lower bound does not hold. Wang (1987a) also gave a similar estimator in the censored case.====Rojo and Samaniego (1994) noticed that the problem in the Wang (1987b) estimator is in the right tail, and that it is also possible to estimate ==== by using the least star-shaped majorant (LSM) of ====, yielding ==== at the order statistics. This turns out to be inconsistent near the left tail. So they proposed a spliced estimator that is the GSM of ==== on ==== and the LSM on ====, for some ==== with ====. They showed that the resulting estimator ==== has the property ====This proves uniform strong consistency over the entire line. The factor ==== is minimized by choosing ==== to be a median of ====. Thus, they chose ==== to be the smallest median of the sample. However, they found that the mean square error (MSE) of the estimator tends to be much larger than that of the empirical near the median. Therefore they recommended an adaptive procedure by choosing ==== to be an order statistic ==== that minimizes ====. Now ==== in (1.2) becomes ====, a RV. Unless ==== and ==== are bounded away from 0 a.s., consistency via (1.2) will not hold. Their simulations show that even in the adaptive case the mean square error (MSE) of their estimator exceeds that of the empirical over substantial ranges of the quantiles. It should be mentioned that ==== is increasing if and only if ==== is decreasing, These authors chose to work with the latter formulation, but we chose the formulation above for ease of comparison with the Wang (1987b) estimator and our new estimator.====If one uses an order restricted estimator, and the ordering holds, one should improve on the empirical in terms of MSE at all quantiles. We propose a new estimator of an IFRA SF utilizing the fact that ==== is increasing implies ==== defines a convex function on ====. Since the ordinary sample estimate ==== will not be convex, we replace it by its greatest convex minorant (GCM), ====, and recover our estimate ==== of ==== by the right derivative of ====. We prove uniform strong consistency of ====. Our method of estimation and proof of consistency apply both to the uncensored and the censored cases. We show by simulations that the new estimator improves on the empirical in terms of MSE at all quantiles in all of a large variety of IFRA distributions. We have also proven asymptotic normality of our estimator at a point ==== where ====. A weak convergence result would require very strong assumptions that may not hold even for many IFR distributions, as in the case of the Wang (1987b) estimator. In Section 5 we show that the last three estimators have some similarities with ours, and give some heuristic reasons as to why they behave poorly at some quantiles. We found the exposition is much simpler by treating the uncensored case first and then stating the minor changes needed for the censored case afterwards.====In Section 2, we describe our estimator. In Section 3, we prove strong uniform consistency of our estimator. In Section 4, we prove asymptotic normality of our estimator at a point of continuity of ==== where ==== is positive. In Section 5 we compare the ratios of the MSEs of various estimators to that of the empirical by simulations. In Section 6, we analyze a real data set to show how our procedure is implemented. In Section 7, we make some concluding remarks.====The properties of isotonic regression used in this paper can be found in Barlow et al. (1972) or Robertson et al. (1988).====The notations ==== stands for the indicator function of ====, and ==== stands for ====. The symbols ==== and ==== will stand for converges in probability, converges almost surely (a.s.), and converges in distribution or converges weakly. The symbol ==== stands for the space of continuous functions on ====, equipped with the topology of uniform convergence on all compact intervals with respect to the topology induced by the metric ====.",Estimation of a distribution function with increasing failure rate average,https://www.sciencedirect.com/science/article/pii/S0378375820301026,12 November 2020,2020,Research Article,137.0
"Alshammri Fayed,Pan Jiazhu","Department of Mathematics & Statistics, University of Strathclyde, Glasgow G1 1XH, UK,Department of Basic Sciences, Saudi Electronic University, Riyadh, Kingdom of Saudi Arabia,School of Mathematics & Statistics, Yangtze Normal University, Chongqing 408100, China","Received 1 February 2020, Revised 11 July 2020, Accepted 2 August 2020, Available online 12 November 2020, Version of Record 20 November 2020.",https://doi.org/10.1016/j.jspi.2020.08.007,Cited by (2),This paper extends the principal component analysis (PCA) to moderately non-stationary vector time series. We propose a method that searches for a ==== of the original series such that the transformed series is segmented into uncorrelated subseries with lower dimensions. A columns’ rearrangement method is proposed to regroup transformed series based on their relationships. We discuss the theoretical properties of the proposed method for fixed and large dimensional cases. Many simulation studies show our approach is suitable for moderately non-stationary data. Illustrations on real data are provided.,"The collection of multivariate time series data grows as the technology advances in many fields, such as finance, healthcare, industry and social networks. Modelling a multivariate time series can be a challenge, especially for high dimensional series. Most of the available tools, such as vector autoregressive integrated moving average (VARIMA) models, produce complex and over-parametrized models when applied to such data. Therefore, the use of dimension reduction method is crucial to model high dimensional series and find a simple representation of the data.====There is a significant amount of dimension reduction methods in the literature. A popular approach is the use of principal component analysis (PCA) by projecting the original data into a space with fewer dimensions. Actually, PCA is a commonly used technique to perform dimension reduction for static and independent multivariate data. But the classical PCA will not be able to capture the dynamic dependence of among the components (i.e. variables) of a multivariate time series.====Ku et al. (1995) extended PCA to time series data by including necessary time lags of the original series in the analysis. Their method is called the dynamic principal component analysis (DPCA), and it produces dynamic principal components that are linear combinations of both current and lagged values of the original data. However, DPCA is limited to stationary series, and will not be applicable to nonstationary series. Alshammri and Pan (2019) proposed the moving dynamic principal component analysis (MDPCA), which extended MDPCA to moderately non-stationary time series. Instead of using the cross-covariance matrix, MDPCA applies eigenanalysis on a moving lagged cross-covariance matrix with adjustable window length to extract information from moderately non-stationarity series.====Another related approach was proposed by Brillinger (1981), where the reduction is made based on a reconstruction criterion. This method produces dynamic principal components that are linear combinations of the original series. Peña and Yohai (2016) proposed the generalized dynamic principal component analysis (GDPCA), which is also based on a reconstruction criterion. The dynamic principal components produced by GDPCA could be a nonlinear combination of the original data. The GDPCA can be applied to both stationary and nonstationary series.====Factor models are also widely used tools to reduce the dimension of multivariate time series where the variables of the observed series are considered as linear combinations of some hidden factors that could be interpreted subjectively. See, for example, Peña and Box (1987), Stock and Watson, 1988, Stock and Watson, 2002, Bai and Ng (2002), Forni et al. (2005), Peña and Poncela (2006), Pan and Yao (2008), Lam and Yao (2012) and many others. These models are related to PCA as they are based on eigenanalysis of the covariance matrix of the original series. Zhang et al. (2019) proposed to do co-integration analysis for dimensional reduction of a high dimensional time series with unit root components based a nonnegative definite matrix when there is co-integration relation among components. The co-integration approach is entirely different from PCA. Canonical correlation analysis of Box and Tiao (1977) and scalar component analysis of Tiao and Tsay (1989) are also popular tools to reduce the dimension of a time series.====Of particular interest, a dimension reduction method proposed recently by Chang et al. (2018) called the principal component analysis for time series (TS-PCA). It produces a linear transformation of the original series such that the transformed series is segmented into uncorrelated subseries with lower dimensions. To be specific, TS-PCA applies eigenanalysis to a quadratic function of the lagged cross-covariance matrix of the data. The results are uncorrelated subseries (univariate and/or multivariate) that can be analysed separately. However, TS-PCA is limited to second-order stationary series.====In this paper, a new dimension reduction tool will be proposed. The new method we propose is general in the sense that it can be applied to a wide range of both stationary and non-stationary time series data. This method can be considered as an extension of the stationary TS-PCA of Chang et al. (2018) to non-stationary data and therefore will be called the generalized principle component analysis for moderately non-stationary vector time series (GTS-PCA). The results of GTS-PCA are also uncorrelated subseries with lower dimensions that can be analysed individually. We construct a positive definite matrix using a moving window covariance matrix which is different from that for TS-PCA where the classical covariance matrix of the data is used in its calculation. In Sections 2 Methodology, 3 Columns’ rearrangement of this paper we provide a full description of the methodology. Section 2 describes how the GTS-PCA is derived and Section 3 presents a new columns’ rearrangement method we propose called the maximum moving cross-correlation method. Theoretical asymptotic properties will be given in Section 4. Performance of the GTS-PCA will be tested in Section 5 on both simulated and real data.",Generalized principal component analysis for moderately non-stationary vector time series,https://www.sciencedirect.com/science/article/pii/S0378375820301130,12 November 2020,2020,Research Article,138.0
"Wu Peng,Xu Xinyi,Tong Xingwei,Jiang Qing,Lu Bo","School of Statistics, Beijing Normal University, China,Department of Statistics, The Ohio State University, United States of America,School of Statistics, Southwestern University of Finance and Economics, China,Division of Biostatistics, College of Public Health, The Ohio State University, United States of America","Received 6 October 2019, Revised 7 October 2020, Accepted 16 October 2020, Available online 12 November 2020, Version of Record 18 November 2020.",https://doi.org/10.1016/j.jspi.2020.10.004,Cited by (6)," of the outcome model is often unknown and there exists a large number of ====. The simulation studies indicate that our methods compare favorably with many competing estimators. Our methods are easy to implement and avoid hazardous impact due to extreme weights as often seen in weighting estimators. They can also be extended to handle subgroup effects with known structure. We apply the proposed methods to data from the Ohio Medicaid Assessment Survey 2012, estimating the effect of having health insurance on self-reported health status for a population with subsidized insurance plan choices under the Affordable Care Act.","Causal inference with observational data is challenging as it has to handle two tasks at the same time, self-selection of treatment and outcome modeling. The self-selection bias refers to the bias in causal effect estimation associated with the difference in relevant pre-treatment covariates. Even under the assumption of ignorable treatment assignment (i.e. all relevant covariates are known), this is not easy since there are usually a large number of variables related to treatment selection, plus many other predictors for the outcome. Early strategies for estimating average causal effects involve mostly parametric models, i.e. linear regression for continuous outcomes. The validity of a fully parametric approach depends heavily on the correct model specification. Results from regression models are very sensitive to small discrepancies in the model setup (Imbens and Rubin, 2015). Especially, with a large number of covariates, it becomes extremely difficult to specify all functional forms correctly.====An improvement to conventional regression models is to use the generalized additive model (GAM) when the dimension of covariates is high (Hastie and Tibshirani, 1986). GAM is advantageous, as it not only allows to fit a non-linear function for each covariate by using B-spline approximation or other local smoothing methods, but also avoids the curse of dimensionality (Hastie et al., 2009). However, GAM is restricted to be additive. Non-additive functional forms cannot be used and important interaction terms of the covariates may be missed.====In causal inference, a primary parameter of interest is the average treatment effect, while coefficients for other covariates can be considered nuisance. Rosenbaum and Rubin (1983) introduced the concept of balancing score to address the high-dimensional covariates in causal effect estimation. A balancing score, ====, is a function of the covariates ==== such that the conditional distribution of ==== given ==== is the same between treated (====) and control (====) groups, that is, ====They further developed a series of methods for estimating the average causal effect based on the propensity score. The propensity score is a scalar function of covariates defined as ==== and it is shown to be the coarsest balancing score (Rosenbaum and Rubin, 1983). The propensity score finds the lowest-dimensional function of the covariates that suffices for removing the selection bias due to pre-treatment variables.====The propensity score can be incorporated in constructing both nonparametric and semiparametric causal estimators. For nonparametric estimation, we may use the propensity score to form matched sets or strata, which mimics a block randomized experiment design (Rosenbaum, 2002). We may also use it as a weight, which leads to estimators similar to the Horvitz–Thompson estimator in survey sampling (Horvitz and Thompson, 1952). This approach is also known as inverse probability weighting (IPW) estimation. For semiparametric methods, Robins et al. (1994) considered a class of augmented IPW (AIPW) estimators by combining propensity score with outcome regression models. AIPW estimators enjoy a property of doubly robustness, which guarantees the consistent estimation of causal effects if either the outcome regression model or the propensity score model is correctly specified (Bang and Robins, 2005). Moreover, such estimators achieve the semiparametric efficiency bound when both the propensity score model and the outcome regression model are correctly specified. AIPW estimator is not always the optimal one, as Tan (2007) showed that the asymptotic variance of the outcome regression estimator is no larger than that of AIPW if the outcome model is correctly specified. Tan, 2006, Tan, 2010 considered other regression estimators based on nonparametric likelihood. van der Laan (2010) also developed doubly robust estimator using machine learning tools based on targeted maximum likelihood estimation (TMLE), both of which further improved.====Doubly robust estimators rely on correct model specification of the propensity score or the outcome, which may not be an easy task in practice. When both models are specified somewhat wrong, the results could be seriously biased (Kang and Schafer, 2007). Even when model specification is not an issue, the estimator can be very volatile when there are propensity score values very close to zero or one (Hahn, 1998, Rubin, 2001, Tsiatis and Davidian, 2007, Lee et al., 2011). This is because the resulting weights are extremely large and they tend to inflate the variance estimation. Tan (2010) limited the effects of large wights and considered double robust estimation based on the restricted nonparametric likelihood. It requires either parametric or nonparametric modeling of the conditional mean ====
 (==== is outcome) or the conditional distribution (Lin et al., 2017), which has similar problems with AIPW.====A recent strand of literature tried to address the problems of extreme weights and the outcome model specification through specially designed weights or machine learning techniques. Li et al. (2018) unified the existing weighting methods and proposed an overlap weighting approach that avoids extreme weights by focusing on the most similar subpopulations between two treatment groups. Chernozhukov et al. (2018) proposed a double/debiased machine learning estimators in case of high-dimensional covariates. Taking a step further, Wager and Athey (2018) and Athey et al. (2019) discussed algorithms to construct causal forest for estimating the heterogeneous treatment effects using generalized random forests.====In this paper, we propose to combine the propensity score with nonparametric regression models to obtain high quality average causal effect estimators. Instead of using propensity scores as weights to remove the selection bias, we include propensity scores directly in the regression model. To improve the robustness, we adopt nonparametric modeling for the outcome, i.e. splines. Though spline methods are widely used in statistical modeling, it has received little discussion in causal effect estimation. We aim to fill the gap by establishing the asymptotic properties of spline estimators for average treatment effects. This approach can be implemented easily and requires minimum model specification. We further modify the estimator by incorporating adjustment for error heteroscedasticity to improve the efficiency. The primary goal of our method is to estimate a constant average causal effect. In practice, to test whether the causal effect is constant, our model can be extended by including interactions between the treatment and subgroups. The main advantage of our approach is to handle high dimensional covariates with arbitrary functional form in causal estimation. The dimension reduction is achieved via the propensity score and the use of spline models avoids the harmful impact of extreme propensity scores on variance results. The simulation studies show that our methods compare favorably with many competing estimators.====The paper is organized as follows. Section 2 introduces the notations and our estimation strategy. Section 3 shows the asymptotic results. In Section 4, we conduct extensive simulations to compare our methods with conventional linear regression model, GAM, subclassification on propensity score, AIPW estimator, Tan’s calibrated likelihood estimator and regression estimator, TMLE and the overlap weight augmented estimation. In Section 5, we apply our methods to estimate the health insurance policy effect on self-reported health status. A brief discussion is presented in Section 6.",Semiparametric estimation for average causal effects using propensity score-based spline,https://www.sciencedirect.com/science/article/pii/S0378375820301099,12 November 2020,2020,Research Article,139.0
"Vidal Ignacio,de Castro Mário","Instituto de Matemáticas y Física, Universidad de Talca, 2 Norte 685, Casilla 747, Talca, Chile,Universidade de São Paulo, Instituto de Ciências Matemáticas e de Computação, São Carlos, SP, Brazil","Received 30 July 2019, Revised 8 October 2020, Accepted 27 October 2020, Available online 11 November 2020, Version of Record 20 November 2020.",https://doi.org/10.1016/j.jspi.2020.10.002,Cited by (2),"The matching problem is known since the beginning of the eighteenth century and ==== solutions have been proposed. In this paper, we present a ","The matching problem is one of the most antique and famous problems of elementary probability. This problem was made known by de Montmort (1708) and it is also known as the problem of rencontres or coincidences. The solution to the problem was published in 1713 in the second edition of the book by de Montmort (1708) after analyzing a generalization of the problem together with Nikolaus Bernoulli. In essence, the problem is that we have ==== objects labeled as ==== and a permutation of them is drawn. It is said that there is a concordance, coincidence, match or rencounter when the label that identifies the object coincides with the order that the object has after the permutation has been made. The original question in this problem was to find the probability of obtaining at least one match when random permutations are made. In fact, the distribution of the exact number of matches ==== assuming random permutations is well known (Feller, 1968 Sect. IV.4) and is given by ====For more details on the history of this problem, the reader is referred to Takács (1980) and references therein. Since then, there have been many generalizations, variants and applications of this problem; see, e.g., Grideman (1960) and Takács (1980).====The most obvious frequentist solution to this problem is the use of the distribution of the number of matches to compute the ====-value in a skill test. It is clear that if, without knowing the order or the prefixed labels, a permutation of these ==== objects were made with a large number of coincidences (greater than those that could reasonably be expected from a random selection), then there would be some gift of divination, skill or knowledge. A Bayesian version of the matching problem with several prior distributions on the set of all possible ==== permutations can be found in Diaconis and Holmes (2002).====The analysis of the matching problem and its variants has been always approached by studying the probability mass function (pmf) of the number of matches; see, for example, Barton, 1958, Abramson, 1973, Knudsen and Skau, 1996, DasGupta, 2005 and Fanshawe (2018). Up to the best of our knowledge, attention has never been paid to the order in which assignments are made. However, since there is dependence between assignments, there are two ways to make erroneous assignments, namely, (i) because the object was already assigned before the current assignment or (ii) because the object was not correctly selected. The second cause of erroneous assignments provides information on the ability to perform correct assignments. A high number of erroneous assignments when they could have coincided (unforced errors) shows evidence against the claimed ability to make correct assignments. Therefore, if unforced errors give information about the ability to make correct matches and it is possible to design an experiment to observe the number of matches and the number of unforced errors, what it is the reason for not doing it?====In this work, our main goal is to know whether from observing permutations of the ==== objects, we can say that there is some ability to obtain matches in a greater number than that obtained from random permutations. We tackle the matching problem with a somewhat different vision than the usual. We will perform the same experiment, but we will observe a different response variable. In our analysis, we will take into account the order in which the assignments are made. This leads us to a likelihood function that will relate the permutations with the parameters that measure the ability to make the identification of each object. We will see that this likelihood function depends not only on the number of coincidences, but also on the number of unforced errors. Since we will also consider that it is possible to have different abilities for the different objects, then we will have a parameter that measures the matching ability for each specific object.====The remaining of the paper is organized in the following manner. In Section 2, we obtain the likelihood function for the parameters considering the response variable in our experiment. In Section 3, posterior distributions and Bayes factors are computed under conjugate prior distributions composed by augmented beta distributions. We conclude with some general remarks in Section 4.",A Bayesian analysis of the matching problem,https://www.sciencedirect.com/science/article/pii/S0378375820301063,11 November 2020,2020,Research Article,140.0
"Pronzato Luc,Wang HaiYing","Université Côte d’Azur, CNRS, I3S, France,Department of Statistics, University of Connecticut, USA","Received 1 April 2020, Revised 27 July 2020, Accepted 2 August 2020, Available online 5 November 2020, Version of Record 20 November 2020.",https://doi.org/10.1016/j.jspi.2020.08.001,Cited by (8),"We consider a design problem where experimental conditions (design points ====) are presented in the form of a sequence of i.i.d. random variables, generated with an unknown probability measure ====, and only a given proportion ==== can be selected. The objective is to select good candidates ==== of the corresponding information matrix. The optimal solution corresponds to the construction of an optimal bounded design measure ====, with the difficulty that ==== is unknown and ==== must be constructed online. The construction proposed relies on the definition of a threshold ==== at the current information matrix, the value of ","Consider a rather general parameter estimation problem in a model with independent observations ==== conditionally on the experimental variables ====, with ==== in some set ====. Suppose that for any ==== there exist a measurable set ==== and a ====-finite measure ==== on ==== such that ==== has the density ==== with respect to ====, with ==== the true value of the model parameters ==== to be estimated, ====. In particular, this covers the case of regression models, with ==== the Lebesgue measure on ==== and ====, where the ==== are independently distributed with zero mean and known variance ==== (or unknown but constant variance ====), and the case of generalized linear models, with ==== in the exponential family and logistic regression as a special case. Denoting by ==== the estimated value of ==== from data ====, ====, under rather weak conditions on the ==== and ====, see below, we have ====where ==== denotes the (normalized) Fisher information matrix for parameters ==== and (asymptotic) design ==== (that is, a probability measure on ====), ==== This is true in particular for randomized designs such that the ==== are independently sampled from ====, and for asymptotically discrete designs, such that ==== is a discrete measure on ==== and the empirical design measure ==== converges strongly to ====; see Pronzato and Pázman (2013). The former case corresponds to the situation considered here. The choice of ==== is somewhat arbitrary, provided that ==== for all ====, and we shall assume that ====. We can then write ====denotes the elementary information matrix at ====.====Taking motivation from (1.1), optimal experimental design (approximate theory) aims at choosing a measure ==== that minimizes a scalar function of the asymptotic covariance matrix ==== of ====, or equivalently, that maximizes a function ==== of ====. For a nonlinear model ==== and ==== depend on the model parameters ====. Since ==== is unknown, the standard approach is local, and consists in constructing an optimal design for a nominal value ==== of ====. This is the point of view we shall adopt here — although sequential estimation of ==== is possible, see Section 6. When ==== is fixed at some ====, there is fundamentally no difference with experimental design in a linear model for which ==== and ==== do not depend on ====. For example, in the linear regression model ====where the errors ==== are independent and identically distributed (i.i.d.), with a density ==== with respect to the Lebesgue measure having finite Fisher information for location ====
 (==== for normal errors ====), then ====, ====. Polynomial regression provides typical examples of such a situation and will be used for illustration in Section 4. The construction of an optimal design measure ==== maximizing ==== usually relies on the application of a specialized algorithm to a discretization of the design space ====; see, e.g., Pronzato and Pázman (2013, Chap. 9).====With the rapid development of connected sensors and the pervasive usage of computers, there exist more and more situations where extraordinary amounts of massive data ====, ====, are available to construct models. When ==== is very large, using all the data to construct ==== is then unfeasible, and selecting the most informative subset through the construction of an ====-point optimal design, ====, over the discrete set ==== is also not feasible. The objective of this paper is to present a method to explore ==== sequentially and select a proportion ==== of the ==== data points to be used to estimate ====. Each candidate ==== is considered only once, which allows very large datasets to be processed: when the ==== are i.i.d. and are received sequentially, they can be selected on the fly which makes the method applicable to data streaming; when ==== data points are available simultaneously, a random permutation allows ==== to be processed as an i.i.d. sequence. When ==== is too large for the storage capacity and the i.i.d. assumption is not tenable, interleaving or scrambling techniques can be used. Since de-scrambling is not necessary here (the objective is only to randomize the sequence), a simple random selection in a fixed size buffer may be sufficient; an example is presented in Section 4.3.====The method is based on the construction of an optimal bounded design measure and draws on the paper (Pronzato, 2006). In that paper, the sequential selection of the ==== relies on a threshold set on the directional derivative of the design criterion, given by the ====-quantile of the distribution of this derivative. At stage ====, all previous ====, ====, are used for the estimation of the quantile ==== that defines the threshold for the possible selection of the candidate ====. In the present paper, we combine this approach with the recursive estimation of ====, following Tierney (1983): as a result, the construction is fully sequential and only requires to record the current value of the information matrix ==== and of the estimated quantile ==== of the distribution of the directional derivative. It relies on a reinterpretation of the approach in Pronzato (2006) as a stochastic approximation method for the solution of the necessary and sufficient optimality conditions for a bounded design measure, which we combine with another stochastic approximation method for quantile estimation to obtain a two-time-scale stochastic approximation scheme.====The paper is organized as follows. Section 2 introduces the notation and assumptions and recalls main results on optimal bounded design measures. Section 3 presents our subsampling algorithm based on a two-time-scale stochastic approximation procedure and contains the main result of the paper. Several illustrative examples are presented in Section 4. We are not aware of any other method for thinning experimental designs that is applicable to data streaming; nevertheless, in Section 5 we compare our algorithm with an exchange method and with the IBOSS algorithm of Wang et al. (2019) in the case where the ==== design points are available and can be processed simultaneously. Section 6 concludes and suggests a few directions for further developments. A series of technical results are provided in the Appendix.",Sequential online subsampling for thinning experimental designs,https://www.sciencedirect.com/science/article/pii/S0378375820300999,5 November 2020,2020,Research Article,141.0
"Song Xiaojun,Taamouti Abderrahim","Department of Business Statistics and Econometrics, Guanghua School of Management and Center for Statistical Science, Peking University, Beijing, 100871, China,Department of Economics and Finance, Durham University Business School, Mill Hill Lane, Durham, DH1 3LB, UK","Received 3 August 2019, Revised 2 August 2020, Accepted 20 August 2020, Available online 4 November 2020, Version of Record 11 November 2020.",https://doi.org/10.1016/j.jspi.2020.08.005,Cited by (1),We introduce a nonparametric measure to quantify the degree of heteroskedasticity at a fixed ,"Regression errors in cross-section and time series models frequently exhibit heteroskedasticity. Even though the latter was always viewed as a problem that one needs to treat to improve efficiency, some authors take a different view and argue that the heterogeneity in the degree of heteroskedasticity can often have important theoretical and substantive implications over and above those for accurate inference. Among others, Newey and Powell (1987) argue that the change in the degree of heteroskedasticity in the conditional distribution of the dependent variable might be viewed as a symptom of misspecification of the regression function [e.g. indicates the presence of an omitted variable].==== ==== In addition, Arabmazar and Schmidt (1981) study the impact of the degree of heteroskedasticity in the error terms on the size of the inconsistency of the MLE estimator. They show that the inconsistency is greater the greater the degree of heteroskedasticity. Others have stressed the importance of understanding the economic meaning of heteroskedasticity when its degree changes across the conditioning variables. For example, Meghir and Pistaferri (2004) among others point out the relevance of allowing for variance of income to vary across different education levels for modeling earnings distribution. Much research has been devoted to building tests of heteroskedasticity. However, to the best of our knowledge, no research really develops measures of the degree of heteroskedasticity. The present paper aims to propose a nonparametric measure of the degree of heteroskedasticity at a given fixed quantile of the conditional distribution of a random variable. The measure can also be used to test for heteroskedasticity.====Measuring the degree of heteroskedasticity might also help us to better understanding the relationship between the latter and the efficiency of the estimates of regression coefficients. The presence of a ==== heteroskedasticity in the data negatively affects the performance (size and power) of classical tests such as ====-test and ====–test. Several econometric textbooks and papers have reported that the available heteroskedasticity consistent standard errors lead to tests/confidence intervals that deliver substantial under or over size/coverage depending on the degree of heteroskedasticity; see Kennedy (1985), Dufour (2003), Cribari-Neto (2004), Dufour and Taamouti (2010), Hausman and Palmer (2012), Cattaneo et al. (2018), among others. For a valid inference, different estimation techniques/tests might need to be applied depending on the degree of heteroskedasticity. Senyo and Adjibolosoo (1989) argue that if the degree of heteroskedasticity is not high, then the ordinary least squares (OLS) estimator might still perform better than the generalized least squares (GLS) estimator. They stress the importance of developing measures of the strength of heteroskedasticity, which can help researchers understand when to use either the OLS estimator or the GLS estimator.====The above motivations illustrate the usefulness of providing measures of the degree of heteroskedasticity in the conditional distribution of the dependent variable. In this paper, we introduce a measure of heteroskedasticity using nonparametric quantile regressions. This measure can quantify the degree of heteroskedasticity at a fixed quantile of the conditional distribution of the variable of interest. Unfortunately, the existing heteroskedasticity tests fail to accomplish this task, because they only provide evidence on the presence or the absence of heteroskedasticity, and statistical significance depends on the available data and test power. A strong heteroskedasticity may not be statistically significant, and a statistically significant heteroskedasticity may not be strong from an economic viewpoint. To the best of our knowledge, this is the first measure of heteroskedasticity, which is based on nonparametric quantile regressions and expressed in terms of unrestricted and restricted expectations of quantile loss functions. It is consistently estimated by replacing the unknown expectations by their nonparametric estimates. Our theoretical results are proven under the assumptions of dependent data, but they are still valid for cross-sectional data.====We also note that there is an abundant literature on nonparametric quantile regression when parametric quantile regression function is not available. For example, Chaudhuri (1991), Yu and Jones (1998) and Guerre and Sabbah (2012) consider nonparametric estimation of conditional quantiles for i.i.d. observations by using local polynomial regression, while Honda (2000), Hall et al. (2002) and Kong et al. (2010) examine the asymptotic properties of the estimator of Chaudhuri (1991) for strongly mixing stationary processes. Nevertheless, none of the aforementioned estimators is designed to measure the degree of heteroskedasticity of the conditional distribution of a random variable. Our paper fills this gap by suggesting a convenient ====–type measure of heteroskedasticity at a fixed quantile based on the nonparametric quantile estimators.====Furthermore, we derive a Bahadur-type representation for the nonparametric estimator of the measure. We provide its asymptotic distribution, which one can use to build tests for the statistical significance of the measure. Moreover, since testing that the value of the measure is equal to zero is equivalent to testing for homoscedasticity, another contribution of this paper consists in providing a test for heteroskedasticity that is robust to the parametric misspecification of conditional location function. The existing parametric specification-based tests for heteroskedasticity generally suffer from the model misspecification problem, and require correct parametric specification of the regression function. Thereafter, we establish the validity of a fixed regressor bootstrap that one can use in finite-sample settings to perform tests for different values of the measure. A Monte Carlo simulation study reveals that the bootstrap test has a good finite sample size and power for a variety of data generating processes and different sample sizes.====Two empirical applications are also provided to illustrate the importance of the proposed measure. In the first application, we are interested in measuring the degree of heteroskedasticity of income conditional on the years of education, and in the second application, we quantify the degree of heteroskedasticity for 30 stock returns. For the first application, our results show that the degree of income variation decreases when the years of education increase. Thus, the income of highly educated people varies less compared with the income of those with low levels of education. Furthermore, we find that the degree of income variation for females is generally smaller than the degree of income variation for males. For the second application, the results confirm that all stock returns under consideration are conditionally heteroskedastic. In addition, these results show that there is some heterogeneity in the degree of heteroskedasticity across the stocks.====To sum up, our contributions are threefold. Firstly, we propose a fully model-free measure to gauge the degree of heteroskedasticity. Secondly, we show that the proposed measure can be used as a test to detect heteroskedasticity. Our test is designed to be particularly robust to the misspecification in the conditional mean and is able to capture various forms of conditional heteroskedasticity. Lastly, we propose an innovative bootstrap procedure to implement the test.====This paper is organized in the following way. Section 2 presents the general theoretical framework that underlies the definition of the measure of heteroskedasticity in the presence of constant and non-constant means. In Section 3, we discuss the estimation of nonparametric quantile regressions and, consequently, of the measure of heteroskedasticity based on the local polynomial estimation of the unrestricted and restricted expectations of quantile loss functions. We derive a Bahadur-type representation for the nonparametric estimator of the measure. We also provide the asymptotic distribution of this estimator, which one can use to build tests for the statistical significance of the measure. In Section 4, we establish the validity of a fixed regressor bootstrap that one can use in finite-sample settings to perform tests. Section 5 presents a Monte Carlo simulation exercise to investigate the finite sample properties of the bootstrap-based test of the measure of heteroskedasticity. Section 6 is devoted to an empirical application, and the conclusion relating to the results is given in Section 7. The main assumptions of the paper and the proofs of the theoretical results are presented in the Appendices A.2.1 and A.2.2, respectively.",A nonparametric measure of heteroskedasticity,https://www.sciencedirect.com/science/article/pii/S0378375820301105,4 November 2020,2020,Research Article,142.0
"Bhattacharyya Dhrubasish,Khan Ruhul Ali,Mitra Murari","Department of Mathematics, Indian Institute of Engineering Science and Technology, Shibpur, P.O. - Botanic Garden, Howrah 711103, West Bengal, India","Received 25 July 2019, Revised 9 October 2020, Accepted 12 October 2020, Available online 2 November 2020, Version of Record 11 November 2020.",https://doi.org/10.1016/j.jspi.2020.10.003,Cited by (12)," and crossings of mean time to failure functions. The test is shown to be consistent and a bound on asymptotic size has been derived. A computational technique is devised for evaluating the proposed test statistic. A simulation study is carried out for assessing the merit of the proposed test procedure. Finally, the test is applied to real-life data sets for illustration.","In order to prevent the in-service failure of an item or a system, it is a common practice to employ an age replacement policy in which a working item is replaced by a new one on its failure or at a prespecified time ====, whichever occurs earlier. In connection with renewal theory and maintenance strategies, the mean time to failure (MTTF) function, introduced by Barlow and Proschan (1965), plays a prominent role in the study of reliability characteristics of systems under an age replacement policy. Let ==== be a non-negative random variable representing the lifetime of an item and the ==== (cdf) and ==== of ==== be ==== and ==== respectively. Further, let ==== be the random variable which represents the first in-service failure time of an item under an age replacement policy with age replacement time ====. Then the MTTF function is defined to be the expected value of ==== and is given by ====A detailed discussion can be found in Barlow and Proschan (1965). The decreasing mean time to failure (DMTTF) class of life distributions has potential applications in renewal theory and maintenance strategies and has received considerable attention in the literature. Klefsjö (1982) showed that the DMTTF class is strictly larger than the IFRA class and also investigated its relationships with other well-known aging classes. Knopik (2005) studied preservation of the DMTTF class under some basic reliability operations while the closure of the said class under convolution and weak convergence were also established in Knopik (2006). Other important contributions in this area were made by Asha and Nair (2010) and Kayid et al. (2013). On the inferential side, the problem of testing exponentiality against alternatives belonging to the DMTTF class of life distributions is an important problem in the study of reliability characteristics of a repairable system. In this context, efforts made by Li and Xu (2008), Kayid et al. (2013) and Kattumannil and Anisha (2019) are significant. Recently, Bhattacharyya et al. (2020b) have developed a test for exponentiality against DMTTF alternatives using L-statistic theory. In order to model nonmonotonic aging scenarios, the increasing then decreasing mean time to failure (IDMTTF) class of life distributions, introduced by Izadi et al. (2018), is significant. The problem of testing exponentiality against IDMTTF class of life distributions is considered in Izadi and Manesh (2019). Khan et al. (2020) have developed a general methodology for the estimation of change point of MTTF function. Recently, shock model theory in different scenarios for classes of life distributions based on the MTTF function has been developed in Khan et al. (2021). They also explored weak convergence and moment convergence issues within the IDMTTF class of life distributions.====Let ==== and ==== be two non-negative random variables with respective MTTF functions ==== and ====. Then ==== is said to be larger than ==== in the MTTF order if ==== and we write ====, see Asha and Nair (2010) and Kayid et al. (2013) for details. For comparing MTTF functions of two distributions, the MTTF order is of profound interest since it enables one to choose an item appropriately over a set of alternatives in order to improve the performance of a system which is under an age replacement policy. However, to the best of our knowledge, no statistical method for comparing the MTTF functions of two distributions based on two independent samples is available in the literature to date. In this paper, we propose a nonparametric two-sample test for detecting MTTF order dominance. We use the intersection-union principle (see, for example Berger and Sinclair, 1984, Berger et al., 1988 and Bhattacharyya et al., 2020a) for developing the test procedure.====In Section 2, we construct a test procedure for testing ==== where ==== and ==== are positive reals. The main important feature of the testing problem (1) is that it has the flexibility of handling crossings of two MTTF functions and may be applied to obtain confidence statements of the form “==== for all ====”, where ==== is a subinterval of ==== determined by the data sets. A test designed only to test the null hypothesis of equality of MTTF functions for all ==== may have a large probability of rejecting this null hypothesis for the realistic situation when the MTTF functions of two populations cross. However, our test is free from this limitation and has the distinct feature of including all possible situations. The proposed test is shown to be consistent and a bound for the asymptotic size is obtained. However, the test statistic proposed does not lend itself to straightforward computation, so in Section 3, we have developed a nice computational technique to evaluate the test statistic. In Section 4, we assess the performance of the test by means of a simulation study. Finally, Section 5 deals with the application of our test to real-life data sets for illustrative purposes.",Two-sample nonparametric test for comparing mean time to failure functions in age replacement,https://www.sciencedirect.com/science/article/pii/S0378375820301075,2 November 2020,2020,Research Article,143.0
"Marchand Éric,Rancourt Fanny","Université de Sherbrooke, Département de mathématiques, Sherbrooke, QC, Canada J1K 2R1,Commission scolaire de Montréal, 3737 rue Sherbrooke E, Montréal, QC, Canada H1X 3B3,Rutgers University, Department of Statistics, 501 Hill Center, Busch Campus, Piscataway, NJ, 08855, USA","Received 2 March 2020, Revised 30 September 2020, Accepted 1 October 2020, Available online 2 November 2020, Version of Record 12 November 2020.",https://doi.org/10.1016/j.jspi.2020.10.001,Cited by (3)," model. We also study a related ==== property, obtaining several non-minimax results. Finally, for discrete models such as Poisson and ","Restricted parameter space inference bring many challenges, several of which are less apparent in corresponding unrestricted parameter space versions. Over the years, a better understanding of decision-theoretic properties of such problems has emerged and several techniques and methods have been developed to address such challenges (e.g., Marchand and Strawderman, 2004, van Eeden, 2006), including minimax analysis (e.g., Casella and Strawderman, 1981, Marchand and Strawderman, 2012).====Interestingly, there exist a large number of problems where the minimax risk for ====, remains the same for the parametric restriction ====, and this for many choices of loss functions (e.g., Marchand and Strawderman, 2012). An early example was addressed by Katz (1961) for ====, squared error loss, ====, and ====. In this case, the estimator ==== is clearly not adapted to the parametric restriction, but knowledge of its minimaxity nevertheless is still a useful benchmark as dominating estimators are necessarily minimax themselves. Then, in such situations, it is further of interest to specify classes of plausible estimators, Bayesian or otherwise, that dominate the given benchmark and that are necessarily minimax themselves.====This paper deals with one-parameter exponential families and a corresponding minimax property for normalized squared error loss. We find it useful to record a unifying minimax finding, which not only includes existing results such as for the normal case among others, but also leads to new findings such as for a lower-bounded negative binomial mean. We also focus on determining dominating minimax estimators, in particular for discrete models as there exists a relative paucity of dominance findings in the literature.====Let ====with ====, be a one-parameter exponential family density with respect to a ====-finite measure ====. Set ====, ====, assume ==== as ==== and ==== as ====. The minimax and non-minimax results of this paper depend critically on the variation of ====.====We study the potential minimaxity of estimators ==== of ==== under weighted squared error loss ====Karlin’s Theorem (Karlin, 1958) concerns exponential families (1.1) and provides sufficient conditions (see Section 2.2) for the admissibility of ==== under squared error loss, and hence under loss (1.2). Moreover, in cases where admissibility holds, it follows immediately that ==== is unique minimax under loss (1.2) with constant risk equal to ====.====When ====, taking ====, we show with Theorem 2.2 that the estimator ==== is also minimax under a lower bound restriction ====, under loss ==== provided Karlin’s conditions for admissibility in the full parameter space hold. The finding is unified and we provide examples, some of which being known minimax results, and some which being new to the best of our knowledge. Also, the result is significant as dominating estimators of ==== will necessarily be minimax. Extensions to a much large class of loss functions (Corollary 2.1) are then obtained. An analogue minimax result (Theorem 2.3) is also established for an upper-bound restriction and implications for the potential minimaxity of estimators ==== under loss ==== are also addressed with Corollary 2.2.====The angle examined here can be described as one of ==== as loss (1.2) is linked to the risk function of the estimator being assessed for minimaxity. This interesting question is first studied in Section 2.1 with examples of non-minimaxity provided for estimators of a mean ==== of the type ====. Although such findings are not necessarily linked to exponential families, they do apply for families in (1.1) and we identify many cases of non ==== when the choice ==== is not made. Section 2.2 contains the principal minimax results described in the paragraph above, and examples are presented in Section 2.3. Section 3 deals with improved estimators under squared error loss of a lower-bounded expectation for discrete models using an adaptation of Kubokawa’s integral expression of risk difference technique. For Poisson and negative binomial models, the classes of improved estimators are minimax under normalized squared error loss by virtue of the minimax findings of Section 2.2.",Minimax estimation of a restricted mean for a one-parameter exponential family,https://www.sciencedirect.com/science/article/pii/S0378375820301038,2 November 2020,2020,Research Article,144.0
"Columbu Silvia,Mameli Valentina,Musio Monica,Dawid Philip","Department of Mathematics and Computer Science, University of Cagliari, Italy,Department of Economics and Statistics, University of Udine, Italy,Department of Pure Mathematics and Mathematical Statistics, University of Cambridge, UK","Received 19 March 2019, Revised 26 December 2019, Accepted 1 August 2020, Available online 1 November 2020, Version of Record 14 November 2020.",https://doi.org/10.1016/j.jspi.2020.08.004,Cited by (0),"In this work we study stationary linear time-series models, and construct and analyse “score-matching” estimators based on the Hyvärinen scoring rule. We consider two scenarios: a single series of increasing length, and an increasing number of independent series of fixed length. In the latter case there are two variants, one based on the full data, and another based on a sufficient statistic.====We study the empirical performance of these estimators in three special cases, autoregressive (AR), moving average (MA) and fractionally differenced white noise (ARFIMA) models, and make comparisons with full and pairwise ====. The results are somewhat model-dependent, with the new estimators doing well for MA and ARFIMA models, but less so for AR models.","Composite likelihoods methods have become an appealing tool, as alternative to the likelihood estimation method, in complex statistical models with interdependencies. The increasing importance of this methodology is due to its computational feasibility in a variety of applications. However, for the first order moving average model (MA(1)), the pairwise likelihood method, which is a special case of composite likelihood, has very poor asymptotic efficiency as the moving average parameter tends to the boundary of the parameter space (Davis and Yau, 2011). Composite likelihood estimation methods form a subset of a more general class of methods based on proper scoring rules, estimation being conducted by minimising the empirical score over distributions in the model (Dawid and Musio, 2014, Dawid et al., 2016). Some important proper scoring rules are the log-score (Good, 1952), which recovers the full (negative log) likelihood, the Brier score (Brier, 1950) and the Hyvärinen score (Hyvärinen, 2005). In the setting of MA(1) we consider alternatives to the pairwise likelihood approach, based on the theory of proper scoring rules, focusing on the Hyvärinen score. This score is a homogeneous proper scoring rule (see Ehm and Gneiting (2012) and Parry et al. (2012)), which is unchanged by applying a positive scale factor to the probability distribution. Homogeneous scoring rules have been characterised for continuous real variables (Parry et al., 2012) and for discrete variables (Dawid et al., 2012). In a Bayesian framework, Dawid and Musio (2015) have shown, for the case of continuous variables, how to handle Bayesian model selection with improper within-model prior distributions, by exploiting the use of homogeneous proper scoring rules. The discrete counterpart has been empirically studied by Dawid et al. (2017). In a recent contribution, Shao et al. (2019) consider the use of the Hyvärinen score for model comparison. Although the majority of contributions involving the use of Hyvärinen scoring rules focus on Euclidean spaces, scholars have also investigated extensions to non-Euclidean spaces: for an early study see Dawid (2007). Recently, Mardia et al. (2016) proposed an extension of the Hyvärinen scoring rule to compact oriented Riemannian manifolds, and Takasu et al. (2018) constructed a novel class of homogeneous strictly proper scoring rules for statistical models on spheres.====Given the growing interest in the use of this scoring rule, in this paper we aim to derive an estimation method based on the Hyvärinen scoring rule not only for moving average model but in general for estimating linear Gaussian time series models.====We distinguish two separate cases: a first in which the length of a single time series increases to infinity, and a second in which the length of the time series is fixed and the number of series increases to infinity.====The consistency and asymptotic distribution of the Hyvärinen estimator are derived for the case of a single time series of increasing length. In particular, under some mild regularity conditions we derive consistency of the proposed estimator for linear Gaussian time series models, and its asymptotic distribution is found in the specific case of autoregressive moving average (ARMA) causal invertible models. For time series with fixed length and the number of time series increasing to infinity the performances of two estimators based on the Hyvärinen scoring rule, namely ==== and ====, are compared through simulation studies with the full maximum likelihood and the pairwise maximum likelihood estimators. To evaluate the novel inferential procedure based on the Hyvärinen scoring rule we consider simple situations where the likelihood function is available. In particular, three simple time series models have been considered in the design of simulations: autoregressive (AR), moving average (MA) and fractionally differenced white noise (ARFIMA).====Different behaviours can be detected for the total Hyvärinen estimator among the settings examined. In particular, it outperforms the pairwise likelihood estimator in terms of efficiency for the MA and ARFIMA processes.====The paper unfolds as follows. Section 2 introduces basic notions on scoring rules. In Section 3 we introduce the Hyvärinen scoring rule for Gaussian linear time series. Some asymptotic results for the Hyvärinen estimator are given. In the specific case of ==== independent series we describe the total Hyvärinen estimator and the matrix Hyvärinen estimator. Section 4 summarises the results of the simulation studies on ==== time series of fixed length ====. Section 5 presents a simulation study for a single time series model and a simple application in a real case study. Section 6 provides some concluding remarks. Technical details are postponed to the Appendix.",The Hyvärinen scoring rule in Gaussian linear time series models,https://www.sciencedirect.com/science/article/pii/S037837582030104X,1 November 2020,2020,Research Article,145.0
Salamanca Juan Jesús,"Escuela Politécnica de Ingeniería, Departamento de Estadística e I.O. y D.M., Universidad de Oviedo, E-33071 Gijón, Spain","Received 23 January 2019, Revised 5 October 2020, Accepted 12 October 2020, Available online 1 November 2020, Version of Record 8 November 2020.",https://doi.org/10.1016/j.jspi.2020.10.005,Cited by (0),"The main goal of this paper is to study a new dispersive order in an arbitrary metric space. More precisely, given two random variables on a metric space, the order decides which one has a more concentrated distribution. It is motivated by the wish to be able to make new comparisons that the currently defined orderings cannot make.====Related to this problem, several statistical parameters of a random variable, giving different kinds of information about the distribution, are studied. It is proved that the new dispersive order also compares some of these parameters. Special attention is paid to ====The principal properties of the order are presented, as well as several applications. Finally, the independence of this order from those already known is proved.","There are important statistical questions that cannot be resolved by means of random variables of a vector space. Is the distribution on Earth of hurricanes more concentrated than the distribution of floods? Is the distribution of the phases of a signal wave more dispersed than another? In the first question, the sample space is the whole Earth, which can be modelled by the round sphere ====. This question appears in the hypothetical case that a global emergency agency must specialize in a catastrophe. In the second question, the sample space is the set of angles; equivalently, the circumference ====. This question is also interesting from a practical viewpoint.====These topics indicate the necessity of an ordering to determine which variable is less disperse (or more concentrated). In other words, a dispersive order that can be used in an arbitrary metric space.====Stochastic orderings have become an interesting and rich field of study. Numerous applications can be described, coming from Decision Theory, for example, or the theory of probability. They have been used in Economics (Atkinson, 1987, Montes et al., 2014), Finance (Levy and Levy, 2001, Ogryczak and Ruszczyński, 1999) and even Agriculture (Rajan and Vijayabalan, 2017), (see also Balakrishnan and Rao, 1998 and Sordo, 2008). In a more theoretical framework, when the sample space is not modelled by a vector space, some distinguished important comparison approaches do not make sense, e.g. stochastic dominance, or statistical preference, (Shaked and Shanthikumar, 2007). In fact, even the notion of expectation fails (instead, several definitions of an expectation set have been proposed, Molchanov, 2005).====Among the stochastic orders, the dispersive ones have their own place, (Shaked and Shanthikumar, 2007). Roughly speaking, a dispersive ordering is a decision technique to determine which random variable is preferable, in the sense of establishing which variable has less variation. For random variables of a finite-dimensional vector space, these orders have been considered extensively in Zamani et al. (2017) as well as in the handbooks Belzunce et al. (2015) and Shaked and Shanthikumar (2007) (see also Sordo et al., 2015). To cite some applications of this kind of order, recently they have been used for ranking forecasts (Ardakani et al., 2018), and for studying competitive bundling with an arbitrary number of firms (Zhou, 2017).====The main aim of this paper is to provide a suitable dispersive order for random variables of an arbitrary metric space. The motivation emerges from the difficulties that the existing dispersive orderings have. Firstly, some dispersive orderings make use of the vector properties of ====. Hence, they cannot be extended to the general case of a metric space (for instance, the multivariate Laplace transform order, Shaked and Shanthikumar, 2007). Secondly, the notion of a contraction (or expansion) map has serious flaws. In fact, those maps are isometries when the metric space is compact (see Lemma 5.1 and Hiraide, 1990). In particular, the strong dispersive ordering does not work well in a round sphere (see Remark 12). Furthermore, the dispersive orders based on these maps do not work either. Finally, the weak dispersive ordering is too strong to be considered in some cases. In fact, there exist random variables that are incomparable in terms of the weak dispersive ordering but their distributions are easily shown to admit such a dispersive order (see Remark 11). The last reason is that the weak dispersive ordering makes use of the distribution of ====, where ==== and ==== are independent, identically distributed random variables. In contrast, to compare two random variables ==== and ==== according to the new order, it is only necessary to make comparisons of the distributions of ==== and ====, where ==== are points of the metric space. That is, the distribution of a random variable is more concentrated at some point than the other random variable is at any other point (see Definition 3.1).====The moments of a random variable on a metric space are related to the new order. In fact, it will be shown how our dispersive order implies a comparison of such parameters. In particular, the new order implies that the (Fréchet) variance of a random variable is smaller than the other one (see Theorem 3.13).====Special attention is paid to Riemannian manifolds, which are a distinguished class of metric spaces. In fact, it is common to find data whose structure is a Riemannian manifold, or a submanifold of a Euclidean space, for instance, data from directional or orientational statistics, medicine or astronomy, (Mardia and Jupp, 2000). For the initial questions, the circumference ==== and the round sphere ==== are simple examples of Riemannian manifolds. Since this class of metric spaces can model a complex data structure, they have been used in numerous real cases (see Émery, 2012, Patrangenaru and Ellingson, 2015 and references therein for a wide exposition). In these rich metric spaces, probability has been studied (Jupp, 2015, Pennec, 2006), as well as estimation (Bhattacharya and Patrangenaru, 2002, Hendriks and Landsman, 1998, Hu et al., 2018), hypothesis testing (Bhattacharya and Dunson, 2012), and large sample theory (Bhattacharya et al., 2003, Bhattacharya and Patrangenaru, 2005, Diaconis et al., 2013). Moreover, asymptotic statistical inference on submanifolds has been investigated in Hendriks et al. (2007). To illustrate the importance of statistics on Riemannian manifolds, some of its applications can be mentioned, such as the processing of 3D ultrasounds (Roche et al., 2001), the analysis of rock fracture (Rasouli, 2002), and statistical learning (Said et al., 2018). In Fefferman et al. (2018), several techniques are developed for manifold learning from data belonging to a submanifold of a Euclidean space. From a more theoretical viewpoint, a study of the topology of the distributions on manifolds can be found in Bobrowski and Mukherjee (2015), and other relevant stochastic equations on manifolds are considered in Li (1994). Note also that it is normally assumed that the sample space is a locally compact Hausdorff second countable topological space (Molchanov, 2005). However, under a few extra hypothesis (in fact, differentiability), this space becomes a manifold (Petersen et al., 2006, O’neill, 1983).====In another setting, recall that spaces of functions arise as metric spaces when some distance is considered. Since the new order can be used on an arbitrary metric space, applications to functional data analysis could be found (see López-Pintado and Romo, 2011 for a measure on these spaces which can be used to rank curves).====This paper is organized as follows. In the next section, there are provided some preliminaries on random variables on metric spaces. There is also recalled the notion of a Riemannian manifold. With this as a basis, in Section 3, the new dispersive order is defined and its main properties are exhibited. In particular, the relationships of this order with the moments of a random variable are examined. Then, special attention is paid to Riemannian manifolds.====Section 4 focuses on model spaces (Euclidean space, the round sphere, and hyperbolic space, which are Riemannian manifolds with rich geometrical properties). Here, some interesting behaviour of the introduced order appears. Finally, Section 5 shows the independence of the new order from the most common ones.","A universal, canonical dispersive ordering in metric spaces",https://www.sciencedirect.com/science/article/pii/S0378375820301117,1 November 2020,2020,Research Article,146.0
"Singh Rakhi,Kunert Joachim,Stufken John","University of North Carolina at Greensboro, Greensboro, NC, USA,Technische Universität Dortmund, Dortmund, Germany","Received 31 March 2020, Revised 30 July 2020, Accepted 2 August 2020, Available online 1 November 2020, Version of Record 11 November 2020.",https://doi.org/10.1016/j.jspi.2020.08.003,Cited by (1),-lag orthogonal designs. We show that these ,"Functional magnetic resonance imaging (fMRI) measures cerebral blood flow, which is used as a surrogate for neuronal brain activity. When an area of the brain is stimulated, it leads to increased blood flow in that area. The fMRI uses the blood-oxygen-level-dependent (BOLD) contrast which is used to map neural activity in the brain of humans or other animals by imaging the change in blood flow (also known as, hemodynamic response) related to energy use by brain cells. Event-related functional magnetic resonance imaging (ER-fMRI) is a technique for studying brain activity in response to mental stimuli (for example, looking at pictures, watching a video, etc.). ER-fMRI is important and popular in neuroscience because this technique does not involve people to undergo brain surgery or to ingest substances or chemicals which may have harmful side-effects.====Experimental subjects are presented with a sequence of mental stimuli of one or more types and during this exposition, an MRI scanner repeatedly scans the subject’s brain to collect a time series from each brain voxel. For each voxel, the observed time series represents the cumulative effects of the stimuli. The effect of a single stimulus is modeled by a Hemodynamic Response Function (HRF), which could be different for different voxels. Estimating the HRF is often the main interest in an fMRI experiment (Lazar, 2008). For design selection, we focus on estimation of the heights of the HRF at selected time points after onset.====In this paper, we consider fMRI studies with one type of stimulus and focus on estimating the HRF function. The design problem for fMRI studies with one type of stimulus is to understand which 0–1 sequence out of the ==== sequences, where ==== is the length of the considered sequence, is the best sequence for estimating the HRF function. Considerations are based on a common model for fMRI studies that is discussed in Section 2.====Existing approaches for finding optimal fMRI designs mainly rely on computer search algorithms (Wager and Nichols, 2003, Kao et al., 2009a, Kao et al., 2009b for example). Recently, analytical results for stimuli of one or two types have been obtained (Kao, 2013, Kao, 2015 for example). To the best of our knowledge, the analytical results are only available for the situations when the observations are independent over time. Kao (2015), while discussing the analytical results for stimuli of two types, mentioned that the assumption of uncorrelated errors may sometimes be violated and it becomes important to find optimal designs for such situations analytically. The current state-of-the-art method to find the best designs is to use the genetic algorithm of Kao et al. (2009a). The algorithm can incorporate several models that occur in practice, and can handle the errors being temporally dependent. The designs provided by the algorithm are highly efficient but there is no surety that these designs are optimal.====In this paper, we study analytically obtaining highly efficient fMRI designs under the assumption of correlated errors. We start with the model that is considered in the papers with the assumption of independent errors (Kao, 2013, Kao, 2015) and introduce the correlated errors assuming an auto-regressive (AR) structure. Since it is quite hard to provide a general solution to the problem, we obtain the optimality results within a subclass of designs. In Sections 2 The model and the information matrix, 3 Optimal designs in, we discuss the model, information matrix, class of designs, optimal designs under our setup, and the definition of ====-lag orthogonal designs. Since the optimal designs may not exist, in Section 4, we identify ====-lag orthogonal designs that (a) are locally optimal when the correlation parameter is not too large, and (b) are highly efficient compared to the optimal designs obtained in this paper. Additionally, we propose a new construction method to obtain larger ====-lag orthogonal designs from smaller designs which provides alternate designs to the ones available in the literature. It is possible to have non-orthogonal designs that are better than the ones we have advocated the use of. Using a genetic algorithm (GA), we study the efficiencies of ====-lag orthogonal designs among all possible designs under the AR(1) structure. We identify ====-lag orthogonal designs that perform well as long as the parameter of AR(1) structure is not too large (efficiencies higher than 0.90). In addition, we also identify efficient ====-lag orthogonal designs for the AR(2) error structure, which sometimes is a more useful error structure in practice (Lenoski et al., 2008).",On optimal fMRI designs for correlated errors,https://www.sciencedirect.com/science/article/pii/S0378375820301014,1 November 2020,2020,Research Article,147.0
"Cao Mingxiang,Sun Peng,Park Junyong","Department of Statistics, Anhui Normal University, Wuhu, China,KLATASDS-MOE, School of Statistics, East China Normal University, Shanghai, China,Department of Statistics, Seoul National University, Seoul, Republic of Korea","Received 3 January 2020, Revised 8 September 2020, Accepted 11 September 2020, Available online 1 November 2020, Version of Record 15 November 2020.",https://doi.org/10.1016/j.jspi.2020.09.003,Cited by (0)," under null and local alternative hypotheses, respectively. Our asymptotic result for proposed test does not need some conditions such as linearity between the sample size and dimension used in existing studies. Simulation results also demonstrate our new test not only can control reasonably the nominal level but also has greater empirical powers than competing tests.","High-dimensional data are increasingly available in more and more fields such as molecular biology, genomics and fMRI. A common feature of high-dimensional data is that the data dimension is larger or much larger than the sample size, so called “large ====, small ====” phenomenon where ==== is the data dimension and ==== is the sample size. Because many classical statistical inference procedures are poor or not applicable at all in high-dimensional settings, there has a growing interest in developing new methods suitable to high-dimensional data. A special example is to test simultaneously the mean vector and the covariance matrix in high-dimensional settings. For example, the image data from fMRI are tested whether it is from a some known population or not. More specifically, ==== are ====-dimensional random samples from some population with unknown mean vector ==== and positive definite covariance matrix ====, of interest is to test the hypothesis ====where ==== and ==== are all known. For the hypothesis (1.1), without loss of generality, we can assume ==== and ====, where ==== and ==== denote respectively the ====-dimensional zero vector and the ==== identity matrix. Thus the hypothesis (1.1) can be rewritten as ====In the classical setting that the data dimension ==== is fixed and the sample size ==== tends to infinity, one of important tests for the hypothesis (1.2) is the classical likelihood ratio test, which can be found in Anderson (2003), that is ====in which ==== is the sample mean, ==== and ==== denote the trace and determinant of ====, respectively. It is known that ==== converges to chi-square distribution with ==== degrees of freedom as ==== tends to infinity. As the data dimension ==== goes to infinity, the asymptotic property of the classical likelihood ratio test is very interesting. On the basis of this, Liu et al. (2017) and Niu et al. (2019) all investigated the hypothesis (1.2) and proposed respectively different test statistics based on the classical likelihood ratio test ====. However, their papers require that ==== and ==== are proportional, viz. ==== in Liu et al. (2017) and ==== in Niu et al. (2019) as ====. This is partly because the ==== statistic includes ==== used in Bai and Saranadasa (1996) which requests the linear relationship between ==== and ====. On the other hand, Chen and Qin (2010) used the statistic excluding ==== from ==== so that their test statistic depends on ==== which can relax the linearity condition between ==== and ====. Similarly, in testing covariance matrix, Chen et al. (2010) do not need any explicit relationship between ==== and ====. In other words, this approach used in Chen and Qin (2010) and Chen et al. (2010) has some advantage in admitting implicit relationship of ==== and ==== instead of linear relationship. Additionally, Liu et al. (2017) and Niu et al. (2019) need the bounded spectral norm of ==== and the convergence of the empirical spectral density to that of ==== under the linear relationship between ==== and ====. In many practical problems, the data may be ultrahigh-dimensional, for example, in DNA microarray data analysis, the dimensionality of gene expressions may grow exponentially with the sample size. Considering these practical applications, the conditions in Liu et al. (2017) and Niu et al. (2019) are somewhat restrictive in contemporary high-dimensional testing problems, so it is desirable to eliminate those conditions such as the linear relationship between ==== and ==== and the bounded spectral norm of ====.====In this paper, we propose a newly simultaneous test statistic for the hypothesis (1.2). The proposed test does not require the relationship of the data dimension ==== and the sample size ====, thus it can be applied to both high and ultrahigh dimensional data. It will be shown that our new test has better behavior than competing tests in Liu et al. (2017) and Niu et al. (2019). We will demonstrate the difference between our proposed test and competing tests through numerical comparisons under a variety of situations.====An outline of this paper is as follows. Section 2 introduces the construction of the proposed test under some regularity conditions. In Section 3, we present the main results and give their proofs. Simulation studies are carried out in Section 4 to compare our proposed test with some existing tests. Section 5 contains some conclusions.",A simultaneous test of mean vector and covariance matrix in high-dimensional settings,https://www.sciencedirect.com/science/article/pii/S0378375820301051,1 November 2020,2020,Research Article,148.0
"Kalogridis Ioannis,Van Aelst Stefan","Department of Mathematics, KU Leuven, Celestijnenlaan 200B, 3001 Leuven, Belgium","Received 23 March 2020, Revised 14 June 2020, Accepted 2 September 2020, Available online 1 November 2020, Version of Record 11 November 2020.",https://doi.org/10.1016/j.jspi.2020.09.004,Cited by (5)," of M-type penalized spline estimators have not been studied. We show in this paper that M-type penalized spline estimators achieve the same rates of convergence as their least-squares counterparts, even with auxiliary scale estimation. We illustrate the benefits of M-type penalized splines in a Monte-Carlo study and two real-data examples, which contain atypical observations.","Based on data ==== with fixed ==== and ====, consider the classical nonparametric regression model ====where ==== is a sufficiently smooth function which we shall endeavour to estimate. The ==== are independent and identically distributed error terms, commonly assumed to have zero mean and finite variance ====.====Nonparametric regression has been a burgeoning field for many years and many ingenious methods have been proposed for the estimation of the regression function ====. These methods broadly comprise kernel regression, orthogonal series, splines and wavelets, see, e.g., Wasserman (2006) for an overview. In this paper, we focus on robust estimation with penalized splines. Owing to their ease of fitting and flexible choice of knots and penalties, penalized splines have been exceedingly popular in recent years following the seminal works of O’Sullivan (1986) and Eilers and Marx (1996). Penalized splines offer a compromise between the simplicity of (unpenalized) regression splines and the computational complexity of smoothing splines, see Wahba (1990, Chapter 7) for this point. Asymptotic properties of least-squares penalized spline estimators have been studied by Hall and Opsomer (2005), who established their consistency, Li and Ruppert (2008), who derived the equivalent kernel representation for lower-order splines, and more broadly by Claeksens et al. (2009), who obtained rates of convergence for penalized splines with derivative-based penalties under various combinations of the number of knots, the sample size and the penalty. More recently, Xiao (2019) unified the study of the large sample properties of spline estimators and extended the results of Claeksens et al. (2009) to a number of popular spline estimators, including splines with difference penalties (P-splines).====It is well-known that estimators derived from the minimization of an ==== norm are susceptible to atypical observations. That is, a single gross outlier can significantly distort the estimates as well as inferences based on them. This fact has motivated proposals that aim to achieve some degree of resistance towards outlying observations. Lee and Oh (2007) proposed replacing the squared loss with a loss function that increases more slowly and supplied an algorithm based on pseudo-observations. In the same vein, Tharmaratnam et al. (2009) proposed minimizing a robust scale of the residuals with an additional penalty term in order to obtain resistant estimates. However, despite the well-studied theoretical properties of least-squares penalized splines, it is curious that no asymptotic results have been established outside that framework. This comes in stark contrast to robust smoothing and regression splines whose asymptotic properties have been established as early as Cox (1983) and Shi and Li (1995) with significant extensions with respect to scale estimation offered by Cunningham et al. (1991) and He and Shi (1995) respectively.====To fill this gap we study the consistency and establish rates of convergence of general M-type penalized spline estimators with a derivative-based penalty. Moreover, we also consider the case where a preliminary scale estimator is used to standardize the residuals in the loss function. As mentioned previously, this type of estimator was already considered by Lee and Oh (2007) from a computational point of view, but without any theoretical support. Our approach further differs from their proposal in three key aspects. Firstly, we use the nearly orthogonal B-spline basis instead of the badly conditioned truncated polynomials and derive theoretical properties based on that representation. Secondly, we use a robust preliminary scale estimate that does not require model fitting as opposed to the originally proposed concomitant scale estimate. This dramatically reduces the computational burden as there is no need anymore to iterate between coefficient and scale estimates. Finally, we propose a fast, effective and automatic method of selecting the penalty parameter which has the advantage that the penalty parameter no longer needs to be chosen at each iteration of the algorithm.====The rest of the paper is structured as follows. We review a few basic facts about spline estimation in Section 2 and describe the proposed M-type penalized estimator and its computation in Section 3. In Section 4 the asymptotic properties of the estimator are studied. We obtain an asymptotic expansion of the estimator and show that it achieves the optimal rates of convergence, even with auxiliary scale estimation. Section 5 examines the performance of the proposed method via a Monte-Carlo study while Section 6 illustrates the advantages of its use in two real data examples. Finally, some possible directions for future research are discussed in the concluding section of this paper.",M-type penalized splines with auxiliary scale estimation,https://www.sciencedirect.com/science/article/pii/S0378375820301087,1 November 2020,2020,Research Article,149.0
"Liu Yin,Tian Guo-Liang,Wang Mingqiu","School of Statistics and Mathematics, Zhongnan University of Economics and Law, Wuhan City, Hubei Province, PR China,Department of Statistics and Data Science, Southern University of Science and Technology, Shenzhen City, Guangdong Province, PR China,School of Statistics, Qufu Normal University, Qufu City, Shandong Province, PR China","Received 11 October 2019, Revised 13 July 2020, Accepted 2 August 2020, Available online 1 November 2020, Version of Record 11 November 2020.",https://doi.org/10.1016/j.jspi.2020.08.006,Cited by (1)," variable and a set of non-sensitive ====, where the information about the sensitive attribute of interest is collected via the variant of the parallel model originally proposed by Liu and Tian (2013b). The EM–NR algorithm is provided to derive the ","In practice, some investigations deal with phenomena that are considered as violation of social morality or illegal activities (such as bribery, tax evasion, illegitimate child, illegal immigration, drug abuse and so on) will make interviewees feel embarrassed and refuse to provide truthful answer when they were asked directly. Thus, to better protect individuals’ privacy as well as encourage truthful answers, studies on helping collecting sensitive information have been developed rapidly during the last decades. Up to now, three main branches of techniques are introduced for collecting and analyzing data in sample surveys with sensitive questions: the randomized response techniques (Warner, 1965, Horvitz et al., 1967, Greenberg et al., 1969, Fox and Tracy, 1986, Chaudhuri and Mukerjee, 1988, Mangat, 1994, Chaudhuri, 2011), the item count techniques or the unmatched count techniques (Miller, 1984, Dalton et al., 1994, Kuklinski et al., 1997, Gilens et al., 1998, LaBrie and Earleywine, 2000, Tsuchiya, 2005, Janus, 2010, Imai, 2011, Petróczi et al., 2011, Tian et al., 2017, Liu et al., 2019) and the non-randomized response techniques (Tian et al., 2007, Tian et al., 2009, Tian G.L et al., 2011, Yu et al., 2008, Tan et al., 2009, Tang et al., 2009, Liu and Tian, 2013a, Liu and Tian, 2013b, Groenitz, 2014, Tian, 2015).====Only focusing on raising new survey models for sensitive information collection with well privacy protection is not enough, sometimes, researchers may also be interested in finding out which factors may have non-ignorable influence on such sensitive attributes of interest. Thus, to construct appropriate regression models is necessary and the logit model is often a good choice for binary response variable. For the randomized response approaches and the item count approaches, several works have been done on establishing a connection between the binary sensitive respond and other non-sensitive potential explanatory variables (see Maddala, 1983, Scheers and Dayton, 1988, Corstange, 2004, van den Hout et al., 2007, Hsieh et al., 2010). Note that the non-randomized response methods contain advantages such as reproducibility, low-cost, easy understanding for both interviewers and interviewees, strong operability, better privacy protection and so forth, it is worthy of our efforts to discuss regression analyzing ways under the non-randomized response models. However, in the field of the non-randomized response approaches, only Tian et al. (2019) proposed a so-called hidden logit parallel model that taking non-sensitive covariates into consideration while the collection of the sensitive information is aided by Tian’s non-randomized parallel model (Tian, 2015). Nevertheless, the parallel model given by Tian asked the successful proportions of the two auxiliary non-sensitive binary variables should be known before experiments, which may certainly restricted the application range of this model. Luckily, the variant of parallel model put forward by Liu and Tian (2013b) could better overcome such a limitation by letting one successful proportions of the two auxiliary non-sensitive variables to be known while the other one keeps unknown. Therefore, the first objective of this paper is to develop a new variant of parallel regression model to detect elements being responsible for the sensitive attribute of interest.====To our best knowledge, little work has been done on confounding variable identification and the variable selection for analysis in surveys with sensitive characteristics, especially for non-randomized response approaches. However, in regression analysis, other than providing effective regression estimators, these problems are two of the important issues that deserve our attention. Both contribute a lot to enhance the accuracy and precision of the regression model. For the former one, since the confounder has impact on both the explained variable and the explanatory variable(s), it may distort the observed relationship between the exposures and outcomes (Greenland et al., 1999, Pearl, 2009, VanderWeele and Shpitser, 2013). Thus, it is of great necessity to identify the confounding variable(s) in regression analysis such that the true associativity between dependent and independent variables can be revealed. On the other hand, for the latter one, it is helpful in reducing the complexity of the model to a great extend since a large number of covariates are usually chosen at the initial stage of modeling aiming at reducing possible modeling biases. However, too many redundancy variables go against the most basic modeling principle, i.e, the principle of parsimony; that is, the model used should require the smallest possible number of parameters that will adequately represent the data. Various kinds of variable selection techniques have been developed to help selecting significant factors by adding a penalty to the objective function (see Frank and Friedman, 1993, Tibshirani, 1996, Fan and Li, 2001, Zou and Hastie, 2005, Zou, 2006, Zhang, 2010, Wang and Wang, 2014, Wang and Wang, 2016). Fan and Li (2001) have pointed out that a good penalty function could help to obtain an estimator with the following three properties:====Consequently, the second objective of this paper is to provide a method of identifying the confounding variable(s) and the third objective is to derive the variable selection technique for the proposed variant of the parallel regression model while the ==== (SCAD) method introduced by Fan and Li (2001) is adopted.====The rest of the paper is organized as follows: In Section 2, we first propose the variant of the parallel regression model and then, a Expectation–Maximization embedded with Newton–Raphson (EM–NR) algorithm is derived for calculating the MLEs of the regression coefficients. In Section 3, we provide the approach for identifying confounding variable(s). The method for variable selection based on SCAD penalty was discussed in Section 4, and in the same section, we also present the asymptotic properties for the penalized estimators. Four simulation studies are performed in Section 5 and a real data example about premarital sexual behavior is analyzed in Section 6 to illustrate the proposed methods. Finally, a discussion is given in Section 7.",A new variant of the parallel regression model with variable selection in surveys with sensitive attribute,https://www.sciencedirect.com/science/article/pii/S0378375820301129,1 November 2020,2020,Research Article,150.0
"Li Yuanbo,Chan Ngai Hang,Yau Chun Yip,Zhang Rongmao","University of International Business and Economics, Beijing, China,Southwestrn University of Finance and Economics, Chengdu, China,The Chinese University of Hong Kong, Hong Kong, China,Zhejiang University, Hangzhou, China","Received 6 March 2019, Revised 16 July 2020, Accepted 2 August 2020, Available online 24 October 2020, Version of Record 10 November 2020.",https://doi.org/10.1016/j.jspi.2020.08.002,Cited by (3),", originated from the high-dimensional variable selection context, is developed for efficiently screening out potential break-points in the first step. A high-dimensional information criterion is proposed for consistent structural breaks estimation in the second step. In the third step, the information criterion further determines the specific components in which structural breaks occur. Monte Carlo experiments are conducted to demonstrate the finite sample performance, and applications to stock data are provided to illustrate the proposed method.","As a simple and intuitive means to analyze nonstationary behaviors in time series, the structural break models received much attention in recent years. An important and fundamental task of this kind of models is to detect the number and the locations of change-points. For univariate time series, various detection methods have been proposed. For example, Adak (1998) uses an adaptive segmentation method in the spectral domain to partition the data into approximate stationary intervals. Ombao et al. (2001) propose the smooth localized complex exponential (SLEX) basis for analyzing bivariate nonstationary time series. Bai and Perron (2003) develop a dynamic programming algorithm to estimate break points for multiple linear regression. Davis et al. (2006) employ the genetic algorithm (GA) to estimate the locations of the change-points based on minimum description length (MDL) principle. Killick et al. (2012) propose the pruned exact linear time (PELT) method, which modifies the dynamic programming method for change-points detection. Fryzlewicz (2015) uses the wild binary segmentation (WBS) technique, which is demonstrated to be better than traditional binary segmentation technique. Chan et al. (2014) re-frame the change-point problem into a regression variable selection context and propose a combined method to estimate piecewise stationary autoregressive models (PSAR) by the least absolute shrinkage and selection operator (LASSO) estimation. The procedure is computationally efficient even when the number of change-points is large and unknown. All the aforementioned methods, however, are only developed for univariate time series.====For multivariate time series, the change-points problem is more challenging due to the correlation between different components. Aue et al. (2009) develop a testing approach to detect the change in the covariance structure. Bai (2010) proposes a least squares method to detect a single common change-points in panel data. Jirak (2015) and Cho (2016) consider estimators based on the CUSUM statistics. Kirch et al. (2015) extend the class of score-type change point statistics to the vector autoregressive model. Cho and Fryzlewicz (2015) use a sparsified binary segmentation algorithm for the second-order structural break. Preuss et al. (2015) propose an nonparametric procedure, referred to as MuBreD, to detect structural breaks in the auto-covariance function of a piecewise stationary process. Gao et al. (2020) decompose the problem of change-points detection in piecewise stationary vector autoregressive models along the component series and apply the group lasso method on each component.====Although many studies consider change-point detection for multivariate time series, most of them assume that changes occur in all components simultaneously. Moreover, few of them investigate the interrelationship among components in the presence of change-points. Consider a realization of a three-dimensional piecewise stationary time series depicted in Fig. 1. From the time series plot, it seems apparent that a change-point occurs in all three components at around ====. However, with reference to model (4.25) that generates the data, only the last two components have a change in the autoregressive structure at time ====. The observed “change” in the first time series is induced by the change in the values of the other two components, but not from a structural change in itself. We call this representative phenomenon in multivariate time series the ====. In conclusion, besides the number and the location of the change-points, detecting the specific components in which change-points occur is an important issue in multivariate time series change-point problems. To the best of our knowledge, the issue of induced change-point has not been considered in the literature.====The main goal of this article is to propose a computationally efficient algorithm for multiple change-point detection in piecewise stationary vector AR (PSVAR) models. We allow the number of change-points to go to infinity with the growth of sample size. Also, the components do not necessarily change simultaneously. Along the direction of Chan et al. (2014), we transform the PSVAR model to a high-dimensional multivariate regression model. The original change-points detection problem is re-framed into a high-dimensional group variables selection task. Therefore, computationally efficient tools from high-dimensional variable selection can be employed. However, the LASSO step used in Chan et al. (2014) and Gao et al. (2020) is well known to be biased and tends to select redundant variables to compensate for its over-shrinkage of large coefficients (e.g., Fan and Li (2001) and Zou (2006)). Fan and Lv (2008) point out that LASSO is not a suitable screening procedure in high-dimensional settings. These problems become more acute in a change-points detection context because the variables in the transformed regression model are highly correlated (see Section 2). To tackle this problem, we borrow other tools from high-dimensional regression to replace LASSO. Specifically, we develop a three-step method for change-point estimation. The first step is a greedy screening algorithm called group orthogonal greedy algorithm (GOGA), which effectively detects the relevant variables as a set of potential change-points. In the second step, we propose a high dimension information criterion (HDIC) which selects the best groups of variables based on the solution path obtained by the GOGA and obtain the estimated change-points. In the third step, the proposed method uses the HDIC to further detect the specific components in which change-points occur. We establish the almost sure screening property and the consistency of the change-points estimation of the proposed method. That is, the potential change-points selected from the first step would include all true change-points as ==== in the sense that each true change-point is an element of a neighborhood of a potential change-point with a probability approaching one as ====. The second and third steps consistently estimate the number and locations of the true change-points of each component in probability. Simulation studies demonstrate that the three-step procedure outperforms the LASSO approach proposed by Chan et al. (2014).====This paper is organized as follows. In Section 2, we illustrate the connection between PSVAR model and high-dimensional regression problem. In Section 3, we introduce the three-step procedure based on GOGA and HDIC for change-point estimation and establish the consistency property. Simulation studies and applications are given in Sections 4 Simulation studies, 5 Applications to real data, respectively, and technical details are deferred to the Appendix.",Group orthogonal greedy algorithm for change-point estimation of multivariate time series,https://www.sciencedirect.com/science/article/pii/S0378375820301002,24 October 2020,2020,Research Article,151.0
"Wang WenWu,Shen Wei,Tong Tiejun","School of Statistics, Qufu Normal University, Jining, Shandong, China,Department of Mathematics, Hong Kong Baptist University, Hong Kong","Received 24 April 2018, Revised 22 March 2019, Accepted 22 July 2020, Available online 21 August 2020, Version of Record 4 September 2020.",https://doi.org/10.1016/j.jspi.2020.07.007,Cited by (0),"In this paper, we propose a robust method for the estimation of ==== are established. In simulations, the new estimator has the less ==== than its main competitors in the presence of platykurtic and heavy-tailed errors.","In nonparametric estimation of regression function, the local polynomial least squares (LS) regression is a successful and popular method, and its asymptotic theory has been well studied in the literature (Fan and Gijbels, 1996). If the errors are normally distributed, the LS estimator is the most efficient estimator and has the likelihood interpretation (Fan et al., 1998). Facing the outliers and heavy-tailed errors, the LS estimator is not robust and its efficiency cannot be guaranteed, i.e., for the Cauchy distribution. Therefore, it is important to develop robust and efficient estimation methods for many applications such as finance and economics (Zhao and Xiao, 2014).====Traditionally, there were two kinds of robust procedures: locally weighted LS regression and local least absolute deviation (LAD) regression. Locally weighted LS procedure aims to reduce the influence of outliers by assigning the down-weight to outliers, and in spirit is locally linear, including locally weighted polynomial LS fitting (Cleveland, 1979), kernel M-smoother method (Härdle and Gasser, 1984), and spline smoother (Silverman, 1985). Local LAD regression is a different procedure. Tukey (1977) proposed various modifications of local median smoothing, Fan and Hall (1994) proposed a framework and gave its asymptotic efficiency. Wang and Scott (1994) proposed the local polynomial LAD regression to further reduce the bias. Welsh (1996) considered the estimation of the regression function and spread functions and their derivatives. Brown et al. (2008) further proposed the wavelet median regression. These methods are robust for outliers and heavy-tailed errors, meanwhile need to satisfy an implied condition to keep the estimation efficiency: the error density is leptokurtic. If the density function value at median becomes smaller, the estimation variance becomes larger and thus the estimation efficiency is lower.====To improve the efficiency, the quantile regression (Koenker and Bassett, 1978, Koenker, 2005) provides a useful technique. Yu and Jones (1998) and Härdle et al. (2013) proposed the local quantile regression. For certain distributions, a quantile estimator at non-median may deliver a more efficient estimator than the LAD estimator. Since quantile regression provides a way of exploiting the distribution information, combining information over multiple quantiles may improve the estimation efficiency. Kai et al. (2010) proposed the local composite quantile regression (CQR) estimator by simple averaging multiple quantile regression estimators, which is asymptotically equivalent to the local LS estimator as the number of quantiles goes to infinity. Fan et al. (2018) generalized CQR to the single-index model. Zhao and Xiao (2014) proposed the locally weighted quantile average (WQA) estimator by optimally weighting multiple quantile regression estimators, which achieves the Cramer–Rao lower bound of variance as the number of quantiles goes to infinity. However, the asymptotic properties of CQR and WQA estimators depend on the information at all quantile points. In practical applications, both estimators are a generalization of the LAD estimator, from one quantile to finite quantiles. The estimation efficiency depends on the error density values at finite quantiles.====In this paper, we propose a new and robust method to deal with platykurtic errors and outliers. By the additive transformation of the original data, we change platykurtic errors into peak errors; then we use the local LAD regression to estimate the regression function. The new function estimation is robust for outliers and heavy-tailed errors, meanwhile improve the estimation efficiency for platykurtic errors.",Robust estimation of nonparametric function via addition sequence,https://www.sciencedirect.com/science/article/pii/S0378375820300896,21 August 2020,2020,Research Article,152.0
"Satter Faysal,Zhao Yichuan","Department of Mathematics and Statistics, Georgia State University, Atlanta, GA 30303, USA","Received 23 January 2018, Revised 23 July 2020, Accepted 29 July 2020, Available online 18 August 2020, Version of Record 26 August 2020.",https://doi.org/10.1016/j.jspi.2020.07.009,Cited by (5)," methods inefficient in such cases. In this paper, jackknife empirical likelihood (JEL) and adjusted jackknife empirical likelihood (AJEL) methods are proposed to construct a nonparametric confidence interval for the mean difference of two independent zero-inflated skewed populations. The JEL and AJEL confidence intervals are compared with the confidence intervals by normal approximation and empirical likelihood proposed by Zhou and Zhou (2005). Simulation studies are performed to assess the new methods. Two real-life datasets are also used as an illustration of the proposed methodologies.","Many disciplines, such as environmental studies, ecology, biology, biometrics, epidemiology, insurance, meteorology, manufacturing, etc., have the potential to generate datasets containing many zero values, and non-zero values are highly positively skewed. These zeros are valid response outcomes, and therefore should not be ignored. For example, in estimating the mean diagnostic cost (testing charges) of a hospital, it is essential to use the diagnostic cost of all the patients. However, many patients may not have done any diagnostic tests during the period of interest, which will result in many zero test charges. Another example is the rainfall observation records, where there could be no rain for some days.====Various parametric/semiparametric methods have been developed to deal with zero-inflated datasets with the assumption of the population distribution. Lachenbruch (1976) assumed some parametric families, including exponential, log-normal distribution, for positive values and compared two groups using the so-called “two-part” test. Duan et al. (1983) used a two-part model under the assumption of the log-normal distribution for positive values. Zhou and Tu, 1999, Zhou and Tu, 2000 also employed a two-part model, which is a combination of binomial and log-normal distributions. However, using these methods depends on the justification of the assumptions about the distribution of the population.====Empirical likelihood method, first introduced by Owen, 1988, Owen, 1990, Owen, 2001, is a nonparametric method for small samples with superior performance. Advantages of empirical likelihood (EL) include data-driven confidence regions, transformation invariance, and many more. Also, DiCiccio et al. (1991) showed that the Bartlett correction improves the coverage rate from ==== to ==== for the sample size ====, which is a notable improvement compared with classic EL. Zhou and Zhou (2005) proposed the empirical likelihood method to find the confidence interval for the mean difference of two zero-inflated skewed populations. Pailden and Chen (2013) also developed a similar empirical likelihood method for two zero-inflated populations.====Nevertheless, the empirical likelihood method involving non-linear estimation equations is complicated and computationally intensive. Jing et al. (2009) proposed jackknife empirical likelihood (JEL) method to overcome this computational difficulty. This method essentially converts the statistic of interest into a sample mean using jackknife pseudo-values. It then applies empirical likelihood procedures on the mean of those pseudo-values (see Jing et al., 2009). This JEL method makes the estimation problem much simpler and computationally efficient. For the inference of only a part of parameters, Li et al. (2011) implemented JEL in the profile empirical likelihood.====Furthermore, Chen et al. (2008) proposed an adjusted empirical likelihood (AEL) method to remove the zero convex hull problem in computing the profile likelihood function. They added one more data point to the original dataset, and it improved the coverage probability substantially. Liu and Yu (2010) studied the Bartlett correction on two-sample adjusted empirical likelihood. More work was done by Liu and Chen (2010), who considered adjusted empirical likelihood for higher order precision. Zhao et al. (2015) proposed adjusted jackknife empirical likelihood for the mean absolute deviation. Chen and Ning (2016) later combined JEL and AEL methods and proposed an adjusted jackknife empirical likelihood (AJEL) for one-sample and two-sample ====-statistics. Another example of using artificial data points is Chen et al. (2015). They considered the high-dimensional data setting when the sample size and the data dimension are comparable. In that setting, they dealt with the convex hull problem by adding two more data points to the data set. They showed the asymptotic normality of the empirical log-likelihood ratio statistic. More recently, Cheng et al. (2018) proposed balanced augmented jackknife empirical likelihood for two-sample ====-statistics, where they added two artificial data points to the pseudo-values produced by the jackknife resampling process.====The aim of this paper is to find a better inference method for the mean difference of two independent zero-inflated skewed populations. The empirical likelihood method proposed by Zhou and Zhou (2005) involves finding solutions to complicated non-linear equations. Zhou and Zhou (2005) treated ==== and ====, which are the probabilities of having zero values in the two populations, as the nuisance parameters and resulting Wilks’ statistic was maximized over ==== and ==== (see the details in Section 2). This EL formulation added more computational costs. To alleviate this complexity, we propose the JEL method, which makes the estimation process simpler and computationally more efficient. The idea is to first estimate ==== and ==== consistently. Then, we use jackknife pseudo-values to construct EL ratio for the mean difference. Motivated by Wang and Zhao (2016) and Alemdjrodo and Zhao (2019), we also propose AJEL method, which improves the estimation performance in terms of coverage accuracy.====The rest of the paper is organized as follows. Jackknife empirical likelihood and adjusted jackknife empirical likelihood methods for the mean difference are proposed in Section 2. In Section 3, we carry out extensive simulation studies. We apply two real-life datasets to illustrate the proposed methodology in Section 4. Section 5 includes the conclusion and some remarks. All proofs are provided in Appendix.",Jackknife empirical likelihood for the mean difference of two zero-inflated skewed populations,https://www.sciencedirect.com/science/article/pii/S0378375820300914,18 August 2020,2020,Research Article,153.0
"Xu Jin,Mu Rongji,Xiong Cui","School of Statistics, East China Normal University, Shanghai 200062, China,Key Laboratory of Advanced Theory and Application in Statistics and Data Science-MOE, East China Normal University, Shanghai 200062, China,Department of Statistics, GlaxoSmithKline (China) R&D Company Limited, Shanghai 200120, China","Received 7 May 2019, Revised 19 July 2020, Accepted 21 July 2020, Available online 30 July 2020, Version of Record 5 August 2020.",https://doi.org/10.1016/j.jspi.2020.07.006,Cited by (2),. The method features adaptive local modeling and nonrecursive iteration. Consistency of the ==== is established. Simulation studies show its superiority in small-sample performance to Robbins–Monro type procedures. Extension to a version of generalized multivariate ==== is presented.,"We consider the problem of finding the unique root ==== of a unknown function ==== in the regression model ====where ==== is unobservable random error and ==== is continuous with ==== for ==== and ==== for ====. The approach by stochastic approximation uses a sequential design strategy to successively choose ==== on which the response ==== is observed with mean ==== so that ==== converges to ==== in some sense. Such a response-adaptive design is attractive and often more efficient than the fixed sample design. Over the years, stochastic approximation and its variants have broadened to applications in design of experiments, clinical trials, dynamic programming, and sequential learning, to name just a few (Kushner and Yin, 1997, Spall, 2003).====Here we give a brief review of some major progresses. In the fundamental paper of Robbins and Monro (1951), they proposed a recursive design of the form ====where ==== a positive constant sequence, and showed that ==== converges to ==== in probability when ==== and ==== assuming ==== satisfies some regularity conditions. It is a stochastic analogy to the deterministic Newton’s method where ==== (the prime denotes the first derivative) and is referred as Robbins–Monro procedure. The almost sure convergence was later proved through different approaches (Dvoretzky, 1956, Gladyshev, 1965, Robbins and Siegmund, 1971). Inspired by the Lyapunov functions in the stability theory of ordinary differential equations, Sacks (1958) established the asymptotic normality of ==== and showed that under certain regularity conditions the asymptotically optimal choice of ==== in (2) is ==== where ====. (See also Chung, 1954, Burkholder, 1956, Hodges and Lehmann, 1956.)====Ever since, many attempts have been made to estimate ====. Lai and Robbins, 1979, Lai and Robbins, 1981 proposed an adaptive Robbins–Monro procedure of the form ====where ==== is a truncated version of the least squares estimate of the regression slope given by ==== and ====. Strong consistency of ==== was established (Lai and Robbins, 1981, Lai and Robbins, 1982). We refer the readers to Venter, 1967, Anderson and Taylor, 1976, Anbar, 1978 and Anderson and Taylor (1979) for some related versions. An excellent review about these variants is given by Lai (2003).====In a different route, Ruppert (1988) and Polyak and Juditsky (1992) proposed using averaged trajectories of (2), ====, to estimate the root and demonstrated the almost sure convergence when ==== satisfies the condition of being sufficiently slowly decreasing in the sense of ==== and ====.====An important case of (1) is when ==== is a distribution function and ==== is binary response. Then, the Robbins–Monro procedure for finding the ====-quantile of ====, assuming it is unique, is given by ====The corresponding adaptive version is ====.====The rationale of these procedures is clear. When observing a ‘success’ at the ====th step (such as explosion in the sensitivity experiment or occurrence of adverse events in dose-finding clinical trial), reduce the current level for the next design point; when observing a ‘failure’, increase the current level for the next design point. As the number of iterations increases, the magnitude of change converges to zero. This type of scheme is in a similar spirit to the ‘up-and-down’ method (Dixon, 1965) for estimating the median in sensitivity experiments. To estimate ==== in the binary data case, Wu (1985) proposed fitting a two-parameter logit model for the available data to obtain an initial maximum likelihood estimate (MLE) of ====. Some initial runs are required to have the condition for the existence and uniqueness of this MLE met. (See also Sitter and Wu (1993).) An important contribution by Joseph (2004) is the proposal of an efficient Robbins–Monro procedure which entails the recursion ====where ====
 ====, ====, ==== and ==== are respectively the distribution function and density of the standard normal variable. The introduction of the constant sequence ==== helps reduce the oscillation of ==== at early steps. It has shown to have a faster convergence than the usual Robbins–Monro procedure when ==== takes extreme values. Wu and Tian (2014) proposed a three-phase design that combines some initial design and Joseph’s efficient modification to obtain a more steady method. Recently, Toulis and Airoldi (2015) proposed an implicit stochastic approximation method which improves the classic Robbins–Monro procedure through a stochastic fixed-point equation. It requires running many additional experiments at every step of (2). Thus, it may not be feasible for a small sample design.====Other model-based designs for quantal response focus on the estimation of the coefficients of a parametric model (Wu, 1986, Chaloner and Larntz, 1989, Chaudhuri and Mykland, 1993, Neyer, 1994, Dror and Steinberg, 2006, Dror and Steinberg, 2008, Hung and Joseph, 2014, Wu and Tian, 2014). The advantage of this approach is that one can use a single design to estimate the global response curve that includes all quantiles. While the disadvantages are that i) it makes assumptions about the model and/or hyperparameters; and ii) the designs usually require as many as ten or even more points as initial data to start with. Hung and Joseph (2014) proposed a simple Bayesian version of Wu (1985)’s logit-MLE method, which makes the design fully sequential from ====. It postulates independent informative priors on the parameters of a logistic model for ==== given by ==== with ==== and ====. And the sequential design estimates the ====-quantile as ====, where ==== and ==== are the maximum-a-posteriori (MAP) estimate of ==== after ==== samples.====In this paper, we limit our study to the root finding problem. We point out several limitations associated with the Robbins–Monro type procedures. First, for these algorithm-based procedures such as (2), the averaged trajectory of (2), (5), the adaptation through the last experiment point ==== via recursion is subject to information loss. Experiments at points in a neighborhood would carry useful information for ==== as well especially in early stages. Second, large oscillation caused by these up-or-down recursions in early iterations can be harmful and inefficient. Third, for the procedures such as (3) that rely heavily on the estimation of ====, as ==== clusters around ====, little information is gained to estimate ==== directly. So even with a consistent estimator, the methods may underperform with finite sample from a practical point of view.====On the other hand, the Bayesian paradigm is known to be suitable for such an adaptive learning problem. Applications in a closely related problem of dose-finding in clinical trials have been reported (Anbar, 1984, O’Quigley and Chevret, 1991, Cheung, 2010, Thall, 2010). Like Hung and Joseph (2014)’s method, Bayesian models are used to update the underlying distribution ====. Little has been seen for solving the ==== root for ====-quantile directly. Using martingale theory, Hu (1998) established the strong consistency of the Bayes estimator under a general setting of a nonlinear regression model. We will make use of this result for later development.====Motivated by the aforementioned drawbacks of the Robbins–Monro type methods and the advantage of Bayesian approach, we propose a novel model-based stochastic approximation procedure that circumvents direct estimation of ==== through integration. More specifically, the new method builds a local linear model for ==== around ==== and obtains the Bayes estimator as a nonrecursive solution for ====. Strong consistency is obtained. These constitute the main contents of Section 2. In Section 3, we give a few important remarks and insights of the proposed method. In Section 4, we compare the proposed method with some existing methods and demonstrate its superiority in finite-sample performance. In Section 5, we apply the new method to the general root-finding problem. In Section 6, we extend the proposed method to estimate a version of generalized multivariate quantile. Section 7 concludes the paper with some discussions.",A Bayesian stochastic approximation method,https://www.sciencedirect.com/science/article/pii/S0378375820300884,30 July 2020,2020,Research Article,154.0
"Ai Mingyao,Huang Yimin,Yu Jun","LMAM, School of Mathematical Sciences and Center for Statistical Science, Peking University, Beijing 100871, China,School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 100081, China","Received 12 September 2019, Revised 17 July 2020, Accepted 21 July 2020, Available online 30 July 2020, Version of Record 10 August 2020.",https://doi.org/10.1016/j.jspi.2020.07.008,Cited by (0),"In recent years, the multi-armed bandit problem regains popularity especially for the case with ","Multi-armed bandit problem introduced in Robbins (1952) is an important class of sequential optimization problems. It is widely applied in many fields such as statistics, operations research, engineering, computer science and economics. The traditional settings of multi-armed bandit problem can be described as follows. There are ==== arms. At each time, one can pull only one of them and obtains a random reward from this arm. The objective is to design a sequential policy that maximizes expected cumulative rewards. Obviously, if one knows the largest mean of these probability distributions, the same best arm will always be pulled. This policy is called an ==== policy. The difference between the oracle policy and a designed policy is called ====. Equivalently, the objective can also be viewed as minimizing the corresponding regret.====Lai and Robbins (1985) gave an asymptotic lower bound for the regret in the classic multi-armed bandit problem and proposed an index strategy that achieved this bound. Lai (1987) showed that when probability distributions belong to a specified exponential family, a policy that pulling the arm having the largest upper confidence bound (UCB) is optimal. The UCB policy is constructed from Kullback–Leibler (KL) information between estimated reward distributions of the arms and the total sample size is needed to pre-specified. Agrawal (1995) modified the UCB policy without knowing total sample size. To better describe applications, the first work beyond the pale of parametric modeling assumptions appeared in Yakowitz and Lowe (1991). As opposed to Lai and Robbins (1985) or (Agrawal, 1995), they proposed non-parametric policies not relied on the assumptions of reward distributions. Auer et al. (2002) provided the policy named UCB1 which can achieve logarithmic regret if reward distributions are supported on ====. Besides this, many arm allocation policy are studied in the non-parametric framework, such as ====-greedy and Boltzmann exploration (see Sutton and Barto, 1998). Furthermore, Chan (2020) proved an efficient non-parametric arm allocation procedures based on subsample comparisons.====However, these works are based on the assumption that rewards of the same arm are identically distributed, which is too restrictive for many applications of interest. Often, a more personalized allocation will be made when the decision-maker observes the side information. To utilize the information of covariates, some of the works impose the relationship between the rewards and covariates information. Such settings were introduced in Woodroofe (1979) under parametric assumptions. And, a series of methods under this setting is developed in Wang et al. (2005), Goldenshluger and Zeevi (2009) and Goldenshluger and Zeevi (2011). Yang and Zhu (2002) assumed that the mean response in each arm conditionally on the covariate value follows a general functional form. Li et al. (2010) extended the UCB method by assuming a linear model between rewards and covariates. As opposite to this setting, Langford and Zhang (2007) utilized the information of covariates to get a sample complexity bound of the regret, but their best policy was restricted in a certain class of policies. Perchet and Rigollet (2013) introduced the binned successive elimination (BSE) policy that decomposed a global problem into some traditional bandit subproblems and provided an upper bound for the cumulative regret. However, unlike the researches in classic bandit problems, they did not provide asymptotic efficiency for their policy, and these policies may not be efficient under general exponential families.====To deal with bandit problems with covariates, the basic idea is to decompose the region of covariates into many disjoint bins which is also applied by Perchet and Rigollet (2013). The real rewards of an arm with different covariates can be represented by the averaging rewards of the arm within the corresponding bin under some smoothing assumptions since the covariates are similar in each bin. Then, the original bandit problem with covariates can be viewed as many classic problems in each bin. Therefore, the method is called binned subsample comparisons. To avoid the best arm being eliminated which may arise by the partitioning or the crude estimation on the finite samples, the proposed method does not abandon any arm among the candidates. Instead, it gives a chance for the arms with poor performance if they have been explored too little.====Compare with the existing works, there are two distinct advantages. First, it is for the nonparametric setting where the specific forms of the relationship between the noisy reward and observable random covariates are not assumed and the specific forms of the reward’s distributions are not needed. Meanwhile, it achieves near optimality for an exponential family when the expected rewards are smooth functions with respect to covariates and the difference among the arms can be captured by the margin condition.====The remainder of this paper is organized as follows. Section 2 introduces the settings about the bandit problems with covariates. In Section 3, the binned subsample mean comparisons (BSC====) policy is proposed for allocating arms by decomposing the covariates region and comparing the subsample means. Section 4 gives the efficiency of our policy when the rewards follow an exponential family. Further, when the rewards follow a one-parameter exponential family or normal distributions, the proposed method is shown to be optimal. Illustrative simulations in Section 5 show the advantages of our policy compared with other existing methods. Section 6 concludes this paper with discussions. All proofs are postponed to Appendix.",A non-parametric solution to the multi-armed bandit problem with covariates,https://www.sciencedirect.com/science/article/pii/S0378375820300902,30 July 2020,2020,Research Article,155.0
"Bentarzi Mohamed,Sadoun Mohamed","Faculty of Mathematics, University of Science and Technology Houari Boumediene, Algiers, Algeria","Received 11 December 2019, Revised 31 March 2020, Accepted 13 July 2020, Available online 21 July 2020, Version of Record 27 July 2020.",https://doi.org/10.1016/j.jspi.2020.07.005,Cited by (6),"The efficient estimation problem of a semi-parametric periodic integer-valued autoregressive ==== model ==== is considered. The unspecified distribution of the innovation process of this model is supposed to satisfy only some mild technical assumptions====We therefore provide efficient estimates for both parameters of the model, namely a periodic autoregressive ==== intensive ==== and an application on real data set.",", exhibit a periodical autocorrelation structure (as examples, ==== ====, studied by Pedeli and Karlis (2011), ==== studied by Ferland et al. (2006), ==== studied, separately studied by Freeland (1998), Zhu and Joe (2006), Fokianos (2012) and recently Bourguignon et al. (2016)). ==== ==== ==== The distribution of a traditional ==== process is mainly described by two blocs of parameters, namely a periodic scalar auto-regression coefficient and a periodic probability distribution on positive integers belonging to parametric family, called an innovation distribution. This paper considers a more realistic semiparametric ==== model in the lags order and in the fact that there are essentially no restrictions on the innovation distribution. We provide a semiparametric efficient estimation of both periodic auto-regression parameters and unspecified periodic innovation distribution. Indeed, we establish, while following the methodology of Drost et al. (2009), an efficient semiparametric estimator on the one hand for the periodic autoregressive parameters and on the other hand for the unspecified periodic distribution of the innovation process. Our estimator might be viewed as an efficient semi-parametric maximum likelihood estimator ==== which extends the one obtained by Drost et al. (2009) for the time-invariant ==== to the periodic one. Recently, the efficient estimation problem of a parametric first-order periodic integer-valued autoregressive ==== model was studied by Sadoun and Bentarzi (2019).====The remainder of the paper is organized as follows: In the second section, we provide some basic notations, definitions and regularity conditions concerning ==== periodic integer-valued autoregressive semiparametric model, which are needed in the forthcoming sections. In the third section, we establish the ==== estimator, solution to the (constrained) polynomial optimization problem, and we show, that this estimator is an infinite dimensional ====-estimator viewed as a solution to an infinite number of moment conditions. This last finding is exploited to establish the limit distribution of our obtained ====. The fourth section, is devoted to prove that our ==== estimator is efficient, where we show first that parametric submodels have the ==== ==== property and the ==== is regular, ====, the efficiency of the ==== follows from the regularity and the special representation of the limiting distribution. In the fifth section, we present some Monte Carlo results, while showing empirically the superiority of our efficient ==== estimator to the ==== ==== and ==== ==== estimators. Furthermore, we provide an application on real data set. The conclusions are summarized in section six. Finally, the seventh section ==== an Appendix where we present some calculations and demonstrations.",Efficient estimation in periodic ,https://www.sciencedirect.com/science/article/pii/S0378375820300872,21 July 2020,2020,Research Article,156.0
"Bhattacharjee Monika,Bose Arup,Srivastava Radhendushka","Department of Mathematics, Indian Institute of Technology, Mumbai, India,Stat.-Math Unit, Indian Statistical Institute, Kolkata, India","Received 15 June 2019, Revised 1 July 2020, Accepted 5 July 2020, Available online 12 July 2020, Version of Record 3 August 2020.",https://doi.org/10.1016/j.jspi.2020.07.004,Cited by (0),", stochastic autoregressive volatility model and autoregressive conditional duration model. Under the alternate hypothesis, for specific models such as infinite order moving average, non-linear moving average and bilinear models with finite 4th moment, we show that with suitable centering and scaling, which is of a different order of magnitude compared to the null case, the test statistic is asymptotically normal.","Suppose ==== is a zero mean ==== with auto-covariance sequence ====. Let ==== be its autocorrelation sequence. Suppose, based on a sample ====, it is required to test whether ==== is a white noise. This corresponds to the null hypothesis ====The white noise test statistic of Hong (1996) is defined as ====where ==== is a ==== and ==== is a ====, ==== In ====, one can also take ==== in the denominator instead of ==== as they are asymptotically equivalent. ==== is rejected for large value of ====. The ==== on ==== and ==== throughout our discussion is that ====It is known that the asymptotic null distribution of ==== is normal under different sets of conditions such as, strict stationarity, martingale difference, geometric moment contraction and finiteness of the ====th moment. As has been pointed out in the literature (see Shao, 2011), the problem of reducing the ====th moment condition is important since this condition usually imposes strong restrictions on the model, including those on the parameter space.====The primary motivation for our work is to explore if asymptotic normality holds under weaker moment condition. We establish the asymptotic normality of ==== for several white noise processes under finiteness of the ====th moment. Since the statistic ==== is a quadratic function of the variables, the finiteness of the ====th moment is the most natural condition.====At the same time, for power computation, the distribution under the alternate hypothesis is required. Unfortunately, only the convergence in probability to a constant for the non-standardized test statistic is known (see Shao, 2011) in the literature. This is the second issue that we address. We prove the asymptotic normality of an appropriately centered and scaled ==== for a class of non-white noise processes under the ====th moment condition. The centering and scaling are very different from the null case.====We now describe in detail the existing results and our results.==== ==== When ==== is ====, ==== and, ==== and ==== satisfy some additional conditions, Hong (1996) showed that ====We are aware of the following two articles that remove the i.i.d. restriction.==== In an unpublished article, Hong and Lee (2003) established (1.4) when ==== is a ====, ====, ==== for some ====, ==== is continuous and ====.==== Suppose ==== is a mean zero white noise process of the form ====where ==== are i.i.d. and ==== is a measurable function. Clearly ==== is ====. Shao (2011) established (1.4) if ==== satisfies (1.5) and has ==== ==== property of order ==== (GMC====), ==== and ==== is differentiable except at finitely many points. Further, GMC==== holds for, (i) various forms of GARCH, (ii) non-linear MA processes, (iii) stochastic autoregressive volatility and, (iv) autoregressive conditional duration models, whenever ====.====In particular, the following strictly stationary non-linear ====-dependent MA white noise process falls under this setup but is not a martingale difference and hence (a) is not applicable. ====Observe that the above three articles derive only the asymptotic null distribution. Moreover, (b) and (c) require ====. It is known that this moment assumption can be very restrictive (see Remark 2.1, p. ==== of Shao, 2011). For example consider the ARCH==== model ====and ==== is independent of ==== for all ====. By Theorem 4.3, p. ==== of Fan and Yao (2003), and the discussion around it, (1.7) has a unique ==== solution with ==== if and only if ====. Moreover, ==== and ==== respectively hold if and only if ====Therefore, if ====, then ==== and (1.7) does not satisfy the condition of (b) or (c).====The condition ==== appears to be crucial in their technical arguments and seems to be a limitation of the martingale approach. On the other hand, since ==== involves quadratic functions of ====, the most suitable condition appears to be ====. It does not seem to be easy to establish the main result of Shao (2011) with the weaker assumption ====.==== ====. Nevertheless, under appropriate assumptions on the kernel ==== and ====, in Theorem 2.1, we show that (1.4) holds for the following four specific processes: the non-linear MA (1.6), the power GARCH, the stochastic autoregressive volatility process and the autoregressive conditional duration process. These processes frequently appear in time series analysis. See for example Di and Gangopadhyay (2011), Han and Zhang (2012) and Qu and Perron (2013). In particular, as a consequence of Theorem 2.1, (1.4) continues to hold for (1.7) when ====, and as a consequence, the applicability of the white noise test is theoretically sound in a larger class of models.====Suppose for the moment that ==== is ====—for all ====, ====, ====) is finite and depends only on ==== but not on ====. A strictly stationary sequence with all moments finite is moment stationary. For Gaussian processes, moment stationarity and strict stationarity are equivalent. Further suppose, again for now, that ====This automatically implies ====. Condition (1.9) is well known and many asymptotic results on estimating the cumulant and power spectra have been established under (1.9) (see for example Brillinger, 2001).====We establish a crucial auxiliary Lemma A.2, which says that under ====, if ==== is ==== (1.9) holds, then (1.4) is true. This is achieved by showing that the second order cumulant of ==== converges to ==== and all its other cumulants converge to ====. ==== from Brillinger (2001) play a crucial role in our arguments.====Reduction in the moment condition is then achieved by a suitable truncation on a case by case basis. For each of the four processes mentioned above, suppose only ====. Then by suitable truncation, we construct ==== which satisfies (1.9) and, in an appropriate metric, ==== and ==== (test statistic based on ====) are close. This is used to conclude the asymptotic normality of ==== solely under ====.====Improvisation is needed for the truncation. For the non-linear process such as (1.6), a standard truncation of the innovation process is enough. For other models such as ARCH, GARCH etc., we need a more subtle two step truncation that does not seem to have been used earlier in the literature. Interestingly, the truncated processes are not of the same type as the parent one, but still come under the ambit of our assumption (1.9). Incidentally, (1.9) fits very nicely with the truncation arguments. While it would be nice to formulate a broad truncation result, this appears to be impossible due to the variety of models. However, given an explicit description of the model in hand, as our four cases show, it may not be hard to come up with an appropriate truncated process to which we can apply (1.9). On the other hand, as is clear from the earlier works, it does not seem to be easy to reduce the moment conditions in the existing results by truncation or otherwise.====Simulations indicate ==== may not be asymptotically normal if ====.====. If ==== is strictly stationary and GMC==== holds, Shao (2011) showed that ====
 Hong and Lee (2003) (unpublished) showed (1.10) under a different set of conditions. Since the limit is a non-random quantity, (1.10) cannot be used for computing power of ==== under alternatives. No convergence result to a non-degenerate distribution is known for a normalized ====.==== ====. As before, by using truncation, if ==== then for the non-white noise processes, (i) ARMA, (ii) bilinear and, (iii) non-linear MA models with i.i.d. innovations we are able to prove that (see Theorem 2.2): ====We also demonstrate on a real data set that after a good model fit, the residuals behave like a white noise with an ARCH structure. These residuals have very high empirical ====th moment but moderate ====th moment and pass the white noise test. The validity of this test is justified by our asymptotic normality result. By a simulation study we also show that the asymptotic normality need not hold in general when the ====th moment is infinite.====In Section 2, we state our main results. In Section 3, we illustrate the finite sample performance of ==== and also the importance of the condition ====. In Section 4, we perform analysis on a price index data set to illustrate how residuals after a fit may be modeled as an ARCH white noise which passes the white noise test but curiously its empirical ====th moment is very high while its empirical ====th moment is moderate. The proof of the main theorems are given in Appendix A.",A white noise test under weak conditions,https://www.sciencedirect.com/science/article/pii/S0378375820300860,12 July 2020,2020,Research Article,157.0
Diop Mamadou Lamine,"THEMA, CY Cergy Paris Université, 33 Boulevard du Port, 95011 Cergy-Pontoise Cedex, France,LERSTAD, Université Gaston Berger, BP 234 Saint-Louis, Senegal","Received 7 November 2019, Revised 24 March 2020, Accepted 2 July 2020, Available online 12 July 2020, Version of Record 15 July 2020.",https://doi.org/10.1016/j.jspi.2020.07.003,Cited by (11)," time complexity. Some simulation results are provided, as well as the applications to the US recession data and the number of trades in the stock of Technofirst.","We consider a ====-valued (====) process ==== where the conditional mean ====is a function (see below) of the whole information ==== up to time ==== and of an unknown parameter ==== belongs to a compact subset ====
 (====). The inference in the cases where ==== is constant or the distribution of ==== is known has been studied by many authors in several directions; see for instance, Fokianos et al. (2009), Fokianos and Tjøstheim, 2011, Fokianos and Tjøstheim, 2012, Davis and Liu (2016), Douc et al. (2017) among others, for some recent works. We consider here a more general setting where ==== is piecewise constant (multiple change-point problem) and that the distribution of ==== is unknown. We refer to Franke et al. (2012), Kang and Lee (2014), Doukhan and Kengne (2015), Leung et al. (2017) and the references therein for some tests for change-point detection in integer-valued time series.====Let ==== be a trajectory generated as in model (1.1) and assume that the parameter ==== is piecewise constant. Also, assume that ====, ==== and ==== such that, ==== is generated from the ====th stationary regime ; i.e., it is a trajectory of the process ==== (which are not actually observed for ====, see Section 2 for some details) satisfying: ====where ==== is the ====-field generated by the whole information up to time ==== and ==== is a measurable non-negative function assumed to be known up to the parameter ====. ==== is the number of segments (or regimes) of the model; the ====th segment corresponds to ==== and depends on the parameter ====. ==== are the change-point locations; by convention, ==== and ====. To ensure the identifiability of the change-point locations, it is reasonable to assume that ==== for ====. The case ==== corresponds to the model without change. In the sequel, we assume that the random variables ====, ==== have the same (up to the parameter ====) distribution ==== and denote by ==== the distribution of ====. For instance, for an INGARCH==== representation, we have ====where ====, ====. The parameters vector of the ====th regime is ====. Therefore, ==== is a compact subset of ==== such that for all ====, ====. For all ====, we assume that ====; hence, there exists a sequence of non-negative real numbers ==== such that ====. Then, ==== for any ====. For instance, if the distribution ==== is Poisson, negative binomial or binary, then we get respectively a Poisson, negative binomial, binary INGARCH process; see some examples in Section 4.====Our main focus of interest is the estimation of the unknown parameters ==== in the model (1.2). This can be viewed as a classical model selection problem. Assume that the observations ==== are generated from (1.2). Let ==== be the upper bound of the number of segments (note that ====). Denote by ==== the set of partitions of ==== into at most ==== contiguous segments. Set ==== a generic element of ==== segments in ====. Consider the collection ==== where, for a given ====, ==== is the families of sequence ==== which are piecewise constant on the partition ====. Any ==== depends on the parameter ==== which is the piecewise values of ==== on each segment. Set ====. Denote by ==== a generic element of ====, with partition ==== and parameter ====. ==== denotes the number of the piecewise segments, also called the dimension of ====. The true model ==== with dimension ====, depends on a partition ==== and the parameter ====.====For any ====, set ==== and denote by ==== the distribution of ====; let ==== be the probability density function of this distribution. For ====, let ==== be the conditional distribution of ====. We consider the log-likelihood contrast conditioned to ====: ====, ====Thus, the minimal contrast estimator ==== of ==== on the collection ==== is obtained by minimizing the contrast ==== over ====; that is, ====. The main approaches of the model selection procedures take into account the model complexity and select the estimator ==== such that, ==== minimizes the penalized criterion ====where ==== is a penalty function, possibly data-dependent. We now address the following issues.====(i) ====. Kashikar et al. (2013) have carried out structural breaks in Poisson INAR process from the MCMC and Gibbs sampling approach. Cleynen and Lebarbier, 2014, Cleynen and Lebarbier, 2017 have recently considered the change-point type problem (1.2) with i.i.d. observations; in their works, the distribution ==== is assumed to be known and could be Poisson, Negative binomial or belongs to the exponential family distribution. From the practical viewpoint, we consider the case where ==== is unknown and deal with the Poisson quasi-likelihood (see for instance, Ahmad and Francq, 2016). So in the sequel, ==== is the Poisson quasi-likelihood contrast and ==== is the Poisson quasi-maximum likelihood estimator (PQMLE).====(ii) ====. This question is tacked by model selection approach. Numerous works have been devoted to this issue; see among others, Lebarbier (2005), Arlot and Massart (2009), Cleynen and Lebarbier, 2014, Cleynen and Lebarbier, 2017 and Arlot et al. (2016).====In this (quasi)log-likelihood framework, it is more usual to consider the Kullback–Leibler risk. For any ====, the Kullback–Leibler divergence between ==== and ==== is ==== where ==== denotes the expectation with respect to the true distribution of the observations. In the case where ==== is the likelihood contrast, we get ====. The “ideal” partition ==== (the one whose estimator is closest to ==== according to the Kullback–Leibler risk) satisfying: ====The corresponding estimator ====, called the ====, depends on the true sample distribution, and cannot be computed in practice. The goal is to calibrate the penalty term, such that the segmentation ==== provides an estimator ==== where the risk of ==== is close as possible to the risk of the ====, namely such that ====for a non-negative constant ====, expected close to 1. This issue is addressed in the above mentioned papers, and the results obtained are heavily relied on the independence of the observations. In our setting here, it seems to be a more difficult task. But, we believe that the coupling method can be used as in Lerasle (2011) to overcome this difficulty. We leave this question as the topic of a different research project.====(iii) ====. The aim here is to consistently estimate the parameters of the change-point model. This issue has been addressed by several authors using the classical contrast/criteria optimization or binary/sequential segmentation/estimation; see for instance, Bai and Perron (1998), Davis et al. (2008), Harchaoui and Lévy-Leduc (2010), Bardet et al. (2012), Davis and Yau (2013), Davis et al. (2016), Ma and Yau (2016), Yau and Zhao (2016), Inclán and Tiao (1994), Bai (1997), Fryzlewicz and Subba Rao (2014), Fryzlewicz (2014), among others, for some advanced towards this issue. These works and many other papers in the literature on the asymptotic study of multiple change-point problem are often focused on continuous valued time series; moreover, the case of a large class of semi-parametric model for discrete-valued time series (such as those discussed earlier) have not yet addressed.====We consider (1.2) and derive a penalized contrast of type (1.3). We assume that there exists a partition ==== of ==== such that ====, where ==== is the corresponding partition of ==== obtained from ====. We provide sufficient conditions on the penalty ====, for which the estimators ==== and ==== are consistent; that is: ====where ==== is the corresponding partition of ==== obtained from ====.====The paper is organized as follows. In Section 2, we set some notations, assumptions and define the Poisson QMLE. In Section 3, we derive the estimation procedure and provide the main results. Some simulation results are displayed in Section 4 whereas Section 5 focuses on applications on the US recession data and the daily number of trades in the stock of Technofirst. Section 6 is devoted to a summary and conclusion. The Supporting Information provides the proofs of the main results.",Piecewise autoregression for general integer-valued time series,https://www.sciencedirect.com/science/article/pii/S0378375820300859,12 July 2020,2020,Research Article,158.0
"Kolkiewicz Adam,Rice Gregory,Xie Yijun","University of Waterloo Department of Statistics and Actuarial Science, Mathematics 3 (M3) University of Waterloo 200 University Avenue West Waterloo, ON, Canada N2L 3G1","Received 10 April 2019, Revised 30 June 2020, Accepted 2 July 2020, Available online 11 July 2020, Version of Record 21 July 2020.",https://doi.org/10.1016/j.jspi.2020.07.001,Cited by (6),", and we develop several new computational tools needed to implement the high-dimensional projection pursuit. As a by-product of evaluating the test statistic, our method furnishes a way of decomposing functional data into its approximately Gaussian and non-Gaussian components, which is useful for the purpose of data visualization and subsequent analyses. A simulation study and analysis of three data sets demonstrate the complimentary advantages of the proposed test to those currently available in the literature.","Statistical methods based on the assumption of normality of the observations and/or model errors are ubiquitous in classical statistics, and are also widely used in more modern settings when the data to be analyzed are high-dimensional or functional in nature. To give some recent examples, Panaretos et al. (2010) and Cuevas et al. (2004) assume normality in order to perform two sample and analysis of variance tests with functional data, and in Kowal et al. (2017) and Kowal et al. (2019) normality of the data is used in performing Bayesian inference with complex functional data. We refer the reader to the text books Horváth and Kokoszka (2012) and Ramsay and Silverman (2002) for introductions to functional data analysis, and some further applications of normality in this setting can be found in Gromenko et al. (2017), Yao et al. (2005), and Constantinou et al. (2017), although this list is far from exhaustive. Given the usefulness of these procedures, it is important to have ways of measuring the validity of the assumption of normality for a given sample of functional data. At the least such a validation would lend further credibility to the conclusions of procedures in which normality is assumed, but evidence for normality of functional data may also be of independent interest.====The much related problem of testing for normality in multivariate data enjoys an enormous literature dating back at least to the 1960s. A myriad of techniques are now available, and, crudely, they can be categorized into four groups based on two characteristics. The first is how departures from normality in the data are measured, in which typically either moment based measures are used, such as the sample skewness and kurtosis, or goodness-of-fit tests involving the empirical distribution or characteristic function are employed. The second is how information is aggregated across the coordinates of the data, which usually amounts to either pooling/averaging the information across coordinates, or searching for linear combinations of the coordinates that maximize a given measure of non-Gaussianity. Approaches following the later paradigm are often termed “projection pursuit” methods, since finding such a linear combination can be framed as a classical projection pursuit problem as put forward in Kruskal (1972), and Friedman and Tukey (1974). Canonical test statistics based on moment methods of each type are Mardia’s multivariate skewness (Mardia et al., 1979), which aggregates the skewness across coordinates, and the skewness measure of Malkovich and Afifi (1973), which is the maximal sample skewness among all linear combinations of the coordinates. One test is expected to be preferable to the other depending on how “sparse” the non-Gaussianity is in the data: data for which all linear combinations of the coordinates are non-Gaussian should be more apparently non-Gaussian by considering aggregation based methods, while non-Gaussianity that can be explained by only a few linear combination of the coordinates would typically be more easily detected using projection pursuit methods. Some examples of multivariate projection pursuit based normality tests can be found in Liang et al. (2000), Henze and Wagner (1997), Baringhaus and Henze (1991), Zhu et al. (1995a), Zhu et al. (1995b), and general reviews of tests for multivariate normality are given in Mecklin and Mundfrom (2004), Henze (2002), and Szekely and Rizzo (2005).====In contrast, testing for normality of functional data objects has received considerably less attention. Methods based on random projections and subsequent Carmér–von Mises and Kolmogorov–Smirnov type goodness-of-fit tests are proposed and reviewed in Cuesta-Albertos et al. (2006), Cuesta-Albertos et al. (2007), Bugni et al. (2009), and Cuevas (2014). To date and to the knowledge of the authors, the only test available for this purpose based on moment methods was put forward in Górecki et al. (2018), henceforth referred to as the GHHK test. Their approach involves projecting the functional data onto the span of the first several functional principal components estimated from the data, and then applying a test based on combining Mardia’s skewness and kurtosis to the vectors of coefficients defining these projections, i.e. applying a multivariate Jarque–Bera test (Jarque and Bera, 1980) to the projection scores. They also extend their method to serially correlated functional data. While this method proves to be effective in many cases, it evidently might be improved upon in several others. One is if the non-Gaussian components of the data are sparse among the leading principal components, analogously to the multivariate setting, but another is if the non-Gaussian components of the data are orthogonal to the leading principal components, in which case the GHHK test would not be expected to have more than trivial power. As we see below, this latter situation might occur more often than one might think, as it can arise from simply misspecifying the basis used to smooth/generate functional data objects from raw data and/or estimate the functional principal components. Although one may argue that this situation could be avoided by including more principal components, as later shown in a data example in Section 4.3, increasing the number of principal components incorporated into the GHHK test may not solve the problem.====In this paper, we propose and study an alternative normality test for functional data based on projection pursuit that overcomes some of these challenges. We consider as test statistics the maximal sample skewness and sample kurtosis among all scalar projections of the data onto a user selected compact subset of the unit ball, and hence the proposed test can be thought of as a functional generalization of the tests of Malkovich and Afifi (1973) and Baringhaus and Henze (1991). We show that the compact subset selected can be taken to be relatively high dimensional, and can also be generated by the functional principal components, which gives the test complimentary strengths to the GHHK test. A complete asymptotic theory is developed for the proposed statistics, and computational tools are introduced to conduct the required high-dimensional projection pursuit. These computational tools may be of independent interest since they may be used for more general projection pursuit problems with functional data. In addition to providing a test for Gaussianity, this projection pursuit method also furnishes a way to decompose functional data into a direct sum of approximately Gaussian and non-Gaussian components useful for data visualization or subsequent analyses, which we demonstrate. This latter application builds upon some recent efforts to develop projection pursuit methods for functional data; see for example Bali et al. (2011). We study the proposed methods and compare them to the GHHK method in a simulation study, as well as in three applications to real data sets, which show the complimentary strengths of the two tests.====The rest of the paper is organized as follows: In Section 2, we define our projection pursuit-based test statistics, and present their asymptotic properties. In Section 3, we detail several computational methods useful for calculating the proposed statistics and their critical values, and also describe and present the results of a simulation study. The results of the data analyses are presented in Section 4. The proofs of all technical results are contained in the online supplementary material.====Below we use the following notation: We let ==== denote the space of real valued functions defined on the unit interval with finite squared integral, which is a Hilbert space when equipped with the inner product defined for ==== by ====. The corresponding norm is defined by ====.",Projection pursuit based tests of normality with functional data,https://www.sciencedirect.com/science/article/pii/S0378375820300835,11 July 2020,2020,Research Article,159.0
"Li Rui,Lu Wenqi,Zhu Zhongyi,Lian Heng","School of Statistics and Information, Shanghai University of International Business and Economics, Shanghai, China,Department of Statistics, Fudan University, Shanghai 200433, China,Department of Mathematics, City University of Hong Kong, Hong Kong, China","Received 11 December 2019, Revised 20 June 2020, Accepted 23 June 2020, Available online 3 July 2020, Version of Record 4 July 2020.",https://doi.org/10.1016/j.jspi.2020.06.010,Cited by (5), of the covariance kernel. We establish its optimal convergence rate in prediction risk using the ,"Functional regression has been studied in the statistics community for decades. The path of developments roughly follows that of the classical regression, with linear regression first extended to the functional case, resulting in the parametric approach to functional regression, which was well-documented in Ramsay and Silverman (2005). The nonparametric approach to functional regression, most notably the approach based on kernel smoothing, was developed in Ferraty and Vieu, 2002, Ferraty and Vieu, 2006. Some other advancements over the years in the area of functional regression include (Cardot et al., 2003, Cai and Hall, 2006, Preda, 2007, Lian, 2007, Ait-Saidi et al., 2008, Yao et al., 2005, Crambes et al., 2009, Ferraty et al., 2011, Lian, 2011).====Most of the literature on functional linear regression focus on the estimation of the mean function. However, estimation of the conditional quantile function is also of substantial interest (Koenker, 2005). Quantile regression has been widely used in many real applications (Hendricks and Koenker, 1992, Heagerty and Pepe, 1999, Wang and He, 2007). It is known that, compared to the mean estimation, the median estimation, a special case of quantile regression, is more robust against outliers. At the same time, quantile regression can handle better heterogeneity in the variance and can obtain different covariate effects at different quantile levels.====The pioneering work of Kato (2012) is the first study on quantile functional linear regression that established the minimax convergence rate of the estimator. His approach is based on using functional principal component analysis with eigenfunctions of the covariance operator serving as the basis functions to approximate the unknown coefficient function. Several subsequent works extended the quantile functional linear regression to models also incorporating scalar predictors (Yao et al., 2017, Ma et al., 2019).====Although PCA is most popular with functional regression, and easily estimated once transformed to the finite-dimensional case, it is not without potential problems. For example, the PCA approach expands the unknown function ==== using the eigenfunction of the predictor covariance operator. For such a strategy to work well, it is usually necessary to assume that such a basis provides a good presentation of the coefficient function, which may not have anything to do with the predictor in terms of basis representation accuracy. Such a shortcoming is inherent in all approaches that use a basis that only depends on the predictors. Furthermore, the number of basis functions to use as the discrete tuning parameter is not as flexible as using a continuous tuning parameter for controlling the complexity of the model. Motivated by these observations, Cai and Yuan (2012) studied the RKHS approach for functional linear regression and demonstrated via simulation studies that the RKHS approach has a clear advantage when the PCA eigenfunctions do not provide a good presentation for the coefficient function. Although we believe neither approach can dominate the other in all situations, the RKHS approach at least provides an interesting alternative.====As noted above, the quantile functional regression has not been carefully studied in the RKHS setting, which is a flexible approach that includes linear function, polynomial function and smoothing splines as special cases. It also potentially allows us to define functions on non-euclidean domains since kernels for graphs or sequences, for example, have been constructed in the literature (Cancedda et al., 2003, Vishwanathan et al., 2010), although such extensions are beyond the scope of the current paper. Combining this with the usefulness of quantile regression in biological sciences, finance and other fields, this alternative estimator we propose is potentially useful in applications. In this paper, we aim to establish the optimal convergence rate of the RKHS estimator in prediction risk. Our proof is different from that in Cai and Yuan (2012) or (Kato, 2012). The least squares problem studied in Cai and Yuan (2012) has a closed-form solution that also directly leads to a bias–variance decomposition of the squared prediction error. For quantile regression, on the other hand, due to the lack of closed-form solution and unsmoothness of the loss function, it is necessary to resort to empirical processes techniques. Although empirical processes were also necessary in Kato (2012), in their model the truncation point for the eigenfunctions serves as the regularization parameter that reduces the model to a finite-(but diverging-) dimensional model resembling the linear regression. The empirical process is then bounded by taking advantage of the finite-dimensional structure, in particular using the fact that the VC-dimension of a function class is bounded by its dimension, to control the covering number of the function class derived from the (sub-)gradient of the loss function, which is then used in the Dudley’s integral bound for the empirical process. Since our empirical process involves function classes of infinite dimensions, we instead resort to Rademacher complexity of the class to bound the empirical process. Our proofs thus bear little resemblance to either Cai and Yuan (2012) or Kato (2012).====The rest of the article is organized as follows. In Section 2 we present the estimator and establish its convergence rate in prediction risk. This rate matches the optimal rate derived in Cai and Yuan (2012) for functional linear mean regression. Proofs are contained in Section 3. In Section 4, some Monte Carlo studies are carried out to illustrate the performances of the proposed estimator. We conclude with some discussions in Section 5.",Optimal prediction of quantile functional linear regression in reproducing kernel Hilbert spaces,https://www.sciencedirect.com/science/article/pii/S037837582030080X,3 July 2020,2020,Research Article,160.0
"McElroy Tucker,Roy Anindya","Research and Methodology Directorate, U.S. Census Bureau, 4600 Silver Hill Road, Washington, D.C. 20233-9100, United States of America,University of Maryland Baltimore County, United States of America","Received 26 November 2019, Revised 23 June 2020, Accepted 27 June 2020, Available online 3 July 2020, Version of Record 10 July 2020.",https://doi.org/10.1016/j.jspi.2020.06.012,Cited by (3),"Peaks in the ==== estimates of seasonally adjusted data are indicative of an inadequate adjustment. ==== are currently assessed in the X-13ARIMA-SEATS program via the visual significance (VS) approach; this paper provides a rigorous statistical foundation for VS by defining measures of uncertainty for spectral peak measures, allowing for formal ","Quarterly or monthly economic time series typically exhibit seasonality, most often described via a nonstationary stochastic process with unit root corresponding to the known seasonal frequencies (Bell and Hillmer, 1983); also see the discussion in Chapter 3 of Hylleberg (1986). Adequate estimation and removal of seasonality should correspond to the absence of spectral peaks at these same seasonal frequencies in the adjusted series. If there is residual seasonality present, the adjustment is inadequate, and hence testing for residual seasonality is a problem of widespread importance; millions of time series are seasonally adjusted each month at statistical agencies around the world, many of whom utilize the software program X-13ARIMA-SEATS (U.S Census Bureau, 2015) of the U.S. Census Bureau. Testing for residual seasonality involves data arising from time series that typically have trend nonstationarity, but no longer have seasonal nonstationarity, and therefore applications of the tests can be formulated for processes that are stationary after differencing. This is the case considered in this paper.====One method of testing for the presence of residual seasonality is based on detection of spectral peaks (Findley, 2005). The detection of seasonality in raw, or unadjusted data, is a different problem that historically has used different tools — we provide a brief discussion in order to frame the problem of testing for residual seasonality.====One approach to the detection of seasonality (in raw data) is to postulate as a null hypothesis the existence of a sinusoid – corresponding to a deterministic seasonal component – at the frequency of interest, and test whether spectral density estimates warrant such a hypothesis. The early literature on spectral peak testing (see Priestley (1981)) focused on this approach, and the stable-seasonality test of Lytras et al. (2007) does as well. Any stochastic seasonal component, conceived as a nonstationary process, can include such a stable sinusoidal component without loss of generality — this is analogous to the fact that a random walk with drift can be decomposed into a linear term (its mean) plus the purely stochastic mean zero portion. Tests for stable sinusoids focus on the deterministic part of seasonality, but are not designed to address the stochastic portion. However, it is important to do so: removal of the deterministic portion alone (say, via regression) does not entail the removal of the whole stochastic seasonal, and such an approach fails to accomplish the goal of seasonal adjustment — for most economic series, the seasonality is too evolutive to be adequately captured by fixed periodic functions (Findley et al., 2017).====In summary, seasonal adjustment will typically remove deterministic seasonality as well as any nonstationary stochastic facets to seasonality. However, residual seasonality is deemed to be present if seasonal peaks exist in the spectral density of the trend-differenced adjusted process. Tests for residual seasonality can therefore be based upon a stationary time series process, and all the earlier work has recognized this. Early work on assessing the effect of seasonal adjustment appeared in Nerlove (1964) and Grether and Nerlove (1970). Pierce, 1976, Pierce, 1979 looked at adequacy of seasonal adjustment by examining the magnitude of the autocorrelations at seasonal lags of the adjusted series. This approach is generalized to the ==== statistic, adopted by TRAMO-SEATS (Maravall, 2012), which is a variant of the Box–Ljung–Pierce test applied to seasonal lag autocorrelations.====Whereas a deterministic sinusoid corresponds to a jump in the spectral distribution function – and will appear in a spectral density estimate as a tall slender peak – stochastic seasonality (i.e., the residual seasonality remaining after seasonal adjustment) instead corresponds to a broader peak in the spectral density estimate (e.g., computed using an Autoregressive Estimator); it will have a broader peak that nonetheless is approaching an infinite height as sample size increases. It becomes important to consider the width of a spectral peak in its assessment. Since the procedure of seasonal adjustment, viewed in the frequency domain, amounts to multiplication of a function with a peak by another function with a trough, whether or not the peak is transformed into a trough or not depends on the width of these functions (see McElroy and Roy (2017)). For this and other reasons, Soukup and Findley (1999) considered a measure of the peak that involved the distance between ordinates of the log spectrum when examined on a grid of frequencies of mesh size ====.====The actual measure proposed in Soukup and Findley (1999) – which has now become somewhat of a standard by virtue of its incorporation into the X-12-ARIMA software used at most international statistical agencies – is computed by comparing the spectral peak ordinate to both nearest neighbor ordinates, with respect to the chosen frequency grid; when both ordinate differences exceed a threshold (selected based upon empirical criteria), the spectral peak is declared to be “visually significant.” So far no distribution theory has been proposed for this statistic, making it difficult to rigorously determine Type I and II error. This paper puts the concept of visual significance upon a rigorous statistical footing by describing the exercise of finding a visually significant peak as a proper hypothesis testing procedure.====In order to develop a statistical theory for spectral peaks, one first requires a theory for spectral density estimation. Spectral density estimates generally fall into two classes: model-based (e.g., the Autoregressive spectral estimator, or other estimators derived from a fitted model) and nonparametric (e.g., based on smoothing the periodogram). We focus on the latter class, based upon tapering the sample autocovariances with a positive definite taper, such as the Bartlett or Daniell kernels (Priestley, 1981). Asymptotic theory for such estimates goes back to Parzen (1957), and the literature adopts the perspective that the taper bandwidth is negligible relative to sample size. More recent literature, as in Hashimzade and Vogelsang (2008), adopts the perspective of the so-called fixed-b asymptotics, where the ratio of bandwidth length to sample size is assumed to be a fixed fraction ====. Because the fixed-b asymptotic framework has several advantages – including a superior approximation of the sampling distribution (McElroy and Politis, 2014, Sun, 2014) – we pursue spectral peak detection with this perspective in mind. Some of the results require extensions of previous literature, such as Hashimzade and Vogelsang (2008) and McElroy and Politis (2014); we allow the frequencies of interest to depend on sample size, and can be more general than Fourier frequencies.",Testing for adequacy of seasonal adjustment in the frequency domain,https://www.sciencedirect.com/science/article/pii/S0378375820300823,3 July 2020,2020,Research Article,161.0
"Xu Kai,Zhou Yeqing","School of Mathematics and Statistics, Anhui Normal University, Wuhu 241002, China,School of Mathematical Sciences, Tongji University, Shanghai 200092, China","Received 15 November 2019, Revised 5 June 2020, Accepted 26 June 2020, Available online 2 July 2020, Version of Record 10 July 2020.",https://doi.org/10.1016/j.jspi.2020.06.011,Cited by (1)," of type I under ====. To reduce the size distortion, we further propose a multiplier ","With the rapid development of information technology, high-dimensional data frequently arise in many advanced research areas, such as genetical studies, high-resolution image analysis and high-frequency financial researches. For instance, in genome-wide association studies, millions of single nucleotide polymorphisms (SNPs) are collected as potential predictors for the phenotypes of a sample involving hundreds of subjects. Analyzing such data poses many challenges to traditional statistical inference since the number of variables is much larger than the sample size. To deal with the high dimensionality, a natural idea is to study the marginal effect of each predictor on a response, which can break a high-dimensional regression into many low dimensional problems. The proposed methods include but are not limited to, marginal screening methods based on correlation learning (Fan and Lv, 2008, Li et al., 2012, Zhu et al., 2011), single-SNP analysis methods, and so forth. However, such marginal methods ignore the correlations among predictors, leading to imprecise statistical analysis when marginal signals are very weak. Therefore, this article aims at developing simultaneous tests on high-dimensional regression coefficients to know whether a set of predictors have any effect on a response.====Consider a linear regression model to characterize the relationship between predictors and a response, ====where ==== is a nuisance intercept, ==== denotes a ====-dimensional vector of ones and ==== is a vector of unknown regression coefficients. The response vector ====, where ==== are independent. The ====th predictor ====. Let ==== and the ====th row of ==== be ====, where ====s are independent and identically distributed with ==== for ====. The random error vector ====, where ====s are assumed to be independent and identically distributed with mean zero. We further assume that ==== has the finite Fisher information quantity, i.e., ==== with the density function ====, and is independent of predictors. The primary interest is to test the overall significance of high-dimensional regression coefficients simultaneously in (1), that is, ====If ==== holds, we can conclude that the linear association between the predictors ==== and the response ==== is insignificant.====When the number of predictors ==== is smaller than the sample size ====, the conventional method to test the hypothesis in (2) is ====-test (Anderson, 2003). However, as demonstrated in Zhong and Chen (2011), the power of ====-test tends to diminish as ==== increases even ====. Moreover, the ====-test breaks down completely when ==== because the sample covariance matrix ==== is not invertible. To overcome these limitations, several tests for high-dimensional data have been proposed in the literature. For high-dimensional linear regression models, Zhong and Chen (2011) proposed a U-type test based on the modified ====-statistic. Goeman et al. (2006) developed an empirical Bayesian test by excluding the inverse term in ====-statistic. Feng et al. (2013) considered a robust score test based on rank regression. Cui et al. (2018) suggested a test statistic based on an estimated U-statistic of order two. For high-dimensional generalized linear regression models, Goeman et al. (2011) extended the test proposed by Goeman et al. (2006) and derived its asymptotic distribution for fixed dimension ====. Guo and Chen (2016) further modified the test statistic of Goeman et al. (2011) and analyzed its property for diverging ====. All the aforementioned test statistics are based on an estimator of ==== for some positive definite matrix ====. Specifically, they construct the sum-of-squares-type statistics to estimate the squared Euclidean norm ====.====These sum-of-squares-type methods usually perform well under the dense alternatives, for example, moderate signals of ==== spread out over a large number of coordinates. However, in many high-dimensional applications, only a small number of predictors truly contribute to the response (Fan and Lv, 2008, Arias-Castro et al., 2011, Liu and Xie, 2019). Examples can be found in the gene expression microarray data, where most of genes are irrelevant to the biological outcomes. In this situation, the signals of ==== are sparse. The sum-of-squares-type tests have much difficulty in detecting sparse signals, resulting in the low power. To handle the sparse alternatives, Tippett (1931) introduced the minimum ====-value method, which has been widely used in genome-wide association studies to select the significant genes (Chen et al., 2006, Ballard et al., 2010). The minimum ====-value method employs a maximum-type statistic, which is formulated as ====where ==== is a ====-statistic from the marginal regression of ==== on ====, namely, ====with ====, where ==== denotes the ====-norm. It can be seen that the maximum-type statistic combines the marginal ====-statistic of each predictor to aggregate all the individual effects. Other effective tests in spare settings, including the higher criticism test (Donoho and Jin, 2004) and Berk–Jones test (Berk and Jones, 1979), also adopt the maximum-type test statistics. To the best of our knowledge, most of the maximum-type tests depending on the marginal test statistic require to assume ====, and hence they are not resistant to the outlying responses and heavy-tailed errors. Moreover, as mentioned above, the marginal test statistics neglect the correlations among predictors. As a consequence, the powers of these maximum-type tests can be repressed when predictors are highly correlated and their marginal effects are very weak. To incorporate the dependence structure, Liu and Xie (2018) considered a maximum-type test using the conditional effects of predictors, which can be regarded as a powerful version of the minimum ====-value method. Zhang and Cheng (2017) proposed a bootstrap-assisted test based on the desparsifying Lasso estimator (Van de Geer et al., 2014). Nevertheless, these two tests also impose assumptions on the error variance.====In this article, we propose new maximum-type tests based on Wilcoxon scores for high-dimensional linear regression coefficients, which have four appealing features. First, the new tests based on Wilcoxon scores are robust to the presence of outlying observations and heavy-tailed errors. This means our proposed tests allow non-Gaussian distributed variables and do not require the existence of error variance. Second, following the idea of Liu and Xie (2018), we take the dependence structure among predictors into account and use the transformed observations of predictors ==== to construct the test statistics, where ==== is a regularized estimator of the precision matrix ====. The new proposed tests essentially quantify the conditional effects of predictors based on a joint regression, which helps enhance their power performances. The difference between our tests and the LX test (Liu and Xie, 2018) lies in at least two aspects. One is that the LX test performs under normality assumption, which is not required in our tests. The other is that the LX test restricts ==== and is infeasible in “large ====, small ====” settings. By contrast, our tests are more flexible and do not assume any explicit relationship between ==== and ====. Third, to reduce the size distortion, we further propose a multiplier bootstrap method based on the high-dimensional Gaussian approximation (GAR for short) theory (Chernozhukov et al., 2013), which does not require structural assumptions on the unknown covariance matrices. For many existing testing procedures, deriving the limiting null distributions of test statistics needs to enforce the structural assumptions on the unknown covariance matrices. However, in analysis of high-dimensional data, we often lack prior information on covariance structures, and these assumptions are difficult to be verified. By contrast, our testing procedures based on the Gaussian approximation are fully data-driven and maintain valid under general covariance structures. Fourth, we compare the powers of our proposed tests with the minimum ====-value method and LX test theoretically, and prove that our proposals are asymptotically more powerful under heavy-tailed distributions.====The rest of this paper is organized as follows. Section 2 introduces the new test statistics and establishes their asymptotic null distributions. We also propose a data-driven Gaussian approximation method to provide accurate critical values. The powers of our proposed tests in comparison with the minimum ====-value method and LX test are carefully studied. Section 3 conducts simulation studies to evaluate the empirical sizes and powers of our proposals. We apply the proposed tests to analyze a gene expression dataset in Section 4. The article concludes with a short discussion in Section 5. All the technical proofs are gathered in the Appendix.",Maximum-type tests for high-dimensional regression coefficients using Wilcoxon scores,https://www.sciencedirect.com/science/article/pii/S0378375820300811,2 July 2020,2020,Research Article,162.0
"Wang Yijun,Zhang Jiajia,Cai Chao,Lu Wenbin,Tang Yincai","School of Statistics and Mathematics, Zhejiang Gongshang University, Hangzhou, Zhejiang Province, 310018, People’s Republic of China,Key Laboratory of Advanced Theory and Application in Statistics and Data Science - MOE, School of Statistics, East China Normal University, Shanghai, 200062, People’s Republic of China,Department of Epidemiology and Biostatistics, University of South Carolina, Columbia, SC 29208, USA,Department of Statistic, North Carolina State University, Raleigh, NC 27695, USA","Received 29 January 2019, Revised 22 June 2020, Accepted 22 June 2020, Available online 27 June 2020, Version of Record 7 July 2020.",https://doi.org/10.1016/j.jspi.2020.06.009,Cited by (3)," of parametric estimators are provided and its performance is demonstrated through comprehensive simulation studies. Finally, the proposed method is applied to a prostate cancer clinical trial dataset.","With advancements in medical research, significant progress has been achieved in disease diagnosis and treatment, which has lead to significant improvements in the survival rate of patients with a broad range of diseases. For example, among men diagnosed with prostate cancer in the mid-1970s, 5-year survival rate is 69%; this rose to 76% in the mid-1980s and is nearly 100% today for men diagnosed with local and regional prostate cancer. The cause of death among these patients would involve causes other than prostate cancer. In one study, prostate cancer clinical trial data (Kleinbaum and Klein, 2006) included a total of 483 prostate cancer patients with complete information at the end of the follow up. Among them, 27% died of prostate cancer, 46.4% died owing to other causes, and the percentage of survivors by the end of the study was 26.7%. To investigate the effect of the drug diethylstilbestrol (DES), patients were randomly administered to different concentrations of DES, where in the group that was given a placebo or less than 0.2-mg estrogen DES was designated as the control group, and patients receiving a dosage higher than 0.2-mg estrogen DES were denoted as the treatment group. The cumulative incidence curves for deaths due to prostate cancer in the treatment group are shown in Fig. 1, and they indicate a statistical difference based on Gray’s test (Gray, 1988)(====-value=0.0224). A positive treatment effect was also revealed by several authors (Byar and Green, 1980, Kay, 1986, Cheng et al., 1998) when using the competing risk approach. When the observe time is long enough, if there is a plateau in the cumulative incidence curve, then there is a cure fraction. The cumulative incidence curves leveled off at 0.34 for the control group and at 0.22 for the treatment group, which may indicate a potential cure for prostate cancer. Furthermore, it is worthwhile pointing out that potential cure always exists in prostate cancer based on the literature (Othus et al., 2009). When patients are in early-stage prostate cancer, surgery (radical prostatectomy) and radiotherapy (external-beam radiotherapy, brachytherapy, or both) can eradicate the disease and induce a significant cure proportion of patients (Hanlon and Hanks, 2000, Jani and Hellman, 2003). Ignoring a potential cure for prostate cancer or any other cause of death in the standard mixture cure model may lead to bias in the estimate of the treatment effect. Accordingly, those two components are essential for understanding the treatment effect in prostate cancer deaths, which highlights the need for developing a mixture cure model that allows non-curable competing risk.====The two-component mixture cure model can simultaneously capture cured and uncured patients (Boag, 1949). Assuming cured patients with a probability of ==== and uncured patients with a survival probability of ====, the two-component mixture cure model is written as ====. Furthermore, ==== is denoted as a vector of covariates of interest that may have an impact on either the cure rate or survival probability of the uncured patients. Then the general two-component mixture cure model (Berkson and Gage, 1952) can be written as ====where ==== is the uncured probability and ==== is the survival probability of the uncured patients. The uncured probability ==== can be modeled through a logit link, probit or log–log link functions (Lam et al., 2005). The survival probability of the uncured patients can be modeled using established survival functions such as the Weibull distribution (Farewell, 1977, Farewell, 1982), lognormal distribution (Berkson and Gage, 1952), and standard survival models including the proportional hazards models (Kuk and Chen, 1992, Sy and Taylor, 2000, Peng, 2003, Corbière et al., 2009), and the accelerated failure time models (Lu, 2010, Scolas et al., 2016).====The proportional hazards mixture cure (PHMC) model considers the proportional hazards model for the survival of the uncured patients. One advantage of the PHMC model is that the risk to the uncured patients can be explained through a hazard risk ratio, which is similar to the proportional hazards model. The PHMC models are primarily estimated based on the expectation–maximization (EM) algorithm (Peng, 2003, Corbière et al., 2009), the multiple imputation approach (Lam et al., 2005, Zhou et al., 2016) and the Bayesian method (Yin, 2005, Chen et al., 2002, Cancho et al., 2011). Large sample properties were established in (Fang et al., 2005). Corbière et al. (2009) extended the PHMC model by including the nonlinear relationship.====In reality, uncured patients may die either as a result of the event of interest or owing to other causes, which are modeled by competing risk models. Competing risk models serve as the primary tools for addressing more than one event in the survival analysis (Kleinbaum and Klein, 2006, Crowder, 2001, David and Moeschberge, 1978, Kalbfleisch and Prentice, 2011, Xu and Tang, 2011, Shen and Xu, 2018, Xu et al., 2015, Xu et al., 2014). The most common nonparametric approach is to use the cumulative incidence function (CIF) to analyze the competing risk data (Kalbfleisch and Prentice, 2011, Fine and Gray, 1999, Tai et al., 2001). Alternatively, cause-specified hazards or subhazards can be used to explain the risk effects (Gaynor et al., 1993, Fusaro et al., 1996, Klein, 2006, Pintilie, 2007, Ohneberg et al., 2017). Moreover, mixture models with failure time distributions employ the marginal distribution of failure type to describe the competing risks (Larson and Dinse, 1985, Kuk, 1992, Ng and McLachlan, 2003). Generally, the independence among events of interest are assumed (Gamel et al., 2000, Lambert et al., 2006, Yu and Tiwari, 2007, Yu et al., 2004).====To the best of our knowledge, some research focused on simultaneously dealing with a cure fraction and competing risk models. Chao (1998) proposed a parametric Bayesian method, whereas Ng and McLachlan (1998) developed a partial maximum likelihood approach having only a small number of failures from other risks. Choi and Zhou (2002) studied the large-sample properties of a class of multivariate parametric models. Nicolaie et al. (2019) proposed a semiparametric model to express the mixing joint distribution. Basu (2010) developed a model that unified the mixture cure and competing risk approaches through the parametric Bayesian method. In this study, we assume that all patients may experience either event 1 or event 2, where the former is a primary event with a potential cure rate and the latter includes all other possible events of interest that are non-curable. Unlike the standard competing risk model, we assume that event 1 can be curable. Patients cured of event 1 could only experience event 2, whereas patients uncured from event 1 could experience either events 1 or 2. The main purpose of this study is to develop a model accounting for both the curable event and the non-curable competing risk in the mixture cure modeling framework, and to develop its semiparametric estimation method, instead of parametric Bayesian method (Basu, 2010, Chao, 1998), or parametric model (Choi and Zhou, 2002). The main difference between (Nicolaie et al., 2019) and the proposed model is decomposition of the joint distribution of failure time ==== and failure type ====. Nicolaie et al. (2019) is based on ====, whereas our model is following ====. And we use the maximum likelihood approach instead of a partial maximum likelihood approach (Ng and McLachlan, 1998). The remainder of the paper is organized as follows. In Section 2, the proportional hazards mixture cure model allowing non-curable competing risk is developed. The semiparametric estimation method based on the EM algorithm is outlined in Section 3. In Section 4, the asymptotic properties of parametric estimators in the proposed model are provided with their proofs presented in Appendix. The proposed model and method are evaluated through comprehensive simulation studies in Section 5 and illustrated using a real data analysis in Section 6.",Semiparametric estimation for proportional hazards mixture cure model allowing non-curable competing risk,https://www.sciencedirect.com/science/article/pii/S0378375820300793,27 June 2020,2020,Research Article,163.0
Maruyama Yuzo,"Graduate School of Business Administration, Kobe University, Hyogo 657–8501, Japan,Department of Statistics, Rutgers University, Piscataway, NJ 08854–8019, USA","Received 4 October 2018, Revised 22 September 2019, Accepted 16 June 2020, Available online 23 June 2020, Version of Record 14 July 2020.",https://doi.org/10.1016/j.jspi.2020.06.007,Cited by (0),This paper reviews ==== relative to right invariant ,"We review some results on minimaxity of best equivariant estimators from what we hope is a fresh and somewhat unified perspective. Our basic approach is to start with a general equivariant estimator, and demonstrate that the best equivariant estimator is a generalized Bayes estimator, ====, with respect to an invariant prior. We then choose an appropriate sequence of Gaussian priors whose support is the entirety of the parameter space and show that the Bayes risks converge to the constant risk of ====. This implies that ==== is minimax. All results on best equivariance and minimaxity, which we consider in this paper, are known in the literature. But, using a sequence of Gaussian priors as a least favorable sequence, simplifies the proofs and gives fresh and unified perspective.====In this paper, we consider the following three estimation problems.====For the first two cases with squared error loss ==== and the entropy loss ====, respectively, the so called Pitman (1939) estimators ==== are well-known to be best equivariant and minimax. Clearly, they are generalized Bayes with respect to ==== and ====, respectively. Girshick and Savage (1951) gave the original proof of minimaxity. Kubokawa (2004) also gives a proof and further developments in the restricted parameter setting. Both use a sequence of uniform distributions on expanding intervals as least favorable priors.====For the last case, James and Stein (1961) show that the best equivariant estimator is given by ====where ==== is from the Cholesky decomposition of ==== and ==== for ====. Note that the group of ==== lower triangular matrices with positive diagonal entries is solvable, and the result of Kiefer (1957) implies the minimaxity of ====. Tsukuma and Kubokawa (2015) give as a sequence of least favorable priors, the invariant prior truncated on a sequence of expanding sets.====Typical minimaxity proofs in the literature use a sequence of truncated versions of the right invariant prior on nested sets increasing to the whole space. This often makes the proofs somewhat complicated. The motivation for this paper is to find a sequence of smooth priors for which the analysis is more straightforward and less complicated. In particular, in each case, the sequence of priors we employ is based on a Gaussian sequence of possibly transformed parameters.====Section 2 is devoted to developing the best equivariant estimator as a generalized Bayes estimator with respect to a right invariant (Haar measure) prior in each case. The general approach is basically that of Hora and Buehler (1966). Section 3 provides minimaxity proofs of the best equivariant procedure by giving a least favorable prior sequence based on (possibly transformed) Gaussian priors in each cases. We give some concluding remarks in Section 4. Some technical proofs are given in Appendix.",A Gaussian sequence approach for proving minimaxity: A Review,https://www.sciencedirect.com/science/article/pii/S0378375818303276,23 June 2020,2020,Research Article,164.0
"Zhong Xiaobo,Cheung Ying Kuen,Qian Min,Cheng Bin","Department of Population Health Science and Policy, Icahn School of Medicine at Mount Sinai, 1425 Madison Avenue, New York, NY 10029, USA,Department of Biostatistics, Mailman School of Public Health, Columbia University, 722 West 168th Street, New York, NY 10032, USA","Received 6 August 2019, Revised 11 June 2020, Accepted 13 June 2020, Available online 20 June 2020, Version of Record 24 June 2020.",https://doi.org/10.1016/j.jspi.2020.06.008,Cited by (0),This paper considers screening of adaptive interventions or adaptive treatment strategies embedded in a sequential multiple assignment ,"An adaptive intervention (AI) is a sequence of treatment decisions made based on a patient’s own historical clinical information, such as the treatment history and responses to the previous treatments, with the hope to improve or optimize treatment effect. A sequential multiple assignment randomized trial (SMART) consists of a collection of AIs randomly assigned to patients, providing information for the estimation of the optimal AIs (Murphy, 2005). The optimal AIs embedded in a SMART trial may be consistently selected by comparing the estimated values of all AIs, using procedures such as the G-computation under structural nested models (Robins, 1986, Lavori and Dawson, 2007) and the inverse probability weighted estimation under the marginal mean models (Murphy et al., 2001, Orellana et al., 2010). This approach entails multiple pairwise comparisons of AIs. As the number of AIs embedded in a SMART is often large, the pairwise comparison procedures, such as the Bonferroni’s adjustment, are known to be conservative. To partially address this issue, gate-keeping approaches whereby pairwise comparisons of AIs will be made only after the hypothesis of no difference among the AIs of interest is rejected by an omnibus test have been proposed by Orellana et al. (2010), Nahum-Shani et al. (2012), Ogbagaber et al. (2016), Zhong et al. (2019).====This article addresses the inferential problem of multiple comparison of AIs in a SMART by generalizing a simultaneous confidence intervals procedure proposed by Edwards and Hsu (1983). Specifically, we adopt the approach of multiple comparison with the best (MCB), a concept originated in the ranking and selection literature to address the subset selection problem (Hsu, 1981, Hsu, 1984). The idea is to compare each treatment with the ==== best treatment, using a multiple comparison with the best confidence interval: treatments with multiple comparison with the best interval excluding zero (equivalently, upper limit less than zero) will be declared inferior and excluded from further investigation; while a treatment whose interval has 0 as its upper limit will be declared the best. The concept of multiple comparison with the best is appealing for a SMART where a typical goal is to identify one or several promising or near-best AIs and eliminate the inferior ones. The proposed method generalizes Edwards and Hsu (1983) in several respects. First, Edwards and Hsu (1983) considered non-adaptive interventions under parallel group designs where the correlation matrix among the intervention effects is known, while in our cases the interventions are adaptive under SMART and the correlation matrix among the AIs is both unknown and less than full rank (i.e., degenerate). Second, Edwards and Hsu (1983) required that the outcomes to be normally distributed with equal variance, while in this paper the outcomes are only required to be from an exponential family distribution.====Under the multiple testing framework, Ertefaie et al. (2016) proposed a method to construct a confidence set for the best adaptive intervention embedded in a sequential multiple assignment trial, a first attempt to address multiple comparisons of AIs via confidence set. However, there are some limitations of their work. First, they fail to realize that the asymptotic covariance of the AIs is not of full rank, which will cause loss of efficiency when ignored. Second, their proposed confidence set, which is an intermediate step of both our method and the original method proposed by Hsu (1984), yields a high false positive rate. Third, their method, which does not produce simultaneous confidence intervals, can not screen out inferior AIs as our method can do.====The rest of this article is organized as follows. Section 2 sets up notations, introduces an asymptotic distribution of the AIs, and proposes the method of building the MCB confidence intervals for comparing multiple AIs. The proposed simultaneous confidence intervals are evaluated using simulation in Section 3, and illustrated using a depression trial data set. This article ends with a discussion in Section 4. Technical detail is relegated to Appendix.",Comparing adaptive interventions under a general sequential multiple assignment randomized trial design via multiple comparisons with the best,https://www.sciencedirect.com/science/article/pii/S0378375820300781,20 June 2020,2020,Research Article,165.0
Dong Yuexiao,"Department of Statistical Science, Temple University, Philadelphia, PA, 19122, United States","Received 14 August 2019, Revised 10 June 2020, Accepted 10 June 2020, Available online 19 June 2020, Version of Record 26 June 2020.",https://doi.org/10.1016/j.jspi.2020.06.006,Cited by (15),"In this paper, we review three families of methods in linear sufficient dimension reduction through optimization. Through minimization of general loss functions, we cast classical methods, such as ordinary least squares and sliced inverse regression, and modern methods, such as principal ","For univariate response ==== and ====-dimensional predictor ====, linear sufficient dimension reduction (SDR) (Cook, 1998) aims to find ==== with the smallest possible column space such that ====where ====
 means independence. The column space of ====, or ====, is known as the central space, and is denoted as ====. The dimension of the central space is referred to as the structural dimension. Denote the columns of ==== as ====, ====. For a continuous response ====, an equivalent form of (1) is ====where ==== is an unknown link function, and ==== is independent of ====. Given i.i.d. samples ==== generated from model (2), SDR focuses on estimating the indices ==== without necessarily estimating the link function ====.====Cook (2007) proposes a general reductive paradigm. A reduction ==== of the predictor ==== is sufficient if any one of the following three equivalent statements holds: ====Here ==== means equal in distribution. For example, statement (ii) above means that the conditional distribution of ==== given ==== is the same as the conditional distribution of ==== given ====. We focus on linear SDR in this review, where the reduction ==== is linear in ====. For nonlinear sufficient dimension reduction, please refer to Wu (2008), Zhu and Li (2011), Lee et al. (2013), and chapter 12 of Li (2018).====The rest of the paper is organized as follows. In Section 2, we review SDR methods from minimizing general loss functions. SDR methods from maximizing general dependence measures and general information criteria are summarized in Section 3 and Section 4, respectively. We conclude the paper with some discussions in Section 5. All the methods we review have the common theme of solving an optimization problem. Throughout the paper, we focus on the population development of linking these optimization problems to linear SDR, or the Fisher consistency of the optimizer as an SDR estimator. For solving the optimization problems at the sample level, we make references to the original paper. We remark that many linear SDR methods have been reviewed in Yin (2010) as well as Ma and Zhu (2013a). In particular, nonparametric SDR methods (Xia et al., 2002) and semiparametric SDR methods (Ma and Zhu, 2012, Ma and Zhu, 2013b, Ma and Zhu, 2014) are surveyed in Ma and Zhu (2013a). While these estimators and their extensions (Yin and Li, 2011, Luo et al., 2014, Kong and Xia, 2014, Dong et al., 2017) can be viewed as solutions of optimization problems, we opt to exclude them in this review. Li (2018) provided an excellent review of classical SDR methods. Our review complements Li (2018) as most methods in this paper are not discussed in Li (2018). Throughout the paper, we denote ==== as the variance of ====. For a ====-dimensional vector ====, ==== denotes its Euclidean norm.",A brief review of linear sufficient dimension reduction through optimization,https://www.sciencedirect.com/science/article/pii/S037837582030077X,19 June 2020,2020,Research Article,166.0
"Yu Lili,Chen Ding-Geng,Liu Jun","Jiann-Ping Hsu College of Public Health, Georgia Southern University, Statesboro, GA, 30460, USA,School of Social Work & Department of Biostatistics, Gillings School of Global Health, University of North Carolina, Chapel Hill, NC, 27519, USA,Department of statistics, University of Pretoria, Pretoria, South Africa,Department of Enterprise Systems & Analytics, Georgia Southern University, Statesboro, GA, 30460, USA","Received 31 October 2018, Revised 3 June 2020, Accepted 5 June 2020, Available online 13 June 2020, Version of Record 20 June 2020.",https://doi.org/10.1016/j.jspi.2020.06.005,Cited by (6),The expectation–maximization (EM) algorithm is a seminal method to calculate the ,"Maximum likelihood estimator (MLE) plays a key role in parameter estimation from observed data in statistics. It is calculated by finding the parameter values that maximize the likelihood function given the data. However, when the likelihood is complex or incomplete, the MLE is difficult to obtain. The expectation–maximization (EM) algorithm (Dempster et al., 1977) was proposed as an iterative procedure to obtain the MLE. It consists of two steps: in the E-step, a simple and complete-data likelihood is obtained and in the M-step, a standard maximum likelihood estimation is performed using the complete data obtained from the E-step. The simple calculation gains the EM algorithm a wide application in statistics. However, the asymptotic variance–covariance matrix of the MLE is not provided directly from the EM algorithm.====As remedies, several methods are proposed to obtain the asymptotic variance–covariance matrix. However, limitations exist for these methods. Louis (1982) provided an estimator with closed form, which requires conditional expectation (conditional on the observed data) of the Hessian matrix of the complete data and of the square of the complete-data score function, which is specific to each problem. Oakes (1999) proposed an estimator with closed form as well. However, this estimator requires the second derivatives of the expected complete-data likelihood, which is not always available. Meng and Rubin (1991) proposed the supplemented EM (SEM) algorithm, which only requires the code for EM itself. However, the SEM requires the calculation of the conditional expectation of the Hessian matrix and is sometimes infeasible (Becker, 1992). Moreover, the computational cost of the SEM is high due to the requirement of running the EM algorithm for each parameter separately (Belin and Rubin, 1995). Furthermore, it seems to be susceptible to numerical inaccuracies and instability, especially in high-dimensional settings (Becker, 1992, McCulloch, 1998, Segal et al., 1994). Monte Carlo method can be employed by multiple imputation (Rubin, 1987) for missing data given the maximum likelihood estimate, but it is time consuming and inaccurate for small number of imputations. Recently, Meng and Spall (2017) showed that the simultaneous perturbation stochastic approximation (Spall, 1992, Spall, 2005) method performs well for the approximation of the Hessian matrix of the incomplete data log-likelihood evaluated at the EM estimator. However, it is still based on numerical differentiation, which may be inaccurate and unstable. Among all these methods to estimate the variance of the EM estimator, the SEM method is deemed to be the best so far and therefore we will compare our newly proposed method in this paper to the best SEM.====The fundamental difficulty to obtain the asymptotic variance–covariance matrix directly from the EM algorithm is due to the unavailability of the derivatives of some known functions. In this paper, we propose an innovative interpolation method to directly estimate the asymptotic variance–covariance matrix of the EM estimators (hereafter named as iEM). The basic idea of iEM is that we use cubic spline interpolatory functions to approximate these known functions. Then we use the derivatives of the cubic spline functions, which are very easy to obtain, to estimate the derivatives of the known functions, hence the variance–covariance matrix. The iEM overcomes all limitations in previous methods. First, it does not require anything other than those provided by the EM algorithm. Therefore, it is a general method that can apply to any problem where the EM algorithm is applied. Second, because it does not need iterative calculations, the iEM is computationally efficient. Third, We derive the optimal mesh size to minimize the global estimation error of the iEM method, so that the accurate and stable estimates can be obtained.====This paper is organized as follows. In Section 2, we give a review of the EM algorithm and the SEM method. We describe the cubic spline interpolation EM (i.e. iEM) in Section 3 and its applications to the EM algorithm with the corresponding theories and the instructions on how to select mesh and mesh size in Section 4. The comparison of the iEM method with the SEM method is given in Section 5. Two numerical examples are presented in Section 6 followed by discussions and conclusions in Section 7.",Efficient and direct estimation of the variance–covariance matrix in EM algorithm with interpolation method,https://www.sciencedirect.com/science/article/pii/S0378375820300744,13 June 2020,2020,Research Article,167.0
"Karl Andrew T.,Zimmerman Dale L.","Adsurgo LLC, Denver, CO, United States of America,Department of Statistics and Actuarial Science, University of Iowa, Iowa City, IA, United States of America","Received 23 October 2019, Revised 1 June 2020, Accepted 5 June 2020, Available online 12 June 2020, Version of Record 20 June 2020.",https://doi.org/10.1016/j.jspi.2020.06.004,Cited by (3),"We explore how violations of the often-overlooked standard assumption that the random effects ==== distribution of the predicted random effects, producing an informative summary graphic for each estimator of interest. This is demonstrated through the examination of sporting outcomes used to estimate a home field advantage.","Standard linear mixed models are built conditional on the model matrices for the fixed and random effects, meaning that these matrices are assumed to be fixed and constructed without reference to the anticipated errors or random effects. Unless the model matrices for the fixed and random effects are orthogonal with respect to the inverse of the error covariance matrix, dependence between the random effects and their corresponding model matrix will induce bias in the estimators of estimable functions of the fixed effects. This paper develops graphical and numeric diagnostics for the bias of estimators of estimable functions of fixed effects in mixed models that are constructed under the assumption that the model matrices are fixed when in fact the random effects model matrix is stochastic.====Consider a linear mixed model with fixed model matrices: ====for a continuous response, ====, where ==== and ==== are independent with ==== and ====. Although we assume normality for these vectors in order to match the most common applications, the main results on bias do not depend on this assumption. The matrices ==== and ==== are assumed to be positive definite, ==== is assumed to be positive, and neither ==== nor ==== is required to be full rank. If ==== is estimable under this model and ====
 ====
 ==== is known, then the best linear unbiased estimator (BLUE) of ==== is ==== where ====. (Here and throughout, for any matrix ====, ==== represents an arbitrary generalized inverse of ====.)====In many important applications of the linear mixed model, ==== is known (often, in fact, ====) but ==== is unknown; more precisely, the elements of ==== are known functions of an unknown parameter ====, i.e., ====. For inference on ==== to proceed in such settings it is necessary to first obtain an estimate ==== which can be substituted for the unknown ==== to obtain an estimate ==== and a corresponding estimate ==== of ====; after that, an empirical BLUE (E-BLUE) of ==== may be calculated as ==== where ====. Though the E-BLUE generally is not linear or best in any sense, it is unbiased when ==== is fixed provided that ==== is an even and translation-invariant estimator (Kackar and Harville, 1981). (An estimator ==== of ==== is even and translation invariant if ==== and ==== for all ==== and all ====.)====As an alternative to a mixed effects model, a fixed effects model could be fit: ====where ====, ====, ====, and ==== is fixed. If ==== is estimable under this model and ==== is known, then the BLUE of ==== is ==== where ====. In order to consider only estimable functions of effects that are treated as fixed in the mixed effects model (1), we will restrict attention to ==== that satisfy ====, where the length of the zero vector is equal to ====, meaning that the BLUE of ==== is ====. The decision to treat effects as fixed or random has been widely explored previously (Robinson, 1991, Stroup, 2012, Allison, 2014).====The E-BLUE ==== and BLUE ==== of, respectively, estimable functions ==== and ==== under the mixed and fixed effects models (1), (2) have differing bias characteristics under more general versions of these models in which ==== is stochastic (Allison, 1994, Lockwood and McCaffrey, 2007). Henceforth we refer to these more general versions as stochastic-==== mixed and fixed effects models. This issue seems to have received more attention in the economics literature (Wu, 1973, Hausman, 1978, Hausman and Taylor, 1981, Allison, 1994, Wooldridge, 2001, Lockwood and McCaffrey, 2007) than in standard statistics textbooks on linear mixed models (Verbeke and Molenberghs, 2000, Littell et al., 2006, Stroup, 2012, Demidenko, 2013). Our aim is to increase this awareness by discussing an application with clear visual evidence and by proposing computationally-light diagnostics and graphics that statistical software could produce in order to help quickly detect bias in ==== on an application-by-application basis.====Section 2 provides a practical motivation for considering this problem by comparing the fixed and mixed effects model estimates of home field scoring advantage for several sports. Section 3 derives sampling properties, including bias, of the E-BLUE and BLUE of estimable functions under the stochastic-==== mixed and fixed effects models. Section 4 proposes the use of a randomized permutation distribution of predicted random effects to assess the magnitude of the bias. Section 5 applies these results to the home field advantage problem. Section 6 simulates the home field advantage problem in order to illustrate the findings of Section 3 by manipulating the team schedules, ====. Appendix A describes simulations that investigate the power of the randomization test.",A diagnostic for bias in linear mixed model estimators induced by dependence between the random effects and the corresponding model matrix,https://www.sciencedirect.com/science/article/pii/S0378375820300732,12 June 2020,2020,Research Article,168.0
Longla Martial,"Department of Mathematics, University of Mississippi, University, MS 38677, USA,Department of Mathematical Sciences, University of Cincinnati, PO Box 210025, Cincinnati, OH 45221-0025, USA","Received 1 September 2018, Revised 11 October 2019, Accepted 3 June 2020, Available online 10 June 2020, Version of Record 13 June 2020.",https://doi.org/10.1016/j.jspi.2020.06.001,Cited by (2),"The goal of this paper is to indicate a new method for constructing normal confidence intervals for the mean, when the data is coming from stochastic structures with possibly long memory, especially when the ",None,New robust confidence intervals for the mean under dependence,https://www.sciencedirect.com/science/article/pii/S0378375818302416,10 June 2020,2020,Research Article,169.0
"Li Huiqin,Yin Yanqing,Zheng Shurong","School of Mathematics and Statistics, Jiangsu Normal University, Xuzhou, 221116, China,KLASMOE, Northeast Normal University, 130024, China","Received 4 November 2019, Revised 3 June 2020, Accepted 3 June 2020, Available online 10 June 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.jspi.2020.06.003,Cited by (1)," where ==== is of ==== dimension, the entries ==== are independent and identically distributed complex variables with zero means and unit variances, ==== is a ==== is an ","Let us investigate the LSD of ==== first. It is well known that there exist unitary matrices ==== dimensional ====, ==== dimensional ====, ==== dimensional ==== and diagonal matrices ==== dimensional ==== and ==== dimensional ==== such that ====Let ====If we suppose that the entries of ==== are standard complex normal random variables, then ==== has the same distribution as ====. Note that the sample covariance matrix ==== has the same eigenvalues with ====. Then from Zhang (2006), with probability ====, as ====, the empirical spectral distribution function of ==== converges weakly to a non-random probability distribution function ==== for which if ==== or ====, ====; otherwise if for each ====, ====is viewed as a system of equations for the complex vector ====, then the Stieltjes transform of ====, denoted by ====, together with the two other functions, denoted by ==== and ====, both of which are analytic on ====, will satisfy that ==== is the unique solution to (1.2) in the set ====On the other hand, if  ==== is real, ==== and ==== is diagonal, then by Paul and Silverstein (2009), with probability 1, ==== converges weakly to a probability distribution function ==== whose Stieltjes transform ====, for ====, is given by ====where ==== is the unique solution in ==== of the equation ====Let ====, and ====. Then ==== also satisfies Eqs. (1.2). Furthermore, we have ====If we let ==== denote ====, then ==== is obtained from ==== with ==== replaced by ==== respectively. Define ====The main result of the present paper, which establishes the CLT of LSS of general separable sample covariance matrices, is stated in the following theorem.====The rest of the present paper is organized as follows. In Section 2, an application of our main theorem to high dimensional white noise test is showed. The main theorem is proved in Section 3. Lemmas and some technical details are included in a supplementary material. The contribution of this paper is as follows. Firstly, we consider a more general separable covariance matrix that covers many time series models. Secondly, to the best of our knowledge, this is the first time that the CLT of LSS of separable covariance matrix with non-Gaussian fourth moment is considered.",Central limit theorem for linear spectral statistics of general separable sample covariance matrices with applications,https://www.sciencedirect.com/science/article/pii/S0378375820300720,10 June 2020,2020,Research Article,170.0
"Kaino Yusuke,Uchida Masayuki","Graduate School of Engineering Science, Osaka University, Japan,Center for Mathematical Modeling and Data Science (MMDS), Osaka University, Japan,JST CREST, Toyonaka, Osaka 560-8531, Japan","Received 31 October 2019, Revised 1 May 2020, Accepted 7 May 2020, Available online 3 June 2020, Version of Record 17 September 2020.",https://doi.org/10.1016/j.jspi.2020.05.004,Cited by (19),We consider ,"We consider a linear parabolic stochastic partial differential equation (SPDE) with one space dimension. ==== where ====, ==== is defined as a cylindrical Brownian motion in the Sobolev space on ====, the initial condition ====, an unknown parameter ==== and ====, and the parameter space ==== is a compact convex subset of ====. Moreover, the true value of parameter ==== and we assume that ====. The data are discrete observations ====, ====, ==== and ==== for ====. For the characteristics of the parameters ====, ====, ==== and ==== of the SPDE (1), see the Appendix below.====Statistical inference for SPDE models based on discrete observations has been developed by many researchers, see for example, Markussen (2003), Cont (2005), Cialenco and Glatt-Holtz (2011), Cialenco and Huang (2020), Bibinger and Trabs (2020), Cialenco et al., 2018, Cialenco et al., 2020, Cialenco (2018), Chong (2020) and references therein. Recently, Bibinger and Trabs (2020) studied the parabolic linear second order SPDE model based on high frequency data observed on a fixed region and proved the asymptotic properties of minimum contrast estimators ==== and ==== for both the normalized volatility parameter ==== and the curvature parameter ====.====In this paper, we propose adaptive maximum likelihood (ML) type estimator of the coefficient parameter ==== of the parabolic linear second order SPDE model (1). For ====, the coordinate process ==== of the SPDE model (1) is ====which satisfies that ==== for independent real-valued standard Brownian motions ==== and ====Note that the coordinate process (2) is the Ornstein–Uhlenbeck process. Using the minimum contrast estimator ==== proposed by Bibinger and Trabs (2020), we obtain the approximate coordinate process ====and the adaptive estimator is constructed by using the property that the coordinate process (2) is a diffusion process. It is also shown that the adaptive ML type estimators satisfy asymptotic normality under some regularity conditions. Furthermore, in order to illustrate the asymptotic performance of the adaptive ML type estimators of the coefficient parameters of the parabolic linear second order SPDE model based on high-frequency data, some examples and simulation results of the adaptive ML type estimators are given. For details of statistical inference for diffusion type processes and stochastic differential equations, see Prakasa Rao, 1983, Prakasa Rao, 1988, Kutoyants, 1984, Kutoyants, 2004, Florens-Zmirou (1989), Yoshida, 1992, Yoshida, 2011, Bibby and Sørensen (1995), Kessler, 1995, Kessler, 1997, Uchida (2010), De Gregorio and Iacus (2013), Kamatani and Uchida (2015), Nakakita and Uchida (2019) for ergodic diffusion processes, and Dohnal (1987), Genon-Catalot and Jacod, 1993, Genon-Catalot and Jacod, 1994, Uchida and Yoshida (2013), Ogihara and Yoshida (2014), Ogihara (2018), for non-ergodic diffusion processes. For adaptive ML type estimators and thinned data for diffusion type processes, see for example, Uchida and Yoshida (2012) and Kaino and Uchida (2018).====This paper is organized as follows. In Section 2, we consider the adaptive estimator of the SPDE model based on the discrete observations in the fixed region ====. The adaptive estimator is constructed by using the minimum contrast estimators of ==== and ==== proposed by Bibinger and Trabs (2020). We use thinned data in space to obtain the minimum contrast estimators of ==== and ====. Based on the minimum contrast estimators and thinned data in time, we construct an approximate of the coordinate process. We obtain the adaptive estimator by using this approximate of the coordinate process. It is shown that the adaptive estimator satisfies asymptotic normality. In Section 3, we deal with the SPDE model based on discrete observations which are observed in the region ==== when ==== is large. The quasi log likelihood function is obtained by using the approximate coordinate process and we prove that the adaptive ML type estimator of ==== satisfies asymptotic normality. In Section 4, concrete examples are given and the asymptotic behavior of the estimators proposed in Sections 2 The case that T is fixed, 3 The case that is verified by simulations. Section 5 is devoted to the proofs of the results presented in Sections 2 The case that T is fixed, 3 The case that. The Appendix contains the sample paths with different values of the parameters to understand the characteristics of the parameters ====, ====, ==== and ==== of the SPDE (1).",Parametric estimation for a parabolic linear SPDE model based on discrete observations,https://www.sciencedirect.com/science/article/pii/S0378375820300628,3 June 2020,2020,Research Article,171.0
"Rigon Tommaso,Durante Daniele","Department of Statistical Science, Duke University, Box 90251, Durham, NC 27708, USA,Department of Decision Sciences and Bocconi Institute for Data Science and Analytics, Bocconi University, via Röntgen 1, 20136, Milan, Italy","Received 11 June 2018, Revised 4 May 2020, Accepted 12 May 2020, Available online 3 June 2020, Version of Record 27 June 2020.",https://doi.org/10.1016/j.jspi.2020.05.009,Cited by (6),"There is a growing interest in learning how the distribution of a response variable changes with a set of observed predictors. ==== nonparametric dependent mixture models provide a flexible approach to address this goal. However, several formulations require computationally demanding algorithms for posterior inference. Motivated by this issue, we study a class of predictor-dependent infinite mixture models, which relies on a simple representation of the stick-breaking prior via sequential ====. This formulation maintains the same ","There is a growing interest in density regression models which allow the entire distribution of a univariate response variable ==== to be unknown and changing with a vector of predictors ====. Indeed, the increased flexibility provided by such formulations allows improvements in inference and prediction compared to classical regression frameworks, as seen in various applications (e.g. Dunson and Park, 2008, Griffin and Steel, 2011, Wade et al., 2014).====Within the Bayesian nonparametric framework, there is a wide variety of methods to provide flexible inference for conditional distributions. Most of these strategies are generalizations of the marginal density estimation problem for ====, which is commonly addressed via Bayesian nonparametric mixture models of the form ====, where ==== denotes a known parametric kernel indexed by ====, and ==== is a random probability measure which is assigned a flexible prior ====. Popular choices for ==== are the Dirichlet process (Ferguson, 1973), the two-parameter Poisson–Dirichlet process (Pitman and Yor, 1997), and other random measures having a stick-breaking representation (Ishwaran and James, 2001). This choice leads to the infinite mixture model ====where ==== for every ====, with ==== and ====. In Eq. (1), the kernel parameters ==== are distributed according to a diffuse base measure ====, whereas the stick-breaking weights ==== have independent ==== priors, so that ==== almost surely.====Model (1) has key computational benefits in allowing the implementation of simple Markov chain Monte Carlo methods for posterior inference (e.g. Escobar and West, 1995, Neal, 2000), and provides a consistent strategy for density estimation (e.g. Ghosal et al., 1999, Tokdar, 2006, Ghosal and Van Der Vaart, 2007). This has motivated different generalizations of model (1) to incorporate the conditional density inference problem for ====, by allowing the random mixing measure ==== to change with ====, under a dependent stick-breaking characterization (MacEachern, 1999, MacEachern, 2000). Popular representations consider predictor-independent mixing weights ====, and incorporate changes with ==== in the atoms ====; see for instance De Iorio et al., 2004, Gelfand et al., 2005 and De la Cruz-Mesía et al. (2007). As noted in MacEachern (2000) and Griffin and Steel (2006), the predictor-independent assumption for the mixing weights might have limited flexibility in practice. This has motivated more general formulations allowing also ==== to change with the predictors. Relevant examples include the order-based dependent Dirichlet process (Griffin and Steel, 2006), the kernel stick-breaking process (Dunson and Park, 2008), the infinite mixture model with predictor-dependent weights (Antoniano-Villalobos et al., 2014), and more recent representations for Bayesian dynamic inference (Gutiérrez et al., 2016). These formulations provide a broader class of priors for Bayesian density regression, but their flexibility comes at a computational cost. In particular, the availability of simple algorithms for tractable posterior inference is limited by the specific construction of these representations.====The above issues motivate alternative formulations which preserve the theoretical properties, but facilitate tractable posterior computation under a broader variety of algorithms. We aim to address this goal via a logit stick-breaking prior (====), which relates each stick-breaking weight ==== to a function ==== of the covariates, using the logit link. The proposed formulation is closely related to the probit stick-breaking prior (====) of Rodriguez and Dunson (2011). Indeed, as we will discuss in Section 2, both ==== and ==== are characterized by a continuation-ratio representation (Tutz, 1991), which allows to express the underlying clustering assignment in terms of independent and sequential binary regressions. This representation has key computational benefits and has been exploited by Rodriguez and Dunson (2011) to derive a Markov chain Monte Carlo (====) algorithm for posterior inference. However, while the ==== for ==== relies on the truncated Gaussian data augmentation for probit regression (Albert and Chib, 1993), the one for ==== exploits the recent Pólya-gamma data augmentation for logistic regression (Polson et al., 2013), which might improve mixing compared to the ====, especially in imbalanced situations (Johndrow et al., 2019). As we will clarify in Section 2, these imbalanced settings can also occur in our case, since the binary regressions are associated to latent clustering allocations. We illustrate the ==== algorithm for the ==== in Section 3.====Besides developing tractable Gibbs sampling methods, we further derive alternative computational routines which address the scalability and mixing issues of ==== in high-dimensional studies. Specifically, in Section 3 we illustrate a tractable expectation–maximization (====) routine for point estimation, and a simple variational Bayes (====) algorithm for scalable inference. Both strategies leverage again the sequential representation of the ==== and the associated Pólya-gamma data augmentation. Note that a ==== routine for ==== is also presented in Ren et al. (2011), but it is based on the bound of Jaakkola and Jordan (2000). As a consequence of the recent theoretical findings in Durante and Rigon (2019), it can be shown that our approach is intimately related to the one of Ren et al. (2011), although being developed by means of seemingly unrelated strategies. Finally, while tractable algorithms such as ==== or ==== could be possibly obtained also for ====, we are not aware of any actual discussion or implementation. Indeed, the analytical derivations might be slightly more complex in the ==== case compared to the ====, as discussed in Section 3.====We shall emphasize that the overarching focus of our contribution is not on developing a novel methodological framework for Bayesian density regression, but on deriving a broad set of routine-use computational strategies under a suitable and tractable representation. To our knowledge this goal remains partially unaddressed, but represents a fundamental condition to facilitate routine implementation of Bayesian density regression by practitioners. The three proposed algorithms are empirically compared in Section 4 using a real data toxicology study, previously considered in Dunson and Park (2008). Section 5 provides concluding remarks.",Tractable Bayesian density regression via logit stick-breaking priors,https://www.sciencedirect.com/science/article/pii/S0378375818300697,3 June 2020,2020,Research Article,172.0
"Domagni Kouakou Francois,Hedayat A.S.,Sinha Bikas Kumar","Department of Mathematics, Statistics, and Computer Science, University of Illinois at Chicago, United States of America","Received 23 January 2020, Revised 14 April 2020, Accepted 21 May 2020, Available online 29 May 2020, Version of Record 8 June 2020.",https://doi.org/10.1016/j.jspi.2020.05.006,Cited by (2),We contemplate an experimental situation in a ====-factorial experiment with acute resource crunch so that we need to conduct just a saturated design [SD] - with the understanding that precision of the estimates cannot be estimated from the data. It is known beforehand which effect(s)/interaction(s) are likely to be negligible. We examine the flexibility to the extent that an experimenter can make a choice of an SD in order to retain information on all the remaining [non-negligible] effects/interactions.,"Two-level factorial designs (TLFD) are widely used in scientific and industrial experimentation for various reasons. Standard textbooks deal with this topic at various lengths — covering concepts such as (i) Unreplicated Full Factorials, (ii) Replicated Full Factorials, (iii) Blocking, (iv) Total, Partial and Balanced/Unbalanced Confounding, (v) Fractional Factorials, etc. Practitioners primarily use TLFD at an early stage of an experimentation to screen potential factors that are involved in the system being investigated. The statistical models underlying TLFD are simple and subject to relatively weak assumptions. Each factor – whether quantitative or qualitative – is assumed to have two levels that are conveniently coded as ==== or ==== in the design matrix which turns out to be a Hadamard matrix. Hadamard matrices have been studied extensively in the literature. As a case in point, see Friedland and Aliabadi (2018) for classical properties of Hadamard matrices. The estimators of the effects/interactions of TLFD are contrasts that are naturally simple to interpret. The effect of a factor is interpreted as a measure of the change in the response variable due to the variation of the factor from low to high — averaged over all other factor levels.====In practice investigators postulate, for one reason or the other, that certain parameters (usually higher order interactions) are unimportant or negligible. When that is the case, it is desirable for them to conduct the experiment with the least number of runs that would ensure unbiased estimation of the important parameters that is, non-negligible main effects and interactions of interest. Regular Fractional Factorial Designs (RFFD) are used in this kind of situation and there is a vast literature available on RFFD. See, for example, Montgomery (1996).====In the framework of a two-level factorial design, one of the drawbacks of RFFD is that the number of runs needed to conduct the experiment is necessarily a multiple of ====. Thus, when the important effects and interactions to be estimated are identified beforehand, using an RFFD may lead to the use of more resources than the bare minimum needed for the estimation of the important effects and interactions. For instance, if the number of factors is ==== and the only important effects are the main effects plus the mean then using an ==== RFFD of resolution ==== would require ==== runs for the experiment. This would actually estimate the ==== main effects plus the mean but also can provide estimates of two other parameters that are known to be negligible.====A Saturated Design (SD) could be used in case of scarce resources when it is clear to the investigator which parameters are important and non-negligible. However, it turns out that the identification of a SD is a challenging problem. Numerous papers available in the literature discuss how to construct SDs under certain conditions. See Hedayat and Pesotan, 1992, Hedayat and Pesotan, 2007. In addition, various computer algorithms have been developed to search for SDs in the TLFD set-up. Some of these are SPAN, DETMAX. See Hedayat and Zhu (2011). It is worth pointing out that when RFFD are used to estimate a certain vector parameter of interest, the estimator of each effect except the mean is a contrast in terms of the runs and it is clear to practitioners that each estimator measures an interaction or the change in the response variable due to the variation of some factor from low to high. The common practice available in the literature is to choose a SD for which the underlying design matrix is non-singular. The Ordinary Least Squares (OLS) method is then used to obtain the estimator of the vector parameter of interest.====The question we may ask is the following “ ====”. Well, if the design matrix is a Hadamard matrix then the answer is trivially ‘yes’ since the SD in that case can be seen as an RFFD. However, when the design matrix is not a Hadamard matrix, the estimator of the vector parameter is given by ====, where ==== is the saturated design matrix. In practice, it is desirable for practitioners to have the estimator of each estimable effect as contrast in terms of the runs for the sake of interpretation. It is interesting to verify that it is indeed so even when the design is not based on a Hadamard matrix. This can be seen as follows.====Let ==== be a non-singular matrix of order ==== with its first column being the vector ==== ( ==== ==== ====). Let ==== denote the column vector of length ====. Then, ==== is a vector of ====’s which means that ====. Hence, whenever ==== is non-singular, ==== which implies that ====. This is equivalent to the statement that the estimates of all model parameters [except the over-all mean] are linear observational contrasts, irrespective of the nature of elements of the matrix ====.====The rest of the paper is organized as follows. In Section 2, we develop a general theory for ‘deletion of exact number of runs’ so as to ensure the estimability of all non-negligible effects/interactions in a saturated ====-factorial experiment. This is done through identification of the runs to be deleted (====), for any given collection of non-negligible effects/interactions. We give some illustrative examples in the case of ====-factorial experiment. It turns out that the admissible runs for deletion may not be unique. Therefore, it is desirable to identify the admissible set for deletion that would ensure the optimality of the design matrix under the D-optimality criterion. Thus, in Section 3, we shed more light on understanding the algorithm developed in Section 2 and explain how it may be used to classify design matrices with respect to the absolute value of their determinants. Then, we take up some illustrative examples in the case of ====-factorial experiments.",On the nature of saturated ,https://www.sciencedirect.com/science/article/pii/S0378375820300641,29 May 2020,2020,Research Article,173.0
"McCloud Nadine,Parmeter Christopher F.","The University of the West Indies at Mona, Jamaica,University of Miami, United States of America","Received 9 January 2019, Revised 29 April 2020, Accepted 5 May 2020, Available online 20 May 2020, Version of Record 21 May 2020.",https://doi.org/10.1016/j.jspi.2020.05.001,Cited by (2),"The matrix that transforms the response variable in a regression to its predicted value is commonly referred to as the ==== of kernel-dependent constants to that of the ANOVA-based hat matrix. Additionally, we document that the trace of the ANOVA-based hat matrix converges to 0 in any setting where the bandwidths diverge. This attrition outcome can occur in the presence of irrelevant continuous covariates or it can arise when the underlying data generating process is in fact of polynomial order.","The ‘hat matrix’ plays a fundamental role in regression analysis. In particular, the trace of the hat matrix is commonly used to calculate degrees of freedom. Degrees of freedom for nonparametric models, a concept pioneered by Hastie and Tibshirani (1990), has been used to test the validity of model assumptions and gauge the significance of covariates via the generalized likelihood ratio statistic (Zhang, 2003); construct variants of F-tests for different null hypotheses (see, e.g., Huang and Chen, 2008, Huang and Su, 2009, Huang and Davidson, 2010, and the references cited therein); and derive procedures for sample size calculations based on non- and semiparametric ANOVA-based F-tests (Gao and Huang, 2016).====For univariate local polynomial estimation of nonparametric models, two prominent hat-matrix approaches have been developed to calculate degrees of freedom. Zhang (2003) uses the trace of the hat matrix stemming directly from the local polynomial method of estimating the unknown conditional mean (Ruppert and Wand, 1994, Fan and Gijbels, 1996), which we characterize as non-ANOVA given that an orthogonal decomposition of the total sum of squares into the constituent explained and residual sum of squares does not exist. Huang and Chen (2008) use the hat matrix implied from an ANOVA decomposition of the total sum of squares into its respective explained and residual components. Thus, the ANOVA and non-ANOVA frameworks stem from differing schools of thought regarding the decomposition of total sum of squares of the regressand which can result in different expressions for the trace of such a hat matrix. For example, as noted by Gao and Huang (2016, pg. 2022), in the univariate local-linear setting, the ANOVA hat matrix takes account of both the constant and the slope estimates to inform degrees of freedom, whilst the non-ANOVA version only uses the constant estimate to inform degrees of freedom. The informational content of both the slope and the intercept is used to advocate on behalf of the use of the ANOVA hat matrix in Gao and Huang (2016). However, Huang and Chen (2008) ANOVA decomposition is local in nature and needs to be integrated to achieve a global hat matrix. In multivariate settings – which are of practical appeal – some users may find the calculation of this global hat matrix, relative to its non-ANOVA counterpart, to be computationally taxing.====Nevertheless, as useful as these methods may currently be, they do not lend themselves to settings with either multiple continuous and discrete covariates or irrelevant covariates, both of which are a prominent feature of many empirical applications. Here our goal is to generalize the work of Zhang (2003) and Huang and Chen (2008) to allow for calculation of degrees of freedom in a multivariate local polynomial setting with a mix of continuous and discrete covariates, some of which may be irrelevant. From this platform we can compare the effective number of parameters, ====, stemming from the trace of the global ANOVA hat matrix (Huang and Chen, 2008) to its non-ANOVA counterpart (Zhang, 2003). Our work here is important given that from both Zhang (2003) and Huang and Chen (2008) it is not obvious ex ante how their theory would hold for discrete only data, data with irrelevant or mixed continuous–discrete covariates.====Given this lack of clarity from the extant literature our theoretical insights fill a necessary gap in the literature on the limiting behavior of ANOVA and non-ANOVA hat matrices allowing us to make the following nontrivial contributions. ====, both the ANOVA and non-ANOVA hat matrices do not at present offer an obvious path for inclusion of discrete covariates in the construction of degrees of freedom calculations. Given that discrete kernels give weight one to common cells, they would not appear directly in either form of the existing hat matrices. Our theory establishes that in the presence of mixed data, the discrete support influences the trace of both hat matrices. Our theoretical work is the first to formally establish this result for both hat matrices.====, in the presence of mixed discrete and continuous covariates, the difference in asymptotic expressions of the trace of the ANOVA and non-ANOVA hat matrices is driven by a linear combination of moments of the underlying continuous kernel. For example, using a bivariate regression, with any kernel from the popular symmetric beta kernel class, and for local constant, local linear, and local cubic estimators, we show that the absolute differences between asymptotic expressions for the trace of the ANOVA and non-ANOVA hat matrices lie in the unit interval. This suggests that the non-ANOVA hat matrix taken directly from the multivariate local polynomial estimator can be used to approximate degrees of freedom for the ANOVA hat matrix quite effectively. This may offer computational expediency as the ANOVA hat matrix requires integration across each of the continuous covariates.====, we show that the non-ANOVA nonparametric framework also lends itself well to meaningful asymptotic expressions for the trace of the implied hat matrix in the presence of irrelevant continuous and discrete covariates. Intuitively, the trace of the non-ANOVA hat matrix is the ratio of two kernel terms that are of equal order of magnitude in the bandwidth vector for the continuous covariates; thus, the influence of the bandwidth vector for the irrelevant continuous covariates on the kernel ratio is dominated by the influence of its relevant counterpart.====Furthermore, we document that the trace of the ANOVA-based hat matrix converges to 0 in any setting where the bandwidths diverge. We demonstrate that the bandwidth vector for the irrelevant continuous covariates has an attrition effect on the trace of the ANOVA hat matrix resulting in the latter converging to zero in probability. Although the trace of the ANOVA hat matrix is also a ratio of two kernel terms, these kernel terms are of different orders of magnitude in the bandwidth vector for the continuous covariates. This paves the way for a sizable influence of the bandwidth vector for the irrelevant continuous covariates relative to its relevant counterpart. Two implications of this attrition effect are that the nonparametric ANOVA-based F-tests developed by Huang and Chen (2008), Huang and Su (2009) and Huang and Davidson (2010) may not be operational in the presence of such covariates and the sample size calculations develop in Gao and Huang (2016) may no longer go through; degrees of freedom from the non-ANOVA framework may be suitable substitutes for their ANOVA counterparts when irrelevant continuous variables are likely to be present in the underlying nonparametric model.====, we formalize the trace concept of the non-ANOVA and ANOVA hat matrices which are predicated on ==== discrete covariates. In the presence of only relevant discrete covariates, the traces of the ANOVA and non-ANOVA hat matrices all converge in probability to the cardinality of the support of the covariates. Although this result also holds when irrelevant discrete covariates are present, the asymptotic trace values in this case can exceed their purely relevant counterparts. This latter result draws on the theoretical contributions of Ouyang et al. (2009) who establish, in a purely discrete-covariate setting with least-squares cross-validation (LSCV), that the irrelevant regressors cannot be smoothed out with probability approaching one as the sample size increases. This also means that these asymptotic trace values may exhibit larger variances in the presence of irrelevant discrete covariates.====The remainder of the paper is organized as follows. Section 2 derives asymptotic results for the trace of the non-ANOVA-based hat matrix from the multivariate local polynomial model with a mix of continuous and discrete covariates. Under similar model specifications, Section 3 derives asymptotic results for the trace of the ANOVA-based hat matrix. Section 4 studies the asymptotic behavior of the trace of both the ANOVA and non-ANOVA hat matrices in the presence of relevant and irrelevant regressors. Section 5 explores the implications of our theoretical results using simulated data. Section 6 contains the conclusion. We place all proofs in a supplementary technical appendix.",Calculating degrees of freedom in multivariate local polynomial regression,https://www.sciencedirect.com/science/article/pii/S0378375820300598,20 May 2020,2020,Research Article,174.0
Matsuda Takeru,"Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan,RIKEN Center for Brain Science, Saitama, Japan,Department of Statistics and Biostatistics, Rutgers University, NJ, USA","Received 30 April 2019, Revised 9 May 2020, Accepted 9 May 2020, Available online 16 May 2020, Version of Record 20 May 2020.",https://doi.org/10.1016/j.jspi.2020.05.005,Cited by (4),We investigate predictive density estimation under the ,"Suppose that we have ==== independent observations ==== from a probability distribution ==== and predict the future observation ==== from a probability distribution ==== by using a predictive density ====, where ==== is an unknown parameter. Let ==== be a loss function that measures the closeness of a predictive density ==== to the true distribution ====. Then, the risk function of the predictive density ==== is defined as the expected loss: ====A predictive density ==== is called minimax if it minimizes the maximum risk: ====A predictive density ==== is said to dominate another predictive density ==== if ====holds for every ==== and also ====holds for some ====. A predictive density ==== is called the Bayesian predictive density with respect to a prior ==== if it minimizes the average risk: ====Predictive density estimation under the Kullback–Leibler loss has been well studied. Aitchison (1975) showed that the Bayesian predictive density with respect to a prior ==== is given by ====For the normal model with known variance, the Bayesian predictive density based on the uniform prior is the best equivariant and also minimax (Liang and Barron, 2004). Komaki (2001) showed that the Bayesian predictive density based on the Stein prior dominates that based on the uniform prior in dimension larger than or equal to three. George et al. (2006) generalized this result and proved that Bayesian predictive densities based on superharmonic priors dominate that based on the uniform prior. Brown et al. (2008) gave a characterization of admissible predictive densities. For the normal model with unknown variance, Kato (2009) found a Bayesian predictive density that dominates the best equivariant one. For general location-scale families, Komaki (2007) showed that the Bayesian predictive densities based on superharmonic priors asymptotically dominate the best equivariant one.====Predictive density estimation has been investigated for other loss functions as well. Corcuera and Giummole (1999) studied the class of alpha divergence losses, which includes the Kullback–Leibler loss, and derived an explicit form of the Bayesian predictive density. Kubokawa et al. (2017b) and Kubokawa et al. (2017a) considered ==== loss and ==== loss, respectively. These studies showed that improved predictive density estimation depends on the loss function.====In this study, we investigate predictive density estimation when the loss function is defined by the Wasserstein distance, which is a distance function between probability measures based on the metric of the underlying space (Olkin and Pukelsheim, 1982, Villani, 2010) and has been widely used in machine learning and computer vision (Peyré and Cuturi, 2019). Since the Wasserstein distance is derived as the optimal transportation cost between distributions, predictive density estimation under the Wasserstein loss is suitable for resource allocation. Namely, suppose that we predict the spatial distribution of demand for some resource and then distribute it accordingly. To reduce the cost of re-transportation, predictive densities with smaller Wasserstein loss are preferable. Here, we consider ==== Wasserstein distance and focus on location families and location-scale families. For both families, plug-in densities are shown to form a complete class (Berger, 1980). For location families, we prove that the Bayesian predictive density is given by the plug-in density with the posterior mean of the location parameter. For location-scale families, we prove that the Bayesian predictive density is given by the plug-in density with the posterior mean of the location and scale parameters. We give Bayesian predictive densities that dominate the best equivariant one in normal models.====This paper is organized as follows. In Section 2, we present useful properties of the Wasserstein distance and also review improved estimation of location and scale parameters. In Sections 3 Location family, 4 Location-scale family, we investigate predictive density estimation under the ==== Wasserstein loss for location families and location-scale families, respectively. In Section 5, we give concluding remarks.",Predictive density estimation under the Wasserstein loss,https://www.sciencedirect.com/science/article/pii/S037837582030063X,16 May 2020,2020,Research Article,175.0
"Shi Gongming,Du Jiang,Sun Zhihua,Zhang Zhongzhan","College of Applied Sciences, Beijing University of Technology, Beijing, China,School of Mathematic Sciences, University of Chinese Academy of Sciences, Beijing, China","Received 10 September 2019, Revised 25 March 2020, Accepted 6 May 2020, Available online 15 May 2020, Version of Record 20 May 2020.",https://doi.org/10.1016/j.jspi.2020.05.003,Cited by (4),". The finite sample properties of the test statistic are illustrated through extensive simulation studies. A real data set of 24 hourly measurements of ozone levels in Sacramento, California is analyzed by the proposed test.","A functional linear model is a powerful tool in functional data modeling with broad applications in the fields of chemometrics, biomedical studies, and environmental science, among many others. It is of the following form ====where ==== is a scalar response, the regressor ==== is a square integrable random function with mean zero defined on some compact set ==== of ====, ==== is the unknown slope function on ====, and ==== is a random error.====In the last two decades, the functional linear model has received considerable attention. Most of the existing literature concerns estimating the unknown coefficient function ====. The details on the estimation of the functional linear mean regression model can refer to Cardot et al., 2003, Müller et al., 2005, Cai and Hall, 2006, Hall and Horowitz, 2007, James et al., 2009, Crambes et al., 2009 and Yuan et al. (2010). Quantile regression, as a valuable alternative of mean regression, is attractive in many applications, since it is not sensitive to outliers and has the power of making inference on the conditional distribution of the response at various quantile levels. By assuming the ====th ==== quantile of the error term ==== given the covariate ==== is zero, Cardot et al. (2005), Ferraty et al. (2005), Chen and Müller (2012), Kato (2012), Shin and Lee (2016) investigated the estimation of the functional linear quantile regression model.====The works mentioned above are based on the assumption that the scalar response and the functional covariate follow a linear relationship. It is an important problem to check whether this linear relationship is reliable. In the context of the mean regression, Patilea et al., 2012, García-Portugués et al., 2014 and Cuesta-Albertos et al. (2019) developed different methods to check the adequacy of such a linear structure. To the best of our knowledge, there is no existing work investigating the adequacy check of the functional linear quantile regression model, which is also a vital issue to avoid the model misspecification.====This article aims for developing a model checking method for the functional linear quantile regression model. To be specific, the following hypothesis is considered: ====with the alternative hypothesis which is the negation of the null hypothesis: ====This problem is undoubtedly very challenging. The objective loss function of the quantile regression is non-smooth. The proofs of the asymptotic properties of the proposed test statistic require new techniques, instead of the general skills employed in the statistical inference of the mean regression context. Moreover, the approximation of the infinite-dimensional parameter by the finite-dimensional parameter in the estimation procedure increases the complexity of the related computation and theoretical proofs of the test statistic.====By combining the nonparametric kernel method with the functional principal component analysis, we propose a U-process based test method for the functional linear quantile regression model. Under mild conditions, we proved that the proposed test statistic has an asymptotic normal distribution under the null hypothesis and can detect Pitman local alternatives. Because the limit distribution of the proposed test statistic involves the unknown density function of the random error, it is difficult to obtain the feasible critical values from the asymptotic normal distribution of the test statistic. To solve this problem, we propose a wild bootstrap method to approximate the critical value of the test. To investigate the finite sample performance of the proposed test method, we conduct extensive Monte Carlo simulations. The simulation results show that the proposed test method performs well.====The rest of the paper is organized as follows. Section 2 develops the test method. In Section 3, the asymptotic behaviors of the test statistic are investigated. Section 4 describes the determination of the critical value. The finite sample properties of the test are examined in Section 5. In Section 6, an empirical illustration of 24 hourly measurements of ozone levels is presented. All proofs are deferred in Appendix.",Checking the adequacy of functional linear quantile regression model,https://www.sciencedirect.com/science/article/pii/S0378375820300616,15 May 2020,2020,Research Article,176.0
"He Xinrui,Bartroff Jay","Acumen LLC and the SPHERE Institute, Burlingame, CA, USA,Department of Mathematics, University of Southern California, Los Angeles, CA, USA","Received 9 December 2019, Revised 1 May 2020, Accepted 6 May 2020, Available online 15 May 2020, Version of Record 22 May 2020.",https://doi.org/10.1016/j.jspi.2020.05.002,Cited by (2)," multiple testing error metric that is bounded between multiples of FWE in a certain sense. This class of metrics includes FDR/FNR but also pFDR/pFNR, the per-comparison and per-family error rates, and the false positive rate. Our analysis includes asymptotic regimes in which the number of null hypotheses approaches ==== as the type 1 and 2 error metrics approach 0.","For decades, the problem of how to efficiently and powerfully test multiple statistical hypotheses while controlling some notion of type 1 error frequency has been fundamental and active in the statistics methodology literature. The majority of this research has concerned testing procedures which operate on fixed-sample data, typically in the form of a collection of ====-values, one for each null hypothesis, which are combined in some way to reach reject/accept decisions for each null. Recently, driven by applications where data is streaming or arrives sequentially, multiple testing procedures that can handle sequential data have been proposed and studied. Applications with data of this type include the analysis of streaming internet data (Wegman and Marchette, 2003), multiple channel signal detection in sensor networks (Dragalin et al., 1999, Mei, 2008), high throughput sequencing technology (Jiang and Salzman, 2012), and multi-arm and multiple endpoint clinical trials (Bartroff and Lai, 2010).====Existing multiple testing procedures for sequential data occur in essentially two forms. In one, the individual data streams can be terminated at different times (e.g., Bartroff and Song, 2014, Bartroff and Song, 2015, Bartroff, 2018, Malloy and Nowak, 2014). In the other form, termination of sampling must occur at the same time for all streams (e.g., De and Baron, 2012a, De and Baron, 2012b, Song and Fellouris, 2017, Song and Fellouris, 2019). The applications mentioned in the previous paragraph span both of these forms. Recently, a third form of “sequential” multiple testing procedure has been studied in which not the data, but rather the hypotheses themselves arrive sequentially in time, each with its own fixed-sample ====-value. Javanmard and Montanari (2018) proposed an FDR-controlling procedure in this setup, which Chen and Arias-Castro (2017) showed to be optimal under certain distributional assumptions. A recent manuscript by Zrnic et al. (2018) studies optimality more generally in this setup.====The current paper adopts the second form described above in which all the data streams are terminated at the same time, and we investigate the optimal choice of that stopping rule subject to desired bounds on the type 1 and 2 error metrics, asymptotically as these bounds approach zero at arbitrary rates. This is done under the condition of prior information on the number of false null hypotheses (“signals”) in the form of a known number of signals, or known bounds on this number. The latter case of known bounds on the number of signals includes the non-informative setting with lower bound ==== and upper bound equal to the total number of null hypotheses.==== ==== Our work springs from and generalizes the results of Song and Fellouris (2017) who found asymptotically optimal procedures in these settings when the error metrics are type 1 and 2 familywise error rates (FWEs). By modifying the procedures of Song and Fellouris, we find the corresponding asymptotically optimal procedures for controlling the false discovery rate (FDR) and its type 2 analog, the false non-discovery rate (FNR). Further, we are able to find the asymptotically optimal procedures for controlling ==== multiple testing error metric that is bounded between multiples of FWE in a certain sense, which includes FDR/FNR, the positive false discovery and non-discovery rates (pFDR and pFNR), the per comparison error rate, and other metrics. Further, we are able to consider asymptotic regimes in which the number ==== of null hypotheses approaches ====.====After introducing notation and describing our approach in Section 2, the case of the number of signals known exactly is addressed in Section 3 where first the general result for arbitrary error metrics is stated, followed by its application to FDR/FNR and pFDR/pFNR. The case of bounds on the number of signals is addressed in Section 4 where, again, the general result for arbitrary error metrics followed by its application to FDR/FNR and pFDR/pFNR. Simulation studies of procedures for FDR/FNR control in finite-sample settings are presented in Section 5, and we conclude with a discussion of related issues in Section 6.",Asymptotically optimal sequential FDR and pFDR control with (or without) prior information on the number of signals,https://www.sciencedirect.com/science/article/pii/S0378375820300604,15 May 2020,2020,Research Article,177.0
"Nadarajah K.,Martin Gael M.,Poskitt D.S.","Department of Economics, The University of Sheffield, United Kingdom,Department of Econometrics and Business Statistics, Monash University, Australia","Received 10 April 2019, Revised 14 April 2020, Accepted 25 April 2020, Available online 11 May 2020, Version of Record 12 June 2020.",https://doi.org/10.1016/j.jspi.2020.04.010,Cited by (1),"We use the jackknife to bias correct the log-periodogram regression (LPR) estimator of the fractional parameter in a stationary fractionally integrated model. The weights for the jackknife estimator are chosen in such a way that bias reduction is achieved without the usual increase in ====, with the estimator viewed as ‘optimal’ in this sense. The theoretical results are valid under both the non-overlapping and moving-block sub-sampling schemes that can be used in the ","Data on many climate, hydrological, economic and financial variables exhibit dynamic patterns characterized by a long lasting response to past shocks. Notable examples include, water levels in rivers (Hurst, 1951), rainfall (Gil-Alana, 2012), aggregate output (Diebold and Rudebusch, 1989, Hassler and Wolters, 1995), interest rates (Baillie, 1996), exchange rates (Cheung, 2016) and stock market volatility (Bollerslev and Mikkelsen, 1996, Andersen et al., 2003). Such ‘long memory processes’ are characterized by non-summable autocovariances that decline at a (slow) hyperbolic rate, in contrast to the usual exponential, and summable, decay associated with a short memory process; the fractionally integrated autoregressive moving average (ARFIMA) model of Adenstedt, 1974, Granger and Joyeux, 1980 and Hosking (1981) being a popular representation. Equivalently, a stationary (potentially) long memory process, ====, ==== , can be represented by the spectral density, ====where the fractional differencing parameter ==== satisfies ====, and ==== is an even function that is continuous on ====, is bounded above and bounded away from zero, and satisfies ====. The process is said to have ==== when ====, ==== when ==== and ==== when ====. The factor ==== controls the (remaining) short memory behaviour associated with the process. For detailed expositions of processes described by (1), including applications, see, Beran (1994), Doukhan et al. (2003) and Robinson (2004).====In estimating the parameter ====, the semi-parametric log-periodogram regression (LPR) estimator of Geweke and Porter-Hudak (1983) and Robinson, 1995a, Robinson, 1995b has been widely used, due to the simplicity of its construction as an ordinary least squares (OLS) estimator, and its avoidance of potentially incorrect specification of the short memory component. However, consistency of the LPR estimator is achieved only at the cost of both a slower rate of convergence than the usual parametric rate and substantial finite sample bias in the presence of ignored short run dynamics (see, for example, Agiakloglou et al., 1993 and Nielsen and Frederiksen, 2005).====Given this well-documented bias, ==== of the LPR estimator has been a focus of the literature. Andrews and Guggenberger (2003), for example, include additional frequencies, to degree ==== for ====, in the log-periodogram regression that defines the LPR estimator, producing an estimator (denoted hereafter by ====) whose bias converges to zero at a faster rate than that of the LPR estimator (recovered by setting ====), when ====. Alternative analytical procedures appear in Moulines and Soulier (1999), Hurvich and Brodsky (2001) and Robinson and Henry (2003), whilst a method based on the pre-filtered sieve bootstrap has been introduced by Poskitt et al. (2016). Critically, all such bias-correction methods come at a cost: namely, an increase in asymptotic variance. Notably, Guggenberger and Sun (2006) produce a weighted average of LPR estimators over different bandwidths that achieves the same degree of bias reduction as ==== for any given ====, but with less variance inflation. This estimator, plus that of Poskitt et al. (2016), serve as important comparators for the alternative bias-corrected estimator that we develop herein.====The approach to bias adjustment adopted in this paper applies the jackknife principle, with the bias-corrected estimator constructed as a weighted average of LPR estimators computed, in turn, from the full sample and ==== sub-samples of a given length. The sub-samples may be created by using either the non-overlapping or the moving-block method. Motivated by the jackknife technique proposed by Chen and Yu (2015) in a unit root setting, weights are chosen to remove bias up to a given order and, at the same time, to minimize the increase in asymptotic variance. The weights are ‘optimal’ in this sense and the associated jackknife estimator referred to as ‘optimal’ accordingly. In the fractional setting, with the LPR estimator being the method to be adjusted, these optimal weights involve two types of covariance terms: ==== covariances between the full-sample and sub-sample log-periodogram ordinates, and ==== covariances between distinct sub-sample log-periodogram values. These covariance terms may, in turn, be represented by cumulants of the discrete Fourier transform (DFT) of the time series. Building on results in Brillinger (1981, Chapters 2 and 4), we first derive closed-form expressions for the association between the corresponding DFTs in terms of cumulants. These expressions are used to derive the form of dependence between the periodograms (at a given ordinate or at different ordinates) associated with the full sample and the sub-samples, which allows us to obtain closed-form expressions for the covariances terms, ==== and ====, and, hence, to evaluate the optimal weights.====We prove the consistency and asymptotic normality of the optimal jackknife estimator. Most notably, we establish that the convergence rate and asymptotic variance are equal to those of the unadjusted LPR estimator. This implies that there is ==== inflation in asymptotic efficiency compared to the ==== LPR estimator of ====, despite the bias reduction that is achieved. This compares with Guggenberger and Sun (2006), in which the goal is to produce an estimator (for a given value of ====) with an asymptotic variance that is smaller than that of the corresponding bias-adjusted estimator of Andrews and Guggenberger (2003), as based on the same value of ====, ====. In particular, in the case where ====, and no bias adjustment is achieved (with ==== equivalent to the raw LPR estimator), the estimator of Guggenberger and Sun is still biased, but with a (possibly) reduced asymptotic variance. In addition, in contrast with Guggenberger and Sun, and the other analytical bias adjustment methods cited above, our theoretical results do not rely on the assumption of Gaussianity. Specifically, expressions for the dominant bias term and variance of the LPR estimator – needed in the construction of the jackknife estimator and as originally derived by Hurvich et al. (1998) for fractional ==== processes – are shown to hold under non-Gaussian assumptions. Hence, all theoretical results for the bias-adjusted estimator hold under similar generality.==== ====Extensive simulation exercises are conducted in order to compare the finite sample performance of the jackknife estimator with that of alternative approaches, including the bias-adjusted estimators of Guggenberger and Sun (2006) and Poskitt et al. (2016). Results show that certain versions of the optimally bias-corrected jackknife estimator out-perform the alternative bias-adjusted estimators of Guggenberger and Sun and Poskitt et al., in terms of bias-reduction and root mean squared error (RMSE), with the RMSE being somewhat close to, or even smaller than, that of the LPR in some cases. In the empirically realistic case where the true values of the parameters – required in order to evaluate the optimal weights in the jackknife estimator – are unknown, we implement the jackknife technique using an iterative procedure. This feasible version of the estimator does not consistently out-perform either the bootstrap-based estimator of Poskitt et al. or (a feasible version of) the method of Guggenberger and Sun, but is not substantially inferior, in terms of either bias or RMSE, and is sometimes still the least biased estimator of all.====We assess the finite sample performance of all bias-adjusted estimators under scenarios of both correct model specification and misspecification and, for completeness, parametric methods based on maximum likelihood estimation (MLE) and pre-whitening are included in the assessment.==== ==== As would be anticipated, given the asymptotic efficiency of MLE under correct specification, no semi-parametric method out-performs the optimal parametric approach in terms of RMSE in this case. However, when the short memory dynamics need to be estimated, a semi-parametric method is typically less biased than both parametric methods. When the model is misspecified, the semi-parametric methods are dominant in terms of both bias and RMSE, with the feasible jackknife estimator producing the least bias in some cases, most notably when the true process has a moving average component that is omitted in the model specification.====In summary, the paper makes two important contributions to the literature on semi-parametric estimation in fractional models. First, a new estimator is derived that bias corrects the popular LPR estimator to a given order, with no associated variance inflation asymptotically. Second, that estimator is shown to perform well in finite samples, under ideal conditions, and to hold its own in empirically relevant scenarios, relative to existing comparators.====The remainder of the paper is organized as follows. In Section 2, we introduce two log-periodogram regression estimators; namely, the LPR estimator originally proposed by Geweke and Porter-Hudak (1983) and the particular bias-reduced estimator of Guggenberger and Sun (2006). In Section 3, we develop the new jackknife estimator that accommodates both bias correction and variance minimization via the appropriate choice of weights. All theoretical results pertaining to the construction of the afore-mentioned covariance terms, and the resultant asymptotic properties of the optimal estimator, are given in Section 4. Section 5 documents the finite sample performance of the estimator by means of a Monte Carlo study.====The proofs of all results are contained in Appendix A, while Appendix B provides various technical results, including the evaluation of the covariances required for the construction of the weights for the optimal jackknife estimator. Appendix C contains Table 2, Table 3, Table 4, Table 5, Table 6, Table 7, Table 8, Table 9, Table 10, Table 11, Table 12, Table 13, Table 14, Table 15, which document the results of the Monte Carlo study, with these results summarized briefly in briefly in Table 16. The following notation is used throughout: “====” denotes convergence in probability, “==== ” denotes convergence in distribution, and “====” is used to indicate the limit as ====, (unless otherwise stated). The ====th -order spectral density function of the time series ==== is denoted by ====, where ==== are fundamental frequencies. For instance, the density function given in (1) is the second-order spectral density of ====.",Optimal bias correction of the log-periodogram estimator of the fractional parameter: A jackknife approach,https://www.sciencedirect.com/science/article/pii/S0378375820300483,11 May 2020,2020,Research Article,178.0
"Díaz-García José A.,Caro-Lopera Francisco J.","Independent Scholar,Universidad de Medellín, Faculty of Basic Sciences, Carrera 87 No.30-65, of. 4-216, Medellín, Colombia","Received 15 March 2018, Revised 30 April 2020, Accepted 30 April 2020, Available online 11 May 2020, Version of Record 21 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.012,Cited by (1),This paper derives the elliptical matrix variate version of the well known univariate Birnbaum–Saunders distribution of 1969. A generalisation based on a ,"Some restricted situations in statistics allow that hypothesis for observational or experimental data is governed by univariate tests. But the complex reality involves multivariate or matrix variate decision problems with several dependent variables acting simultaneously.====This intricate dynamics motivates the multivariate or matrix variate generalisations of the univariate probability distributions. However, the successful generalisations have required the creation of advanced mathematics, usually out of the scope of popular books and high impact journals of decision sciences. Sometimes the techniques and representations are not unique, then the associated theoretical relations enlarge the problem. For example, consider the generalisation of the univariate chi-square distribution into the termed Wishart distribution. This matrix variate distribution required the creation of the advanced zonal polynomials of matrix arguments in the 50’s. Three different methods reached the non singular central distribution: the singular value decomposition (SVD), the polar factorisation and the QR decomposition; see for example James (1954), Herz (1955) and Roy (1957), respectively. However, computation of the joint latent root distribution in the central case took more than 50 years. In fact, the relations among the addressed three densities are unclear today. Moreover, the non central Wishart case resides as an open problem. The distribution demanded the construction of the invariant polynomials of several matrix arguments of Davis (1979), but the calculation is impossible even in this time of super computers.====Now, vector or matrix extensions of univariate random variables are achieved in two ways:====For example, suppose a random variable ==== with a ==== of ==== degrees of freedom, that is, ====. Assume also that the random vector ==== follows an ====-====, with mean vector ==== and covariance matrix ====. In notation, ====, where ==== is a vector of zeros and ==== is the ==== identity matrix. Recall that ====; where ==== stands for equally distributed and ==== represents the Euclidian norm of ====. Thus, the multivariate version of the random variable ==== is required.====The first method proceeds as follows: let ====, with ====, ====, ==== and ====. Then, define the random variables ====, ==== and the random vector ====. Thus the ==== ==== ==== ==== has been derived, where ====, ====; see Libby and Novick (1982). Using the same technique, the multivariate version of the random variable ==== can be found. Sometimes the matrix case is obtained directly from the multivariate (vector) version: let ==== be the vectorisation of the matrix ====, with ====. Define ====, then the distribution of ==== follows from the distribution of the random vector ====.====Alternatively, the matrix variate extension of the ====-distribution became more popular than the addressed multivariate case. Assume ==== independent ====, with ==== and ====. Define the random matrix ====If ====, then ==== is positive definite (====) and ==== is said to have a ====. Otherwise, if ====, then ==== is positive semidefinite, (====) and ==== is said to have a ====. The associated notations are given by ==== and ====, respectively, see Srivastava and Khatri (1979) and Muirhead (2005), among many others. If ====, ==== is a scalar, say ====, then ====. Observe also the impossibility of a vector version derived from the Wishart matrix ====. In addition, an element ==== of ==== does not follow a ====-distribution. Note that, if the univariate random variable is a function of ====, then the extension under a matrix transformation must be a random square matrix, strictly, a random symmetric matrix, see Cadet (1996), Olkin and Rubin (1964) and Muirhead (2005), and references therein. Thus, the matrix version includes the univariate case, but the vector case does not exist. Moreover, the elements of the matrix do not follow the original univariate distribution.====However, a matrix variate version via element-to-element has not order constraint. The vectorial and the univariate cases can be derived directly from the matrix case, and all the elements of the matrix follow the original univariate distribution, see Chen and Novick (1984) and Libby and Novick (1982).====Extreme unusual cases allow equivalence among the vector version, the element-to-element representation and the matrix transformation. This occurs in the ====, which is a consequence of a property for the t-distribution family, see Kotz and Nadarajah (2004, p. 2, 4). For instance, a random ====-dimensional vector with ==== distribution can be defined in two ways. Let ==== where ==== is independent of ====, then the following definitions are known: ====with ====, see Muirhead (2005, Theorem A9.3, p. 588) and ==== a constant vector.====Nevertheless, this unusual property is not fulfilled in the matrix case. Consider the sample ==== of a multivariate population with ==== distribution, and consider the matrix ====, then ====where ====, and ====. But the random matrix ==== does not have the same distribution under the above two representations, even for equally distributed rows. In the first representation, ==== has a ==== ====, but in the second expression ==== has a ==== ====, see Kotz and Nadarajah (2004, p. 2, 4). Also, note that the matricvariate ====-distribution cannot be obtained from the matrix-variate ====-distribution, and vice versa.====Now we focus on the distribution of this work, the ====, introduced by Birnbaum and Saunders (1969). It appeared in the context of a lifetime model for fatigue failure caused by cyclic loading, where the failure obeys the development and growth of a dominant crack. A more general derivation was given by Desmond (1985) based on a biological model.====The original univariate random variable was supported by a normal distribution, then the Gaussian Birnbaum–Saunders random variable ==== is the distribution of ====where ====. A fact denoted by ====, where ==== is the shape parameter, and ==== is the scale parameter and the median value of the distribution. Then, the inverse relation establishes that ====
 Díaz-García and Leiva-Sánchez, 2005, Díaz-García and Leiva-Sánchez, 2006 propose a generalisation of the Birnbaum–Saunders distribution, replacing the Gaussian hypothesis in (2) by a ====, i.e. they assume that ====. Recall that the density function of ==== is defined as ====, for ====. Therefore, (1) defines the ====, which is denoted by ====. Note the long delay for the elliptical univariate version. In fact, the element-to-element elliptical matrix variate version of Birnbaum and Saunders (1969) was published very late in Caro-Lopera et al. (2012). It demanded a theory to connect the Hadamard product and the usual matrix product. Then, Sánchez et al. (2015) performed estimation for the corresponding matrix parameters. Finally, a second element-to-element matrix version was derived by Caro-Lopera and Díaz-García (2016) in terms of the named diagonalisation matrix. But a matrix transformation has been elusive in the literature of the matrix variate distribution theory. Now, the importance of the Birnbaum–Saunders distribution is indisputable, recently Balakrishnan and Kundu (2019) made a detailed compilation of this distribution. That review of 108 pages and 281 references, describes widely and profusely the univariate and multivariate cases in a long history since the 60s. However, the matrix variate case was covered in only 1 existing reference (Caro-Lopera et al., 2012).====Finally, the differences between the element-to-element representation and the matrix transformation version of this paper can be highlighted as follows: Both matrix versions have only one aspect in common, they include the univariate generalised Birnbaum–Saunders distribution as a particular case. However, for higher dimensions the new version provides a natural way of introducing matrix distributions from the univariate case. In fact, the proposed matrix version allows the use of the classical matrix variate distribution theory, matrix transformations and general inference, because it is set in terms on matrices, instead of the elements of the matrix. The key point for the solution is shown in the next table.====The exposition of the solution is organised as follows: in Section 2 some preliminary results and new Jacobians are established. Then the main result of the paper is derived in Section 3. Basic properties are also studied and the expected corollaries are obtained. Finally, a complete example for estimation and comparison under a subfamily of elliptical distributions is developed in Section 4.",Matrix variate Birnbaum–Saunders distribution under elliptical models,https://www.sciencedirect.com/science/article/pii/S0378375820300562,11 May 2020,2020,Research Article,179.0
"Neumann André,Bodnar Taras,Dickhaus Thorsten","Institute for Statistics, University of Bremen, Bibliothekstraße 1, D-28359 Bremen, Germany,Department of Mathematics, Stockholm University, Roslagsvägen 101, SE-10691 Stockholm, Sweden","Received 6 December 2018, Revised 27 April 2020, Accepted 27 April 2020, Available online 7 May 2020, Version of Record 20 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.011,Cited by (5),"Many estimators for the proportion of true null hypotheses in the literature, which are defined for independent ====-values, struggle under dependence. In particular, the variance of the classical Schweder–Spjøtvoll estimator increases with the degree of dependence among the ====-values and bagging to reduce the variance of the estimator. The theoretical validity of the resulting Bootstrap-Schweder–Spjøtvoll procedure is analyzed and its performance is illustrated on simulated data.",None,Estimating the proportion of true null hypotheses under dependency: A marginal bootstrap approach,https://www.sciencedirect.com/science/article/pii/S0378375820300495,7 May 2020,2020,Research Article,180.0
Clairon Quentin,"University of Bordeaux, Inria Bordeaux Sud-Ouest, Inserm, Bordeaux Population Health Research Center, SISTM Team, UMR 1219, France","Received 15 April 2019, Revised 19 March 2020, Accepted 23 April 2020, Available online 5 May 2020, Version of Record 14 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.007,Cited by (4)," brought by our approach to the problem comparing to exact methods such as Non-linear least squares. Moreover, this discrete optimal control based procedure is computationally less intensive and more accurate in sparse sample case than the one based on continuous control techniques. We finally test our approach on a real data example.","We are interested by parameter estimation in Ordinary Differential Equation (ODE) models of the form ====where the state ==== is in ====, ==== is a vector field from ==== to ====, ==== is a parameter that belongs to a subset ==== of ====, ==== is a functional parameter from ==== to ==== and ==== is the initial condition that belongs to a subset ==== of ====. ODEs are much used in practice as they provide an efficient framework for analyzing and predicting complex systems (see e.g. Fall et al., 2002, Goldbeter, 1997, Mirsky et al., 2009, Wu et al., 2014). In particular, there has recently been focus on joint use of ODE models and control theory methods for the purpose of optimal treatment design (Guo and Sun, 2012, Agusto and Adekunle, 2014, Zhang and Xu, 2016).====Our aim is to estimate the true parameters, denoted by ==== and ====, starting from data ====, that are realizations of an observation process for ====
 ====on the observation interval ==== where ==== is the solution of (1) for ====, ==== and ====, ==== is a ==== observation matrix and ==== is centered observation noise. That is, we want to estimate ==== starting from discrete, partial and noisy observations of ==== at observation times ====. In absence of ====, estimation of ==== is a standard parametric nonlinear regression problem and can be solved by classical methods such as Nonlinear Least Squares (NLS), Maximum Likelihood Estimation (MLE), or Bayesian Inference (Esposito and Floudas, 2000, Li et al., 2005, Rodriguez-Fernandez et al., 2006, Wu et al., 2010). However, in the case of ODE models, there is a risk of an ill-posed inverse problem (Engl et al., 2009, Stuart, 2010).====To explain why, let us denote as ==== the solution to (1). The Fisher information matrix which controls the Cramer–Rao bound is proportional to ====. Instabilities in estimation arise when the matrices ==== are badly-conditioned because in this case the inverse problem is very sensitive to any source of perturbations and the objective function (NLS or MLE criteria) is nearly flat around its minimum. This practical identifiability problem can be measured by computing the spectrum ==== of ==== and is associated to a weak condition number ====. The problem arises in part from the observation process, the sparsity and location of the observation times and also from the need to estimate the nuisance parameter ====. Complication in ODEs also arises due to the complex geometry of the manifold ==== induced by the mapping ==== where there can be a small number (in comparison with ====) of important directions of variation very skewed from the original parameter axes (Gutenkunst et al., 2007, Transtrum et al., 2011, Transtrum et al., 2015). This situation is termed sloppiness and leads to a regular and widespread distribution of the eigenvalues ==== with no clear one to one correspondence between the eigenvectors of ==== and the original ODE parametrization. Numerous ODEs used for example in systems biology (Gutenkunst et al., 2007) and neuroscience (Leary et al., 2015) have been identified as sloppy. Sloppiness is a phenomenon arising from interactions between intrinsic system properties and the experimental design, it is due to the sparse and block structure of ==== with highly correlated entries (Tonsing et al., 2014). Since we cannot clearly distinguish important parameters from the others, there is no clear mechanism to suppress irrelevant parameters in the model. Moreover, methods based on optimal experimental design to circumvent sloppiness can lead to experiments which push the system in a state where the assumed model is no longer valid. This can cause model error problems when trying to estimate parameters from the new data set and reduce model predictive ability (White et al., 2016). Despite that sloppiness and practical identifiability are not rigorously the same problem, the former often induces the latter by making some subset of parameters unidentifiable. Thus, there is a need to improve estimation methods which use the existing data without resorting to new experiments.====Another issue in ODE parameter estimation comes from the fact that the selected model can suffer from model misspecification issues. By resuming the terminology of Kennedy and Hagan (2001), we refer to model misspecification when the ODE model suffers from 1/ Model inadequacy: discrepancy between the mean model response and real world process. ODEs are derived by approximations, simplification of interactions and omission of external factors influence can cause such discrepancy. 2/ Residual variability issues: many biological processes are known to be stochastic and the justification of deterministic modeling comes from the approximation of stochastic processes by ODE solutions see Kurtz, 1970, Kurtz, 1978, Gillespie, 2000 and Kampen (1992). Hence, inference of the parameters has to be done while recognizing that the model is false (Kirk et al., 2016, Brynjarsdottir and O’Hagan, 2014).====In this work, we propose a new estimation procedure to address these challenges, based on an approximate solution of the original ODE. The use of approximate solutions for statistical inference, such as the two-step approaches (Varah, 1982, Gugushvili and Klaassen, 2011, Liang et al., 2010, Brunel and D’Alche-Buc, 2014, Dattner, 2015), Generalized Profiling (GP) (Hooker et al., 2011, Ramsay et al., 2007) or in a Bayesian framework (Chkrebtii et al., 2016, Jaeger and Lambert, 2011), has already proven to be useful for regularizing the inverse problem of parameter estimation. In presence of poorly identifiable parameters, the appeal of such methods is their ability to bypass the Cramer–Rao bound which imposes to exact methods a dramatic increase of estimator variance. In case of model misspecification, they can improve estimation accuracy for they relax the constraint imposed by the ODE model and then account for model discrepancy in the criteria to optimize (Brynjarsdottir and O’Hagan, 2014).====Our proposed method presents similarities with the ones introduced in Brunel and Clairon, 2015, Clairon and Brunel, 2019 and Clairon and Brunel (2018), where an approximation ==== is a solution of the perturbed ODE ==== where the perturbation ==== captures different sources of model misspecification. After a pre-smoothing step to obtain a nonparametric curve estimator ====, the estimator ==== is defined as the minimizer of the cost ==== profiled on the possible perturbations ====: ====, where ====. This estimator, called the Tracking Estimator (TE), is thus defined as the parameter which needs the smallest perturbation ==== in order to track ====, the balance between the two contrary objectives of data fidelity (i.e. ====) and original model fidelity (i.e. ====) is done through the choice of an hyperparameter ====. For each value ====, the optimal control problem ==== is solved by using the Pontryagin maximum principle (Pontryagin et al., 1962). In comparison with GP and NLS, the TE generally has a lower variance and mean square error with the difference in performance even more marked in the presence of model misspecification. In the parametric case and for well-specified models, the TE is consistent with a ====-convergence rate under very mild model regularity conditions and provided ====, with ==== a positive model dependent bound. Another attractive feature of the tracking framework is the seamless estimation of finite-dimensional and time-varying parameters. The estimation of ==== is turned into an optimal control problem and estimator ==== is a by-product of ==== estimation which does not require the use of standard approximations such as sieves or basis expansions (Xue et al., 2010, Hooker et al., 2011, Wang et al., 2014). However, there are two main limitations for the method presented in Clairon and Brunel (2018). First, the computational time: solving the optimal control problem by using the Pontryagin maximum principle leads to a boundary value problem (BVP) for each new ==== value and ==== has to be estimated as nuisance parameter. Second, the method requires a nonparametric estimator which can be biased in sparse data case, this bias can then be spread to the parametric estimation. Here, while we still consider an optimal control based approach, we change the cost function ==== as well as the numerical procedure used to solve the related optimal control problem. We rely on discrete control theory and a numerical method inspired by Cimen and Banks (2004b). This allows us to construct a method which:====In order to define our estimators, we present in the next section the optimal control problem required to introduce our functional criteria and describe our approach for semi-parametric estimation. In Section 3, we derive the numerical procedures. In Section 4, we study the asymptotic behavior of our estimators. In Section 5, we use Monte Carlo experiments to compare the Tracking, NLS and GP estimators on ODE examples from chemistry and biology with both well-specified and misspecified models. The discrete control theory based method allows us to obtain more accurate estimates than GP and NLS. We also investigate differences between the method developed here and the one in Clairon and Brunel (2018). We emphasize the advantage of using this discrete based one in terms of computational time and estimation accuracy, in particular for sparse sample cases. In Section 6, we consider parameter estimation with real data in a model used to study microbiotal population evolution.",A regularization method for the parameter estimation problem in ordinary differential equations via discrete optimal control theory,https://www.sciencedirect.com/science/article/pii/S0378375820300458,5 May 2020,2020,Research Article,181.0
Mies Fabian,"RWTH Aachen University, Institute of Statistics, Wüllnerstraße 3, D-52062 Aachen, Germany","Received 27 June 2019, Revised 25 November 2019, Accepted 25 April 2020, Available online 4 May 2020, Version of Record 21 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.009,Cited by (2),"The jump behavior of an infinitely active Itô semimartingale can be conveniently characterized by a jump activity index of Blumenthal–Getoor type, typically assumed to be constant in time. We study Markovian semimartingales with a non-constant, state-dependent jump activity index and a non-vanishing continuous diffusion component. A nonparametric estimator for the functional jump activity index is proposed and shown to be asymptotically normal under combined high-frequency and long-time-span asymptotics. Furthermore, we propose a nonparametric drift estimator which is robust to symmetric jumps of infinite variance and infinite variation, and which attains the same ","As data is available at finer temporal resolution, rather detailed models of stochastic processes become statistically tractable. Extending diffusion-based continuous time models, processes with some form of jump behavior have gained attention in the literature. While classical diffusion processes are fully described by their local volatility and drift, processes with jumps admit a greater flexibility, as the conditional jump behavior is summarized by a compensating measure, which is in general an infinite-dimensional object. Thus, statistical modeling of the jumps is essentially a nonparametric problem. In this paper, we specify the form of the local jump measure semiparametrically, while considering state-dependence of these parameters in a nonparametric framework. More precisely, we focus on the behavior of the infinite activity component of a jump diffusion in the form of an index of jump activity, similar to Aït-Sahalia and Jacod (2009).====In particular, we develop estimators for a continuous-time, scalar Markov process of the type ==== Here ====, ====, ==== are measurable functions, and ==== is a Poisson counting measure on ==== with intensity ==== such that ==== almost surely, for each ====. Furthermore, we assume the jump term ==== to be symmetric in the sense that for each ====, the instantaneous compensating measure ==== given by the image measure ==== is symmetric. Throughout, we assume that the solution of (1) exists, which can be guaranteed, for example, by suitable Lipschitz conditions (see Applebaum, 2004). Hence, ==== is a semimartingale. Detailed regularity assumptions on the functions ==== and ==== will be imposed in the sequel. Regarding the jump component, we impose the following conditions.====Assumption (J2) ensures that the jump-activity index of ==== is uniformly bounded away from ====, that is ==== for all ====. Furthermore, (J2) bounds the tails of the intensity measure so that the Lévy process corresponding to the intensity ==== has finite first moments. The uniformity in ==== requires a suitable form of smoothness.====We consider the statistical setting of discrete observations ==== for an equidistant grid ==== of meshsize ====, and we study joint high-frequency and ergodic asymptotics, i.e. ==== and ==== simultaneously. Our interest lies in nonparametric estimation of the drift function ====, as well as the state-dependent behavior of the small jumps of ====. In a more general framework, in particular, without imposing symmetry and a tail bound on the jump measure, Ueltzhöfer (2013) studies the jump dynamics by estimating ==== nonparametrically for finitely many values ====. In contrast to the mentioned study, we are interested in the detailed behavior of ==== as ====. More precisely, we will consider the case ==== for ==== in an appropriate sense to be made precise, and ====. Thus, the small jumps behave locally like an ====-stable process. This property is also referred to as locally-stable in the literature (e.g. Masuda, 2019), and ==== is the (spot) jump activity index of ====. If ==== for all ==== exactly, the jump process is a stable-like process as considered by Bass, 1988a, Bass, 1988b. For a Lévy process, ==== is known as the Blumenthal–Getoor index (Blumenthal and Getoor, 1961). In Section 4, we will construct nonparametric estimators of ==== and ==== and derive their asymptotic distribution. To the best of our knowledge, the only other statistical treatment of a non-constant jump activity index ==== is due to Todorov (2017), where a pure-jump process without a Brownian component is studied. The importance of the presence of the diffusion term is discussed below, together with further related work.====From a purely statistical perspective, the jump activity ==== resp. ==== is of interest because it can be estimated in a pure high-frequency setting, keeping ==== fixed. This case has been initially studied by Aït-Sahalia and Jacod (2009), and later by Jing et al. (2012) and Bull (2016). In contrast, estimation of the drift and the full Lévy measure requires observations over an increasing time span. We will always let ====, and the results derived in this paper reflect the latter distinction as the rate of convergence of our estimator for ==== is faster than for the drift ====. Another motivation to study the jump activity index is raised by arbitrage theory in mathematical finance. Even for a Lévy process, different values of ==== and ==== induce singular probability measures on path space (Aït-Sahalia and Jacod, 2012), in accordance with the identifiability from high-frequency observations. Thus, any full specification of an equivalent pricing measure in continuous time needs to match the jump activity index of the process under the physical probability measure.====Besides the jump component, the second object of interest to us is the drift ====. For jump processes of the form (1), the drift is in general not well defined as it depends on the specific truncation function ==== used to define the compensated Poisson integral. Here, we employ the truncation function ==== to fix the value of the drift. Nevertheless, the quantity ==== is somewhat universal if we impose the jumps to be symmetric. Under condition (J1), any truncation satisfying ==== will yield the same drift value. Another option to define a canonical drift term is to suppose that the jumps are summable, i.e. ====, such that no truncation is necessary at all. The latter approach is used, e.g., by Gloter et al. (2018). A third option is to assume that the law of the jumps is fully known to the statistician, as done by Amorino and Gloter (2020a), such that the effect of changing ==== can be accounted for. In this paper, we will assume symmetry when identifying the drift. We propose a nonparametric estimator for ==== and derive its asymptotic distribution. By employing a jump-filtering technique similar to Gloter et al. (2018), we are able to derive the same asymptotic variance as in the diffusion case, while allowing for symmetric jumps of infinite variation and infinite variance as a nuisance. Our estimator is similar to the nonparametric thresholded drift estimator of Mancini and Reno (2011), who require the jumps to be of finite activity.====The estimators proposed in this paper are based on smoothly truncated increments of the form ==== for a bounded smooth function ==== and a sequence ==== as ====. Here, we use the Markov property of ==== to derive non-asymptotic approximations for the conditional moments of ====. These approximations make use of new analytical bounds on the infinitesimal Markov generator of ====. The transformed increments ==== are localized by a Nadaraya–Watson kernel estimator with bandwidth ====, only considering those increments where ====. For estimation of the jump activity, we choose a suitable function ==== as specified below and find that ==== scales like ====. This scaling is exploited to estimate ====. The exact construction of the estimator is described in Section 4, where we also derive its asymptotic normality at rate ====. The effective rate is thus determined by some required upper bounds on ====, and we can achieve at least the rate ====. Using the derived approximation of conditional expectations, we are also able to construct a pointwise estimator for the drift ==== by choosing a nonlinear odd function ==== of suitable form, and ====. This tempered drift estimator is asymptotically normal at rate ====, even in the presence of infinite variation jumps with infinite variance. The asymptotic variance is found to be affected by the jump component. A jump-filtered drift estimator can be realized by letting ==== slowly, recovering the asymptotic variance of the continuous diffusion case.",Estimation of state-dependent jump activity and drift for Markovian semimartingales,https://www.sciencedirect.com/science/article/pii/S0378375820300471,4 May 2020,2020,Research Article,182.0
"Arias-Castro Ery,Chen Shiyun,Ying Andrew","Department of Mathematics, University of California, San Diego, USA","Received 11 January 2019, Revised 6 February 2020, Accepted 25 April 2020, Available online 4 May 2020, Version of Record 19 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.008,Cited by (2),"In a multiple testing setting, we consider testing ==== null hypotheses, denoted by ====, with the corresponding p-values ==== for each ====. We propose a new method which ‘scans’ all intervals in the unit interval ","Multiple testing problems arise in a wide range of applications, and are most acute in contexts where data are large and complex, and where standard data analysis pipelines involve performing a large number of tests. Benjamini and Hochberg (1995) proposed to control the false discovery rate (FDR) as a much less conservative criterion than the family-wise error rate (FWER). They also proposed a method (referred to as the ==== henceforth) for achieving this under some conditions, such as independence of the P-values. Since then, FDR controlling methods have been proposed and in turn adopted by practitioners faced with large-scale testing problems. Although a number of variants have been proposed, most of these methods are also based on computing a threshold based on the P-values and rejecting the null hypotheses corresponding to P-values below that threshold (Genovese and Wasserman, 2004, Storey, 2002, Storey et al., 2004). See Roquain (2011) for a survey.====A threshold approach to multiple testing is natural stemming from the fact that the smaller a ====-value is, the more evidence it provides against the null hypothesis being tested. However, we argue that this is not so obvious in the context of multiple testing, particularly in harder cases where the alternatives are not easily identified and in which most of the smallest P-values come from true null hypotheses. This was already understood by Chi (2007), who studied how to modify the BH procedure in order to improve the power. He proposed a sophisticated method which applies the BH procedure at multiple locations in the unit interval, with each location playing the role of the origin, resulting in a rejection region made of possibly multiple intervals. The method has more power than the BH method.====In the present paper we propose a simpler approach based on the longest interval whose estimated FDR is below the prescribed level. Compared to Chi (2007), the method is simpler and is already shown to outperform the BH method in some settings of potential interest, such as in power-law location models. The method can be seen as a direct extension of the approach of Storey (2002). It thus presents a sort of minimal working example where looking beyond threshold methods can be beneficial.====Scanning over intervals is a common procedure for detecting areas of interest in a point process at least since the work of Naus (1965). In this context, and its extension to discrete signals, the main task has been to test for homogeneity, and some articles have tackled such situations from a multiple testing angle (Siegmund et al., 2011, Picard et al., 2018, Benjamini and Heller, 2007, Caldas de Castro and Singer, 2006, Pacifico et al., 2007, Perone Pacifico et al., 2004). While these papers aim at controlling the FDR when scanning spatiotemporal data, here we consider a standard multiple testing situation with a priori no spatiotemporal structure, and offer scanning as a way to generalize and potentially improve upon threshold procedures.",A scan procedure for multiple testing: Beyond threshold-type procedures,https://www.sciencedirect.com/science/article/pii/S037837582030046X,4 May 2020,2020,Research Article,183.0
"Xu Xiaojian,Sinha Sanjoy K.","Department of Mathematics and Statistics, Brock University, Canada,School of Mathematics and Statistics, Carleton University, Canada","Received 24 March 2019, Revised 12 April 2020, Accepted 12 April 2020, Available online 21 April 2020, Version of Record 14 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.006,Cited by (8)," for estimating the parameters in GLMMs and investigate both I-optimal and D-optimal design criteria for the construction of robust sequential designs. To study the empirical properties of these sequential designs, we ran a series of simulations using both logistic and Poisson mixed models. As indicated in the simulation results, the I-optimal design generally outperforms the D-optimal design for all scenarios considered. Both designs are more efficient than the conventionally used uniform design and the classical D-optimal design obtained under the assumption that the fitted models are correctly specified. The proposed designs are also illustrated in an example using actual data from a dose–response experiment.","In this article, we discuss the construction of model-robust designs for generalized linear mixed models (GLMMs). The majority of literature on model-robust designs are for linear models and a few are for generalized linear models (GLMs), but little work has been done on model-robust designs for GLMMs. A possible reason for the dearth of work in this area may be explained by (i) the complexity of the design problem even when the assumed model is exactly correct and (ii) the computational difficulty involved in design problems for GLMMs.====With an awareness of the possibility that the assumed generalized linear model (GLM) might not be exactly correct, Adewale and Wiens (2009) developed criteria to generate robust designs for logistic regression models with possible inaccuracy in assumed linear predictors. In addition, Adewale and Xu (2010) discussed the construction of robust designs, in general, for generalized linear models with protections against not only possible misspecification in assumed linear predictors but also inadequately assumed link function and possible overdispersion.====Robust designs for GLMs were also investigated by Woods et al. (2006), and Li and Wiens (2011). Woods et al. (2006) proposed a method for finding exact designs for experiments with generalized linear models, which uses a design criterion that is robust to uncertainty in the link function, the linear predictor, or the regression parameters. Li and Wiens (2011) developed experimental designs for dose–response studies, which are robust against possibly misspecified link functions. The proposed design minimizes the maximum mean squared error of the estimated dose required to attain a response in certain percentage of the target population.====With the notion that the assumed model is correctly specified, Sinha and Xu (2011) constructed D-optimal sequential designs for GLMMs, while Sinha and Xu (2016) further provided practical methods of design construction for GLMMs which can overcome the computational intensiveness and difficulty involved in constructing optimal designs for the GLMM. In this paper, we aim to investigate optimal and robust sequential designs for GLMMs under the consideration of possible misspecifications in the assumed parametric forms. There are several scenarios in which GLMMs can be potentially misspecified. These scenarios include GLMMs with (M1) misspecified linear predictors, (M2) imprecisions in assumed link functions, (M3) overdispersion and (M4) misspecified distributions for random effects. Any combination of departures from (M1)–(M4) may also occur. Our goal is to consider a design process which would be robust against any of the aforementioned departures from the model assumptions. From our investigation, it appears that both (M1) and (M2) have a greater impact on either predicting the mean response or estimating the fixed effects parameters in GLMMs, while (M3) and (M4) have more impacts on the estimation of variance components. A common goal of optimal designs for GLMMs is the prediction or estimation of the conditional mean response structure rather than the variability across clusters. Therefore, we will focus on robust designs for GLMMs which can address (M1) and (M2) types of model departures.====For (M1), either the covariates included in the systematic component or the linear predictor of a GLMM may not reflect the influence of covariates correctly. This situation often occurs due to the use of an incorrectly specified functional form of the covariates in the model or an omission of essential covariates from the regression model. For instance, Heagerty and Kurland (2001) investigated the impact of model violations on the estimates of regression coefficients in GLMMs. Their study indicates that there can be substantial bias in the conditionally specified regression estimators that may result from using a misspecified random intercept model, where the true random effects distribution depends on measured covariates or when there are autoregressive random effects. For (M2), the link function adopted in the assumed GLMM model is often an approximation to the truth. For example, we often use the logit link in a binary regression model, which is the canonical link for the binomial distribution, when, in fact, the complementary log–log link or the probit link may be more appropriate. In an investigation of designs for nonlinear models, Sinha and Wiens (2002, p.602) have asserted that “although the theoretical response functions are very similar in shape, and possibly indistinguishable if noisy data must be relied upon, the appropriate designs can be quite dissimilar”. Similarly, since the link function determines the response function in a GLMM, the misspecification of the link function may also have an impact on the resulting optimal designs.====Here we develop our robust methods for constructing adaptive sequential designs in GLMMs by addressing possible imprecisions in the assumed linear predictors or link functions, where the maximum likelihood method is adopted for fitting GLMMs. The rest of this paper is organized as follows. Section 2 introduces the model and notation, and reviews the maximum likelihood (ML), score function and Fisher information for fitting a GLMM and for finding the asymptotic variance–covariance matrix of the ML estimators. Section 3 discusses how the two types of departures (M1 and M2) are related and what their impacts are on the estimates of the parameters in GLMMs or on the prediction of the overall mean response function. Section 4 presents our proposed I-optimal and D-optimal designs and provides a sequential procedure for finding robust designs in GLMMs. Section 5 explores the distributions of the optimal sequential design points attained by our proposed methods using Monte Carlo simulations and compares the performance of the I- and D-optimal designs with other competitors. Section 6 illustrates our proposed method using an example of a real dataset from a dose–response experiment. Section 7 offers some concluding remarks. Derivations are provided in the Appendix A Proof of, Appendix B Additional plots from simulations:.",Robust designs for generalized linear mixed models with possible model misspecification,https://www.sciencedirect.com/science/article/pii/S0378375820300446,21 April 2020,2020,Research Article,184.0
Rosa Samuel,"Faculty of Mathematics, Physics and Informatics, Comenius University in Bratislava, Slovakia","Received 9 July 2019, Revised 13 April 2020, Accepted 13 April 2020, Available online 25 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.005,Cited by (2),"-optimality of product designs by providing product designs that are optimal with respect to general eigenvalue-based criteria. In particular, ====- and ====-optimal product designs are obtained. We then formulate a method based on linear programming for constructing optimal designs with smaller supports from the optimal product designs. The sparser designs can be more easily converted to practically applicable exact designs. The provided results and the proposed sparsification method are demonstrated in some examples.","In the experiments performed to estimate the effects of a selected set of treatments, it is common that the responses are also affected by some other experimental conditions (the covariates — e.g., time trend, block effects, the ages or the gender of the subjects in a clinical trial). The effects of the covariates are traditionally considered to be nuisance effects in the experimental design literature (e.g., Cox, 1951, Majumdar and Notz, 1983, Atkinson and Donev, 1996, Jacroux et al., 1997, Rosa and Harman, 2016). Recently, Atkinson (2015) suggested that particularly with the growing importance of personalized medicine, the covariate effects may also be of prominent interest.====Moreover, the interest often lies in some functions of the model parameters rather than in the parameters themselves. The comparison of test treatments with a control or with the placebo is a common example, especially in clinical trials. The design and analysis are sometimes further complicated by the presence of heteroscedasticity — the responses under different treatments may have different variances. In the present paper, we study optimal approximate designs in heteroscedastic models with treatment effects and covariate effects where there is interest in a set of treatment contrasts and in a set of linear combinations of the covariate effects.====For the abovementioned settings, Atkinson (2015) studied ====-optimality in a model with only two treatments. Wang and Ai (2016) extended his results by providing ====-optimal product designs for arbitrary numbers of treatments. The ====-optimality is the most popular criterion in optimal design literature and is particularly nice to work with analytically. However, if the interest lies in a set of treatment contrasts (e.g., in the test treatment-control comparisons, which are a natural choice when placebo is included in the clinical trial), ====-optimality tends to ignore the special interest in the chosen contrasts; i.e., this criterion tends to select designs that do not provide more information on the contrasts of interest compared to other treatment contrasts. As such, ====-optimality is generally not recommended for such experimental interests (cf. Hedayat et al., 1988, Morgan and Wang, 2010). This also corresponds to the observation by Wang and Ai (2016) that if the experimental objective is to estimate the test treatment-control comparisons and all covariate effects, then the ====-optimal designs are the same as if there was interest in all covariate effects and a uniform interest in all the treatments. That is, in such a case, the ====-optimality does not place any special emphasis on the test treatment-control comparisons.====In contrast, the ====-optimality criterion possesses a natural statistical interpretation for the considered settings — it minimizes the average variance for the linear functions of interest. Hence, ====-optimality is very popular for the comparisons with the control (e.g., see Hedayat et al., 1988). Recently, ====-optimality was also argued to be meaningful for estimating treatment contrasts (e.g., Morgan and Wang, 2011, Rosa, 2019). In this paper, we therefore extend the results by Wang and Ai (2016) to other eigenvalue-based optimality criteria (see Section 2). In particular, we obtain ====-optimal product designs for the other Kiefer’s ====-optimality criteria besides ====-optimality, including ====- and ====-optimality.====The observation that the product designs are generally optimal for multi-factor models (like the treatment-covariate one) is extensively used in the literature; e.g., see Schwabe and Wierich (1995), Schwabe (1996), Rodriguez and Ortiz (2005) and Graßhoff et al. (2007). One drawback of the optimal product designs is that they have large supports, and therefore it is sometimes difficult to construct designs for the actual experiments from the product designs by rounding procedures==== ==== (e.g., the well-known efficient rounding procedure by Pukelsheim and Rieder (1992)). However, in Section 3, we provide an entire class of ====-optimal designs characterized by linear constraints. This allows us to formulate a linear programming method for constructing optimal designs with smaller supports from the optimal product designs. Similar approach was employed by Rosa and Harman (2016) in a homoscedastic model, where the covariate effects were considered to be nuisance parameters.====The application of the theoretical results, and in particular the construction of the optimal designs with sparser supports and their usefulness in obtaining efficient exact designs of experiments are demonstrated in some examples in Section 4.",Optimal experimental designs for treatment contrasts in heteroscedastic models with covariates,https://www.sciencedirect.com/science/article/pii/S0378375820300434,25 April 2020,2020,Research Article,191.0
"Hao Shuai,Yang Min","Abbvie Inc. North Chicago, IL, 60064, United States of America,Department of Mathematics, Statistics, and Computer Science, University of Illinois at Chicago Chicago, IL 60607, United States of America","Received 16 April 2019, Revised 18 March 2020, Accepted 22 March 2020, Available online 25 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.006,Cited by (2),While multinomial ,None,Support point of locally optimal designs for multinomial logistic regression models,https://www.sciencedirect.com/science/article/pii/S0378375820300355,25 April 2020,2020,Research Article,192.0
"Cheng Qianshun,Wang HaiYing,Yang Min","Monsanto(Bayer) Company, United States of America,University of Connecticut, United States of America,Department of Mathematics, Statistics, and Computer Science University of Illinois at Chicago, Chicago, IL, 60607, United States of America","Received 16 October 2019, Revised 10 March 2020, Accepted 22 March 2020, Available online 20 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.004,Cited by (16),Technological advances have enabled an ,"Technological advances have enabled an exponential growth in data collection and the size of data sets. For example, the cross-continental Square Kilometer Array, the next generation of astronomical telescopes, will generate 700 TB of data per second (Mattmann et al., 2014). While the extraordinary sizes of data sets provide researchers golden opportunities for scientific discoveries, they also bring tremendous challenges when attempting to analyze these large data sets. Proven statistical methods are no longer applicable due to computational limitations. Recent advances in statistical analysis to deal with these challenges are arguably on two major different strategies: the divide-and-conquer approach and the subdata selection approach.====The divide-and-conquer approach takes advantage of the parallel computing technology. A large data set is split into chunks of reasonable sizes, and analysis is implemented separately on each chunk of data and a specified aggregation method is implemented to merge pieces of information from chunks to produce final analysis. The analysis and aggregation methods depend on the structure of the data set and model assumptions. For the linear regression model, the least squares estimate can be directly decomposed into a weighted average of the least squares estimate based on each chunk. This has become the standard aggregation method for merging solutions from blocks with linear models. For nonlinear models, several aggregation methods are proposed. Lin and Xi (2011) proposed an approach for approximating the estimating equation estimator using a first order Taylor expansion. Under certain conditions, accuracy of the final estimator from aggregation is proved to be close to the direct estimator from the full data. Chen and Xie (2014) considered a divide-and-conquer approach for generalized linear models (GLM) where both the number of observations ==== and the number of covariates ==== are large. They incorporated variable selection via penalized regression into the subset processing step, and showed that, under certain regularity conditions, the aggregated estimator in model selection is consistent and asymptotically equivalent to the penalized estimator based on the full dataset. In Schifano et al. (2016), an approach similar to the divide-and-conquer approach is proposed, where accumulated parameter estimators based on data chunks arrived can be updated using future coming data. The divide-and-conquer approach gains efficiency mainly from the implementation of parallel computing, and it may not reduce computational time if implemented with a single core. The subdata approach reduces the computation burden by downsizing the data volume. The key question here is how to select an informative subdata such that it maintains as much information as possible. As noted in a recent NSF program guideline, “Tradeoffs between computational costs and statistical efficiency” is one of six research directions that needs to be addressed for theoretical foundation of data science (NSF, 2016).====Existing subdata approaches are mainly based on random subsampling. Combining the methods of subsampling (Politis et al., 1999) and bootstrapping Efron (1979), Bickel et al. (1997) and Kleiner et al. (2014) proposed a novel approach called bags of little bootstraps (BLB) to achieve computational efficiency. Liang et al. (2013) proposed a mean log-likelihood approach using Monte Carlo averages of estimates from subsamples to approximate the quantities needed in the analysis. The BLB and mean log-likelihood methods select subsamples using simple random sampling. Another line of the subsampling method is based on leverage sampling algorithms. In this approach, a sampling probability is assigned to each dataline according to its leverage score. Ma et al. (2015) reviewed existing subsampling methods in the context of linear regression and termed the methods leveraging algorithms, considered the statistical properties of leveraging algorithms, and proposed a shrinkage algorithmic leveraging method.====A major limitation of random subsampling methods is that the amount of information in a resulting subdata is proportional to the size of the subdata, which is often significantly smaller than the full data size. Wang et al. (2019) proved that, in linear regression, the variance of an estimator based on the random subsampling method converges to zero at a rate proportional to the inverse of the subdata size. Is it possible that the information contained in a subdata is related to the size of the full data rather than that of the subdata only? Ideally we would want to choose the subdata with the maximum amount of information among all possible subdata sets. However this is infeasible in practice since there are ==== subsets of data with size ==== from a full data set of size ====. This combination number is quickly out of reach even for moderate ==== and ====, so an alternative approach has to be employed. Under linear models, Wang et al. (2019) proposed a novel approach called Information-Based Optimal Subdata Selection (IBOSS) to select a subdata. Unlike random subsampling methods, IBOSS is a deterministic approach. It selects a subdata based on the characterization of the ====-optimal design. Under certain conditions, Wang et al. (2019) showed that the variance of the resultant estimator converges to zero at a rate corresponding to the size of the full data. The simulation studies demonstrated that the IBOSS approach significantly outperformed random subsampling approaches.====While the IBOSS approach effectively addresses the trade off between the computational complexity and statistical efficiency, it is under the linear model context. Does this strategy also work under nonlinear models? Unlike linear models, where the corresponding information matrices are relatively simple with an explicit form, the problem for nonlinear models is remarkably different, where the information matrices are much more complicated and depend on unknown parameters. Consequently, the problem under nonlinear models is considerably harder than that under linear models.====Nonlinear models, however, are widely applied in practice. Specifically, logistic regression models have played important roles in categorical data analysis. They have been used in various fields, like finance, medicine, and social sciences. Unlike linear models, where the estimators have closed form solutions, the estimators for logistic regression models have no closed form solutions in general. We have to utilize iterative approaches to calculate the estimates numerically. Compared with linear regression models, the computation cost for logistic regression models is much higher for big data sets. There is limited research on how to choose a subdata from a full data set for a logistic regression model, perhaps due to the complexity of the nonlinearity feature. Wang et al. (2018) proposed the optimal subsampling method under the A-optimality criterion (OSMAC) algorithm, where the probability weights are specified according to the A-optimality in optimal design theory (Kiefer, 1959). However, like many other random subsampling approaches for linear models, we shall show in the next section that the information extracted from the OSMAC approach is limited by the subsample size.====In this paper, we study subdata selection under logistic regression models utilizing the IBOSS strategy. A new algorithm of selecting subdata is proposed. Compared with existing subsampling approaches, the new algorithm has the following two advantages: the estimation efficiency of the algorithm is significantly higher and the computational cost is competitive.====The key contribution of this paper is that, under logistic regression models, it (i) proves that the information from random subsampling based subdata selection method is limited by the size of the subdata, (ii) proposes a new approach for the trade off between the computational complexity and statistical efficiency, and (iii) proves that the information from the new algorithm increases along with the size of full data. These results give a theoretical justification for the information based subdata selection under nonlinear models. Since “data reduction is perhaps the most critical component in retrieving information in big data” (Yildirim et al., 2014), this is a significant step in big data analysis under nonlinear models.====The rest of the paper is organized as follows: Section 2 introduces notations, provides a summary of existing methods, and presents lower-bounds of the variance covariance matrices for subsampling-based estimators. Section 3 introduces a new algorithm and discusses its asymptotic properties. Section 4 compares the performance of the new algorithm, the OSMAC algorithm, and the simple random sampling method using various simulation settings. Section 5 provides a brief summary of this paper and its possible extensions. All technical details are provided in the supplemental material.",Information-based optimal subdata selection for big data logistic regression,https://www.sciencedirect.com/science/article/pii/S0378375820300331,20 April 2020,2020,Research Article,193.0
"Graßhoff Ulrike,Röttger Frank,Schwabe Rainer","Wirtschaftswissenschaftliche Fakultät, Humboldt-Universität zu Berlin, Unter den Linden 6, 10099 Berlin, Germany,Westfälische Wilhelms-Universität Münster, Psychologisches Institut IV, Fliednerstr. 21, 48149 Münster, Germany,Fakultät für Mathematik, Otto-von-Guericke Universität Magdeburg, Universitätsplatz 2, 39106 Magdeburg, Germany,MPI MiS Leipzig, Inselstraße 22, 04103 Leipzig, Germany","Received 14 November 2019, Revised 1 April 2020, Accepted 9 April 2020, Available online 20 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.004,Cited by (1),This paper studies optimal designs for linear regression models with correlated effects for single responses. We introduce the concept of rhombic design to reduce the computational complexity and find a semi-algebraic description for the ,"Hierarchical regression models with random coefficients enjoy growing importance in biological and psychological applications, whenever there is a variation with respect to the observed subjects. Hereby we often cannot expect the random coefficients to be uncorrelated, which means that we assume that the random coefficients are e.g. normally distributed with a population mean and a non-diagonal covariance matrix. A special model that will be the topic of this paper are random effects models for linear regression with singular responses, which means that we obtain only one observation per unit. This particular model was motivated by Freund and Holling (2008) and Patan and Bogacka (2007). A natural question that arises is to find optimal experimental designs for these models with respect to some optimality criterion. Graßhoff et al. determined ====-optimal designs that maximize the determinant of the corresponding information matrix, for a couple of different covariance structures in Graßhoff et al. (2009) and Graßhoff et al. (2012). They found that in contrast to fixed effects models for multiple linear regression optimal settings may, surprisingly, occur in the interior of the design region under certain conditions on the covariance structure of the random coefficients. In the present paper, we investigate conditions on the covariance structure to discriminate situations in which optimal designs are completely supported on the boundary of the design region as in fixed effects models and situations in which optimal designs may have additional support points in the interior. This is done for the special class of ==== designs, which are invariant with respect to permutations of the regressors and simultaneous sign change and which we will introduce in Section 3. Section 4 shows via the Kiefer–Wolfowitz equivalence theorem (Silvey, 1980 Theorem 3.7) how the parameter regions for which rhombic designs with or without interior points are D-optimal are described by semi-algebraic sets, which are sets defined by polynomial inequalities and equations and how the optimality depends on the covariance structure. Furthermore, we show that for the assumed covariance structure of the random coefficients, the ====-optimality of designs with interior support points translates to a simple matrix equation for the information. We show as a consequence of the results in Section 4 that the distinction, whether a ====-optimal rhombic design requires interior support points or not, can be made by evaluating a polynomial only dependent on the covariance matrix of the random coefficients. Based on these results, we are able to compute optimal designs and their optimality regions explicitly for small to moderate dimensions in Section 5 and we conjecture results for arbitrary dimensions in Section 6.",Optimality regions for designs in multiple linear regression models with correlated random coefficients,https://www.sciencedirect.com/science/article/pii/S0378375820300422,20 April 2020,2020,Research Article,194.0
"Liu Qingyang,Zhang Yuping","Department of Statistics, University of Connecticut, Storrs, CT, USA","Received 15 April 2019, Revised 4 April 2020, Accepted 5 April 2020, Available online 17 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.003,Cited by (3), framework. We also establish structure recovery consistency for the proposed joint network learning. The practical merits of the proposed integrative structural learning method are demonstrated through simulations and real applications to discovering regulatory relationships among heterogeneous biological variables from distinct but related types of cancer.,"In genomic medicine, exploring dependency structures between biological variables is critical for finding intrinsic causal relationships, so as to develop highly targeted medical therapies and medicines (Ma et al., 2012, Tseng et al., 2015, Lee et al., 2016). Undirected graphical models, or Markov Random Fields, are useful in discovering conditional dependency for high-dimensional data. Widely used examples include Gaussian graphical models for symmetric and thin-tailed continuous data (Li et al., 2012, Ren et al., 2015, Cai et al., 2016, Du and Ghosal, 2019), as well as Ising models for binary data (Ravikumar et al., 2010). Yang et al. (2012) introduced a more general subclass of Markov Random Fields by assuming node-wise distributions arising from univariate exponential families. Through solving node-wise, regularized generalized linear regressions (Meinshausen and Bühlmann, 2006), it succeeded to model more various types of variables, such as count data (Poisson graphical models) and skewed continuous data (exponential graphical models).====Our research is motivated by establishing regulatory relationships among heterogeneous multi-omic variables from distinct but related biological conditions. In modern high-throughput biology, data can be mixtures of gene expressions, point mutations, copy number variations, and epigenetic states, including binary, categorical, count and continuous variables. These biological conditions can be different tissue types or different cancer types, etc., which have potentially shared regulation mechanisms. Thus, taking advantages of similarity among biological conditions, joint modeling leads to more accurate structure estimation. Additionally, it is also valuable to detect significant differences among groups in order to discover disease-specific correlations. To jointly learn multiple distinct but similar networks, there exist past literature handling multiple Gaussian networks, including Ma and Michailidis (2016) assuming prior grouping structure, as well as Danaher et al. (2014) and Mohan et al. (2014), which took similarity in edges or nodes among multiple networks into consideration. Zhang et al. (2017) developed data integration methods on mixtures of categorical and Gaussian variables using group lasso or fused lasso regularization added on pseudo-likelihood.====In this paper, we extend the joint modeling of undirected graphical models to heterogeneous variables from multi-parameter exponential families. Our joint structural learning problem is established on an approximate likelihood approach, which avoids the asymmetry problem of nodewise learning. We organize the paper as follows. In Section 2, we first elaborate the statistical knowledge about pairwise exponential Markov Random Field. Then, we introduce the extended data-integration framework for multiple networks through multivariate exponential family distributions. In Section 3, we discuss relevant computational techniques and propose our algorithm. In Section 4, we investigate the consistency in edge recovery using the approximate estimation and provide sparsistency results. We first illustrate them through a single network scenario for simplicity. We then establish structure recovery consistency for the proposed joint network learning framework. In Section 5, we introduce the methods for tuning parameter selections. We then systematically investigate the effects of tuning-parameter selection methods on the proposed statistical framework for data integration in Section 6 through Erdős–Rényi graphs and scale-free networks. In Section 7, we further demonstrate the practical merits of the proposed method through a real case study, which establishes networks of interactions between copy number variations (CNV) and mutations for two types of cancers. We finally conclude our paper in Section 8.",Joint estimation of heterogeneous exponential Markov Random Fields through an approximate likelihood inference,https://www.sciencedirect.com/science/article/pii/S0378375820300410,17 April 2020,2020,Research Article,195.0
"Patilea Valentin,Sánchez-Sellero César","Univ Rennes, Ensai, CNRS, CREST-UMR 9194, F-35000 Rennes, France,Departamento de Estadística, Análisis Matemático y Optimización, Universidade de Santiago de Compostela, Spain","Received 6 February 2019, Revised 5 November 2019, Accepted 6 April 2020, Available online 17 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.002,Cited by (5),A lack-of-fit test for functional regression models is proposed. The test is based on the fact that checking the no-effect of a functional ,"Recently, there has been a large amount of work on functional data analysis. The monographs of Ramsay and Silverman (2005) and Horváth and Kokoszka (2012) provide a comprehensive landscape of the importance of statistical methods for functional data. See also Wang et al. (2016) for a recent review. Estimation and prediction with functional covariates has received substantial attention: see for example Ferraty and Vieu (2006), Cai and Hall (2006), Hall and Horowitz (2007), Crambes et al. (2009), Yao and Müller (2010) and the references therein. The lack-of-fit problem remains less explored, especially general regression model checks against general alternatives.====To illustrate the problem we address in this paper, let ==== be the space of the square-integrable real-valued functions defined on the unit interval. For any ====, let ==== Consider a functional linear model ====, where ==== and ==== are random, while ==== and ==== are the unknown parameters of the model. Testing for lack-of-fit of this regression model against general alternatives means to test the null hypothesis ====against the nonparametric alternative ====.====There is a large literature on regression model checks against nonparametric alternatives when ==== is a finite-dimension vector, see for instance Härdle and Mammen (1993), Stute (1997), Hart (1997), Horowitz and Spokoiny (2001), Guerre and Lavergne (2005). See González-Manteiga and Crujeiras (2013) for a recent review.====An increasing interest for extending the regression model check approaches to functional data is evidenced lately. Delsol et al. (2011) extended the idea of Härdle and Mammen (1993) to a functional data framework. However, their results seem of little practical use since they are derived under strong assumptions, for instance on the so-called small ball probabilities. Bücher et al. (2011) proposed a test based on a stochastic process estimating ====distances between the response and the regression function, but restricted their investigation to the functional linear model. Some more work was done for testing for lack-of-fit in a functional linear model, see Cardot et al., 2003, Cardot et al., 2004, Hilgert et al. (2013), or for testing the functional linear model against quadratic alternatives, see Horvàth and Reeder (2013). Though these approaches could be quite effective in some cases, by construction, they are not designed to detect general departures from the null hypothesis (1.1).====The test we introduce herein is based on the following dimension reduction idea: condition (1.1) is equivalent to the nullity of the conditional expectation of ==== given a sufficiently rich set of projections of ==== on elements of norm 1 from finite-dimension subspaces of ====. This remark was used by Lavergne and Patilea (2008) in a finite dimension framework. Patilea et al. (2016) used a similar dimension reduction idea. Next, the idea is to search in finite-dimension subspaces of ==== for a least favorable element of norm 1 and to check the nullity of the conditional expectation of ==== given the scalar product between ==== and the selected least favorable direction. Our test is able to detect nonparametric alternatives, including polynomial ones. The conditional variance of ==== given ==== need not be constant and the expression of this conditional variance need not be known. We do not require the law of the covariate ==== to be given or to be of a certain type, as for instance Gaussian. A related approach was considered recently by Cuesta-Albertos et al. (2019). Extending the approach of Stute (1997), they propose to draw randomly an element of ====, to project ==== in the direction of this element and to check the nullity of the conditional expectation of ==== given this random projection using the functionals of a marked empirical process. With finite samples, the random draw is also realized in finite-dimension subspaces of ====.====The paper is organized as follows. In Section 2 we derive the fundamental lemma for the dimension reduction idea. In Section 3 the new test is introduced in a simplified setting where ==== is observed and the no-effect of ==== on ==== is being tested. Our statistic is a quadratic form, based on ==== kernel smoothing, that behaves asymptotically like a ==== random variable under ====. We prove that, under mild assumptions, the induced test is consistent against ==== type of fixed alternatives and against sequences of directional alternatives approaching the null hypothesis at a suitable rate. Moreover, a wild bootstrap procedure is proposed as a mean to approximate the critical values with finite samples and its asymptotic validity is proved. In Section 4 we apply our projection-based approach for nonparametric checks of the functional regression models. For the sake of clarity, we focus on the functional linear model. We still obtain standard normal critical values and consistency against nonparametric alternatives, fixed or approaching the null hypothesis. In Section 5 an extensive empirical study with simulated data is reported. Finally, the new test is applied to check the lack-of-fit of the functional linear model and the functional quadratic model for the Tecator data set. Some concluding remarks are gathered in Section 6. The assumptions and the main proofs are relegated to Appendix A. The remaining proofs, completed by additional technical proofs and details, and additional simulation results are provided in a Supplementary Material.",Testing for lack-of-fit in functional regression models against general alternatives,https://www.sciencedirect.com/science/article/pii/S0378375820300409,17 April 2020,2020,Research Article,196.0
"Wang Zhe,Martin Ryan","North Carolina State University, Department of Statistics, United States","Received 19 June 2019, Revised 10 February 2020, Accepted 29 March 2020, Available online 9 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.008,Cited by (17),"The area under the receiver operating characteristic curve (AUC) serves as a summary of a binary classifier’s performance. For inference on the AUC, a common modeling assumption is ","First proposed during World War II to assess the performance of radar receiver operators (Calì and Longobardi, 2015), the receiver operating characteristic (ROC) curve is now an essential tool for analyzing the performance of binary classifiers in areas such as signal detection (Green and Swets, 1966), psychology examination (Swets, 1973, Swets, 1986), radiology (Lusted, 1960, Hanley and McNeil, 1982), medical diagnosis (Swets and Pickett, 1982, Hanley, 1989), and data mining (Spackman, 1989, Fawcett, 2006). One informative summary of the ROC curve is the corresponding area under the curve (AUC). This measure provides an overall assessment of classifier’s performance, independent of the choice of threshold, and is, therefore, the preferred method for evaluating classification algorithms (Provost and Fawcett, 1997, Provost et al., 1998, Bradley, 1997, Huang and Ling, 2005). The AUC is an unknown quantity, and our goal is to use the information contained in the data to make inference about the AUC. The specific set up is as follows. For a binary classifier which produces a random ==== to indicate the propensity for, say, Group 1; individuals with scores higher than a threshold are classified to Group 1, the rest are classified to Group 0. Let ==== and ==== be independent scores corresponding to Group 1 and Group 0, respectively. Given a threshold ====, define the ==== and ==== as ==== and ====. Then the ROC curve is a plot of the parametric curve ==== as ==== takes all possible values for scores. While the ROC curve summarizes the classifier’s tradeoff between sensitivity and specificity as the threshold varies, the AUC measures the probability of correctly assigning scores for two individuals from two groups, which equals ==== (Bamber, 1975), and is independent of the choice of threshold. Consequently, the AUC is a functional of the joint distribution of ====, denoted by ====, so the ROC curve is actually not needed to identify AUC.====In the context of inference on the AUC, when the scores are continuous, it is common to assume that ==== satisfies a so-called ==== assumption, which states that there exists a monotone increasing transformation that maps both ==== and ==== to normal random variables (Hanley, 1988). For most medical diagnostic tests, where the classifiers are simple and ready-to-use without training, such an assumption serves well (Hanley, 1988, Metz et al., 1998, Cai and Moskowitz, 2004), although it has been argued that other distributions can be more appropriate for some specific tests (e.g., Guignard and Salehi, 1983, Goddard and Hinberg, 1990). But for complicated classifiers which involve multiple predictors, as often arise in machine learning applications, binormality – or any other model assumption for that matter – becomes a burden. This motivates our pursuit of a “model-free” approach to inference about the AUC.====Specifically, our goal is the construction of a type of posterior distribution for the AUC. The most familiar such construction is via Bayes’s formula, but this requires a likelihood function and, hence, a statistical model. The only way one can be effectively “model-free” within a Bayesian framework is to make the model extra flexible, which requires lots of parameters. In the extreme case, a so-called Bayesian nonparametric approach would take the distribution ==== itself as the model parameter (e.g., Ghosal and van der Vaart, 2017, Gu et al., 2008). When the model includes lots of parameters, then the analyst has the burden of specifying prior distributions for these, based on little or no genuine prior information, and also computation of a high-dimensional posterior. But since the AUC is just a one-dimensional feature of this complicated set of parameters, there is no obvious return on the investment into prior specification and posterior computation. A better approach would be to construct the posterior distribution for the AUC directly, using available prior information about the AUC only, without specifying a model and without the introduction of artificial model parameters. That way, the data analyst can avoid the burdens of prior specification and posterior computation, bias due to model misspecification, and issues that can arise as a result of non-linear marginalization (e.g., Martin, 2019, Fraser, 2011).====As an alternative to the traditional Bayesian approach, we consider here the construction of a so-called ==== for the AUC. In general, the Gibbs posterior construction proceeds by defining the quantity of interest as the minimizer of a suitable risk function, treating an empirical version of that loss function like a negative log-likelihood, and then combining with a prior distribution like in Bayes’s formula. General discussion of Gibbs posteriors can be found in Zhang, 2006a, Zhang, 2006b, Bissiri et al., 2016 and Alquier et al. (2016), and some statistical applications are discussed in Jiang and Tanner (2008) and Syring and Martin, 2017, Syring and Martin, 2019a, Syring and Martin, 2019b. Again, the advantage is that Gibbs posteriors avoid model misspecification bias and the need to deal with nuisance parameters. Moreover, under suitable conditions, Gibbs posteriors can be shown to have desirable asymptotic concentration properties (e.g., Syring and Martin, 2020, Bhattacharya and Martin, 2020, Chernozhukov and Hong, 2003), with theory that parallels that of Bayesian posteriors under model misspecification (e.g., Kleijn and van der Vaart, 2006, Kleijn and van der Vaart, 2012).====A subtle point is that, while the risk minimization problem that defines the quantity of interest is independent of the scale of the loss function, the Gibbs posterior is not. This scale factor is often referred to as the ==== (e.g., Grünwald, 2012) and, because it controls the spread of the Gibbs posterior, its specification needs to be handled carefully. There are various approaches to the specification of the learning rate parameter (e.g., Grünwald, 2012, Grünwald and Van Ommen, 2017, Bissiri et al., 2016, Holmes and Walker, 2017, Lyddon et al., 2019). Here we adopt the approach in Syring and Martin (2019a) that aims to set the learning rate so that, in addition to its robustness to model misspecification and asymptotic concentration properties, the Gibbs posterior credible sets have the nominal frequentist coverage probability. When the sample size is large, we recommend an (asymptotically) equivalent calibration method that is simpler to compute.====The present paper is organized as follows. In Section 2.1, we review some methods for making inference on the AUC based on the binormality assumption, in particular, the Bayesian approach in Gu and Ghosal (2009) that involves a suitable rank-based likelihood. In Section 2.2, we argue that the binormality assumption is generally inappropriate in machine learning applications, and provide one illustrative example involving a support vector machine. This difficulty with model specification leads us to the Gibbs posterior, a model-free alternative to a Bayesian posterior, which is reviewed in Section 2.3. We develop the Gibbs posterior for inference on the AUC, derive its asymptotic concentration properties, and investigate how to properly scale the risk function in Section 3. Simulation experiments are carried out in Section 4, where a Gibbs posterior estimator performs favorably compared with the Bayesian approach based on a rank-based likelihood and another two Bayesian nonparametric methods. We also apply the Gibbs posterior on a real dataset for evaluating the performance of a biomarker for pancreatic cancer and compare our result with those based on some existing Bayesian methods. Finally, we give some concluding remarks in Section 5.",Model-free posterior inference on the area under the receiver operating characteristic curve,https://www.sciencedirect.com/science/article/pii/S0378375820300379,9 April 2020,2020,Research Article,197.0
"Britos Grisel M.,Ojeda Silvia M.,Rodríguez Astrain Laura A.,Bustos Oscar H.","Instituto Gulich (CONICET), Universidad Nacional de Córdoba-CONAE, Centro Espacial Teófilo Tabanera, Ruta 45 km 8, Falda del Cañete, Córdoba, Argentina,FaMAF-CIEM (CONICET), Universidad Nacional de Córdoba, Medina Allende s/n, Ciudad Universitaria, 5000 Córdoba, Argentina,FaMAF, Universidad Nacional de Córdoba, Medina Allende s/n, Ciudad Universitaria, 5000 Córdoba, Argentina","Received 23 October 2019, Revised 21 March 2020, Accepted 1 April 2020, Available online 8 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.04.001,Cited by (1),"In this work, we present the BMM 2D estimator, a ","The approach of generalizing one-dimensional proposals to the case of two or more dimensions is the strategy that is naturally used in many areas of science. Particularly, in the area of statistical image processing, the focus is on the two-dimensional generalization of proposals made for time series. This is because, in image processing, the observations are obtained on a rectangular two-dimensional lattice or grid (Whittle, 1954, Tjøstheim, 1978, Basu and Reinsel, 1993, Bustos et al., 2009b). Two-dimensional autoregressive models (AR-2D) were introduced by Whittle (1954) as a class of models capable of capturing spatial correlation in the data collected in images. These models have proven to be of great importance in several areas that benefit from image processing (Smith et al., 1986, Dormann et al., 2007, Sain and Cressie, 2007, Basu and Reinsel, 1993, Bustos et al., 2009b) since they make it possible to represent the intensity of an image through a small number of parameters and naturally extend the definition of autoregressive models for time series to ====. It is usual to assume that the image intensity array has a multivariate Gaussian distribution. The Gaussian assumption is used in estimating parameters of the model fitted to the image. When this assumption is true for the unilateral AR-2D models, the least-squares estimator (LS) is the most appropriate estimation method of the parameters and coincides with the maximum likelihood estimator. However, when the image is contaminated, that is, the Gaussian distribution condition for intensity is not satisfied, these estimation methods fail, providing misguided estimates. Because of this excessive sensitivity of least-squares estimators, robust estimators are needed in image models (Kashyap and Eom, 1988).====Consequently, several of the robust tools developed to estimate the parameters of the one-dimensional autoregressive model have been implemented for AR-2D models under contaminated spatial data (see Kashyap and Eom, 1988, Allende et al., 1998, Ojeda et al., 2002). In the area of image processing and computer vision, the use and development of techniques called “robust” are frequent, not referring to the term robustness imperiously from a formal statistical perspective. Within this framework, the classic tools do not consider the structure or topology inherent to the images and, in most situations, the models require strong hypotheses about the laws that govern the observed process (Alata and Olivier, 2003, Bustos et al., 2009a, Dormann et al., 2007, Latha et al., 2014, Ojeda et al., 2010, Quintana et al., 2011, Sahu et al., 2015, Sain and Cressie, 2007, Vallejos and Mardesic, 2004) and (Zielinski et al., 2010).====From a formal perspective, in the context of robust estimators of the AR-2D model parameters with a finite and arbitrary number of parameters, at least three estimators have been defined and studied: the M, GM and, RA estimators.====In 1988, Kashyap and Eom (1988) presented the M estimators for the AR-2D models. Then, for the same models, Allende et al. (1998) implemented an extension of the M estimators: the Generalized M estimators (GM). They extend the GM estimators defined for unidimensional AR models used to model process in time series. While the performance of these two estimators is acceptable for contaminated data with innovative contamination, there are no known rigorous studies on its asymptotic properties. Similarly, robust Residual Autocovariance (RA) estimators were introduced by Ojeda et al. (2002) for two-dimensional autoregressive models extending the definition for time series of the estimator with the same name (Bustos and Yohai, 1986). The performance of this estimator is better than the M estimator and slightly higher than the GM estimator under innovative and additive contamination. In addition, the RA estimator outperforms its competitors M and GM because its asymptotic properties are known. Indeed, this estimator is strongly consistent and asymptotically normal with known variance–covariance matrix. In contrast, the main disadvantage of the RA estimator compared to the M and GM estimators is its high computational cost, which makes it an ineligible tool in practical applications. Finally, it should be noted that in Britos and Ojeda (2019) an estimator, called BMM 2D, was defined to estimate the parameters of the two-dimensional autoregressive model with three parameters, partially extending the definition given in Muler et al. (2009) for time series. This estimator proved to be a successful tool for estimating the three parameters of the model when the spatial data are contaminated by different contamination schemes, showing good performance (in precision, as well as accuracy and computational time) compared to the estimators mentioned above.====In this paper we present the BMM 2D estimator for the parameters of the unilateral autoregressive spatial processes with ==== parameters, generalizing the definition established in Britos and Ojeda (2019). This estimator preliminarily estimates the M-scale of the innovation process and then makes an M-estimation of the parameters, relying on an auxiliary model called BIP-AR 2D which allows to control the effect of the outliers in the innovative process. Later, we study the asymptotic behavior of the BMM 2D estimator and we give precise conditions for the strongly consistency and asymptotic normality of the estimator. The paper is organized as follows. Section 2 presents the motivation of this study based on: (a) the adequacy of AR-2D models in the representation of real images (Experiment 1); (b) the impact of contamination on the performance of classical least square estimators (Experiments 1 and 2) and (c) the asymptotic behavior, and the robustness of the BMM estimator under additive contamination comparing with the least square estimator (Experiment 3). In Section 3, the AR-2D model is formally defined with ==== parameters and the definition of the auxiliary model BIP-AR 2D is presented. Section 4 defines the BMM estimator of a 2D autoregressive model with ==== parameters. In Section 5, the theorems that give strong consistency and asymptotic normality to the BMM estimator are established. Section 6 discusses some final remarks and directions for future work. Appendix A proves the theorems presented in Section 5 and enunciates the lemmas necessary to achieve these demonstrations. Finally, an additional experiment that complements the experiments developed in Section 2 was carried out in Appendix B.",Asymptotic properties of BMM-estimator in bidimensional autoregressive processes,https://www.sciencedirect.com/science/article/pii/S0378375820300392,8 April 2020,2020,Research Article,198.0
"Gao Lucy L.,Zhou Julie","Department of Biostatistics University of Washington, Seattle, WA, 98195-7232, USA,Department of Mathematics and Statistics University of Victoria, Victoria, BC, Canada V8W 2Y2","Received 3 September 2019, Revised 9 March 2020, Accepted 29 March 2020, Available online 7 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.007,Cited by (7),", and develop a flexible algorithm for computing minimax D-optimal designs, which can be applied to any multi-response model with a discrete design space. We also derive several theoretical results for minimax D-optimal designs.","Consider the following multivariate regression model: ==== where ==== is the ====th observed vector for the ==== response variables ====, ==== with ==== are the ==== unknown regression parameters, and ==== is given by ====where ==== is the ====th design point for the ==== design variables ==== in a design space ====, and ==== is a ====-vector of linear or non-linear functions of ==== for ====. The design variables ==== may include both quantitative variables and qualitative factors. We assume that ==== and ==== are uncorrelated for ====. Model (1)–(3) is commonly used for experiments across biology, chemistry, toxicology, engineering, and other applied sciences.====Let ==== be a ==== positive definite (PD) matrix that could be a possible covariance matrix of ====, and consider the generalized least squares estimator (GLSE): ====Under model (1)–(3), the covariance matrix of ==== is given by ====In the special case that ==== in (4), where ==== and ==== is the ==== identity matrix, the GLSE is equivalent to the ordinary least squares estimator (OLSE) of ====, which we denote as ====. It follows from (5) that ====When ==== is known, we can use ==== in (4), and the GLSE is the best linear unbiased estimator (BLUE) for ====. Many papers have investigated optimal designs for the GLSE with ==== under model (1)–(3); see e.g. Atashgah and Seifi, 2007, Atashgah and Seifi, 2009, Liu et al., 2011, Liu and Yue, 2013, and Wong et al. (2019). Another body of work investigated optimal designs under continuous time regression models with correlated errors (Dette et al., 2016, Dette et al., 2017a, Dette et al., 2017b, Schorning et al., 2017, Dette et al., 2018), including continuous time versions of model (1)–(3).====Unfortunately, in practice, ==== is never known, which makes it impossible to use the GLSE with ====, or the optimal designs for the GLSE with ====. However, we often have a PD ==== matrix ==== which we believe is close to ====. For example, the matrix ==== may be derived from subject matter knowledge, or be derived from the results of a small pilot study. Thus, we can use ==== in (4), or use the OLSE for ====. Consider the loss functions ====where ==== represents the design measure of design points ====. We could compute D-optimal designs for the GLSE or the OLSE which minimize ==== or ====, respectively. However, the D-optimal designs would depend on the unknown ====, and computing the D-optimal designs under the assumption that ==== could lead to a loss in efficiency when ====.====Thus, in this paper we propose a new robust minimax D-optimality criterion, which approximates ==== with a neighborhood of matrices centered at ====. We consider both the GLSE with ==== and the OLSE. The minimax approach for regression designs has been investigated in the literature to construct designs which are robust against small departures of model assumptions; see Wiens (2015) for a review and for results for various one-response models. However, as far as the authors are aware, this approach has not been studied for robust designs for multi-response models against possible misspecification of ====.====It is extremely challenging to obtain minimax D-optimal designs analytically, even in the one-response model case, since the objective functions of the corresponding optimization problems are not convex (Wiens, 2015). Several numerical methods have been developed and used to compute optimal and robust designs, including multiplicative, exchange, genetic, simulated annealing and particle swarm optimization algorithms. Mandal et al. (2015) provide a review on these algorithms for finding optimal designs, and in general they work well for convex optimization problems. Atashgah and Seifi (2009) and Wong et al. (2019) have also investigated efficient algorithms for solving convex optimization problems for multivariate regression models. However, the optimization problem corresponding to the minimax D-optimal design problem is not a convex optimization problem, which makes it challenging to construct the minimax D-optimal designs numerically.====Nevertheless, we can show that the optimization problem is a difference of convex programming problem (Tao and Souad, 1986, Tuy, 1995, Lipp and Boyd, 2016, Le Thi and Pham Dinh, 2018). This allows us to use results from the difference of convex programming literature to develop a computationally efficient algorithm for computing minimax D-optimal designs on discrete design spaces. The algorithm can be applied to find minimax D-optimal designs for any multivariate regression model with discrete design space, which in turn makes it possible to conduct sensitivity analysis of the designs, and to explore special features of the designs. Difference of convex (DC) programming algorithms may also be applied to solve other optimization problems in statistics. For example, Nam et al. (2018) applied DC programming to a hierarchical clustering problem.====The rest of the paper is organized as follows. In Section 2 we propose a minimax D-optimal design criterion and derive its theoretical properties. In Section 3 we develop a general algorithm to compute minimax D-optimal designs on discrete design spaces. We present applications in Section 4 and make concluding remarks in Section 5. All proofs and derivations are in Appendix.",Minimax D-optimal designs for multivariate regression models with multi-factors,https://www.sciencedirect.com/science/article/pii/S0378375820300367,7 April 2020,2020,Research Article,199.0
"Zhao Qianqian,Zhao Shengli","School of Statistics, Qufu Normal University, Qufu 273165, China","Received 6 June 2019, Revised 7 November 2019, Accepted 22 March 2020, Available online 7 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.005,Cited by (10),"In practice, fractional ","Fractional factorial (FF) designs are widely used in scientific investigations and industrial experiments. When an FF experiment is performed, a completely random allocation of the treatment combinations to the experimental units is required. However, sometimes we may be unable to completely randomize the order of the runs since it is very difficult or expensive to control or change the levels of some factors. Then fractional factorial split-plot (FFSP) design is a practical strategy.====Suppose we wish to run an experiment with ==== factors, each at two levels. Among them, there are ==== factors whose levels are very difficult or expensive to change. The levels of the remaining ==== factors are relatively easy to change. The hard-to-change factors are called ==== (WP) ==== and the rest are called ==== (SP) ====. An FFSP design, denoted as ====, involves a two-phase randomization which results in two sources of errors in the analysis of variance, the WP and SP error terms. Usually, the former is larger than the latter. This implies that the power to detect significant effects in data analysis is not the same for the two kinds of factors. Hence the WP and SP factors cannot be treated equally.====Minimum aberration (MA) is a criterion for selecting optimal two-level FF designs proposed by Fries and Hunter (1980). Bingham and Sitter (1999a) gave a construction method of the MA FFSP designs and tabulated a catalog of MA two-level FFSP designs with 8 and 16 runs. Bingham and Sitter (1999b) discussed the impact of randomization restrictions on the choice of FFSP designs and developed theoretical results on MA FFSP designs. Yang et al. (2009) extended the results of Bingham and Sitter (1999b) to multi-level designs. Bingham and Sitter (2001) listed MA FFSP designs with up to 32 runs. Mukerjee and Fang (2002) explored a criterion of minimum secondary aberration (MSA), called MSA-FFSP criterion, which significantly narrows the class of competing nonisomorphic MA FFSP designs and hence often yields the unique optimal one. Ai and Zhang (2006) constructed MSA-FFSP designs in terms of consulting designs.====The technique of complementary designs is powerful and particularly useful in constructing designs when the number of columns of the complementary designs is relatively small compared to that in the original designs. This approach has found much applicability in the study of optimal designs under the MA, the maximum estimation capacity (MEC) and the general minimum lower order confounding (GMC) criteria. A comprehensive treatment of MA ==== designs with ==== was given by Chen and Wu (1991) and Robillard (1968). For larger ====, a direct attack on the problem becomes unmanageable. Tang and Wu (1996) developed the method of complementary designs and obtained all the MA ==== designs with ====, where ==== is the number of columns of the complementary designs. Suen et al. (1997) established the relationship between the wordlength pattern of a ====-level design and that of its complementary design, and obtained the MA ==== designs with ====. By using the technique of complementary designs, Mukerjee and Wu (2001) constructed mixed-level MA designs. For constructing the optimal designs under the MEC criterion, the main technical tool was again the complementary designs (see Cheng and Mukerjee, 1998). Zhang and Mukerjee (2009a) derived explicit formulae connecting the key terms for GMC criterion with the complementary designs and applied the results to find two- and three-level GMC designs. Zhang and Mukerjee (2009b) obtained blocked GMC designs in terms of complementary designs. One can refer to Chen and Cheng (1999) and Cheng and Tang (2005) for other applications of the complementary designs.====Montgomery (2013) mentioned an experiment in which the factors affecting uniformity in a single-wafer plasma etching process were investigated. The example showed real context that the WP factors are more important than the SP factors. Tichon et al. (2012) considered five scenarios for more flexible split-plot design choices. One of them is the setting “WP factors are more important than SP factors”. Consequently, if the experimenter has some prior information that the WP factors are possibly more significant than the SP factors, then we need a new criterion to select FFSP designs. Wang et al. (2019) proposed the minimum aberration of type WP (WP-MA) criterion and constructed the 8, 16 and 32 runs WP-MA designs for ==== using the first moment. In some situations, such as the digital experimentation area, the experimenters may have a lot of factors to be tested on the product at the same time. Then WP-MA FFSP design with large ==== or ==== is needed. In this case, ==== and/or ==== become larger for given runs and therefore a different approach for constructing WP-MA FFSP designs is required.====The purpose of the present work is to develop a theory for constructing WP-MA FFSP designs via complementary designs. The rest of the paper is organized as follows. Section 2 introduces the definition of the WP-MA criterion and some related definitions and notation. Section 3 develops a general theoretical approach to the construction of WP-MA FFSP designs via complementary designs, which simplifies the derivation of WP-MA FFSP designs for relatively large ==== and hence is of practical interest. Using the theoretical results obtained in Section 3, Section 4 gives the complementary sets of the WP-MA FFSP designs for ==== and ====, where ==== and ==== are the numbers of columns of the complementary sets for the WP and SP parts respectively. Section 5 gives a conclusion and proposes directions for future research. All proofs and the tables for complementary sets of the WP-MA FFSP designs with ==== are given in Appendix A Proofs of the theorems, Appendix B Tables of the complementary sets for the WP-MA designs. Even for 16 runs, the WP-MA designs in Wang et al. (2019) can accommodate at most ==== factors, while our results contain all the nonisomorphic FFSP designs and hence complement WP-MA designs with large ====. Our final results and tables are expected to be particularly useful in practice when the complementary designs are relatively small in size and hence easy to handle.",Constructing minimum aberration split-plot designs via complementary sets when the whole plot factors are important,https://www.sciencedirect.com/science/article/pii/S0378375820300343,7 April 2020,2020,Research Article,200.0
Kakizawa Yoshihide,"Faculty of Economics, Hokkaido University, Nishi 7, Kita 9, Kita-ku, Sapporo 060–0809, Japan","Received 12 July 2019, Revised 14 January 2020, Accepted 28 March 2020, Available online 6 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.009,Cited by (7),"A multivariate Birnbaum–Saunders distribution is introduced to consider a new nonparametric multivariate density estimation for nonnegative data. By construction, the estimator is the average of varying nonnegative kernel with correlation structure, whose support matches the support of the density to be estimated, unlike the classical Rosenblatt–Parzen ====. The finite sample performance of the estimator is investigated for the ====.","The Rosenblatt–Parzen kernel density estimator is perhaps the most popular in the nonparametric density estimation and its asymptotic property is well documented when ====, where ==== is the density to be estimated and ==== is the dimension of the data. See, e.g. Silverman (1986) and Wand and Jones (1995) for details. However, if ====, the Rosenblatt–Parzen kernel density estimator suffers from the boundary bias, generally. As an illustration, we consider the bivariate situation where a random sample ====, ==== is drawn from an unknown density ==== with support ==== (say). The standard kernel density estimator is defined by ====(here, the product-type kernel ==== is used for simplicity), where a one-dimensional symmetric kernel ==== is assumed to have the support ====, and a bandwidth ====; ==== tends to zero as ====. Write ==== (of course, ==== for ====, provided that ====). If ==== has continuous second partial derivatives, then, ==== A similar calculation shows that ==== for any ====. Due to the inconsistency near the boundary region (note that, without bias correction, the downward bias of the Rosenblatt–Parzen kernel density estimator (1) at the corner ==== is approximately equal to ==== (see (2))), the boundary correction for removing the boundary bias is an important topic. Jones (1993) gave, in the univariate setting, an extensive review on renormalization, reflection, and generalized jackknifing, until 1993. See also Zhang et al. (1999) and Karunamuni and Alberts, 2005a, Karunamuni and Alberts, 2005b for more advanced reflection techniques.====Over the last two decades, there has been a growing interest in the use of asymmetric kernel (AK), whose support matches the support of the density to be estimated. To the best of our knowledge, Silverman (1986, p28) first mentioned a possible application of gamma or log-normal density as the kernel. Some specific proposals date back to Chen, 1999, Chen, 2000. Assuming that a random sample of size ====; ==== is drawn from an unknown univariate density ==== with support ==== or ====, Chen, 1999, Chen, 2000 suggested using beta or gamma density as the kernel. This type of research, i.e., nonparametric AK density estimation, has received considerable attention in the literature. Especially, the univariate AK density estimator at point ==== is defined by ====, on the basis of various kernels ====, where ==== is a smoothing parameter which converges to zero. Chen (2000), Chaubey et al. (2012), and Igarashi and Kakizawa (2014) used gamma density, Jin and Kawczak (2003) Birnbaum–Saunders (BS) and log-normal densities, Scaillet (2004) inverse Gaussian and reciprocal inverse Gaussian (IG/RIG) densities, Koul and Song (2013) and Kakizawa and Igarashi (2017) inverse gamma density, Marchant et al. (2013) and Saulo et al. (2013) generalized BS density, Igarashi and Kakizawa (2014) generalized IG density, Igarashi (2016) weighted log-normal density, and Igarashi and Kakizawa (2018) (see also Hirukawa and Sakudo (2015)) generalized gamma density. Note that Igarashi and Kakizawa (2014) treated the IG, RIG, and BS kernel density estimators in a unified way, referred to as a mixture of IG (MIG) kernel density estimator. Kakizawa (2018) developed symmetrical-based IG, RIG, and BS kernel density estimators, that is an extension of (normal-based) MIG kernel density estimator, including log-symmetrical kernel density estimator.====For the ====-variate case with ==== or ====, Bouezmarni and Rombouts (2010), Funke and Kawka (2015), Zougab et al. (2018), and Igarashi and Kakizawa (2020) studied the product-type AK density estimation, using the numerous existing (one-dimensional) AKs as mentioned above. However, there is little work on non-product-type AK density estimation, except that Kokonendji and Somé (2018) used bivariate beta-Sarmanov density and Igarashi (2018) multivariate weighted log-normal density. In this paper, we further attempt to develop the non-product-type AK density estimation on the basis of multivariate BS density. Ideally, such a correlation structure of the kernel under consideration may offer the potential for improved density estimation. That is the reason why we focus on the non-product-type.====The rest of the paper is organized as follows. In Section 2, we start with the boundary corrected kernel density estimator. In Section 3, we introduce bivariate non-central BS (BNBS) distribution and propose the BNBS kernel density estimator. In Section 4, we study the asymptotic properties under suitable conditions. The ====-dimensional extension is further presented in Section 5. Section 6 contains a numerical study for the bivariate case. Section 7 concludes the paper, along with future works.",Multivariate non-central Birnbaum–Saunders kernel density estimator for nonnegative data,https://www.sciencedirect.com/science/article/pii/S0378375820300380,6 April 2020,2020,Research Article,201.0
"Fuchs Mathias,Hornung Roman,Boulesteix Anne-Laure,De Bin Riccardo","Computation and Design Research Group, Zaha Hadid Architects London, United Kingdom,Institute for Medical Information Processing, Biometry and Epidemiology, University of Munich, Germany,Department of Mathematics, University of Oslo, Norway","Received 31 May 2019, Revised 24 January 2020, Accepted 16 March 2020, Available online 2 April 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.003,Cited by (3),-statistic can be formulated explicitly as a ,"Let ====, ==== be ====-dimensional random vectors with an arbitrary distribution. Let ====, and let ==== be an arbitrary measurable symmetric function of ==== arguments, where “symmetric” means “invariant under permutation of its arguments”. Write ==== for the well-defined value of ==== at those ==== with indices from ====. Consider the average of ==== in the maximal family ==== of unordered size-==== subsets ====Any statistic of such a form is called a ====-statistic. If ==== is the smallest number such that there exists a symmetric function ==== that represents a given parameter ====, with ==== a probability distribution on ====, in the form ====then ==== is called the degree of ==== or of ====, and the function ==== is called a kernel of ====. A parameter ==== (hereafter only ====) of the form (2) is called a regular parameter. Note that here ‘symmetrization’ means taking the average over all permutations of the ==== arguments. It is also possible to associate a ====-statistic with a non-symmetric kernel ====, i.e., to estimate ==== by a ====-statistic. The cost is to deal with ==== summands, many more than just the binomial coefficient ====. The symmetrization, indeed, consists in grouping all ==== summands involving the same unordered index set together.====A ====-statistic enjoys several properties, including the fact that it is an unbiased estimator of the associated parameter and has minimal variance among all unbiased estimators over any family of distributions ==== containing all properly discontinuous distributions (Hoeffding, 1948 page 297). Moreover, Hoeffding (1948) shows the asymptotic normality of ====-statistics as ====. The theory of ====-statistics provides a unified framework in which several classical tools can be investigated. For example the sample mean can be seen as a ====-statistic with ==== associated with ====, and the sample variance as a ====-statistic associated with ==== and ====. More interestingly, non-parametric tools like the Wilcoxon or the Mann–Whitney statistics (just to cite the most famous) and general estimation processes based on resampling (e.g., bootstrapping, jackknifing, cross-validation) can also be studied within the ====-statistic framework.====With the application to cross-validation in mind, Wang and Lindsay (2014) addressed a general problem within the ====-statistic framework, namely the estimation of the variance of a ====-statistic (see also Lee, 1990 Chapter 5). In particular, they demonstrated that when ====, one can construct an unbiased estimator of the variance of a ====-statistic, which is a ====-statistic itself (Wang and Lindsay, 2014 Theorem 1). However, the kernel of their ====-statistic for variance estimation depends on ====, which precluded them from applying the ====-statistic central limit theorem and thereby deducing immediate consequences about the large sample size asymptotic behaviour (Wang and Lindsay, 2014 Remark 3). The main contribution of the present work is to fill this gap.====Papers related to this work include those of Schucany and Bancson (1989), who show that there exists an unbiased variance estimator for an arbitrary ====-statistic of degree two, and of Peel et al. (2010), who investigate tail inequalities for ====-statistics of degree two in ranking problems. Moreover, Fuchs and Krautenbacher (2016) recently investigated incomplete ====-statistics associated with non-symmetric kernels to derive conditions under which balanced incomplete block designs minimize the variance of the error estimator among all cross-validation procedures of a fixed size. Further results on variance estimators for U-statistics can be found in Maesono (1998).====The structure of the paper is as follows. In Section 2 we define the ====-statistic estimator of the variance and recall some properties. The main results are presented in Section 3: in Theorem 3.1 we demonstrate consistency of the estimator and its minimum variance among the unbiased estimators; in Theorem 3.2 we show its asymptotic normality. These results allow us to derive an asymptotically exact hypothesis test for our application problem, namely a test of equality of the true errors of two learning algorithms (Section 4). Here we call a test “asymptotically exact” if the limit of the type I error rate approaches the significance level for ==== going to infinity. The confidence interval’s coverage probability and the hypothesis test are illustrated through simulations and application to a dataset from the UCI Machine Learning Repository in Section 5.",On the asymptotic behaviour of the variance estimator of a ,https://www.sciencedirect.com/science/article/pii/S037837582030032X,2 April 2020,2020,Research Article,202.0
"Ai Chunrong,Huang Lukang,Zhang Zheng","School of Management and Economics, Chinese University of Hong Kong, Shenzhen, China,Institute of Statistics and Big Data, Renmin University of China, Beijing, China","Received 27 August 2019, Revised 2 March 2020, Accepted 3 March 2020, Available online 14 March 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.002,Cited by (6),"This article considers a Mann–Whitney test of distributional effects in a multivalued treatment. Specifically, we first show that, under the unconfoundedness condition, the counterfactual distributions are weighted averages, with weights satisfying some moment restrictions. We estimate the weights directly from those restrictions by maximizing a globally concave objective function and then construct the Mann–Whitney statistics with the estimated distributions. We show that our Mann–Whitney statistics are efficient, attaining the semiparametric efficiency bound which is also derived here. A simulation study and an application to the analysis of racial discrimination illustrate the practical value of the proposed approach.","Evaluating the effect of the exposure of a set of units to a program, or treatment, on some outcome, is the central problem studied in the literature on the causal effect of programs. The object of interest of this literature is a comparison of the two outcomes for the same unit when exposed and when not exposed to the treatment, and a key feature is the accommodation of general heterogeneity in the outcomes. Most of this literature focus on the comparison of the two average outcomes over the whole population (e.g., the so-called average treatment effect (ATE)) or the treated subpopulation (e.g., the so-called average treatment effect on the treated (ATT)). Example studies of this sort include Rosenbaum and Rubin, 1983, Rosenbaum and Rubin, 1984, Rosenbaum, 1987, Hahn (1998), Hirano et al. (2003), Cattaneo (2010), Cattaneo and Farrell (2011), Farrell (2015), Chan et al. (2016) and Peng et al. (2019) among others. Few studies focus on the comparison of the quantiles of the outcomes (e.g., the so-called quantile treatment effect (QTE) or quantile treatment effect on the treated (QTT)). See Firpo (2007) for an example, Imbens and Wooldridge (2009) for an extensive literature survey.====Despite simplicity of the average and quantile treatment effects, these parameters do not tell the whole story. It is possible that, on average, there does not exist the treatment effect when some units experience positive treatment effects while other units experience negative treatment effects. Thus, focusing on the average treatment effect may miss out the treatment effects on subgroups. It is also possible that some quantile treatment effects are positive while other quantile treatment effects are negative. Again, focusing on some quantiles may miss out the treatment effects on other quantiles. Ideally, one would like to compare the outcome distributions, not just some moments. For example, one may be interested in the comparison of the two outcome distributions for the whole population when exposed and when not exposed to the treatment. These distributions could reveal not just the magnitude of the effects (if any) but also the proportion of the population benefitted from (or harmed by) the treatment. One may also be interested in the comparison of the two outcome distributions for a subpopulation (e.g., treated or untreated population) when exposed and when not exposed to the treatment. These distributions may help decision makers developing targeted programs or treatments. In light of the importance of the distribution comparison, the literature call the distribution difference as the distributional effect.====Clearly, to detect and analyze the distributional effects, one must have consistent distribution estimates available. Since we only observe each unit either exposed or not exposed to the treatment, participation selection, common in observational data, could complicate the estimation. Under the condition of unconfoundedness and multivalued treatment, Cattaneo (2010) and Lee (2018) develop the efficiency bound for the functional of the marginal distribution of potential outcome, which includes the bound of the counterfactual distribution as a particular case. Efficient estimators are also proposed, see Cattaneo, 2010, Lee, 2018, Cattaneo and Farrell, 2011, Farrell, 2015, Donald and Hsu, 2014 and Ao et al. (2019); indeed, Cattaneo (2010) proposes inverse probability weighting and efficient influence function estimators. Cattaneo and Farrell (2011) propose an efficient estimation through subclassification. Farrell (2015) proposes a doubly robust estimator using group lasso selection when the number of covariates is large. Donald and Hsu (2014) propose an inverse probability weighting estimator for the counterfactual distribution, and they also suggest some stochastic dominance statistics for detecting the distributional effects. Ao et al. (2019) propose inverse probability weighting and doubly robust estimators. In addition, Chernozhukov et al. (2013) estimate the counterfactual distribution by regressing outcome on covariates.====The Mann–Whitney statistic, which compares the outcome distributions between two groups, is often applied to detect the distributional effect over the whole population (e.g., Wu et al., 2014, Vermeulen et al., 2015, Zhang et al., 2019). Zhang et al. (2019) even computes the semiparametric efficiency bound of the Mann–Whitney statistic. However, their estimation of Mann–Whitney type parameters are based on parametric models, hence they suffer from the model misspecification problems. In a missing data setting where two samples are generated independently from two distributions, Schisterman and Rotnitzky (2001), Cheung (2005), Chen et al. (2013) and Lin et al. (2018) propose Mann–Whitney statistics to test the difference of the two distributions. Lin et al. (2018) also derive the semiparametric efficiency bound of the Mann–Whitney statistics for the two-samples problem and show that their Mann–Whitney statistics attain the semiparametric efficiency bound under some additional probabilistic index model. We note that the treatment effect problem is not the same as the two-samples problem. For example, condition 5 of Lin et al. (2018) is not satisfied by the treatment effect model. Even if we can create two artificial samples from the treatment effect data using the approach suggested in Chen et al. (2013) and Lin et al. (2018), the two samples are not independent. Therefore, the semiparametric efficiency bound derived in Lin et al. (2018) for the two-samples problem may not be the semiparametric efficiency bound for the treatment model.====To summarize, efficient estimation of the counterfactual distributions over the whole population has been studied extensively in the literature, However, efficient Mann–Whitney test of distributional effects over the whole population or a subpopulation receives little attention from the literature. The main objective of this paper is to fill in this gap. Specifically, following the general setup of Lee (2018), we shall derive the semiparametric efficiency bounds of the Mann–Whitney indicators of distributional effects for the treatment effect models under the condition of unconfounded treatment assignment. We shall then propose estimation of the counterfactual distributions and Mann–Whitney statistics, derive their asymptotic distributions and show that they attain the semiparametric efficiency bounds. Intuitive and consistent variance estimators are also provided.====The remainder of the paper is organized as follows. Section 2 describes the counterfactuals and the Mann–Whitney indicators in the discrete treatment model. Section 3 derives the efficiency bounds for the Mann–Whitney indicators in Section 2. Section 4 presents an estimation of the counterfactual distributions and the Mann–Whitney indicators in Section 2. Section 5 derives large sample properties of the proposed estimators in Section 4. Section 6 provides a data-driven approach to select the smoothing parameter and some consistent covariance matrices for the proposed estimators. Section 7 reports on a small scale simulation study. Section 8 reports on an empirical application, followed by some concluding remarks in Section 9. Sketched proofs of the main theorems are presented in Appendix A Proof of, Appendix B Proof of, Appendix C Sketched proof of, Appendix D Proof of, Appendix E Derivation of, while detailed arguments are relegated to the supplemental material.",A Mann–Whitney test of distributional effects in a multivalued treatment,https://www.sciencedirect.com/science/article/pii/S0378375820300318,14 March 2020,2020,Research Article,203.0
Großmann Heiko,"Institut für Mathematische Stochastik, Otto-von-Guericke-Universität Magdeburg, Postfach 4120, 39016 Magdeburg, Germany","Received 5 July 2019, Revised 5 December 2019, Accepted 1 March 2020, Available online 6 March 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.03.001,Cited by (3),"-optimal utility-neutral designs for the modified version of the model have been derived by orthogonally blocking ====-optimal utility-neutral designs for the model without block effects. In this paper, we first discuss the proper interpretation of the block effects showing that they only explain a special and non-standard type of respondent heterogeneity which refers to the order in which the alternatives in each pair are presented. Secondly, for the special case of the choice model with block effects in which the main effects of ==== two-level attributes are to be estimated we show how by using a previously unnoticed relationship with blocked two-level main effects plans for the linear ====-optimal in a certain subclass of designs.","Over the past years there has been considerable interest in the optimal and efficient design of stated choice experiments and advances in the field have been summarized at various stages and from different perspectives  (Großmann et al., 2002, Louviere et al., 2004, Street and Burgess, 2007, Burgess et al., 2012, Street and Burgess, 2012, Rose and Bliemer, 2014, Großmann and Schwabe, 2015). Paired comparison experiments (see, e.g., David, 1988, Bradley, 1976) in which respondents need to choose between alternatives which are presented in pairs are an important special case. Often, data from such experiments are modelled by using the logistic versions of the Bradley–Terry model (Bradley and Terry, 1952) or the equivalent choice model of Luce (1959), both of which coincide with the popular multinomial logit (MNL) model (Louviere et al., 2000, Train, 2003) when the latter is restricted to pairs. The need and scope for applying experimental design theory arise from the fact that in most applications the alternatives have a factorial structure and are vectors of levels of certain qualitative attributes whose effects on the choice probabilities are to be investigated.====In a recent paper, Singh et al. (2019) considered a variant of the MNL model for factorial paired comparison experiments in which they introduced additional parameters for block effects into the linear predictor for the utility differences. These block effects are intended to account for a certain type of respondent heterogeneity. Optimal designs for estimating the effects of the attribute levels are derived by orthogonally blocking optimal paired comparison designs for the model without blocks (for example, designs of Street and Burgess, 2007, Graßhoff et al., 2004, Demirkale et al., 2013).====The main goals of this paper are to clarify the meaning of block effects in the MNL model for paired comparisons and to complement the results of Singh et al. (2019) for the case where all attributes have two levels and only the main effects are to be estimated. By noting that under the so-called indifference assumption the problem of finding optimal designs for the MNL model with blocked pairs is closely related to the optimal design problem for blocked two-level factorial experiments it is possible to translate optimality results for the latter experiments into optimality results for choice experiments. In particular, ====-optimal two-level main effects plans for blocked factorial experiments derived by Jacroux, 2011a, Jacroux, 2011b, Jacroux, 2013a are also ====-optimal or highly efficient within a certain class of designs for estimating the utility parameters in factorial paired comparison experiments with blocks of equal size. These designs can be used in cases where the constructions of Singh et al. (2019) are not applicable.====The remainder of the paper is organized as follows. In Section 2 we discuss the meaning of block effects in the MNL model for paired comparisons and clarify what type of respondent heterogeneity they account for. How the ====-optimality criteria for the MNL model with and without block effects are related is considered in Section 3. Section 4 shows how the ====-optimality of blocked two-level main effects plans carries over to the MNL model for paired comparisons with block effects. Finally, Section 5 offers some conclusions.",On the meaning of block effects in paired comparison choice experiments and a relationship with blocked ,https://www.sciencedirect.com/science/article/pii/S0378375820300240,6 March 2020,2020,Research Article,204.0
"Bachoc F.,Lagnoux A.","Institut de Mathématiques de Toulouse; UMR5219, Université de Toulouse; CNRS, UT3, F-31062 Toulouse, France,Institut de Mathématiques de Toulouse; UMR5219, Université de Toulouse; CNRS, UT2J, F-31058 Toulouse, France","Received 26 March 2019, Revised 25 October 2019, Accepted 24 February 2020, Available online 5 March 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.02.008,Cited by (7),"We consider the estimation of the variance and ====. We study the fixed-domain ====, the composite likelihood estimator converges at a sub-optimal rate and we provide its non-Gaussian ====. For large values of ====, the estimator converges at the optimal rate. Second, we consider the case where the variance and the ==== are jointly estimated. We obtain the same conclusion as for the first case for the estimation of the microergodic parameter. The theoretical results are confirmed in numerical simulations.","Gaussian processes are widely used in statistical science to model spatial data. When fitting a Gaussian random field, one has to deal with the issue of the estimation of its covariance function. In many cases, it is assumed that this function belongs to a given parametric model or family of covariance functions, which turns the problem into a parametric estimation problem. Within this framework, the maximum likelihood estimator (MLE) (Rasmussen and Williams, 2006, Stein, 1999) of the covariance parameters has been widely studied in the last years in the two following asymptotic frameworks. The fixed-domain asymptotic framework, sometimes called infill asymptotics (Cressie, 1993, Stein, 1999), corresponds to the case where more and more data are observed in some fixed bounded sampling domain in ====; while the increasing-domain asymptotic framework corresponds to the case where the sampling domain, also in ====, increases with the number of observed data and the distance between any two sampling locations is bounded away from ====. The asymptotic behavior of the MLE of the covariance parameters can be quite different under these two frameworks (Zhang and Zimmerman, 2005).====Under increasing-domain asymptotics, generally speaking, for all (identifiable) covariance parameters, the MLE is consistent and asymptotically normal under some mild regularity conditions. The asymptotic covariance matrix is equal to the inverse of the (asymptotic) Fisher information matrix (Bachoc, 2014, Cressie and Lahiri, 1996, Mardia and Marshall, 1984, Shaby and Ruppert, 2012).====The situation is significantly different under fixed-domain asymptotics. Indeed, two types of covariance parameters can be distinguished: microergodic and non-microergodic parameters. A covariance parameter is microergodic if, for two different values of it, the two corresponding Gaussian measures are orthogonal, see Ibragimov and Rozanov, 1978, Stein, 1999. It is non-microergodic if, even for two different values of it, the two corresponding Gaussian measures are equivalent. Non-microergodic parameters cannot be estimated consistently, but misspecifying them asymptotically results in the same statistical inference as specifying them correctly (Stein, 1988, Stein, 1990a, Stein, 1990b, Zhang, 2004). In the case of isotropic Matérn covariance functions with ====, Zhang (2004) shows that only a reparametrized quantity obtained from the variance and the spatial scale parameters is microergodic. The asymptotic normality of the MLE of this microergodic parameter is then obtained in Kaufman and Shaby (2013). Similar results for the special case of the exponential covariance function were obtained previously in Ying (1991).====The maximum likelihood method is generally considered as the best option for estimating the covariance parameters of a Gaussian process (at least in the framework of the present paper, where the true covariance function does belong to the parametric model, see also (Bachoc, 2013, Bachoc, 2018)). Nevertheless, the evaluation of the likelihood function requires to solve a system of linear equations and to compute a determinant. For a data set of ==== observations, the computational burden is ====, making this method computationally untractable for large data sets (at the exception of a few specific situations, see for instance (Hartikainen and Särkkä, 2010), where the computational cost of the MLE is reduced to ==== in the special case of the Matérn covariance function in dimension one and for a smoothness parameter of the form ==== with ====). This fact motivates the search for estimation methods with a good balance between computational complexity and statistical efficiency. Among these methods, we can mention low rank approximation (see Stein (2014) and the references therein for a review), sparse approximation (Hensman and Fusi, 2013), covariance tapering (Furrer et al., 2006, Kaufman et al., 2008), Gaussian Markov random fields approximation (Datta et al., 2016, Rue and Held, 2005), submodel aggregation (Cao and Fleet, 2014, Deisenroth and Ng, 2015, Hinton, 2002, Rullière et al., 2018, Tresp, 2000, van Stein et al., 2015) and composite likelihood. With composite likelihood, we indicate a general class of objective functions based on the likelihood of marginal or conditional events (Varin et al., 2011). This kind of estimation method has two important benefits: it is generally appealing when dealing with large data sets and it can be helpful when it is difficult to specify the full likelihood. The composite likelihood approach is also used in emulating computationally expensive com- puter models. See e.g. [19, Section 7].====Consider the observations ==== of a Gaussian process ==== corresponding to the observation points ====. In this work, we focus on composite likelihood estimators (CLEs) of the covariance parameters that maximize the sum, over ====, of the conditional log likelihood of ==== given a subset of ==== that corresponds to observation points that are nearby ====. These estimators have been considered in several references, including Mateu et al., 2007, Pardo-Igúzquiza and Dowd, 1997, Stein et al., 2004, Vecchia, 1988. More generally, the principle of conditioning based on neighbor observation points rather than on the full set of observation points is widely applied for Gaussian processes (Gramacy and Apley, 2015, Gramacy et al., 2016).====Despite their popularity in practice, no general fixed-domain asymptotic results exist for the above CLEs. The existing results address the exponential covariance function in dimension one. In this case, letting ==== be the observation points, the CLE coincides with the MLE due to the Markov property when the likelihood of each ==== is evaluated conditionally to the previous observations ==== for any arbitrary value of ==== (see Ying (1991)). The CLE of the microergodic parameter is asymptotically Gaussian in this special case. When each ==== is evaluated conditionally to its two neighbor observations ====, then the CLE of the microergodic parameter is also asymptotically Gaussian (Bachoc et al., 2017). Finally, we remark that also pairwise likelihood estimators have been analyzed recently, in the case of the exponential covariance function in dimension one (Bachoc et al., 2019).====In this work, we provide a fixed-domain asymptotic analysis of composite likelihood, for Gaussian processes in dimension one, that extends the previous references considerably, in terms of generality. Indeed, we allow for covariance functions ==== where ==== where the remainder ==== is negligible compared to ==== as ====. Here ==== is the variance parameter, ==== is the spatial scale parameter and ==== is the fixed smoothness parameter, with ====. In contrast, only the special case with ==== corresponding to exponential covariance functions is considered in Bachoc et al., 2019, Bachoc et al., 2017, Ying, 1991. In particular, we allow for general Matérn covariance functions with smoothness parameter ==== between ==== and 0.75, while in Bachoc et al., 2019, Bachoc et al., 2017, Ying, 1991, only the case ==== is considered. For more details on the Matérn covariance function the reader may refer to Section 3.4. Furthermore, we allow for any fixed number of neighbor observation points, both on the left and on the right, for the composite likelihood, as opposed to two neighbor points or only points on the left in Bachoc et al., 2017, Ying, 1991.====First we consider the case where only the variance parameter ==== is estimated. We show that if ====, then the CLE converges at the sub-optimal rate ====, with an explicit asymptotic variance and is not asymptotically Gaussian, regardless of the number of neighbors used. Furthermore, we provide its non-Gaussian asymptotic distribution. This result is somehow surprising since, in this setting, quadratic variation estimators, also having a small computational cost compared to the MLE, converge at rate ==== (Azaïs et al., 2018, Istas and Lang, 1997). This could motivate practical adjustments of composite likelihood for Gaussian processes with small smoothness. For ====, the CLE converges at the optimal rate ====.====Second, we consider the case where the variance ==== and the spatial scale ==== are jointly estimated, in which case ==== is microergodic. We obtain the same conclusions as above. For ====, the CLE has sub-optimal rate ==== and we provide its non-Gaussian asymptotic approximation. Furthermore, the CLE converges at the optimal rate ==== for ====.====Many of the proof techniques we suggest are original, notably to take into account several neighbor points, on the left and on the right, for the composite likelihood. This situation was not explored theoretically in the references Bachoc et al., 2019, Bachoc et al., 2017. In particular, we approximate the conditional expectations and variances of prediction, given a fixed number of neighbor observations under fixed-domain asymptotics, see (S.9) and (S.10) in the proofs in the supplementary material. Furthermore, we apply some concepts from the literature of quadratic variation estimators (Azaïs et al., 2018, Istas and Lang, 1997) to composite likelihood, such as finite sequences applied to functions with given orders of differentiability.====For ====, we confirm that the rate is ==== and that the asymptotic variance is provided by our expression, in numerical simulation. We observe that the number of observations ==== may need to be very large for the asymptotic results to provide an accurate approximation of the finite sample results.====Most of our results are provided for equispaced observation points, but an extension to non equispaced points is also given. We also discuss potential extensions to the multi-dimensional case and explain the additional technical obstacles that occur then.====The rest of the paper is organized as follows. In Section 2, we introduce the model and the CLE. In Section 3, we provide the results for the estimation of the variance parameter ==== while Section 4 is dedicated to the estimation of the microergodic parameter ====. Section 5 presents the numerical results. In Section 6, we discuss the two extensions to the case of non regular designs and to the two-dimensional setting. Concluding remarks are given in Section 7. All the proofs are given in the supplementary material.",Fixed-domain asymptotic properties of maximum composite likelihood estimators for Gaussian processes,https://www.sciencedirect.com/science/article/pii/S0378375820300239,5 March 2020,2020,Research Article,205.0
"Hu Jianhua,Ding Hao,Liu Liangyuan,Feng Jingyan","School of Statistics and Management, and Key Laboratory of Mathematical Economics (MOE), Shanghai University of Finance and Economics, Shanghai 200433, China,School of Mathematics and Statistics, Shanxi Datong University, Shanxi 037009, China","Received 23 July 2019, Revised 23 February 2020, Accepted 23 February 2020, Available online 4 March 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.02.006,Cited by (1), applications in finance and epidemiology are provided.,"There often exist nonlinear features in time series data arisen from many scientific fields such as finance, sound analysis, epidemiology and neuroscience. As a result, the traditional linear ARMA modeling techniques may not work well in real applications (Box and Jenkins, 1970). A good alternative is the nonparametric modeling technique. Due to these facts that it has appreciable flexibility and interpretability to the mean structure, and can also avoid the “curse of dimensionality”, the functional coefficient modeling technique is highly recommended for analyzing the time series data with nonlinear features, for instance Cai et al., 2000a, Cai and Xiao, 2012, Cai and Xu, 2008, Chen et al., 2015, Huang and Shen, 2004 and Qiu et al. (2015). Usually, a functional coefficient model is defined as the following form ====where ==== is the response, ==== and ==== are the explanatory variables, ==== is the random error and ==== means the model variables forming a triangular array instead of a sequence.====Almost all aforementioned papers made the assumption that time series ==== are jointly ====. In many practical situations there often exists a non-constant time trend in time series data. Consider three time series data sets: annual global temperature deviation series (Gao and Hawthorne, 2006, Douc et al., 2014), daily volatilities of the NASDAQ Composite index and hospital admissions of circulation & respiration (Fan and Zhang, 1999, Fan and Zhang, 2000, Cai et al., 2000a, Zhang and Wu, 2012). These time series data are shown in Fig. 1. It is obviously seen that these time series data have non-constant time trend. The non-constant time trends are estimated by smoothing technique and the results are shown in Fig. 1 as well.====In order to satisfy the (strictly) stationary condition, one often has to preprocess the real data to eliminate the trend by differencing or detrending. Thus, the model (1.1) only reveals the underlying dynamics of the detrended time series, while the evolutionary nature of the original data is difficult to interpret. It seems more desirable to directly model the original time series with trend, seasonal components and other nonstationary factors.====Suppose that ==== is the original data. Without loss of generality, the original data could be written as ====where ==== and ==== are unknown smoothing functions which represent the time trends (====), ==== and ==== are the detrended time series data. As usual in the literature, the functions ==== and ==== do not depend on real time ==== but rather on rescaled time ====. We denote ====, ====
 ==== for brevity. And substituting (1.2) into (1.1) we have ====Let ==== and ====. Then a more general functional model for the original data could be written as ====For the explanatory variables ==== with ==== we allow them to deviate from stationarity, especially when the length of time series tends to infinity.====An alternative realistic approach is to assume the second-order characteristic of time series vary over time slowly. Such a natural generalization of stationary to nonstationary time series is called locally stationary process introduced by Dahlhaus, 1996b, Dahlhaus, 1996a, Dahlhaus, 1997, which can set down a meaningful asymptotic inference such as consistency, asymptotic normality, efficiency and so on. Intuitively speaking, the locally stationary process locally at each time are close to a stationary process, but its second order characteristic are gradually changing in an unspecific way as time evolves. Several models have been proposed for locally stationary processes (Hu et al., 2019, Dette et al., 2011, Pei et al., 2018, Li et al., 2020). A more formal definition of locally stationary process is the following one,",Statistical inference of locally stationary functional coefficient models,https://www.sciencedirect.com/science/article/pii/S0378375820300215,4 March 2020,2020,Research Article,206.0
"Xu Wenchao,Ding Hui,Zhang Riquan,Liang Hua","Key Laboratory of Advanced Theory and Application in Statistics and Data Science, MOE, and School of Statistics, East China Normal University, Shanghai, 200062, China,School of Statistics and Mathematics, Shanghai Lixin University of Accounting and Finance, Shanghai, 201609, China,School of Economics, Nanjing University of Finance and Economics, Nanjing, 210023, China,Department of Statistics, George Washington University, Washington, DC 20052, USA","Received 4 July 2019, Revised 25 February 2020, Accepted 25 February 2020, Available online 3 March 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.02.007,Cited by (4)," component, and construct confidence bands centered at FPCA-based estimator for the slope functions and verify its asymptotic validity. The performance of the proposed procedures is illustrated via simulation studies and an analysis of a diffusion tensor imaging data application.","Functional linear regression (FLR) with scalar response (scalar-on-function linear regression) is one of the most basic regression models among all functional regressions, which has been extensively studied in the literature (Cardot et al., 1999, Cai and Hall, 2006, Hall and Horowitz, 2007, Li and Hsing, 2007, Yuan and Cai, 2010, Cai and Yuan, 2012, Lei, 2014, Shang and Cheng, 2015, Cai et al., 2018, Imaizumi and Kato, 2019). Functional principal component analysis (FPCA) and reproducing kernel Hilbert space (RKHS) are two popular methods for estimation of this model. For example, Cai and Hall (2006) and Hall and Horowitz (2007) derived the minimax optimal rates of convergence for prediction and estimation of FPCA-based estimator. Lei (2014) proposed a global test for the slope function based on FPCA approach. Imaizumi and Kato (2019) developed a simple method to construct confidence bands centered at the FPCA-based estimator for the slope function. Yuan and Cai (2010) and Cai and Yuan (2012) studied minimax estimation and prediction based on RKHS approach. Shang and Cheng (2015) developed statistical inference for a generalized functional linear regression in the RKHS framework. Recent reviews of functional regression are referred to Morris, 2015, Wang et al., 2016 and Reiss et al. (2017).====In practice, one may collect additional finite-dimensional vector covariates. Partially functional linear regression (PFLR) was proposed first by Zhang et al. (2007) to study the relationship between a scalar response and “mixed data” including a vector and a function-valued random variable. Shin (2009) proposed an estimation procedure based on FPCA and studied its asymptotic theories. Shin and Lee (2012) studied the prediction problem based on FPCA and Tikhonov regularization, respectively. Yu et al. (2016) considered a linear hypothesis testing for the parameter component by extending the generalized likelihood ratio statistic (Fan et al., 2001) to PFLR. Lu et al. (2014) further considered estimation for partially functional linear quantile regression models. Li et al. (2018) studied PFLR with nonignorable missing responses.====Recently, functional regression with ==== functional covariates, an important case when two or more functional covariates are collected, has been received substantial attention. For example, Fan et al. (2015) and Lian (2013) studied variable selection of the multiple FLR. Luo and Qi (2017) proposed a signal compression approach for the multiple FLR with functional response. In this paper, we focus on PFLR with multiple functional covariates. To the best of our knowledge, only a few works have studied this model. For example, Goldsmith et al. (2011) fitted the generalized version of this model using a mixed effect model. Kong et al. (2016) considered variable selection for both functional and non-functional parts based on FPCA approach; Zhang et al. (2019) used wavelet-based sparse group lasso to select important functional predictors for partially functional linear quantile regression models with multiple functional covariates. But the estimation properties and statistical inference in this model have actually not been well studied in the literature. This paper aims to fill these gaps. We adopted the most popular dimension reduction procedure in functional data analysis, FPCA, for estimation as in the FLR literature (Dauxois et al., 1982, Yao et al., 2005, Hall and Hosseini-Nasab, 2006, Li and Hsing, 2010). The methods developed in this paper are generations of the methods in the FLR or PFLR literature. But these generations are not straightforward. We overcome several difficulties, especially in technical development. For instance, to estimate error in FPCA, we adopt the perturbation theory of linear operator (Hall and Horowitz, 2007, Imaizumi and Kato, 2018); to deal with the correlation between different functional covariates, we make Assumption (A4) and establish Lemma B.1 provided in Appendix B.====We make contributions to the PFLR literature in five folds. First, we derive the asymptotic distribution of the FPCA-based estimator for parameter component and show that this estimator is semiparametrically efficient in the sense of Bickel et al. (1993) under the Gaussian random error. Second, we prove that the FPCA-based estimator of the slope functions can achieve the same minimax optimal rate of convergence as those in Hall and Horowitz (2007). However, Lian (2013) and Kong et al. (2016) do not pursuit this result successfully. Third, we obtain the optimal rates of convergence for prediction problem as that in Shin and Lee (2012), that did not study the optimality of the rates. Fourth, the generalized likelihood ratio statistic (Fan et al., 2001) is proposed for testing the parameter component, which can be treated as the extension of Yu et al. (2016). Fifth, we extend the method proposed by Imaizumi and Kato (2019) to construct the confidence bands centered at the FPCA-based estimators for the slope functions, and present theoretical justifications.====The rest of the paper is organized as follows. Section 2 describes the FPCA-based estimation procedure, and presents the main assumptions and theoretical results. Section 3 obtains the optimal rate of convergence for prediction problem. Section 4 proposes a generalized likelihood ratio statistic for parametric component. Section 5 proposes confidence bands for the slope functions. Section 6 presents the results of finite sample simulations. Section 7 analyzes a data set from the diffusion tensor imaging study. The proofs of Theorem 1, Theorem 2 are given in Appendix, while the proofs of other Theorems and all technical lemmas are relegated to the Supplementary Material.",Estimation and inference in partially functional linear regression with multiple functional covariates,https://www.sciencedirect.com/science/article/pii/S0378375820300227,3 March 2020,2020,Research Article,207.0
"Cuevas Antonio,Fraiman Ricardo","Departamento de Matemáticas, Universidad Autónoma de Madrid, Spain,Centro de Matemática, Universidad de la República, Uruguay","Received 7 February 2019, Revised 20 February 2020, Accepted 20 February 2020, Available online 28 February 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.02.005,Cited by (0),"The problem under study is detecting the presence, and identifying the exact position, of a block ==== of “anomalous” observations in a finite sequence of independent random variables ====. By “anomalous” we mean that the distribution ==== of the observations in the block ==== is possibly different from the “under control” distribution ==== of the remaining ====’s. We first propose a nonparametric approach, based on classical goodness of fit (Kolmogorov–Smirnov (KS) and Cramer–von Mises (CvM)) statistics, for the case of real random variables ====. An application in stochastic geometry is outlined. Lastly, we focus on the case where the ==== are functional data, that is, trajectories of a stochastic process ====, ====. The strategy we follow in this case is to take the functional data to the real line with an appropriate transformation and then using the nonparametric detection/identification methodologies mentioned above. The real valued transformation we propose is the Radon–Nikodym derivative of the “under control” distribution (that assumed for most observations) with respect to another suitably chosen reference distribution.","The statistical problem of detecting “anomalous” observations arises in different settings, including sequential change point detection in statistical quality control, see Poor and Hadjiliadis (2009) and outlier detection (Aggarwal, 2017). We are concerned here with another classical instance of this general problem, namely detection and identification of heterogeneous observations which are assumed to appear according to some specific structure. This means that there exists a small subset of indexes ==== such that the observations ==== for ==== have a different distribution from that of the remaining data.====In more precise terms, the model we consider is as follows. Let ==== be a sequence of independent random elements of type ====where ==== is an unknown set of indices belonging to a given, known family ==== of subsets of ====, the ==== follow a distribution ==== and the distribution of the ==== is ====. We assume throughout ====, ==== for all ====, where ==== and ==== are known and ==== denotes the cardinality of ====. The “simplest” case, where the distribution ==== of ==== is known have been studied at some depth, especially when ==== is Gaussian and the distribution ==== of ==== might be unknown, but still Gaussian; see e.g., Addario-Berry et al. (2010) and references therein. See also Cai et al. (2012) for a more general statement of the model (1), which considers the possible presence of ==== exceptional blocks of observations (with ====), but still in the Gaussian case. We will address here this problem in a nonparametric framework, including the case of unknown ====.====In model (1), one could think that, while ==== represents the main distribution of the bulk of the data, ==== corresponds to the distribution of a “small” subset of indices which arises as a consequence of some exceptional phenomenon of interest.====In this setting, it is natural to address the problem of deciding whether the “exceptional” distribution ==== does really appears. In formal statistical terms this amounts to consider the following testing problem ====In what follows, (2) will be referred to as the ====.====If ==== is accepted in (2), it is also natural to consider the ==== of estimating the “exceptional set” ==== from the data ====.====To gain some intuitive insight, one might think that the “in control” distribution ==== corresponds to a background noise and the “exceptional distribution” ==== arises in a string of observations ==== carrying some signal. We want to detect the existence of such a signal and, when pertinent, to identify its precise location.====We now briefly comment some other references where these ideas have been considered. The list is non-exhaustive.====An early contribution, concerning the Rayleigh distribution, is due to Dobrushin (1958). Arias-Castro et al. (2005) consider signals which could be present in some subset ==== on a two-dimensional array of pixels ====. The observations made in all pixels are always affected by some noise ====. More precisely, the actual observations ==== recorded on the pixels follow a model of type ====. The aim is to test the presence of a set of pixels ==== for which the distribution of ==== is different from that of the noise ====. If such “signal set” ==== does exist, we also want to estimate ====. The set ==== is assumed to belong to a given class ====, subject to some restrictions.====Closely related models are considered in Arias-Castro et al. (2008), Addario-Berry et al. (2010) and Arias-Castro et al. (2011). In all these cases the “under control” distribution ==== in (1) is ==== and the “exceptional” distribution ==== is of type ====. The Gaussianity assumption allows these authors to obtain some very deep general results; see, e.g. the bounds for the detection problem provided in Addario-Berry et al. (2010).====In Jeng et al. (2010) the model (1) is also considered, assuming that the class ==== of exceptional indices is made of unions of ==== disjoint “intervals” (sets of consecutive indices). These authors describe the problem in terms of ====. They provide an interesting practical motivation in genomics in the analysis of DNA copy number variation. While they mostly focus on the Gaussian case, they explicitly point out that ====.====Some other previous approaches (Donoho and Jin, 2004), related to the basic idea behind model (1), deal with the Gaussian case, stating the problem in terms of ====, ====where ==== is a constant and ==== with ====. Such mixture models are closely related (but not formally equivalent) to the testing problem ====where ==== is the identity matrix and ==== is a vector whose entries are zero except for a small subset of indices ==== where the entries are equal to ====. The vector ==== is sparse in the sense that the cardinality of the set ==== with non-zero entries is ==== where, again, ==== with ====.====The mixture-type formulation is followed as well, for example, in Ingster (1997). A more general approach, that includes non-Gaussian situations, is considered in Cai and Wu (2014). These authors provide also interesting comments on the bibliography and practical applications of model (3) to different fields (signal processing, biostatistics, astrophysics, …).====First, in Section 2 we deal with model (1) in the usual univariate setting but we address the detection and identification problems in a nonparametric setup, where the distributions ==== and ==== are not assumed to be Gaussian or even known. The detection and identification tools we analyse there are based on the classical Kolmogorov–Smirnov and Cramer–von Mises statistics. The relevant theoretical results are given in Theorem 1, Theorem 2. An application in stochastic geometry (set estimation) is briefly outlined in Section 2.5. In Section 3 we suggest a mathematical framework to deal in practice with the functional case, that is, with those situations in which the data ==== are independent trajectories ====, ==== from a stochastic process. The basic idea is to take the functional data to the real line by using appropriate transformations, defined in terms of the Radon–Nikodym derivatives of the involved processes, then, the resulting (real) transformed data are treated with the methods proposed in Section 2. Such idea is formalized in Proposition 1.====Two small empirical studies are included at the end of Section 2 and in Section 4.",Nonparametric detection for univariate and functional data,https://www.sciencedirect.com/science/article/pii/S0378375820300203,28 February 2020,2020,Research Article,208.0
"Ghosh Sayan,Davidov Ori","Department of Statistics, University of Haifa, Mount Carmel, Haifa 3498838, Israel","Received 11 July 2019, Revised 12 February 2020, Accepted 13 February 2020, Available online 24 February 2020, Version of Record 8 May 2020.",https://doi.org/10.1016/j.jspi.2020.02.004,Cited by (1),"Paired comparison data is often used to rank or order a set of items. In this paper we study a method for estimating the parameters associated with completely ordered cardinal paired comparison data. The analysis is carried out within the framework of graphical linear models but rather than using the least ====, which may be difficult to analyze, we consider the average of all tree-based estimators for the connected comparison graph. The resulting estimator is a simple linear function of the sufficient statistics and has an easy to understand graph-theoretic interpretation. The statistical properties of this estimator are studied and it is shown to be unbiased, strongly consistent and asymptotically normal. Examples and numerical comparisons are provided and extensions are discussed.","There are many situations in which ranking of a set of items is desired. Examples include the evaluation of political candidates, sports, information retrieval, and a variety of modern internet and e-commerce applications. An ordering of a set of items can be inferred from different types of data including scores (Balinski and Lariki, 2010) and ranked lists (Marden, 1996). In particular, paired comparison data (PCD) is obtained if all comparisons involve only two items (David, 1988).====Suppose that there are ==== items labeled ==== which we would like to rank. Let ==== denote the outcome of the ====th comparison among items ==== and ====. The random variable (RV) ==== may be binary, ordinal or cardinal. There is a large literature on binary PCD for which the best known models are by Thurstone (1927), and Bradley and Terry (1952). A state of the art summary in this area is provided by Cattelan (2012). Some recent papers (Oliveira et al., 2018, Orbán-Mihálykó et al., 2019a, Orbán-Mihálykó et al., 2019b) provide various generalizations of the above mentioned models. Finally, González-Díaz et al. (2014) provide an axiomatic approach for binary, ordinal and cardinal PCD.====In this paper, we consider only cardinal, i.e., continuous PCD. We assume that the observations ==== for ==== and ==== satisfy ====where ==== and ==== are independent zero mean RVs. If we further assume that ==== are IID ==== RVs then (1.1) is a homoscedastic normal linear model. We refer to such models as linear models on graphs or more compactly graph-LMs. It is useful to note that we may view ==== as the outcome of the ====th “game” between team ==== and team ==== where team ==== scored ==== points while team ==== scored ==== points. Then naturally ==== and ==== and thus ==== so that ====. The most common structural assumption on the parameters ==== is that ====for all ====. We refer to the parameters ==== as merits or scores. Least squares estimators for model (1.1) assuming (1.2) have a long history and have been studied in diverse fields, cf., Mosteller (1951), Kwiesielewicz (1996) and Csató (2015), and the references therein.====Suppose there are ==== comparisons between item ==== and item ====. We do not assume that ==== for all or even most pairs ====. Each item is viewed as a vertex of a graph and the set of vertices is denoted by ====. If ====, then items ==== and ==== are connected by an edge denoted by the pair ====. Let ==== be the set of edges. The structure ==== is called a graph (Gould, 2012). With each edge ====, we associate a random sample ==== of ==== comparisons and denote the set of all samples by ====. We call the pair ==== a pairwise comparison graph (PCG). The function ====is a sum of squares over ==== where ====. The least squares estimator (LSE) is given by ====where ==== is fixed and prechosen. The constraint ==== is required for identifiability since ==== for any ====. It has been shown that when ==== is connected the LSE is unique and given by ====where ==== is the Moore–Penrose inverse of ====, the Laplacian of the comparison graph ====. The diagonal elements of ==== are ==== whereas the off diagonal ones are ====. Here ==== where ==== and ====. Under the assumption of normality, the LSEs in (1.5) are the maximum likelihood estimators (MLEs). Henceforth we shall assume, as commonly done, that ==== so the LSE (1.5) reduces to ====. This is a well known result, cf. Csató (2015) and the references therein.====Note that the elements of ==== cannot be written down explicitly. This makes the LSE difficult to analyze, interpret or express in terms of the number of pairwise comparisons ====-s. Hence, we study a graph based estimator (GBE) of ====. The GBE is a weighted function of the pairwise means ====. The weights have closed-form expressions and a clear graph-theoretic interpretation in terms of paths among vertices of the underlying graph ====. Some conditions are provided for equality of the GBE and the LSE. A thorough statistical analysis of the GBE is conducted. In particular, the GBE is shown to be unbiased, strongly consistent, and asymptotically normal. Examples illustrating the GBE are provided and simulation studies compare the performance of the GBE to that of the LSE.====The paper is organized in the following way. Section 2 provides some graph theoretic preliminaries. In Section 3 we introduce our GBE and in Section 4 we study its theoretical properties. Section 5 presents some numerical experiments comparing our GBE to the LSE. Section 6 provides a short summary and a brief discussion. In addition we show that the GBE studied here is a member of a much broader family of GBEs. The Supplementary Material contains various appendices for proofs along with additional examples and numerical experiments. All proofs are collected in Appendix A. Appendix B provides examples illustrating the concepts and results of Sections 2 Some graph theoretic preliminaries, 3 A GBE for linear models on graphs. Some numerical studies of the alternative estimators discussed in Section 6 are included in Appendix C.",Graph-based estimators for paired comparison data,https://www.sciencedirect.com/science/article/pii/S0378375820300197,24 February 2020,2020,Research Article,209.0
"Torabi Mahmoud,Jiang Jiming","Departments of Community Health Sciences and Statistics, University of Manitoba, Winnipeg, Canada,Department of Statistics, University of California, Davis, USA","Received 24 March 2018, Revised 15 July 2019, Accepted 4 February 2020, Available online 19 February 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jspi.2020.02.001,Cited by (5), (MSPE). We also provide ==== of MSPE of small area predictors using ,"Sample surveys are conducted with the purpose of providing reliable predictors for the finite population characteristics such as totals or means. Methods used in deriving such predictors (direct survey predictors) are based on total sample size. However in the past few decades, there have been increasing demand in using same sample survey data to get predictions for sub-populations, such as health regions or gender–age groups. Such sub-populations for which reliable predictions are needed are called small areas in the literature. The term small area refers to small sample size compared to population size in that area. The traditional area-specific direct predictors tend to have inadequate precision due to small sample sizes corresponding to population sizes for each small area. Since policy decisions about implementing specific projects to these small areas are made using predictions on underlying characteristics, survey researchers are developing methods to provide more reliable predictions for small areas. To this end, model-based estimators (Jiang and Lahiri, 2006, Rao and Molina, 2015, Jiang, 2017 Chap. 4) have been proposed to borrow strength from other related sources such as past surveys and census. For this purpose, mixed models are commonly used in small area estimation.====In particular, in the context of linear mixed models (LMMs), such small area models may be classified into two broad types: (i) Area-level models that relate small area direct estimates to area-specific covariates; such models are used if unit-level data are not available. (ii) Unit-level models that relate the unit values of a study variable to associated unit-level covariates with known area means and area-specific covariates. A comprehensive account of model-based small area estimation under area-level and unit-level models is given by Rao and Molina (2015). Among other approaches, parameters of the LMM can be estimated using either the maximum likelihood (ML) or restricted ML (REML). Although it is somewhat straightforward to predict the small area parameters under the LMM, e.g., using the best linear unbiased predictor (BLUP), obtaining its prediction error and associated prediction interval is difficult.====Fay and Herriot (1979, hereafter by FH) used an area-level model assuming independent small areas to predict small area parameters. Following this seminal work, many developments have been then made in small area estimation to predict small area characteristics (e.g., mean) and to obtain the corresponding mean squared prediction error (MSPE) estimation. There are many situations, however, that the small area parameters are related to their locations. For instance, it is an interest of policy makers (and public) to know the spatial pattern of a chronic disease (e.g., asthma) to identify small areas with high risk of disease for possible preventions. There is limited literature in small area estimation assuming small areas are spatially correlated.====Spatial models on the area specific effects are used when “neighboring” areas can be defined for each small area. Such models induce correlations among the small areas on geographical proximity for example in the context of estimating area-level mortality disease rates. Cressie (1991) used conditional auto-regressive (CAR) to account for the area-level spatial random effects (Besag, 1974) for small area estimation in the context of US census undercount. Petrucci and Salvati (2006) used simultaneous auto-regressive (SAR) to account for the area-level spatial random effects to estimate the amount of erosion delivered to streams in the Rathbun Lake Watershed in Iowa. Pratesi and Salvati (2008) used the same model to estimate the mean per capita income (PCI) in sub-regions of Tuscany using data from the 2001 Life Condition Survey from Tuscany.====In terms of spatial model parameters estimation, Cressie and Chan (1989) studied ML estimation of spatial model parameters for the spatial FH model. Since spatial FH models are special cases of general linear mixed model, the MSPE of the BLUP has been provided in the literature (Kackar and Harville, 1984). However, a rigorous second-order MSPE approximation for the spatial empirical BLUP (EBLUP) cannot be obtained from the general framework of Das et al. (2004) because of the presence of correlation among observations from different small areas.====Singh et al. (2005) also studied the spatial FH models in small area estimation. In particular, they considered the SAR to account for the spatial random effects and ML approach to estimate the model parameters. They heuristically derived the second-order MSPE of EBLUP of small area mean and obtained the corresponding second-order unbiased estimator of MSPE using Taylor expansion assuming that the number of small areas is finite. Petrucci and Salvati (2006) also used the MSPE estimators using Taylor expansion with model parameters estimated by REML or ML, respectively, in the case of SAR to account for the spatial random effects. Molina et al. (2009) considered a spatial FH model with SAR random effects and obtained the EBLUP of the small area mean and used bootstrap MSPE estimator for spatial EBLUP of small area means.====Singh et al. (2005) also used spatio-temporal FH models to develop EBLUP estimators to study the relative performance of spatial and spatio-temporal models on monthly data on per capita consumer expenditure from India. Marhuenda et al. (2013) also considered the same set-up as Molina et al. (2009) but for spatio-temporal FH model by assuming that area effects in the Rao–Yu model (Rao and Yu, 1994, Torabi and Shokoohi, 2012) follow a SAR to account for the spatial random effects. Schmid and Münnich (2014) extended the theory of robust EBLUP (Sinha and Rao, 2009) to spatial linear mixed models. Using Bayesian inference, Porter et al. (2014) used spatial FH model to analyze relative change of percent household Spanish-speaking in the U.S using direct estimators for the states (small areas) from the American Community Survey (ACS) and big data covariates from Google Trends searches over time as functional covariates. Porter et al. (2015) extended the FH model to the multivariate FH models with latent spatial dependence in the Bayesian framework. Chandra et al. (2015) extended the FH model that accounts for the SAR spatial non-stationarity where the parameters of regression model vary spatially. Baldermann et al. (2018) extended the work of Schmid and Münnich (2014) to the SAR spatial non-stationary model.====It is well-known that the conditional spatial dependence parameter defined through the SAR model can be inconsistent unlike the CAR model (page 340 Schabenberger and Gotway, 2004; Banerjee et al., 2014). In this paper, we introduce the spatial FH model by considering CAR to account for the spatial random effects (Banerjee et al., 2014) and use the generalized weighted least squared approach to estimate the regression coefficients and the REML to estimate the variance components. We also rigorously obtain the MSPE of EBLUP of small area means as well as the estimators of MSPEs of the EBLUP of small area means.====The rest of the paper is organized as follows. In Section 2, we introduce the general set-up for spatial linear mixed model and the BLUP of small area means and corresponding MSPE of the BLUP of small area means. In Section 3, we use the REML method to estimate the variance components to get the EBLUP of small area means. In Section 4, we provide asymptotic expression of the MSPE of EBLUP of small area means. The estimation of MSPE of EBLUP of small area means is considered in Section 5 using the Taylor expansion and parametric bootstrap approaches. We spell out the spatial linear mixed model theory for the special case of FH model (Section 6). In Section 7, performance of the proposed approach is evaluated using a real application of physician visits for Total Respiratory Morbidity conditions in Manitoba, Canada, during 2000–2010. We also evaluate our proposed approach using a simulation study in Section 8. Finally, concluding remarks are given in Section 9. Technical details and computer codes are provided as supplementary materials.",Estimation of mean squared prediction error of empirically spatial predictor of small area means under a linear mixed model,https://www.sciencedirect.com/science/article/pii/S0378375820300161,19 February 2020,2020,Research Article,210.0
"Geng Xin,Martins-Filho Carlos,Yao Feng","School of Finance, Nankai University, Tianjin 300350, PR China,Department of Economics, University of Colorado, Boulder, CO 80309-0256, USA,Department of Economics, West Virginia University, Morgantown, WV 26505, USA,School of Economics and Trade, Guangdong University of Foreign Studies, Guangzhou, Guangdong 510006, PR China","Received 24 October 2018, Revised 16 October 2019, Accepted 5 February 2020, Available online 17 February 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jspi.2020.02.002,Cited by (1),We propose kernel-based estimators for ,"The estimation of partially linear regression models has been the subject of a large literature since the seminal work of Heckman (1986) and Robinson (1988). See, e.g., Chamberlain (1992), Cuzick (1992), Linton (1995), Fan et al., 1998, Fan and Li, 1999 and Juhl and Xiao (2005) among many in this literature. A number of papers (Newey et al., 1999, Li and Wooldridge, 2000, Pinkse, 2000, Fan and Li, 2003, Manzan and Zerom, 2005, Su and Ullah, 2008, Yu et al., 2011, Martins-Filho and Yao, 2012) have considered the estimation and asymptotic efficiency gains that may result from knowledge that the nonparametric component of a partially linear regression model has a partial or fully additive structure. Generally speaking, this additivity can emerge directly from primitive assumptions (Li and Wooldridge, 2000, Manzan and Zerom, 2005) or from the specification of systems of regressions that lead to additivity of the regression of interest. A frequently occurring example of the latter case is econometric models that include “endogenous” covariates, where identification and estimation result from the specification of control functions (Newey et al., 1999, Pinkse, 2000, Su and Ullah, 2008, Martins-Filho and Yao, 2012, Ozabaci et al., 2014). Hence, consider the following partially linear triangular simultaneous equations model ==== where ==== is a scalar regressand, ==== is a subvector of ==== with ====, ==== and ==== are non-overlapping subvectors of ==== of dimensions ==== and ==== with ====, ====, ==== and ====. Note that by subtracting ==== from Eq. (1) and defining ==== we obtain ====In this paper, our primary goal is to propose and study the asymptotic properties of estimators for ==== the finite dimensional parameter ==== and the infinite dimensional parameter ==== in Eq. (3). A complicating factor in the estimation of this model is that some of the covariates appearing in the additive nonparametric component are not observed (====) and must be generated (estimated) by the auxiliary regression (2).====The asymptotic properties of a general class of estimators of the finite dimensional parameter of semiparametric models with generated covariates have been recently studied by Mammen et al. (2016). They establish consistency and ==== asymptotic normality of a parametric estimator that is obtained by a three-step estimation procedure. The first step involves nonparametric estimation (generation) of the unobserved covariates. The second step involves nonparametric estimation of a nuisance regression that may depend on the finite dimensional parameter (profiling), and the third step involves the minimization of a GMM-type objective function based on the first two steps. Although the class of semiparametric models they consider includes the partially linear additive nonparametric regression with generated regressors we study in this paper, their estimation procedure fails to incorporate the additivity that is present in our model. In contrast, the estimators we propose for ==== the finite and infinite dimensional parameters make full use of the additive structure of the nonparametric component. In this regard, our paper is more closely related to Newey et al. (1999) or Su and Ullah (2008), although the moment conditions we use in motivating and deriving our estimators are different from those used in these papers.====The moment conditions we use to motivate our estimators are similar, but critically different, from those employed by Manzan and Zerom (2005) to estimate a model that is identical to our Eq. (3) but where the covariates ==== are observed. Hence, the asymptotic normality we obtain for our estimator of the finite dimensional parameter can be viewed as an extension of their main result to the case of a partially linear regression model with generated regressors. In fact, as explained in Section 2 of this paper, we have found critical flaws in the proof of the main theorem in Manzan and Zerom (2005), casting doubt on the asymptotic normality of their estimator.==== ==== Thus, as a special case of our result we obtain the asymptotic normality of ==== estimator when ==== is observed. It also reveals that a proof for the main theorem in Manzan and Zerom (2005) may not exist for reasons that are related to those that prevented (Martins-Filho and Yao, 2012) from giving an asymptotic characterization for their estimator.====Our proposed estimators are kernel based and relatively easy to implement since they do not require any numerical optimization or iterated procedures. As will be shown, they are consistent and asymptotically normally distributed, with the estimator for the finite dimensional parameter converging at the parametric ==== rate, and the nonparametric estimator converging at the expected nonparametric rate that is a function of the rate of decay of the bandwidth and the dimensionality of the underlying regressions.====Newey et al. (1999) proposed series estimators (power and splines) for a model where ==== and the partially linear structure in (3) is generically modeled as ====.==== ==== Otherwise, their model is identical to ours. Given that our partially linear structure is a restriction on ====, their estimation method can be adapted to the model described by (1), (2) (see Section 6 of their paper). In Section 3 of this paper, we contrast the additional assumptions they make to characterize some of the asymptotic behavior of their estimators with those we make to obtain similar results. Martins-Filho and Yao (2012) proposed kernel-based estimators for ==== and ====, but although their estimators appear to have good finite-sample properties, they have failed to provide a characterization of their asymptotic behavior. In fact, our theoretical work suggests that their estimators cannot be shown to be asymptotically normally distributed under standard parametric and nonparametric normalizations (see details in Section 2).====Although the estimation procedure we consider is conceptually simple and fairly easy to implement, its asymptotic characterization is non-trivial, requiring repeated analysis of ====-Statistics of high degree. This has been greatly facilitated by results in Yao and Martins-Filho (2015), which are used frequently in our proofs. The ancillary results required to obtain our theorems are, to our knowledge, novel and can be used in other contexts where generated regressors are encountered in various types of two stage kernel-based estimators.====The rest of this paper is organized as follows. Section 2 describes the model in greater detail, considers identification and the moment conditions used in estimation, and provides a detailed algorithm for estimation. Section 3 gives asymptotic characterizations for our estimators and the assumptions we use to obtain our results. Where appropriate, we contrast our assumptions with those in Newey et al. (1999). Section 4 contains a small Monte Carlo study that sheds some light on the finite sample performance of our estimators and contrasts them to the series estimator proposed by Newey et al. (1999). Section 5 concludes. Supporting lemmas and the proofs of all theorems are given in Appendix A. An online appendix (OA) provides the proofs for the lemmas and details on the order of the ====-Statistics appearing in the proofs of the theorems.",Estimation of a partially linear additive model with generated covariates,https://www.sciencedirect.com/science/article/pii/S0378375820300173,17 February 2020,2020,Research Article,211.0
Maitra Trisha,"Interdisciplinary Statistical Research Unit, Indian Statistical Institute, 203, B. T. Road, Kolkata 700108, India","Received 13 November 2018, Revised 21 September 2019, Accepted 12 January 2020, Available online 3 February 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jspi.2020.01.007,Cited by (3),"s) in a random effects setup. Under the independent and identical (====s) of the population parameters of the random effects. In this article, respecting the increasing importance and versatility of normal mixtures and their ability to approximate any standard distribution, we consider the random effects having mixture of normal distributions and prove asymptotic results associated with the ====s in both independent and identical (====) and independent but not identical (non-====) situations. Besides, we consider ==== and non-==== setup with normal mixture distribution of the random effect parameters but considered only the ==== case and proved only weak consistency of the ","Data pertaining to inter-individual variability and intra-individual variability with respect to continuous time can be modeled through systems of stochastic differential equations (====s) consisting of random effects. In this regard, Delattre et al. (2013), Maitra and Bhattacharya (2015) and Maitra and Bhattacharya (2016) investigate asymptotic inference in the context of systems of ====s of the following form: ====where, for ====, the stochastic process ==== is assumed to be continuously observed on the time interval ==== with ==== known, and corresponding to the ====th process initial values ==== are also assumed to be known. Here ==== are random effect parameters independent of the Brownian motions ====. The above authors assume that ==== are independently and identically distributed (====) with common distribution ==== where ==== is a density with respect to a dominating measure ==== on ==== for all ====
 (==== is the real line and ==== is the dimension). Here the unknown parameter to be estimated is ====
 (====). In particular, the above authors assume that ==== is the Gaussian density with unknown means and covariance matrix, which are to be learned from the data and the model for classical inference (see Delattre et al. (2013), Maitra and Bhattacharya (2016)), and from a combination of the data, model and the prior for Bayesian inference (see Maitra and Bhattacharya (2015)). Statistically, the ====th process ==== corresponds to the ====th individual and the corresponding random effect is ====. The following conditions (see Delattre et al. (2013), Maitra and Bhattacharya (2015) and Maitra and Bhattacharya (2016)) that we assume ensure existence of solutions of (1.1):====Delattre et al. (2013) show that the likelihood, depending upon ====, admits a relatively simple form involving the following sufficient statistics: ====The exact likelihood is given by ====where ====For the Gaussian distribution of ==== with mean ==== and variance ====, that is, with ====, it is easy to obtain the following form of ==== (see Delattre et al. (2013)): ====where ====. As in Delattre et al. (2013), Maitra and Bhattacharya (2016) and Maitra and Bhattacharya (2015) here also we assume that====Delattre et al. (2013) consider ==== and ==== for ====, so that the setup boils down to the ==== situation, and investigate asymptotic properties of the ==== of ====, providing proofs of consistency and asymptotic normality. As an alternative, Maitra and Bhattacharya (2016) verify the regularity conditions of existing results in general setups provided in Schervish (1995) and Hoadley (1971) to prove asymptotic properties of the ==== in this ==== setup in both ==== and non-==== cases. Here, by the non-==== setup, we mean that the processes ==== are independent, but not identical, which ensues when we allow for unequal initial values ==== and unequal time points ====.====Interestingly, the alternative way of verification of existing general results allowed Maitra and Bhattacharya (2016) to come up with stronger results under weaker assumptions, compared to Delattre et al. (2013).====Maitra and Bhattacharya (2015), for the first time in the literature, established Bayesian asymptotic results ====-based random effects model, for both ==== and non-==== setups. Specifically, considering prior distributions ==== of ====, they established asymptotic properties of the corresponding posterior ====as the sample size ==== tends to infinity, through the verification of regularity conditions existing in Choi and Schervish (2007) and Schervish (1995).====In this article, we extend the asymptotic works of Maitra and Bhattacharya (2016) and Maitra and Bhattacharya (2015) assuming that the random effects are modeled by mixtures of normal distributions. The importance and generality of such mixture models are briefly discussed in Section 1.1.",On classical and Bayesian asymptotics in stochastic differential equations with random effects having mixture normal distributions,https://www.sciencedirect.com/science/article/pii/S0378375820300057,3 February 2020,2020,Research Article,212.0
"Wu Wenbo,Yin Xiangrong","One UTSA Circle, Department of Management Science and Statistics, University of Texas at San Antonio, San Antonio, TX 78249, United States of America,319 Multidisciplinary Science Building, Department of Statistics, University of Kentucky, Lexington, KY 40536, United States of America","Received 3 November 2018, Revised 11 January 2020, Accepted 18 January 2020, Available online 3 February 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jspi.2020.01.006,Cited by (0)," small ====” problem is proposed. We study the theoretical properties of the proposed pseudo estimator and variable selection procedure. Furthermore, we use an ensemble step to stabilize the pseudo estimation (variable selection) results. The advantages of the proposed method are demonstrated by both simulation studies and real data analyses.","In a high dimensional statistical analysis, singularity of the sample covariance matrix is often a problem. Suppose that ==== is a response variable and ==== is a ==== predictor vector, consider a linear regression model, ====where ==== is a ==== parameter vector and ==== is a random noise that is independent of ====. Based on an i.i.d. sample of size ====, the sample covariance matrix of the predictors, ====, is not stable/invertible, due to high correlations between the predictors or when ====, the “large ==== small ====” problem. In general, the development of statistical methods differs in cases of ==== and ====. While the study under ==== has a long history, the recent advancement in theory under ==== (Meinshausen and Bühlmann, 2006, Zhao and Yu, 2006) has led the statistical research to a new direction, see a review by Fan and Lv (2010). Regardless, dealing with an ill-conditioned sample covariance matrix remains to be a necessary and important step for the data analysis. When the sample covariance matrix is singular or near singular, the estimation of ==== becomes unstable and impossible sometimes. There is a rich literature on how to obtain an estimate of ==== when ==== is singular. One popular approach is the ridge estimator (Hoerl and Kennard, 1970), which is biased, but has a smaller mean squared error (MSE) in estimating ====. While many methods (Hoerl et al., 1975, Lawless and Wang, 1976, Dempster et al., 1977, Kibria, 2003) have been proposed to choose the tuning parameter for the ridge estimator, none of them seems to dominate the others consistently as ridge estimates are sensitive to the selected tuning parameter.====In this paper, we connect the ridge regression to measurement error regression based on an invariance law established by Li and Yin (2007). Such a connection provides an alternative but clear interpretation on how ridge regression is related to adding noises to the predictors. We call the estimator obtained by adding noises to the predictor a ====. Adding noises to the original sample is useful for different purposes (Bishop, 1995, Wager et al., 2013, Barber and Candés, 2015). Based on our knowledge, the existing literature mainly focuses on either establishing the connections to the regularization (Bishop, 1995, Wager et al., 2013) or improving existing penalized variable selection procedure (Barber and Candés, 2015, Zhang, 2017), but not on investigating the statistical properties. Focusing on model (1), we investigate the theoretical properties and numerical advantages of the pseudo estimator. Our contributions are as follows:====The rest of the paper is organized as follows. In Section 2, we introduce the concept of pseudo estimation and illustrate that the ridge type estimators belong to the family of pseudo estimators. We propose the pseudo estimation and variable selection for model (1) in Section 3. Theoretical properties of the pseudo estimator are studied in Section 4. In Section 5 we provide a rule of thumb for creating pseudo samples and in Section 6 we propose an ensemble approach to stabilize the pseudo estimation bypassing the choice of an optimal tuning parameter. We conduct simulations and analyze the prostate cancer data in Section 7. Further comments are arranged in Section 8. All proofs are delayed to the appendix in the supplementary file.",Pseudo estimation and variable selection in regression,https://www.sciencedirect.com/science/article/pii/S0378375820300045,3 February 2020,2020,Research Article,213.0
Proïa Frédéric,"Laboratoire angevin de recherche en mathématiques, LAREMA, UMR 6093, CNRS, UNIV Angers, SFR MathSTIC, 2 Bd Lavoisier, 49045 Angers Cedex 01, France","Received 7 May 2019, Revised 19 October 2019, Accepted 26 January 2020, Available online 1 February 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jspi.2020.01.009,Cited by (4)," satisfying ====. In that framework, we establish a moderate deviation principle for the empirical covariance only relying on the elements of ==== through ==== is singular, we also provide a compromise in the form of a moderate deviation principle for a penalized version of the estimator. Our proofs essentially rely on truncations and deviations of ====–dependent sequences, with an unbounded rate ====.",None,Moderate deviations in a class of stable but nearly unstable processes,https://www.sciencedirect.com/science/article/pii/S0378375820300070,1 February 2020,2020,Research Article,214.0
"Abraham Christophe,Grollemund Paul-Marie","MISTEA, Montpellier SupAgro, INRA, Univ Montpellier, France","Received 9 July 2019, Revised 17 December 2019, Accepted 5 January 2020, Available online 30 January 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jspi.2020.01.008,Cited by (0),"We address a Bayesian regression model with a functional covariate and a scalar response. The model is misspecified in two ways: the posterior distribution is calculated by using normal errors and, mostly, by restraining the functional regression coefficient to a possibly small subset of ====. A first general concentration result shows that the posterior distribution concentrates within Kullback–Leibler type neighborhoods of a set called the asymptotic carrier. This result is valid whether the asymptotic carrier is empty or not and includes the misspecified and the well specified cases. We focus on the particular misspecified case in which the functional regression parameter is restrained to a union of finite dimensional linear spaces. This restriction is motivated by the practical situation in which the functional regression parameter is expressed by a finite combination of B-splines with free knots. We provide sufficient conditions for the posterior concentration with respect to the norm of the parameter space along with a precise description of the asymptotic carrier.","Consider the regression model with functional covariate ==== and scalar response ==== defined by ====where ====, ====, ==== are independent and identically distributed random variables and ==== denotes the usual inner product of the Hilbert space ====
 ====We assume that ==== and ====. The functional covariates ==== are assumed to be non-random. With no loss of generality, their range has been fixed at ====. Model (1.1) will be referred to as the true distribution of ==== in the sequel.====It is common to write the unknown functional parameter ==== as a linear combination of vectors in a basis of ====, then to truncate the combination and estimate the scalar coefficients, possibly using a regularization technique. The number of vectors in the truncated basis typically increases with the sample size ==== in order to obtain a consistent estimate of ====. Although this strategy is consistent from a theoretical point of view, it may be unsatisfactory in practice as it may require an unnecessarily large truncated basis. As a typical example, assume that the basis is orthogonal and that ==== is simply the thousandth vector in the basis. Then, the estimator of ==== will remain orthogonal to ==== as long as the number of vectors of the truncated basis is less than 1000. For these reasons, some authors prefer to use a small flexible basis instead of large fixed ones. By flexible, we mean a basis ==== whose vectors ==== are free to change in order to capture the important part of ====. By small, we mean a basis with a fixed number of ==== vectors. Commonly, ==== is of the form ==== for some parameter ==== and a given function ====. A typical example for ==== is a B-spline with free knots ====. The order of the B-spline is usually fixed in order to obtain an estimator with a given degree of smoothness but a basis with B-splines with different orders can be also considered. Our early motivation for using a small flexible basis comes from the aim of providing an interpretable estimate of the coefficient function. Following James et al. (2009), Grollemund et al. (2019) propose a Bayesian model in which the coefficient function is a sparse step function, that is a function of the form ==== where ==== are intervals of ====. Both ==== and ==== are unknown and have to be estimated. Such a function ==== is sparse in the sense that ==== is included but not equal to ====. Assuming ==== is time, periods of time outside ==== have no impact on the response in (1.1); hence an interpretability of ====. The authors apply their method to predict the production of black truffles given the rainfall curves and estimate the periods of time for which rainfall does not have any impact on the production. In this study as in many practical situations, the true coefficient function ==== is probably not a sparse step function but it can still be of interest to approximate it by such a function for interpretability.====The posterior distribution for the parameter ==== will be derived from the full Bayesian model (1.2) ====where the variables ==== are conditionally independent given ====. Model (1.2) is misspecified in two ways. Firstly, it assumes that the error terms ==== of (1.1) are normally distributed. This can be untrue in our context as it is only assumed that the true distribution of ==== is centered with a finite variance ====. Secondly, the support of the prior distribution ==== may not contain the true value of the parameter ====. This is particularly the case when ==== is of the form ==== with ====-probability one, where ==== is fixed while ==== and ==== are random; ==== being restrained to a given (small) subset of ====. Obviously, the true functional coefficient ==== need not be of the same form as ==== as it is only assumed that ====. Therefore, if ==== is not in the support of ====, the posterior distribution cannot concentrate on the true parameter as ==== increases and it is of interest to know whether the posterior distribution still concentrates on a possible set. Following Berk (1966), such a set will be called the asymptotic carrier in the sequel.====Early (and most) studies on posterior consistency consider a well specified model and independent identically distributed (iid) observations; see Ghosh and Ramamoorthi (2003) and Choi and Ramamoorthi (2008) for an account of the theory. The independent but non identically distributed (inid) case which includes regression models with non-random covariates is studied in several papers as Amewou-Atisso et al., 2003, Choi and Schervish, 2007 and Xiang and Walker (2013). The covariates of these papers are finite dimensional. Misspecified models have received less attention although early studies date back to Berk (1966) and involve mainly the iid case as in Bunke and Milhaud, 1998, Kleijn and van der Vaart, 2006, De Blasi and Walker, 2013 and Ramamoorthi et al. (2015). The inid case is studied in Section 6 of the latter reference but their assumptions are not fulfilled in our context.====In the present paper, we address a regression model with functional covariates and independent and identically distributed errors. A first posterior concentration result is established under weak assumptions on the covariate sequence and under the usual assumption that the prior probability of neighborhoods of Kullback–Leibler type is positive. It is shown that the posterior distribution concentrates within Kullback–Leibler type neighborhoods. This result includes the misspecified and the well specified cases; the latter being simply a particular case of the former. The result is valid even if the asymptotic carrier ==== is empty. We focus on the misspecified case in which the functional regression parameter ==== is restrained to a union of ====-dimensional linear spaces (====) while no restriction is assumed for the true functional parameter. Although the functional regression parameter typically belongs to a parametric set, it is functional in essence as each coefficient ==== is associated to a varying basis function ====. A precise description of the asymptotic carrier ==== is provided as well as a sufficient condition for ==== to be non empty. This condition involves the sequence of covariates and the functional parameter set. Finally, it is shown that the posterior distribution under the misspecified model concentrates within any neighborhood of ====, the neighborhood being defined according to the norm of the parameter set.====Section 2 describes the assumptions on the covariate sequence and provides a general concentration result within Kullback–Leibler type neighborhoods. Section 3 is devoted to the misspecified case with the description of the asymptotic carrier and the posterior concentration with respect to the norm of the parameter set. A discussion is given in Section 4. Auxiliary results and proof can be found in the Appendix accessible as Supplementary material.",Posterior concentration for a misspecified Bayesian regression model with functional covariates,https://www.sciencedirect.com/science/article/pii/S0378375820300069,30 January 2020,2020,Research Article,215.0
"Zhang Jie,Wang Dehui,Yang Kai,Xu Yanju","School of Mathematics, Jilin University, Changchun 130012, China,School of Mathematics and Statistics, Changchun University of Technology, Changchun 130000, China","Received 7 April 2019, Revised 13 January 2020, Accepted 13 January 2020, Available online 24 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2020.01.005,Cited by (5), of the estimators are established. Some simulation studies are conducted to verify the proposed procedure. A real example is analyzed to illustrate the advantages of our model.,"There is growing interest in recent years on integer-valued time series with a great number of articles arising in the literature. It is worth mentioning that most of the literature on this topic focused on the analysis for time series of counts with an infinite range. The most widely used model is the first-order integer-valued autoregressive (INAR(1)) process proposed by Al-Osh and Alzaid (1987) and McKenzie (1985): ====which was based on the binomial thinning operator “====” proposed by Steutel and Van Harn (1979). The definition of thinning operator is: ====, where ==== is a sequence of independent and identically distributed (i.i.d.) Bernoulli random variables with distribution ====, i.e. ====. Some expansion can be found as: Weiß, 2008, Ristić et al., 2012, Pedeli and Karlis, 2013, Liu et al., 2016, Popović and Bakouch, 2018, Zhang et al., 2019.====In contrast, there is not much attention paid on the analysis of integer-valued time series with a finite range. In many situations, such as the weekly number of rainy day, instrument testing and medicine monitoring, the total amount of participation in the experiment is always fixed, while the amount of that in the two states is time-dependent. The origin of the application of this kind of data can be traced back to McKenzie (1985) who suggested to replace INAR(1) process by ====
 with ====, ==== for ==== and ====, which guarantees that ====. This process, referred as binomial AR(1) process, is a stationary Markov chain with binomial marginal distribution ====.====Binomial AR(1) has an intuitive and well-interpretable structure and can be applied to many kinds of realistic problems, such as the monitoring of computer pools (Weiß, 2009a, Weiß and Kim, 2013). With its development, tourist accommodation effects of festivals were modeled by Brännäs and Nordström (2006) to replace ==== with ====. Weiß (2009b) considered a new class of ==== order autoregressive models based on BAR(1) model. Weiß and Pollett (2014) introduced the density-dependent binomial AR(1) process with random coefficients. Möller et al. (2016) proposed the related self-exciting threshold BAR processes. Möller et al. (2018) considered the zero inflation model in count data time series with bounded support. Yang et al. (2018) discussed the threshold autoregressive analysis for finite-range time series of counts with an application on measles data. Extending to the bivariate processes, Scotto et al. (2014) proposed some types of bivariate binomial autoregressive models. He et al. (2016) studied a control scheme for autocorrelated bivariate binomial data. Recently, Ristić and Popović (2019) introduced a new bivariate binomial time series model whose both two components have the identical binomial distributions.====However, one important limitation of binomial AR(1) process is that each member can only have two states interpreted as Eq. (1) (i.e. patients ==== or not patients ====). It means that the binomial autoregressive process cannot well fit the data sets with more states, such as the data representing people in states of risk preference, risk neutrality or risk aversion. In addition, there are many kinds of problems such as the estimation of probability for people switching between homosexuality, heterosexuality, and bisexuality; the prediction of election results on votes in favor, against or abstaining during the presidential election; and some kinds of multi-state data sets in biostatistics. Many statisticians (Hausman and McFadden, 1984, Böckenholt, 1999a, Böckenholt, 1999b, Bhat, 2001, Dow and Endersby, 2004) have considered these kinds of problems via multinomial regression models. Based on the above discussion, we try to extend the binomial AR(1) model to a new multinomial autoregressive model with three states in the current paper.====The remainder of the paper is organized as follows: In Section 2, a finite-range multinomial autoregressive (F-MAR) process is introduced and some basic properties of the process are investigated. In Section 3, the methods for the estimates of parameters in the model are discussed and the related theoretical properties are studied. Then, the related simulation studies are presented in Section 4. In Section 5, the proposed model is applied to a real example, which demonstrates the usefulness and flexibility of the proposed model in fitting finite range data with three states. Some concluding remarks are provided in Section 6. All details of proofs for theorems are reported in Appendix.",A multinomial autoregressive model for finite-range time series of counts,https://www.sciencedirect.com/science/article/pii/S0378375820300033,24 January 2020,2020,Research Article,216.0
"Wang Weiwei,Wu Xianyi,Zhao Xiaobing,Zhou Xian","School of Statistics and Mathematics, Zhejiang Gongshang University, Hangzhou, Zhejiang Province, China,School of Statistics, East China Normal University, Shanghai, China,School of Data Sciences, Zhejiang University of Finance and Economics, Hangzhou, Zhejiang Province, China,Department of Actuarial Studies and Data Analytics, Macquarie University, NSW, Australia","Received 10 April 2019, Revised 16 December 2019, Accepted 24 December 2019, Available online 21 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.12.005,Cited by (2),"Panel count data frequently arise in such areas as medical research and reliability studies, and various estimation methods have been developed for analyzing this type of data. In the literature, however, there are few methods incorporating the correlation within subjects. In this paper, on the basis of quadratic inference functions, we apply the ==== to analyze panel count data with time-varying coefficients. The proposed procedure can easily take into account the correlation within subjects and yields more efficient estimators even if the working correlation is misspecified. An efficient ====. Simulation studies are carried out to evaluate the finite-sample behavior of the method and to compare the estimation efficiency. Finally, an application of the method is demonstrated by re-analyzing a dataset from a bladder tumor study.","Panel count data often occur in longitudinal follow-up studies, such as medical research, reliability studies and tumorigenicity experiences. This type of data are collected at discrete time points in recurrent events process. Thus, only the occurrence numbers of the events between subsequent observation times are available and the exact occurrence times of the events are unknown. The number of the observations and their observation times may vary from subject to subject, so that the data obtained in the process are unbalanced.====Various methods have been developed for the analysis of the panel count data. For example, Sun and Kalbfleisch (1995) and Wellner and Zhang (2000) studied nonparametric estimation of the mean function. Sun and Wei, 2000, Zhang, 2002 and Wellner and Zhang (2007) considered the regression analysis of panel count data. Lu et al., 2007, Lu et al., 2009 studied the spline-based sieve version of MPLE and MLE by approximating the baseline mean function using monotone B-spline functions. He et al., 2009, Zhao and Tong, 2011 and Zhao et al. (2013) developed some joint models for panel count data. Li et al., 2010, Li, 2011 and Ni et al. (2013) proposed some semiparametric transformation models. The above approaches in modeling panel count data are based on the assumption that the regression coefficients are constant over time. In practice, however, such an assumption is often violated, the regression coefficients may vary over time, and sometimes it may be of a primary interest to know the temporal effects of the covariates. For example, in medical studies, we are interested in the temporal effects of a new drug over time. Time-varying coefficient models can provide a nice graphical summary of dynamics of covariates. Recently, He et al. (2016) proposed a partially varying coefficients model of panel count data, with the nonlinear interactions between covariates, Zhao et al. (2018) proposed a nonparametric time-varying coefficient model for panel count data. Wang and Yu (2019) proposed to use local kernel regression method for estimation panel count model with time-varying coefficients. More comprehensive introductions of panel count data can be found in the book of Sun and Zhao (2013). These authors developed various estimation methods for panel count data, but they have not discussed how to incorporate the information on the correlation structure within subjects into their estimation methods. Hua and Zhang (2012) proposed the spline-based semiparametric projected generalized estimating equation (GEE) method for panel count data. The application of GEE, however, is restricted by the requirement that the correlation within subjects is correctly specified. If the correlation is misspecified, the estimation efficiency of GEE is lost. In addition, the GEE method is very sensitive to outliers or contaminated data (Qu and Song, 2004, Song, 2007). Yao et al. (2016) developed a maximum likelihood approach for analyzing panel count data under the gamma frailty non-homogeneous Poisson process model. The approach allows one to estimate the baseline mean function and the regression parameters jointly while taking the within-subject correlation into account, but the assumption of the gamma frailty non-homogeneous Poisson process may be violated in practice.====To overcome these shortcomings of GEE, the quadratic inference functions (QIF) approach, which was proposed by Qu et al. (2000), has recently received considerable attentions. Qu and Li (2006) proposed an efficient estimation procedure under varying-coefficient models for longitudinal data by QIF. Bai et al. (2008) utilized QIF in the estimating equations for the longitudinal partial linear models. Xue et al. (2010) considered the generalized additive model for correlated data. Lai et al. (2013) developed QIF for partially linear single-index models with longitudinal data. Wang et al. (2014) derived the QIF based estimators for generalized additive partial linear models with diverging number of covariates. Ma et al. (2014) employed the QIF together with profile principle to derive the estimation for partially linear single index models. Zhao et al. (2017) investigated composite quantile regression estimation for linear regression models on the basis of QIF.====Quantile regression is a good alternative to the conditional mean models and has been extensively used in the analysis of longitudinal data. Fitting data at a set of quantiles provides a more comprehensive description of the response distribution than does the mean. This approach, however, is lagging in the theory and methodology for the panel count data. Due to the discreteness of the panel count data, quantile regression cannot be directly used. Instead, a smoothing technique (“jittering”) can be applied to smooth panel count data and then the quantile regression can be used on the smoothed data.====Motivated by the advantages of the QIF and quantile regression, we investigate in this paper the performance of the estimators for time-varying coefficient panel count data model by combining QIF and quantile regression. The unknown baseline function and coefficient functions are approximated by the spline functions, and the “jittering” smoothing technique is adopted to smooth the discrete panel count data. Then, the quantile regression method is developed for the model inference on the basis of quadratic inference functions. Thus the correlation within subjects is taken into account in our estimation procedure.====The rest of the paper is organized as follows: Section 2 introduces some notations and the time-varying coefficient panel count data model. In Section 3, quantile regression combined with the QIF method is proposed to estimate the panel count data model. Asymptotic properties of the estimators based on the proposed method are provided in Section 4. Some simulation results are reported in Section 5 to confirm and assess the finite-sample behaviors of the method. Section 6 illustrates the method by re-analyzing a set of bladder tumor data. Finally, some concluding remarks are provided in Section 7.",Quantile regression for panel count data based on quadratic inference functions,https://www.sciencedirect.com/science/article/pii/S0378375819301247,21 January 2020,2020,Research Article,217.0
"Jiang Peiyun,Kurozumi Eiji","Department of Economics, Hitotsubashi University, Japan","Received 28 October 2019, Revised 30 December 2019, Accepted 8 January 2020, Available online 20 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2020.01.004,Cited by (4)," (2016). The simulation and empirical results indicate that although neither procedure is uniformly superior to the other, the CUSUM test is more suitable for an early break.","Structural change has long been an important issue in the statistics and econometrics literature. Much research effort has been devoted to testing for parameter instability and estimating the change points given a fixed size dataset. Such an approach in a given sample is called a retrospective or a posteriori test. However, a retrospective test cannot be applied to detect parameter changes every time new data become available because multiple testing with a given critical value will result in an uncontrollable empirical size of the test. On the contrary, to sequentially detect structural changes, a sequential or priori test is designed such that the model can be estimated from historical and new data and an online decision then made.====The first contribution to continuously monitoring parameter changes in the econometrics literature was by Chu et al. (1996). They introduced a monitoring scheme by setting a training period of size ==== in which the parameters are known to be stable as a reference for comparison with new data and argued that the key feature of the sequential tests is to construct a nondecreasing boundary function such that the tests can maintain a proper size. This approach has been developed in many directions. Leisch et al. (2000) extended the fluctuation test of Chu et al. (1996) based on moving estimates, with the boundary function having a slower growth rate to improve the sensitivity to a late break in a monitoring period. The MOSUM (moving sum) procedure was further investigated by Horváth et al. (2008), who indicated that prior information on the moment structure of innovations is required to choose a suitable boundary function. Horváth et al. (2004) discussed two classes of the residual-based cumulative sum (CUSUM) monitoring procedure with an infinite monitoring horizon and introduced an appropriate boundary function with the parameter ==== to deal with different timings of changes. Since the speed of detection is a crucial measure, Aue and Horváth (2004) derived the limit distribution of the stopping time for a changing mean model, which is asymptotically normal, while Aue et al. (2009) extended a local-level model to a linear regression model. They found that ==== close to ==== implies a shorter detection delay for an early break. Horváth et al. (2007), Aue et al. (2008b), and Aue and Kühn (2008) further investigated the behaviors of the delay time in the case of ====. Following the work of Aue and Horváth (2004), Fremdt, 2014, Fremdt, 2015 derived the asymptotic distribution of Page’s sequential CUSUM procedure and compared the asymptotic normality of the stopping time with that of the ordinary CUSUM version under a weaker condition on the change. Furthermore, the monitoring procedure for sequentially detecting parameter instability has been investigated extensively in various models. For example, Carsoule and Franses (2003) and Lee et al. (2009) developed sequential tests in autoregressive models, while Na et al. (2011) applied the monitoring procedure to detect changes for autocorrelation function, parameter instability in GARCH models, and distributional changes. Xia et al. (2011) and Kurozumi (2017) considered a monitoring scheme for linear models with endogenous regressors.====All the aforementioned sequential tests focus on models with nontrending regressors. However, as noted by Perron (1989) and others, macroeconomic time series are sometimes better characterized by trend-stationary series with possible change(s) in deterministics. Such evidence with an upward or downward trend has also been found in the fields of tourism, marketing, and environmental studies. For models with trending regressors, Chu and White (1992), Kuan (1998), and Aue et al. (2008a) among others proposed tests of parameter instability based on a given historical sample, while Qi et al. (2016) extended the generalized fluctuation test to monitor structural changes in polynomial regressions.====In this study, we develop a CUSUM-type monitoring scheme based on ordinary least squares (OLS) residuals to detect parameter instability in a model with a trend. A new boundary function is introduced to maintain a proper size. We derive the limit distribution of the CUSUM detecting statistic under the null hypothesis, while proving that the test is consistent under the alternative. Moreover, we investigate the asymptotic distribution of the delay time for the CUSUM (OLS-based) test as well as the fluctuation one proposed by Qi et al. (2016) in a model with an early change. We find that the delay time of the CUSUM test grows at a slower rate than that of the fluctuation test, which implies that the latter requires a longer time to detect an early change than the former. We also extend the CUSUM monitoring procedure to models with higher order polynomial trends. Then, we compare the CUSUM and fluctuation tests in a small simulation study and apply them to macroeconomic time series. The results confirm that the performance of the tests strongly depends on the timing of changes. The CUSUM test is good at detecting an early change soon after the training period and has a shorter detection time than the fluctuation test, while the fluctuation test is suitable for a late break.====The remainder of the paper is as follows. In Section 2, we introduce the model and our assumptions. The asymptotic properties are investigated in Section 3. We extend the CUSUM monitoring procedure to models with higher order polynomial trends in Section 4. Then, we compare the CUSUM and fluctuation tests in finite samples via Monte Carlo simulations in Section 5. Section 6 provides an empirical example and concluding remarks are given in Section 7. The mathematical proofs are relegated to Appendix.",Monitoring parameter changes in models with a trend,https://www.sciencedirect.com/science/article/pii/S0378375820300021,20 January 2020,2020,Research Article,218.0
"Zhou Yang,Chen Di-Rong","School of Mathematics and System Science, Beihang University, Beijing 100191, PR China","Received 14 April 2019, Revised 17 December 2019, Accepted 3 January 2020, Available online 16 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2020.01.003,Cited by (2),"Functional ==== (FCCA) has been applied in many contexts, but the ==== have not yet been studied enough. In this paper we consider a general setup resembling that of Eubank and Hsing (2008). We focus on the convergence of estimates of the associated canonical functions to their population counterparts. A regularized estimator is proposed based on the Tikhinov regularized approach. Under some conditions, an upper bound is derived for this estimator and furthermore, a sharp lower bound is established. Consequently, the ==== optimal rate is obtained and depends on the level of dependence between the two stochastic processes. A simple simulation is performed to illustrate our methods.","Functional canonical correlation analysis (FCCA), as an extension of classical concepts of canonical correlation analysis (CCA; Hotelling, 1936) to the infinite-dimensional functional domain, has been developed in recent years. Specifically, let ==== and ==== be two second-order stochastic processes taking values in ==== and ====, respectively, where for ====, ==== are compact metric spaces and ==== are Hilbert spaces of square integrable functions on ==== with the usual inner product ====. For identifying and quantifying the associations between ==== and ====, FCCA finds two random variables ==== and ==== associated with ==== and ====, respectively, to maximize their correlation ====.====One way is to regard ==== as the projection of ==== on a square integrable function ====, that is ====, and then the FCCA problem is transformed to find two weight functions ==== and ====, respectively (more details see He et al. (2002)). We refer to this setting HMW. The maximization problem, in HMW, is characterized through the singular-value decomposition of the so-called cross-correlation operator ====where ==== are the covariance operators for ==== and ==== is the cross-covariance operator. However, the covariance operators ==== and ==== are not generally invertible, which leads to the non-existences of weight functions. To circumvent this problem, He et al. (2002) imposed stronger dependence between ==== and ==== (see condition 4.5 in He et al. (2002)) to ensure the existence of weight functions.====An alternative tactic, proposed by Eubank and Hsing (2008) (we refer to this setting EH), is to seek random variables ==== and ==== in the Hilbert spaces spanned by ==== and ====, denoted by ==== and ====, respectively. Then on the basis of the Loève-Parzen classical isometry (Parzen, 1961), the FCCA problem is transformed to find functions, called canonical functions, in the reproducing kernel Hilbert spaces (RKHS) generated by their covariance kernels. Kupresanin et al. (2010) claims that EH is a more complete definition than HMW, by showing that the set of random variables ==== is a dense subspace of ====. As mentioned in Kupresanin et al. (2010), EH provides a viable setting for development of FCCA and a rigorous solution to the FCCA problem can’t be formulated without the use of the RKHS congruences. On the other aspects, Shin (2008) applied this congruence to functional Fisher’s discriminant analysis. Recently, using the theory of RKHS in functional classification, Berrendero et al. (2018) provided explicit expressions for the optimal rule and the minimal classification error probability.====EH has an advantage over HMW, but not much is known about its theoretical properties. In this paper, we focus on minimax convergence rates of the canonical functions in EH. The one-to-one correspondence between the canonical functions and canonical variables indicates that the estimation of the canonical functions is a key step for estimating the canonical correlations and variables. So it is of interest to understand how well canonical functions can be estimated in the minimax sense as a benchmark of estimation performance. Another motivation is that the canonical functions play a similar role with the weight functions that are the main goals in HMW. Lian (2014) has derived convergence rates of the weight functions and established the minimax rates in ====-norm and the prediction risk. However, these optimal rates rely on the assumption of condition 4.5 therein. In contrast, the canonical functions always exist without this assumption. So it is nontrivial to consider minimax convergence rate of the canonical functions in EH.====The motivating examples on FCCA can be referred to early work (Hannan, 1961, Dauxois and Nkiet, 1997, Tsay and Tiao, 1985, Tiao and Tsay, 1989, Jewell and Bloomfield, 1988, Leurgans et al., 1993). Cupidon et al. (2008) suggested using regularized FCCA even in the population to avoid the existence problem and Cupidon et al. (2007) derived the asymptotic distributions of estimators for the regularized functional canonical correlation and variables with the help of delta method. On the computational aspects, a sample version of smoothed functional canonical correlation was defined by Leurgans et al. (1993), who demonstrated the necessity for regularization in FCCA. He et al. (2004) proposed several computational methods for dense functional data. Under the linear mixed-effects models, Shin and Lee (2015) developed a method for irregularly and sparsely functional data.====The remainder of the paper is organized as follows. In Section 2, we review the FCCA problem in EH and discuss the special case of the processes possessing finite dimensional expansions. In Section 3, we propose a regularized estimator for the canonical functions. In Section 4, the minimax rate of the canonical functions in RKHS norm is established and it depends on the dependence between ==== and ====. A simulation example is presented in Section 5.",The optimal rate of canonical correlation analysis for stochastic processes,https://www.sciencedirect.com/science/article/pii/S037837582030001X,16 January 2020,2020,Research Article,219.0
"Delevoye Angèle,Sävje Fredrik","Department of Political Science, Yale University, United States of America,Department of Statistics and Data Science, Yale University, United States of America","Received 17 July 2019, Revised 2 October 2019, Accepted 17 December 2019, Available online 15 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.12.002,Cited by (5),"We extend current concentration results for the Horvitz–Thompson estimator in finite population settings. The estimator is demonstrated to converge in quadratic mean to its target under weaker and more general conditions than previously known. Specifically, we do not require that the variables of interest nor the normalized ==== are bounded. Rates of convergence are provided.","The quantity of interest is the average of some characteristic ==== in a finite population of ==== units indexed by ====: ====We observe ==== only for a subset ==== of the units, and the task is to estimate ==== based on this information.====Narain (1951) and Horvitz and Thompson (1952) provide an estimator of ==== when the subset of observed units is random. The estimator is conventionally named after the second set of authors, and we will not depart from that convention. At the core of the Horvitz–Thompson estimator is the probability distribution of ==== over the power set of the unit indices, which is said to be the design of the study. Given a design, the estimator is ====where ==== is the inclusion probability for unit ====. These probabilities are taken to be known in this note, but they may sometimes be estimated. Examples of such settings include estimation of response propensities for unit non-response in survey sampling and estimation of propensity scores for unknown assignment mechanisms in causal inference.====The application of the estimator to questions in survey sampling is immediate. The characteristics are survey responses, and ==== collects the sampled units. Its application to causal questions is also straightforward. The characteristics are in this case potential outcomes given by treatments assigned to the units (Neyman, 1990/1923, Holland, 1986). For example, if ==== denotes unit ====’s outcome when assigned to an active treatment and ==== denotes the outcome when assigned to a control treatment, the average treatment effect can be written as ==== where ==== and ==== are the population averages of ==== and ====. The inferential challenge is that no more than one potential outcome is observed for any of the units. The other outcomes are counterfactual, and at least one (but generally both) of ==== and ==== is unobserved even when the complete population is sampled. In this case, ==== collects all sampled units assigned to a certain treatment condition, and the Horvitz–Thompson estimator provides an estimate of the average of the corresponding potential outcome. The contrast between two such estimators is an estimate of the average treatment effect.====The purpose of this note is to investigate some of the asymptotic properties of the Horvitz–Thompson estimator. Our particular focus is to extend current concentration results for general designs. We show that the estimator is consistent under weaker and more general conditions than previously known. The estimator has inspired a large class of estimators providing improvements in various directions. In an appendix, we show that our results extend to some of these improved estimators as well.",Consistency of the Horvitz–Thompson estimator under general sampling and experimental designs,https://www.sciencedirect.com/science/article/pii/S0378375819301211,15 January 2020,2020,Research Article,220.0
"Li Wei,Yang Shu,Han Peisong","Department of Mathematics, Syracuse University, Syracuse, NY 13244, USA,Department of Statistics, North Carolina State University, Raleigh, NC 27695, USA,Department of Biostatistics, University of Michigan, Ann Arbor, MI 48109, USA","Received 4 July 2018, Revised 10 July 2019, Accepted 2 January 2020, Available online 13 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2020.01.001,Cited by (4),"We consider estimation for parameters defined through moment conditions when data are missing not at random. The ==== is correctly modeled, we show that if any one of the multiple models for the ==== is correctly specified, the proposed estimator is consistent for the true value. A simulation study confirms that our estimator has multiple robustness when the outcome data is missing not at random. The method is also applied to an application.","Missing data analyses have received much attention in statistics. Data are missing at random (Rubin, 1976) if the missingness depends on the observed but not on the missing values; whereas data are missing not at random if the missingness depends on both the observed and missing values. Under missingness not at random, the full data distribution is not identifiable without further assumptions. Researchers have considered parametric assumptions, such as pattern-mixture models (Little, 1993) and sample selection models (Heckman, 1979) and assumptions based on instrumental variables (Tang et al., 2003, D’Haultfoeuille, 2010, Wang et al., 2014) or shadow variables (Miao and Tchetgen Tchetgen, 2016, Kott and Liao, 2017).====Under a fully parametric model, identification of parameters can be achieved by sufficiently stringent modeling restrictions, which then invokes either the likelihood or Bayesian method; however, fully parametric approaches are sensitive to model misspecification. Researchers have also developed semiparametric methods for which one of the outcome model and the missing data mechanism model is parametric and the other is nonparametric. Among these, Tang et al. (2003) and Zhao and Shao (2015) proposed maximum pseudo likelihood estimators without modeling the nonresponse mechanism, and D’Haultfoeuille (2010) considered a regression analysis using a nonparametric nonresponse model. Scharfstein et al. (1999) and Shao and Wang (2016) proposed semiparametric nonresponse models and inverse probability weighted estimation. Qin et al. (2002), Kim and Yu (2011), Tang et al. (2014), and Zhao et al. (2017) used semiparametric or empirical likelihood approaches with parametric assumptions on the missing data mechanism.====In general, the missingness mechanism cannot be determined from the data alone, and inference under missingness not at random may be sensitive to unverifiable assumptions about the missingness mechanism. For identification, one common approach is to choose among the covariates a valid nonresponse instrument, a variable that is related to the outcome and can be excluded from the nonresponse model when the outcome and other covariates are included. The selection of a valid nonresponse instrument can be challenging. To address this issue, researchers have developed sensitivity analysis methods (e.g. Robins et al., 2000). Although widely used in practice, sensitivity analysis cannot provide point identification of parameter of interest.====In this article, we focus on estimation for general parameters defined through moment conditions and develop a robust estimation method with multiple working models for the response mechanisms. These response models can be based on different assumptions of the mechanism. For example, models for both missingness at random and missingness not at random with different identification conditions can be simultaneously considered. With multiple nonresponse models, we consider weights on the complete cases derived based on a set of calibration constraints. Construction of these calibration constraints is essential. Under missingness at random, calibration constraints imposed only on covariates are sufficient to eliminate selection bias after reweighting the complete cases (Han, 2014). However, this is not clear in that paper whether a similar approach can be applied under missingness not at random. This paper fills in this gap by proposing to construct calibration constraints directly on the score equations for the parameter of interest under multiple working models. Using such calibration weights, the proposed estimator is multiply robust, in the sense that, under a correct specification of the conditional distribution of the response given covariates, it is consistent if any one of the multiple response models is correctly specified. Such a robustness property renders the estimator more protection against misspecification of the response model. The multiple robustness has been studied in Han and Wang, 2013, Chan and Yam, 2014 and Han (2014), under missingness at random and in Han (2017), for the mean of outcome under missingness not at random. Our contribution is to develop multiple robust estimators for general parameters defined through moment conditions with multiple response models under missingness not at random.====The multiple-robustness property discussed in this article is different from that in some other articles. Consider a classical setting where likelihood function can be factorized into non-response and outcome mechanisms, each specified with a working model. The well-known “Double robustness” refers to the consistency property that allows misspecification of either one of the two models. Some articles generalize this classical concept in a likelihood model with multiple components. For instance, Molina et al. (2017) studied factorized likelihood models, where both nonresponse and outcome mechanisms can be further factorized into several components. A working model is specified for each of these components. Multiply robust estimators, according to their definition, are those that are consistent when some (not necessarily all) of these models are correct. In other words, their multiple robustness offers protection against misspecification of two or more than two components of the likelihood function. Wang and Tchetgen Tchetgen (2018) interpreted multiple robustness in a similar way. In contrast, we specify multiple models for the nonresponse mechanism. The multiple robustness discussed here refers to the consistency property that requires correct specification of only one of multiple models for the nonresponse mechanism.====The rest of this article is organized as follows. In Section 2, we introduce the setup, discuss assumptions on the response mechanism, and provide estimation methods. In Sections 3 Multiple robust estimation, 4 Main result, we derive the proposed multiply robust estimator for general parameters under multiple response models and its consistency. In Section 5, we evaluate the finite sample performance of the proposed estimators via simulations. In Section 6, we apply the method to an application. We then end with a brief discussion in Section 7.",Robust estimation for moment condition models with data missing not at random,https://www.sciencedirect.com/science/article/pii/S0378375818301204,13 January 2020,2020,Research Article,221.0
"Wu Tung-Lung,Li Ping","Department of Mathematics and Statistics, Mississippi State University, Starkville, MS 39759, USA,Cognitive Computing Lab, Baidu Research, Bellevue, WA 98004, USA","Received 18 September 2018, Revised 18 November 2019, Accepted 18 November 2019, Available online 9 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.11.003,Cited by (3),The classic ,"One-sample and two-sample testing problems for high-dimensional covariance matrices are considered in this paper. In high-dimensional settings, conventional methods such as the likelihood ratio test fail usually due to the singularity of sample covariance matrices if the sample size is larger than the data dimension. Consider the one-sample test and let ==== be a random sample of size ==== and follow a ====-dimensional normal distribution ====. We want to test ====where ==== is a ==== identity matrix. Note that for a given covariance matrix ==== we can always test (1) based on the transformed data ====. The conventional likelihood ratio test statistic for (1) is given by ====where ==== is the sample covariance matrix and ==== denotes the trace of ====. The likelihood ratio test performs poorly when ==== increases as ==== tends to infinity. It has been shown numerically that the size of the test based on (2) is 100% in the case ==== (see Bai et al. (2009)). Further, the test statistic is undefined when ==== due to the singularity of the sample covariance matrix. Bai et al. (2009) proposed a corrected likelihood ratio test (CLRT) with a condition ====. They established the asymptotic normality result for a corrected version of ==== using random matrix theory. Some related works on CLRT can be found in papers of Jiang et al. (2012) and Jiang and Yang (2013).====The two-sample test has the same issue as the one-sample test. Let ==== follow a ====-dimensional normal distribution ==== and ==== follow a ====-dimensional normal distribution ====. We want to test ====The likelihood ratio test statistic ====is undefined when ==== with ====, where ==== and ==== are the sample covariance matrices of ==== and ====, respectively, and ====, ====.====There is a vast literature on the high-dimensional testing problems in recent years. Existing methods may be broken into three approaches. The first approach derives the limiting distribution of extreme eigenvalues of the sample covariance matrix based on random matrix theory (see Bai (1993) and Bai and Yin (1993)). In the second approach, researchers aim to develop accurate estimators of the covariance matrices. In the one-sample test, Li and Xue (2015) proposed a combination of both quadric form statistic and extreme value form statistic for dense and sparse alternatives and proved that the two statistics are asymptotically independent. Schott (2007) and Li and Chen (2012) constructed better consistent and unbiased estimators for ====. The third approach is the use of random projection for testing two-sample means in high-dimension (see Lopes et al. (2012) and Srivastava et al. (2016)). Other recent advances can be found in Jiang et al. (2012), Liu et al. (2017), Ishii et al. (2016), Srivastava and Yanagihara (2010), Cai et al. (2014) and Chang et al. (2017).====In this paper, we study the random projection method with focus on projecting the data onto only one-dimensional subspace so that any conventional one-dimensional test is readily available to be used for the projected data. High-dimensional tests have gained popularity in many areas, including gene expression data analysis. People analyzing gene expression data do not necessarily have strong background in statistics. In part, this motivates us to develop a simple and easy-to-interpret procedure for high-dimensional data analysis for practitioners. Also, more insights are gained through one-dimensional projections as illustrated in Section 4. Surprisingly, the performance of one-dimensional random projection method turns out to be quite remarkable. This lies on the foundation of the random projection method in a lemma of Johnson and Lindenstrauss (1984) where the lemma states that the distances between projected data points are approximately preserved. The reason for adopting the method is threefold: (i) conceptually simple, (ii) easy to program and (iii) efficient in computation. Compared to the works of Lopes et al. (2012) and Srivastava et al. (2016), we apply the random projection method to high-dimensional covariance matrices, and our contributions are to understand the driving force of the one-dimensional projection and to increase the power through the extremal types theorem.====This paper consists of two parts. The first part deals with a class of matrices where the difference between two matrices is either positive or negative definite, i.e., ==== in the one-sample test and ==== in the two-sample test. The second part deals with neither positive nor negative definite matrices. We also want to emphasize that by reducing the dimension using random projection, we do not require any explicit relationship between ==== and ====, unless otherwise mentioned. For a clear exposition of our method, we first assume data are normally distributed, and then an extreme value type result is established to cover the non-normal cases in Section 2.1.====This rest of the paper is organized in the following way. In Section 2, the one-sample test is considered. An extreme value type result is established, and an algorithm based on Poisson process approximation is given to evaluate the significance level. In Section 3, the two-sample test is considered. The results for the one-sample test are also extended to the two-sample test. In Section 4, the performance of the proposed tests is studied under two cases: positive (negative) definite matrices and non-positive (non-negative) definite matrices. A transformation using the precision matrix is used to partially recover the information lost during the one-dimensional projection. Simulations are given in Section 5. An application to the Acute Lymphoblastic Leukemia (ALL) data is studied in Section 6. Summary and discussion are given in Section 7.",Projected tests for high-dimensional covariance matrices,https://www.sciencedirect.com/science/article/pii/S0378375818302817,9 January 2020,2020,Research Article,222.0
"Zhang Qiuyan,Hu Jiang,Bai Zhidong","Key Laboratory for Applied Statistics of the Ministry of Education, School of Mathematics and Statistics, Northeast Normal University, China","Received 24 September 2018, Revised 15 November 2019, Accepted 2 January 2020, Available online 9 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2020.01.002,Cited by (5),The goal of this study was to test the equality of two ==== by using modified Pillai’s trace ,"High-dimensional data are common in modern scientific domains, such as finance and wireless communication. Hence, the testing of covariance matrices under high-dimensional settings constitutes an important issue in these areas. The following three main tests have been investigated widely by statisticians for one-sample tests: (i) sphericity test, (ii) identity matrix test, and (iii) diagonal matrix test. Ledoit and Wolf (2002) investigated the properties of the sphericity and identity matrix tests when the sample size and the dimension converge to infinity proportionally, and Birke and Dette (2005) generalized Ledoit and Wolf’s (2002) conclusion to the case where the sample size and dimension are not of the same order. Srivastava (2005) proved the asymptotic null and alternative distributions of the testing statistics for normally distributed data. Furthermore, Chen et al. (2010) proposed a nonparametric method and reported that its data could come from any distribution with a specified data structure. Cai and Ma (2013) developed an identity matrix test procedure based on minimax analysis and showed that the power of their test uniformly dominates the power of the corrected likelihood ratio test by Bai et al. (2009) over the entire asymptotic regime. Under the alternative hypothesis, Chen and Jiang (2018) demonstrated the central limit theorem (CLT) of the likelihood ratio test (LRT) statistic. Schott (2006), Fisher et al. (2010), Srivastava et al. (2011), Qiu and Chen (2012) and Wu and Li (2015) also analyzed this issue in depth.====Moreover, testing procedures for the equivalence of high-dimensional two-sample covariance matrices are also frequently considered. Regarding the hypothesis test problem, ====where ==== and ==== are two population covariance matrices. Schott (2007) proposed a statistic based on the idea of an unbiased estimation of the squared Frobenius norm of ====, and showed its asymptotic distribution under the condition that the sample sizes and the dimension converge to infinity proportionally. A similar idea was adopted by Li and Chen (2012) and Gao et al. (2013). In addition, Srivastava (2007) considered the lower bound of this Frobenius norm, and Zhang et al. (2018) generalized Li and Chen’s statistic to multiple samples. Srivastava and Yanagihara (2010) considered the distance measure ==== and proposed a test based on a consistent estimation of this distance. Moreover, Cai et al. (2013) developed an estimator to find the maximum difference between entries in two-sample covariance matrices. Bai et al. (2009), Zhang et al. (2019) and Jiang et al. (2012) presented the asymptotic distribution of the correctional LRT under high-dimensional assumptions. Later, Zheng et al. (2015) extended the results of Bai et al. (2009) to general populations with unknown means.====The goal of this study is to test the hypothesis (1). Assume that our samples ==== are drawn independently from populations ==== with mean ==== and covariance matrices ====, where ==== is sample sizes. We denote ====, where ====, ====. For the test problem (1), we choose Pillai’s classic trace statistic ==== which was first proposed by Pillai (1954). For convenience, we subsequently denote ====, which is called the Beta matrix and was proposed by Bai et al. (2015). From this definition, we note that to guarantee the reversibility of ====, ==== must be smaller than ====. The asymptotic property of Pillai’s statistic has been obtained by using the moment method under the condition that the sample sizes diverge but the dimension is fixed. Motivated by Bai et al. (2015), in this paper, we modify Pillai’s trace statistic by removing the one and zero eigenvalues of ====, that is, ====where ==== are eigenvalues of ====.====In a similar fashion, we modify another of Pillai’s trace statistics, ====and transform ==== to ====where ====, ====, ==== and ==== are eigenvalues of ====. In the next section, we will show the CLTs of ==== and ==== under a high-dimensional setting under the null hypothesis.====The main technical tool employed in this paper is random matrix theory (RMT), which is a powerful method when the dimension ==== is large. Marchenko and Pastur (1967) determined the limiting spectral distribution of a large-dimensional sample covariance matrix. Bai and Silverstein (2004) proposed a CLT for the linear spectral statistics (LSS) of large-dimensional sample covariance matrices that highlights this issue. Zheng (2012) considers a CLT for the LSS of a large-dimensional F matrix, which is used to fulfill the two-sample test. However, the drawback of their method is that the dimension ==== must be smaller than ====. Bai and Yao (2008) focused on the spiked model, which was first proposed by Johnstone (2000), and established a limit theorem of extreme sample eigenvalues. Similar works include Baik and Silverstein (2006), Paul (2007), Bai et al. (2013) and Passemier et al. (2015). Recently, Bai et al. (2015) proved the CLT for the LSS of the Beta matrix using the asymptotically normally distributed property of the sum of the martingale difference sequence and extended the dimension to a high-dimensional situation.====One should notice that the main technical tool used here is Cauchy’s residue theorem—the same technique utilized in Zhang et al. (2019); however, the difference is that the integrands in the current paper are linear functions, whereas the integrands for the LRT statistics proposed in Zhang et al. (2019) are logarithmic functions. Moreover, these linear functions can be implemented more rapidly and in a less source-consuming way than the abovementioned logarithmic functions, which have greater computational complexity. In addition, it is clear that when ==== or ==== tend to ====, the Beta matrix will have eigenvalues tend to ==== or ==== which causes logarithm function tend to infinity. Therefore, the variance of the statistic would tend to infinity which surely makes the test less powerful. However, due to the linearity of the integrand functions, Pillai’s trace statistics can be utilized in a space where ==== or ====. Therefore, Bai et al. (2015), Zhang et al. (2019) and the current paper can be viewed as a series of works aimed at improving the classic test statistics of two-sample covariance matrices (see (1.2) in Bai et al. (2015)) from a low-dimensional framework to a high-dimensional framework. In Section 3, we compare the test statistics proposed in this paper with Li and Chen’s (2012) statistic, Cai et al.’s (2013) statistic and Zhang et al.’s (2019) statistics through simulations.====The remainder of this paper is organized as follows. Section 2 presents the main conclusions related to the proposed statistics. The results of the simulations, including the comparison with Li and Chen’s (2012) statistic, Cai et al.’s (2013) statistic and Zhang et al.’s (2019) statistics, are presented in Section 3. Section 4 includes an analysis using real Standard and Poor’s 500 index data. The proof is presented in the Appendix.",Modified Pillai’s trace statistics for two high-dimensional sample covariance matrices,https://www.sciencedirect.com/science/article/pii/S037837581830301X,9 January 2020,2020,Research Article,223.0
"Wei Ran,Ghosal Subhashis","North Carolina State University, United States of America","Received 14 April 2018, Revised 17 November 2019, Accepted 16 December 2019, Available online 8 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.12.004,Cited by (20),-sense. It is shown that the proposed contraction rate is comparable with the point mass prior that is studied in Atchadé (2017). The simulation study under the ,"For statistical models such as linear regression, logistic regression or normal means model, high-dimensional data analysis is challenging due to the computational burden and inherent limitations of any procedure under the curse of dimensionality. In these models, it is essential to impose a lower-dimensional structure, such as sparsity, which leads to the problem of variable selection in a regression model. In the non-Bayesian approaches, penalization procedures such as the LASSO is popularly used. Bayesian variable selection procedures are generally more informative than penalization methods because they automatically address the model selection uncertainty. There is an extensive literature on Bayesian variable selection methods in recent years. Since the value zero of a regression coefficient is equivalent to having the corresponding predictor dropping out of the model, priors are designed to give special emphasis to the value zero. For example, a point-mass prior combines a probability mass at zero and a non-zero continuous distribution. Computation is generally carried out by Reversible Jump Markov Chain Monte Carlo methods (Green, 1995). A spike-and-slab prior (Ishwaran and Rao, 2005) is typically a mixture of two normal distributions with one highly concentrated at around zero. The Stochastic Search Variable Selection (SSVS) method (George and McCulloch, 1993) can be used to compute the posterior distribution corresponding to a spike-and-slab prior. Due to high computational costs, these methods are not scalable to very high dimensional situations commonly arising in recent applications.====To address the limitations of these methods mentioned above under high dimensional problems, Bayesian LASSO (Park and Casella, 2008) uses a double exponential prior distribution on the coefficients. Clearly, the posterior mode of the Bayesian LASSO is the LASSO estimator in linear regression model, and the posterior can be computed by a simple Gibbs sampling procedure. However, the Bayesian LASSO does not make the posterior distribution concentrate near the true value in large samples (Castillo et al., 2015), although its mode the LASSO estimator has good estimation and variable selection properties under appropriate conditions. In recent years, a variety of continuous prior densities with good shrinkage properties have been introduced in the literature, such as the horseshoe prior (Carvalho et al., 2010), normal-gamma prior (Griffin and Brown, 2010), double-Pareto prior (Armagan et al., 2013a), Dirichlet–Laplace (DL) prior (Bhattacharya et al., 2015) and the horseshoe+ prior (Bhadra et al., 2017). Unlike a spike-and-slab prior, these priors have one single component like the double exponential prior, but has much higher concentration near zero, and have typically thicker tails, so that they mimic a point-mass prior. Defined as a global–local scale mixture of Gaussian distribution, shrinkage priors give computationally-efficient alternatives to point mass prior. These priors go by the names continuous shrinkage priors, or one-component priors or global–local priors. Recent contributions in the literature show their promising posterior concentration properties near the true value and ability to identify true non-zero coefficients. It may be noted that under such priors, the posterior probability of hitting the exact value zero is always zero, so for variable selection, some appropriate thresholding procedure needs to accompany the Bayesian procedure.====Earliest posterior concentration results in models of dimension increasing to infinity with the sample size are provided by Ghosal, 1997, Ghosal, 1999, Ghosal, 2000, respectively for generalized linear models, regression models and exponential families. In his results, no sparsity conditions are assumed on the truth and posterior concentration at the truth and asymptotic normality of the posterior are established, provided that the growth of the dimension is sufficiently slow compared with the sample size. In more high dimensional settings, assuming sparsity conditions, Jiang (2007) first studied posterior contraction under the Hellinger distance. Castillo and van der Vaart (2012) and Belitser and Nurushev (2020) established posterior concentration and variable selection properties for certain point-mass priors in the many normal means model. The latter paper also established asymptotic coverage from frequentist perspective. Posterior concentration and variable selection in high dimensional linear models are obtained by Castillo et al. (2015), Martin et al. (2017) and Belitser and Ghosal (2019) for certain point-mass priors. The last one also showed that some suitable empirical Bayes Bayesian credible regions with optimal size for any sparsity level have adequate frequentist coverage under an “excessive bias restriction” condition, generalizing the result of Belitser and Nurushev (2020) from sparse normal mean setting to linear regression. Recent theoretical breakthroughs establish concentration properties of posterior distributions of continuous shrinkage priors. Armagan et al. (2013b) showed posterior consistency in a linear regression model with shrinkage priors for a low-dimensional setting where the number of covariates does not exceed the number of observations. Furthermore, Van Der Pas et al. (2014) showed that the posterior based on the horseshoe prior concentrates at the optimal rate for the many normal-means problem. Bhattacharya et al. (2015) obtained an analogous result using the DL prior. Under appropriate choice of the hyper-parameter in DL prior, the posterior contraction property is applied to the coefficients of high-dimension linear regression model. Song and Liang (2017) considered a general class of continuous shrinkage priors and obtained posterior contraction rate in linear regression models depending on concentration and tail properties of the density of the continuous shrinkage prior. Essentially their conclusion may be summarized as the following statement: under appropriate conditions, the posterior contraction rates and variable selection ability of continuous shrinkage priors are close to those of the point-mass priors in high dimensional linear regression models.====Compared to the papers in the literature which address convergence results on Bayesian variable selection in a linear regression model, similar literature focusing on contraction properties in generalized linear models such as logistic regression are limited. Shen and Ghosal (2016) considered estimating conditional density in a high dimensional setting using tensor products of B-splines where the true conditional density is assumed to be a function of only a few predictors. They showed that the oracle contraction rate can be matched adaptively up to a logarithmic factor. A similar result by using Dirichlet process mixtures was obtained by Norets and Pati (2017). Yang and Tokdar (2015) and Belitser and Ghosal (2019) obtained optimal posterior contraction rate in the setting of high dimensional additive regression models using a Gaussian process prior. The logistic regression model is another important model for practical applications and often comes with a large of predictors, among which the important ones need to be selected for precise estimation and sensible interpretation. Atchadé (2017), as a special case of his more general results on quasi-posterior distributions in possibly nonlinear models, derived posterior contraction properties in a logistic regression model using a point-mass prior. They showed that the posterior contracts at the rate ====, where ==== is the true number of active predictors. However, the properties of posterior distributions under shrinkage priors in the logistic regression model has not been studied in the literature. Borrowing a few techniques from Song and Liang (2017), we extend the results of Atchadé (2017) on logistic regression to continuous shrinkage priors. Our finding can be summarized to the statement that, provided that a prior has a sufficient concentration near zero and has sufficiently thick tails, posterior concentrates near the true vector of coefficients at a described rate and with high posterior probability, only selects (effectively) sparse vectors like a point-mass prior. The theoretical result is amply supported by simulations.====The remainder of the paper is organized as follows. Section 2 presents the logistic regression model and the assumptions on shrinkage prior densities. The main results on the posterior contraction of the parameters being estimated are shown in Theorem 2.1. Several examples of shrinkage priors and continuous spike-and-slab prior are introduced in Section 3 to demonstrate the conditions for posterior contraction. For different type Bayesian variable selection technique, Section 4 evaluates the performance in recovering the logistic coefficients, identifying non-zero subsets of covariates and predicting on test design matrix. The proof of the main posterior contraction result is given in Appendix.",Contraction properties of shrinkage priors in logistic regression,https://www.sciencedirect.com/science/article/pii/S0378375819301235,8 January 2020,2020,Research Article,224.0
"Kuwada Masahide,Hyodo Yoshifumi","Hiroshima University, Higashi-Hiroshima, 739-8521, Japan,Graduate School of Informatics, Okayama University of Science, Okayama 700-0005, Japan","Received 22 February 2019, Revised 25 September 2019, Accepted 2 October 2019, Available online 7 January 2020, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.10.009,Cited by (0),"This paper presents a lower bound for the number of assemblies, ====, say, in fractional ====, where ====. Furthermore in the class of balanced fractional ====-BFF) designs of resolution ==== derived from simple arrays (SA’s), there exists a design such that it attains to ==== for ==== and ====, and ==== and ====. However there exists a ====-BFF design of resolution ==== derived from an SA such that it does not attain to ==== for ==== and ====.","The minimum number of assemblies (or treatment combinations) in fractional factorial designs is the interesting problem in a theoretical sense and also in a practical one. The information matrix of odd resolution designs is non-singular. Thus in this case, we can easily obtain the minimum number of assemblies. However even resolution designs have some nuisance parameters. Hence it is not so easy to obtain the minimum number of assemblies. In fractional ==== factorial (====-FF) designs of resolution IV, the lower bound for the number of assemblies, ====, say, was independently presented by Margolin (1969) and Webb (1968). Its bound is equal to ====.====As a generalization of orthogonal arrays, the concept of balanced arrays (BA’s) was first introduced by Chakravarti (1956) as partially BA’s. However they are also a generalization of BIB rather than of PBIB designs, and hence Srivastava and Chopra (1971) called them BA’s. A BA of strength ====, size ====, ==== constraints, two symbols and index set ==== turns out to be a balanced fractional ==== factorial (====-BFF) design (see Srivastava and Chopra (1971), and Yamamoto et al. (1975)). The class of ====-BFF designs is a subclass of ====-FF designs. A BA of ==== does not always exist (e.g., Kuriki (1984)). On the other hand, a BA of ==== always exists for any index set ==== and ====, where ====. It is called a simple array (see Shirakura (1977)) and is briefly denoted by SA====. The characteristic roots of the information matrix of ====-BFF designs of resolution V were obtained by Srivastava and Chopra (1971). By using the properties of the triangular multidimensional partially balanced (TMDPB) association algebra, their results were generalized by Yamamoto et al. (1976) for ====-BFF designs of resolution ====, where ====. A necessary and sufficient condition for a BA of strength ==== to be a ====-BFF design of resolution ==== was presented by Shirakura (1980). Furthermore the existence conditions for ====-BFF designs of resolution ==== derived from SA’s were obtained by Hyodo et al. (2015).====In this paper, we present a lower bound for the number of assemblies, ====, say, in ====-FF designs of resolution ====, where ====. Furthermore in the class of ====-BFF designs of resolution ==== derived from SA’s, there exists a design such that it attains to ==== for ==== and ====, and ==== and ====. However when ==== and ====, there exists a ====-BFF design of resolution ==== derived from an SA such that it does not attain to ====.",On a lower bound for the number of assemblies in fractional ,https://www.sciencedirect.com/science/article/pii/S037837581930120X,7 January 2020,2020,Research Article,225.0
"Comte Fabienne,Marie Nicolas","Laboratoire MAP5, Université Paris Descartes, Paris, France,Laboratoire Modal’X, Université Paris Nanterre, Nanterre, France,ESME Sudria, Paris, France","Received 2 February 2019, Revised 18 December 2019, Accepted 19 December 2019, Available online 30 December 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.12.003,Cited by (14),"For ==== independent random variables having the same Hölder continuous density, this paper deals with controls of the Wolverton–Wagner’s estimator MSE and MISE. Then, for a bandwidth ====, estimators of ==== are obtained by a Goldenshluger–Lepski type method and a Lacour–Massart–Rivoirard type method. Some numerical experiments are provided for this last method.","Consider ==== independent random variables ==== having the same probability distribution of density ==== with respect to Lebesgue’s measure.====The usual Parzen (1962) and Rosenblatt (1956) kernel estimator of ==== is defined by ====where ==== and ==== is a kernel. In 1969, Wolverton and Wagner introduced in Wolverton and Wagner (1969) a variant of ==== defined by ====where ==== and ====. Thanks to its recursive form, this type of estimator is well-suited to online treatment of data: by denoting ====, ====Thus, up-dating the estimator when new observations are available is easy and fast.====We can mention here that several variants or generalizations of the Wolverton and Wagner (WW) estimator have been proposed: see Yamato (1970/71), Wegman and Davies (1979) and Hall and Patil (1994). They were studied from almost sure convergence point of view, or asymptotic rates of convergence under fixed regularity assumptions. We choose to focus on Wolverton and Wagner estimator but our results and discussions may be applied to these.====Theoretical developments concerning either classical Parzen–Rosenblatt or WW recursive kernels estimators occurred recently following different and independent roads.====On the one hand, several recent works are dedicated to efficient and data-driven bandwidth selection, see Goldenshluger and Lepski (2011) and several companion papers by these authors, or (Lacour et al., 2017) who proposed a modification of the method. The original Goldenshluger and Lepski (GL) method was difficult to implement because it turned out to be numerically consuming and with calibration difficulties, see Comte and Rebafka (2016). This is why the improvement proposed in Lacour et al. (2017) has both theoretical and practical interest.====On the other hand, the increase of computer speed and of data sets sizes made fast up-dating of estimators mandatory. The theoretical developments in this context are in the field of stochastic algorithms (see e.g. Mokkadem et al., 2009) or in view of specific applications (see Bercu et al., 2019).====Bandwidths have to be chosen for WW estimators as for Parzen–Rosenblatt ones, and this choice is crucial to obtain good performances. This is why we propose to extend to this context general risk study as described in Tsybakov (2009) and the GL method as improved by Lacour et al. (2017). More precisely, considering for instance ==== for a parameter ==== in formula (1), we study adaptive selection of ====. We prove risk bounds for the Mean Integrated Squares Error (MISE) of the resulting estimator ==== where ==== and ====.====Amiri (2009) proved that for ==== with regularity 2 and an adequate choice of the bandwidth, Parzen–Rosenblatt’s estimator had asymptotical smaller risk than the WW estimator. We propose an empirical finite sample study of this question, together with an interesting insight on the gain brought by higher order kernels.====Now, clearly, plugging ==== in the estimator makes the recursivity fail. Therefore, an adequate strategy is required, either with initial estimation of ==== on the first ====-sample and recursive up-dating relying on this “frozen” value on the following ====-sample, or with adequate matrix updating for ==== selection. This is what is experimented in our final section, and empirically illustrated and discussed.====This paper provides in Section 2 controls of the MSE and of the MISE of the estimator ==== under general regularity conditions on ====. Then, in Section 3, the well-known Goldenshluger–Lepski’s bandwidth selection method for Parzen–Rosenblatt’s estimator is extended to Wolverton–Wagner’s estimator. Lastly, an estimator in the spirit of Lacour et al. (2017) is studied from both theoretical and practical points of view in Section 4. In particular, a recursive global strategy is proposed. Concluding remarks are given in Section 5. Section 6 deals with the proof of Proposition 4.2 which provides a suitable control of the MISE of the adaptative estimator obtained via the LMR-type selection method. All the other proofs are relegated to a ==== document.",Bandwidth selection for the Wolverton–Wagner estimator,https://www.sciencedirect.com/science/article/pii/S0378375819301223,30 December 2019,2019,Research Article,226.0
"Shevchenko Radomyra,Slaoui Meryem,Tudor C.A.","Fakultät für Mathematik, LSIV, TU Dortmund, Vogelpothsweg 87, 44227 Dortmund, Germany,Laboratoire Paul Painlevé, Université de Lille, F-59655 Villeneuve d’Ascq, France","Received 14 February 2019, Revised 5 September 2019, Accepted 2 October 2019, Available online 26 December 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.10.008,Cited by (7),We analyze ==== ==== in time and which is white in space. The ====-variations are defined along ==== of any order ==== and of any length. We show that the sequence of generalized ==== and analyze these estimators theoretically and numerically.,"For several decades the statistical inference in stochastic (partial) differential equations (S(P)DEs in the sequel) constitutes an intensive research direction in probability theory and mathematical statistics. Traditionally, the disturbance term in such stochastic models is a standard space–time white noise, i.e. a Gaussian field that behaves as a Brownian motion in time and in space. We refer, among many others, to the monographs or surveys (Cialenco, 2018, Lototsky and Rozovsky, 2017) or (Kutoyants, 2004). A part of the scientific literature on statistical inference for SPDEs concerns the parameter estimation for such equations based on the observation of the solution at discrete points in time and/or in space, which also constitutes the main goal of our work. Among the first contributions to these topics, we refer to Mohapl (1997) and Markussen (2003) for the maximum likelihood and least square estimators for parabolic or elliptic-type SPDEs respectively, driven by a space–time white noise. The study in Markussen (2003) has been then extended in Bibinger and Trabs (2017) by adding a time-varying volatility in the noise term and by using power variation techniques to estimate various parameters of the model. Other recent works on parameter estimates for discretely sampled SPDEs, some of them using generalized variations, are Cialenco and Huang (2017), Chong (2018), Bibinger and Trabs (2019), Khalil-Mahdi and Tudor (2018) or Pospíšil and Tribe (2007).====Nowadays, a particular case of wide interest is represented by the S(P)DEs driven by fractional Brownian motion (fBm) and related processes, due to the vast area of application of such stochastic models. Many recent works concern the estimation of the drift parameter for stochastic (partial) differential equations driven by fractional Brownian motion (we refer, among many others, to Azmoodeh and Viitasaari 2015, Hu and Nualart 2011, Kleptsyna and Le Breton 2002, Prakasa Rao 2010, Tudor and Viens 2007), while fewer works deal with the estimation of the Hurst parameter in such stochastic equations (Khalil and Tudor, 2018, Torres et al., 2014, as far as we know).====In this paper, we consider the one-dimensional stochastic wave equation driven by an additive Gaussian noise which behaves as a fractional Brownian motion in time and as a Wiener process in space (we call it ====). Our purpose is to construct and analyze estimators for the Hurst parameter of the solution to this SPDE based on the observation of the solution at a fixed time and at a discrete number of points in space. The wave equation with fractional noise in time and/or in space has been studied in several works, such as Balan and Tudor (2010), Clarke De la Cerda and Tudor (2014), Gubinelli et al. (2006), Khalil and Tudor (2018) and Quer-Sardanyons and Tindel (2007) etc. We will use a standard method to construct estimators for the Hurst parameter, which is based on the ====-variations of the observed process. The method has been recently employed in Khalil and Tudor (2018) for the case of quadratic variations, i.e. ====. As for the fBm, it was shown that the standard quadratic variation estimator is not asymptotically normal when the Hurst index becomes bigger than ==== and this is inconvenient for statistical applications. In order to avoid this restriction and to get an estimator which is asymptotically Gaussian for every ====, we will use the generalized ====-variations, which means that the usual increment of the process is replaced by a higher order increment. The idea comes from Istas and Lang (1997) and since then it has been used by many authors (see e.g. Coeurjolly, 2001 or Chronopoulou et al., 2009). More precisely, if ==== denotes the solution to the wave equation with fractional-white noise, we define the (centered) generalized ====-variation statistics (==== integer) as ====where ==== represents the spatial increment of the solution ==== at ==== along a ==== ==== of power (order) ==== and length ==== (see the next section for the precise definition).====By using chaos expansion and recent developments in the Stein–Malliavin calculus we show that the sequence ==== satisfies a central limit theorem (CLT) as ==== (in the spirit of Breuer and Major, 1983) whenever ==== and in this way the restriction ==== can be avoided by choosing a filter of order ====, i.e. by replacing, for example, the usual increment by a rectangular or a higher order increment. We will obtain the rate of convergence under the Wasserstein distance for this convergence in law and we also prove a multidimensional CLT. So we generalize the findings in Khalil and Tudor (2018) to filters of any power ==== and to ====-variations of any order ==== and in addition we show that in the special case ==== and ==== a non-Gaussian limit theorem occurs with limit distribution related to the Rosenblatt distribution (but having a more complex structure).====These theoretical results are then applied to the estimation of Hurst index of the solution of the fractional-white wave equation. Based on the behavior of the sequence ==== we prove that the associated ====-variation estimators for ==== are consistent and asymptotically normal. Moreover, we provide a numerical analysis of the estimators when ==== by analyzing their performance on various filters and for several values of the Hurst parameter and confirming via simulation the theoretical results.====We organized the paper as follows. Section 2 contains some preliminaries. In this part we present the basic facts concerning the solution to the fractional-white wave equation, we introduce the filters and the increment of the solution along filters. In Section 3, we prove a CLT for the sequence ==== for any integer ====, and we obtain the rate of convergence when ==== is even by using the Stein–Malliavin theory. In Section 4 we show a non-central limit theorem in the case ==== and for filters of order ====. Section 5 concerns the estimation of the Hurst parameter of the solution to the fractional-white wave equation. We included theoretical results related to the behavior of the ====-variations estimators for the Hurst index as well as simulations and numerical analysis for the performance of the estimators. Appendix contains the basic tools from Malliavin calculus needed in the paper and the proofs of some technical results.",Generalized ,https://www.sciencedirect.com/science/article/pii/S0378375819301193,26 December 2019,2019,Research Article,227.0
"Guo Wenge,Sarkar Sanat","Department of Mathematical Sciences, New Jersey Institute of Technology, Newark, NJ 07102, USA,Department of Statistics, Temple University, Philadelphia, PA 19122, USA","Received 6 November 2016, Revised 9 November 2017, Accepted 13 March 2018, Available online 24 December 2019, Version of Record 12 March 2020.",https://doi.org/10.1016/j.jspi.2018.03.008,Cited by (3),"Often in multiple testing, the hypotheses appear in non-overlapping blocks with the associated ","In many multiple hypothesis testing problems arising in modern scientific investigations, the hypotheses appear in non-overlapping blocks. Such block formation is often a natural phenomenon due to the underlying experimental process or can be created based on other considerations. For instance, the hypotheses corresponding to (i) the different time-points in a microarray time-course experiment (Guo et al., 2010, Sun and Wei, 2011) for each gene; or (ii) the phenotypes (or the genetic models) with (or using) which each marker is tested in a genome-wide association study (Lei et al., 2006); or (iii) the conditions (or subjects) considered for each voxel in brain imaging (Heller et al., 2007), naturally form a block. While applying multiple testing in astronomical transient source detection from nightly telescopic image consisting of large number of pixels (each corresponding to a hypotheses), Clements et al. (2012) considered grouping the pixels into blocks of equal size based on telescope ‘point spread function.’====A special type of dependence, which we call block dependence, is the relevant dependence structure that one should take into account while constructing multiple testing procedures in presence of such blocks. This dependence can be simply described by saying that the hypotheses or the corresponding ====-values are mostly dependent within but not between blocks. Also known as the clumpy dependence (Storey, 2003), this has been considered mainly in simulation studies to investigate how multiple testing procedures proposed under independence continue to perform under it (Benjamini et al., 2006, Finner et al., 2007, Sarkar et al., 2012, Storey et al., 2004), not in offering FDR or FWER controlling procedures precisely utilizing it. In this article, we focus on constructing procedures controlling the FDR and the FWER that incorporate the block dependence in a non-asymptotic setting in an attempt to improve the corresponding procedures that ignore this structure. More specifically, we consider the Benjamini–Hochberg (BH, 1995) method for the FDR control and the Bonferroni method for the FWER control and adapt them to the data in two ways — incorporating the block dependence and estimating the number of true null hypotheses capturing such dependence.====Adapting to unknown number of true nulls has been a popular way to improve the FDR and FWER controls of the BH and Bonferroni methods, respectively. However, construction of such adaptive methods with proven control of the ultimate FDR or FWER in a non-asymptotic setting and providing real improvements under dependence is an open problem (Benjamini et al., 2006, Blanchard and Roquain, 2009). We offer some solutions to this open problem in this paper under a commonly encountered type of dependence, the block dependence.",Adaptive controls of FWER and FDR under block dependence,https://www.sciencedirect.com/science/article/pii/S0378375819301181,24 December 2019,2019,Research Article,228.0
"Cai Kaida,Shen Hua,Lu Xuewen","Department of Mathematics and Statistics, University of Calgary, 2500 University Drive NW, Calgary, AB T2N 1N4, Canada","Received 5 December 2018, Revised 15 October 2019, Accepted 22 November 2019, Available online 23 December 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.11.005,Cited by (3),"In many scientific applications, such as biological studies, the predictors or ","In many application areas, subjects under study may experience repeated events over time. A recurrent event process is a studying process which could generate events repeatedly over time and the data provided by this process is called recurrent event data. The intrinsic dependencies of inter-event times, the effects of past events on the course of the complex process, study designs and observation schemes such as intermittent observations and censoring make the analysis of recurrent events a stand-alone subject in statistics. Statistical methods and models for the analysis of recurrent events have been developed for solving different issues that arise in fields such as business, industry, reliability, medicine and public health. Books on point processes (e.g., Cox and Isham, 1980, Karr, 1991, Daley and Vere-Jones, 2003) have discussed the probabilistic background and models for recurrent event processes with examples of application. Statistical methods for the analysis of recurrent events are also considered by many authors, see Andersen et al. (1993), Lawless (1995) and Nelson (2003), among others.====Some fundamental models for recurrent events processes are the Poisson processes and renewal processes. There are three main methods of inference in the Poisson process settings: parametric, nonparametric and semi-parametric methods; all of them are widely applied in the analysis of recurrent events and event history data. Nelson (1972) and Aalen (1978) considered the nonparametric estimation of the mean function for general processes. They introduced the same estimator which is now called the Nelson–Aalen estimator. Andersen and Gill (1982) considered a semi-parametric model that is sometimes called the Andersen–Gill (AG) model and derived the asymptotic distributional results using counting process theory. The robust method for semi-parametric regression was developed by Lawless and Nadeau (1995). The likelihood construction for these models was given by Andersen et al. (1993) and Cook and Lawless (2007).====Data sets with grouped predictors are produced in a wide variety of scientific fields, such as genetic and medical studies, where only a small portion of predictors and groups are important. That is, the important groups and covariates are sparse. Therefore, methods for variable sparsity and group sparsity are important research topics in science and statistics. In order to identify the important predictors, variable selection methods have been introduced in the past three decades. A penalized variable selection method is a common approach that shrinks parameters of unimportant predictors to zero, which improves the traditional model selection techniques such as stepwise deletion and subset selection, and does variable selection automatically and simultaneously, see Fan and Li (2001) for more discussions on the good features of the penalized variable selection method. In order to handle the different types of sparsity problems, different penalties were introduced. For instance, Donoho and Johnstone (1994) and Tibshirani (1996) introduced an ====-norm penalty, and Hoerl and Kennard (1970) proposed an ====-norm penalty. A smoothly clipped absolute deviation (SCAD) penalty was applied to the generalized linear model and the Cox model by Fan and Li (2001). However, these methods only select predictors individually. Variable selection methods which could remove the unimportant group predictors are necessary when group predictors exist. Different group variable selection methods have been introduced by several authors. Yuan and Lin (2006) introduced a group Lasso to select groups of predictors into or out of the model together. Blockwise thresholding was discussed by Antoniadis and Fan (2001) and Cai (2001). The group minimax concave penalty (MCP) was proposed by Breheny and Huang (2009). In addition to selecting important groups, we also need to consider the situation that some individual covariates in the important groups are unimportant. Then, the all-in-all-out fashion variable selection methods do not work well and bi-level selection methods have been introduced to handle this kind of problems. Bi-level selection methods can identify the important groups and the important individual covariates in the important groups. Friedman et al. (2010) discussed a sparse group Lasso which blends both the Lasso and the group Lasso penalty in the objective function. Another important bi-level selection method, called the hierarchically penalized method, was introduced by Wang et al. (2009). It uses a hierarchical framework in the penalty part to shrink the parameters for the unimportant groups, and the unimportant variables in the important groups to zero, simultaneously. Huang et al. (2009) used a specially designed group-bridge penalty to perform group and within group individual variable selection. Different bi-level penalties and a general framework were discussed by Breheny and Huang (2009).====There are few papers which consider the variable selection for the recurrent event data models. Tong et al. (2009) considered a nonconcave penalized estimating function for recurrent event data. Variable selection issues for recurrent event data with informative censoring were considered by Cheng and Luo (2012). Wu (2013) proposed a Lasso penalized variable selection method for recurrent event data via coordinate descent. Zhao et al. (2018) presented a new method with a new penalty function for variable selection in the additive rate model, which is referred to as the broken adaptive ridge regression approach, it can not only allow for estimation and variable selection simultaneously but also accommodate the clustering effect when covariates are highly correlated. Their Theorem 2 indicates that the highly correlated covariates should have similar regression coefficients and be selected or deleted simultaneously, it leads to the pairwise grouping effect property. All these papers only consider the sparsity problem by selecting predictors individually or pairwise instead of in a bi-level fashion. To fill the gap, we consider recurrent event data with both individual sparsity and group sparsity. Huang et al. (2009) and Wang et al. (2009) studied the group variable selection problem for the linear regression model and the Cox regression model, respectively. In this paper, we propose a general bi-level group-bridge penalty to perform group and within group variable selection in the AG model for recurrent event data. Our work extends that of Wang et al. (2009) for right-censored data to recurrent event data; moreover. We study the performance of the proposed method by comparing the empirical and estimated standard errors of the bi-level penalized estimates of the coefficients for important covariates. We construct three methods for tuning parameter selection, which are generalized cross validation (GCV), Akaike information criterion (AIC) and Bayesian information criterion (BIC). An efficient algorithm was proposed to carry out the proposed method. We investigate the oracle properties for the two important bi-level penalized methods in the context of recurrent event data model. Our proofs include some innovations in techniques, for example, shaper bounds are obtained for some inequalities used in proving Theorem 1.====The remaining of this article is organized as follows. Section 2 introduces new methodologies for group selection in recurrent events models and three tuning parameter selection criteria named GCV, AIC and BIC, respectively. Asymptotic properties of the proposed methods are explored in Section 3. Simulation studies are conducted in Section 4. Section 5 presents a real life data analysis using the proposed method. Section 6 includes discussion and conclusion. Proofs are relegated to the Appendix of the Supplementary Material.",Group variable selection in the Andersen–Gill model for recurrent event data,https://www.sciencedirect.com/science/article/pii/S0378375819301144,23 December 2019,2019,Research Article,229.0
"Xu Kai,Chen Fangxue","School of Mathematics and Statistics, Anhui Normal University, Wuhu 241002, PR China","Received 14 October 2018, Revised 4 May 2019, Accepted 2 October 2019, Available online 23 December 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.10.007,Cited by (0),"In this article, we use the martingale difference divergence (Shao and Zhang, 2014) to propose novel tests for the goodness-of-fit of linear ====. We investigate distribution theory for the test statistics under both the null and the local and fixed alternative sequences. A ","The conditional quantiles of a response variable ==== given covariates ==== are often of interest in a wide range of applications. In a quantile regression setting, the relationship between ==== and ==== can then be denoted by ====where ==== is the quantile regression function and ==== is the error, which has a conditional ====-quantile (====) equal to zero, i.e., ====. Here, ==== is the indicator function, i.e., ==== if event ==== occurs and ==== otherwise. In linear quantile regression, we assume that ==== belongs to the following parametric class, i.e., ====where ==== is the vector of known covariate functions and ==== is the unknown parameter vector. We focus on testing the null hypothesis ====Since Koenker and Bassett (1978), quantile regression models have drawn a great deal of attention in the literature; see Koenker (2005) for excellent literature review. This is because compared to classical mean regression, quantile regression models provide a more detailed description of the conditional distribution of the response. A parametric quantile regression model are frequently used in data analysis. The mis-specification of ==== can lead to improper conclusions on the process under study. Therefore, testing the correctness of the specification of parametric quantile regression models is necessary. Many statisticians make contributions to this area. Examples include, but are not limited to, Zheng (1998), He and Zhu (2003) and Conde-Amboage et al. (2015). These three types of tests extended respectively the mean restriction tests of Zheng (1996), Stute (1997) and Escanciano (2006) to the quantile restriction case. Zheng’s method, further enhanced by Maistre et al. (2017) to break the curse of dimensionality in nonparametric testing, is based on smoothing kernel and requires user-chosen parameters (bandwidth or kernel). He and Zhu’s test, further improved by Conde-Amboage et al. (2015), is built on the cusum process of the regressors marked by the residuals and has the poor empirical power when fitting a multiple regression.====Here we propose novel consistent tests to examine the adequacy of linear quantile regression models by means of the popular martingale difference divergence (Shao and Zhang, 2014, Zhang et al., 2018 MDD). The MDD fully characterizes the conditional mean independence between a scalar variable and a vector variable in the sense that it is zero if and only if the conditional mean of the scalar variable given the vector variable is independent of the vector variable; see related work by Park et al. (2015) and Lee and Shao (2018). Methodologically, our proposals are applicable when regression models are heteroscedastic, are computationally simple, and do not require the subjective choice of parameters such as bandwidths, kernels, and integrating measures. We would here like to mention the related work by Su and Zheng (2017) who also considered a model specification test based on the MDD. However, Su and Zheng’s paper focused on testing mean restrictions. This paper extends the mean restriction test of Su and Zheng (2017) to the quantile restriction case. Theoretically, different from Su and Zheng (2017), the analyses are not trivial mainly due to the fact that the residuals from quantile regression involve indicator functions that are not differentiable in the regression parameter. By means of the theory of ====-statistics indexed by parameters, we investigate the limiting distributions of the test statistics under both the null and the local and fixed alternative hypotheses. To approximate the null distributions of our statistics, we propose a bootstrap scheme and prove its consistency.====The rest of the paper is organized as follows. In Section 2, we present the test statistics and develop the asymptotic distribution of our proposed test statistics. A bootstrap scheme is also devised in this section to provide ====-values. In Section 3, numerical studies are presented to demonstrate the competitive size and power performance of the proposed approaches with finite sample size. In Section 4, we conclude the article with a short discussion. All the technical proofs are gathered in Appendix.",Martingale-difference-divergence-based tests for goodness-of-fit in quantile models,https://www.sciencedirect.com/science/article/pii/S037837581930117X,23 December 2019,2019,Research Article,230.0
"Zhou Weiping,Yang Jian-Feng,Liu Min-Qian","School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China","Received 14 March 2019, Revised 14 September 2019, Accepted 22 November 2019, Available online 18 December 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.11.006,Cited by (6)," full factorial design and a series of saturated two-level regular designs, a number of maximin distance LHDs are constructed via the rotation method. Some of the constructed LHDs are exactly optimal and the others are asymptotically optimal under the maximin ====-distance criterion. The constructed maximin distance LHDs have two prominent advantages: (i) no computer search is needed; and (ii) they are orthogonal or nearly orthogonal. Detailed comparisons with existing LHDs show that the constructed LHDs have larger minimum distances between design points.","In computer experiments, complex systems are increasingly investigated through space-filling designs, which aim to distribute the design points over the design space as evenly as possible. Latin hypercube designs (LHDs), first introduced by McKay et al. (1979), are used as a popular class of space-filling designs. As we know, LHDs achieve one-dimensional space-filling property. One disadvantage of LHDs is that any such design is not necessarily space-filling in the full-dimensional space. To solve this problem, maximin distance criterion (Johnson et al., 1990) was proposed for constructing good LHDs. The maximin distance criterion is to maximize the minimum distance between design points, which guarantees the good space-filling property in the full-dimensional space. The maximin distance designs are asymptotically optimal for fitting Gaussian process models under a Bayesian setting (Johnson et al., 1990), and the maximin distance LHDs are well-suited for computer experiments (Lin and Tang, 2015).====There are many algorithms for constructing maximin distance LHDs, such as the simulated annealing (Morris and Mitchell, 1995, Joseph and Hung, 2008, Ba et al., 2015), swarm optimization algorithms (Moon et al., 2011, Chen et al., 2013) and the threshold-accepting method (Xiao and Xu, 2018). However, due to the computational complexity, these methods are not suitable to construct large LHDs which are needed in computer experiments (see for example, Morris, 1991, Kleijnen, 1997, Cioppa and Lucas, 2007, Gramacy et al., 2015). In order to overcome the challenges for constructing large LHDs, Zhou and Xu (2015) considered linear permutations to construct maximin ====- and ====-distance LHDs based on good lattice point sets; Xiao and Xu (2017) constructed LHDs with large minimum ====-distance via Costas arrays; Wang et al. (2018b) employed the Williams transformation to construct optimal maximin ====-distance LHDs.====The rotation method, firstly presented by Beattie and Lin, 2004, Beattie and Lin, 2005, is simple and useful for constructing designs for computer experiments. This method was further employed to construct orthogonal LHDs, see e.g., Steinberg and Lin (2006), Lin et al. (2009), Pang et al. (2009), Sun and Tang (2017), and Wang et al. (2018a), among others. In this paper, by combining the rotation method and the doubling operator of a design (Chen and Cheng, 2006), we propose several methods to construct maximin ====-distance LHDs without any computer search. Firstly, based on a ==== full factorial design, a class of asymptotically optimal maximin ====-distance LHDs are constructed via the rotation method. Moreover, we show that these LHDs are orthogonal. Next, based on a series of saturated two-level regular designs, a good deal of maximin ====-distance LHDs are constructed via the rotation method. Some of these LHDs are exactly optimal and the others are asymptotically optimal under the maximin ====-distance criterion. Furthermore, the average correlations of these LHDs converge to zero as the design sizes increase, which is desirable for Gaussian process with linear trend (Wang et al., 2018a, Wang et al., 2018b).====The rest of this paper is organized as follows. Section 2 provides relevant notation and definitions. Section 3 presents the construction methods, along with some discussions of asymptotic properties for the ====-distance efficiency of the resulting designs. Section 4 discusses several convergence properties of the average correlations for the resulting designs. Section 5 provides some concluding remarks. All proofs are deferred to Appendix.",Optimal maximin ,https://www.sciencedirect.com/science/article/pii/S0378375819301156,18 December 2019,2019,Research Article,231.0
"Weng Haolei,Feng Yang","Department of Statistics and Probability Michigan State University, East Lansing, MI, 48824, United States of America,Department of Biostatistics, College of Global Public Health, New York University, New York, NY, 10003, United States of America","Received 25 April 2018, Revised 2 September 2019, Accepted 5 September 2019, Available online 18 December 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.09.016,Cited by (0),We consider a ==== convergence rate. Thus we reveal a phase transition phenomenon regarding the ==== of latent continuous variables while preserving the estimability of the correlation. Numerical experiments are performed to validate the conclusions.,"Data sets consisting of dependent binary outcomes are common in quantitative fields. For instance, in longitudinal studies, monthly presence or absence of a disease for residents in a neighborhood might be recorded over a certain period; in social network analyses, linkages between individuals or organizations on a social media website can be accessible; in financial econometrics, the rise or fall of stock prices from the same sectors are observed. The existence of dependence makes it subtle to analyze such types of data. Several sophisticated modeling frameworks for dependent binary data have been well developed over the last two decades, such as graphical models (Koller and Friedman, 2009) and exponential random graph models (Lusher et al., 2013).====In this paper, however, we consider an alternative and more transparent modeling method. Specifically, let ==== be the binary observations. We assume the data is generated from thresholding a latent continuous vector ==== with ==== and ====: ====where ==== is a pre-defined threshold. The dependency of ==== can then be modeled by specific structures on ====. The idea of thresholding continuous variables to obtain discrete ones has been widely adopted in binary and ordinal regression problems (McCullagh, 1980, McCullagh and Nelder, 1989). For example, in probit models, each binary response variable is obtained by truncating an independent latent normal variable. Nevertheless, different from regular regression problems, the model formulation (1) puts emphasis on the dependency structure of the observations.====We use network data example to elaborate on the model formulation (1). For notational convenience, we rewrite the observations ==== by a matrix ==== and the latent continuous variables ==== by ====. The binary entry ==== represents whether there exists an edge between nodes ==== and ====. We consider undirected networks where ==== for simplicity. The covariance matrix ==== belongs to ====. Due to rich structures exhibited in different types of networked systems (Newman, 2003), there has been an extensive literature on network modeling including Erdös–Rényi random graph model (Erdös and Rényi, 1959), exponential random graph model (Robins et al., 2007), stochastic blockmodel (Holland et al., 1983), and latent space model (Hoff et al., 2002), among others. As an alternative, model (1) assumes the edges between nodes are generated from some underlying continuous variables, and the covariance matrix ==== captures the possible dependency among different edges. One specification is ==== The mean parameter ==== incorporates heterogeneity across different edges. It can be assumed of low-rank based on the hypothesis that the generation mechanism of edges is driven by a few node-specific factors. This is in the same spirit of both stochastic block model and latent space model. Letting ==== be the covariance between ==== and ====, a general structure can be imposed for ====: ====where ==== is the empty set. In this case, the dependencies among edges that share common nodes and those which do not are characterized by two different parameters. The covariance structure with ==== has been considered in relational data modeling works (Warner et al., 1979, Gill and Swartz, 2001, Westveld and Hoff, 2011). Here (2) generalizes further to take into account dependencies between edges without common nodes.====Under the model setup (1), a fundamental question is regarding the estimation of parameters in ====, which provide important dependency information for the binary observations ====. Given that classical asymptotic results do not hold in the current case, a delicate study of the estimation problem is not only theoretically appealing, but helpful for practitioners to better model and analyze dependent binary data sets. To fix idea, we consider a compound symmetry covariance structure: ====The correlation parameter ==== characterizes the dependency in the binary sequence ====. This covariance matrix (3) is a special case of the spiked covariance form proposed in Johnstone and Lu, 2004, Johnstone and Lu, 2009 for studying high-dimensional principal component analysis. A full understanding of this structure can be seen as a gateway for understanding more complicated ones. Note that (2) contains (3) as a subset. The central question we focus on is====As ====, can ==== be consistently estimated from the sequence ====?====Surprisingly, the answer turns out to be negative. We present in detail our theoretical discovery in Section 2. In particular, we analyze the likelihood function and reveal its infeasibility of producing consistent estimates. We then formally prove the nonestimability of ==== under the binary sequence model (1). Interestingly, we further demonstrate that ====-consistent estimates emerge under a variant of (1). Section 3 presents numerical experiments to support our theoretical findings. Section 4 contains a discussion of insightful implications and future research in light of the main results in the paper. To improve readability, we put all the technical materials in Section 5 and Appendix.",On the estimation of correlation in a binary sequence model,https://www.sciencedirect.com/science/article/pii/S0378375819301168,18 December 2019,2019,Research Article,232.0
"Meziani Aymen,Medkour Tarek,Djouani Karim","Laboratoire Images, Signaux et Systèmes Intelligents (LiSSi), Université de PARIS-EST, Paris, France,Department of Statistics and Probability, University of USTHB, Algiers, Algeria,Tshwane University of Technology, FSATI, Pretoria, South Africa","Received 4 September 2018, Revised 27 November 2019, Accepted 30 November 2019, Available online 13 December 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.11.004,Cited by (1),The present paper introduces an extension of the ==== regularised version is proposed. The ==== of the new spectral estimator are discussed and simulations are performed in order to show its efficiency in the presence of hidden periodicities and under different types of noise.,"Spectral analysis techniques aim to quantify the energy generated over the frequency components of a given time series. The periodogram has commonly been used as a non-parametric estimator of the spectrum. However, it may not exhibit robustness, especially in the presence of heavy-tailed data or outliers; see for example Bloomfield (2004).====The ordinary periodogram uses least squares estimates for the projection of the time series onto the Hilbert space generated by the sinusoidal base. The least squares estimates focus only on the conditional mean. The quantile regression introduced in Koenker and Bassett Jr (1978) attempts to evaluate the variation of the conditional quantiles with respect to the response variable. The quantile regression covers the whole distribution of the data and exhibits robustness against outliers. The quantile periodogram introduced in Li (2012) summarises the impact of the cyclical behaviour on the distribution of the time series. The random coefficients autoregressive models, as described in Hagemann (2011), are an example for the application of the quantile periodogram.====The quantilogram introduced in Linton and Whang (2007), has been used to evaluate the directional predictability in the time domain. The quantile spectrum studies introduced in Hagemann (2011) and Dette et al. (2015) use the level crossings for spectral estimation instead of the time series. Based on the spectral density, the authors in Lee and Rao (2011) propose a test for the sequential dependence of non-linear time series. The composite quantile periodogram is proposed in Lim and Oh (2016) in order to overcome the problem of the choice of the quantile of interest.====In order to improve the prediction accuracy and to reduce the variance of estimators, different types of penalisation methods have been introduced. The ==== penalisation family is among the most commonly used, with a wide interest in the ==== penalty, such as the LASSO penalty (see, e.g., Tibshirani (1996)) and the ==== penalty used in ridge regression. Since the ==== penalisation family does not satisfy the Oracle properties, different types of penalisation functions have been proposed in Fan and Li (2001) and Zou (2006).====The ==== norm was used in the classical spectral estimation in Chen and Donoho (1998) and Kato and Uemura (2012), where the authors obtained robust estimators against leakage for noisy and uneven data.====The present paper discusses a regularised version of the quantile periodogram proposed in Li (2012) to overcome its spectral leakage issue. Based on the ==== norm, the penalised quantile periodogram could be used as a sparse representation of the quantile spectra. The proposed estimator exhibits a good frequency domain representation of serial dependence. In addition, it can reduce the spectral leakage produced by the quantile periodogram as it shrinks some elements towards zero. This is illustrated using the Fisher test for processes with hidden periodicities. It is also demonstrated that the penalised quantile periodogram is related to the level-crossing spectrum. Thus, it shares the invariance to non-linear distortions.====The paper is organised as follows. A brief introduction to classical spectral analysis and the quantile periodogram is provided in Sections 2 Classical spectral analysis, 3 Quantile periodogram respectively; putting emphasis on definitions required for further developments. Our contribution is introduced in Section 4, where the penalised quantile periodogram is described. The asymptotic theory of the new estimator is presented in Section 5. In Section 6, comparative study on the performances of the different spectral estimators on simulated as well as real data is presented. The conclusion and future work are discussed in the last section.",Penalised quantile periodogram for spectral estimation,https://www.sciencedirect.com/science/article/pii/S0378375818302489,13 December 2019,2019,Research Article,233.0
Yang Hojin,"Department of Mathematics and Statistics, University of Nevada, Reno, NV 89557, USA","Received 3 December 2018, Revised 30 July 2019, Accepted 31 October 2019, Available online 24 November 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.10.005,Cited by (5),"The aim of this paper is to develop a random distribution on scalar regression model framework to account for the entire subject-specific distribution of the outcome, and to relate these distributions to a set of ","Let ==== be an A-dimensional random vector, ==== be a continuous random variable, and ====, ==== and ==== be a corresponding probability density function, cumulative distribution function, and quantile function defined on ====, respectively. We consider regression models in the situation where the distribution for ==== is a random object, so that it can be regressed as outcome on predictor ====. Ideally, random copies containing the information about the subject-specific distributions, for instance, ==== of ==== would have been observed, where the subject-specific distribution and ==== vector of covariates are denoted by ==== and ====, for each subject ====. However, there is a challenge that we observe a ==== vector of continuous responses, denoted by ==== as the random samples from ====, instead of observing ====, where ==== is a large number of repeated measurements for each subject. Thereby, the observed data consist of a set of ==== independent observations ====. Our goal is to estimate the regression relation ==== or equivalently ==== and investigate the association between predictor and random distribution.====Although this situation is common in reality, limited approaches have been available to deal with such data. Examples include cancer imaging data consisting of intensity measurements for a large number of pixels, climate data consisting of climate variables, such as daily temperature over a long duration, and various social and scientific data consisting of frequencies for a repeated event, for instance, frequency of popular name over a long time (Han et al., 2019). The naive approach is to compute the summary statistics such as mean, median, summary quantiles, and extreme values based on the subject-specific sample and relate them with predictor to understand the subject-specific distribution. The problem is that these features need to be determined at the initial stage, which can miss information for the entire subject-specific distribution. Hence, it is imperative to develop a method to account for the entire distribution to ensure visibility of all important insights.====Let ==== and ==== be each subject-specific quantile and density functions and ==== be a square integrable functional space, such that ====. Suppose ==== throughout this paper. We consider the following random distribution model given by ====where ==== is a random function such that ====, and ==== is an unknown ==== dimensional vector of functions to be estimated such that ==== for ====. Focusing on the quantile function ==== to understand the entire subject-specific distribution has several advantages compared to ==== and ====. Specifically, as described in Yang et al. (2019), for all subject-specific distributions, quantile functions ==== are defined on a fixed common domain ====, compared to ==== defined on ====, and have straightforward estimators by using a linear interpolation method between order statistics (Parzen, 2004) without any need for the bandwidth selections in the kernel density estimator (Silverman, 1986). Also, they have the capacity to compute any characteristics of the distribution defined by ==== such as distributional moments. With these advantages, we consider an approach to represent each subject-specific distribution as functional response, and consider the functional response regression (see review article by Morris (2015) for an overview) on a set of scalar covariates ==== through the model in (1). For notational convenience, we henceforth denote ==== for ==== and ==== for ====, respectively.====By treating ==== as functional response, a basis representation, which allows to borrow strength across ==== and various estimation methods for functional coefficients ==== in the projected space spanned by the basis function, are available. Much of the literature on functional linear regressions has concentrated on the functional principal component analysis, and penalization approaches for estimating functional coefficient ==== for ====
 (Ferraty and Vieu, 2006, Hall and Hosseini-Nasab, 2006, Hall et al., 2007, Morris, 2015, Yao and Lee, 2006, Ramsay and Silverman, 2006, Yao et al., 2005, Müller and Stadtmüller, 2005, Kong et al., 2016, Barber et al., 2017). However, these methods do not guarantee that the estimators for the entire quantile function will be non-decreasing functions in ====. Despite the fact that modeling on the monotone increasing function for each individual subject can be regarded as special case of the functional response model in (1), little works (Han et al., 2019, Yang et al., 2019) have been done to develop methodology to construct a model to account for the entire distribution, in terms of the functional regression framework.====The aim of this paper is to develop a random distribution on scalar regression framework accounting for the entire subject-specific distribution of the outcome and relating these distributions to a set of covariates. We develop a basis representation by approximating entire quantile functions of distributions measured at the grid points in [0,1] based on the non-decreasing basis function. Then, we estimate functional basis coefficients of covariates by using an empirical risk minimization method, with the restriction that all subject-specific quantile functions have the monotone increasing property in the original data space. Finally, we conduct a test procedure to identify the association between the distribution and a set of covariates.====This paper is organized in the following manner. In Section 2, we introduce the general estimation procedure. In Section 3, we systematically investigate the theoretical properties of our estimator. In Section 4, we conduct simulation studies to evaluate the finite-sample performance of our method. In Section 5, we apply our method to the analysis of Canadian weather data obtained from the historical database. We provide concluding remarks in Section 6.",Random distributional response model based on spline method,https://www.sciencedirect.com/science/article/pii/S0378375819301016,24 November 2019,2019,Research Article,234.0
"Sun Hanmei,Luan Yihui,Jiang Jiming","Shandong Normal University, China,Shandong University, China,University of California, Davis, USA","Received 22 May 2019, Revised 27 October 2019, Accepted 8 November 2019, Available online 18 November 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.11.001,Cited by (2),We develop a new method for classified mixed model prediction (CMMP). The original CMMP method (,"In small area estimation study, classified mixed model prediction (Jiang et al., 2018b) is a new method of mixed effect prediction based on the idea of matching a random effect associated with the new observations and one of the random effects associated with the training data. It was shown that CMMP improves predictive performance over the traditional regression-based prediction substantially. The method has potential application in such fields as precision medicine, where the primary interests are at the subject-level, and business, where prediction of business values at customer-level is of interest.====However, the current CMMP method does not utilize covariate information in its matching procedure; in other words, only the observed mean response is used in the matching. As a result, the probability of correct match is low, even though the predictive performance of CMMP may still be satisfactory. But, the performance can be improved, if the precision of the matching improves. In practice, there are often covariates at the group or cluster level, which are associated with the group-specific random effects. For example, consider the following nested-error regression (NER) model (Battese et al., 1988): ====, ====, ====, where ==== is the ====th observed (or sampled) response in the ====th cluster, ==== is a vector of covariates, ==== is a vector of unknown regression coefficients (the fixed effects), ==== is a cluster-specific random effect, and ==== is an additional error. On one hand, the random effect ==== is often used to “capture the un-captured”, that is, describe the variation which is not captured by the mean function, ====, at the cluster level. On the other hand, some components of ==== may be also at the cluster level, that is, they depend on ==== but not ====. It is natural to think that there may be association between ==== and some of the cluster-level components of ====; however, we do not know what kind of association it is except that it must be nonlinear (because, otherwise, it would be captured by ====). Nevertheless, if such covariate information can be utilized, the precision of the matching of random effects in CMMP, hence the (predictive) performance of CMMP can be improved. Sun et al. (2018) have considered this association and incorporated cluster-level covariates to identify the matching cluster. This idea has been proved to be successful for the clustered binary data.====Furthermore, the current CMMP method does not provide an uncertainty measure for the predictor, such as the mean squared prediction error (MSPE). The MSPE is extensively used in mixed model prediction (e.g., Jiang and Lahiri, 2006, Rao and Molina, 2015). However, when it comes to CMMP, because the latter involves a matching procedure which is non-differentiable, the traditional methods of deriving MSPE estimators, especially second-order unbiased MSPE estimators, do not apply.====The goal of this paper is two-fold. First, we are going to implement the idea described above regarding incorporating the covariate information in the CMMP matching. Second, we develop a simple, unified, Monte-Carlo assisted (Sumca) method for estimating the MSPE of CMMP, as noted above. Theoretical and empirical performance of the proposed new CMMP method and the Sumca MSPE estimator are carefully studied. Our results show that the new CMMP method can improve performance of CMMP substantially; furthermore, the Sumca estimator is second-order unbiased in estimating the MSPE of CMMP.====In Section 2 we introduce a new CMMP procedure that incorporates covariate information in matching the random effects. Estimation of MSPE of CMMP is considered in Section 3. Section 4 presents empirical results regarding performance of the new CMMP as well as that of the proposed MSPE estimator. A real data application is considered in Section 5. Proofs of theoretical results are deferred to Appendix.",A new classified mixed model predictor,https://www.sciencedirect.com/science/article/pii/S0378375819301028,18 November 2019,2019,Research Article,235.0
"Kabaila Paul,Welsh A.H.,Wijethunga Christeen","Department of Mathematics and Statistics, La Trobe University, Victoria 3086, Australia,Research School of Finance, Actuarial Studies and Statistics, The Australian National University, ACT 2601, Australia","Received 18 June 2019, Revised 29 October 2019, Accepted 29 October 2019, Available online 18 November 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.10.004,Cited by (3),We examine confidence intervals centered on the ==== model averaged estimator proposed by Buckland et al. (1997). We consider two formulas for the standard error of this estimator: the estimate put forward by Buckland et al. (1997) of their formula (9) and the square root of formula (6.12) of Burnham and Anderson (2002). We also consider four procedures that have been suggested in the literature for obtaining the half-width of the confidence interval from the chosen standard error. We assess the exact finite sample performances of the eight resulting confidence intervals using a simple testbed situation consisting of two nested linear regression models. This is done by deriving exact expressions for the confidence intervals and then for the coverages and scaled expected lengths of these confidence intervals. We also explore the performances of these confidence intervals in the limit as the residual degrees of freedom diverges to infinity.,"Buckland et al. (1997) proposed a frequentist model averaged estimator of a general scalar parameter that is a weighted average of estimators obtained under different models. The data-based model weights were constructed by exponentiating an information criterion, such as the Akaike Information Criterion (AIC), see Buckland et al. (1997, pp. 605–606). This kind of model weighting has been adopted in much of the later literature (Fletcher and Dillingham, 2011, Fletcher and Turek, 2011).====We examine confidence intervals centered on this model averaged estimator. We consider two formulas for the standard error of this estimator: the estimate put forward by Buckland et al. (1997) of their formula (9) and the square root of formula (6.12) of Burnham and Anderson (2002). We also consider four procedures that have been suggested in the literature for obtaining the half-width of the confidence interval from the chosen standard error. These procedures include the use of “model averaged degrees of freedom” as suggested by Lukacs et al. (2010) and the procedure proposed by Burnham and Anderson (2002, p. 164). As evidenced by the four ==== packages (briefly reviewed in Section 7) that implement these confidence intervals, the resulting eight confidence intervals seem to be widely used in practice in ecological statistics (see also Fletcher, 2018).====Hjort and Claeskens (2003, Section 4.3) and Claeskens and Hjort (2008, Section 7.5.1) criticized the confidence interval centered on a model averaged estimator and with half-width proportional to the standard error given by the estimate put forward by Buckland et al. (1997) of their formula (9). In the context of a general regression model, which includes linear regression and logistic regression as particular cases, they showed that in large samples the actual coverage probability of this confidence interval can fall substantially below the desired coverage. The analyses of Hjort and Claeskens (2003) and Claeskens and Hjort (2008) do not seem to have had much impact in applied fields. Although the conclusions are clear and hold for very general regression models, the results themselves are complicated, difficult to follow and, as large sample results, deemed not very relevant to practice.====Kabaila et al. (2016) set up a very simple testbed situation for evaluating the exact finite sample frequentist properties of model averaged confidence intervals. This testbed involves computing a confidence interval by model averaging over two nested linear regression models with unknown error variance, and then computing the coverage probability and scaled expected length properties of this confidence interval. The scaled expected length is the expected length of the model averaged confidence interval divided by the expected length of the standard confidence interval (with the same minimum coverage probability and for the same parameter) computed under the full model. Its computation gives far more insight than the coverage alone, allowing us for example to see when good coverage is obtained at the expense of excessive length. The testbed was used by Kabaila et al. (2016) to evaluate both the model averaged profile likelihood confidence interval of Fletcher and Turek (2011) and the model averaged tail area confidence interval of Turek and Fletcher (2012). This testbed was also used by Kabaila et al. (2017) to further evaluate the tail area confidence interval of Turek and Fletcher (2012). These papers showed that the tail area interval performs quite well provided that we do not put too much weight on the simpler of the two models. On the other hand, as Kabaila et al. (2016) illustrate numerically, just as with other profile likelihood based methods, the model averaged profile likelihood interval will perform poorly when the number of nuisance parameters is not small compared to the sample size.====Our main aim is to analyze the exact finite sample properties of the eight confidence intervals centered on the model averaged estimator in the testbed situation of two nested linear regression models with unknown error variance. We also explore the performances of these confidence intervals in the limit as the residual degrees of freedom diverges to infinity. We derive computationally convenient exact formulas for the finite sample coverage probabilities and scaled expected lengths of these confidence intervals. These formulas are valid for a wide range of model selection criteria, that includes the Bayesian Information Criterion (BIC), in addition to AIC. However, the results of Kabaila et al., 2016, Kabaila et al., 2017 and Kabaila (2018) suggest that BIC weights put too much weight on the simpler model, producing confidence intervals with poorer performance than the AIC weights. For this reason, the numerically computed results that we present for the coverage and scaled expected length are restricted to AIC weights.====We define the testbed situation and the parametrization we use in Section 2. The first confidence interval, with nominal coverage ====, centered on a model averaged estimator that we consider is denoted by ==== and is obtained as follows. The half-width of this interval is equal to the ==== quantile of the t distribution (with degrees of freedom equal to the residual degrees of freedom of the full model) multiplied by the standard error given by the estimate put forward by Buckland et al. (1997) of their formula (9).====The eight confidence intervals that we consider are similar and are assessed in the same way. Consequently, we first treat the confidence interval ==== in great detail. In Section 3, we obtain explicit expressions for the model averaged estimator and the standard error given by the estimate put forward by Buckland et al. (1997) of their formula (9), in the testbed situation. These expressions together enable us to obtain an explicit expression for the confidence interval ==== in the testbed situation. In Section 4, we then derive exact expressions for the coverage probability and scaled expected length of the confidence interval ==== in the testbed situation. These expressions have allowed us to, in effect, make findings that are valid for all design matrices, all parameters of interest and all parameters that, when set to zero, specify the simpler model. We present numerical results for small and medium-sized residual degrees of freedom ==== under various parameter settings in Section 5. We then consider the limiting case as ====, with the dimension of the regression parameter vector fixed, in Section 6 and compare these with the large sample results of Hjort and Claeskens (2003) in Section 6.5. In Section 7 we describe the differences that are needed to assess in the same way the performances of the other seven confidence intervals centered on a model averaged estimator. We conclude with a discussion in Section 8.",Finite sample properties of confidence intervals centered on a model averaged estimator,https://www.sciencedirect.com/science/article/pii/S0378375819300990,18 November 2019,2019,Research Article,236.0
"Alawieh Leen,Goodman Jonathan,Bell John B.","Mechanical Engineering Department, American University of Beirut, Beirut 1107 2020, Lebanon,Courant Institute of Mathematical Sciences, New York University, New York, NY 10012, USA,Center for Computational Sciences and Engineering, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA","Received 14 February 2019, Revised 23 October 2019, Accepted 8 November 2019, Available online 16 November 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.11.002,Cited by (4),A new algorithm is developed to tackle the issue of sampling non-Gaussian model parameter posterior probability distributions that arise from solutions to ,"Mathematical models are constructed to approximate physical systems, which are then used to make predictions about their behavior at a given set of inputs. This constitutes solving the forward problem. On the other hand, inverse problems involve using observations in order to make inferences about the model inputs, or even inferences about the form of the models themselves. Posing the inverse problem in a Bayesian setting allows one to regularize any ill-posedness present, to account for any source of noise in the observations and prior uncertainty in the forward models, and to subsequently infer a posterior probability distribution for the inputs or models, as opposed to inferring a single best set of input values or a single best model (Iglesias and Stuart, 2014, Tarantola, 2006, Tarantola, 2005). The inferred posterior probability distribution summarizes all available information about the inputs or models, including a quantifiable measure of uncertainty that could, in turn, be propagated forward to provide a measure of uncertainty in the resulting predictions (Marzouk et al., 2007, Martin et al., 2012).====For most inverse problems of interest, there is no analytical representation of the posterior probability distribution, so statistical information about the distribution is typically extracted using Markov chain Monte Carlo (MCMC) sampling techniques, which entails solving the forward problem several times. As a result, standard MCMC methods are known to struggle when the underlying posterior distribution is complex, and when the forward model is computationally expensive (Marzouk et al., 2007, Martin et al., 2012). There exist several approaches for tackling such challenges, some of which focus either on devising better sampling strategies (Martin et al., 2012, Morzfeld et al., 2018, Haario et al., 2006, Christen and Fox, 2005, Christen et al., 2010, Apte et al., 2007, Roberts et al., 1996, Dostert et al., 2006, Efendiev et al., 2006, Berger et al., 2003, Neal et al., 2011, Geweke and Tanizaki, 1999, Girolami and Calderhead, 2011, Cui et al., 2016), or on alleviating the cost of the forward model computations through developing reduced order models (Bui-Thanh et al., 2008a, Bui-Thanh et al., 2008b, Arridge et al., 2006, Wang and Zabaras, 2005, Rozza et al., 2007, Ghanem and Spanos, 1991, Le Maître and Knio, 2010), or cheaper surrogate representations that can (locally or globally) approximate the forward model (Habib et al., 2007, Heitmann et al., 2010, Narayanan and Zabaras, 2004, Higdon et al., 2008, Kennedy and O’Hagan, 2001, Kennedy et al., 2006, Bilionis et al., 2013, Blight and Ott, 1975, Koehler and Owen, 1996, Morris et al., 1993, Oakley and O’hagan, 2002, O’Hagan and Kingman, 1978, Conrad et al., 2016). In this work, we focus on the surrogate representation approach.====The approximation to the forward model could be either deterministic or statistical in nature. For instance, Gilks et al. (1995), Martino et al. (2015), Martino et al. (2018), Meyer et al. (2008), Cai et al. (2008) and Shao et al. (2013) aimed at constructing adaptive polynomial or spline approximations of the target density using deterministic regression techniques, and utilized these approximations as proposal densities for MCMC sampling. On the other hand, studies such as Busby (2009), Martino et al. (2017), OHagan (2006) and Wang et al. (2011) aimed at constructing a probabilistic approximation of the forward model (known as an emulator), which they then used either for Bayesian optimization purposes or for conducting uncertainty and sensitivity analyses. While our underlying goal here is analogous to the former set of studies, the approach that we take is similar to the latter set of studies in that we implement a probabilistic, kernel-based regression method in order to approximate the target density and then seek to sequentially improve the approximation as further explained below in more details.====In this paper, we adopt the surrogate model approach, where we aim at constructing a faithful approximation of the posterior probability density using Gaussian process regression (GPR). Given that the forward model is an expensive black box computer simulation, we address the question of how to efficiently select data points for training the Gaussian Process (GP), such that we obtain a relatively accurate surrogate model using the minimum number of training points. To this end, we take an active learning approach, and iteratively build our surrogate surface. We develop a data selection criterion to decide, at each iteration stage, where in input space we should run the computer simulation next. Several previous studies have tackled a similar problem to the one presented here  (Cohn et al., 1996, Cohn, 1996, Cohn, 1997, Seo et al., 2000, MacKay, 1992, Gramacy et al., 2004, Gramacy and Lee, 2008, Gramacy and Lee, 2009, Gramacy and Apley, 2015, Seeger et al., 2003, Paass and Kindermann, 1995, Rasmussen, 2003, Kandasamy et al., 2017, Sacks et al., 1989, Currin, 1988, Currin et al., 1991, Preuss and von Toussaint, 2018, Jones et al., 1998, Martino et al., 2017, Busby, 2009, Christen and Sans, 2011), however in most of these studies, the data selection criterion implemented is either not appropriate for our current purposes due to its associated computational expense, or is coupled with an unnecessarily complicated GP model. For example, in Cohn (1996), Cohn (1997), Seo et al. (2000), MacKay (1992) and Gramacy et al. (2004), the selection criteria aim to minimize the predictive error (or simply the predictive variance) of the surrogate model, but they require estimating integrals over the entire input space or computing local sensitivity derivatives, which could be computationally expensive or not even readily feasible. Rasmussen (2003) takes a similar approach to ours, in that he couples an MCMC sampler (specifically, Hamiltonian Monte Carlo sampler) with a GP model, in order to obtain potential data point candidates for training the GP. However, he offers no mathematical justification to show how he arrived to the specific selection heuristic used for choosing the data point candidates. Moreover, Rasmussen’s algorithm seems to require computing first and second order covariance derivatives for optimizing the GP model, as well as computing the forward model and its partial derivatives several times at each iteration stage. On the contrary, beyond solving the forward model at the input point selected to add to the training set, the algorithm we develop in this paper does not require solving the forward problem or any of its derivatives while seeking potential data point candidates. In addition to that, no optimization of the GP model is required at any stage of the algorithm. In fact, we will demonstrate that, despite choosing a naive unoptimized GP model, our algorithm can still construct a faithful surrogate model. Kandasamy et al. (2017) suggest the same data selection criterion as the one we derive here, however in order to locate the optimal data point that maximizes the criterion, they evaluate the criterion on a coarse grid in input space, which becomes impractical in high dimensions. We circumvent this hurdle by coupling our GP model with an MCMC sampler, which allows us to cheaply locate the optimal data point.====This paper is organized as follows. In Section 2 we start by briefly introducing Bayesian inference and Gaussian process regression, and then move on to develop the algorithm for the optimal selection of GP training data. Implementation of the developed algorithm on a number of network toy problems of increasing dimensionality is presented in Section 3. Major conclusions are finally summarized in Section 4.",Iterative construction of Gaussian process surrogate models for Bayesian inference,https://www.sciencedirect.com/science/article/pii/S037837581930103X,16 November 2019,2019,Research Article,237.0
"Li Chang,Zhang Chongqi","Department of Economics and Statistics, Guangzhou University, 230 Wai Huan Xi Road, Guangzhou Higher Education Mega Center, Guangzhou, PR China","Received 28 January 2019, Revised 16 October 2019, Accepted 19 October 2019, Available online 15 November 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.10.003,Cited by (3),"A mixture canonical polynomial with spline, which consists of several grafted polynomial submodels, has attracted a great deal of researchers in recent years. There is a typical mixture polynomial with spline, in which the contribution of one ingredient proportion to the response variable is different from that of the others. Much of the previous work mainly focuses on the D-optimal design for this kind of model, in large part because of the ","Mixture experiments (Scheffe, 1963) are experiments with two or more components blending together, and the response depends only on the relative proportions of the mixture components. The experimental region of a mixture experiment with q components is a (q-1)-simplex ==== given by ====Mixture experiments have a wide range of applications in food processing, pharmaceutical industry, chemical, agricultural, and other industries. Some typical recent research includes: Furlanetto et al. (2011) discussed optimal mixture experiments models in the pharmaceutical industry, Huang et al. (2009) constructed robust D- and A-optimal designs for mixture experiments on linear and quadratic models, Huang and Huang (2009) proposed ==== optimal designs for linear log contrast models for mixture experiments. Zhang et al. (2012) presented a methodology to solve compound objective optimal designs for mixture experiments. Goos and Donev, 2005, Goos and Donev, 2007 introduced minimum support mixture experimental designs.====Let ==== be a general linear regression model on ====, where ==== is the response variable, ==== is a known vector of functions for ====, and ==== is the unknown parameter vector. An experimental design ==== for a mixture model is a probability measure on ==== with finite support points (each distinct design point is a support point for the design). That is, if we have ==== support points: ====, ====, denote the weight on ==== as ====, then ====The statistical properties of a design ==== are measured by its information matrix: ====.====The A-optimal design aims to minimize the sum of the variance of the components of the least squares estimator. If there exists a design ==== such that ====for any design ====, then ==== is an A-optimal design. Here ==== is the inverse of the information matrix of the design ====. By the equivalence theorem of Kiefer, 1974, Kiefer, 1975, a design ==== is A-optimal iff ====and ==== at the support points of the design ====. The term ==== is usually denoted as ====.====In some experiment design problems, the response variable ==== can only be represented by a piecewise smooth function of the input variable ====. In this case, we should divide the experimental region into several subsets, and provide a different function of the input variable ==== on each subset. This kind of experiment is named experimental design for a polynomial with spline. Some typical researches in this area are Dette et al. (2011), Dette and Melas (2010), Dette et al. (2006) and Heiligers (1999).====There is a typical mixture experimental design for a polynomial with spline, in which the contribution of one component proportion, say, ====, is different from that of the others to the response variable. When ==== is less than a threshold value ====
 (==== is a constant, ====), the interaction of ==== with other components in the mixture system is unsensitive and can be neglected. In Zhang and Peng (2012), the authors proposed the D-optimal design for mixture polynomials with spline. The model of canonical polynomials with spline is in the form: ====where ====This type of the model is widely used in medical science. A typical application is in the prescription of traditional Chinese medicine: in a mixed medicine prescription, some kinds of Chinese medicines are not easily to have interaction with other medicines due to their intrinsic characteristic (Tang et al., 2015, Yang et al., 2015). Denote the proportion of this kind of medicine in a prescription as ====, then if ==== is less than a threshold value (denote it as ====), the interaction of this kind of medicine with other medicine can be neglected (Zhou et al., 2018, Yu and Cao, 2017).====Besides, since ==== is the threshold value of ====, if ==== far outweigh ==== or the proportion of other medicines, it will disrupt the balance of the prescription, and produce certain side-effects. In view of the summary of traditional Chinese medicine prescriptions, we take the common upper bound for ====: ====.",A-optimal designs for quadratic mixture canonical polynomials with spline,https://www.sciencedirect.com/science/article/pii/S0378375819300989,15 November 2019,2019,Research Article,238.0
"Hoyos-Argüelles Ricardo,Nieto-Barajas Luis","Direction of Financial System Information, Banco de México, Mexico,Department of Statistics, ITAM, Mexico","Received 18 October 2018, Revised 12 August 2019, Accepted 2 September 2019, Available online 12 November 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.015,Cited by (7),An ====. Inference on the model is done under a ==== and for some prior specifications we are able to perform an independence test. Properties of the model are illustrated with a simulation study as well as with a real dataset.,"Let ==== be a continuous, strictly decreasing function from ==== to ==== such that ====. Let ==== be the inverse or the pseudo-inverse of ====, where the latter is defined as zero for ====. If ==== as ==== the generator is called strict. For instance, ====, is an example of a strict generator. An Archimedean copula ==== with generator ==== is a function from ==== to ==== defined as ====A further requirement for (1) to be well defined is that ==== must be convex (e.g. Nelsen, 2006).====There are many properties that characterise Archimedean copulas, for instance, they are symmetric, associative and their diagonal section ==== is always less than ==== for all ====. Generators ==== are usually parametric families defined by a single parameter. Most of them are summarised in Nelsen (2006, Table 4.1) and few of them are also included in Table 1.====Association measures induced by Archimedean copulas are a function of the generator. For instance, Kendall’s tau becomes ====where ==== denotes the right derivative of ==== at ====.====In this work we propose a Bayesian semiparametric generator defined through a quadratic spline. Within a survival analysis context, we model the first derivative of a hazard rate function with a piecewise constant function. The hazard rate and the cumulative hazard functions become linear and quadratic continuous functions, respectively. The induced survival function is used as an inverse generator for an Archimedean copula. Convexity constraints are properly addressed and inference on the model is done under a Bayesian approach.====Other studies on semiparametric generators for Archimedean copulas can be found in Genest and Rivest (1993) where their model is based on an empirical Kendall’s process. A new approach and extensions of this latter methodology can be found in Genest et al. (2011). In Guillote and Perron (2015) the model arises from the one-to-one correspondence between an Archimedean generator and a distribution function of a nonnegative random variable. In particular they use a mixture of Pólya trees as a prior for the corresponding distribution function under a Bayesian nonparametric approach. In a work more related to ours, Vandenhende and Lambert (2005) use the relationship between quantile functions and Archimedean generators to define a semiparametric generator by supplementing a parametric generator with ==== dependence parameters. Differing to their work, our model is not based on any parametric generator and the Kendall’s tau can take values on the whole interval ====.====The contents of the rest of the paper is as follows. In Section 2 we present our proposal and characterise its properties. In Section 3 we provide details of how to make posterior inference under a Bayesian approach. In Section 4 we illustrate the performance of our model with a simulation study as well as with a real data set. We conclude with some remarks in Section 5.====Before proceeding we introduce notation: ==== denotes a continuous uniform density on the interval ====; and ==== denotes a normal density with mean ==== and variance ====.",A Bayesian semiparametric Archimedean copula,https://www.sciencedirect.com/science/article/pii/S0378375819301004,12 November 2019,2019,Research Article,239.0
"Salles Gabriel,Bordes Laurent","Univ. Pau & Pays de l’Adour, CNRS, E2S UPPA, Laboratoire de Mathématiques et de leurs Applications de Pau - IPRA, UMR5142, 64000 Pau, France","Received 11 April 2019, Revised 28 August 2019, Accepted 22 September 2019, Available online 23 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.014,Cited by (11),"A system is considered, which is deteriorating over time according to a non homogeneous gamma process with unknown parameters. The system is subject to periodic and instantaneous imperfect maintenance actions (repairs). Each imperfect repair removes a proportion ==== of the accumulated degradation since the previous repair. The parameter ==== hence appears as a measure for the maintenance efficiency. This model is called arithmetic reduction of degradation of order 1. The system is inspected right before each maintenance action, thus providing some multivariate measurement of the successively observed deterioration levels. Based on these data, a semiparametric estimator of ====’s, which depends on the data. Under technical assumptions, consistency results are obtained, with surprisingly high convergence rates (up to exponential). The case where several i.i.d. systems are observed is next envisioned. Consistency results are obtained for the efficiency estimator, as the number of systems tends to infinity, with a convergence rate that can be higher or lower than the classical square root rate. Finally, the performances of the estimators are illustrated on a few numerical examples.","Safety and dependability are crucial issues in many industries (such as, e.g., railways, aircraft engines or nuclear power plants), which have led to the development of the so-called reliability theory. For many years, only lifetime data were available and the first reliability studies were focused on lifetime data analysis (see, e.g., Meeker and Escobar, 1998), which still remains of interest in many cases. In that context and in case of repairable systems with instantaneous repairs, successive failure (or repair) times appear as the arrival points of a counting process, and failures hence correspond to recurrent events. As for the type of possible repairs, typical classical models are perfect (As-Good-As-New) and minimal (As-Bad-As-Old) repairs, leading to renewal and non homogeneous Poisson processes as underlying counting processes, respectively (see Barlow and Proschan, 1965). The reality often lies in-between, leading to the class of imperfect repairs. Many models have been envisioned in the literature for their modeling, such as, e.g., virtual age models introduced by Kijima (1989) and further studied in de Toledo et al. (2015) and Doyen and Gaudoin (2004), geometric processes (Lam, 2007) (extended in Bordes and Mercier, 2013) or models based on reduction of failure intensity (de Toledo et al., 2015, Doyen and Gaudoin, 2004). See, e.g., Doyen et al. (2017) for a recent account and extensions of such models. See also Pham and Wang (1996) for more references and other models.====Nowadays, the development of online monitoring and the increasing use of sensors for safety assessment make it possible to get specific information on the health of a system and on its effective evolution over time, without waiting for the system failure. This information is often synthesized into a scalar indicator, which can for instance stand for the length of a crack, the thickness of a cable, the intensity of vibrations, corrosion level, etc. This scalar indicator can be considered as a measurement of the deterioration level of the system. The evolution of this deterioration indicator over time is nowadays commonly modeled through a continuous-time and continuous-state stochastic process, which is often considered to have an increasing trend. Classical models include inverse Gaussian (Ye and Chen, 2014) or Wiener processes with trend (Hu et al., 2015, Liu et al., 2017, Zhang et al., 2015), which are also quite common in many other fields out of reliability theory, such as finance, insurance or epidemiology. This paper focuses on gamma processes, which are widely used since they were introduced in the reliability field by Çinlar et al. (1977). See Van Noortwijk (2009) and its references for a large overview.====In order to mitigate the degradation of the system over time and extends its lifetime, preventive maintenance actions can be considered, in addition to corrective repairs which are performed at failure. In the context of deteriorating systems, many preventive maintenance policies from the literature consider condition-based maintenance (CBM) actions, where the preventive repair is triggered by the reaching of a preventive maintenance threshold by the deterioration level. In that context, “most of the existing CBM models have been limited to perfect maintenance actions”, as noted by Alaswad and Xiang (2017) (see also Zhang et al., 2015). Some imperfect repair models are however emerging in the latest reliability literature, in this new context of deteriorating systems, see Alaswad and Xiang (2017) for a recent review. Some models are based on the notion of virtual age previously introduced in the context of recurrent events (see, e.g., Giorgio and Pulcini, 2018, Mercier and Castro, 2013), where the system is rejuvenated by a maintenance action. Other models consider that an imperfect repair reduces the deterioration level of the system, such as Khatab et al. (2018), Letot et al. (2017), Nicolai et al. (2009), Ponchet et al. (2011) and Wu et al. (2015), which can be accompanied by some increase in the deterioration rate, as in Do et al. (2015). Also, some papers consider that the efficiency decreases with the number of repair (see, e.g., Liu et al., 2016, Zhang and Xie, 2017), and further studies, as in Huynh (2019), deal with imperfect maintenance models such that (i) repairs have a random efficiency (ii) the deterioration rate increases with the number of repairs. In all these papers however, the main point mostly is on the optimization of a maintenance policy, including these imperfect maintenance actions together with perfect repairs (replacements). Up to our knowledge, very few papers from the literature deal with statistical issues concerning imperfect repair models for deteriorating systems, except from Zhang et al. (2015), where the authors suggest a maximum likelihood method for estimating the parameters of the Wiener process together with an iterative procedure based on a Kalman filter for the different factors implied in successive imperfect repairs. This estimation procedure is developed in a fully parametric context and validated on simulated data, without any study of the asymptotic properties of the estimators.====The evaluation of the maintenance actions efficiency is mainly used for maintenance policies optimization. Once the repair efficiency has been estimated, the future behavior of the maintained system can be predicted, which allows to adapt (optimize) the periodicity of the maintenance actions and efficiently plan a general overhaul. From a safety point of view, the principal inquiry is to ensure that the maintenance actions are effective enough to keep with a high probability the degradation level below a fixed threshold (safety level). As long as this safety level is not reached, the maintenance actions must be adjusted, either by adapting their periodicity or by improving their efficiency (if possible). Of course, apart from the previous safety concern, the maintenance costs are another issue. As an example, in Wu et al. (2015), the costs minimization is based on the monitoring time and on the imperfect maintenance efficiency. In Huynh (2019), the author considers a threshold for the degradation, beyond which an imperfect maintenance is performed. The optimization is made with respect to this threshold, the inspections periodicity and the repairs efficiency. See, e.g., both papers cited above and their reference for an overview on maintenance policies optimization.====This paper focuses on a specific imperfect repair model, where each maintenance action reduces the deterioration level of the system. The model was first introduced in Castro and Mercier (2016) and further studied in Mercier and Castro (2019), where it was called Arithmetic Reduction of Degradation model of order ==== (ARD====). Mimicking Arithmetic Reduction of Age (ARA====) and Arithmetic Reduction of Intensity (ARI====) models of order ==== developed by Doyen and Gaudoin (2004) in the context of recurrent events, the idea of an ARD==== model is that a maintenance action removes a proportion ==== of the degradation accumulated by the system from the last maintenance action (where ====). The parameter ==== appears as a measure of the maintenance efficiency, which lies between As-Good-As-New when ==== and As-Bad-As-Old when ====. Along the same lines as Castro and Mercier (2016), Doyen and Gaudoin (2004) and Mercier and Castro (2019), the maintenance actions efficiency is here assumed to be fixed and independent of the intrinsic degradation.====This paper is concerned with the development and study of an estimation procedure for the maintenance efficiency parameter ====, in the context of a gamma deteriorating system subject to periodic ARD==== imperfect repairs. Observations are lead on just before each maintenance action. Considering ==== successive repairs, this leads to multivariate data, from where an estimator of ==== is proposed. The idea of this estimator has come from a preliminary study in a parametric framework based on the maximum likelihood method, where we have observed that the minimum of admissible ====’s has quite an interesting behavior, getting quickly very close to the unknown efficiency parameter when ==== increases. This has led to the proposition of an original estimator for ====, which depends only on the data, and not on the shape function and rate parameter of the gamma process, leading to a semiparametric framework. Under technical assumptions, the strong consistency of this new estimator is shown, as the number ==== of repairs tends towards infinity. Also, the convergence rate is proved to be surprisingly high, and can even reach an exponential speed in some cases. This estimator hence appears to be super consistent (under specific conditions). This is illustrated on simulated data at the end of the paper, where we provide two examples for which we observe that roughly ==== of the estimates are exact at the machine precision level (====) as soon as ==== and ====, respectively, with a mean error below 10==== in both cases. The study is next extended to the case where ==== independent and identical systems are observed (==== times each). A similar semiparametric estimator is proposed for the (common) maintenance efficiency and the strong consistency is proved to hold as ==== tends towards infinity, no matter the fixed value of ==== and out of any technical condition requirement. The convergence rate is studied, which is shown to depend on the shape function of the gamma process and on the maintenance period, leading to a speed that can be either slower or faster than ====, according to the case.====The outline of this paper is as follows. The framework is specified in Section 2, which covers the gamma deterioration process, the ARD==== imperfect repair model and the observation scheme. Section 3 is devoted to the study of the semiparametric estimator in the case where one single system is observed, which includes its asymptotic properties when the number of repairs tends towards infinity. Section 4 deals with the extension to several systems and considers the asymptotic properties with respect to the number of observed systems. Some illustrations of the estimator performances are provided in Section 5 and conclusions are formulated in Section 6.",Semiparametric estimate of the efficiency of imperfect maintenance actions for a gamma deteriorating system,https://www.sciencedirect.com/science/article/pii/S0378375819300977,23 October 2019,2019,Research Article,240.0
"Bugni Federico A.,Caner Mehmet,Bredahl Kock Anders,Lahiri Soumendra","Department of Economics, Duke University, United States of America,Department of Economics, North Carolina State University, United States of America,Department of Economics, University of Oxford and Aarhus University, CREATES, United Kingdom and Denmark,Department of Statistics, North Carolina State University, United States of America","Received 15 May 2018, Revised 27 June 2019, Accepted 30 September 2019, Available online 23 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.013,Cited by (2),"This paper considers inference in a partially identified moment (in)equality model with many moment inequalities. We propose a novel two-step inference procedure that combines the methods proposed by Chernozhukov et al. (2018a) (Chernozhukov et al., 2018a, hereafter) with a first step moment inequality selection based on the Lasso. Our method controls asymptotic size uniformly, both in the underlying parameter and the data distribution. Also, the power of our method compares favorably with that of the corresponding two-step method in Chernozhukov et al. (2018a) for large parts of the parameter space, both in theory and in simulations. Finally, we show that our Lasso-based first step can be implemented by thresholding standardized sample averages, and so it is straightforward to implement.","This paper contributes to the growing literature on inference in partially identified econometric models defined by many unconditional moment (in)equalities, i.e., inequalities and equalities. Consider an economic model with a parameter ==== belonging to a parameter space ====, whose main prediction is that the true value of ====, denoted by ====, satisfies a collection of moment (in)equalities. This model is partially identified, i.e., the restrictions of the model do not necessarily restrict ==== to a single value, but rather they constrain it to belong to a certain set, called the identified set. The literature on partially identified models discusses several examples of economic models that satisfy this structure, such as selection problems, missing data, or multiplicity of equilibria (see, e.g., Manski, 1995 and Tamer, 2003).====The first contributions in the literature of partially identified moment (in)equalities focus on the case in which there is a fixed and finite number of moment (in)equalities, both unconditionally==== ==== and conditionally.==== ==== In practice, however, there are many relevant econometric models that produce a large set of moment conditions (even infinitely many). As the literature shows (e.g. Menzel, 2009, Menzel, 2014), the associated inference problems cannot be properly addressed by an asymptotic framework with a fixed number of moment (in)equalities.==== ==== To address this issue, Chernozhukov et al. (2018a) (hereafter referred to as CCK18) obtain inference results in a partially identified model with ==== moment inequalities.==== ==== According to this asymptotic framework, the number of moment inequalities, denoted by ====, is allowed to be larger than the sample size ====. In fact, the asymptotic framework allows ==== to be an increasing function of ==== and even to grow at exponential rates. Furthermore, CCK18 allow their moment inequalities to be “unstructured”, in the sense that they do not impose restrictions on the correlation structure of the sample moment conditions.==== ==== For these reasons, CCK18 represents a significant advancement relative to the previous literature on inference in moment inequalities. In this paper, we generalize their econometric framework by also allowing for the presence of many unstructured moment equalities, i.e., we consider a partially identified model with many moment (in)equalities. This generalization is relevant in practice as applications of partially identified econometric models often include both moment inequalities and equalities.====This paper builds on the inference methods proposed in CCK18. Their goal is to test whether a collection of ==== moment inequalities simultaneously holds or not. Their hypothesis test compares a test statistic given by the maximum of ==== Studentized statistics, with suitable critical values that are based on three methods: self-normalization, multiplier bootstrap, and empirical bootstrap. In addition, each one of these can be implemented with an optional first-stage moment selection procedure with the objective of detecting slack moment inequalities, thus increasing the statistical power. If the first stage is used, it can be in turn based on the same three methods: self-normalization, multiplier bootstrap, and empirical bootstrap. So, for example, one possible critical value is one based on empirical bootstrap with a first-step moment selection procedure based on self normalization. According to their simulation results in CCK18, using a first-step moment selection procedure can produce significant power gains.====We contribute to this literature by proposing new critical values for the hypothesis testing problem in CCK18. Our critical values are the result of combining the approximation methods in CCK18 in the second step (i.e. self-normalization, multiplier bootstrap, or empirical bootstrap), with a novel first-step moment inequality selection procedure based on the Lasso. Besides proposing a different first-step moment selection procedure, our inference method uses a second step that can ignore the first-step moment inequality selection, thus increasing statistical power. We refer to the resulting hypothesis test as two-step Lasso-based inference methods.====On the theoretical front, we contribute by investigating the asymptotic properties of our two-step Lasso inference methods. We establish the following results. First, we provide conditions under which our methods are uniformly valid, both in the underlying parameter ==== and the distribution of the data. According to the literature in moment (in)equalities, obtaining uniformly valid asymptotic results is important to guarantee that the asymptotic analysis provides an accurate approximation to finite sample results.==== ==== Second, by virtue of results in CCK18, all of our proposed tests are asymptotically optimal in a minimax sense. Third, we compare the power of our methods to the corresponding one in CCK18, both in theory and in simulations. Since our two-step procedure and the corresponding one in CCK18 are based on the same approximations, our power comparison is a comparison of the Lasso-based first step vis-à-vis the ones in CCK18. On the theory front, we obtain a region of underlying parameters under which the power of our method dominates that of CCK18. We also conduct extensive simulations to explore the practical consequences of our theoretical findings. Our simulations indicate that a Lasso-based first step is usually as powerful as the one in CCK18, and can sometimes be more powerful. Fourth, we show that our Lasso-based first step can be implemented by thresholding standardized sample averages (see Lemma 3.2) and so it is straightforward to implement. At the same time, this implies that our Lasso-based first step coincides with a self-normalization first step with a very specific choice of the tuning parameter. This choice of tuning parameter is an important aspect of our contribution, as it is responsible for the aforementioned power advantages.====As in any other Lasso problem, the validity of our method depends on the appropriate choice of the Lasso tuning parameter. One limitation of our approach is that the validity of our Lasso-based first step is shown for a tuning parameter choice that depends on unknown population moments (see Eq. (3.4)). This is not specific to our problem as it is a characteristic feature of much of the Lasso literature. To implement our Lasso-based first step in practice, we propose replacing population moments with their sample counterparts. Our simulations show that this choice delivers excellent results in simulations, in terms of both size control and power. However, we acknowledge that our current theoretical results do not take into account the sample variability of our empirical version of the tuning parameter. We consider that a rigorous handling of these issues is beyond the scope of this paper.====The Lasso was first proposed in the seminal contribution by Tibshirani (1996) as a regularization technique in the linear regression model in which the number of regressors is allowed to exceed the sample size. Since then, this method has found wide use as a dimension reduction technique in large dimensional models with strong theoretical underpinnings.==== ==== It is precisely these powerful shrinkage properties that serve as motivation to consider the Lasso as a procedure to separate out and select binding moment inequalities from the non-binding ones in a partially identified model with many moment (in)equalities.====This paper proposes using the Lasso to select moments in a partially identified moment (in)equality model. In the context of point identified problems, there is an existing literature that proposes the Lasso to address estimation and moment selection in GMM settings. In particular, Caner (2009) introduces Lasso type GMM-Bridge estimators to estimate structural parameters in a general model. The problem of selection of moments in GMM is studied in Liao (2013) and Cheng and Liao (2015). In addition, Caner and Zhang (2014) and Caner et al. (2016) provide a method to estimate parameters in a GMM model with diverging number of moments/parameters and to select valid moments among many valid or invalid moments, respectively. In addition, Fan et al. (2015) consider the problem of inference in high dimensional models with sparse alternatives. Finally, Caner and Fan (2015) propose a hybrid two-step estimation procedure based on Generalized Empirical Likelihood, where instruments are chosen in a first-stage using an adaptive Lasso procedure.====The remainder of the paper is organized as follows. Section 2 describes the inference problem and introduces our assumptions. Section 3 introduces the Lasso as a method to distinguish binding moment inequalities from non-binding ones and Section 4 considers inference methods that use the Lasso as a first-step moment selection procedure. Section 5 compares the power properties of inference methods based on the Lasso with the ones in the literature. Section 6 provides evidence of the finite sample performance using Monte Carlo simulations. Section 7 concludes. Proofs of the main results and several intermediate results are reported in the Appendix.====Throughout the paper, we use the following notation. For any set ====, ==== denotes its cardinality, and for any vector ====, ====.",Inference in partially identified models with many moment inequalities using Lasso,https://www.sciencedirect.com/science/article/pii/S0378375819300941,23 October 2019,2019,Research Article,241.0
"Lu Renjie,Yu Philip L.H.","Department of Statistics and Actuarial Science, The University of Hong Kong, Hong Kong","Received 7 January 2019, Revised 26 July 2019, Accepted 30 September 2019, Available online 23 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.012,Cited by (0),"In this paper, we study a new model called the smooth buffered autoregressive (SBA) model. A sufficient condition is given for geometric ====. A conditional least squares (CLS) estimation procedure is discussed, and consistency and normality of the estimators are derived. We investigate the effectiveness of our methods by simulation studies. Two applications are considered: annual sunspot number and the U.S. unemployment rate.","Classical linear autoregressive (AR) models provide a fundamental framework in many fields of applied researches such as business, economics, finance and environment studies. Although they are useful and important, it has been found that nonlinearity often exists to explain structural changes in many economic and financial time series (Clements et al., 2004, Hansen, 2011) and hence linear models like the AR models are not suitable to handle such nonlinearity. The threshold-type models introduced by Tong, 1978, Tong, 1993 are one of the most popular nonlinear time series models which can mimic nonlinear features such as limit cycles and time-irreversibility. See also Tsay (1998). The traditional threshold autoregressive model (TAR) assumes that the probability structure of a time series switches from one regime to another immediately once the threshold variable crosses up or down a certain threshold. However, it is common to observe a delay of the regime switching, often called the hysteretic phenomenon, in many fields including economics, engineering and biology. For example, during a recession, struggling companies may start taking layoffs in order to survive when an economic index attains its low value ====. The companies will restart hiring if and only if the economic index equals a high value ==== (which shows that the current economy is strong). Therefore, during the interval between ==== and ====, the action of hiring or not hiring remains unchanged. The above case implies that the regime switching mechanism should have a buffered region (i.e., in this region the probability structure of a process stays the same). As a result, the intermediate switching mechanism inherited from TAR is not reasonable.====To describe such phenomenon and demonstrate some successful applications, Li et al. (2015) first proposed the hysteretic (or buffered) AR model with regime indicator ====, ====where ====, ====, ====, ====, integer ==== is the delay parameter, ==== are the threshold parameters of the model and the ====’s are independent and identically distributed random variables with mean zero and variance one. Note that from the definition of ====, it can be rewritten as ==== where ==== is an indicator function. They applied the above model to the sunspot series as well as the U.S. GNP series, and found that for both series the buffered AR (BAR) model performed better than TAR in terms of AIC and BIC. Zhu et al. (2014) developed a quasi-likelihood ratio test to detect the existence of a buffering regime. In their application to the GNP series, they found that a significant buffering regime exists in the GNP series, and argued that the delay of the regime switching was possibly because the government might not have a large or quick response to a moderate growth in GNP, and hence the structure of the GNP series was most likely unchanged shortly after a market shock. Lo et al. (2016) and Zhu et al. (2017) extended the buffered models to the conditional variances and introduced the buffered GARCH and buffered AR-GARCH models. Their empirical examples to a U.S. stock and several exchange rates highlighted the importance of buffered GARCH models in describing the asymmetric behavior of volatility in financial time series.====It is easy to see that the classical threshold autoregressive models use the so-called hard threshold in the sense that there will be a sudden jump from one regime to another. Smooth transition autoregressive (STAR) models are often used to tackle this problem by using a soft threshold, i.e., a smooth transition function governing the switching between the regimes. Luukkonen et al. (1988) addressed the problem of testing linearity against STAR alternatives. They proposed an approximate Lagrange Multiplier test by using a Taylor series expansion to approximate the transition function. See Terasvirta (1994) for other inference problems for STAR models. For more reviews pertaining to the recent development of STAR models, see van Dijk et al. (2002), Medeiros and Veiga (2005) and Hubrich and Terasvirta (2013). So far a hard threshold has been used in the buffered autoregressive models, in this paper we therefore introduce the smooth transition structure in describing the buffered regime switching.====The rest of this paper is organized as follows. In Section 2, we first describe the so-called smooth buffered autoregressive (SBA) model, discuss a conditional least squares method for parameter estimation and derive the relevant asymptotic properties of the least square estimates. Simulation studies are considered in Section 3. In Section 4, we apply the proposed model to the annual sunspot number data and the U.S. monthly unemployment rate. Section 5 concludes.",Smooth buffered autoregressive time series models,https://www.sciencedirect.com/science/article/pii/S037837581930093X,23 October 2019,2019,Research Article,242.0
"Zhang Jia,Chen Xin","School of Statistics, Southwestern University of Finance and Economics, Chengdu 611130, China,Department of Statistics and Data Science, Southern University of Science and Technology, Shenzhen 518055, China","Received 25 February 2019, Revised 2 September 2019, Accepted 2 October 2019, Available online 18 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.10.001,Cited by (4),"Principal component analysis (PCA) is widely used in various fields to reduce high dimensional data sets to lower dimensions. Traditionally, the first a few principal components that capture most of the variance in the data are thought to be important. Tipping and Bishop (1999) introduced probabilistic principal component analysis (PPCA) in which they assumed an isotropic error in a ====. Motivated by a general error structure and incorporating the novel idea of “envelope” proposed by Cook et al. (2010), we construct principal envelope models (PEM) which demonstrate the possibility that any subset of the principal components could retain most of the sample’s information. The useful principal components can be found through maximum likelihood approaches. We also embed the PEM to a factor model setting to illustrate its reasonableness and validity. Numerical results indicate the potentials of the proposed method.","Principal component analysis (PCA) is a popular data processing and dimension reduction technique. First introduced by Pearson (1901), PCA has a long history and is now widely used in various areas, including agriculture, ecology, genetics and economics. PCA seeks uncorrelated linear combinations of the original variables that capture maximal variance. Suppose we have ==== observations on ==== features ====. Let ==== denote the ====th observation, ====, and ==== be the vector variable. Let ==== denote the centered observation vectors, ====, and ==== be the ==== centered data matrix with row ==== and rank ====. Since there is no response involved, this article is mainly about unsupervised multivariate dimension reduction method.====Let ==== be the eigenvectors of the sample covariance matrix ==== corresponding to its non-zero eigenvalues. Without loss of generality, ==== are ordered by descending eigenvalues. Let ==== be the non-zero eigenvalues with descending order. The principal component directions ====, ====, can also be obtained by maximizing ==== successively subject to ==== and ====, ====. This demonstrates that PCA pursues the linear combinations of the original variables such that the derived variables capture maximal variance. The sample variance of the ====th principal component (PC) equals ==== (Anderson, 1963). There are many methods for selecting the number of principal components, depending on specific requirements for different applications, see Jolliffe (2002).====PCA enjoys high popularity, but it is not based on a probability model. Tipping and Bishop (1999) introduced probabilistic principal component analysis (PPCA) in which the first few principal component directions can be obtained through maximum likelihood estimation. However, the assumption of an isotropic error in the PPCA model is quite limited. By assuming a general error structure and incorporating the novel “envelope” idea of Cook et al. (2010), we establish principal envelope models that encompass PPCA as a special case and demonstrate the possibility that any subset of principal components could retain most of the sample’s information. Since the introduction of “envelope” into statistical literature by Cook et al. (2010), various envelope models have been developed, including partial envelopes (Su and Cook, 2011), inner envelopes (Su and Cook, 2012) and simultaneous envelopes (Cook and Zhang, 2015a). Cook and Zhang (2015b) provides a comprehensive overview of “envelope” development.====We revisit PPCA in Section 2.1 to investigate the link between PPCA and principal envelope models. In Section 2.2 we describe the concept of an envelope and demonstrate the possibility that any subset of principal components could retain most of the sample’s information. We build some intermediate models in Section 2.3. The log-likelihood function of one specific principal envelope model has the same form as probabilistic extreme components analysis (PXCA) (Welling et al., 2003) if the dimension of the envelope is the same as the minimum dimension reduction subspace. However, the concepts and statistical meanings of these two approaches are quite different. In Section 2.4, we employ the likelihood ratio test to determine the dimension of the envelop. Results of simulation studies are presented in Section 3. An extension to factor model is given in Section 4. Real data analysis is presented in Section 5. A brief discussion about the proposed methods can be found in Section 6. Technical details are given in Appendix.",Principal envelope model,https://www.sciencedirect.com/science/article/pii/S0378375819300953,18 October 2019,2019,Research Article,243.0
"Wang Jiali,Verbyla Arūnas P.,Jiang Bomin,Zwart Alexander B.,Ong Cheng Soon,Sirault Xavier R.R.,Verbyla Klara L.","Data61, Commonwealth Scientific and Industrial Research Organisation, Australia,MIT Institute for Data, Systems, and Society, USA,Department of Computer Science, Australian National University, Australia,Agriculture and Food, Commonwealth Scientific and Industrial Research Organisation, Australia,The High Resolution Plant Phenomics Centre, Australian Plant Phenomics Facility, Commonwealth Scientific and Industrial Research Organisation, Australia","Received 1 August 2019, Revised 26 September 2019, Accepted 9 October 2019, Available online 16 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.10.002,Cited by (1),"We consider the design problem of collecting temporal/longitudinal data. The adaptive smoothing spline is used as the analysis model where the prior curvature information can be naturally incorporated as a weighted smoothness penalty. The estimator of the curve is expressed in ==== form, and the information matrix of the parameters is derived. The D-optimality criterion is then used to compute the optimal design points. An extension is considered, for the case where subpopulations exert different prior curvature patterns. We compare properties of the optimal designs with the uniform design using simulated data and apply our method to the Berkeley growth data to estimate the optimal ages to measure heights for males and females. The approach is implemented in an R package called “ODsplines”, which is available from github.com/jialiwang1211/ODsplines.","Data collection can be costly preventing researchers from performing measurements on the continuum. Optimal design concerns the matter of planning for the collection of data from experiments in an efficient manner. In a broad sense, randomization, blocking and replication are the fundamental principles when designing an experiment and there is a vast literature on these topics (Atkinson et al., 2007, Montgomery, 2017). In biological and agricultural studies, researchers often monitor the growth of plants, for example, responses to stress conditions over time and different growing trajectories with different genotypes. Even with modern high-throughput automatic scanning platforms, cost and time constraints may still make collecting high-frequency data from a large number of biological replicates unrealistic. Therefore, principled guidance for collecting data is required to maximize the information gain from limited data and reduce bias when making an inference. In this article, we limit our attention to the design problem when collecting temporal/longitudinal data, that is, when to perform measurements of the experimental units over time in order to best capture the true behaviour of the system being observed.====The optimality of a design depends upon the statistical model that will be used to analyse the data. Consider the model ====, where ==== denotes time, ==== denotes the response and ==== is the vector of model parameters. Note that while we use time ==== as the time ordinate in this paper, more generally the ordinate can be of other types, such as dose concentration in a dose–response curve. The design objective with respect to this model is to find the vector ==== (which we refer to as the ‘optimal design’ or the set of ‘optimal design points’).====Various choices of linear model can be applied to time dependent data, for example, a polynomial regression model with Gaussian noise. The information matrix of the parameters can be estimated using the method of least squares and does not depend on ====, hence the solution ==== is independent of the parameter values of the fitted curve. However, linear models can be too restrictive for describing a dynamic system and are prone to under-/over-fitting. Parametric nonlinear models can be fitted instead, for example, three-parameter logistic model and Gompertz model (Paine et al., 2012). Linearization by Taylor expansion about a parameter value ==== can be used to convert the nonlinear model into linear model form, however a prior knowledge of ==== is then required (Atkinson et al., 2007 Chapter 17) to derive the optimal design. Optimal design for the logistic model has been studied in Li and Majumdar (2008) and for the Gompertz model in Li (2012). As an alternative, the Bayesian approach to optimal design incorporates uncertainty in the parameter values when constructing the objective function by averaging the information matrix over the prior distribution of the parameters; see Chaloner and Verdinelli (1995) for a review, and Donev et al. (2008) for an application.====Nonlinear models with few parameters may remain insufficiently flexible in many applications, and optimal designs derived from these models can be very sensitive to the choice of ====. Instead we consider nonparametric models, specifically the smoothing spline as the analysis model from which optimal designs are derived. Due to its nonlinear nature, solving the design problem for smoothing spline also requires prior knowledge about the parameters (potentially in a high dimensional space), and choosing a good prior is challenging. There has been extensive research on knot selection for fitting spline models after the data have been collected (Miyata and Shen, 2003, DiMatteo et al., 2001), but there is a relative sparsity of literature on the determination of design points prior to conducting an experiment. Park (1978) derived the D-optimal design for segmented polynomial regression with a single knot. Some extensions were made by Kaishev (1989) and Heiligers (1998), to consider an arbitrary number of knots and multiplicities in polynomial spline regression. In these cases, knots did not need to coincide with the design points and did need to be determined as the prior input. Dette et al. (2008) indicated that the optimal designs were not necessarily robust with respect to the prior guess for the vector of knots, and they proposed a standardized maximin D-optimal design for free knot least square splines which they found was less sensitive to the specification of the unknown knots. Instead of working with polynomial splines via least squares, Dette et al. (2011) assumed the curve was estimated from a smoothing spline where the smoothness was controlled by the smoothing parameter ====, and they derived the information matrix via a system of new basis functions. The prior knowledge that was required to optimize the design was the level of smoothness, and they showed through simulations that as the smoothing parameter increased, G-optimal design points became more concentrated at the boundaries of the design region, but that D-optimal design points were less affected. Notice that in Dette et al. (2011), the design points were distributed symmetrically across the design interval due to the fact that the only prior knowledge used in optimization was the global smoothness parameter ====, so that no local properties could be incorporated into the design. It is worth noting that there are some recent works on D-optimal designs for active learning in the discipline of computer vision and they were primarily applied to determine the unlabelled data to better the separation of images (He, 2009, Gu and Jin, 2013). The neighbourhood structure of the data was preserved by imposing a similarity based locality preserving regularizer, based on the prior belief that if two points are close to each other, their measurements should be close as well.====In this paper, we propose incorporating the curvature (or second derivative/acceleration) of the curve as the prior information for optimization. The prior curvature knowledge is particularly informative when determining optimal sampling points for longitudinal data, because intuitively more observations should be placed at the locations where the shape of the curve is changing rapidly. To the best of our knowledge, including curvature as the prior information in a smoothing spline optimal design problem has not been explored in the literature.====The paper is structured as follows. In Section 2, we review the adaptive smoothing spline model and show how curvature information can be incorporated naturally into the design problem. Under some mild conditions, the estimated curve can be represented in matrix form similar to the natural cubic spline, which has the equivalent linear mixed model formulation. The D-optimality criterion is used to define the optimization problem. A numerical approach to finding the optimal design points is then outlined. In Section 3, we consider the issues involved in obtaining prior curvature information from historical data, as well as choosing the smoothing parameter and the number of design points. In Section 4, two simulation studies are performed for growth curves that are assumed to follow a logistic model, and a mixture parametric model. We compare the optimal design with the uniform design with respect to the distribution of the design points and goodness-of-fit. The female height data set from the Berkeley Growth study (Tuddenham, 1954) is used as a real data example. In Section 5 we consider the determination of optimal design points where subpopulations with different curvature patterns exist. Results from a third simulation for two logistic curves are presented and the Berkeley Growth study data set with both males and females is used to illustrate this scenario. Section 6 concludes the paper and presents some future research directions.",Optimal design for adaptive smoothing splines,https://www.sciencedirect.com/science/article/pii/S0378375819300965,16 October 2019,2019,Research Article,244.0
"Zhang Chenguang,He Hua,Li Jian,Tang Wan","Department of Biostatistics and Data Science, School of Public Health and Tropical Medicine, Tulane University, New Orleans, LA, USA,Department of Epidemiology, School of Public Health and Tropical Medicine, Tulane University, New Orleans, LA, USA","Received 6 September 2018, Revised 6 September 2019, Accepted 21 September 2019, Available online 11 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.010,Cited by (0)," of the new estimates are derived and the asymptotic orders of the mean integrated square errors (MISE) are compared with existing methods. We prove that the doubly robust estimate can be more efficient than the IPW estimate when the model for the missing mechanism is correctly specified. We also prove that the prediction-model based approaches may have smaller MISEs than the complete data when the prediction model is correctly specified, suggesting that those methods can be applied even when there are no missing values in the memberships. Simulation studies are carried out to assess the performances of methods and a real study example is used for illustrative purposes.","The kernel density estimate is a popular nonparametric method for estimating density functions (Parzen, 1962, Wand and Jones, 1995, Simonoff, 1996). However, the method cannot be directly applied to estimate density functions for subpopulations if the memberships for some of the subjects are missing. In practice, it is not uncommon that the outcomes of interest are observed for all the subjects, but the memberships are missing for some subjects. For example, two-stage designs where the subjects are first recruited from a larger population in the first stage and then their memberships are ascertained in the second stage are quite common in modern diagnostic test studies (White, 1982). For ethical, cost, and feasibility reasons, some of the subjects may not be administrated with the gold standard test for the memberships in the second stage, thus resulting in missing values in the memberships. The naïve analysis, which only uses subjects with known memberships, is not valid unless the missingness of the membership only depends on the membership itself. In these cases, the memberships are missing completely at random (MCAR) for the subpopulation, i.e., the missingness of the memberships does not depend on any observed or unobserved data for subjects in the subpopulation. This issue is well recognized in the statistical analysis of diagnostic studies, and there is a large body of literature on correcting the so-called verification bias (Alonzo and Pepe, 2005, Begg and Greenes, 1983, He and McDermott, 2012).====For nonparametric estimates of the density functions of subpopulations, Tang et al. (2012) have developed an inverse probability weighting (IPW) kernel smoothing method under the more plausible missing at random (MAR) assumption when the mechanism of the missing values is well understood. The missing values in the memberships are addressed by weighting those subjects with their memberships confirmed by the inverse of the probabilities of the memberships being observed. When the relationship between the memberships and observed variables is well understood, He et al. (2017) developed prediction model based kernel density methods by replacing the memberships with the predicted probabilities of the memberships based on a prediction model. As in the literature for correcting verification bias for evaluating diagnostic tests, one may replace ====, whether or not the disease status is missing, with the corresponding predicted values of the disease based on the prediction model to correct verification bias (Begg and Greenes, 1983). One may also replace ==== with their predicted values; this is generally referred as the mean score (MS) method (Pepe et al., 1994, Reilly and Pepe, 1995). The IPW approach may yield biased estimates if the model for the missing mechanism is misspecified. Similarly, the validity of the prediction-model based MS and Begg-Greene (BG) approaches needs the prediction model to be correctly specified. In this paper, we develop a doubly robust (DR) kernel density estimate method to estimate the density function of the outcome of interest for a subpopulation by integrating information from both models for the missing mechanism and the memberships. The DR method yields double robust estimates in the sense that the estimates are consistent as long as either one of the two models is correctly specified.====The paper is structured as follows. We first give a brief review of the IPW approach and prediction-model based MS and BG methods in Section 2. Then, we propose the DR method in Section 3. Bandwidth selection for the proposed smoothing approaches is presented in Section 4. Simulation studies are included in Section 5 to assess the performances of the methods. A real study example is given in Section 6. The conclusion and discussion about these approaches are included in Section 7.",Doubly robust kernel density estimation when group membership is missing at random,https://www.sciencedirect.com/science/article/pii/S0378375818302532,11 October 2019,2019,Research Article,245.0
Yin Yanqing,"School of Mathematics and Statistics, Jiangsu Normal University, Xuzhou, 221116, PR China","Received 1 April 2019, Revised 3 August 2019, Accepted 30 September 2019, Available online 10 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.011,Cited by (12),Testing for series correlation among error terms is a basic problem in linear regression model diagnostics. The famous Durbin–Watson test and Durbin’s h-test rely on certain model assumptions about the response and ==== variables. The present paper proposes simple tests for series correlation that are applicable in both fixed and random design linear regression models. The test ====. Good performance of the proposed tests is demonstrated by the simulation results.,"Linear regression is an important topic in statistics and is useful in almost all aspects of data science, especially in business and economics statistics and biostatistics. Consider the following multivariate linear regression model: ====where ==== is the response variable, ==== is a ====-dimensional vector of regressors, ====
 ====
 ==== is a ====-dimensional regression coefficient vector, and ==== is a random error with a mean of zero. Suppose we obtain ==== samples from this model, that is, ==== with design matrix ====, where for ====, ====
 ====
 ====. The first task in a regression problem is to make a statistical inference about the regression coefficient vector. By applying the ordinary least squares (OLS) method, we obtain the estimate ====
 ====
 ==== for the coefficient vector ====. In most applications of linear regression models, we need the assumption that the random errors ==== are uncorrelated and homoscedastic. That is, we assume ====where ==== are unknown. With this assumption, the Gauss–Markov theorem states that the ordinary least squares estimate (OLSE) ==== is the best linear unbiased estimator (BLUE). When this assumption does not hold, we suffer from a loss of efficiency and, even worse, make wrong inferences in using OLS. For example, positive serial correlation in the regression error terms will typically lead to artificially small standard errors for the regression coefficient when we apply the classic linear regression method, which will cause the estimated t-statistic to be inflated, indicating significance even when there is none. Therefore, tests for heteroscedasticity and series correlation are important when applying linear regression.====For detecting heteroscedasticity, in one of the most-cited papers in econometrics, White (1980) proposed a test based on comparing the Huber–White covariance estimator to the usual covariance estimator under homoscedasticity. Many other researchers have considered this problem, for example, Breusch and Pagan (1979), Dette and Munk (1998), Glejser (1969), Harrison and McCabe (1979), Cook and Weisberg (1983), and Azzalini and Bowman (1993). Recently, Li and Yao (2015) and Bai et al. (2018) proposed tests for heteroscedasticity that are valid in both low- and high-dimensional regressions. Simulations showed that their tests performed better than some classic tests.====The most famous test for series correlation, the Durbin–Watson test, was proposed in Durbin and Watson, 1950, Durbin and Watson, 1951, Durbin and Watson, 1971. The Durbin–Watson test statistic is based on the residuals ==== from linear regression. The researchers considered the statistic ====whose small-sample distribution was derived by John von Neumann. In the original papers, Durbin and Watson investigated the distribution of this statistic under the classic independent framework, described the test procedures and provided tables of the bounds of significance. However, the asymptotic results were derived under the normality assumption on the error term, and as noted by Nerlove and Wallis (1966), although the Durbin–Watson test appears to work well in an independent observations framework, it may be asymptotically biased and lead to inadequate conclusions for linear regression models containing lagged dependent random variables. New alternative test procedures, for instance, Durbin’s h-test and t-test (Durbin, 1970), were proposed to address this problem; see also Inder (1986), King and Wu (1991), Stocker (2007), Bercu and Proia (2013), Gençay and Signori (2015) and Li and Gençay (2017) and references therein. However, all of these tests were proposed under some model assumptions on the regressors and/or the response variable. Moreover, Durbin’s h-test requires a Gaussian distribution of the error term. Thus, some common models are excluded. In fact, since it is difficult to assess whether the regressors and/or the response are lag dependent, model-free tests for the regressors and response variable appear to be appropriate.====The present paper proposes a simple test procedure without assumptions on the response variable and regressors that is valid in both low- and high-dimensional multivariate linear regressions. The main idea, which is simple but proves to be useful, is to express the mean and variance of the test statistic by making use of the residual maker matrix. In addition to a general joint central limit theorem for several quadratic forms, which is proved in this paper and may have its own interest, we consider a Box–Pierce-type test for series correlation. Monte Carlo simulations show that our test procedures perform well in situations where some classic test procedures are inapplicable.",Model-free tests for series correlation in multivariate linear regression,https://www.sciencedirect.com/science/article/pii/S0378375819300928,10 October 2019,2019,Research Article,246.0
"Ng Chi Tim,Lee Woojoo,Lee Youngjo","Department of Statistics, Chonnam National University, Gwangju 500-757, Korea,Department of Statistics, Inha University, Incheon 402-751, Korea,Department of Statistics, Seoul National University, Seoul 151-747, Korea","Received 9 July 2018, Revised 20 September 2019, Accepted 20 September 2019, Available online 5 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.009,Cited by (2),"In this study, we reformulate ==== as penalized likelihood estimation problems based on the ==== penalties on the ","Pairwise comparison methods have been widely used in various areas such as medicine, biology and education. Agresti et al. (2008) studied pairwise differences between proportion of adverse events such as nausea when four treatment groups from a randomized trial related to early Huntington’s disease were compared. Gelman et al. (2012) compared all states based on average scores on the National Assessment of Educational Progress fourth-grade mathematics test. Lin et al. (2014) and Lu et al. (2017) considered a clinical problem involving pairwise comparisons of several treatments to assess their relative efficacy. The classical pairwise comparison methods were described in Miller (1981) and Hochberg and Tamhane (1987). However, these methods can result in logically contradictory conclusions (Lehmann and Romano, 2005). For example, suppose there are three groups with means ==== and ==== respectively, and the problem of interest employs multiple tests on three hypotheses: ==== vs. ====, ==== vs. ====, and ==== vs. ====The classical methods may yield the logically contradictory result that ====, ====, and ====.====To circumvent the logical contradiction, the concepts of coherence and consonance have been introduced in Gabriel (1969), Sonnemann (2008) and Zhao et al. (2010). Finner and Strassberger (2002) regard coherence as a minimal requirement for multiple tests and Sonnemann (2008) shows that any coherent test can be constructed using the closure method proposed by Marcus et al. (1976). Sonnemann and Finner (1998) show that any incoherent multiple testing procedure can be replaced by a coherent one that rejects the same hypotheses and possibly more. Romano et al. (2011) show that many optimal tests satisfy the consonance property. Furthermore, Zhao et al. (2010) developed general methods for constructing coherent and consonant tests based on the partitioning method of Finner and Strassberger (2002).====To apply coherent and consonant testing procedures, various computational methods have been proposed. However, in practice, they are computationally intractable as the number of hypotheses ==== increases. For example, with ==== hypotheses, the number of tests required by the closure method is ==== (Romano et al., 2011). This raises a severe computational difficulty because the total number of tests increases exponentially with ====. Furthermore, constructing test statistics for the hypotheses is not easy in general. In order to reduce the number of tests, the so-called shortcut procedures have been studied in the literature (Sonnemann and Finner, 1998, Brannah and Bretz, 2010). However, they have focused mainly on some special cases such as consonant procedures (Goeman and Solari, 2011). Thus, to overcome such computationally difficulties in pairwise comparison test, we propose a procedure that automatically does not generate logically contradictory results.====In this study, we reformulate pairwise comparisons as penalized likelihood problems. The proposed method eliminates logically contradictory conclusions and does not require that we perform ==== tests. To illustrate this, consider the three-group example again. The ==== penalty can be defined as ====where ==== is a tuning parameter. Let ==== be the maximum penalized likelihood estimator. Our proposed method rejects the null ==== if ====. Due to the ==== penalty on the pairwise differences, for example, ==== can take the value zero. Therefore, ==== is possible. Obviously, ====, ====, and ==== cannot occur at the same time. On the contrary, if some non-zero critical values ==== are used as thresholding values to determine whether ====, ==== and ==== are in fact zeros, the possibility that ====, ====, and ==== hold at the same time cannot be ruled out. Hocking et al. (2011) studied a similar penalized likelihood for clustering problems. Though Zhang and Zhang (2012) studied the penalized likelihood procedures based on the so-called structured LASSO, the validity of their assumptions in the above-mentioned penalty function requires justifications.====Suppose that there are ==== groups and the group sizes are ====. Consider multiple tests ==== versus ==== for ====, where ====. In this paper, we study “test consistency” of the proposed penalized likelihood procedure that as ==== increases, ====In multiple tests, such as pairwise comparisons, the family-wise error rate (FWER) is often controlled at a fixed level of 5% or 1%. To control the FWER, Tukey (1953) proposes using simultaneous confidence intervals. Kramer (1956), Cheung and Chan (1996) and Lin et al. (2014) constructed simultaneous pairwise confidence intervals for pairwise comparisons based on the work of Tukey (1953). However, under appropriate conditions, as we shall see, the FWER goes to zero as the group size increases. Table 1 shows the quantities arising in the multiple tests. The test consistency is equivalent to ==== and ==== as ====, where ====, ====, and ==== depend on ====. In pairwise comparisons, the number of hypotheses ==== is increasing with the number of groups. Gretton and Györfi (2010) establish test consistency results for a non-parametric test for the independence of two groups and Lee and Bjørnstad (2013) show the test consistency for independent multiple hypotheses. In this paper, we study the test consistency of pairwise comparisons.====The remainder of the paper is organized as follows. In Section 2, we discuss pairwise comparisons under the penalized likelihood approach, and show that test consistency can be achieved under certain conditions. Then, in Sections 3 Numerical study, 4 Real data analysis, we describe a numerical study and two applications of the proposed method respectively. Section 5 concludes the paper including a discussion of possible extensions of the theory and methods developed in this paper. All proofs are presented in Appendix A. In Appendix B, we present a numerical algorithm that implements the penalized likelihood method.",Logical and test consistency in pairwise multiple comparisons,https://www.sciencedirect.com/science/article/pii/S0378375818301290,5 October 2019,2019,Research Article,247.0
"Gelein Brigitte,Chauvet Guillaume","Univ Rennes, ENSAI, CNRS, IRMAR - UMR 6625, F-35000 Rennes, France","Received 15 October 2018, Revised 29 July 2019, Accepted 20 August 2019, Available online 5 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.08.006,Cited by (0),Item non-response in surveys is usually handled by ,"Item non-response may affect the quality of the estimates when the respondents and the non-respondents exhibit different characteristics with respect to the variables of interest. Item non-response in surveys is usually handled by single imputation, whose main objective is to reduce the non-response bias. Two approaches are commonly used in sample surveys to motivate imputation. Under the non-response model approach (NM), the response mechanism is explicitly modeled, whereas under the imputation model approach (IM), the variable under study is explicitly modeled.====Single imputation consists of replacing a missing value with an artificial one. It leads to a single imputed data set, constructed so that it is possible to apply complete data estimation procedures for obtaining point estimates. The response indicators are therefore not required. On the other hand, multiple imputation methods (Rubin, 1987, Little and Rubin, 1987) consist in building ==== imputed datasets, and in estimating the parameters under study for each of them. The ==== analyses are then combined for inference. Multiple imputation has been extensively studied in the literature, some recent references include Iacus and Porro (2007), White and Carlin (2010) and Templ et al. (2011). However, multiple imputation is not commonly used in sample surveys. Under the NM approach, multiple imputation needs to be proper for valid inference. Some sufficient conditions are given in Rubin (1987), pp. 118–119, but they are usually difficult to check for complex sampling designs, see Binder and Sun (1996), Fay, 1992, Fay, 1996 and Nielsen (2003). Also, under the IM approach, the multiple imputation variance estimator does not track the variance correctly, and can be considerably biased, see Kott (1995), Kim et al., 2006a, Kim et al., 2006b and Beaumont et al. (2011). Therefore, we focus in this paper on single imputation methods.====The Imputation Model (IM) approach is of common use to treat item non-response in surveys. The imputation methods are then motivated by a modeling of the relationship between the variable of interest and the available auxiliary variables. Both the imputation model and the imputation methods need to be adapted to the study variable. For instance, in business surveys, the interest variables often contain a large number of zeros. In the Capital Expenditure Survey conducted at Statistics Canada, approximately 70% of businesses reported a value of zero to Capital Machinery and 50% reported a value of zero to Capital Construction (Haziza et al., 2014). In case of some interest variable containing a large amount of zeros, Haziza et al. (2014) propose imputation methods based on a mixture regression model. They prove that these methods lead to doubly robust estimators of the population mean, i.e. the imputed estimator of the mean is consistent whether the interest variable or the non-response mechanism is adequately modeled. However, these methods are not appropriate when estimating more complex parameters such as the population distribution function.====In this work, we propose an imputation which enables to preserve the distribution function for zero inflated data. This is an important practical property if the data users are not only interested in estimating means or totals, but also parameters related to the distribution of the imputed variable, e.g. the Gini coefficient. We use the IM approach, without explicit assumptions on the non-response mechanism for the interest variable. We propose a random imputation method which leads to a ====-consistent estimator of the total, and to a mean-square consistent estimator of the distribution function.====As recalled in Haziza et al. (2014), random imputation methods suffer from an additional variability due to the imputation variance. Three approaches have been proposed in survey sampling to reduce this variance. Fractional imputation is somewhat similar to multiple imputation, and consists in replacing some missing value with ==== imputed values to which some weights are given (Kalton and Kish, 1981, Kalton and Kish, 1984, Fay, 1996, Kim and Fuller, 2004, Fuller and Kim, 2005). The imputation variance decreases as ==== increases. The second approach consists in using some standard imputation mechanism, and in modifying the imputed values in order to suppress the imputation variance (Chen et al., 2000). Finally, the third approach consists of directly imputing artificial values in such a way that the imputation variance is eliminated (Kalton and Kish, 1981, Kalton and Kish, 1984, Deville, 2006, Chauvet et al., 2011, Chauvet and Haziza, 2012, Hasler and Tillé, 2014, Chaput et al., 2018). This last approach is of particular interest because it leads to a single imputed dataset, which is attractive from a data user’s perspective, and it does not require any modification of the imputed values.====In this paper, we propose a balanced version of our imputation method, which enables to greatly reduce the imputation variance. It consists of randomly generating the imputed values while satisfying appropriate balancing constraints, by using an adaptation of the Cube algorithm (Deville and Tillé, 2004, Chauvet et al., 2011). Our simulation results prove that the balanced imputation method succeeds in preserving the distribution function of the imputed variable, with large variance reductions as compared to the proposed non-balanced imputation method. In order to produce confidence intervals for the estimated parameters with appropriate coverage, we also propose variance estimators adapted from the linearization variance estimators proposed by Kim and Rao (2009). Our simulation results indicate that these estimators perform well, both in terms of relative bias and of coverage rate.====The paper is organized as follows. In Section 2, we describe the theoretical set-up and the notation used in the paper. In Section 3, we briefly recall the two imputation procedures proposed by Haziza et al. (2014), and introduce our two proposed imputation methods. In Section 4, we prove that the proposed random imputation procedure yields a consistent estimator of the total and of the distribution function. Variance estimation for the imputed estimator of the total is discussed in Section 5. The results of a simulation study comparing the four procedures and evaluating the proposed variance estimator are presented in Section 6. An application of the proposed methodology on data modeled in the Monthly Retail Trade Survey is presented in Section 7. We conclude in Section 8. All the proofs are given in Appendix A Proof of, Appendix B Proof of, Appendix C Proof of. Some additional simulation results are available in the Supplementary Material.",Preserving the distribution function in surveys in case of imputation for zero inflated data,https://www.sciencedirect.com/science/article/pii/S0378375819300886,5 October 2019,2019,Research Article,248.0
"Huang Shih-Hao,Lo Huang Mong-Na,Lin Cheng-Wei","Department of Mathematics, National Central University, Taiwan,Department of Applied Mathematics, National Sun Yat-sen University, Taiwan","Received 22 May 2019, Accepted 14 September 2019, Available online 30 September 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.006,Cited by (2),"-optimal design exists for any given ====. In particular, we explicitly identify ====-optimal designs for logit, probit, double exponential, double reciprocal models within the class.","Binary response experiments are frequently performed in scientific studies. The most commonly seen models for describing the relationship between a binary response and explanatory covariates are the generalized linear models, such as logit, probit, double exponential, and double reciprocal models. The optimal design problems for binary response models with single covariate have been well investigated; see for example, Ford et al., 1992, Sitter and Wu, 1993, and Biedermann et al. (2006). However, only limited literature theoretically considers the design problems for binary models with multiple covariates.====The model that we are interested in is a generalized linear model for binary response experiments with multiple covariates, as ====Here ==== is the response with the covariates at levels ==== respectively, ==== is the vector of unknown parameters with ====, and the link function ==== is a cumulative distribution function. Sitter and Torsney (1995) investigated optimal designs for two covariates by the geometric approach (Elfving, 1952). They indicated that if the design space is unbounded, then in a multi-covariate case, the ====-criterion can be made arbitrarily large by choice of design, which is quite different from that in the one-covariate case. Therefore, they adopted a design space ====, where ====. By using the complete class approach (see, e.g., Yang, 2010, Yang and Stufken, 2012), Yang et al. (2011) extended Sitter’s work from two covariates to a given finite number of covariates with design space ====.====On the other hand, Haines et al. (2007) and later Kabera et al. (2015) also investigated optimal designs for two covariates, but in contrast, they adopted another design space ====, by using the general equivalence theorem (Kiefer, 1974). It motivates us to extend their work to consider a more general case of multiple nonnegative covariates, and provide theoretical justifications showing why these optimal designs on ==== have the specific form. We note that neither our design space ==== nor the design space in Yang et al. (2011) is a special case of the other, and so the structure of our optimal designs, which have ==== or ==== support points, is different from theirs.====This paper is constructed as follows. In Section 2 we provide some background knowledge of designs and notations. In Section 3, we characterize an essentially complete class consisting of permutation-invariant designs supported on the coordinate axes. Therefore, seeking optimal designs within the class is no longer a ====- but a one-dimensional optimization problem. We then obtain ====-optimal designs for logit, probit, double exponential, and double reciprocal models within the class in Section 4. Section 5 ends with conclusion and future works. All proofs are presented in the Appendix.",Optimal designs for binary response models with multiple nonnegative variables,https://www.sciencedirect.com/science/article/pii/S0378375819300813,30 September 2019,2019,Research Article,249.0
"Sofro A’yunin,Shi Jian Qing,Cao Chunzheng","Department of Mathematics, Universitas Negeri Surabaya, Indonesia,School of Mathematics, Statistics and Physics, Newcastle University, UK,School of Mathematics & Statistics, Nanjing University of Information Science and Technology, China","Received 23 October 2018, Revised 19 August 2019, Accepted 6 September 2019, Available online 30 September 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.005,Cited by (4),"Research on ====. The definition of the model, the inference and the implementation, as well as its ====, are discussed. Comprehensive numerical examples with both simulation studies and real data are presented.","Regression analysis for non-Gaussian process data has developed rapidly in the last several decades. In this paper, we will focus on process data of counts. One method is to extend the conventional Poisson regression model by considering a specific covariance structure, e.g. the intrinsic conditional autoregressive (ICAR) (Besag and Kooperberg, 1995). However, the problem of modelling becomes more complex when there is more than one response variable. We illustrate the challenges using the example of dengue fever and malaria data that we will discuss in detail later in this paper. The outputs are the number of cases of dengue fever and malaria occurring in different regions in East Java in Indonesia. Both diseases are transmitted by a virus via mosquitoes and occur often in tropical regions particularly in developing countries. They have similar signs and symptoms. The outbreak of the diseases depends on many factors such as living condition and healthy behaviour. The data are spatially correlated due to the movement of population, analogues of the environment and the healthy behaviour. The study of such problems focuses on the following three aspects. First of all, we want to study how the count of cases depends on a set of covariates. A parametric model is usually used since it can provide a physical explanation of the relationship between the disease and the covariates. Secondly, we are interested in finding the structure of the spatial correlation of the dependent data for each disease and, finally, to find the geographical patterns. This will provide a useful tool for epidemiological study. Due to the nature of the problem, it requires a flexible covariance model and ideally the covariance structure and the pattern can be learned from data rather than any assumption given in advance. Thirdly, we want to study similar diseases or response variables at the same time. We are interested in knowing if there are similar geographical patterns for these diseases and how they are spatially correlated and cross-correlated. The findings will provide important information for policy making on how to control the spread and transmission of the diseases.====Poisson regression analysis for a univariate count response variable with correlation structure has been studied by many researchers. The ICAR model is one of the popular methods and was introduced by Besag and Kooperberg (1995). This method has been extended into a spatial or temporal correlated generalized linear mixed model (Sun et al., 2000, MacNab and Dean, 2001, Martínez-Beneito et al., 2008, Silva et al., 2008). A generalized linear mixed model using prior distribution for spatially structured random effect is an alternative method, see Banerjee et al. (2015). Rue and Held (2005) and Mohebbi et al. (2011) demonstrated how to apply these methods to analyse cancer data. However, based on extensive studies by Wall (2004), the spatially correlated structure of ICAR approach is quite complicated and it is not easy to give a physical explanation for some problems. Martínez-Beneito (2013) has also pointed out that preliminary knowledge and a good understanding may be needed in determining and investigating the effect of the choice of covariance matrix. Thus, it is essential to develop a more flexible method to model the spatial correlation. One alternative is to use a Gaussian process (GP) prior (or kriging under spatial statistics, see Diggle et al., 1998, Ver Hoef and Barry, 1998) to model the covariance structure (see e.g. Rasmussen and Williams (2006) and Shi and Choi (2011)).====The model provides a flexible method of modelling different types of covariance structure via the data. The Bayesian framework, using GP priors with different covariance functions, provides flexibility on fitting data with different degrees of nonlinearity and smoothness. It can also cope with multi-dimensional covariates. Some recent development can be found in e.g. Gramacy and Lian, 2012, Ba and Joseph, 2012, Wang and Shi, 2014, Datta et al., 2016, Wang et al., 2017 and Cao et al. (2017).====For problems involving multivariate response variables, we need to model the covariance structure for each component as well as the cross-covariance between them. One of the challenges here is how to find a model which can model the covariance and cross-variance flexibly, while the overall covariance function is positive or non-negative definite. Several methods have been proposed, for example, two-fold CAR model (Kim et al., 2001) and multivariate CAR (MCAR) (Gelfand and Vounatsou, 2003). Jin et al. (2005) proposed a general framework for MCAR by using a conditional approach ====, where ==== and ==== stand for the two components respectively. Crainiceanu et al. (2008) also used the idea of conditional distributions but the covariance structure is modelled by a GP prior. This provides a promising result for some types of problem. However the covariance structure of ==== depends on the covariance structure of ====. If these two components have very different covariance structures, the model may be inappropriate. The performance also depends on the ordering of the components. An additional problem is that it is not easy to extend it to cases with more than two components.====A similar difficulty is found in a multivariate count time series. Although many different methods have been developed for univariate cases (see e.g. Fokianos, 2012, Cameron and Trivedi, 2013, Davis et al., 2016, Weiss, 2018), the discussion for multivariate cases is limited (Inouye et al., 2017). Among others, Veraart (2019) proposed a new modelling framework based on integer-valued trawl processes, which are special cases of multivariate mixed moving average processes; and Halliday and Boshnakov (2018) proposed a copula approach, which the non-uniqueness of the associated copula may be a problem for the discrete data (Trivedi and Zimmer, 2017). Those work focused mainly on modelling the auto- and cross-covariance structure for the multivariate count time series. This paper uses a different way to model the covariance and cross-covariance structure based on a Gaussian process prior for process data of counts defined in a large dimensional domain, i.e. ==== where ==== could be quite large. Many applications, particularly the spatial–temporal data, involve such data. Regression problems with multiple covariates, the focus of this paper, is another example.====Specifically we propose to use convolved GP (CGP) (Higdon, 2002, Boyle and Frean, 2005) and provide a general framework on modelling individual and covariance structure for each component and, at the same time, modelling cross-covariance for multivariate count data. The method can be easily extended to the multivariate case in any dimension. It inherits nice properties of GP models, for example, it offers a semiparametric regression model for Poisson data with multivariate responses; it models mean structure and covariance structure simultaneously and it enables us to handle large dimensional covariates.The covariance and cross-covariance structures are learned from the data nonparametrically. The idea can also be used to model the count time series, which is a special case of ====.====This paper is organized as follows. In Section 2, we will discuss how to construct multivariate dependent Gaussian processes using convolution. We will then explain how to define a multivariate CGP for dependent count data. The details of inference including estimation, prediction and asymptotic theory will also be provided in the section. Comprehensive simulation studies and real data applications will be discussed in Section 3. The final conclusive remarks will be given in Section 4.",Regression analysis for multivariate process data of counts using convolved Gaussian processes,https://www.sciencedirect.com/science/article/pii/S0378375819300801,30 September 2019,2019,Research Article,250.0
"Xu Xinyi,Li Xiangjie,Zhang Jingxiao","Center for Applied Statistics, School of Statistics, Renmin University of China, Beijing 100872, China","Received 29 May 2018, Revised 21 September 2019, Accepted 21 September 2019, Available online 28 September 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.007,Cited by (3)," effects, select key instruments, and replace the CF-based hypothesis test with variable selection to identify truly endogenous predictors. Under appropriate conditions, we establish theoretical properties of the RCF estimators, including the consistency of coefficient estimation and model selection. Simulation results confirm that the RCF method is effective and superior to its main competitor, the penalized least squares (PLS) method. The proposed method also provides insightful and interpretable results on a real data analysis.","Modern scientific research allows for data whose number of predictors ==== is increasingly large relative to the sample size ====. Many high-dimensional regularization methods taking the form of “loss function ＋ penalty” have been well established, based on the assumption of sparsity and exogeneity. The terminology “exogeneity” borrowed from econometrics means that none of the regressors are correlated with the regression error, and it promises consistent selection of the important regressors. However no matter in applied econometrics or medical researches (Angrist and Keueger, 1991, Guo and Small, 2016), endogeneity (the unobservable error to be correlated with the explanatory variables) incidentally arises from a large pool of predictors. For conventional penalized methods such as Lasso (Tibshirani, 1996), SCAD (Fan and Li, 2001), and MCP (Zhang, 2010), such endogeneity causes inconsistency of variable selection and coefficients estimation, and possible false scientific discoveries (Fan and Liao, 2014).====A natural idea to address the endogeneity problem in high-dimensional models is to learn the instrumental variable (IV) technique from econometrics. For example, Belloni et al. (2012) apply Lasso-type methods to the IV model to estimate optimal instruments. Gautier and Tsybakov (2018) present an inference procedure based on the self tuning instrumental variables (STIV) estimator. Fan and Liao (2014) construct a penalized focused generalized method of moments (FGMM) which applies the IV method and achieves the oracle property. Lin et al. (2015) propose a two-stage regularization (2SR) framework based on the classical two-stage least squares (2SLS) estimator. Belloni et al. (2018) focus on the generalized method of moments problems, and discuss cases including linear instrumental variable models as econometric applications. All the existing methods, however, are not able to identify endogenous regressors, and thus have problem in determining whether the predictor of interest is truly endogenous.====In this article, we have designed to extend the control function (CF) method, which is inherently a kind of IV method and widely implemented in econometrics, to the high-dimensional sparse situation. In addition to dealing with endogeneity, the CF approach leads to a robust, regression-based Hausman test of whether the suspected endogenous variables are actually endogenous (Wooldridge, 2010). In high-dimensional models where the classical test fails due to loss of the equation’s identifiability, we try to transform this test into a variable selection problem via imposing penalty functions in both two stages of the CF model. Theoretical analysis, simulation results and real data analysis all verify that, under appropriate assumptions, the proposed regularized control function (RCF) estimators achieve the consistency of variable selection and coefficient estimation, meanwhile identifying optimal instruments and truly (or the most) endogenous predictors. In fact, most literatures focus on the endogeneity of one or a few key variables, or most of the endogeneity is contained only in a small part of predictors. Therefore, exploiting the exact or approximate sparsity of endogenous variables is reasonable, and sheds light on the actual endogenous situation.====The remainder of this article is organized as follows. In Section 2, we propose the regularization framework for high-dimensional sparse CF models. In Section 3, we investigate theoretical properties of the RCF estimators. Section 4 reports simulation results. Section 5 gives a real data analysis. In Section 6, we discuss possible directions of future development for the RCF method. Section 7 concludes the paper. All technical proofs are given in Supplementary Materials.",Regularization methods for high-dimensional sparse control function models,https://www.sciencedirect.com/science/article/pii/S0378375819300904,28 September 2019,2019,Research Article,251.0
"Córdoba Irene,Bielza Concha,Larrañaga Pedro","Universidad Politécnica de Madrid, Madrid, Spain","Received 11 January 2018, Revised 17 September 2019, Accepted 17 September 2019, Available online 28 September 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.008,Cited by (4),"Markov models lie at the interface between statistical independence in a probability distribution and graph separation properties. We review model selection and estimation in directed and undirected Markov models with Gaussian parametrization, emphasizing the main similarities and differences. These two model classes are similar but not equivalent, although they share a common intersection. We present the existing results from a historical perspective, taking into account the amount of literature existing from both the artificial intelligence and ","Markov models, or probabilistic graphical models, explicitly establish a correspondence between statistical independence in a probability distribution and certain separation criteria holding in a graph. They were originated at the interface between statistics, where Markov random fields were predominant (Darroch et al., 1980), and artificial intelligence, with a focus on Bayesian networks (Pearl, 1985, Pearl, 1986). These two model classes are now considered the traditional ones, but still are widely applied and nowadays there is a significant amount of research devoted to them (Daly et al., 2011, Uhler, 2012). They both share the modelling of conditional independences: Bayesian networks relate them with acyclic directed graphs, whereas in Markov fields they are associated with undirected graphs. However, the models they represent are only equivalent under additional assumptions on the respective graphs.====In this paper, we review the existing methods for model selection and estimation in undirected and acyclic directed Markov models with a Gaussian parametrization. The multivariate Gaussian distribution is among the most widely developed and applied statistical family in this context (Werhli et al., 2006, Ibáñez et al., 2016), and allows for an explicit parametric comparison of their similarities and differences. The highly interdisciplinary nature of these Markov model classes has led to a wide range of terminology in methodological developments and theoretical results. They have usually been studied separately, with some exceptions (Wermuth, 1980, Pearl, 1988), and most unifying works (Sadeghi and Lauritzen, 2014, Wermuth, 2015) are characterized by a high-level view, where the models are embedded in other, more expressive classes, and the focus is on the properties of these container classes. By contrast, in this paper we review them from a low-level perspective. In doing so, we use a unified notation that allows for a direct comparison between the two types of classes. Furthermore, throughout each section we explicitly compare them, in terms of both methodological and theoretical developments.====The paper is structured as follows. A historical introduction to Markov models is presented in Section 2, emphasizing the different research areas that contributed to their birth. Afterwards, preliminary concepts from graph theory are presented in Section 3. In Section 4, undirected and acyclic directed Markov model classes are introduced, under no distributional assumptions. This is because many foundational relationships between them can already be established from this general perspective. Next, we restrict their parametrization to multivariate Gaussian distributions, and explore the main derived properties from this in Section 5. We review maximum likelihood estimation in Section 6. These estimates are used for model selection via hypothesis testing, as we present in Section 7. When maximum likelihood estimators are not guaranteed to exist, a popular technique is to employ regularization, which we overview in Section 8. Finally, the alternative Bayesian approach for model selection and estimation is treated in Section 9. We explore the relationship of Gaussian acyclic directed and undirected Markov models with other, higher level model classes in Section 10. Alternatives to the Gaussian distribution are discussed in Section 11. We close the paper discussing the main real applications of the Gaussian Markov model classes reviewed in Section 12.",A review of Gaussian Markov models for conditional independence,https://www.sciencedirect.com/science/article/pii/S0378375819300916,28 September 2019,2019,Research Article,252.0
"Brairi Houssem,Medkour Tarek","Laboratory of MSTD, Department of Probability and Statistics, University of Science and Technology Houari Boumediene, Algiers, Algeria","Received 1 April 2019, Revised 30 July 2019, Accepted 7 September 2019, Available online 26 September 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.004,Cited by (0),"We consider the problem of testing a univariate discrete-valued time series for whiteness in the sequency domain, using Walsh–Fourier analysis. We show that the distribution of the lag window estimator of the Walsh ","White noise processes have always been of particular importance, and the problem of testing time series for whiteness plays an important role in determining the adequacy of the fitted model. A large body of literature has extensively investigated this problem, early works date back to Durbin and Watson (1950), where they developed a test to detect serial correlation at lag 1 in residuals from least squares regression. Box and Pierce (1970) and Ljung and Box (1978) introduced the popular portmanteau tests, they are based on the sum of the squared of the first ==== autocorrelation coefficients of an ARMA model. Xiao and Wu (2011) considered nonparametric tests for serial correlations based on the maximum and the quadratic deviations. Lobato and Velasco (2004) presented a new testing procedure to detect uncorrelatedness when the process is possibly dependent, whereas, Delgado and Velasco (2011) proposed an asymptotically distribution-free transform of the sample autocorrelations of residuals in general parametric time series. White noise tests have also been constructed in the frequency domain, Fisher (1929) derived an exact test based on the periodogram ordinates. Bartlett (1978) and Grenander and Rosenblatt (1957) proposed to apply the Kolmogorov–Smirnov test on the ordinates of the standardized cumulative periodogram which converges to the Brownian Bridge. Other works include Paparoditis (2000) and Deo (2000) (see Lobato and Velasco, 2004, Guay et al., 2013 for a survey).====There has been a growing interest in a particular type of time series, namely discrete-valued time series. It has been utilized in many applications in neurology (Stoffer et al., 1988), cryptography (Lu and Desmedt, 2016) and geoscience (Negi et al., 1993). This type of time series can occur in the form of count time series (such as the number of traffic accidents (Pedeli and Karlis, 2011)), categorical time series (such as the neonatal sleep-state cycles and DNA sequences (Shumway and Stoffer, 2017)) or as a discretization of a continuous time series (such as the reduction of daily rainfall volumes to a wet and dry days Delleur et al., 1989). Despite the growing attention, greater focus has been put on time domain methods (McKenzie, 2003, Davis et al., 2016, Weiß, 2018), as a result, notably less work has been established in the spectral domain. Works in this domain include Kohn, 1980a, Kohn, 1980b, Stoffer, 1987, Stoffer, 1991, Stoffer and Panchalingam, 1987 and Stoffer et al. (1988), which discuss an approach to the analysis of discrete-valued time series using the Walsh–Fourier transform and its properties, as well as an application on the effects of moderate maternal alcohol consumption on neonatal sleep-state cycling. We also mention the work of McGee and Ensor, 1998, McGee, 2007, where they used Walsh–Fourier analysis in order to test for peaks in the spectra of categorical time series.====Since discrete-valued time series can present sharp discontinuities, Stoffer (1987) has argued that it makes little statistical sense to think of this type of time series as a superposition of well-separated sinusoids, and has suggested, on account of the fact that the Walsh functions are square waveforms that take only two values (==== and ====), that the spectral analysis to be done in the sequency domain using the Walsh–Fourier transform. Moreover, the superiority of the Walsh transform for discrete-valued time series has been supported by various authors who have compared the Fourier and Walsh–Fourier analyses. Stoffer and Panchalingam (1987) point out: “It has been known, primarily in the engineering disciplines, that Walsh spectral analysis is superior to Fourier spectral analysis for non-sinusoidal time series. This is primarily due to the empirical analyses performed by Ahmed and Rao, 1975, Beauchamp, 1975, Harmuth, 1972, and Robinson (1972)”. In addition, when analyzing the sleep-states of neonataes, Stoffer (1991), found out that Walsh–Fourier transform exhibited more information than the Fourier transform did.====In consideration of the foregoing, the aim of this paper is to address the problem of testing a univariate discrete-valued time series for whiteness in the spectral domain, which, to the authors’ best knowledge, has not been considered yet. The problem is formulated as follows: Let ==== be a sample of length ==== from a zero mean, second-order stationary discrete-valued time series. Let ==== be the autocovariance function. We consider the null hypothesis ==== that ====, ====, is a white noise, i.e. an uncorrelated time series with ==== for all ====. Equivalently, in terms of the Walsh spectral density function ====, we want to test ====To test ====, we propose three tests: the first one is based on the normalized cumulative Walsh periodogram, the second test is based the Cramer–von Mises functional applied to an estimate of the Walsh spectral density ====, while the last one is based on a distance to whiteness. These test statistics have been constructed based on their respective Fourier counter-parts introduced in Bartlett, 1978, Lobato and Velasco, 2004, and Drouiche (2007).====The remainder of the paper is organized as follows: Section 2 outlines the theory of Walsh–Fourier analysis for real-time stationary time series. In Section 3, we derive the distribution of a class of estimators of the Walsh spectral density ==== and extend the definition of the spectral bandwidth to the sequency domain. The proposed tests are then presented in Section 4. Section 5 is devoted to simulation results, while in Section 6, we apply the proposed tests to the brain functional connectivity of schizophrenic patients.",Testing discrete-valued time series for whiteness,https://www.sciencedirect.com/science/article/pii/S0378375819300795,26 September 2019,2019,Research Article,253.0
"Zhang Xue-Ru,Liu Min-Qian,Zhou Yong-Dao","School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China","Received 28 February 2019, Revised 21 August 2019, Accepted 21 August 2019, Available online 25 September 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.08.007,Cited by (11),"Composite designs are frequently utilized for fitting response surfaces in practice. This paper proposes a new type of composite designs, orthogonal uniform composite designs (OUCDs), which combine ==== and uniform designs. Such designs not only inherit the advantages of orthogonal-array composite designs such as high estimation efficiencies and ability for multiple analysis for cross validation, but also have more flexible run sizes than central composite designs and orthogonal-array composite designs. Moreover, OUCDs are more robust than other types of composite designs under certain conditions. Some construction methods for OUCDs under the maximin distance criterion are provided and their properties are also studied. It is shown that many constructed OUCDs are maximin distance designs.","The response surface methodology, proposed by Box and Wilson (1951), is widely applied to explore the unknown relationship between explanatory variables and interesting responses. Second-order models can be used to fit such a nonlinear relationship. A design is called a second-order design if it can be used to fit the second-order model. Several types of second-order designs have been proposed in the literature, such as central composite designs (CCDs) proposed by Box and Wilson (1951), small composite designs (Draper and Lin, 1990), subset designs (Gilmour, 2006), augmented pairs designs (Morris, 2000), definitive screening composite designs (DSCDs) proposed by Zhou and Xu (2017) and orthogonal-array composite designs (OACDs) introduced by Xu et al. (2014). Among them, OACDs which combine two-level and three-level orthogonal arrays (OAs) have appealing properties, for example, they have higher ====-efficiencies than many other types of designs under second-order models and can perform separate analysis for the two-level OAs and three-level OAs. When second-order models are insufficient to describe the relationship between variables of importance and responses, Zhang et al. (2018b) studied OACDs which combine two-level and four-level OAs and can be used for fitting third-order models. However, the three-level or four-level OAs in the OACDs often have large number of runs.====This paper proposes a new type of composite designs, called orthogonal uniform composite designs (OUCDs), which combine two-level OAs and uniform designs (UDs), to provide more flexible run sizes than OACDs and still keep the good properties of OACDs. Roughly speaking, OUCDs replace the three-level or four-level OAs in OACDs by UDs. The main idea of UDs is to scatter design points uniformly in the experimental region (Fang et al., 2018). Discrepancy is often used to measure the uniformity of designs, such as the warp-around ====-discrepancy (Hickernell, 1998) and mixture discrepancy (MD) proposed by Zhou et al. (2013). The MD can overcome the shortcomings of other discrepancies and is employed in this paper. It will be shown that OUCDs are robust under certain conditions. We will discuss ====-efficiencies, ====-efficiencies and their lower bounds for OUCDs under second-order models. Moreover, space-filling properties of OUCDs under the maximin distance criterion (Johnson et al., 1990) will be investigated, and the corresponding construction methods for maximin OUCDs will be provided. Furthermore, OUCDs will be compared with other types of composite designs such as OACDs, CCDs and DSCDs. The run sizes in OUCDs are more flexible than other types of composite designs. It will be shown that OUCDs always have larger ====-distances and larger estimation efficiencies than CCDs. The two different parts of an OUCD can be used for cross validation.====The remainder of this paper is organized as follows. In Section 2, the definition of OUCDs and some examples are provided. Section 3 shows the appealing properties of OUCDs in terms of robustness, maximin distance criterion and estimation efficiency. Section 4 compares OUCDs with other composite designs such as CCDs, OACDs and DSCDs, as well as UDs. Both empirical and theoretical results are provided in this section. The concluding remarks are provided in Section 5, and the proofs of theorems and propositions are shown in Appendix A. All the detailed designs are given in the Supplementary Material.",Orthogonal uniform composite designs,https://www.sciencedirect.com/science/article/pii/S0378375819300898,25 September 2019,2019,Research Article,254.0
"Lin Zhantao,Flournoy Nancy,Rosenberger William F.","Department of Statistics, George Mason University, Fairfax, VA 22032, United States of America,Department of Statistics, University of Missouri, Columbia, MO 65201, United States of America","Received 26 November 2018, Revised 2 August 2019, Accepted 4 September 2019, Available online 16 September 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.003,Cited by (6)," fails to normalize the estimator adequately asymptotically, because of dependencies. In this situation, three alternative random information measures are presented and are shown to provide better normalization of the MLE asymptotically. The performance of random information measures is investigated in simulation studies, and the results suggest that the observed information performs best when the sample size is small.","Two-stage designs are used for many purposes, including enrichment, sample size re-estimation and to modify randomization probabilities to improve the efficiency and/or efficacy of estimators. All these procedures use accumulated data to change the operation of the experimental design, which induces dependencies between the first and second stage data. Our interest lies in the effects of such dependencies on inference at the end of a study in which the first stage sample size is small and fixed, and the second stage sample size is large.====In two-stage enrichment designs, patients more likely to benefit from the treatment are identified based on data from the first stage, and second stage trials are conducted in the identified subpopulation (e.g., Simon and Maitournam (2004), Ivanova and Tamura (2011), Rosenblum and van der Laan (2011), Trippa et al. (2012), Zang and Guo (2018)). Two-stage sample size re-estimation, methods are constructed by revising the final sample size with parameter estimation from the first stage (e.g., Stein (1945), Proschan (2005), Shih (2006), Schwartz and Denne (2006), Zhong et al. (2013), Tarima et al. (2016), Broberg and Miller (2017), and Tarima and Flournoy (2019). In two-stage adaptive optimal designs, information from the first stage is used to estimate optimal treatment assignment probabilities for the second stage (e.g., Haines et al. (2003), Dragalin and Fedorov, 2006, Dragalin et al., 2008, Lane and Flournoy (2012), Englert and Kieser (2013), Lane et al. (2014), Shan et al. (2016)).====Lane and Flournoy (2012) studied asymptotic distributional properties of the maximum likelihood estimator for nonlinear regression models with independent normal errors. In their study, they used the Fisher information to norm the score function when taking limits, obtaining a limiting distribution for the maximum likelihood estimator that is a random scale mixture of normal random variables. Use of this result requires knowledge of the distribution of the limiting scaling random variable. Lane and Flournoy found this distribution in the special case of an exponential mean function. But the method used is not generalizable, and so their result is informative, but not generally useful in practice.====In their review paper on likelihood theory for stochastic processes, Barndorff-Nielsen and Sørensen (1994) describe conditions under which maximum likelihood estimators normed with the Fisher information converge to randomly scaled mixture of normal distributions, as was the case in Lane and Flournoy (2012). Limiting random mixtures of normal random variables also arise in Ivanova et al. (2000), Ivanova and Flournoy (2001), and May and Flournoy (2009). But Barndorff-Nielsen and Sørensen describe a solution to this problem. Namely, they describe how using a random norming in lieu of the Fisher information can lead to a standard normal distribution instead.====This paper examines the use of random normings in a practical situation. In particular, random norms are evaluated in the context of  Lane and Flournoy (2012) and Lane et al. (2014), and how to use them to obtain the usual standard normal distribution is shown. Then rates of convergence and efficiencies of the different norming alternatives are compared.====Accordingly, this paper is organized as follows. In Section 2, the model studied in this paper is described. In Section 3, stable and mixing convergence, and a generalized version of the Cramér–Slutsky theorem are introduced. In Section 4, the main asymptotic results for maximum likelihood estimators with random normings are presented. Finally, simulation studies are conducted to compare the efficiencies obtained with random normings for exponential and logistic models in Section 5.",Random norming aids analysis of non-linear regression models with sequential informative dose selection,https://www.sciencedirect.com/science/article/pii/S0378375819300783,16 September 2019,2019,Research Article,255.0
"Grant Sheridan,Perlman Michael D.,Grant Darren","Department of Statistics, University of Washington, United States of America,Department of Economics and International Business, Sam Houston State University, United States of America","Received 30 June 2018, Revised 12 June 2019, Accepted 4 September 2019, Available online 9 September 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.09.002,Cited by (2),"Statistical methods are developed for assessing the likelihood of prejudicial bias in agent-assigned ====, such as the ordering of candidates on an election ballot. The null hypothesis of an unbiased order assignment is represented by several forms of probabilistic ==== of the random orderings, while bias is represented either by compatibility with an assumed ranking of the items with respect to a hypothesized preference criterion (PC) or by linear concordance with assumed scores of the items on a PC scale. A power analysis indicates the superiority of these methods to a neutral alternative when appropriate a ==== is available; their usefulness is affirmed in an application to the ordering of candidates on 2014 Texas Republican primary election ballots. Significant evidence of bias is found in three of the five races studied, a finding that does not obtain using currently available tests.","Each year an unusual ritual takes place in school district offices, city halls, and county courthouses across the state of Texas: the drawing of the order in which candidates for public office will be placed on the ballot, as required by state law. Candidates often attend these drawings to ensure that the agent conducting them does not manipulate the ordering and place a competitor higher on the ballot, conferring upon them an electoral advantage known as the ====. In Texas primary and runoff elections for statewide office (Grant, 2017) find this effect to be sizeable and monotonic in ballot order, especially in low-profile or low-information races. This finding is corroborated for other states by several studies cited therein, while Meredith and Salant (2013) obtain broadly similar results for local elections in California. The possibility that such orderings might be prejudicially biased, either consciously or unconsciously, by the agents executing them is not far-fetched. Darcy and McAllister (1990) noted the evidence for such bias in their review of the early literature on the ballot order effect.==== ==== One can test statistically for such bias when orderings (i.e. permutations) of the same set of ==== items are conducted repeatedly and (presumably) independently ==== times, as in Texas, where ballot order for primary and runoff elections is determined by agents at the county level, even for offices contested statewide. A test for uniform random ordering when ==== is elementary, but this is not the case when multiple items are being ordered.====The general problem of testing for the uniform randomness of permutations does not just arise in political science. A classic example from computer science is testing random number generators via the randomness of repeated sequences of digits (Knuth, 1981).==== ==== In addition, when ordering contestants in musical or athletic contests, randomization helps ensure fairness because arbiters’ fastidiousness can vary over the course of a day or competition. It is desirable in sequencing courtroom trials for the same reason (Danziger et al., 2011). Most generally, the ballot order effect is an example of a more general psychological phenomenon, the primacy effect (cf. Murdock, 1962), in which the first-listed of a set of options tends to be chosen more frequently. Thus, in many scenarios in which a set of competing decisions must be made sequentially without prejudice, the agents ordering those decisions may be tempted to manipulate the orderings in accordance with their preferences. Testing for randomization should reduce the likelihood of such manipulations and can uncover them when they occur.====Despite the generality of the problem, however, a consensus on testing methodology has not emerged. Even within the ballot order literature, a variety of options are used. Grant (2017) applies Fisher’s Exact Test to the cross-tabulation of candidates and ballot positions in Texas, to determine if this cross-tabulation is likely to have occurred by random chance. Meredith and Salant (2013) apply Pearson’s chi-squared test to determine if incumbent candidates are equally likely to end up at any position on the ballot. And Ho and Imai (2008) apply a series of rank tests based on the average absolute difference in rank between pairs of letters to randomized alphabets that are used for ballot ordering in California.====Each approach has limitations. The first procedure, Fisher’s Exact Test, aggregates the ==== observed orderings into a ==== contingency table, where ==== is the number of items and order positions. Among other problems,==== ==== such aggregation loses relevant information contained in the orderings themselves. It is possible – and, in political applications, probable – that agents with opposing prejudicial biases manipulate orderings in opposite directions, but these offsetting manipulations may not be apparent in the aggregate. Similarly, important information is lost by applying Pearson’s chi-squared test to incumbents alone: all information about non-incumbents is ignored.====In addition, unless ==== is very small or ==== very large, these tests, along with those of Ho and Imai (2008), may lack the sensitivity needed to detect the specific deviations from uniform randomness that may be encountered. This limited sensitivity would derive, in part, from the untargeted nature of such tests, which do not utilize ==== information that could increase their power. Such information is often available in political and other applications in which human agents perform the orderings; any deviations from uniform randomness are likely to reflect these agents’ preferences.====In this paper, procedures are developed that utilize the individual orderings, not their aggregation, and that will detect departures from uniform randomness that are to be expected from ==== information about the characteristics of the items and the preference criteria of the agents executing the ordering. In Section 2, the null hypothesis of an unbiased order assignment is represented by several forms of exchangeability of a random permutation. In Section 3, the alternative hypothesis of bias in order assignment is represented by compatibility with an assumed preferential ranking (ties permitted) of the items, while in Section 4 bias is represented by linear concordance with assumed preference scores of the items. In both cases methods for detecting the corresponding alternatives are obtained. Section 5 analyzes these tests’ power relative to one another and an neutral alternative – the rank test of Ho and Imai (2008) – and outlines their practical application when the true form of deviations from uniform randomness is unknown. In Section 6 these procedures are applied to five races in the 2014 Texas Republican primary. Significant evidence of bias in at least one of the approximately 245 reporting counties is found in three of the five races; in two of these, significant evidence is found for bias in at least six and ten counties.====The tests developed in this paper rely on assumptions about agents’ preferences that, while appropriate in the political context, are somewhat strong. A sequel develops power-enhancing tests for more general sets of preferences and for the most general case of all, in which no ==== knowledge is available.","Targeted testing for bias in order assignment, with an application to Texas election ballots",https://www.sciencedirect.com/science/article/pii/S0378375818300867,9 September 2019,2019,Research Article,256.0
"Chen Xin,Ma Xuejun,Zhou Wang","Department of Statistics and Data Science, Southern University of Science and Technology, 518055, Shenzhen, China,School of Mathematical Sciences, Soochow University, 215006, Suzhou, China,Department of Statistics and Applied Probability, National University of Singapore, 117546, Singapore","Received 4 October 2018, Revised 27 August 2019, Accepted 1 September 2019, Available online 6 September 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.09.001,Cited by (5),"Linear regression is a fundamental and popular statistical method. There are various kinds of linear regression, such as mean regression and ==== also demonstrate the effectiveness and the flexibility of the proposed method.","Linear regression is an important tool to explore the relationship between the response and predictors in statistics, with much existing work including mean regression and quantile regression.====To study regression, we start with ====, a random sample from population ====, where ==== is a p-dimensional predictor, ==== is a univariate response, and ====’s satisfy: ====Here ==== is a p-dimensional vector of unknown parameters, ==== is the intercept term, and the error terms ====’s are independent and identically distributed (i.i.d.) with unknown density function ====, and are assumed to be independent of ====’s.====The commonly used method to estimate ==== is to find ==== which minimizes the objective function: ====If ====, then ==== is the mean regression or least square estimator. If ==== with ====, then ==== is the quantile regression estimator; see Koenker and Bassett (1978). On the other hand, if the density function ==== is known, the maximum likelihood is the best approach to estimating ====. Clearly the maximum likelihood function is ====As we know, if the error density function ==== is the normal density, the maximum likelihood estimator is equivalent to the mean regression estimator, and if ==== is the Laplace density, the maximum likelihood estimator is the same as the quantile regression estimator (Geraci and Bottai, 2006). There is a huge body of literature about the semiparametric models. The semiparametric efficiency for the estimation of the parametric part can be established, provided that the density of the error is symmetric about zero and absolutely continuous with respect to the Lebesgue measure (Bickel et al., 1993). However, in reality, we do not know how the distribution of the error term looks like. We might have to test whether it is symmetric or not, unimodal or multimodal. This motivates us to apply nonparametric techniques to estimate ==== as a first step. After this we plug our estimated ==== into (1.3) and find the minimizer ====. This is the method we propose in the article. We call it the kernel density regression, which is a distribution free linear regression and can be considered as generalizations of both mean regression and quantile regression. We also note that a similar idea appeared in Yao and Zhao (2013). However they require that the density of the error is symmetric about zero or has a positive lower bound.====We show that our kernel density regression estimator is robust and possesses nice finite and large sample properties. The establishment of our asymptotic theories is very challenging because our nonparametric likelihood function cannot be transformed into sum of i.i.d. random variables. Numerical studies indicate that the kernel density regression is much better compared to existing methods under many settings, particularly for asymmetrical heavy-tailed distribution or multimodal distribution of the error term. Our procedure essentially can be applied to other models, such as single or multiple index models, partial linear model and varying coefficient model. As a general robust approach, it has potentials to be used in testing the goodness of parameter estimation of mean regression.====When there are many predictors or factors, we would like to determine a smaller subset that is most related to the response so that we have a concise model. The penalty is a very popular and powerful tool in the linear regression, which sacrifices a little bias. However it can reduce the variance of the predicted values and hence may improve the overall prediction accuracy. This idea may date back to Tibshirani (1996), who studied the mean regression via LASSO. Later on, the adaptive LASSO (Zou, 2006), SCAD (Fan and Li, 2001), MCP (Zhang, 2010) and many others were proposed. Other related works to our paper are: SCAD and adaptive LASSO in the quantile regression (Wu and Liu, 2009), local quadratic approximation (LQA, Fan and Li (2001)) and local linear approximation (LLA, Zou and Li (2008)). In this article, we combine adaptive LASSO with our method under the high dimensional setting and adopt LQA in the numerical estimation. Furthermore, a new iterative coordinate descent algorithm is suggested for our estimator, which is an extended version of coordinate descent algorithm (Wu and Lange, 2008, Lin et al., 2014).====The rest of this article is organized as follows. In Section 2, we propose the kernel density regression and establish its theoretical property. Sparse estimation in the high dimension setting is presented in Section 3. In Section 4, we examine the finite sample performance of the proposed method via Monte Carlo simulations, and also illustrate the effectiveness through several empirical examples. All technical proofs of the main results are given in Appendix.",Kernel density regression,https://www.sciencedirect.com/science/article/pii/S0378375818303264,6 September 2019,2019,Research Article,257.0
"Wang Lei,Ma Xuejun,Zhang Jingxiao","Center for Applied Statistics, School of Statistics, 100872, Renmin University of China, China,School of Mathematical Sciences, Soochow University, China","Received 4 November 2018, Revised 27 May 2019, Accepted 29 August 2019, Available online 5 September 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.08.005,Cited by (1),"This paper introduces a sure screening method for ultrahigh-dimensional additive ==== ones. This screening process ranks the nonparametric components according to their norms of the marginal likelihood estimate. Under appropriate conditions, the proposed method is shown to possess sure screening property with a vanishing false selection rate. In ","Additive model, whose first reference goes back to Stone (1985), is more flexible compared with parametric models, since it relaxes model assumption and requires less prior information. Shortly afterwards, Hastie and Tibshirani (1986) introduced generalized additive model of the form ====. The generalized additive model with logit link, known as additive logistic model, is very useful in social and biological research where binary outcome exists, such as disease detection. The rise of big data calls for effective feature screening methods to deal with the challenge of locating factors that have an influence on the outcome, in the context of ultrahigh-dimensional additive logistic models.====Many component selection approaches have been developed for generalized additive models. When responses are related within the same cluster, Xue et al. (2010) applied SCAD penalty to quadratic inference function (Qu et al., 2000) to achieve the goal of simultaneous estimation and variable selection. The framework proposed by Wang et al. (2014), under the setting of generalized additive partial linear models, also derives from penalized quadratic inference function for correlated data. However, in many circumstances such as multicenter clinical studies, outcomes are unrelated if samples are collected from diverse regions. The existence of linear components in a generalized additive model also needs to be verified. Therefore, applications of Wang et al. (2014) and Xue et al. (2010) are stunted in aforementioned situations. In a manuscript, Chouldechova and Hastie (2015) introduced GAMSEL (Generalized Additive Model Selection), in which a selection penalty and an end-of-path penalty are innovatively added to the loss function. Nevertheless, GAMSEL is not applicable in ultrahigh dimensional settings because of the complexity of solving convex optimization problem with elaborate penalties. To deal with feature screening for ultrahigh-dimensional categorical data, Fan and Song (2010) established a linkage between the estimate of coefficient of predictor-wise regression and the marginal correlation in generalized linear model, a special case of generalized additive model.====In literature, model-free screening procedures are also appealing. Since response of additive logistic model is binary, here we just focus on approaches mitigating the impact of high dimensionality in classification problems. Fan and Fan (2008) extracted salient features according to two-sample t-test statistic, resulting in Features Annealed Independence Rules (FAIR). Besides, Mai and Zou (2013) introduced a feature screening procedure based on Kolmogorov–Smirnov statistic. Cui et al. (2015) defined ====, which can be seen as the weighted average of Cramér–von Mises distances, as a measure of marginal utility for predictors in ultrahigh dimensional discriminant analysis. Pan et al. (2016) proposed a procedure named after pairwise sure independence screening (PSIS) based on absolute mean difference. Tibshirani et al. (2002) eliminated noise genes by shrinking centroids of genes via soft thresholding (PAMR). DC-SIS, which is demonstrated by Li et al. (2012), derives from distance covariance and is effective in the presence of nonlinear relationship between ==== and ====. We refer to Liu et al. (2015) for a comprehensive introduction to feature screening methods.====Model-free screening procedures do not rely on strong modeling assumptions, therefore, they are expected to exhibit desirable performance when model is misspecified. Nevertheless, if there is prior information indicating that the effects of covariates take an additive form, it benefits more from adopting screening approaches designed for additive logistic models. In fact, additive logistic models significantly increase both flexibility and interpretability of ordinary parametric models. It is worth noting that the proposed method, ALNIS, enjoys desirable theoretical properties under mild conditions. Additive components are only subjected to some smoothness assumptions and boundedness assumptions. In this paper, we focus on feature screening for additive logistic models for ultrahigh-dimensional data. We first approximate additive components by linear combinations of B-spline basis functions. Then we fit ==== component-wise regressions using these approximations instead of ====, through which nonparametric component regression problems are transformed into parameter estimation problems. For the purpose of feature screening, we rank norms of the marginal likelihood estimate of nonparametric components. The reason for choosing this measure is explained in detail in the following pages. We proved that the proposed procedure possesses sure screening property (Fan and Lv, 2008) with a vanishing false selection rate. Further, we supplement the theoretical study with simulation studies and a real data analysis.====The remainder of this paper is organized as follows. In Section 2, we briefly review the definition of additive logistic models and then introduce the feature screening method under sparse and ultrahigh-dimensional setting. Theoretical properties of the proposed method are presented in Section 3. Section 4 contains the results of simulation studies and a real data analysis. We wrap up with some discussions in the last section and relegate the proofs to Supplementary Materials.",Feature screening for ultrahigh-dimensional additive logistic models,https://www.sciencedirect.com/science/article/pii/S0378375819300771,5 September 2019,2019,Research Article,258.0
"Eck Daniel J.,Geyer Charles J.,Cook R. Dennis","Department of Statistics, University of Illinois, Champaign-Urbana, IL, United States,School of Statistics, University of Minnesota, Minneapolis, MN, United States","Received 29 August 2018, Revised 10 May 2019, Accepted 11 August 2019, Available online 2 September 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.08.002,Cited by (4), flowers is provided. Useful variance reduction is obtained in both analyses.,"Life history analysis is the study of a population of organisms as they progress through their lifecycle. A plant or animal’s Darwinian fitness – the number of offspring produced over its lifetime – is the primary outcome measure in life history analysis. The estimation of expected Darwinian fitness, the expected lifetime number of offspring an organism has, is one of the most important procedures in evolutionary biology. The importance of life history analyses are not just limited to evolutionary biology, it is important for public policy. With genetic theory and simulation studies, Burger and Lynch (1995) show that, under certain conditions, a changing environment leads to extinction of species. In a field study, Etterson and Shaw (2001) argued that the predicted evolutionary response to predicted rates of climate change are far too slow. In these papers, and all life history analyses of their kind, Darwinian fitness is the response variable. The interesting scientific conclusions are drawn from it.====In many life history analyses, values of expected Darwinian fitness are plotted using a fitness landscape (Lande and Arnold, 1983, Shaw and Geyer, 2010). A fitness landscape is the conditional expectation of Darwinian fitness given phenotypic trait values considered as a function of those values. When fitness is the response variable in a regression model and phenotypic traits are the covariates, the fitness landscape is the regression function. Estimation of this regression function is not a straight forward task since the lifecycles of individual organisms vary in complexity across organisms within and between species. Estimation of the fitness landscape began with Lande and Arnold (1983). They use ordinary least squares regression of fitness on phenotypes to estimate the best linear approximation of the fitness landscape and quadratic regression to estimate the best quadratic approximation. Here “best” means minimum variance unbiased, as in the Gauss–Markov theorem. Their use of ==== and ==== tests and confidence intervals requires the assumption that fitness is conditionally homoscedastically normally distributed given phenotypic trait values. This assumption is almost always grossly incorrect when one uses a good surrogate for Darwinian fitness (Mitchell-Olds and Shaw, 1987, Shaw et al., 2008).====Aster models (Geyer et al., 2007, Shaw et al., 2008) were designed to fix all of the problems of the Lande and Arnold (1983) approach and of all other approaches to life history analysis (Shaw et al., 2008). The aster model is the state-of-the-art model for life history analyses in which the estimation of expected Darwinian fitness is the primary goal. Aster modeling allows one to combine estimates of selection acting through different components of fitness in a unifying statistical framework to gain insight into overall Darwinian fitness (Siepielski et al., 2011). Geyer et al., 2007, Shaw et al., 2008, Stanton-Geddes et al., 2012, Shaw et al., 2015 and Eck et al. (2015) show various kinds of life history data for which aster models are necessary. Assumptions for aster models are given in Section 2.====In this paper, we demonstrate an efficient approach to computing expected Darwinian fitness by combining a variance reduction technique called “envelope methodology” (Cook et al., 2010, Cook and Zhang, 2015, Su and Cook, 2011) with aster models to estimate the fitness landscape in life history analysis. The primary emphasis is that this combination of methods estimates the fitness landscape with less variability than is possible with aster models alone. We first show how existing envelope estimators constructed from the 1D algorithm (Cook and Zhang, 2015, Cook and Zhang, 2016, Zhang and Mai, 2018) can reduce variability in estimation of the fitness landscape. We then develop a new envelope estimator that avoids the potential numerical pitfalls of the 1D algorithm. Variance reduction is assessed using parametric bootstrap techniques in Efron (2014, Section 4). These bootstrap algorithms account for variability in model selection. Our methodology provides the most precise estimation of expected Darwinian fitness to date. Researchers using our methods can therefore draw stronger conclusions about the driving forces of Darwinian fitness from their life history analyses.====In a life history analysis of ==== flowers and a simulated example, we show that our methodology leads to variance reduction in estimation of expected Darwinian fitness when compared with analyses that use aster models alone. We show that this variance reduction leads to sharper scientific inferences about the potential causes of Darwinian fitness in the ==== life history analysis. Our examples are fully reproducible, and the calculations necessary for their reproduction are included in an accompanying technical report (Eck et al., 2018).",Combining envelope methodology and aster models for variance reduction in life history analyses,https://www.sciencedirect.com/science/article/pii/S0378375818302374,2 September 2019,2019,Research Article,259.0
"Detmer Felicitas J.,Cebral Juan,Slawski Martin","Department of Bioengineering, George Mason University, Fairfax, VA 22030, USA,Department of Statistics, George Mason University, Fairfax, VA 22030, USA","Received 17 May 2018, Revised 27 March 2019, Accepted 19 August 2019, Available online 30 August 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.jspi.2019.08.003,Cited by (5)," in the presence of an intercept, or equivalently, using one of various coding schemes. As proposed in Yuan and Lin (2006), the group lasso is a natural and computationally convenient approach to perform variable selection in settings with categorical ","The treatment of categorical predictor variables is covered in many widely used textbooks on linear regression. Given ==== observations ==== of a categorical variable with levels ====, we can define indicator variables ==== such that ==== if ==== and ==== otherwise, ====, ====. Denoting the corresponding regression coefficients by ====, we note that the parameters of a linear predictor-based regression model with intercept ====
 ====where ==== denotes the linear predictor for observation ====, are not identifiable as can be seen, e.g., from the corresponding matrix representation ====with ==== representing a vector of ones for a positive integer ====, and ==== and ==== denoting the column-wise respectively row-wise concatenation of matrices ==== and ==== having an identical number of rows respectively columns. By construction, ====, where ==== returns the column space of a matrix, and thus ==== has a non-trivial null space. Identifiability of parameters can be restored by imposing linear constraints on ====. Common constraints include ==== for some ==== or ====. Model (1) can accordingly be re-parameterized as ====with the columns of ==== forming a basis of the linear space ==== with ==== for ==== such that ====; here, “＋ ” denotes the sum of linear spaces. Depending on the specific choice of the linear constraint represented by ====, the matrix ==== in (2) can be chosen to match common coding schemes (see Fig. 1 for an illustration), e.g.:====: ==== is such that ==== if ====, ====, while all other entries are equal to zero; without loss of generality, the ====th level is here taken as reference category.====: ==== is such that for ====, we have ==== and ====, ====, ==== if ====, and if ====, then ==== for all ====.====: ==== is such that for ====, we have ==== for ====, ==== for ====, and ==== if ====.====Consider now generic model fitting problems of the form ==== where ==== are observed responses and ==== is a loss function that is strictly convex in its second argument. Let ==== and ==== denote the minimizers of (3), (4), respectively. In virtue of the requirement ====, minimization problems (3), (4) are equivalent in terms of fit, i.e., ====independent of the specific coding scheme underlying ====, and also independent of whether the linear constraint on ==== in (3) matches ==== in (4). If ==== is in correspondence to ==== according to the explanation after (2), it additionally holds that ====.",A note on coding and standardization of categorical variables in (sparse) group lasso regression,https://www.sciencedirect.com/science/article/pii/S0378375819300758,30 August 2019,2019,Research Article,260.0
"Liu Wenyu,Coad D. Stephen","Cancer Research UK Clinical Trials Unit, Institute of Cancer and Genomic Sciences, University of Birmingham, Edgbaston, Birmingham, B15 2TT, UK,School of Mathematical Sciences, Queen Mary, University of London, Mile End Road, London E1 4NS, UK","Received 3 May 2018, Revised 22 August 2019, Accepted 25 August 2019, Available online 29 August 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.08.004,Cited by (1),"Previous work on two-treatment comparisons for immediate responses has shown that the use of optimal response-adaptive randomisation with group sequential analysis can allocate more patients to the better-performing treatment while preserving the error rates. In this paper, the application of the combined approach to censored survival responses is investigated and different optimal response-adaptive randomised procedures are compared. For a maximum duration trial, the information level at the final look is usually unpredictable. An approximate information time is defined. Group sequential tests and optimal allocations for two measures of treatment difference are given. Operating characteristics of the combined approach are investigated by simulation, including cases of exponential and Weibull survival responses and redesign of a clinical trial. The results reveal that the existing boundaries for standard group sequential designs derived based on the error-spending approach can be applied as approximate tests to control the overall type I error rate. Compared to the group sequential complete randomisation design, the combined approach is found to retain ethical advantages as in previous work on immediate responses while the power is not adversely affected.","Periodic group sequential designs, in which a number of interim analyses are conducted after groups of observations, can require fewer patients than a fixed-sample design to achieve the same error probabilities (Jennison and Turnbull, 2000). Since early termination of trials is allowed, patients can be prevented from being exposed to inferior or unsafe treatments. In addition, response-adaptive randomisation, which skews the allocation proportion of the sample sizes towards the more promising treatments based on the cumulative responses, can further reduce the numbers of participants allocated to the inferior treatments compared to complete randomisation (Atkinson and Biswas, 2014). The use of the combined approach of group sequential analysis and response-adaptive randomisation can achieve both individual and collective ethics.====Few studies of the application of the combined approach to two-armed trials with immediate responses have been investigated.  Jennison and Turnbull (2001) derived theory to support that the combined approach still maintains the overall error rates for two-armed normal trials with known variances. The authors proved that the joint distribution of the test statistics has a standard form similar to that for a group-sequential non-adaptive design, but with the additional feature that the information level can depend on previous test statistics. A reduction in the inferior treatment number can be achieved at a cost of a slight increase in the expected total sample size. In addition, Morgan (2003a) proposed two inferential methods for the treatment mean difference following such a group-sequential response-adaptive design: an approximate confidence interval using a pivotal method and a bias-adjusted maximum likelihood estimator.====Morgan (2003b) investigated the combined approach for normal responses with unknown variances. As inaccurate estimates of the variances of the responses can influence the power considerably, she suggested using sample size re-estimation based on the new estimates of the variances updated by the observed responses. For two-armed binary trials, Morgan and Coad (2007) compared several adaptive allocation rules in a group sequential setting, including urn-model type designs and the doubly-adaptive biased coin design (DBCD) (Eisele and Woodroofe, 1995). Among the designs they investigated, the drop-the-loser rule (Ivanova, 2003) is found to be the most efficient method for achieving the competing objectives of reducing the expected number of failures and the expected total sample size.====For normal and binary responses, Zhu and Hu (2010) considered monitoring the DBCD at continuous information time utilising critical boundaries derived by the error-spending approach (Lan and DeMets, 1983). They considered the ====-spending function, which spends the type I error rate as a function of the information time. Their simulation results revealed that the use of the combined approach can preserve the advantages of both group sequential analysis and optimal response-adaptive randomisation.====In this paper, the combined approach generalised to censored survival responses is explored, which allows staggered entry and right-censoring. We initially assume an exponential survival model in which the arrival and censoring times are both uniformly distributed. Since such a model may be misclassified in practice, some of these assumptions are later relaxed and a more robust approach considered.====For survival responses, the information levels usually cannot be attained accurately, since they depend on the realised pattern of events and censoring (Jennison and Turnbull, 2000). There are maximum duration trials and maximum information trials. The former are more feasible in practice, since the maximum length of the trials is fixed, whereas the latter consider a pre-determined maximum information level. In practice, the trial may not achieve the required information level at the end of the study, or the information level may be attained soon after the trial begins. We consider maximum duration trials that use an approximate information time. The optimal response-adaptive randomisation procedures are used to target different optimal allocations derived based on some optimality criteria, for instance, minimising the total sample size or the expected number of failure events. In addition to the DBCD, we also consider the efficient randomised-adaptive design (ERADE) (Hu et al., 2009).====The structure of the remaining sections is as follows. In Section 2, the parametric model for the responses, which characterises the staggered entry, the right-censoring and the survival time, is introduced. An approximate information time based on the model assumptions is defined. Group sequential tests for two measures of treatment difference, the simple difference and the log hazard ratio, are given in Section 3. In Section 4, the optimal allocations derived based on different optimality criteria for the two measures of treatment difference are shown. Then optimal response-adaptive randomisation procedures, which aim to target the pre-specified optimal allocations, are described. Simulation results comparing the designs are presented in Section 5, including the error probabilities, the expected number of patients, the expected number of failures and the average allocation proportion with its variability. In addition, the redesign of a clinical trial is investigated. Conclusions and further work are in Section 6. Supplementary material provides the derivation of the probability of an event for the model-based approach, comparison with a nonparametric approach based on the logrank test and a simulation study of model misspecification.",Group-sequential response-adaptive designs for censored survival outcomes,https://www.sciencedirect.com/science/article/pii/S037837581930076X,29 August 2019,2019,Research Article,261.0
"Mondal Anirban,Mandal Abhijit","Case Western Reserve University, Cleveland, OH 44106, USA,Wayne State University, Detroit, MI 48202, USA","Received 21 January 2019, Revised 3 July 2019, Accepted 12 August 2019, Available online 20 August 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.08.001,Cited by (8),A new approach of obtaining ,"Mathematical models are widely used by engineers and scientists to describe physical, economic, and social processes. Often these models are complex in nature and are described by a system of ordinary or partial differential equations, which cannot be solved analytically. Given the values of the input parameters of the process, complex computer codes are widely used to solve such systems numerically, providing the corresponding outputs. These computer models, also known as forward models, are used for prediction, uncertainty analysis, sensitivity analysis, and model calibration. For such analysis, generally a Monte Carlo approach is used, where the computer code is evaluated multiple times to estimate a given function of the outputs. However, complex computer codes are often too expensive to run multiple times. Design of computer experiment plays an important role in such situations, where the goal is to choose the input configuration in an optimal way such that the uncertainty and sensitivity analysis can be done with a smaller number of simulation runs. Moreover, the computation cost is often reduced by replacing the computer model either by a simpler mathematical approximation, called as the surrogate or meta model (Volkova et al., 2008, Simpson et al., 2001), or by a statistical approximation, called emulator (Oakley and O’hagan, 2002, O’Hagan, 2006, Conti et al., 2009). Such surrogate models or emulators are also widely used for model calibration (Kennedy and O’Hagan, 2001). The surrogate or the emulator model is built based on set of training simulations of the original computer model. The full optimal exploration of the variation domain of the input variables is therefore very important in order to avoid non-informative simulation points (Fang et al., 2006, Sacks et al., 1989).====Mainly two types of designs are widely used in computer simulations — input based space filling design and model or output based design. The focus of this article is on the first type of input based design for computer simulations with random inputs. In most situations, the simple random sampling (SRS) needs a very large number of samples from the input space to achieve a fixed level of efficiency in Monte Carlo estimation of a function of such computer model outputs. The required sample size increases rapidly with the increase in dimension of the input space. On the other hand, the Latin hypercube sampling (LHS) introduced by McKay et al. (1979) needs smaller sample sizes than the SRS to achieve the same level of efficiency. Compared to the SRS, which only ensures independence between samples, the LHS also ensures the full coverage of the range of the input variables through stratification over the marginal probability distributions of the inputs. Thus, the LHS method is more efficient and robust if the components of the inputs are independently distributed. In case of dependent inputs Iman and Conover (1980) have proposed an approximate version of the LHS based on the rank correlation. Stein (1987) further improved this procedure so that the sample vector has approximately the correct joint distribution when the sample size is large. However, it is noticed that in many scenarios, these rank-based methods result in a large bias and small efficiency of the estimators, particularly for small sample sizes. Moreover, the joint distribution of the inputs is also not completely preserved in these sampling schemes even for moderately large sample sizes. Therefore, these rank-based techniques are not very useful for many real applications where one is restricted to a small sample size due to the computational burden of the expensive forward simulator. To overcome this situation, we propose a novel sampling scheme for dependent inputs that precisely gives a random sample from the target joint distribution while keeping the mean squared error of the estimator smaller than the existing methods. In the traditional LHS, where the components of inputs are independent, the stratification of the marginal distributions leads to the stratification of the joint distribution. However, for dependent random variables, all the marginal distributions and the joint distribution cannot be stratified simultaneously. Here, we propose a new sampling scheme, called the Latin hypercube sampling for dependent random variables (LHSD), where we ensure that the conditional probability distributions of the inputs are stratified. The main algorithm of the LHSD is similar to the traditional LHS, hence it retains all important properties of the LHS. The joint distribution of the inputs is preserved in our sampling scheme as it is precisely the product of the conditional distributions.====In some practical situations the joint probability distribution of the inputs, and hence the corresponding conditional probability distributions may be unknown. In these situations, we propose a copula-based method to construct the joint probability distribution from the marginal distributions. For finite sample, it is shown that the variance of the estimators based on LHSD is always smaller than the variance of the estimator based on the SRS. The large sample properties of the LHSD based estimators are also provided. We consider two simulation-based examples and one practical example, where the results show that the traditional LHS and rank-based LHS method have considerable bias in the estimator when the inputs are dependent. Our proposed LHSD outperforms the SRS, the traditional LHS, and rank-based LHS in terms of the mean squared error (MSE), for small and moderately large sample sizes. The simulation results also show that the rank-based LHS fails to retain the joint probability distribution of the input variables even for moderately large sample sizes, which is the reason for the considerable bias in the estimators. On the other hand, the joint distribution is completely retained in our proposed sampling scheme and hence the estimators are unbiased.====The paper is organized as follows. In the next section, we first formulate the estimation problem, then we describe the LHS algorithm for independent inputs (McKay et al., 1979) and our proposed LHSD algorithm for dependent inputs. Another variant of the proposed method, which will be called the centered Latin hypercube sampling for dependent random variables (LHSD====), is also described here. The use of copula, when the joint probability distribution of the inputs is not known, is also discussed in this section. The large sample properties of the estimators using the LHSD are discussed in Section 3. Section 4 provides the numerical results from two simulation examples, where the performance of different sampling schemes are compared. The application of the proposed sampling scheme to a real field example on a river model is presented in Section 5. A concluding remark is given at the end of the paper, and the proofs of the theorems are provided in the Appendix.",Stratified random sampling for dependent inputs in Monte Carlo simulations from computer experiments,https://www.sciencedirect.com/science/article/pii/S0378375819300746,20 August 2019,2019,Research Article,262.0
"Baek Changryong,Kechagias Stefanos,Pipiras Vladas","Department of Statistics, Sungkyunkwan University, 25-2, Sungkyunkwan-ro, Jongno-gu, Seoul, 110-745, Korea,SAS Institute, 100 SAS Campus Drive, Cary, NC 27513, USA,Department of Statistics and Operations Research, UNC at Chapel Hill, CB#3260, Hanes Hall, Chapel Hill, NC 27599, USA","Received 12 April 2018, Revised 13 June 2019, Accepted 24 July 2019, Available online 12 August 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.07.007,Cited by (5),"Several methodological and numerical issues behind the local Whittle estimation of long and short memory in ==== ==== with possible fractional cointegration are reexamined. These issues include the ==== for all model parameters, local Whittle plots for phase parameter and fractal connectivity, and others. For fractal connectivity, in particular, it is advocated to work with a model parametrization for which the model parameters associated with this phenomenon are identifiable and could be tested naturally within the local Whittle estimation framework. A simulation study and data applications are also considered.","Our goal is to clarify a number of issues behind the local Whittle estimation in bivariate stationary systems proposed and studied by Robinson (2008), with particular attention to long memory (see also Nielsen (2007), Nielsen and Shimotsu (2007), Shimotsu, 2007, Shimotsu, 2012 and Nielsen (2011)). More specifically, consider a bivariate (second-order) stationary time series ==== with zero mean (for simplicity), the autocovariance matrix function ====, ====, and the spectral density matrix ====, ====, assumed to exist and, by convention, to satisfy ====. Suppose that as ====, ====where ====, ====, ====, and ==== is Hermitian symmetric and positive definite (i.e.==== ==== and ====). In (1.1), the asymptotic equivalence ==== is entry-wise and ==== for a constant ==== and a power ==== means that ====, including the case when ====. For univariate series, the case ==== is associated with long memory, ==== with short memory and ==== with anti-persistence (e.g. Robinson (2003), Giraitis et al. (2012), Beran et al. (2013) and Pipiras and Taqqu (2017)). Writing ====, ====, ====, ==== with ====, ==== and setting ====, the relation (1.1) can also be expressed as ====where ==== and ==== is a real-valued, symmetric, positive definite matrix. The bar above ==== in (1.2) indicates complex conjugation componentwise. The parameters ==== are known as memory parameters (long-memory parameters when they are positive) and ==== as a phase parameter (at the zero frequency).====We gather the model parameters above as ====It will also be convenient to work in ====where ==== are the real and imaginary part of ====, namely satisfying ====. (The letters “P” in (1.3) and “C” in (1.4) stand for “Polar” and “Complex”, respectively.) One reason that we are interested in Parametrization C is related to the so-called ====, which is associated with the case ====and that of fractal non-connectivity with the case ====. See, for example, Achard et al. (2008), Wendt et al. (2009), Kristoufek (2013) and Wendt et al. (2017). But testing for fractal non-connectivity is not possible in Parametrization P since for ====, the model parameter ==== is not identifiable. There is no such issue in Parametrization C, where fractal connectivity ==== is now associated with ====and the case ==== with ==== and ====.====Working with Parametrization P, Robinson (2008) introduced and studied the local Whittle estimators of ====, ==== and ==== defined as ====with ====where ==== denotes the determinant of a matrix ====, ==== are the Fourier frequencies for a sample size ====, ====is the periodogram of the series ==== and ==== is the number of frequencies used in estimation. The optimization problem (1.7) is taken in theory essentially over ====, ==== and positive definite ====; in practice, as in their Whittle plots (e.g. Section 6), one often allows ====, ==== to exceed ====. The asymptotic normality result for ==== is provided in Robinson (2008) under suitable assumptions, in particular, on ====.====Robinson (2008) also considers the case of possible fractional cointegration, where (1.1) is replaced by ====with ====the case ==== corresponding to (1.1) with no fractional cointegration and the case ==== associated with fractional cointegration. Furthermore, it is assumed that ====. In the local Whittle estimation (1.7)–(1.8), ==== is added as another parameter, with the negative log-likelihood having the same form as (1.8) but replacing ==== by ====. Robinson (2008) also established the asymptotic normality of the local Whittle estimators ====, ==== and ====. We also note that the spectral density in the fractionally cointegrated case (1.10) with ==== can be expressed as (1.1) but with the parameters in (1.1) (not to be confused with those in (1.10)) satisfying ====(see also Remark 2.4). But the possibility of the second relation in (1.12) is excluded in the asymptotics of the local Whittle estimation (1.7)–(1.8).====Local Whittle estimation plays a fundamental role in the analysis of time series, especially when long memory is thought to be present. Our work contributes to the understanding of the estimation method in the following ways:====In all our asymptotic results for both parametrizations, we provide closed form expressions for the limiting covariance matrices of the local Whittle estimators.====The rest of the paper is organized as follows. Our asymptotic normality results are presented in Section 2, and optimization issues are discussed in Section 3. We present a small numerical study to assess the validity and relevance of the derived asymptotic results and suggested test statistics in Section 4. We discuss local Whittle plots in Section 5, and consider real data applications in Section 6. Section 7 contains conclusions, including some open questions. All technical proofs are moved to Appendix A Proofs of, Appendix B Proofs of, Appendix C Proofs for optimization reduction.",Asymptotics of bivariate local Whittle estimators with applications to fractal connectivity,https://www.sciencedirect.com/science/article/pii/S0378375819300734,12 August 2019,2019,Research Article,263.0
"Novikov Andrey,Reyes-Pérez Pedro","Department of Mathematics, Metropolitan Autonomous University - Iztapalapa, San Rafael Atlixco #186, col. Vicentina, C.P. 09340, Mexico City, Mexico","Received 11 December 2018, Revised 19 July 2019, Accepted 24 July 2019, Available online 2 August 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.07.005,Cited by (1),"This article deals with problems of sequential testing of two simple hypotheses about the distribution of a stochastic process. We consider sequential testing procedures with a finite maximum number (====, ====) of stages. Under some natural assumptions about the structure of the cost of observations, we describe the sequential procedures minimizing the average cost in the class of all ====-stage sequential tests with respect to the Wald’s SPRT and the Neyman-Pearson test, for ====, 3 and 4 stages.","This article is motivated by the results of Novikov (2004) about the efficiency of two-stage hypotheses tests. As shown in Novikov (2004), when testing two simple hypotheses, two-stage tests are “in the middle” between fully sequential and non-sequential (one-stage) tests, from the point of view of their efficiency. In this article, we construct the optimal sequential tests based on (at most) ==== stages, ====. Numerical evaluation of their efficiency is done in the particular case of a Wiener process with a linear drift.====We suppose that the observations are generated by a stochastic process, and have to be used in a sequential statistical procedure, with the aim to distinguish between two simple hypotheses about the distribution of the process. Throughout the article we suppose that the observations are received in a finite number ==== of stages (====) whose durations are fixed and chosen before the experiment starts. In particular, for the discrete-time case, this covers the case of “unitary stage durations”, when every process value ==== can potentially be observed. The sequential tests corresponding to this case are usually called in the literature “truncated”, o “finite-horizon” tests.====Another important particular case of the present scheme isthe so-called “group-sequential” tests, corresponding to group sizes larger than 1 (typically, groups of the same size (====) are used, see Jennison and Turnbull (1999) for very important applications).====In the continuous-time case, the respective stages are formed by “slices” of the process ====, ====, with some positive stage durations ====, ====. Alternatively, a discretized version of a continuous-time process may be observed, when the observations available for the statistical analysis are ====, ====.====Assuming a natural cost structure, we construct the ====-stage tests minimizing average cost, in the class of the tests whose type I and type II error probabilities do not exceed some given levels. In Section 2, we introduce the necessary definitions and assumptions and formalize the problem. Section 3 deals with the structure of optimal ====-stage sequential tests.====In Section 4, the general theory is applied to the case of a Wiener process with a linear drift. Conclusions are drawn.",Optimal multistage sequential hypothesis testing,https://www.sciencedirect.com/science/article/pii/S0378375819300710,2 August 2019,2019,Research Article,264.0
"Stocker Russell,Adekpedjou Akim","Department of Mathematical and Computer Sciences, Indiana University of Pennsylvania, Indiana, PA 15705, United States of America,Department of Mathematics and Statistics, Missouri University of Science and Technology, Rolla, MO 65409, United States of America","Received 25 October 2018, Revised 4 June 2019, Accepted 24 July 2019, Available online 1 August 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.07.006,Cited by (1),"Recurrent event data is a special case of multivariate lifetime data that is present in a large variety of studies from numerous disciplines. Due to its pervasiveness, it is essential that appropriate models and inference procedures exist for its analysis. We propose a general class of additive semiparametric models for examining recurrent event data that uses an effective age process to take into account the impact of interventions applied to units after an event occurrence. The effect of ","Recurrent event data occurs in a wide variety of settings and disciplines including the biomedical, engineering, and social sciences. Examples include the multiple hospitalizations of kidney transplant patients, software programs that experience numerous bugs, turnover in presidential cabinets, and recidivism of former incarcerated individuals. The nature of the data presents numerous challenges when developing statistical methodology. These include taking into account the dependency of event times, the effect of interventions applied to units after an event, and the influence of covariates.====Numerous statistical models and inference procedures are currently available for analyzing recurrent event data. These include marginal intensity approaches (Wei et al., 1989) and condition intensity models (Prentice et al., 1981, Andersen and Gill, 1982, Chang and Wang, 1999). Methodologies using the rate function have also been proposed such as those examined in Pepe and Cai (1993), Lawless and Nadeau (1995), Lin et al. (2000), and Schaubel et al. (2006). Additional methodologies for recurrent event data are available in the texts of Hougaard (2000), Rigdon and Basu (2000), and Cook and Lawless (2007).====Semiparametric models for recurrent event data that assume the effect of covariates is multiplicative are very popular and make up a large percentage of the proposed methodologies. The multiplicative assumption may be incorrect, thus leading to erroneous conclusions. An alternative approach considered by Schaubel et al. (2006) is to use a rates model that assumes the effect of covariates is additive. The rates model of Schaubel et al. (2006) extends the semiparametric additive hazards model of Lin and Ying (1994) to the recurrent event setting. Interpretations of the regression parameter estimates in the additive model are often more natural and easily understood by practitioners (Schaubel et al., 2006). Thus, if the multiplicative and additive models are both appropriate for a data set then the additive model may be preferred (Schaubel et al., 2006).====We propose a general class of semiparametric additive models that use an effective age process to take into account both how units age across time and the effects of interventions applied to units after an event occurrence. Peña and Hollander (2004) used the same class of effective age processes under the assumption of a multiplicative intensity based model. The generality of the class of effective age processes comes from the fact that it subsumes many specific ones considered in the literature and easily allows for the creation of new ones. These include the minimal repair aging process considered in the multiplicative models of Prentice et al. (1981), Brown and Proschan (1983), and Lawless (1987). The perfect repair process used in the models of Gill (1981) and Peña et al. (2001) is also a special case. Additionally, it includes the imperfect repair aging process used in the models of Block et al. (1985), Dorado et al. (1997), and Adekpedjou and Stocker (2015). Peña et al. (2007) and Peña (2016) investigate the finite sampling and asymptotic properties for the multiplicative class of models considered in Peña and Hollander (2004) respectively.====The article is organized as follows. In Section 2, we provide the mathematical setting and describe the class of models. In Section 3, we reformulate the class of models in terms of both calendar and gap time. The reformulation is necessary to obtain estimators and asymptotic properties. Section 4 constructs estimators for the unknown parameters and Section 5 presents their asymptotic properties. We use the tools of empirical process theory to prove the results of Section 5. In Section 6, we use a computer simulation study to investigate finite sample properties. In Section 7, we analyze a real data set of indolent lymphoma recurrence times to illustrate the class of models. In Section 8, we discuss how to choose the effective age process. Additionally, the indolent lymphoma data set and a data set of failure times for the hydraulic subsystems of load–haul–dump machines are used to demonstrate the impact that the effective age process has on parameter estimates and inference procedures. Section 9 provides some concluding remarks.",A general class of additive semiparametric models for recurrent event data,https://www.sciencedirect.com/science/article/pii/S0378375819300722,1 August 2019,2019,Research Article,265.0
"Lu Hezhi,Jin Hua","School of Mathematical Science, South China Normal University, Guangzhou 510631, China","Received 23 April 2018, Revised 1 June 2019, Accepted 3 July 2019, Available online 30 July 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.07.001,Cited by (2),"The prediction interval (PI) is useful to predict future observations based on the information available in a given sample and has many practical applications. The Score, Bayesian and Fiducial PIs are well-known existing methods. Recently, two PIs based on the inferential model (IM) have been established. However, they have either poor or conservative behavior of the coverage ","The prediction interval (PI) is a very useful tool to predict future observations based on the information available in a given sample. It is a fundamental problem in statistics. For example, in medical applications, it is important to investigate the number of diseased patients in a population that follows a binomial distribution. In this article, we focus on prediction interval for the binomial random variable.====For continuous distributions, prediction intervals have been extensively studied in the literature, such as Patel (1989), Hamada et al. (2004), Lawless and Fredette (2005), Olive (2007), Cai et al. (2008), Wang (2010), Wang et al. (2012) and Martin and Lingham (2016). However, there are fewer investigations for discrete distributions. From the frequentist point of view, Nelson (1982) proposed a closed form prediction interval for binomial random variable, and Bain and Patel (1993) suggested another approach. Although the PI with a closed form can be easily employed in applications, it has poor coverage probabilities when the binomial parameter is near 0.5 (Wang, 2010). On the other hand, Bayesian approach provides another way of thinking, which begins from a given prior of the unknown parameter (Berger, 1985, Hamada et al., 2004, Bernardo, 2005, Tuyl et al., 2009). It is a good alternative to the frequentist, but there is no clear choice of prior. Different priors can lead to different PIs. Compared to the Bayesian, the Fiducial idea may be more attractive because no prior distributions are required. In general, the Fiducial inference is a valid statistical method with many good characteristics, see Hannig et al. (2006), Hannig (2009). Wang et al. (2012) established a Fiducial PI that performed well (Hannig, 2013). However, similar to Bayesian posterior or predictive distributions, fiducial distributions may not be calibrated for meaningful probabilistic inference (Liu and Martin, 2015). Martin and Liu (2013) proposed a general framework for prior-free probabilistic inference, called inferential model (IM). According to the IM, Martin and Lingham (2016) provided an alternative solution to the binomial problem of predicting future observations. IM has some connections and parallels with the fiducial (see Fisher, 1959, Hannig, 2009, Hannig, 2013) and Dempster–Shafer theory (see Shafer, 1976, Dempster, 2008). The key difference is the way to work out for the auxiliary variables. The IM solution does not require a prior to be specified, but provides probabilistic summaries of the information in data concerning the parameter of interest and produces probabilistic inferential results that have desirable frequency properties.====In this article, we propose a new solution to the binomial problem of predicting future observations based on inferential model. This idea is inspired by the randomized interval estimations proposed by Stevens (1950) and Agresti and Min (2005). These days, statisticians regard randomized inference as a tool for the mathematical convenience of achieving exactly the discrete data, such as Agresti and Min (2005), Hannig et al. (2016) and Lu et al. (2018). The remainder of this paper is structured as follows: We present five existing prediction intervals in Section 2 and propose a more efficient plausibility function for a two-sided prediction interval in Section 3. In Section 4, we compare the proposed method with the five prediction intervals by simulation studies. In Section 5, we analyze a real data for illustration of our methods in practice. Finally, we give some discussion and an extension to other discrete populations in Section 6.",A new prediction interval for binomial random variable based on inferential models,https://www.sciencedirect.com/science/article/pii/S0378375819300680,30 July 2019,2019,Research Article,266.0
Buonaguidi B.,"Data Science Lab, Institute of Computational Science, Università della Svizzera italiana, Lugano, Switzerland,Department of Statistical Science, Università Cattolica, Milan, Italy","Received 25 March 2019, Accepted 6 July 2019, Available online 26 July 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.07.004,Cited by (4),"We study the problem of detecting as quickly as possible the disorder time at which a purely jump ==== changes its probabilistic features. Assuming that its jumps are completely monotone, the monitored process is approximated by a sequence of hyperexponential processes. Then, the solution to the disorder problem for a hyperexponential process is used to approximate the one of the original problem. The efficiency of the proposed approximation scheme is investigated for some popular Lévy processes, such as the gamma, inverse Gaussian, variance-gamma and CGMY processes.","In the disorder or quickest detection problem for a continuously monitored process ====, at some unobservable time ====, the so called disorder time, ==== undergoes a change of its probabilistic features. The goal is to detect ==== as accurately and as soon as possible through a detection rule which minimizes the trade-off between the losses arising from declaring the occurrence of the disorder too early and too late. In this paper, it is assumed that ==== is a ==== Lévy process and the Lévy measures characterizing ==== before and after ==== are ==== (see Section 2). The disorder problem is studied in its more general Bayesian version: ==== is a random variable taking on value 0 with positive probability and, conditionally on the event that ====, is exponentially distributed; for a given detection rule, the associated risk is expressed as the linear combination of the probability (or the expected advance) of declaring ==== before it occurs and the expected (linear or exponential) delay since the realization of ====.====The disorder problem for continuous time processes has been deeply investigated and its capstone can be identified in Shiryaev (1978, Chap. 4.4) where the early detection of a change in the drift of a Brownian motion is analyzed. Subsequent works in this direction solve the disorder problem for the drift of a Brownian motion with finite time horizon (Gapeev and Peskir, 2006) or for the drift of a more general diffusion process, like the Bessel process (Gapeev and Shiryaev, 2013, Johnson and Peskir, 2017). Another appealing stream of literature deals with the problem of the quickest detection of a change in the characteristics of a jump process: for the case of a Poisson process partial solutions can be found in Gal’Chuk and Rozovskii (1971) and  Davis (1976), while the complete solution is given in Peskir and Shiryaev (2002). This problem has been generalized to different risk criteria or information schemes in Bayraktar and Dayanik, 2006, Bayraktar et al., 2005 and Herberts and Jensen (2004); in Bayraktar et al. (2006) the case of an unknown post-disorder intensity of the Poisson process is studied. The works (Gapeev, 2005, Buonaguidi and Muliere, 2015) provide explicit solutions when a compound Poisson process with exponential jumps or a negative binomial process, respectively, is observed; in Dayanik and Sezer (2006) the problem is solved for any compound Poisson process, independently of its intensity and jump distribution, and in Dayanik et al. (2008) this result is extended to the case where a compound Poisson process and a Brownian motion are jointly monitored.====The novelty of this paper is in the sample path properties of the process we consider and the way the problem is solved. The process of interest ==== is a purely jump Lévy process with completely monotone jumps: this class is reach enough to contain processes which jump infinitely many times on bounded time intervals and which are not therefore compound Poisson processes. More precisely, we consider Lévy processes with no Gaussian component, zero drift, infinite jump activity and whose Lévy measure is completely monotone. To the best of our knowledge, solely in Gapeev (2005) the disorder problem for this type of processes is approached, but only a partial answer is given. Our method completes the results for the gamma and the inverse Gaussian processes in Gapeev (2005, Examples 3.2 and 3.3) and can also be used for other Lévy processes which are notable in applications, like the variance-gamma or the tempered stable processes. We observe that our assumptions on ==== imply that it has finite variation: the case of infinite variation is left open for future research.====New is also the way of approaching the problem. Usually the optimal stopping problem in which the disorder problem can be reformulated is reduced to a free-boundary problem (see Peskir and Shiryaev (2006) and Shiryaev (1978)). The latter seems of hard solution in this context due to the complex integro-differential equation satisfied by the value function and the singularity points that in some cases can characterize this equation (see Buonaguidi and Muliere, 2015, Gapeev, 2005 and Peskir and Shiryaev (2002) for specific examples). Then, we tackle the problem differently: we find a sequence of processes ====, ====, satisfying the following conditions: (1) ==== converges in distribution to ==== as ==== and (2) for a fixed ====, the disorder problem for ==== can be solved. In this way, we use the solution to the disorder problem for ==== to approximate the one of the original problem for ====. The recent results derived in Hackmann and Kuznetsov (2016) provide a way to find ==== within the class of the hyperexponential processes; the second condition is consequently satisfied because the compound Poisson structure of a hyperexponential process allows us to employ the methodology in Dayanik and Sezer (2006).====A method similar to ours has been employed in Coquet and Toldo, 2007, Dolinsky, 2010 and Szimayer and Maller (2007) for optimal stopping problems related to mathematical finance; however, in these works the original process is approximated by a sequence of discrete time processes, unlike our continuous time approximations ====, ====. Another aspect of interest of our work is that we test the efficiency of the approximations ==== in an optimal stopping problem, thus extending the framework of the numerical computations in Hackmann and Kuznetsov (2016, Sec. 4), where expectations are evaluated at a fixed time.====This paper is organized as follows. In Section 2, we state the main assumptions we work with and we recall the notions of completely monotone and hyperexponential Lévy processes. In Section 3, we describe more formally the problem and we derive the stochastic differential equations of the involved quantities. Using the change of measure derived in Johnson and Peskir (2017), we rewrite the quickest detection problem as an optimal stopping problem for a one-dimensional Markov process. This representation will be very useful for the purposes of the subsequent sections. Similar results are obtained in Bayraktar et al. (2005) and Dayanik and Sezer (2006), but there the approach departs from ours since the model is built under a different probability measure; additionally, in Bayraktar et al. (2005) and Dayanik and Sezer (2006) only the case of compound Poisson processes is considered. Our results are also an extension to Lévy processes of the results obtained in Johnson and Peskir (2017, Sec. 4) for diffusion processes. We conclude this section by analyzing the associated free-boundary problem, which is numerically solved in the last section to obtain some benchmark values. In Section 4, the numerical scheme employed for deriving the solution to the disorder problem is developed. In particular, we use the results in Hackmann and Kuznetsov (2016) to derive the sequence of hyperexponential processes ====, ====, which converge in distribution to ==== and we adapt the algorithm developed in Dayanik and Sezer (2006) to the disorder problem for ====. Finally, in Section 5, we illustrate the results of some numerical computations for the quickest detection problem of the gamma, inverse Gaussian, variance-gamma and CGMY processes (the latter two having important applications in finance for modeling stock prices), in order to demonstrate the efficiency of the proposed approximation method.",The disorder problem for purely jump Lévy processes with completely monotone jumps,https://www.sciencedirect.com/science/article/pii/S0378375819300709,26 July 2019,2019,Research Article,267.0
"Szabo Zsolt,Liu Xiaoyu,Xiang Liming","School of Physical and Mathematical Sciences, Nanyang Technological University, Singapore","Received 4 September 2018, Revised 17 March 2019, Accepted 10 July 2019, Available online 22 July 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.07.002,Cited by (4),Interval ,"In many medical and public health studies, interval censored data arise frequently, especially when the occurrence of the event can be diagnosed only at periodic doctor’s visits or laboratory tests. One of the typical interval censoring types is case 2 interval censoring, which means that the true survival time is only known to be between two consecutive clinical visits or examination times (Sun, 2007). In a study of diabetes conversion (Rasmussen et al., 2008), for example, the event was a conversion to type 2 diabetes, which could not be directly observed, but can be detected by fasting blood tests performed periodically but not continuously. As a result, the true event time is recorded as an interval with two endpoints given by the last negative blood test date and the first positive test date when diabetes is detected. In such an application, to assess the impact of risk factors on developing disbetes is of scientific importance.====In the literature, regression analysis of interval censored data has been investigated based on various conventional semiparametric models, including the proportional hazards (PH) model studied by Huang and Wellner (1995), Rosenberg (1995) and Pan (1999) using maximum likelihood estimation (MLE) and Zhang et al. (2010) using the sieve MLE method; the proportional odds (PO) and accelerated failure time (AFT) models investigated by Zhang and Davidian (2008) through a smooth parametric estimation method. All these existing methods are developed under the assumption of fixed stochastic order of individuals. In addition, these conventional models implicitly require a covariate, say intervention, to take an immediate effect right after its application. However, such requirement may be inappropriate in practice. In a screening and intervention study of prediabetes (Rasmussen et al., 2008), for example, subjects were recruited from a general population and then randomized into either the lifestyle intervention group or the control group. The intervention effects in this case are not immediately, but gradually released and there is a lag period before the intervention is effective.====To relax the assumption of fixed stochastic order and account for gradual effects of covariates in the right censoring cases, Chen and Wang (2000) proposed a new model, the accelerated hazards (AH) model, which allows a time scale change between hazard functions. For the failure time of interest ====, the AH model assumes that the hazard function of ==== given covariate vector ==== is scaled by the factor ==== and has the form: ====where ==== is a nonnegative baseline hazard function and ==== is a vector of regression parameters. ==== is the hazard progression time ratio, reflecting how the covariate ==== makes adjustments to the time scale of the underlying hazard progression. The ====th covariate ==== accelerates (or decelerates) the hazard progression if its corresponding coefficient ==== (or ====). Model (1) is particularly appealing to model the gradual effect of the intervention or other covariates, such as age of an individual, on the failure time distribution, while without using the sophisticated time-dependent covariates the conventional PH, PO and AFT models are not appropriate in this situation.====Regression analysis based on the AH model with right censored data was initially studied by Chen and Wang (2000) and Chen (2001), who developed the method of estimating equations to find the parameter vector ==== with a note that the estimating equations in general do not have a unique solution due to nonsmooth estimating functions. This problem has attracted broad attention in recent years. Zhang et al. (2011) developed a smoothing estimating equation approach for the AH model based on a kernel smoothing method, alleviating computational challenges in the earlier works.  Li et al. (2012) further addressed to solve nonsmoothing estimating equations using an induced smoothing procedure so that a unique solution for estimation can be achieved.  Chen et al. (2014) studied a Bayesian AH model with a spline prior for smooth survival densities based on standard parametric distribution families. Despite significant recent development in the estimation of the AH regression, the statistical theory of the aforementioned methods generally cannot be applied to the situation with interval censoring, where two related random variables corresponding to interval endpoints have to be considered.====The sieve semiparametric maximum likelihood estimation (Grenader, 1981) has become an attractive tool in nonparametric and semiparametric inference of interval censored data, due to its ability to simultaneously estimate the parametric and nonparametric components in the framework of complicated semi/nonparametric models. Shen and Wong (1994) established the theory for the convergence rate of the sieve estimation and showed that the resulting estimators for both components achieved optimal convergence rates, respectively. Their theory has consequently been applied to the development of large sample properties for maximum likelihood estimators in the analysis of interval censored data under various models, including the proportional odds model by Huang and Rossini (1997), the proportional hazards model by Zhang et al. (2010), the PH-based copula joint model for dependent current status data by Ma et al. (2015) and a class of semiparametric transformation models for bivariate data by Zhou et al. (2017) more recently.====When estimating semiparametric survival models, a challenge is to fit the unknown cumulative baseline hazard function, which is even more complicated under interval censoring than in the right censored case. One possible method is to model the cumulative baseline hazard function nonparametrically with B-splines, proposed by Rosenberg (1995) for the PH model. The B-spline estimator for the nonparametric component in the model offers a natural way to generalize the maximum likelihood estimator to a sieve estimator, and consequently large sample properties can be established. In this paper, we adopt B-splines to approximate the cumulative baseline hazard function and develop a sieve maximum likelihood estimation procedure for the AH model with interval censored data. We propose semiparametric efficient estimators of parameters that are included in the nonparametric component and establish the consistency, convergence rate and asymptotic normality of the estimators. As for the implementation, we propose a two-step maximization algorithm in which the interval that the B-splines are defined on, can be updated in each iteration step. An appealing feature of the proposed algorithm is that it allows us to adjust the support interval of the B-splines at each iteration step so that the model fits better to observed interval endpoints.====The remaining of the paper is organized as follows. In Section 2 we describe the AH model for interval censored data. In Section 3 we give estimation procedure using the B-splines to estimate the cumulative baseline hazard function. We establish large sample properties in Section 4, and report simulation results to show the finite sample efficiency of the proposed method in Section 5, followed by applying the method to a real example in Section 6. Finally, we conclude the paper in Section 7. All technical proofs are given in Appendix.",Semiparametric sieve maximum likelihood estimation for accelerated hazards model with interval-censored data,https://www.sciencedirect.com/science/article/pii/S0378375818302477,22 July 2019,2019,Research Article,268.0
"Das Ashish,Singh Rakhi","Indian Institute of Technology Bombay, Mumbai, India,Technische Universität Dortmund, Dortmund, Germany","Received 28 November 2018, Revised 20 June 2019, Accepted 10 July 2019, Available online 19 July 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.07.003,Cited by (1),"Discrete choice experiments have gained importance over the years while supporting the marketing industry significantly. Several author-groups have contributed to the theoretical development of discrete choice experiments and for finding optimal choice designs under the ====There have also been some confusion regarding the inference parameters expressed as linear functions of the systematic component of utilities corresponding to the options. We theoretically establish a unified approach to discrete choice experiments and introduce the general inference problem in terms of a simple linear function of such utilities. This allows us to show that the commonly used effects coding under the ====-criterion for the non-singular full-rank inference problem inherently attaches unequal importance to the elementary contrasts of attribute levels. On the contrary, we see that the orthonormal coding leads to attaching equal importance to the elementary contrasts of attribute levels. However, for a singular full-rank inference problem involving the full set of effects-coded parameters, we show that the orthonormal coding provides an equivalent approach to obtain ====-optimal designs.","Discrete choice experiments (or simply, choice experiments) are widely used in various areas of industry, in health economics (Clark et al., 2014), in marketing (Zwerina, 1997), in environmental studies (Alriksson and Öberg, 2008, Hoyos, 2010), in transport and logistic studies, to name a few. A choice experiment consists of ==== choice sets and each choice set contains ==== options. A respondent is shown each of the choice sets and is in turn asked for the preferred option, among the ==== options, as per his perceived utility. Each of the ==== options are described by ==== attributes. The primary objective of a discrete choice experiment is to study the impact of ==== attributes of an option where the ====th attribute has ====
 (====) levels labeled ====; ====. With the options being described by the levels of the attributes, each option is a ====-tuple and there are a total of ==== possible options. Let the ==== lexicographically arranged options be denoted by ==== where, ====. Here, ==== denotes the lexicographic number of the option ==== and is given by ====. A choice design is a collection of choice sets employed in a choice experiment. For an excellent review of designs for choice experiments, one may refer to Street and Burgess, 2007, Street and Burgess, 2012, and Großmann and Schwabe (2015).====Discrete choice experiments have been discussed primarily under the multinomial logit model (MNL model) setup. Street and Burgess (2007) have derived the information matrix to study choice experiments under the MNL model. Independently, Huber and Zwerina (1996) have also derived the information matrix under the same model. We refer to the two approaches as SB approach and HZ approach. The information matrices under the two approaches look superficially different and moreover, the attributes are also coded differently, even though we have considered the average information matrix in both the SB and the HZ approach. Throughout this paper, we write information matrix to mean the average information matrix per choice set.====One of the many objectives of a choice experiment is to optimally or efficiently estimate the parameters of interest which essentially consists of either only the main effects or the main plus two-factor interaction effects of the ==== attributes. ====-optimal designs have been obtained theoretically under the utility-neutral setup, for example, see Graßhoff et al. (2004), Street and Burgess (2007), Demirkale et al. (2013), and Singh et al. (2015). Additionally, Sun and Dean, 2016, Sun and Dean, 2017, and Chai et al. (2017) have obtained ====-optimal choice designs. For such optimal designs, researchers either used the information matrix following the HZ approach under effects coding, or used the information matrix following the SB approach under orthonormal coding.====The author-groups SB and HZ have used seemingly different information matrices under the MNL model. There have also been some confusion regarding the inference parameters expressed as linear functions of the systematic component of utilities corresponding to the options, ====. Throughout this paper, ==== is referred to as the utility parameter vector. We theoretically establish a unified approach to discrete choice experiments and introduce the general inference problem in terms of a simple linear function of ====; say ====.====After introducing the SB and HZ approaches in Section 2, we show their equivalence in Section 3. In Section 4, the inference problem under different codings is expressed in simple terms as a function of ====. In Section 5, with respect to the ====-criterion, we discuss how different codings may be interpreted. We also propose a related coding which is appropriate for test-control discrete choice experiments wherein some new test levels of an attribute are compared with an existing control level. Finally, in Section 6 we summarize the results along with a short discussion.",Discrete choice experiments—A unified approach,https://www.sciencedirect.com/science/article/pii/S0378375819300692,19 July 2019,2019,Research Article,269.0
"Park Hyung,Petkova Eva,Tarpey Thaddeus,Ogden R. Todd","Department of Population Health, New York University, New York, NY 10016, USA,Department of Biostatistics, Columbia University, New York, NY 10032, USA","Received 16 August 2018, Revised 16 January 2019, Accepted 27 May 2019, Available online 4 July 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.05.008,Cited by (7)," that targets the effect of the interaction between the treatment conditions and the vector of covariates on the outcome, a ","In precision medicine, a critical concern is to identify baseline measures that have distinct relationships with the outcome from different treatments so that patient-specific treatment decisions can be made (Murphy, 2003, Robins, 2004). Such variables are called treatment effect modifiers, and these can be useful in determining a treatment decision rule that will select a treatment for a patient based on observations made at baseline. There is a growing need to extract treatment effect modifiers from (usually noisy) baseline patient data that, more and more commonly, consist of a large number of clinical and biological characteristics.====Typically, treatment effect modifiers (or, “moderators”) are identified either one by one, using one model for each potential predictor, or from a large model which includes all potential predictors and their (two-way) interactions with treatment, and then testing for significance of the interaction terms, almost exclusively using linear models. In the linear model context, Petkova et al. (2016) proposed a model using a linear combination (i.e., an index) of patients’ characteristics, termed a generated effect modifier (GEM) constructed to optimize the interaction with a treatment indicator. Such a composite variable approach is especially appealing for complex diseases such as psychiatric diseases, in which each baseline characteristic may only have a small treatment modifying effect. In such settings, it is not common to find variables that are individually strong moderators of treatment effects.====Here we present novel flexible methods for determining composite variables that permit non-linear association with the outcome. In particular, the proposed methods allow the conditional expectation of the outcomes to have a flexible treatment-specific link function with an index. We define the index to be a one-dimensional linear combination of the covariates. This approach is related to ==== (Brillinger, 1982, Stoker, 1986, Powell et al., 1989, Hardle et al., 1993, Xia and Li, 1999, Horowitz, 2009, Antoniadis et al., 2004), as well as to single-index model generalizations such as projection pursuit regression (Friedman and Stuetzle, 1981) and ==== models (Xia, 2008, Yuan, 2011). We employ a single projection of the covariates (i.e., an index) to summarize the variability of the baseline covariates, and multiple link functions to connect the derived single-index to the treatment-specific mean responses; we call these ==== (SIMML). This single-index model with multiple-links provides a parsimonious extension of the single-index model in modeling the effect of the interaction between a categorical treatment variable and a vector-valued covariate. The dependence of treatment-specific outcomes on a common single-index improves the interpretability, and helps in determining treatment decision rules. This approach generalizes the notion of a composite “treatment effect modifier” from the linear model setting, to a nonparametric context, to define a nonparametric generated effect modifier.",A single-index model with multiple-links,https://www.sciencedirect.com/science/article/pii/S0378375818301678,4 July 2019,2019,Research Article,270.0
Kim Ilmun,"Department of Statistics & Data Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA","Received 18 October 2018, Revised 22 February 2019, Accepted 16 June 2019, Available online 28 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.06.005,Cited by (3),"-statistic is asymptotically Poisson or Gaussian, and investigate its power function under each asymptotic regime. Furthermore, we introduce a class of weights for the ","Suppose there are ==== independent random vectors ==== from a multinomial distribution with unknown parameters ==== and ====Given a specific choice of parameter vector ====, the goodness-of-fit test for multinomial distributions is concerned with distinguishing ====Pearson’s chi-squared statistic (Pearson, 1900) is one of the well-known test statistics for this problem. Let ==== for ====. Then Pearson’s chi-squared statistic is defined by ====The properties of ==== have been well-studied in a classical low-dimensional setting (Lehmann and Romano, 2006, Read and Cressie, 2012, Balakrishnan et al., 2013). For instance, the test based on ==== is asymptotically optimal against local alternatives when ==== is fixed (see, e.g. Chapter 14 of Lehmann and Romano, 2006). However, in the high-dimensional regime where the dimension is comparable with or much larger than the sample size, ==== suffers from the fact that it can have substantial bias for the testing problem. In other words, the power of the test can be much smaller than the significance level ==== against certain local alternatives. The major cause of the testing bias is due to the expected value of ====: ====When the null distribution is not the discrete uniform distribution, it is possible to observe ====, which can trigger a significant bias problem of ==== for some ==== level. This bias problem becomes more serious when the dimension is large but the sample size is small (see, Haberman, 1988 for details).====To alleviate the issue, we first view Pearson’s chi-squared statistic as a ====-statistic (Lemma 2.1). In general, the expected value of a ====-statistic is not equal to the parameter of interest (e.g. Chapter 3.5.3 of Shao, 2003). The corresponding ====-statistic, on the other hand, is known to be an unbiased estimator (Lee, 1990). Motivated by these observations, we propose a modified ==== based on the ====-statistic. In particular, the modified ==== is an unbiased estimator of ==== and its expectation becomes zero if and only if the null hypothesis is true. As a result, the modified ==== avoids the testing bias caused by the expected value and can have significant power in the high-dimensional regime where classical ==== is substantially biased (Fig. 1).====Another limitation of ==== in sparse multinomial settings is that it puts too much weight on small entries in ====, and these small entries make the statistic highly unstable (Marriott et al., 2015, Valiant and Valiant, 2017, Balakrishnan and Wasserman, 2019). In this case, one might need to consider different weights to obtain higher power of the test. Motivated by these observations, we consider a family of ====-statistics of the form ==== where ==== is some positive definite matrix.====The primary objective of this work is to investigate the limiting behavior of the proposed ====-statistic in high-dimensions and determine a sufficient condition for ==== under which the resulting test is minimax rate optimal for multinomial goodness-of-fit.==== The main results of this paper are as follows:==== A considerable amount of literature has been published on the high-dimensional behavior of ==== (e.g. Tumanyan, 1954, Tumanyan, 1956, Steck, 1957, Holst, 1972, Morris, 1975, Read and Cressie, 2012, Rempała and Wesołowski, 2016 and the references therein). Our work is especially motivated by Rempała and Wesołowski (2016) who present conditions of the Poissonian and Gaussian asymptotics for ====. One can generalize their result to our ====-statistic framework by using the relationship between ====- and ====-statistics. However, their analysis is restricted to the case of the null hypothesis and does not easily generalize to other cases with different weights. Hence, we take different approaches to overcome such shortcomings. The present study is also closely related to the work by Zelterman, 1986, Zelterman, 1987 who proposes a modified ==== to handle the testing bias of the chi-squared test. The modified statistic is given by ====It can be shown that ==== is equivalent to the proposed ====-statistic up to some constant factors when we take the weight matrix as ====
 (Remark 2.1), and thus ==== falls into our ====-statistic framework.====Recently, Diakonikolas et al. (2016) show that their collision-based test is minimax rate optimal for multinomial uniformity testing where ====
 (Remark 4.1). Their test statistic is a special case of our ====-statistics by taking the identity weight matrix given in (6). We generalize their minimax result to an arbitrary null probability by providing a class of ==== that leads to the minimax optimal test. Our result includes the truncated weight considered in Balakrishnan and Wasserman (2019) as an example.==== The rest of the paper is organized as follows. In Section 2, we view Pearson’s chi-squared statistic as a ====-statistic and provide a modified and generalized ==== based on the ====-statistic. In Section 3, we investigate the Poissonian and Gaussian asymptotics for the proposed ====-statistic in the high-dimensional regime. In Section 4, we present a sufficient condition for ==== under which the test based on the proposed ====-statistic is minimax rate optimal. We summarize the results and discuss future work in Section 5. Finally, all of the proofs and additional results are presented in Appendix.",Multinomial goodness-of-fit based on ,https://www.sciencedirect.com/science/article/pii/S0378375819300667,28 June 2019,2019,Research Article,271.0
"El Machkouri M.,Fan X.,Reding L.","Laboratoire de Mathématiques Raphaël Salem, UMR CNRS 6085, Université de Rouen Normandie, France,Center for Applied Mathematics Tianjin University, Tianjin, China","Received 17 July 2018, Revised 13 May 2019, Accepted 17 June 2019, Available online 27 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.06.006,Cited by (6), where ,"In many situations, practicians want to know the relationship between some predictors and a response. If the form of the functional relation is unknown then a nonparametric approach is necessary. This is a natural question and a very important task in statistics. A very popular tool to handle this problem is the Nadaraya–Watson estimator (NWE) introduced by Nadaraya (1965) and Watson (1964). In this work, we investigate the asymptotic normality of the NWE in the context of dependent irregularly spaced spatial data. Let ====, ==== and ==== be positive integers. Let also ==== be a strictly stationary ====-valued random field defined on a probability space ====. We assume that the common law ==== of the random variables ==== is absolutely continuous with respect to the Lebesgue measure on ====. We denote by ==== the unknown probability density function of ====. Let ==== be a finite region of ==== and let ==== be iid ====-valued random variables with zero mean and finite variance and independent of ====. The regression model is characterized by the relation ==== for ==== in ==== where ==== is an unknown functional. In our setting, it is important to note that no regularity condition is imposed on ==== which can be very general (irregularly spaced data). The regression function ==== is defined for any ==== in ==== by ====and the NWE ==== of ==== is defined for any ==== in ==== by ====where ==== is the number of elements in the region ====, the function ==== is a probability kernel (that is ====) and the bandwidth parameter ==== is a positive constant going to zero as ==== goes to infinity. For time series (i.e. for ====), the problem which we are concerned has been extensively studied. One can refer, e.g., to Lu and Cheng (1997), Masry and Fan (1997), Robinson (1983), Roussas (1988) and many references therein. In the spatial case (i.e. for ====), some contributions for strongly mixing random fields were made by Biau and Cadre (2004), Carbon et al. (2007), Dabo-Niang et al., 2011, Dabo-Niang and Yao, 2007, El Machkouri (2007), El Machkouri and Stoica (2010), Hallin et al. (2004) and Lu and Chen, 2002, Lu and Chen, 2004. The main motivation of this work is to provide sufficient simple conditions for the NWE to be asymptotically normal in the context of mixing but also non-mixing random fields. More precisely, we consider strongly mixing random fields in the sense of Rosenblatt (1956a) and weakly dependent random fields in the sense of Wu (2005) (see also El Machkouri et al. (2013)). To the best of our knowledge, our work provides the first central limit theorem (Theorem 2) for the NWE under minimal conditions on the bandwidth parameter and irregularly spaced dependent spatial data (i.e. ==== and ==== as ====). In particular, our result improves in several directions a previous central limit theorem for the NWE for spatial data established by Biau and Cadre (2004) (see the comments after Corollary 1).====The paper is organized as follows. Our main results are stated and discussed in Section 2 whereas proofs of the main results and its preliminary lemmas are deferred to Sections 4 Preliminary lemmas, 5 Proofs of Theorems. Finally, Section 3 is devoted to a numerical illustration of the central limit theorem obtained in Section 2.",On the Nadaraya–Watson kernel regression estimator for irregularly spaced spatial data,https://www.sciencedirect.com/science/article/pii/S0378375818301435,27 June 2019,2019,Research Article,272.0
"Guo Feifei,Ling Shiqing","Hong Kong University of Science and Technology, Hong Kong","Received 21 April 2018, Revised 18 May 2019, Accepted 13 June 2019, Available online 26 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.06.008,Cited by (0),consistent and asymptotically normal. The performance of the QMLE is assessed via simulation studies and a real example is given to illustrate our procedure.,"Estimating change-point in a statistical model has been an important problem in statistics and other fields for a long time. The earliest work was Hinkley (1970) who first investigated the maximum likelihood estimator (MLE) of the change-point in a sequence of i.i.d. random variables and showed that the MLE converges in distribution to the location of the maxima of a double-sided random walk. Yao (1987) derived a nice distribution for approximating Hinkley’s limiting distribution. Picard (1985) first studied the MLE of change-points in autoregressive (AR) models and obtained the same limiting distribution as that in Yao (1987) when the magnitude of the changed parameters is approaching zero. Dümbgen (1991) proposed a nonparametric method to estimate the change-point. Bai, 1994, Bai, 1995 and Bai et al. (1998) studied the estimated change-point in the multivariate AR model and in the co-integrated time series model, respectively, see also Chong (2001). Saikkonen et al. (2006) and Kejriwal and Perron (2008) used a similar method to estimate the change-point in VAR models and in co-integrated regression models, respectively. Chan et al. (2014) used the group Lasso method to estimate the parameters in a structural break AR model. Ling (2016) developed an asymptotic theory for the estimated change-point in linear and nonlinear time series models.====Gao and Ling (2017) established the asymptotic theory of the estimated changed-point in the threshold AR (TAR) model. The TAR model was proposed by Tong (1978). It has been one of the most important and the most popular nonlinear time series models. This class of models has been extensively applied in many areas, see Tong (2011). Chan (1993) was the first one to establish the asymptotic theory of the least square estimator (LSE) of the threshold parameter. Chan and Tsay (1998) showed that the estimated threshold parameter is ====-consistent and asymptotically normal when the AR function is continuous. Berkes et al. (2011) developed the likelihood ratio test for the structural change of an AR model to a threshold AR model. Li and Ling (2012) and Li et al. (2013) investigated the multiple-regime TAR model and the TMA model, respectively. Hansen (2000) obtained an asymptotic distribution theory for threshold time series regression models when the difference of parameters in two regimes approaches zero. Seo and Linton (2007) proposed a smoothed LSE for the TAR regression model and showed that the estimated threshold is asymptotically normal. Liu et al. (2011) and Gao et al. (2013) studied the LSE for the non-stationary first-order TAR model. Li et al. (2015) investigated the multiple threshold double AR model and developed a procedure for statistical inference of this model.====The aim of this paper is to study the quasi-MLE (QMLE) of the structure-changed threshold double AR (TDAR) model. It is shown that both the estimated threshold and the estimated change-point are ====-consistent, and they converge weakly to the smallest minimizer of a compound Poisson process and the location of minima of a two-sided random walk, respectively. Other estimated parameters are ====consistent and asymptotically normal. Simulation studies are conducted to assess the performance of the QMLE. We also use the U.S. real GNP data as an application to illustrate our procedure.====This paper is organized as follows. Section 2 presents the model and the estimation procedure. The asymptotic properties of QMLE and the limiting distributions of the estimated thresholds and the estimated change-points are presented in Section 3. Section 4 reports simulation results. Section 5 analyzes the U.S. GNP data. Appendix gives the proofs of theorems.",Quasi-likelihood estimation of structure-changed threshold double autoregressive models,https://www.sciencedirect.com/science/article/pii/S0378375819300679,26 June 2019,2019,Research Article,273.0
"Gao Lei,Zhu Hongjian,Zhang Lanju","Vertex Pharmaceuticals, Inc., 50 Northern Avenue, Boston, MA, 02210, USA,Department of Biostatistics and Data Science, University of Texas Health Science Center at Houston, 1200 Pressler Street,RAS W 922, Houston, TX 77030, USA,Data and Statistical Sciences, AbbVie Inc, North Chicago, 60064, USA","Received 6 August 2018, Revised 17 May 2019, Accepted 17 June 2019, Available online 26 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.06.007,Cited by (2),"Response-adaptive randomization can be used in phase III confirmatory trials to achieve various aims related to ethics and efficiency such as minimizing the total number of failures and maximizing the power by targeting different optimal allocation proportions. Sequential monitoring and sample size re-estimation are also popular and are often required in modern clinical trials. In this paper, we propose to combine these three adaptive procedures to sequentially monitor response-adaptive randomized clinical trials with sample size re-estimation. We obtain the asymptotic results for this sequential procedure under certain conditions and find that the type I error rate can be controlled asymptotically with commonly used stopping boundaries from traditional group sequential designs. Assuming continuous or binary endpoints, we show in simulations that the proposed method can achieve various objectives while maintaining the type I error rate on finite samples. In addition, we include a case study to demonstrate how the method can be used in practice to permit ethical considerations while allowing sample size re-estimation to account for uncertainties in the treatment effect.","Randomized controlled clinical trials (RCTs) are the gold-standard research method for evaluating the efficacy and safety of investigational drugs for regulatory approval. A traditional design for such trials is a parallel two-arm (an investigational drug arm versus a control arm) trial with a fixed sample size per arm, determined based on an assumed effect size targeting a study power to test a hypothesis. The required number of patients is enrolled and randomized, typically with a 1:1 ratio, to the two arms. At the end of the trial, the data are analyzed, and a conclusion is obtained with respect to whether the trial is positive or negative.====There are three problems with this traditional design. First, it may not be ethical or it may reduce power to randomize patients using the 1:1 ratio in a Phase III clinical trial (Rosenberger and Lachin, 2015). One can avoid this problem by using response-adaptive randomization (RAR), in which the randomization ratio can be updated during the trial using available data to achieve different efficient and ethical objectives. The advantages of the RAR design and its theoretical and numerical properties have been well studied (Ivanova, 2003, Wei and Durham, 1978, Zhang et al., 2011, Hu and Zhang, 2004, Hu et al., 2009, Eisele and Woodroofe, 1995, Rosenberger et al., 2001, Tymofyeyev et al., 2007, Hu and Rosenberger, 2003, Biswas et al., 2011, Atkinson and Biswas, 2013, Bandyopadhyay and Biswas, 1997, Zhang and Rosenberger, 2010, Baldi Antognini et al., 2016, Zhang and Rosenberger, 2006, Zhang and Rosenberger, 2007, Zhang and Rosenberger, 2014, Zhu and Hu, 2009, Wang et al., 2017).====Second, the assumed effect size and variance, usually based on data for other drugs, limited earlier phase data, or the best guesses of clinical experts, is very uncertain. The inaccurate specification of these parameters often leads to an under-powered study, and thus the failure of a potentially successful program or a waste of time and resources. Gan et al. (2012) found that “investigators consistently make overly optimistic assumptions regarding treatment benefits when designing RCTs”, after they reviewed 253 phase III oncology trials. To address this problem, sample size re-estimation (SSR) design has been proposed. In such a design, the trial starts with a sample size based on an optimistic effect size. In an interim analysis, the sample size is re-estimated based on the observed effect size and the target power (Cui et al., 1999, Lehmacher and Wassmer, 1999, Müller and Schäfer, 2001, Shun et al., 2001, Chen et al., 2004, Gao et al., 2008, Mehta and Pocock, 2011).====Third, it is natural to conduct a sequential analysis in clinical studies where data accumulate sequentially. Without sequential monitoring, we lose opportunities to ensure that patients are not exposed to dangerous treatments and the protocol is not violated. Sequential monitoring can also decrease sample size and cost, and has now become a standard technique in conducting clinical trials (Jennison and Turnbull, 2000, Proschan et al., 2006, Whitehead, 1997, Pocock, 1977, O’Brien and Fleming, 1979, Lan and DeMets, 1983). The implementation software (e.g., EAST and ADDPLAN) have also been developed.====Trial designs that combine the above methods have been proposed. For example, Zhu and Hu, 2010, Zhu and Hu, 2012 combined RAR and group sequential design; Cui et al. (1999) combined group sequential design and SSR; Jennison and Turnbull (2001) established a general theory for group sequential design with RAR for comparing two normal responses with known variance; and Morgan (2003) numerically studied several SSR methods in group sequential trials also with normal responses. In this paper, we propose to sequentially monitor RAR clinical trials with unblinded SSR to simultaneously address the three problems above, and our methods apply to different types of endpoints. We explore three questions. First, is there a framework that can combine RAR design, sequential monitoring, and SSR in a single clinical trial? Second, can we control the type I error rate? Third, what are the advantages of this combination?====An important statistical problem for all confirmatory adaptive designs is the control of the type I error rate. Sequential monitoring tends to inflate the type I error rate because of the multiple interim looks; RAR designs introduce new challenges due to the complicated correlations between the treatment assignments and responses; and SSR leads to type I error rate inflation because of the dependence of the sample size on the interim observed data. We overcome these hurdles by employing appropriate framework and SSR methods, and deriving the asymptotic results for the proposed procedure. We theoretically show that our method can easily control the type I error rate via commonly used stopping boundaries from traditional group sequential designs. The numerical studies support our theoretical findings and demonstrate the advantages of the procedure. It is worth noting that Bayesian adaptive designs have also been proposed and studied in the literature (Berry, 2005, Cheng and Shen, 2005, Lewis et al., 2007, Wathen and Thall, 2008). In this paper, we focused on frequentist approaches.====We present the framework and the main theoretical results in Section 2. Section 3 presents the simulation results, and Section 4 discusses a case study. Section 5 provides a discussion, and the proof is in Appendix.",Sequential monitoring of response-adaptive randomized clinical trials with sample size re-estimation,https://www.sciencedirect.com/science/article/pii/S0378375818301873,26 June 2019,2019,Research Article,274.0
"Chen Pingyan,Wen Luliang,Sung Soo Hak","Department of Mathematics, Jinan University, Guangzhou, 510630, PR China,Department of Statistics, Jinan University, Guangzhou, 510630, PR China,Department of Applied Mathematics, Pai Chai University, Daejeon, 35345, South Korea","Received 14 December 2018, Revised 12 June 2019, Accepted 13 June 2019, Available online 19 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.06.004,Cited by (5),"For a simple linear errors-in-variables regression model, we obtain a ==== for the convergence rate in the strong consistency of the least ==== for each of the unknown parameters. We also obtain a ==== for the convergence rate in the weak consistency.","To correct the effects of the sampling errors, Deaton (1985) proposed the errors-in-variables (EV) regression model. A simple linear EV regression model is ====where ==== are unknown parameters or constants, ====, are random vectors and ====, are observable variables. From (1.1), we have ====As a usual regression model of ==== on ==== with the errors ====, we can obtain the least squares (LS) estimators of ==== and ==== as ====where ====. The notations of ====, and ==== are defined in the same way. Based on these notations, we have ====and ====The EV model is somewhat more practical than the ordinary regression model, and hence has attracted more and more attention. Fuller (1987) summarized many early works for the EV models. The weak and strong consistency of the LS estimators for the unknown parameters have been studied by Hu et al. (2017), Liu and Chen (2005), Martsynyuk (2013), Miao et al. (2011), Shen (2017), Wang and Hu (2017), Wang et al., 2015, Wang et al., 2018, and Wu et al. (2017).====This paper deals with the EV model with independent and identically distributed (i.i.d.) errors having finite variances. Liu and Chen (2005) obtained necessary and sufficient conditions for the weak and strong consistency of the least squares estimators for the unknown parameters ==== and ==== as follows.====If ==== for some ====, then ====, and so Theorem 1.2 gives a convergence rate in the strong consistency of ====. Miao et al. (2011) also obtained a convergence rate in the weak consistency of ==== and obtained convergence rates in the weak and strong consistency of ====. However, their results, including Theorem 1.2, give only sufficient conditions.====Martsynyuk (2013), among other things, obtained convergence rates in the weak and strong consistency assuming that the i.i.d. errors ==== and ==== are in the domain of attraction of the normal law. Hence it is possible that ==== or ====, and ==== or ====. Here we consider only the case with finite variances.====Martsynyuk (2013) also obtained a convergence rate in the weak consistency of ==== and obtained convergence rates in the weak and strong consistency of ====. However, the results of Martsynyuk (2013), including Theorem 1.3, give only sufficient conditions.====In the paper, we provide necessary and sufficient conditions for the convergence rates in the weak and strong consistency for the unknown parameters ==== and ====.",Strong and weak consistency of least squares estimators in simple linear EV regression models,https://www.sciencedirect.com/science/article/pii/S0378375819300655,19 June 2019,2019,Research Article,275.0
"Tong Hongzhi,Wu Qiang","School of Statistics, University of International Business and Economics, Beijing 100029, PR China,Department of Mathematical Sciences, Middle Tennessee State University, Murfressboro, TN 37132, USA","Received 28 May 2017, Revised 3 November 2018, Accepted 7 June 2019, Available online 14 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.06.003,Cited by (0)," in the learning theory framework. The main results include an inequality that bridges the gap between the global risk and local risk, a characterization of the approximation that shows the moving technique allows to approximate very complicated functions by simple function classes, and a learning rate analysis. These results indicate that the moving quantile regression method converges fast under mild conditions.","Regression analysis discovers the relationship between explanatory variables and response variables and plays important roles in many fields of science. Existing regression methods can be roughly classified into two categories, global methods and localized methods. Global methods aim to fit the whole data simultaneously by minimizing the total prediction error on all data points. The advantage is that global convergence usually implies local convergence, while the disadvantage is that, if the target function has quite different behaviors in different regions, the performance is constrained by the region where the function is most complicated and thus most difficult to estimate. Compared with global methods, localized methods have at least two benefits: one is that localized methods make prediction for each input based only on the data points around it, which reduces calculations; the other is that it can approximate the complicated function by simple function class. This is advantageous in some applications. But localized methods may suffer from generalization problem because local convergence does not necessarily imply global convergence without additional assumptions. In the context of mean regression problem, both global methods and localized methods have been well studied. For instance, global least square mean regression methods have been studied in Smale and Zhou (2007), Wu et al. (2006), Caponnetto and De Vito (2007), Steinwart et al. (2009), Cucker and Zhou (2007) and Steinwart and Christmann (2008a) and the references therein. Localized methods have been studied in the literature of statistics (Fan and Gijbels, 1996, Wand and Jones, 1995), approximation theory (Levin, 1998, Wendland, 2001, Wendland, 2005), and learning theory (Wang, 2011, Wang et al., 2010, Tong and Wu, 2017). In this paper, we will focus on another important regression problem, the quantile regression, for which many research works on global methods have been done in the literature, but research on localized methods is sparse. The purpose of this paper is to propose a localized method for quantile regression, the moving quantile regression with kernel regularization, analyze its local convergence, and verify some conditions toward global convergence.====Quantile regression or conditional quantile regression is a crucial element of analysis in many quantitative problems. It extends the classical least squares regression (mean regression) and provides richer information about the distributions of the response variables (Koenker, 2009). Let ==== be a compact subset of ====, ====. The goal of quantile regression is to estimate the conditional distribution of a Borel probability measure ==== on ====. Denote by ==== the conditional probability distribution of ==== at ====, the conditional ====-quantile is a set-valued function defined by ====where ==== is a fixed constant specifying the desired quantile level. Throughout this paper, we assume that ==== has its support in [−1,1] and ==== consists of singletons, i.e., there exists an ====, called the conditional ====-quantile, such that ==== for ====-almost all ====, here ==== denotes the marginal distribution of ==== on ====.====Quantile regression has been investigated by kernel-based regularization schemes in a learning theory literature; see e.g. Eberts and Steinwart (2013), Steinwart and Christmann (2008b), Steinwart and Christmann (2011), Xiang (2013) and Xiang et al. (2012). Given a sample ==== of ==== i.i.d. observations drawn from ====, these regularization schemes take the form ====where ==== is the pinball loss defined by ====
 ==== is a regularization parameter, ==== is the reproducing kernel Hilbert space (RKHS) with kernel ====. Recall a function ==== is called a reproducing kernel if it is continuous, symmetric and positive semidefinite. The associated RKHS ==== is defined (Aronszajn, 1950) as the completion of the linear span of the set of functions ==== with the inner product ==== induced by ====. The reproducing property takes the form ====Denote ====. Then the reproducing property (1.2) tells us ====This ensures that ==== can be embedded into ====. It is noteworthy that scheme (1.1) is a kind of global learning algorithm, i.e., the learning samples are commonly regular or concentrated. In order to deal with the irregular or scattered samples, recently Wang, 2011, Wang et al., 2010 applied a moving least-square method to the regression problem in learning theory. This motivates us to consider in this paper a moving quantile regression algorithm for learning the conditional quantile function ====. It defines an approximation ==== from the samples ==== in a pointwise manner by ====where, for each ====, ====where ==== is a scaling parameter, ==== is a weight function that ensures the learning process is conducted locally. Since ==== is piece wise and convex, the minimization problem in (1.4) can be solved either by gradient descent algorithm for the primal problem or by quadratic programming for the dual problem (Takeuchi et al., 2006). The convergence of the optimization process is guaranteed because the optimization problem is well conditioned thanks to the regularization.====Throughout this paper, the weight function is assumed to satisfy the following conditions.====These conditions are fulfilled by many commonly used weight functions such as the biweight kernel ====, the tricube kernel ====, and the Gaussian kernel ====. For each ====, the local estimator ==== is either fully determined by or largely dependent on the data in a local neighborhood of ====. The choice of the parameter ==== determines the size of the local neighborhood, and ensures the learning process is conducted locally.====In learning theory we are interested in global approximation of ==== by ==== on the whole input space ====. In particular, for some ==== the error ==== and its convergence rates as ==== are used to measure the performance of the learning algorithm. It thus leads to a difficulty for us to bridge the gap between the pointwise nature of scheme (1.4) and the global measure ====. In this paper, we overcome this difficulty by establishing a novel comparison relationship, this together with some standard analysis techniques in learning theory helps us in deriving an explicit learning rate of algorithm (1.4) under some natural and simple conditions.====Before we move on to state our main results and give the proofs, let us review the related works in the literature and clarify our contributions. The study of moving regression has focused on the moving least square method which is an important topic of approximation theory and is closely related to the weighted local polynomial regression in statistics. While in approximation theory the approximation ability of moving least square method has been extensively studied for noise free data, in statistics the pointwise estimation performance of local polynomial regression was investigated for noisy data. As local estimation methods, it is quite intuitive that moving regression performs well around high density points and poorly around low density points. But it turns out proceeding from local estimation to global estimation is not an easy task. Recently, some global analyses for moving least square regression were done under some very restricted conditions. In Wang (2011) and Wang et al. (2010), a norming condition on the hypothesis space and a regularity condition on the marginal distribution were assumed. The former, if not impossible, is very difficult to verify and interpret. The latter requires the local neighborhood of each point ==== has marginal probability lower bounded, which roughly asks the density function of ==== uniformly lower bounded. This excludes a natural setting where the marginal distribution degenerates near the boundary. More recently, the authors refined the analysis for moving least square method by incorporating the regularization technique, which enables the global convergence analysis without the norming condition (Tong and Wu, 2017). But the regularity condition on the marginal distribution is still required by the analysis. The first contribution of this paper is to establish relationship between the local risk and global risk without any restriction on the marginal distribution so that our global convergence analysis applies to very general situations. Recall that an important advantage of the moving approaches is to learn globally complicated functions through simple functions. But this has not been reflected in Wang (2011), Wang et al. (2010) and Tong and Wu (2017) where the target function is assumed to be either in or well approximated by the hypothesis space. Although moving technique may help improve the performance in these situations, strong necessity is not justified. The second contribution of this paper is that we will introduce a novel assumption to characterize the approximation ability of the hypothesis space for moving quantile regression, which enables us to better interpret the power of moving regression method. Although we focus on the analysis of moving quantile regression in this paper, the techniques developed here may be used to refine the analysis of moving least square regression.====The rest of the paper is organized as follows. In Section 2 we present the algorithm, assumptions, and our main theorems. In Section 3 we establish the relationship between the local risk and global risk, which plays the key role for us to derive global convergence from local convergence. In Section 4 we discuss the approximation error of moving quantile regression and verify the reasonableness of our assumption on the approximation error. In Section 5 we estimate the sample error, derive the learning rates, and complete the proofs of our main theorems.",Moving quantile regression,https://www.sciencedirect.com/science/article/pii/S037837581930062X,14 June 2019,2019,Research Article,276.0
"Fontana Roberto,Crucinio Francesca Romana","Department of Mathematical Sciences, Politecnico di Torino, Italy,Department of Statistics, University of Warwick, United Kingdom","Received 12 October 2018, Revised 3 April 2019, Accepted 29 May 2019, Available online 13 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.05.007,Cited by (1),Algebraic sampling methods are a powerful tool to perform ,"The problem of comparing two measures of location for two random samples is one of the classical problems which arise in statistics. We consider two independent samples, ==== of size ==== from ==== and ==== of size ==== from ====, and we assume that ==== and ==== belong to the same non-negative discrete exponential family with natural parameter ====, base measure ==== and normalizing constant ==== taking values in the set of integers ====
 ====This assumption is not too restrictive. Most widely-used discrete distributions (e.g. Poisson, Geometric and Binomial) belong to the exponential family.====We are interested in checking if the two distributions are equal, that is if the pooled sample ==== is formed by ==== observations coming from the same distribution ====. Thus we perform the hypothesis test ====Several testing procedures are available, both parametric and non-parametric. We consider uniformly most powerful unbiased (UMPU) tests for discrete exponential families that make use of the joint distribution of sample ==== (Lehmann and Romano, 2006), which (with a slight abuse of notation) we denote by ====
 ====Given a test statistic ==== we use Markov bases (Diaconis and Sturmfels, 1998) to build Markov Chain Monte Carlo (MCMC) algorithms that approximate the distribution of ====. We briefly introduce UMPU tests for discrete exponential families and Markov bases in Section 2.====Section 3 introduces the standard construction of MCMC algorithms through Markov bases (Diaconis and Sturmfels, 1998). Section 4 presents the main result of our work, a 2-step MCMC sampling which in the simulation study described in Section 7 appears to be more efficient than that of Diaconis and Sturmfels (1998) for conditional tests for non-negative discrete exponential families. This new MCMC sampling underlines a link between algebraic-statistics-based sampling (e.g. Aoki and Takemura (2010)) and standard permutation sampling. Section 5 is concerned with some theoretical properties of the proposed algorithm while Section 7 summarizes the results of a simulation study on Poisson data. Further applications are in Section 6.",Orbit-based conditional tests. A link between permutations and Markov bases,https://www.sciencedirect.com/science/article/pii/S0378375819300539,13 June 2019,2019,Research Article,277.0
"Chen Xiongzhi,Sarkar Sanat K.","Department of Mathematics and Statistics, Washington State University, Pullman, WA 99164, USA,Department of Statistical Science and Fox School of Business, Temple University, Philadelphia, PA 19122, USA","Received 25 April 2018, Revised 21 January 2019, Accepted 1 June 2019, Available online 6 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.06.001,Cited by (9),"Multiple testing with discrete ====-values with continuous distributions, are too conservative, and so may not be as powerful as one would hope for. Therefore, improving the BH procedure by suitably adapting it to discrete ====-values without losing its FDR control is currently an important path of research. This paper studies the FDR control of the BH procedure when it is applied to mid ====-values and derive conditions under which it is conservative. Our simulation study reveals that the BH procedure applied to mid ====-values may be conservative under much more general settings than characterized in this work, and that an adaptive version of the BH procedure applied to mid ====-values is as powerful as an existing adaptive procedure based on randomized ====-values.","Multiple testing based on discrete test statistics aiming at false discovery rate (FDR) control has been widely conducted in many fields; see, e.g., Chen and Doerge (2017) and references therein. Knowing that many FDR procedures, e.g., the Benjamini–Hochberg (BH) procedure in Benjamini and Hochberg (1995) and Storey’s procedure in Storey et al. (2004), tend to be less powerful when applied to discrete ====-values, three lines of research have been attempted to address this issue. Among them, one is based on randomized ====-values as in the work of Habiger (2015). Since randomized ====-values are uniformly distributed marginally, multiple testing based on such ====-values are essentially routed back to the continuous setting. However, results of multiple testing based on randomized ====-values may not be reproducible or stable due to the use of randomized decision rules. On the other hand, mid ====-values (Lancaster, 1961) are smaller than conventional ====-values almost surely, and a multiple testing procedure (MTP) may have larger power when applied to mid ====-values than conventional ones. However, there does not seem to be a formal study on the BH procedure applied to mid ====-values.====In this article, we focus on the FDR control of the BH procedure applied to two-sided mid ====-values of Binomial tests (BT’s) and Fisher’s exact tests (FET’s). Since mid ====-values are not super-uniform, we derive simple conditions under which the BH procedure is conservative in these settings. Compared to multiple testing with ====-values that are super-uniform, these conditions are new and depict the critical role of the proportion of true null hypotheses for FDR control when the cumulative distribution functions (CDF’s) of ====-values are càdlàg in general. In particular, they explicitly show the interactions between the supremum norms of the probability density functions (PDF’s) of ====-values, the proportion of true null hypotheses, the nominal FDR level and the number of hypotheses to test in order to ensure the conservativeness of the BH procedure applied to two-sided mid ====-values. Our simulation study provides strong numerical evidence on the conservativeness and improved power of the BH procedure applied to mid ====-values.====The rest of the article is organized as follows. Section 2 introduces some notations, three definitions of two-sided ====-value and the setting for multiple testing based on ====-values. Section 3 discusses FDR bounds for step-up procedures based on ====-values with càdlàg CDF’s and those for the BH procedure applied to two-sided mid ====-values. Section 4 presents a simulation study on the BH procedure and its adaptive version for mid ====-values and conventional ====-values. Section 5 provides an application of the BH based on two-sided mid ====-values to an HIV study. Section 6 ends the article with a discussion.",On Benjamini–Hochberg procedure applied to mid ,https://www.sciencedirect.com/science/article/pii/S0378375819300540,6 June 2019,2019,Research Article,278.0
"Ruli Erlis,Sartori Nicola,Ventura Laura","Department of Statistical Sciences, University of Padova, Italy","Received 26 June 2018, Revised 11 January 2019, Accepted 10 May 2019, Available online 4 June 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.05.006,Cited by (6),"-estimating functions using Approximate Bayesian Computation (ABC) methods. In particular, we use ==== implementation is also provided in the ==== package.","The normality assumption is the usual basis of many statistical analyses in several fields, such as medicine, health sciences, quality control and engineering statistics. Under this assumption, standard parametric estimation and testing procedures are simple and efficient. However, both from a frequentist or a Bayesian perspective, it is well known that these procedures are not robust when the normal distribution is just an approximate model or in the presence of outliers in the observed data. In these situations, robust statistical methods can be considered in order to produce statistical procedures that are stable with respect to small changes in the data or to small model departures; see Huber and Ronchetti (2009) for a review on robust methods.====The concept of robustness has been widely discussed in the frequentist literature; see, for instance, Hampel et al. (1986), Tsou and Royall (1995) and Markatou et al. (1998). Also Bayesian robustness with respect to model misspecification have attracted considerable attention. For instance, Lazar (2003), Greco et al. (2008), Ventura et al. (2010) and Agostinelli and Greco (2013) discuss approaches based on robust pseudo-likelihood functions, such as the empirical likelihood, as replacement of the genuine likelihood in Bayes’ formula. Lewis et al. (2014) discuss an approach for building posterior distributions from robust ====-estimators using constrained Markov Chain Monte Carlo (MCMC) methods. Recent approaches based on tilted likelihoods can be found in Grünwald and van Ommen (2017), Watson and Holmes (2016), Miller and Dunson (2018). Finally, approaches based on model embedding through heavy-tailed distributions are discussed by Andrade and O’Hagan (2006).====The aforementioned approaches may present some drawbacks. The empirical likelihood is not computable for small sample sizes and posterior distributions based on the quasi-likelihood can be easily obtained only for scalar parameters. The restricted likelihood approach of Lewis et al. (2014), as well as all the approaches based on estimating equations can be computationally cumbersome with some robust ====-estimating functions (such as, for instance, those used in linear mixed effects models). The tilted and the weighted likelihood approaches refer to concepts of robustness that are not directly related to the one considered in this paper, which is based on the influence function (Hampel et al., 1986, Huber and Ronchetti, 2009). Finally, the idea of embedding the model in a larger structure has the cost of requiring the elicitation of a prior distribution for the extra parameters introduced. Moreover, the statistical procedures derived under an embedded model are not necessarily robust in a broad sense, since the larger model may still be too restricted.====Here we focus on the robustness approach based on the influence function and on the derivation of robust posterior distributions from robust ====-estimating functions, i.e. estimating equations with bounded influence function (see, e.g., Huber and Ronchetti, 2009, Chap. 3). In particular, we propose an approach based on Approximate Bayesian Computation (ABC) methods (see, e.g., Beaumont et al., 2002) using robust ====-estimating functions as summary statistics. The idea extends results of Ruli et al. (2016) on composite score functions to Bayesian robustness. The method is easy to implement and computationally efficient, even when the ====-estimating functions are potentially cumbersome to evaluate. Theoretical properties, implementation details and simulation results are discussed.====The rest of the paper is structured as follows. Section 2 sets the background. Section 3 describes the proposed method and its properties. Section 4 investigates the properties of the proposed method in the context of linear mixed models through simulations and an application to a clinical study. Concluding remarks are given in Section 5.",Robust approximate Bayesian inference,https://www.sciencedirect.com/science/article/pii/S037837581830106X,4 June 2019,2019,Research Article,279.0
"Peng Jiayu,Lin Dennis K.J.","Department of Statistics, The Pennsylvania State University, State College, PA 16802, United States","Received 9 December 2017, Revised 13 March 2019, Accepted 1 April 2019, Available online 29 May 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.jspi.2019.04.011,Cited by (0),"Consider the problem of a design for estimating all main effects and the overall error variance. The conventional choice is a saturated ==== are evaluated to illustrate our theory. Simulation results are provided to demonstrate that the proposed design achieves a more reliable estimate of error variance, as well as a reliable significant test. Furthermore, we study the optimal follow-up plan. Relevant ==== theories are established and a convenient construction method for optimal follow-up design is proposed.","To provide extra degrees of freedom for estimating the error variance (====), one traditional design strategy is to add center points. Under a first-order main-effect model with ==== experimental factors, suppose one is asked to conduct an ====-run design where ==== is slightly greater than ====. The conventional wisdom is to perform a ==== orthogonal main-effect design, e.g., a Plackett–Burman (Plackett and Burman, 1946) design, and then add ==== center points. In this article, we show that adding center points may not be a favorable plan for estimating ====. We show that to perform the ==== design as a whole, a better and in fact optimal plan is to choose ==== column from an ====-run orthogonal main-effect design. Optimality theorems are derived and illustrative examples are given.====As a motivative example, consider a study involving seven factors ====, via a linear first-order model ====, where ====’s are independent and identically distributed (i.i.d.) with mean ==== and variance ====. Here we have eight effect parameters ====, as well as the error variance ==== to estimate. For estimation of ====, an eight-run experiment will be sufficient. Suppose we decide to add four additional runs for estimating ====, thus a total of 12 runs is called for.====How to design such an experiment in 12 runs? The conventional wisdom is to add four center points, to a ==== fractional factorial design, or here we call it an 8 × 7 “H-design”; see Design A in Table 1(a). We propose an alternative plan: choose any ==== columns from a 12 × 11 H-design; see Design B in Table 1(b). Throughout, an ====-run H-design refers to any ==== matrix ==== such that ==== forms an Hadamard matrix. (Here ==== is any multiplier of 4; ==== is a column of ====’s; the two levels in the Hadamard matrix are labeled as “＋ ” and “====”.) As will be shown, Design B is more preferable than Design A in terms of estimating ====. Explicitly, ==== is smaller under Design B, where ==== is the estimate of ====.====The rest of the paper is organized as follows. In Section 2, we present the explicit problem formulation, and establish the optimality of Design B in estimating ====. Under various common distributions, theoretical values of ==== have been evaluated for both Designs A and B. It is shown that Design B achieves a substantially less dispersed ==== than Design A. Section 3 presents the summary of a lengthy simulation study. The simulation results support our theory, and show that the significance tests achieve more reliable performance under Design B. Section 4 is devoted to a slightly different scenario, where designs are restricted to be a (saturated) main-effect design supplemented by several follow-up runs. A theory of the optimal follow-up design is established, and a convenient method is proposed to construct (nearly-)optimal follow-up designs. Concluding remarks are given in Section 5. All proofs are deferred to the Supplementary Material.",Small screening design when the overall variance is unknown,https://www.sciencedirect.com/science/article/pii/S0378375819300515,29 May 2019,2019,Research Article,280.0
"Ohishi Mineaki,Yanagihara Hirokazu,Fujikoshi Yasunori","Department of Mathematics, Graduate School of Science, Hiroshima University, 1-3-1 Kagamiyama, Higashi-Hiroshima, Hiroshima 739-8526, Japan","Received 18 May 2017, Revised 27 December 2018, Accepted 10 April 2019, Available online 23 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.010,Cited by (7),"In this paper, we deal with the optimization method of ridge parameters in a generalized ridge regression by minimizing a ==== (MSC). The optimization methods based on minimizations of generalized ","A linear regression model constitutes one important tool for clarifying the relationship between a scalar response variable and one or more explanatory variables. Let ==== be an ====-dimensional vector of response variables and ==== be an ==== matrix of nonstochastic centralized explanatory variables (====) with ====, where ==== is the sample size, ==== is the number of explanatory variables, ==== is an ====-dimensional vector of ones, and ==== is a ====-dimensional vector of zeros. The equation of the linear regression model is given by ====where ==== is an unknown location parameter, ==== is a ====-dimensional vector of unknown regression coefficients, and ==== is an ====-dimensional vector of independent error variables from a distribution with mean 0 and variance ====.====One of the methods for avoiding the serious problem of multicollinearity among explanatory variables is the generalized ridge regression (GRR) proposed by Hoerl and Kennard (1970), which gives a shrinkage estimate of ==== towards ==== via multiple ridge parameters ====, where ====
 (====). Although the GRR was proposed 49 years ago, it is still used in many research fields (e.g., Liu et al., 2011, Shen et al., 2013). Since the estimate of the GRR estimator (GRRE) of ==== changes depending on ====, it is important how to optimize ====.====An optimization method based on a minimization of a model selection criterion (MSC) with respect to ==== (called MSC-minimization method) is one of the common methods for optimizing ====
 (see e.g., Lawless, 1981, Walker and Page, 2001, Yanagihara, 2018). The generalized ====
 (====) criterion (see e.g., Atkinson, 1980, Nagai et al., 2012) and the generalized cross-validation (GCV) criterion (see e.g., Craven and Wahba, 1979, Ye, 1998, Yanagihara, 2018) are famous model selection criteria. The ====- and the GCV-minimization methods are fast because their minimizers can be derived as closed forms. Unfortunately, although the methods are fast, they do not work well when ==== is larger than ====. This is because the ==== criterion cannot be defined and the GRRE derived from the GCV-minimization method hardly shrinks towards ====
 (see Yanagihara, 2018) in many cases when ==== is larger than ====. On the other hand, there are other MSCs, e.g., a generalized information criterion (GIC; see Nishii, 1984), a bias-corrected Akaike information criterion (AIC) (====; see Hurvich and Tsai, 1989, Hurvich et al., 1998), and a small-sample corrected GCV (====) criterion (see Boonstra et al., 2015). Although it is possible that the optimization methods based on the MSCs work well even if ==== is larger than ====, the methods are not fast because the minimizers of the MSCs have not been derived as closed forms. Even though the minimization problems of the MSCs cannot be solved analytically, we propose a fast optimization algorithm to minimize MSC by specifying the small number of minimizer candidates.====This paper is organized as follows: In Section 2, we describe several results for deriving the main theorem. In Section 3, we propose a fast optimization algorithm for the MSC-minimization method. In Section 4, we show fast algorithms for solving minimization problems of existing MSCs and new MSC. Numerical examinations are discussed in Section 5. Technical details are provided in Appendix.",A fast algorithm for optimizing ridge parameters in a generalized ridge regression by minimizing a model selection criterion,https://www.sciencedirect.com/science/article/pii/S0378375819300503,23 May 2019,2019,Research Article,281.0
Pilz Maximilian,"Institute of Medical Biometry and Informatics, University of Heidelberg, Im Neuenheimer Feld 130.3, 69120 Heidelberg, Germany","Received 21 August 2018, Revised 29 January 2019, Accepted 26 April 2019, Available online 21 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.007,Cited by (1)," (2009) developed a theoretical framework to analyse the ==== and proved important properties as the consistency of the Elastic Net estimator under certain model assumptions. In this paper, these assumptions are relaxed and extended to a wider class of noise distributions. It is shown that the consistency of the Elastic Net still holds true under a finite second moment assumption on the noise term.",None,Consistency of the Elastic Net under a finite second moment assumption on the noise,https://www.sciencedirect.com/science/article/pii/S0378375818302210,21 May 2019,2019,Research Article,282.0
"Zou Yuye,Fan Guoliang,Zhang Riquan","School of Economics and Management, Shanghai Maritime University, Shanghai, 201306, China,Key Laboratory of Advanced Theory and Application in Statistics and Data Science - MOE, School of Statistics, East China Normal University, Shanghai, 200062, China","Received 17 December 2018, Revised 1 April 2019, Accepted 29 April 2019, Available online 20 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.008,Cited by (10)," of the proposed weighted QR estimators for unknown parameters and the link function are established. Moreover, to select the important predictors, a variable selection procedure is introduced by applying adaptive LASSO penalized and the oracle property of the proposed weighted penalized estimators is obtained simultaneously. The finite sample performance of the proposed estimation methods and variable selection procedure are evaluated via simulation study. We also illustrate the proposed methods by using a dataset from a breast cancer clinical trial.","Various semi-parametric models have been extensively studied due to the explanatory power and the flexibility of modeling with multivariate covariates. Among the semi-parametric modeling literature, the following partially linear single-index (PLSI) models play an important role, which has form ====where ==== and ==== are covariates, ==== is the response, ==== is an unknown function, ==== is unknown parametric vector in ==== with ==== (where ==== denotes the Euclidean norm) for identifiability and the first nonzero element of ==== is positive. The error ==== is independent of ==== with ==== and ====. Model (1.1) which can avoid the “curse of dimensionality”, is a combination of the single-index models (Ichimura, 1987, Härdle et al., 1993) with ====, and the partially linear models (Speckman, 1988, Härdle et al., 2000) with ====. Model (1.1) has drawn much attention of many researchers, and various methods have been proposed to study this model. For example, Zhu and Xue (2006) discussed empirical-likelihood-based inference for the parameters in PLSI models. They defined a bias-corrected empirical log-likelihood ratio and obtained the standard chi-squared distribution as its asymptotic distribution. Liang et al. (2010) constructed efficient estimation of the parameters by profile likelihood method and also considered the variable selection problem for the PLSI models. Further studies of model (1.1) can be found in Lv et al., 2014, Lv et al., 2015 and Yu et al. (2017) among others.====In practical applications, especially in survival analysis and biomedical studies, the response ==== cannot be completely observed due to some censoring variables ==== with distribution function (df) ====. One can only observe ====
 ====. triples ==== with df ==== and the censoring indicator ====. Usually, the censoring indicators are assumed to be observable, however, the censoring indicator may not be observed completely in application. For instance, in bioassay trail, some subjects may not be autopsied to save expense; in epidemiological studies, relevant death certificate information can be missing due to emigration; in clinical trials, one may distinguish between deaths attributable to the disease of interest and deaths due to all other causes (Wang and Shen, 2008). In such situations, some censoring indicators are missing. The existing estimations with censored data cannot be applied directly to such a case. One simple solution is to use the complete case (CC) approach by simply ignoring the missing data. However, this CC estimation is highly inefficient as the degree of missingness is significant. Alternatively, some analysis directly combine survival data with missing censoring indicators. For convenience, we define a missing indicator ====, which is 1 if ==== is observed and is 0 otherwise. In this paper, we assume that ==== is independent ==== and the censoring indicator is missing at random (MAR), which implies that ==== and ==== are conditionally independent given ====, i.e., ====. Hence the observed data are ====The MAR assumption is common in statistical analysis with missing data and is reasonable in many practical situations; see Little and Rubin (1987) for example. For survival data with censoring indicators MAR, Subramanian (2004) and Wang and Ng (2008) constructed different estimators of the survival function, and established their asymptotical properties. Wang et al. (2009) firstly defined two inverse-probability-of-censoring weighted estimators of the density function, and obtained asymptotically normality and uniformly strongly consistency of the proposed estimators. Wang and Dinse (2011) and Li and Wang (2012) investigated three different estimators for the regression coefficient vector in linear regression.====The most existing estimation methods for the PLSI models focus on mean regression based on least squares or likelihood method. However, they are sensitive to outliers and may be inefficient for some non-normal errors. The quantile regression (QR) proposed by Koenker and Basset (1978) is more robust alternative to explore the underlying relationship between the covariates and the response. See Koenker (2005) for comprehensive review. Cai and Xiao (2012) discussed the QR in dynamic models with time series. Lv et al. (2015) considered the QR and variable selection of PLSI models under complete data. For the random right-censored models, Leng and Tong (2013) constructed a QR estimator through an unbiased estimating equation. Xie et al. (2015) developed the QR estimation by weighted inverse probability approach based on varying-coefficient models. For the random missing data, Yi and He (2009) considered the linear QR with missing response. Sherwood et al. (2013) and  Yang and Liu (2016) investigated the linear QR models with missing covariates. For the right-censored data with missing censoring indicators, Shen and Liang (2017) proposed a three-stage approach to construct the estimators of the linear part and nonparametric function for partially linear varying-coefficient QR models. As far as we know, there is no literature related to the PLSI model with censoring indicators MAR, and we shall focus on the QR estimation problems for this model.====Furthermore, to select the important variables in PLSI QR model with censoring indicators MAR, the variable selection for predictors is necessary when the observed data include irrelevant covariates, especially for the high-dimension covariates. Various powerful penalization methods have been developed to select important variables, such as LASSO (Tibshiranit, 1996), SCAD (Fan and Li, 2001), adaptive LASSO (Zou, 2006) and so on. It has been shown that the LASSO penalty produces biases even in the simple regression setting (Fan and Li, 2001) due to the linear increase of the penalty on regression coefficients. SCAD and adaptive LASSO can remedy this bias issue and adaptive LASSO is easy computation which can be solved conveniently by linear programming. We shall propose the variable selection procedure by applying adaptive LASSO penalty.====It is worth pointing out that there is no result available in the literature for the PLSI model with censoring indicators MAR. In this paper, our aim is to study this model. Such considerations are motivated by a clinical trial (Cummings et al., 1985) which was conducted by the Eastern Cooperative Oncology Group with the aim to evaluate tamoxifen as a treatment for stage II breast cancer among elderly women, where the time to death due to breast cancer is censored and the censoring indicator is MAR. Our contribution includes the following aspects. Firstly, we develop QR method to study this model and introduce weighted estimating function approach, whereby the contribution to the estimating function is weighted by product-limit estimator and the regression calibration, imputation and inverse probability weighting methods. The weighted QR estimators of unknown parameters and function are proposed and the asymptotic properties of the estimators are established. Further, in order to build parsimonious and robust models, we propose weighted penalized estimators with the help of adaptive LASSO to select significant parametric components. The oracle properties of the penalized estimators are also obtained.====The rest of this paper is organized as follows. In Section 2, we construct QR estimators of the parametric part and link function for the PLSI model with censoring indicators MAR. The variable selection procedure is proposed in this section too. Main results of the proposed estimators are given in Section 3. A simulation study is conducted in Section 4. Concluding remarks are given in Section 5. All technical details are given in an online supplement.",Quantile regression and variable selection for partially linear single-index models with missing censoring indicators,https://www.sciencedirect.com/science/article/pii/S0378375819300382,20 May 2019,2019,Research Article,283.0
"Han Xiaoxue,Liu Min-Qian,Yang Jian-Feng,Zhao Shengli","School of Statistics and Data Science, LPMC & KLMDASR, Nankai University, Tianjin 300071, China,School of Statistics, Qufu Normal University, Qufu 273165, China","Received 24 January 2018, Revised 4 March 2019, Accepted 26 April 2019, Available online 20 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.012,Cited by (7),"-level FFSP designs of resolution III or IV to contain clear main effects or two-factor interaction components. Particularly, the sufficient conditions are proved through constructing the corresponding designs. The new results here are more general and include Zhao and Chen (2012a,b)’s results as special cases for ====.","Fractional factorial (FF) designs, due to the run size economy, are commonly used for factorial experiments in many fields, such as agriculture, medicine, chemistry, and high-tech industry. A ==== design denotes a regular fraction with ==== runs and ==== two-level factors, which has ==== independent columns and is determined by ==== independent defining words. Such designs have a simple aliasing structure in that any two effects are either orthogonal to or fully aliased with each other. If the levels of some factors in an experiment are difficult to change or control, it may be impractical or even impossible to conduct the runs in a completely random order, which makes one consider a special design called the fractional factorial split-plot (FFSP) design. In general, an FFSP experiment has two types of factors: hard-to-change factors named whole-plot (WP) factors and the rest factors named sub-plot (SP) factors. One can randomly choose one of the level-settings of WP factors and then run all of the level-combinations of the SP factors in a random order with the WP factors fixed. The procedure above is repeated for other WP factor-level combinations until the experiment is complete.====In terms of ranking designs, minimum aberration and clear effects are two most commonly used criteria. Minimum aberration criterion was first introduced by Fries and Hunter (1980) for distinguishing FF designs and was studied extensively by Chen (1992), Chen and Hedayat (1996), Tang and Wu (1996) and others. Huang et al. (1998) and Bingham and Sitter, 1999a, Bingham and Sitter, 1999b extended this criterion to ranking FFSP designs. The notion of clear effect was proposed by Wu and Chen (1992). A main effect or a two-factor interaction (2FI) of a ==== design is said to be clear if it is not aliased with any other main effect or 2FI. A clear effect can be estimated unbiasedly under the weak assumption that effects involving three or more factors can be ignored. In any resolution IV ==== design, all the main effects are clear. Then, a resolution IV ==== design with the most clear two-factor interactions is preferred, see Wu and Hamada (2009, p. 217). For a resolution III ==== design, we can assume the magnitude of the main effect is much larger than that of the 2FI according to effect hierarchy principle (Wu and Hamada, 2009 p. 172). When some background knowledge suggests that certain effects are potentially important, we will choose the designs with more clear effects. Yang et al. (2006) gave necessary and sufficient conditions for the existence of two-level FFSP designs containing various clear effects. Zi et al. (2006) derived the upper and lower bounds on the maximum numbers of clear effects for FFSP designs. For comprehensive discussions on the theory of clear effects, we refer the readers to Tang et al. (2002), Wu and Wu (2002), Ai and Zhang (2004), and Chen et al. (2006). Cheng and Tsai (2009) proposed a general and unified approach to the selection of regular FF designs with split-plot designs as a special case.====In practice, mixed-level FF designs are commonly used in the experiments when the numbers of levels of the factors are not all equal to each other. Zhao et al., 2008, Zhao and Zhang, 2008 and Zhao and Zhao (2015) studied the mixed-level designs containing clear effects. Zhao and Chen, 2012a, Zhao and Chen, 2012b investigated the mixed-level split-plot designs with a four-level and some two-level factors and gave the conditions for such designs to have various clear effects.====High-level factors are often encountered in real life. One such example can be found in Example 1.6 of Montgomery (2013), which considers an eight-level factor in an experiment of designing a web page. If frequently changing the levels of the factor is not allowed, we will need an FFSP design with one eight-level factor in WP part. Usually, for designs containing clear effects, there are two main research topics. The first one considers the conditions for a design to have clear effects, and the second one is about the construction of the design with the most clear effects. This paper focuses on the first one and considers the ==== regular mixed-level FFSP design that contains ==== 2-level WP factors, ==== 2-level SP factors and one ====-level WP or SP factor. Section 2 gives such notation and definitions. Section 3 provides the conditions for ==== designs to contain clear main effects and two-factor interaction components, and Section 4 discusses when ==== designs contain clear main effects and two-factor interaction components, where the subscript ==== or ==== means the ====-level factor is an SP or WP factor. Section 5 contains some concluding remarks.",Mixed 2- and ,https://www.sciencedirect.com/science/article/pii/S0378375819300527,20 May 2019,2019,Research Article,284.0
"Weller Zachary D.,Hoeting Jennifer A.","Department of Statistics, Colorado State University, United States","Received 9 August 2017, Revised 1 October 2018, Accepted 10 May 2019, Available online 20 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.05.005,Cited by (3),", but their use for exploring spatially-referenced data is relatively newer and less common. Within spatial ","Modeling spatially-referenced data typically requires choosing a covariance function to describe spatial dependence. A common simplifying assumption is that the covariance function is isotropic. Isotropy implies that the dependence between any two observations depends only on the Euclidean distance between their sampling locations and not on their relative orientation. Graphical techniques, such as directional sample variograms, are typically used to check the assumption of isotropy, but their interpretation is subjective and open to interpretation. Thus, there is a need for more objective assessments of the assumption of isotropy.====There are several approaches for testing spatial isotropy. Haskard (2007) develops a parametric test of geometric anisotropy. Ecker and Gelfand (1999) provide a parametric Bayesian framework for inferences about geometric anisotropy parameters. One concern with these two methods is potential misspecification of the covariance function. The Haskard (2007) approach also requires parameter estimation under the null and alternative hypotheses, which can be computationally demanding for large data sets. The proposed nonparametric test avoids the problems of choosing a covariance function from the plethora of available models and potential misspecification of the covariance function. Additionally, the new method we propose does not assume geometric anisotropy and avoids parameter estimation under the null and alternative hypotheses.====Weller and Hoeting (2016) provide a review of nonparametric hypothesis tests of isotropy properties in spatial data. Guan et al. (2004), Maity and Sherman (2012), Petrakis and Hristopulos (2016), and others have developed nonparametric tests of isotropy in the spatial domain. Petrakis and Hristopulos (2016) approximate the sampling distribution of geometric anisotropy parameters to test isotropy. The methods from Guan et al. (2004) and Maity and Sherman (2012) can be computationally intensive due to the requirement of spatial resampling. These nonparametric spatial domain tests also require the user to make several choices before implementing the test, such as the spatial lag set and resampling block size. These choices can affect test results, and their optimality remains an open question. The proposed spatial isotropy spectral (SIS) test is free of user choices and computationally efficient.====There are several methods that use nonparametric estimation of the spectral density function to test second order properties in spatially-referenced data. For example, Lu and Zimmerman (2005) and Scaccia and Martin, 2005, Scaccia and Martin, 2002 test symmetry properties of the covariance function for lattice processes. Fuentes (2005) develops a nonparametric, spatially varying spectral density function to test for nonstationarity. Van Hala et al. (2014) use a spatial frequency domain empirical likelihood to test isotropy and separability of the covariance function. None of these methods were developed with the primary goal of testing isotropy. Thus, while many of the concepts that are used for the proposed SIS test are not new, their combined use for testing isotropy in geostatistical processes has not been previously exploited or explored. The SIS test is the first to use the spectral representation of the covariance function for the primary goal of testing the assumption of spatial isotropy.====The remainder of this paper is organized as follows: in Section 2 we outline important background on spectral methods and provide the motivating theorem for the spectral-based test; Section 3 describes the periodogram, the nonparametric estimator of the spectral density function, and its properties; Section 4 develops the proposed SIS test and describes associated challenges; Section 5 offers a simulation study and application of the SIS test; we conclude with a discussion in Section 6.",A nonparametric spectral domain test of spatial isotropy,https://www.sciencedirect.com/science/article/pii/S0378375819300497,20 May 2019,2019,Research Article,285.0
"Yu Meiju,Wang Dehui,Yang Kai,Liu Yan","School of Mathematics, Jilin University, Changchun 130012, China,School of Mathematics, Tonghua Normal University, Tonghua 134000, China,School of Mathematics and Statistics, Changchun University of Technology, Changchun 130012, China,School of Science, Changchun University, Changchun 130022, China","Received 20 January 2018, Revised 27 September 2018, Accepted 10 May 2019, Available online 17 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.05.004,Cited by (8)," of the estimators are established. The performance of these estimators is compared through a simulation experiment. Moreover, the coherent forecasting for BRCINAR(1) model is addressed. Finally, an application to a real data example is investigated to assess the performance of the model.","In many fields, it is fairly common to model and predict non-negative integer-valued time series, e.g., in reliability theory, insurance theory, medicine, finance and ecology. One of the most straightforward way to construct integer-valued time series models is by means of replacing the multiplication in the well-known ARMA models by an appropriate thinning operator. The most widely known thinning operator is binomial thinning operator ==== which was introduced by Steutel and Van Harn (1979) and defined as ====where ==== is an integer-valued random variable, ====, ==== is a sequence of independent and identically distributed (i.i.d.) Bernoulli random variables with ==== and is independent of ====. Al-Osh and Alzaid (1987) used binomial thinning operator to define a first-order integer-valued autoregressive (INAR(1)) process with Poisson marginal. In recent years, various modified thinning operators have been proposed to capture the specificity of the observed data and many new INAR-type processes are defined based on these new thinning operators. For example,  Gauthier and Latour (1994) introduced the generalized thinning operator, Zheng et al. (2007) introduced the random coefficient thinning operator, Kim and Park (2008) introduced signed binomial thinning, Ristić et al. (2009) introduced the negative binomial thinning operator, Zhang et al. (2010) introduced the signed generalized power series thinning operator, etc. For more details on thinning operators for modelling time series, we can refer to Weiß (2008) and Scotto et al. (2015).====When there are cross-correlations between some time series of counts, the multivariate integer-valued time series models based on thinning operator are needed. The extension of above INAR-type processes to the multivariate cases has become a topic of active research in recent years. Franke and Subba Rao (1993) introduced a multivariate INAR(1) model based upon matrix-binomial thinning operator. Latour (1997) proposed an extension for multivariate integer-valued autoregressive models based on generalized thinning operator. Pedeli and Karlis, 2011, Pedeli and Karlis, 2013a, Pedeli and Karlis, 2013b used matrix-binomial thinning operator to extend the INAR(1) model to multivariate situation and introduced some properties, estimation and applications. Ristić et al. (2012) discussed a bivariate INAR(1) model with geometric marginal based on negative binomial thinning operator. Scotto et al. (2014) proposed some bivariate binomial autoregressive models in order to fit count time series with a finite range of counts. Liu et al. (2016) proposed a new stationary bivariate INAR(1) process with zero truncated Poisson marginal distribution. Popović et al. (2016) introduced a bivariate INAR(1) model with geometric marginal distribution.====Let ==== and ==== be bivariate integer-valued random vectors and ====, then the bivariate INAR(1) (BINAR(1)) model introduced by Pedeli and Karlis (2011) is defined as ====where ==== is binomial thinning operator and ==== is a matrical operation which acts as the usual matrix multiplication and keeps the properties of the binomial thinning operation. Furthermore, it is assumed that all binomial thinning operations are performed independently of each other. ==== is usually referred to as innovation, which is a sequence of i.i.d. bivariate non-negative integer-valued random vectors and the dependence between ==== and ==== is allowed. The advantage of BINAR(1) model is that it not only takes into account the cross-correlation between the two sequences, but more importantly it preserves the properties of some moments of the univariate INAR(1) model. If ==== respectively denotes the number of patients infected by both different diseases in one area at the ==== time and ==== describes the survival probability of the ====th disease for ====. As we know, the survival probability ====
 is affected by various environmental factors, such as local medical levels. The environmental factors can change over time, so ==== should vary with time and may be a random variable. One drawback of BINAR(1) model is that its autoregressive coefficient ==== within each regime is fixed constant. The motivation for this paper is to introduce a bivariate random coefficient INAR(1) process by replacing ==== by random sequences ==== and study some probabilistic and statistical properties of the process.====The paper is organized as follows. In Section 2, we introduce a new bivariate random coefficient INAR(1) process and establish some probabilistic and statistical properties of this model. In Section 3, we use Yule–Walker, conditional least squares and conditional maximum likelihood methods to estimate the unknown parameters and give asymptotic properties of these estimators. In Section 4, we give some simulation study to compare the performances of three estimation methods. In Section 5, we discuss the coherent forecasting for the bivariate random coefficient INAR(1) model. Section 6 gives an application to a real data set and Section 7 concludes.",Bivariate first-order random coefficient integer-valued autoregressive processes,https://www.sciencedirect.com/science/article/pii/S0378375819300485,17 May 2019,2019,Research Article,286.0
"Balabdaoui Fadoua,de Fournas-Labrosse Gabriella","Seminar für Statistik, ETH, Zürich, 8092, Schweiz, Switzerland","Received 16 August 2018, Revised 25 January 2019, Accepted 24 April 2019, Available online 16 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.006,Cited by (6)," exists, is strongly consistent and converges weakly to the truth at the ====-rate. Furthermore, we fully describe its limit distribution as the ",None,Least squares estimation of a completely monotone pmf: From Analysis to Statistics,https://www.sciencedirect.com/science/article/pii/S0378375818302064,16 May 2019,2019,Research Article,287.0
"Hu Yuping,Xue Liugen,Zhao Jing,Zhang Liying","School of Mathematics and Statistics, Zhengzhou University, Zhengzhou, 450001, China,College of Applied Sciences, Beijing University of Technology, Beijing 100124, China,China National Institute of Standardization, Beijing 100191, China","Received 9 August 2018, Revised 1 May 2019, Accepted 6 May 2019, Available online 15 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.05.001,Cited by (3), of the proposed estimators and test are established and finite sample behavior is studied through a small simulation experiment.,"In the recent literature, there has been an increasing interest in functional data analysis with its extensive application in biometrics, chemometrics, econometrics, medical research as well as other fields. Functional data are intrinsically infinite dimension and thus, classical methods for multivariate observations are no longer applicable. There has been intensive methodological and theoretical development in function data analysis, see Besse and Ramsay (1986), Rice and Silverman (1991), Ramsay and Silverman (2002) and Horváth et al. (2013), Lian and Li (2014), Feng and Xue (2016), Huang et al. (2016) and Yu et al. (2017). It is also frequently the case that a response will be related to both a vector of finite length and a function-valued random variable as predictor variables, which refers to the partial functional linear models. Some research on partial functional linear models has been provided. For example, Zhang et al. (2007) discussed a partial functional linear model by incorporating a parametric linear regression into functional linear regression models. Shin (2009) proposed new estimators and their asymptotic properties for the parameters of a partial functional linear model. Furthermore, Lu et al. (2014) extended the results of Shin (2009) to quantile regression setting. Yu et al. (2016) investigated the hypothesis test of the parametric component in partial functional linear regression. However, the aforementioned articles have two significant limitations. First, they assume a single functional predictor model so it cannot be applied in a situation involving ==== different functional predictors. Second, these articles mainly focus on the estimation of the functional linear regression models with common errors which have symmetric property. However, in many fields such as economics, finance, biomedicine, environment, demography, and pharmacokinetics etc., sometimes error structures in regression models no longer satisfy symmetric property. Often there is a presence of high skewness and fat tails. Therefore it is of practical interest to develop more flexible approaches using a broader family of distributions.====The first of these limitations can, in principle, be easily solved using the multiple functional predictors. The second limitation, that of symmetric property, can be addressed using a skew-normal distribution, which represents a generalization of normal distribution but has an additional shape parameter. There has been considerable work in non-normal distribution. Cancho et al. (2010) relaxed the normality assumption of the error in nonlinear regression models by using skew-normal distribution introduced by Sahu et al. (2003). A generalized flexible skew-elliptical distribution for random effects density was adopted by Ma and Genton (2004). Xie et al. (2009) considered some diagnostics for skew-normal nonlinear regression models with AR(1) errors. Wu et al. (2017) discussed linear mixed effect models with skew-normal errors and distribution-free random effects. Thus, in this paper, we consider partial functional linear regression models based on replacing the assumption of normal distribution by a weaker assumption of skew-normal distribution.====Various approaches are available for fitting functional data, such as, functional principal component method, spline methods, rough penalty, and so on. The more traditional approach, masterfully documented in the monograph by Ramsay and Silverman (2005), typically starts by representing functional data with an expansion with respect to a certain basis, and subsequent inferences are carried out on the coefficients. The common used bases include the B-spline basis for non-periodic data and the Fourier basis for periodic data. Recently, methods based on functional principal component analysis (FPCA) are popular and widely used by researchers. It allows finite dimensional analysis of a problem that is intrinsically infinite dimensional. Dauxois et al. (1982) obtained the basic results related to asymptotic theory of FPCA when the functions are directly observable. Cardot et al. (1999) applied FPCA to estimate the slope function and derived convergence of the estimator in functional linear regression models with scalar response. Hall and Hosseini-nasab (2009) and Hall and Horowitz (2007) proved that this approach provides an orthogonal data-driven basis providing optimal convergence rates in the functional linear regression model.====In another direction of statistical research, when we use regression models to analyze data, over dispersion is a common problem of concern. If we have variance heterogeneity in the regression models, then it would be more difficult to deal with statistical inference. So it is of great importance to consider the test of detecting varying scale of the skew-normal functional linear regression model.====This paper is organized as follows. In Section 2, we describe the FPCA approach to skew-normal partial functional linear models and investigate the asymptotic properties. Section 3 discusses the score test for homogeneity of variance in skew normal partial functional linear models. Some simulation studies are given in Section 4, which include the asymptotic properties and powers of the score test. Section 5 demonstrates the effectiveness of the proposed method by an application for the Tecator data set. In Section 6, we conclude our results with a discussion. The technical proofs are collected in the Appendix.",Skew-normal partial functional linear model and homogeneity test,https://www.sciencedirect.com/science/article/pii/S0378375818301903,15 May 2019,2019,Research Article,288.0
"Schmidt Marius,Schwabe Rainer","Institute for Mathematical Stochastics, Otto-von-Guericke-University Magdeburg, PF 4120, 39016 Magdeburg, Germany","Received 15 August 2018, Revised 9 December 2018, Accepted 6 May 2019, Available online 15 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.05.002,Cited by (4),-optimality and ====- and ====-optimality it is shown that the optimal designs in the Poisson and Poisson–Gamma model coincide.,"Count data arises in experiments, where the number of objects or occurrences of events of interest is observed. Frequently, the Poisson model is used to model such data, in which the expected value of the Poisson distributed response variable is linked to a linear predictor consisting of covariates and unknown model parameters. In such experiments there may be repeated measurements for each statistical unit. Assuming a Gamma distributed random effect for each statistical unit, we obtain the Poisson–Gamma model as a generalization of the Poisson model.====The estimates of the unknown model parameters depend on the choice of the covariates. In order to obtain the most accurate parameter estimates, we determine optimal designs, which specify the optimal values and frequencies of the covariates. With such designs the number of experimental units can be reduced, leading to a lowering of experimental costs. Furthermore, for example in animal testing, the use of optimal designs may be required because of ethical reasons.====For the Poisson model Ford and Torsney, 1992, Rodríguez-Torreblanca and Rodríguez-Díaz, 2007 determined ====- and ====-optimal designs for the case of one covariate. Wang et al. (2006) made numerical investigations for two covariates with and without an additional interaction term. For the case of multiple regression with an arbitrary number of covariates Russell et al. (2009) derived ====-optimal designs and Schmidt (2019) determined ====-, ====- and ====-optimal designs. In the context of intelligence testing Graßhoff et al., 2016, Graßhoff et al., 2018 considered the Poisson–Gamma model with one measurement per statistical unit and computed ====-optimal designs for a binary design region.====In Section 2 we introduce the Poisson–Gamma model and derive the Fisher information matrix. Section 3 gives a brief introduction to the theory of optimal design of experiments and deals with information matrix relations between the Poisson and Poisson–Gamma model. We will be concerned with the determination of ====-optimal designs for multiple regression with an arbitrary number of covariates in Section 4 and with optimal designs for linear optimality criteria in Section 5. Since the model under consideration is nonlinear, the optimal designs depend on the unknown parameters and are therefore called locally optimal (cf. Chernoff, 1953). We note that most proofs are deferred to an Appendix.",Optimal designs for Poisson count data with Gamma block effects,https://www.sciencedirect.com/science/article/pii/S0378375818302039,15 May 2019,2019,Research Article,289.0
"Chatterjee A.,Bandyopadhyay T.","Theoretical Statistics and Mathematics Unit, Indian Statistical Institute, Delhi, India,Production & Quantitative Methods Group, Indian Institute of Management, Ahmedabad, India","Received 16 July 2018, Revised 2 February 2019, Accepted 8 May 2019, Available online 15 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.05.003,Cited by (1),Group testing has been widely used in epidemiology and related fields to estimate prevalence of rare diseases. Parametric binary regression models are used in group testing to estimate the covariate adjusted prevalence. Unlike the standard binary regression model (,"Estimation of prevalence of a rare disease based on group testing data has been of considerable interest among the researchers (cf. Gastwirth and Hammick (1989), Tu et al. (1995), Chen and Swallow (1990) and Liu et al. (2012)) for quite some time. Subsequently, attention has been shifted to the estimation of covariate adjusted prevalence in a generalized linear regression framework (Vansteelandt et al., 2000), Xie (2001) using group testing data. The data comprise the covariate information at the individual level, and the binary responses at the group level. Recently, the researchers (Bilder and Tebbs, 2009, Chen et al., 2009, Bilder et al., 2010, McMahan et al., 2013, Zhang et al., 2013b, Wang et al., 2015, Huang and Warasi, 2017) have been investigating other important aspects of group testing in the parametric regression framework. Nonparametric and semiparametric approaches to group testing regression models have been explored by several authors, ====, Delaigle and Meister (2011), Delaigle and Hall (2012), Delaigle et al. (2014), Wang et al. (2013), Wang et al. (2014) and the references therein.====In order to describe the main contributions of this paper, we formulate the group testing regression model in a parametric framework. Suppose, ==== is a binary variable denoting the presence ==== or absence ==== of a trait in an individual and ====, denotes the ====-dimensional covariate vector associated with ====. The parametric binary regression model connecting ==== and ==== is assumed to be, ====where, ==== is a ====-dimensional unknown regression coefficient, ==== is a strictly increasing and continuous distribution function and ==== denotes the true distribution of ====, when the underlying parameter is ====. We assume the link function ==== is ====. In the group testing framework with pool size ====, individual binary responses are unobservable, and instead one observes ====, and the ==== dimensional group covariate ====. Obviously, ====, if at least one subject in the group has the trait, and ====, if no subject in the group has the trait. In many applications, however, the diagnostic tests are not accurate, and have their own sensitivity (====) and specificity (====). In such cases, ==== may be misclassified, and we observe ==== instead of ====, the misclassified version of ====. We thus have, ====The probabilities ==== and ==== are assumed to be ====, not depending on the covariates and are same for all groups (cf. Section 2 of Bilder et al. (2010) and Huang and Warasi (2017)). Typically, ==== are specific for the diagnostic procedure or instrument used for testing, and are known in practice. Let ==== denote the observed value of ====. Then the conditional distribution of ==== given ==== (under the true parameter ====) is, ==== It is well known that the parameter ==== in the parent binary regression model (1.1) is identifiable, if ==== is strictly monotone and continuous and the covariate ====
 has a non-degenerate distribution (cf. Example 5.40 of van der Vaart (1998)). However, unlike the parent model (1.1), the link function ==== connecting group based binary response ==== and covariate ==== in the derived model (1.3) is a multivariable function, which depends on all group covariates, group size and known sensitivity and specificity values. The notion of a ==== link, which is crucial for proving identifiability of ==== in the model (1.1) breaks down in case of the derived regression model (1.3).====In the parametric framework, for inference on the regression parameters based on group testing data, the regression model (1.3) has been widely used by many researchers (see Bilder and Tebbs (2009), Chen et al. (2009), McMahan et al. (2013), Zhang et al. (2013b), Wang et al. (2015), and Huang and Warasi (2017)), by tacitly assuming that (i) the parameter ==== is identifiable on the basis of the derived model, which is of fundamental importance for a meaningful estimation of the parameters, and (ii) the standard asymptotic theory for maximum likelihood estimators (MLE’s) is valid when the link function ==== is used. Possibly, the researchers believe that the proof of these assumptions would directly follow from the results available for ==== binary regression models. It is evident from the discussion in the above paragraph, the identifiability of the regression model using the link ==== does not follow from the identifiability of the binary regression model using the link ====. It turns out, proving both these assumptions are non-trivial theoretical exercises.====In Section 3 we provide a proof of the identifiability of the regression parameter ====, when ==== values are observed and the underlying link is the multivariable function ==== (cf. (1.3)), and the proof is carried out under ==== same conditions which ensure identifiability of ==== in the parent model (1.1).====To develop an asymptotic theory for the MLE’s of the regression parameters we formulate the estimation problem as a root-finding problem (Z-estimation) (Chapter 5 of van der Vaart (1998)). The proof involves two crucial steps: (i) showing uniform convergence of the appropriately scaled score function to its limiting version, and (ii) showing the limiting score function has a well-separated zero at the true model parameter (cf. Theorem 5.9 of van der Vaart (1998)). Proving (ii) is usually difficult, and it requires model identifiability, smoothness properties (concavity) of the log-likelihood function (cf. Amemiya (1985)), and precise assumptions on the parameter space and covariates, which can vary with the choice of link function (cf. Gouriéroux and Monfort (1981)). For standard binary regression, Newey and McFadden (1994) and Amemiya (1985) noted, for different links, even for commonly used logistic and probit links, different inequalities and technical assumptions are needed (also see Example 5.40 of van der Vaart (1998), Fahrmeir and Kaufmann (1985) and Hjort and Pollard (1993) for other sets of assumptions and approaches). For group testing regression models, thus, it is of interest to find out the exact technical assumptions that would be required for developing an asymptotic theory of MLE’s. In Section 4, we prove that under mild regularity conditions on the link ====, parameter space and covariates, the ==== asymptotic theory for MLE’s holds for the group testing regression model (1.3). Our results are valid for a wide range of binary link functions, including the logistic, probit and complementary log–log links. The asymptotic theory is first developed for equal group size, and then it is extended to unequal group sizes in Section 4.1.====Group testing was originally proposed to identify infected individuals (cf. Dorfman (1943)). Subsequently, various re-testing algorithms have been proposed by researchers (cf. Wang et al. (2015)) for detecting infected individuals in a positively tested group. Consequently, in the group testing regression framework, for improved statistical inference, several researchers propose combining responses from the re-tested individuals with group-level responses (cf. Zhang et al. (2013a), Wang et al. (2014), McMahan et al. (2017) and Xie (2001)). One of the most frequently used such procedure (cf. Dorfman (1943)) is to retest every individual in a positively tested group. As suggested by two reviewers, we discuss conditions for developing asymptotic theory for maximum likelihood estimator of ==== based on the data obtained by using Dorfman procedure.====The rest of the paper is organized as follows. In Section 2 we describe the MLE obtained from group testing data and the technical assumptions. Proofs of identifiability and asymptotic normality, for equal group sizes are given in Section 3 and Section 4, respectively. Theoretical results are extended for unequal group size case in Section 4.1. In Section 5 we consider the case of Dorfman retesting. In Section 6 we give the concluding remarks. Appendix contains proofs of auxiliary technical results.",Regression models for group testing: Identifiability and asymptotics,https://www.sciencedirect.com/science/article/pii/S0378375818301423,15 May 2019,2019,Research Article,290.0
"Antonczyk Dirk,Fitzenberger Bernd,Mammen Enno,Yu Kyusang","Amazon, IZA, USA,Humboldt-University Berlin, ZEW, IZA, IFS, ROA and CESifo, Germany,Heidelberg University, Germany,Konkuk University, Republic of Korea","Received 6 December 2017, Revised 30 April 2019, Accepted 30 April 2019, Available online 11 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.009,Cited by (2),"Empirical studies in the social sciences and biometrics often rely on data and models where a number of individuals born at different dates are observed at several points in time, and the relationship of interest centers on the effects of age ====, cohort ====, and time ====. Because of ","This paper develops a new nonparametric approach for statistical inference in models that include age, cohort, and time effects. Empirical studies in the social sciences often rely on data and models where a number of individuals born at different dates are observed at several points in time, and the relationship of interest centers on the age, cohort, and time effects. Examples are studies on the productivity of researchers where the number of articles is modeled as a function of age, time, cohort, and other variables, see Hall et al. (2007) or studies in biometrics, see e.g. Holford, 1983, Holford, 1991, Clayton and Schifflers (1987) and Banatvala et al. (1993) or in psychometrics and sociometrics, see e.g. Hasin and Link (1988) and Robinson and Jackson (2001). Some vintage capital models and hedonic pricing models disentangle depreciation (the age effect), embodied technical change (the cohort effect), and disembodied technical change (the period effect), see Coulson and McMillen (2008). General discussions on the identification problem in models with age, cohort and time effects can be found in Palmore (1978), Rodgers (1982), Mason and Fienberg (1985) and Willets (2004), and more recently in Kuang et al., 2008a, Kuang et al., 2008b. Nonlinear time, age, and cohort effects can be identified in an additive model, as discussed by MaCurdy and Mroz (1995) or Kuang et al., 2008a, Kuang et al., 2008b using parametric models. We develop an additive nonparametric model for this purpose, which neither requires a parametric specification nor assessing the impact of such a specification on the identification problem.====Our motivating example comes from empirical research on the development of wages in Germany. Wage inequality has been increasing in Germany (Dustmann et al., 2009, Dustmann et al., 2014, Fitzenberger and Wunderlich, 2002). In the 1980s, this phenomenon was restricted to the upper part of the wage distribution. Since the mid-1990s, rising wage inequality has also occurred in the lower part of the wage distribution. This study describes how selected quantiles of the conditional wage distribution evolved between 1975 and 2004. We decompose wage growth by quantile into three additive effects: the effect of aging on wage growth, which is the life-cycle effect, the effect of time on wage growth, which is the (macroeconomic) time-trend, and possible cohort-effects; these are effects which are associated with the time a specific cohort was born and which have a permanent effect on this specific cohort. When employing these cohort effects, one is automatically confronted with the (linear) identification problem of time, age, and cohort effects (Heckman and Robb, 1985): time - age = cohort. Several studies implement a parametric approach to address this problem. MaCurdy and Mroz (1995) estimate linear regressions of quantiles in age–time–cohort cells as proposed by Chamberlain (1994). For other papers using age–cohort time models to study the development of wage structures, see e.g. Berger (1985), Gosling et al. (2000), Fitzenberger et al. (2001), Fitzenberger and Wunderlich (2002) and Morgan (1998). The volume by Mason and Fienberg (1985) involves various contributions by different authors on the estimation of age, period, and cohort effects.====The present study seeks to solve the identification problem by employing a nonparametric estimation approach: We develop an additive model which is solved using a backfitting algorithm, in the spirit of Mammen et al. (1999). As stated above, this approach has the advantage that we do not have to worry about the parametric specification, such as the degree of polynomials in the three dimensions. The results can easily be interpreted, as the smooth backfitting algorithm is a projection of the data onto the space of additive models. Based on a plausible normalization of the linear effects, our main results on the economic side can be summarized as followed: We find that life-cycle wage growth is positively associated with the workers’ skill-level. Over the life-cycle (when workers age), wage inequality is growing for a given cohort of workers who entered the labor market in the same year. In line with evidence reported in the literature, we find that wage growth over the life-cycle is lower for female workers. Regarding the time-trends, we find that individuals at the bottom of the wage distribution incurred dramatic losses since the beginning of the 1990s. The time-trend for female workers is higher though than for their male counterparts, leading to a reduction of the gender wage gap over time.====Our paper is organized as follows. We will discuss two versions of nonparametric age–period–cohort models. In the first specification, age, period, and cohort are random. In the second version they are fixed and lie on a lattice. The first model is studied in Section 2. In Section 2.1 we discuss nonparametric identification. We will show that the effects can be estimated up to linear trends. This result extends findings in Kuang et al., 2008a, Kuang et al., 2008b who considered purely parametric models. Section 2.2 contains a description of our nonparametric estimation approach. The estimation procedure makes use of a smoothed least-squares criterion. The estimator can be calculated by iterations of a backfitting algorithm. Details on the implementation of this algorithm are given in Appendix A. This appendix also contains a theoretical result on the numerical convergence of the backfitting algorithm. The asymptotic properties of our estimator are discussed in Section 2.3. Section 3 contains a discussion of nonparametric age–period–cohort models with lattice fixed design. This model is used in our empirical application. Simulations on the finite sample properties of our estimator are given in Section 4. Section 5 contains our empirical study.","A nonparametric approach to identify age, time, and cohort effects",https://www.sciencedirect.com/science/article/pii/S037837581930045X,11 May 2019,2019,Research Article,291.0
Lopes Miles E.,"University of California, Davis, United States","Received 25 May 2018, Revised 25 January 2019, Accepted 19 April 2019, Available online 2 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.004,Cited by (7),"When randomized ensembles such as bagging or random forests are used for ====, the prediction error of the ensemble tends to decrease and stabilize as the number of classifiers increases. However, the precise relationship between prediction error and ensemble size is unknown in practice. In the standard case when classifiers are aggregated by majority vote, the present work offers a way to quantify this convergence in terms of “algorithmic variance,” i.e. the variance of prediction error due only to the randomized training algorithm. Specifically, we study a theoretical upper bound on this variance, and show that it is sharp — in the sense that it is attained by a specific family of randomized classifiers. Next, we address the problem of estimating the unknown value of the bound, which leads to a unique twist on the classical problem of non-parametric density estimation. In particular, we develop an estimator for the bound and show that its MSE matches optimal non-parametric rates under certain conditions. (Concurrent with this work, some closely related results have also been considered in Cannings and Samworth (2017) and Lopes (2019).)","During the past two decades, randomized ensemble methods such as bagging and random forests have become established as some of the most popular prediction methods (Breiman, 1996, Breiman, 2001). Although the literature has thoroughly explored how the prediction error of these methods depends on training sample size (e.g. Bühlmann and Yu, 2002, Breiman, 2004, Hall and Samworth, 2005, Lin and Jeon, 2006, Biau et al., 2008, Biau, 2012, Scornet et al., 2015 among others), comparatively little is known about how the prediction error depends on the number of classifiers (ensemble size). In particular, only a handful of works have considered this question from a theoretical standpoint (Ng and Jordan, 2001, Hernández-Lobato et al., 2013, Cannings and Samworth, 2017, Lopes, 2019) (cf. Section 1.2).====Indeed, it is of basic interest to have some guarantee that an ensemble is large enough so that it has reached “algorithmic convergence” — i.e. when the prediction error is close to the ideal level of an infinite ensemble (on a given dataset). Specifically, this type of guarantee prevents wasteful computation on an excessively large ensemble, and it ensures that any potential gains in accuracy from a larger ensemble are minor.",Estimating a sharp convergence bound for randomized ensembles,https://www.sciencedirect.com/science/article/pii/S0378375819300369,2 May 2019,2019,Research Article,292.0
"Freise Fritjof,Schwabe Rainer","TU Dortmund University, Department of Statistics, Vogelpothsweg 87, 44227 Dortmund, Germany,University of Münster, Institute for Psychology, Fliednerstraße 21, 48149 Münster, Germany,University of Magdeburg, Institute for Mathematical Stochastics, Universitätsplatz 2, 39106 Magdeburg, Germany","Received 5 November 2018, Revised 6 April 2019, Accepted 22 April 2019, Available online 2 May 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.005,Cited by (2),We develop ====-optimal designs for linear main effects models on a subset of the ,"This work was motivated by the problem of calibrating item difficulties of so-called rule-based tests, an advanced form of tests in educational and psychological testing (e.g. Arendasy and Sommer, 2007). The items of such tests, i.e. the tasks an individual has to work on, are developed based on a set of principles so that the difficulty of the items is determined to a great extent by certain attributes of the items or the stimuli the items consist of. In psychological testing, these attributes are often called rules. Here, one important challenge is the analysis of the impact of the rules on the item difficulty.====As an example, an item of a rule-based test measuring human short term memory will be illustrated. Here, an item consists of several, e.g. 20, stimuli that will be presented to a respondent for a certain time, e.g. 30 s. After some further time, the respondent has to identify the presented stimuli within a larger set of stimuli. For measuring figural short term memory, different figures, such as circles or triangles, are often used as stimuli. These figures are further differentiated by attributes (rules) such as size (small vs. large), shading (shaded vs. non-shaded) or colour (black vs. red). The more attributes are used to define the stimuli within one item, the more information has to be remembered and the difficulty of the item increases. The item difficulty is usually measured as the expectation of the number of correctly reproduced stimuli. For the present example, a linear model can be used to analyse the influence of the attributes on the item difficulty, while for tests yielding binary responses logistic regression models are usually applied. The rules occur as two-level factors in the linear predictor of the corresponding models and hence the items are represented by a certain setting of the factors.====A restriction arising in this scenario is the increasing difficulty of items if the number of attributes (rules) is increased. Hence, from a practical point of view it would make little sense to only use one rule or no rule at all, because the item would be too easy. On the other hand, the item would become too difficult if the number of active rules were too large. In these situations, it is doubtful that due to skewed distributions of the responses a linear model can be assumed to be valid. This matter imposes a restriction on the design region, which allows only such items with a bounded number of active rules, i.e. of factors set to the higher level. For example, in an experiment with six different rules it would be meaningful to restrict to items with at least two and at most four active rules. In particular, under these constraints, neither the full factorial nor regular fractional factorial designs can be used anymore.====In Section 2 the model and its information matrix are presented, which is the basis for the comparison of designs. After a short introduction to optimal design and invariance, the special structure of the information matrix is discussed in Section 3. This is followed in the subsequent section by the main results, which constitute conditions on designs for ====-optimality. The findings are illustrated with tables of designs. The proofs are deferred to an appendix.====Some related findings were published recently in Freise and Schwabe (2018) for a model including two-factor interactions, but restricted to symmetric constraints.====The present results can be connected with work in spring balance weighing and chemical balance weighing designs. In contrast to the model considered here these usually do not incorporate an intercept. For ====- and ====-optimal designs with restrictions on the number of objects used in each weighing see Huda and Mukerjee (1988). Optimality for spring balance designs without restrictions but including a constant term in the model is considered in Filová et al. (2011).",Optimal designs for two-level main effects models on a restricted design region,https://www.sciencedirect.com/science/article/pii/S0378375819300370,2 May 2019,2019,Research Article,293.0
"Wang Guochang,Zhang Fode,Lian Heng","College of Economics, Jinan University, Guangzhou, 510632, China,Center of Statistical Research, School of Statistics, Southwestern University of Finance and Economics, Chengdu, 611130, China,Department of Mathematics, City University Hong Kong, Kowloon Tong, Hong Kong","Received 5 July 2018, Revised 18 February 2019, Accepted 29 March 2019, Available online 20 April 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.03.011,Cited by (1)," estimation error and prediction risk. Somewhat surprisingly, the optimal rates for both error measures are achieved at the same truncation level. Effect of discrete sampling of functional predictor on a dense grid is also considered. Simulations and a real data application illustrate the favorable finite sample performance of the method.","Regression problems for functional data have attracted a lot of attention recently. We consider here the standard functional regression problem with a scalar response ==== and a functional predictor ====, which is mathematically assumed to be a random element in ====, where ==== is a bounded interval and we assume ==== without loss of generality throughout. Since the early work of Ramsay, 1982, Ramsay and Dalzell, 1991, parametric functional regression has been widely studied, for example in Cardot et al. (1999), Cai and Hall (2006) and Hall and Horowitz (2007). On the other hand, nonparametric approaches to functional regression were used in Ferraty and Vieu (2002). Extension of semiparametric methodology to functional context has been considered in Müller and Yao (2008), Chen et al. (2011) and Radchenko et al. (2015).====In this article, we focus on effective dimension reduction which is a class of semiparametric models based on the following multiple index structure: ====where ==== is a random error independent of ====, and the ==== indices ==== are of our main interest, serving the purpose of dimension reduction such that ==== and ====
 become independent conditional on ====. The link function ====, although unknown, is treated as a nuisance parameter. The relevance of the model for functional data analysis is similar to that for multivariate data, as discussed in previous works on functional sliced inverse regression. The model is more flexible than linear functional regression without imposing a specific mean structure, which is important given that constructing structural tests are more difficult for functional regression than for multivariate regression. Also, the estimation procedure based on sufficient dimensional reduction is typically simpler which does not require estimation of the nonparametric function simultaneously with ====. One can plot the estimated indices ==== against the responses to explore their relationships before finally coming up with a form of the nonparametric regression function.====Sufficient dimension reduction has a long history for multivariate data. Pioneered by Duan and Li (1991) and Li (1991), sliced inverse regression (SIR) aims to characterize the effective dimension reduction space, requiring neither specification nor estimation of the link function. SIR only uses the first moment of the conditional predictor distribution ==== to target the dimension reduction space. Sliced average variance estimator (Cook and Weisberg, 1991, Cook, 2000), on the other hand, incorporating the second moment ====, can generally discover more directions in the dimension reduction space. Other approaches are proposed in the literature. For example, Yin and Cook (2002) used higher order moments, Xia et al. (2002) and Yin and Li (2011) considered minimum average variance estimator, and Li (1992) used Hessian matrix of the regression function.====Extensions of effective dimension reduction methods to functional data are relatively scarce. Ferre and Yao (2003) proposed sliced inverse regression for functional data. Li and Hsing (2010) considered testing procedure for determining the dimension of the effective dimension reduction space in functional sliced inverse regression. Jiang et al. (2014) and Yao et al. (2015a) proposed sliced inverse regression method for sparse functional data where one observes only a few noisy measurements for each subject. Functional approaches for effective dimension reduction other than sliced inverse regression include Lian and Li (2014) and Wang et al. (2013a).====The two earliest developments in effective dimension reduction, sliced inverse regression and sliced average variance estimator, are still the two most popular methods used to date. However, the former is known to fail when the response distribution is symmetric about the origin, while the latter is not very efficient in estimating monotone trends. Directional regression (DR) as a natural and simple combination of the first two moments, based on empirical directions, was introduced by Li and Wang (2007), and shown to have high accuracy and fast computation time. Although simple contour regression and general contour regression (Li et al., 2005) are based on similar principle, DR directly regresses empirical directions on the responses, and only has ==== time complexity, compared with ==== and ==== complexity for simple contour regression and general contour regression, respectively.====Here we focus on directional regression for fully observed functional data, based on functional principal component analysis. We establish consistency and convergence rate for functional directional regression, for both ==== estimation error and prediction risk. Optimal convergence rates for functional semiparametric models are generally very challenging to establish, and thus the present work contributes to advancing the understanding of semiparametric functional modeling.====Our contribution in this paper is to adopt DR for functional data. Compared to multivariate data, regularization (truncation of spectral decomposition) needs to be used for functional data. Technically, although our proof used pieces of results from Hall and Horowitz (2007) such as the bounds on eigenvalues and eigenfunctions from functional PCA, the overall strategy is not the same. The dimension reduction estimator aims at the eigenfunctions of some operators, which is much more complicated in form than any operators that appeared in Hall and Horowitz (2007). Furthermore, we consider two risks while Hall and Horowitz (2007) only consider the ==== estimation error. Finally, we further clarify the effect of discretely (although densely) observed functional predictor. These aspects make our study different from that of Hall and Horowitz (2007) and is technically nontrivial.====The rest of the article is organized as follows. In Section 2, we formally propose the estimation method based on functional principal component analysis. In Section 3, we present the consistency and rates of convergence of the PCA-based estimator. In Section 4, we report our simulation studies and a data application to illustrate the superior performance of the estimator. Section 5 contains some discussions and also mentions future directions for research. Appendix contains all the proofs for the theoretical results.",Directional regression for functional data,https://www.sciencedirect.com/science/article/pii/S0378375818301228,20 April 2019,2019,Research Article,294.0
"Yoon Hwan-Jin,Welsh Alan H.","Statistical Consulting Unit, The Australian National University, Canberra, ACT 2601, Australia,Mathematical Sciences Institute, The Australian National University, Canberra, ACT 2601, Australia","Received 16 May 2018, Revised 6 December 2018, Accepted 2 April 2019, Available online 17 April 2019, Version of Record 13 June 2019.",https://doi.org/10.1016/j.jspi.2019.04.001,Cited by (6), of the parameters. (iii) Standard statistical software can return local rather than global ML and ,"Suppose that from each of ==== independent clusters, we observe ==== pairs of observations ====, ====, ====, and we want to quantify the linear relationship between the variables ==== and ====. A common approach is to fit the linear mixed model (LMM) ====where ==== and ==== are unknown regression parameters, ==== are unobserved independent and identically distributed ==== random variables with unknown variance ====, ==== are unobserved independent and identically distributed ==== random variables with unknown variance ====, and ==== are independent of ====. The model (1) describes the correlation structure in the response ==== as exchangeable within clusters; the responses ==== in different clusters are independent and the correlation between pairs of responses in the same cluster is ====. However, the model (1) conditions on ==== and ignores the fact that ==== is also clustered, often with the same correlation structure as ====. Does this matter and, if it does, what can be done about it?====To explore the effect of ignoring clustering in ====, we need to specify a data generating process for the clustered pairs ====. Let ====where ==== is the grand mean, ==== are independent ==== cluster random effects, and ==== are independent ==== individual random effects that are independent of the ====, ====, ====. We parameterise the covariance matrices as ====where ==== and ==== are between and within cluster regression coefficients. The within cluster variance of ==== is ==== and the within cluster correlation in ==== is ==== so there is clustering in ==== whenever ==== and the within cluster correlation tends to one as ====. Conditioning on the observed ==== values ==== leads to a conditional model that is equivalent to the contextual linear mixed model (CLMM) ====with ==== the cluster mean of ====, ==== unobserved independent and identically distributed (i.i.d) ==== random variables, ==== unobserved i.i.d ==== random variables, and ==== independent of ====, where ==== In the social sciences, ==== is called the contextual coefficient (e.g. Raudenbush and Willms, 1995). We assume ==== so that the contextual coefficient is zero for ==== and nonzero otherwise. Consequently, the CLMM (3) reduces to the LMM (1) when ====, but the two models are different and the LMM (1) is incompatible with the data generating process (2) when ====. The purpose of this paper is to relate the ML and restricted maximum likelihood (REML) estimators of the regression and variance parameters in the LMM to ==== and to explore what happens as ==== increases.====Scott and Holt (1982) considered the behaviour of the generalised least squared (GLS) estimator of ==== in the LMM (1) when the CLMM (3) holds with ====. They showed that ====, where ==== and ==== are the ordinary least squares estimators of the slopes in the within and between cluster regressions and ==== is a function of ====, ==== and the within cluster correlation in ====; see Section 4 for more details. Scott and Holt were not specifically concerned with the behaviour of ==== as a function of ==== (the misspecification can be interpreted as omitting ==== (called a group-level confounder, Berlin et al., 1999, Chao et al., 1997) from the CLMM (3) without discussing why it is needed in the model), their result implies that the GLS estimator is a smooth function of ====. It seems to be generally believed that this result also holds for the ML and REML estimators of ==== because these are interpreted as GLS estimators (see Raudenbush and Willms, 1995 p327; Baltagi, 2001) but, as we will show in this paper, this is incorrect and the behaviour of these estimators is different from that of GLS estimators. There are no results in the literature on estimating the variance components ==== and ==== (which are usually treated as known). A different approach adopted by Neuhaus and Jewell (1993) and Neuhaus and Kalbfleisch (1998) is to derive relationships between the regression parameters in the two models. However, this approach cannot explain why different estimators behave differently and we need to study the different estimators explicitly. More recently, Neuhaus and McCulloch (2006) considered problems in which the covariates and the random effects are correlated and Neuhaus and McCulloch (2014) considered problems with missing data. A recent overview of the problem is given by Enders (2013).====We consider the ML and REML estimators of both the regression and variance parameters in the LMM (1) when ==== increases from zero to infinity and obtain a number of completely new and unanticipated results. Although fitting the LMM (1) when ==== is a regular problem and the log-likelihood and the REML criterion function have a single maximum, we show that, as ==== increases, the log-likelihood and the REML criterion functions can have one or two maxima and, in the latter case, which of the two is the global maximum depends on ====. We show further that both the ML and REML estimators of the regression and variance parameters in (1) are discontinuous functions of ==== with a discrete jump at the value of ==== (which is different for the different estimators) when the global maximum of the log-likelihood or REML criterion function changes from one local maximum to the other. A particularly important practical finding is that standard statistical software for fitting linear mixed models can produce estimates that do not maximise the log-likelihood or REML criterion. We compute asymptotic mean squared errors (AMSEs) for the fitted values from different local maximisers and use these to compare the quality of the fit to the data by the different local and global maximisers of the log-likelihood and REML criterion functions. These calculations show that a local maximiser of the log-likelihood or REML criterion can fit the data better than its global counterpart and, moreover, that the ordinary least squares estimator can achieve better fit than either of these estimators.====We make two important choices in this paper. First, as noted by Scott and Holt (1982), it is difficult to get insight and understanding from general formulas, so we choose to focus on a specific simple case (represented by the LMM (1) under the data generating process (2) with equal size clusters (====)). This means that we can tease out explicit results and obtain a clear understanding of what drives these results. It also shows that the results hold in very simple cases and hence are important to all users of linear mixed models. Of course, the results and the insights derived from the special case can be generalised to more general cases, including unequal cluster sizes, multiple covariates and other correlation structures, but we do not consider these extensions in this paper. Second, we also choose the nonstandard approach of presenting our numerical results before our theoretical results. The main reason for doing this is that the numerical results motivate the theoretical work, but it is also the case that they convey the practical implications of this paper in an accessible way that should be understandable to practitioners who want to avoid the theoretical work. We therefore illustrate the effect of ignoring correlation in ==== when fitting linear mixed models to clustered data with simulated data in Section 2 and a real data set in Section 3. These sections convey the main findings of this paper in a direct and accessible way; they may suffice for applied readers. The numerical results in Sections 2 Numerical results, 3 Enzyme kinetics data are of course based on theoretical calculations and results, which give additional insight. We present calculations for ML and REML estimators in the special case of fitting the LMM (1) under the data generating process (2) in Section 4. In Section 5, we obtain formulas for AMSE of the local maximisers (and hence also the global maximisers) of the log-likelihood and the REML criterion. We first establish central limit theorems for local maximisers in a general problem of fitting misspecified linear mixed models and then use these results (which are of interest in their own right) to obtain formulas for AMSE of the local maximisers (and hence also the global maximisers) of the log-likelihood and the REML criterion in our problem. Finally, we present our conclusions in Section 6.",On the effect of ignoring correlation in the covariates when fitting linear mixed models,https://www.sciencedirect.com/science/article/pii/S0378375819300333,17 April 2019,2019,Research Article,295.0
"Grant Sheridan,Perlman Michael,Grant Darren","University of Washington, United States of America,Sam Houston State University, United States of America","Available online 18 December 2019, Version of Record 7 February 2020.",https://doi.org/10.1016/j.jspi.2019.12.001,Cited by (0),None,None,"Corrigendum to “Targeted testing for bias in order assignment, with an application to Texas election ballots” [J. Statist. Plann. Inference (2020) 12–28]",https://www.sciencedirect.com/science/article/pii/S0378375819301120,18 December 2019,2019,Research Article,299.0
"Li Qiyu,Molchanov Ilya","University of Bern, Institute of Mathematical Statistics and Actuarial Science, Sidlerstrasse 5, CH-3012 Bern, Switzerland,Swiss Group for Clinical Cancer Research (SAKK), Effingerstrasse 35, CH-3008 Bern, Switzerland","Received 11 May 2018, Revised 13 February 2019, Accepted 2 April 2019, Available online 11 April 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.04.003,Cited by (3),"We consider the partially identified regression model with set-identified responses, where the estimator is the set of the least ==== obtained for all possible choices of points sampled from set-identified observations. We address the issue of determining the optimal design for this case and show that, for objective functions mimicking those for several classical optimal designs, their set-identified analogues coincide with the optimal designs for point-identified real-valued responses.","Consider the basic regression model ====where the design points ==== belong to ==== called the ====, ====, ====, are observed real-valued responses, ==== is a vector of ==== unknown numerical parameters, and ==== are independent identically distributed (i.i.d.) centred random variables with variance ====. This setting includes the classical multivariate linear model, and also other models, like the quadratic one that appears if ====The basic problem in the theory of optimal design for regression models aims to identify the locations of design points ==== which ensure the best properties of the unbiased estimator ==== of ====. As the objective function to minimize, one can choose, e.g., the sum of the variances of the components of ==== (the criterion function for the ====-optimal design) or the largest variance of ==== over all unit vectors ==== (which yields the ====-optimal design). Further optimality criteria lead to a multitude of other optimal designs, see Atkinson et al. (2007) and Silvey (1980).====In this paper we consider the situation when the possibly multivariate response ==== is set-identified, so instead of observing ====, the statistician is only given sets ==== that contain the true observations. It is assumed that the specific points ==== are chosen by a completely unknown selection mechanism which is not a subject to statistical modelling. In this ==== setting, it is not possible to come up with a single-valued estimator for ====. We follow the approach advocated by Beresteanu and Molinari (2008) who suggested considering all possible points (selections) ====, ====, fitting to them the linear regression model in order to obtain particular (least squares) estimator ==== and, finally, use the set of all estimators ==== obtained in this way as the estimator for the set-identified regression, see also Molchanov and Molinari (2018). The most important special case arises if the observations ==== are intervals on the line; then one talks about interval regression, see also Blanco-Fernández et al., 2013, Diamond, 1990 for an alternative approach based on the interval arithmetics. The main reason of having interval-identified data is variability and uncertainty. For example, the temperature on a certain day is typically reported by weather forecasts as an interval between the lowest and the highest temperature. This interval represents the variability of the temperature. In social surveys, salaries of respondents are usually reported as intervals. Another example in the field of oncology is the time to recurrence of a tumour. The recurrence status of a patient is assessed by imaging techniques such as a CT scan at every visit, which is not scheduled every day but rather every two or three months. Therefore, we only know that recurrence occurs between two visits but not its exact time point. In this case, the data of time to recurrence are also interval-identified. In case of several interval-identified responses, the obtained multiple response is set-identified by a parallelepiped or its subset determined by the imposed constraints.====In this paper, we address the issue of optimal design in the partially identified least squares setting of Beresteanu and Molinari (2008). The crucial issue is to properly handle the variance of the estimated parameters; unlike the expectation, the variance of random sets is rather poorly understood, see Molchanov (2017).====In Section 2 we introduce the notation used throughout the paper and recall some definitions and results from random set theory. This is followed by Section 3, where we recall the classical ====-, ====- and ====-optimal designs with point-identified data. In Section 4, we introduce the objective functions for the set-identified setting and prove that the corresponding optimal designs coincide with the classical ====-, ====- and ====-optimal design under some assumptions on the model structure. As a corollary, we deduce that the ====-, ====-, and ====-optimal multiresponse designs in the multiresponse point-identified setting coincide with their classical analogues; this extends the result of Chang (1994) derived for ====-optimal designs.",Optimal design for multivariate multiple linear regression with set-identified response,https://www.sciencedirect.com/science/article/pii/S0378375819300357,11 April 2019,2019,Research Article,303.0
"Gaffke Norbert,Idais Osama,Schwabe Rainer","University of Magdeburg, Faculty of Mathematics, PF 4120, D-39016 Magdeburg, Germany","Received 26 April 2018, Revised 2 November 2018, Accepted 2 April 2019, Available online 10 April 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.04.002,Cited by (8),Gamma models with a power link function are considered constituting a particular class of ,"The main target of optimal design is to find a design that achieves most precise estimates of the model parameters. In case of a generalized linear model (GLM), the information matrix depends on the parameters through an intensity function. For specified values of the parameters locally optimal designs can be derived through maximizing a specific optimality criterion. The present paper addresses optimal design for generalized linear models with gamma distributed response variables. Gamma-distributed responses are continuous, non-negative and right-skewed. There are many applications where the generalized linear models for gamma-distributed responses are fitted. For example; in Subsection 8.4.1 of McCullagh and Nelder (1989) car insurance claims were fitted by a first order gamma model with natural link function. In medical context, a similar model was used to fit clotting times of blood in Subsection 8.4.2 in that book. A first order gamma model with natural link function was also applied to fit the reaction time that elders need to recognize words on a computer monitor (see Verkuilen et al. (2010), Section 2.3). A two-factor gamma model with interaction and natural link was used to fit car insurance data (see de Jong and Heller (2008), Section 8.1). An example for fitting a gamma model under unreplicated ==== factorial design using the log-link function was provided by Myers and Montgomery (1997).====In particular, locally D- and A-optimal designs are derived for various setups on the linear predictor including quantitative factors without or with interactions. We extend the results of Burridge and Sebastiani (1994) who derived locally D-optimal designs for a first order gamma model with an arbitrary number of factors. Related but different generalized linear models were treated by Haines et al. (2018) who derived locally D-optimal designs for a two-factor logit model with interaction, and by Ford et al. (1992) who identified locally D-optimal designs for a wide class of nonlinear models with one factor including the case of gamma distributions. Further work on generalized linear models with one factor was done by Mathew and Sinha (2001) who presented a unified approach for the derivation of locally D- and A-optimal designs for the logistic model. In Yang (2008) an algebraic method was introduced to construct locally A-optimal designs for a greater class of generalized linear models, and in Yang and Stufken (2009) results were obtained related to our notion of locally essentially complete classes of designs. The recent paper (Aminnejad and Jafari, 2017) also addressed gamma models and locally D- and A-optimal designs. However, their results are questionable or need an interpretation (see our remark in Section 4 following Theorem 4.1).====In Section 2 the specification of the present model is given and the notions of locally D- and A-optimal designs and of locally complete or essentially complete classes of designs are introduced. In Section 3 some relevant cases of linear predictors are considered and locally complete classes and locally essentially complete classes of designs are found leading to a considerable reduction of the problems of locally D- and A-optimal designs. Based on these results locally D- and A-optimal designs are determined in Sections 4 First order model, 5 Model with interactions. However, various conditions on the parameter point have to be imposed to describe the locally D- and A-optimal designs analytically. On the other hand, those conditions cover relevant subregions of the parameter space. So, our results on locally D- or A-optimality are applicable for the majority of possible parameter points. Finally, the performance of some derived locally D-optimal designs compared with particular non-optimal designs are examined in Section 6. In this paper, the numerical computations are conducted by making use of the software package ==== (see R Core Team (2018)).",Locally optimal designs for gamma models,https://www.sciencedirect.com/science/article/pii/S0378375819300345,10 April 2019,2019,Research Article,304.0
Hashimoto Shintaro,"Department of Mathematics, Hiroshima University, Hiroshima, Japan","Received 6 April 2018, Revised 18 December 2018, Accepted 26 March 2019, Available online 5 April 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.009,Cited by (4),This paper presents moment matching priors for non-regular models whose supports depend on an unknown parameter. Both one-parameter and multi-parameter cases are considered. The resulting priors are given by matching the posterior mean and bias-adjusted ==== up to the higher order. Some examples of proposed priors are also given.,"In Bayesian inference, the selection of priors has been an important and much discussed problem. When we have a little prior information or it is required the objectivity of data analysis, we often use ‘objective priors’ or ‘default priors’. Then we are often faced with a problem of the selection of an appropriate objective prior in a given context. For a regular family of distributions, the Jeffreys prior which is proportional to the positive square root of the Fisher information number is widely used as the objective prior. It is also known that the Jeffreys prior is invariant under smooth one-to-one transformation. Another class of objective priors is the reference prior, which was proposed by Bernardo (1979). The reference prior is defined by maximizing the expected Kullback–Leibler divergence between the prior and the posterior under some regularity conditions. The probability matching prior proposed by Welch and Peers (1963) is also known as an objective prior (see also Tibshirani (1989) and Datta and Mukarjee (2004)). These priors match the Bayesian credible intervals with the corresponding frequentist coverage probabilities, either exactly or approximately. Since the situations in which there exist exact probability matching priors are very limited, we often focus on approximating them based on the asymptotic theory of the maximum likelihood estimator under some regularity conditions. Recently, Ghosh and Liu (2011) derived the priors which are based on the moment matching criterion for regular one-parameter and multi-parameter family of distributions. Moment matching criterion leads the prior which is higher order matching of the moment of the posterior and the maximum likelihood estimator (MLE), and such priors are called the moment matching priors. Therefore the moment matching prior leads to the posterior mean which shares the asymptotic optimality of the MLE’s up to the higher order. As stated in Ghosh and Liu (2011), if one is interested in asymptotic bias or mean squared error reduction of the MLE’s through some adjustment, the same adjustment applies directly to the posterior means. In this sense, it is possible to achieve Bayesian-frequentist synthesis of point estimates. Interestingly, Ghosh and Liu (2011) showed that the moment matching prior is different from the Jeffreys or probability matching priors in regular cases.====However, these objective priors strongly depend on the regularity of statistical models and cannot be applied for non-regular distributions which do not satisfy regularity conditions such as the models with parameter-dependent supports. These non-regular models have appeared in many practical situations. For examples, the auction and search models in structural econometric models have a jump in the density and the jump is very informative about the parameters. In such non-regular cases, for example, the asymptotic normality of the posterior distribution does not hold. In non-regular cases, the reference priors in the sense of Bernardo (1979) were obtained by Ghosal and Samanta (1997a) for one-parameter case and by Ghosal (1997) for multi-parameter case in the presence of nuisance parameter. Ghosal (1999) derived the probability matching prior for both one-parameter and multi-parameter non-regular cases and made comparison with the corresponding reference priors. Furthermore, Wang and Sun (2012) derived the objective prior for another type of non-regular model.====In this paper, we deal with the same non-regular models as those of Ghosal (1999) and derive the moment matching priors which match the posterior mean and bias-adjusted MLE in such models. Both one-parameter and multi-parameter cases are considered. The resulting priors are given by solving certain differential equations. Further, we show some properties of the resulting prior and make comparison with the corresponding reference or probability matching priors for non-regular case.====This paper is organized as follows: In Section 2, we give the moment matching prior for one-parameter case by using the higher order asymptotic expression of posterior. In Section 3, we extend the result in Section 2 to multi-parameter case in the presence of nuisance parameter. In Section 4, we give some examples of the proposed priors.",Moment matching priors for non-regular models,https://www.sciencedirect.com/science/article/pii/S037837581930031X,5 April 2019,2019,Research Article,305.0
"Xiong Lanyu,Zhu Fukang","School of Mathematics, Jilin University, Changchun 130012, China","Received 28 January 2018, Revised 25 March 2019, Accepted 26 March 2019, Available online 3 April 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.010,Cited by (12),". Third, the performances of these ==== in the presence of transient shifts and additive outliers are investigated via simulations. We apply the robust estimator to two stock-market data sets and their prediction performances are assessed by in-sample and out-of-sample predictions.","Integer-valued time series models have received growing attention recently. In general, these models can be broadly classified into two categories: ‘thinning’ operator and regression type models. See Scotto et al. (2015) for a recent review for the former, Fokianos, 2012, Fokianos, 2016 and Tjøstheim, 2012, Tjøstheim, 2016 for comprehensive summaries about recent progress for the latter. As the non-negativeness and the presence of the positive correlation between the observed data, count time series often display overdispersion, which means its variability exceeds the mean. For this problem, a commonly used model is the integer-valued generalized autoregressive conditional heteroscedastic (INGARCH) model with Poisson deviates proposed by Ferland et al. (2006), which is defined as follows ====for ====. ==== is an intercept, ====, ====, are non-negative regression parameters and ==== stands for the ====-field generated by ==== representing the whole information up to time ====. Many authors have studied this model, see Fokianos et al. (2009), Neumann (2011) and Doukhan et al. (2012), among others. However, the Poisson distribution is not usually suitable as the conditional variance equals the conditional mean, which can lead to poor performances in the existence of potential extreme observations. To solve this problem, Zhu (2011) proposed a negative binomial (NB) INGARCH model that can deal with both overdispersion and potential extreme observations simultaneously. The negative binomial distribution, a natural extension of the Poisson distribution, is quite flexible and allows for overdispersion. The NB-INGARCH(1,1) process ==== proposed by Zhu (2011) is given through the relationships ====where ==== is a positive number, ==== satisfies ====and the probability mass function (pmf) of ==== conditionally on the past has the form ====for ====, ====, ==== with ====.====Obviously, it is easy to see that there is no method to estimate both regression parameters and ==== together, so Zhu (2011) used a two-stage method and assumed ==== was known in the first stage. A main reason for this is that ==== is really a discrete-valued parameter and the differentiation with respect to ==== is problematic. As Zhu (2011) suggested, a possible solution to this problem is to define the NB-INGARCH(1,1) in the following form ====Latter, Christou and Fokianos (2014) studied probabilistic properties and quasi-likelihood estimation for this process based on distribution (1.3). Here, we consider another parameterized NB-INGARCH(1,1) model as follows ====which yields the following pmf ====This parameterization is based on Lawless (1987) and denoted as “NB2” by Hilbe (2011). Here ==== is positive and is not necessarily an integer, from model (1.4) we have ====, ====, thus ==== is referred to as the dispersion parameter. We chose to use this parameterization over the other common one involving ==== applied in (1.3) for the reason similar to Aeberhard et al. (2014). That is this parameterization has the advantage of greater numerical stability in the estimation of the dispersion parameter. Moreover, the other advantage of model (1.4) over model (1.1) is that the conditional mean of this model is not dependent on the positive integer-value ====.====The maximum likelihood approach is the commonly used method to estimate parameters, but it is highly affected by interventions, which lead to this method cannot estimate parameters satisfactorily. There are three common forms of interventions: transient shifts (TSs), this type of interventions yields a sudden effect to the observation and eventually decays; level shifts (LSs), this intervention changes the mean of the process; and additive outliers (AOs), can be viewed as measurement errors and exert an effect on the level of the series but not on the evolution of the underlying volatility, as pointed out by Bahamonde and Veiga (2016) and Kitromilidon and Fokianos (2016a). Taking into account, it is necessary for us to resort to some robust estimation methods and several robust estimation procedures have been suggested by Künsch et al. (1989), Bergesio and Yohai (2011) and Valdora and Yohai (2014), among others. In this study, we turn to the Mallows’ quasi-likelihood estimator (MQLE) defined by Cantoni and Ronchetti (2001) to achieve robustness because it is based on a generalization of quasi-likelihood function which can make the resulting estimators attractive. Besides, its performance is investigated under the intervention of TSs and AOs, respectively.====Studies on robust estimation for GARCH models have been advancing over the last years, see Muler and Yohai (2008) and Mukherjee (2008). Recently, Elsaied and Fried (2014) employed an iterative procedure to obtain robust M-estimators for INARCH model, Kitromilidon and Fokianos (2016a) studied robust versions of maximum likelihood estimator (MLE) under three different forms of interventions, Li et al. (2016) robustified the closed-form moment estimators to deal with outliers in INGARCH(1,1) models, and Kim and Lee (2017) considered a robust estimation for zero-inflated Poisson autoregressive models using the minimum density power divergence estimator.====The remainder of this article is organized as follows. In Section 2 we introduce some probabilistic properties under the framework of Christou and Fokianos (2014) and give the MLE of model (1.4). Section 3 estimates the parameters using a robust method. Moreover, we establish the consistency and asymptotic normality of the resulting robust estimators. For transient shifts and additive outliers, some results from a simulation study are presented in Section 4. Section 5 illustrates the performances of the proposed estimators by two empirical applications, and finally, Section 6 summarizes.","Robust quasi-likelihood estimation for the negative binomial integer-valued GARCH(1,1) model with an application to transaction counts",https://www.sciencedirect.com/science/article/pii/S0378375819300321,3 April 2019,2019,Research Article,306.0
"Lagos-Álvarez Bernardo,Padilla Leonardo,Mateu Jorge,Ferreira Guillermo","Department of Statistics, University of Concepción, Chile,Department of Mathematics, University Jaume I, Castellón, Spain","Received 19 December 2017, Revised 23 October 2018, Accepted 14 March 2019, Available online 29 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.005,Cited by (13)," in terms of both parameter estimation and prediction at unobserved locations. We put in relevance the nugget effect at the observation equation. We test our procedure and compare it with classical kriging prediction via an intensive simulation study. We show that the Kalman filter is superior in both the estimation, without using a plug-in approach, and prediction for spatio-temporal data, providing a suitable formal procedure for the statistical analysis of space–time data. Finally, an application to the prediction of daily air temperature data in some regions of southern Chile is presented.","During the last decades, the Kalman filter (==== henceforth) algorithm has proved to be a powerful tool for the statistical treatment of state-space models, providing the estimation of parameters (given in the state vector), and the prediction of unobserved values both in time and space. The ==== algorithm is based on the state-space representation of a linear process. This can be described by two equations, the first is called the ==== which is a linear combination of a vector of unobservable state variables and some process noise, whereas the second is called the ==== which is formed by means of a linear relationship with the state variables and a random noise. Once the state-space system is defined, the ==== can be used to estimate the state vector, estimating the model parameters, and classically to build the one-step-ahead predictor of the process and its mean square error matrix. The idea of developing techniques for the estimation of an unobserved state from the observed process goes back to the sixties. For example, Kalman (1960) published his famous paper describing a recursive solution to the discrete-data linear filtering problem. A friendly introduction to the general idea of the ==== algorithm is offered in Maybeck (1979) and in Meinhold and Singpurwalla (1983). More extensive references include Brockwell and Davis (1991), Harvey (1992), Durbin and Koopman (2001), Grewal et al. (2007) and Grewal (2011).====As mentioned, the ==== is one of the fundamental algorithms for the statistical treatment of classical time series models. For more complex time series models, there are also variants of the ====, see e.g. Naveau et al. (2005), Ferreira et al. (2013), Grassi and de Magistris (2014), Rezaie and Eidsvik (2014) and Rehman et al. (2016). This favorable evidence makes it a natural candidate for performing statistical inferences for space–time processes. In this context, the space–time ==== has been implemented to perform inference and prediction for processes in the physical, environmental, and biological sciences, among others.  Huang and Cressie (1996) were perhaps one of the first to use the ==== for the prediction of locations that have greater storage of snow water in southwest Colorado. Hughes et al. (1999) used hidden Markov models with unobserved weather states to model space–time atmospheric precipitation. Wikle (2003) developed empirical Bayesian space–time ==== models for monthly precipitation. An application of ==== to electroencephalographic source locations is proposed by Barton et al. (2009). The above applications have been performed with a limited number of observations or by using a plug-in method to estimate the parameters. Xu and Wikle (2007) proposed a space–time dynamic model formulation with parameter matrices restricted to prior scientific knowledge and/or to common spatial models. The estimation can be carried out via the expectation–maximization (EM) algorithm or by a general EM algorithm, as done by Amisigo and Van De Giesen (2005), Fassò and Cameletti (2009) and Katzfuss and Cressie (2011). An approach to nonlinear state estimation, known as the ensemble ====, has been discussed for example by Anderson (2001), Gillijns et al. (2006) and Stroud et al. (2010), among others. Mardia et al. (1998) established a connection between the ==== algorithm and the kriging methodology (named as kriged Kalman Filter), in which the state equation incorporates different forms of temporal dynamics to model space–time interactions.====In this paper we devote particular attention to compare the performance of spatial predictors, between the predictor, generated by the ==== applied to our proposed model, which uses all available information up to time ====, and the more classical kriging prediction methodology that, unlike ====, uses only the information associated to the current time ====. The particular model in space-state form we here propose places an error component which includes both the measurement error and the small-scale error inside the observation equation. This allows to model another source of variability, the innovation, which explains the spatial structure of the process. Our method facilitates the computation, at the time of estimation (from the convergence point of view), without sacrificing the desired variability in predictions. Moreover, we provide a brief overview of this important field of research in space–time analysis, discussing estimation and prediction techniques as well as applications to real-life data. In the context of parameter estimation, we discuss the use of maximum likelihood estimators (MLE) through the state-space systems.====The remaining of this article is structured as follows. Section 2 presents the class of space–time processes that we are working with comparing it with existing and alternative classes. Section 3 presents the Kalman filter and the corresponding estimation procedure for our state-space representations. Then Section 4 is devoted to present a simulation study to evaluate the performance of the estimation and prediction methods. Section 5 presents an application to predict air temperature data in Chile. The paper ends with some conclusions in Section 6.",A Kalman filter method for estimation and prediction of space–time data with an autoregressive structure,https://www.sciencedirect.com/science/article/pii/S0378375819300278,29 March 2019,2019,Research Article,307.0
"Barati Fahimeh,Talebi Hooshang","Department of Statistics, University of Isfahan, Isfahan, Iran","Received 2 May 2018, Revised 14 March 2019, Accepted 17 March 2019, Available online 26 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.007,Cited by (1),"MEP and MEP.==== have been designated for ==== and ==== runs, respectively, to estimate all main effects, while the sparsity principle implies that only a small number of factors are active. A supersaturated design is superior in this regard because it focuses on active factors and saves runs. In this paper, we consider the problem of searching for and estimating ==== and ==== non-zero main and 2-factor interaction effects, respectively, that are not known ====. A family of designs has been constructed and given for ==== runs for ====, ==== and ====. These designs are able to estimate all possible models consisting any set of at most 4 main and 2 2-factor interaction effects. It is shown that the obtained designs are near D-optimal.","Main effect plans (MEP) are used in screening stage of factorial experiments to screen the active factors under the absence of interaction effects. Meanwhile, the presence of any non-zero interaction may mislead the detection of significant factors, i.e. a significant interaction may hide an important main effect or a non-significant main effect may appear active see Phoa et al., 2009 and pp 424–426 Wu and Hamada, 2009. A way to avoid such a misleading detection is simultaneous estimation of main effects (MEs) and lower order interactions, using the non-economical higher resolution designs, e.g. ==== or ====. However, Srivastava (1975) considered the sparsity principle in regard to the small number of non-zero interactions, ====, and introduced the search design (SD). An SD enables experimenters to search for and estimate lower order interactions in addition to estimating MEs; such an SD is called main effect plus ==== plan (MEP.====) see Ghosh et al., 2007. Employing MEP.==== saves runs in comparison to non-economic high resolution plans. It is crucial search for an economical design that is able to simultaneously identify the active factors and lower order interactions in the screening stage of an experiment. In this study, constructions of such designs with a reasonable number of runs in the context of search designs are investigated.====Let====In this paper, we provide the theoretical basis for construction of a design that simultaneously searches for and estimates the non-zero factorial effects in ==== and ==== in addition to estimating the general mean for a ==== factorial experiment. Such a design is called a search design with resolvability ====, where ==== is an integer so that all ====- and higher order interactions are in the set ====. This is an extension of ====-resolvability given by Srivastava (1975).====The paper is organized as follows. In Section 2, the theoretical perspective of the search design and its extended resolvability property will be addressed. Furthermore, we will describe the search procedure and model identification. In Section 3, a family of search designs with resolvability ==== that we have obtained are given. The main theorem, which establishes the resolvability of obtained designs, will be presented in Section 4. The concluding remarks are in Section 5. To enhance the readability of the article, the proof of propositions and theorems are moved to Appendix.",Families of search designs for simultaneous detection of the active main and 2-factor interaction effects,https://www.sciencedirect.com/science/article/pii/S0378375819300291,26 March 2019,2019,Research Article,308.0
"Hyun Noorie,Couper David J.,Zeng Donglin","Biostatistics, Institute of Health and Equity, Medical College of Wisconsin, Milwaukee, Wisconsin, United States,Biostatistics, School of Public Health, University of North Carolina at Chapel Hill, Chapel Hill, North Carolina, United States","Received 31 January 2018, Revised 10 September 2018, Accepted 17 March 2019, Available online 21 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.008,Cited by (2), of the proposed estimators and summarize simulation results to assess the numerical performance of the proposed method. The method is illustrated through an application to data from a diabetes ancillary study to the Atherosclerosis Risk in Communities (ARIC) Study.,"Right- or left-skewed continuous biomarker measurements are often confronted in practice. For analyzing such skewed data, data transformation for approximating to a normal distribution or an application of generalized linear models with a gamma distribution can be commonly used approaches. However, for the case that data is seriously skewed, and extreme outliers are of interest, modeling based on skewed distributions has been proposed and studied in multiple fields, such as climatology, environmental science, biomedicine, and finance. For instance, risk analysis identifying a subgroup at high risk or analysis of extreme events occurring with small probability, such as trends in annual high or low temperatures has used extreme value theory.====Fisher and Tippett (1928) first identified the extreme value limit distribution and Gnedenko (1943) provided the asymptotic proof for the limit distribution of extreme values. The corresponding three types of distributions, Gumbel, Fréchet and Weibull are unified into a single generalized extreme value (GEV)-form (von Mises, 1936, Gumbel, 1958). An alternative approach to extreme events is based on exceedances above thresholds leading to the generalized pareto distribution (Balkema and De Haan, 1974, Pickands, 1975); however, the threshold methods are not covered in this paper. Huerta and Sansó (2007) modeled extreme values using GEV distributions with parameters varying in time. A link function based on GEV distributions was introduced for modeling imbalanced binary and ordinary responses (Wang and Dey, 2010, Roy and Dey, 2014). Ghosh and Mallick (2011) proposed Bayesian hierarchical regression models for repeatedly measured continuous outcomes, which are marginally generated from a GEV distribution. Using the power-transformed quantile regression, Wang and Li (2013) developed a new three-stage estimation procedure, estimating intermediate conditional quantiles and extrapolating the estimates to tails. However, we believe that there has not been a study involving modeling continuous biomarker data based on Gumbel distributions, which are GEV distributions with shape parameter ====, while adjusting for a monotone age effect on biomarker values.====Furthermore, most biomarkers suffer from at least some measurement error: every assay has some inherent variability, so if the assay is run twice on a sample from an individual, the results may not be identical. Additionally, even though there may be a smooth underlying trend in an individual’s levels of the biomarker, there is likely to be short-term intra-individual variability, resulting in variation around that underlying trend. Some biases may be more likely in a particular direction, such as with “white-coat hypertension”, whereas others are essentially random. In clinical practice, ==== approaches that are used to take into account biomarker variability include taking two or more measurements over a period of time.====We propose new regression models for investigating associations between exposures and continuous biomarker while addressing abnormally high values and measurement error in a biomarker. We consider a monotone increasing biomarker over time (particularly age) and incorporate a function of time in the model. The proposed method involves a semi-parametric likelihood approach based on a mixture distribution of a normal and Gumbel distribution. We propose an efficient estimator for the model parameters based on nonparametric maximum likelihood estimation. In Section 2, inference procedures using the expectation–maximization algorithm for parameter estimation are presented, and variance estimation is also proposed. Asymptotic results based on the proposed model are established in Section 3. The detailed proofs are provided in the Supplementary material. A simulation study and an application to real data are illustrated in Section 4. Finally, in Section 5 we discuss limitations of our approach and related future research problems.",Gumbel regression models for a monotone increasing continuous biomarker subject to measurement error,https://www.sciencedirect.com/science/article/pii/S0378375819300308,21 March 2019,2019,Research Article,309.0
"Nakakita Shogo H.,Uchida Masayuki","Graduate School of Engineering Science, Osaka University, Japan,Center for Mathematical Modeling and Data Science, Osaka University, Japan","Received 10 April 2018, Revised 14 March 2019, Accepted 14 March 2019, Available online 21 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.006,Cited by (7),"We propose some ==== for an ergodic diffusion-plus-noise model, which is a version of state-space modelling in ==== for stochastic ","Our study is concerned with the ====-dimensional diffusion process as a solution for the following stochastic differential equation: ====where ==== is the ====-dimensional Wiener process, ==== is an ====-valued random variable independent of ==== such that ====, ====, ====, ====, ==== are the compact and convex parameter spaces, ==== and ==== are known functions. We assume that the true value of parameter ==== belongs to ====.====We set the observational scheme as that by Nakakita and Uchida, 2017, Nakakita and Uchida, 2018 such that for ====, ====where ==== is the discretisation step satisfying ==== and ====, ==== is a positive semi-definite matrix, and ==== is an independent and identically distributed (i.i.d.) sequence of random variables such that ====, ====, and each component is independent of other components as well as ==== and ====. Let us define ==== the compact and convex parameter space of ====, which is half-vectorisation of ====, ====, and ====.====The statistical framework for the analysis of time series data has been mainly based on discrete-time stochastic processes such as ARMA model (e.g., see Brockwell and Davis, 1991). Those discrete-time models confront with some difficulties to express complex phenomena such that innovation term whose variance is dependent on state ==== itself. One of the solutions for those difficulties is the modelling with stochastic differential equations, which flexibly describes the probabilistic perturbation dependent on ====. The parametric inference for diffusion processes modelled with stochastic differential equations has been researched enthusiastically (e.g., see Florens-Zmirou, 1989, Yoshida, 1992, Yoshida, 2011, Bibby and Sørensen, 1995, Kessler, 1995, Kessler, 1997, Iacus, 2008, Uchida and Yoshida, 2012, Uchida and Yoshida, 2014).====It is well known that parametric estimation for one model is not sufficient in the context of real data analysis; thus, we need a methodology to compare multiple parametric models in terms of goodness-of-fit (for i.i.d. case, see Ferguson, 1996, Lehmann and Romano, 2005). Kitagawa and Uchida (2014) proposed a likelihood-ratio-type test statistic for discretely observed ergodic diffusions to examine parametric hypotheses such as that by Ferguson (1996) and showed the convergence in law of the test statistic and the consistency of the test (with respect to statistical test for diffusion processes with discrete observation, see also De Gregorio and Iacus, 2010, De Gregorio and Iacus, 2013, De Gregorio and Iacus, 2018). As a different approach, Uchida (2010) studies contrast-based information criterion for ergodic diffusion processes with discretised observation scheme (see also Fujii and Uchida, 2014, Eguchi and Masuda, 2018).====The classical time series analysis itself also has instruments of complex modelling such as state-space model (see Brockwell and Davis, 1991). One simple version of state-space modelling decomposes the randomness of observation into endogenous perturbation of the system of interest and exogenous noise that contaminates only observation and does not influence the system itself. The importance of this decomposition has attracted attention in the research not only of time series analysis but also that of statistics for stochastic differential equations. For instance, the existence of observation noise in high-frequency financial data called microstructure noise is one of the major research topics in financial econometrics. Gloter and Jacod, 2001a, Gloter and Jacod, 2001b and Jacod et al. (2009) studied the diffusion with noise contaminating observation in the observation framework such that ==== is fixed. The statistics for diffusion-plus-noise with the setting ==== has been also studied, e.g., by Favetto, 2014, Favetto, 2016, and Nakakita and Uchida, 2017, Nakakita and Uchida, 2018. Favetto (2014) proposes the consistent estimator for the variance of noise and parameter of the diffusion process and Favetto (2016) constructs the estimator for the parameter of the diffusion with asymptotic normality when the variance of noise is known. Nakakita and Uchida, 2017, Nakakita and Uchida, 2018 provide the estimator for the parameter of the diffusion process and as well as for the variance of noise with asymptotic normality when the variance of noise is unknown. However, as discussed above, it is necessary to construct the way to compare the goodness-of-fit of candidate models in practice, and in this study, we try to achieve it with likelihood-ratio-type test statistics with quasi-likelihood-functions proposed by Nakakita and Uchida (2018) in the same manner as that of Kitagawa and Uchida (2014) discussing the same problem under the assumption that exogenous noise in observation does not exist.====We also analyse some real data with our methods besides theoretical construction of test statistics. Our data of interest is MetData (NWTC Information Portal, 2018) which represents wind velocity with high-frequency observation. Nakakita and Uchida, 2017, Nakakita and Uchida, 2018 illustrate the relationship between latent processes without noise and observation processes with noise and examine existence of noise in some partial data (the plot is shown in Fig. 1) in MetData and show statistical significance of the existence, which indicates the motivation for using diffusion-plus-noise modelling rather than diffusion modelling without observation noise. We also use the same data and see if the diffusion coefficient ==== is dependent on ==== or not and check if we are motivated to utilise the flexible modelling of stochastic diffusion equations.====The rest of this paper is organised into four parts: firstly, our assumption and notation are shown before discussing concrete statements for parametric tests in Section 2. Next, we state some theorems in Section 3 which show the asymptotic behaviour of adaptive likelihood-ratio-type test statistics, whose proofs are shown later. Furthermore, Section 4 examines the behaviour of statistics proposed in the previous section with computational simulation for 1-dimensional diffusion processes and 2-dimensional ones. Finally, in Section 5, MetData – real data for wind with high-frequency observation is used to see what our method concludes regarding the property of wind observed in data.",Adaptive test for ergodic diffusions plus noise,https://www.sciencedirect.com/science/article/pii/S037837581930028X,21 March 2019,2019,Research Article,310.0
"Radloff Martin,Schwabe Rainer","Institute for Mathematical Stochastics, Otto-von-Guericke-University, PF 4120, 39016 Magdeburg, Germany","Received 25 July 2018, Revised 28 February 2019, Accepted 11 March 2019, Available online 19 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.004,Cited by (3),In this paper we construct (locally) ,"Optimal designs on spherical design regions have been discussed by various authors. Among the first were Kiefer (1961) and Farrell et al. (1967) who considered polynomial regression on the ball. In our opinion there is a big amount of practical real-life problems for example in engineering or physics where the phenomenon has to be examined and evaluated only in a ball-shaped neighborhood of its occurrence or where the validity of a model may be assumed on a spherical region around a target value. So it is obvious that models on spherical design spaces – here the design region is the ====-dimensional unit ball – are important and worth to be investigated.====Our object of study is a wide class of non-linear multiple regression models on the mentioned design space. Here we want to find an optimal design, which means to find optimal settings of control variables, with respect to the ====-criterion. We will use results for equivariance and invariance in Radloff and Schwabe (2016). So it is possible to reduce this multidimensional problem to a one-dimensional marginal problem — this concept of reducing dimensions but for invariance with respect to permutations of qualitative factors can be found, for example, in Schwabe (1996, Section 6.2). One-dimensional models belonging to our class of models were investigated, for example, in Konstantinou et al. (2014).====The result for multiple regression models on a ball shaped design region but for the linear case is well-known, see, for example, in Pukelsheim (1993, Section 15.12), and will be revisited in Section 3.====Schmidt and Schwabe (2017) considered the same class of models with ==== covariates, but on a ====-dimensional cuboid. They found a way to divide this problem into ==== marginal sub-problems with only one covariate in the form like Konstantinou et al. (2014). However, our approach is completely different. Instead of directly decomposing the information matrix we first reduce the design problem by symmetrization to a one-dimensional one and then solve the resulting marginal problem. Our main result for non-linear models can be found in Section 4.====In Section 5 we will discuss some examples. In the case of Poisson regression we will have an explicit formula to determine such an optimal design. In the case of negative binomial regression and censoring data models some numerical results are obtained.====In the final Section 6 we have a short look at the generalization to an arbitrary ellipsoidal design region.",Locally ,https://www.sciencedirect.com/science/article/pii/S0378375818301502,19 March 2019,2019,Research Article,311.0
"Amei Amei,Xu Jianbo","Department of Mathematical Sciences, University of Nevada, Las Vegas, United States,DataX, Ltd, Las Vegas, Nevada, United States","Received 2 September 2018, Revised 18 February 2019, Accepted 21 February 2019, Available online 18 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.02.005,Cited by (1),"Aligned DNA sequences have been widely used for quantitatively analyzing and interpreting evolutionary processes. By comparing the information between intraspecific polymorphism with interspecific divergence in two sibling species, Poisson random field (PRF) theory offers a statistical framework with which various genetic parameters such as natural selection intensity, mutation rate and speciation time can be effectively estimated. A recently developed time-inhomogeneous PRF model has reinforced the original method by removing the assumption of stationary site frequency, but it keeps the assumption that the two sibling species share same effective population size with their ancestral species. This paper explores a relaxation of this biologically unrealistic assumption by hypothesizing that each of the two descendant species experienced a sudden change in population size at the times of the divergence from their most recent common ancestor. The newly developed PRF model with non-constant population size is applied to a set of 91 genes in African population of ==== program is integrated with ==== code and a parallel executing technique is designed to run the program with multiple CPU cores.","A goal of population genetics is to identify and quantify the role of genetic forces such as mutation, selection, and demographic forces in an evolution history of certain genes in a population. Statistical inference using Poisson random field (PRF) models provides powerful likelihood and Bayesian methods for quantifying these forces. At a single genetic locus, Sawyer and Hartl first proposed such a model to estimate the selective effects by using the numbers of site polymorphisms between and within two closely related species (Akashi, 1999, Bustamante et al., 2001, Hartl et al., 1994, Li, 1997, Sawyer and Hartl, 1992, Sawyer, 1994). The extensions of the model to large numbers of loci are shown to be well suited for the analysis of polymorphism and divergence across a genome (Baines et al., 2008, Boyko et al., 2008, Bustamante et al., 2002, Williamson et al., 2005, Sawyer et al., 2003, Sawyer et al., 2007, Amei and Sawyer, 2010, Amei and Sawyer, 2012, Amei and Smith, 2014, Amei et al., 2014).====Although the original PRF model provides a powerful and attractive approach to analyzing and interpreting DNA site polymorphism within and between species, there still exist potential improvements of the model by relaxing or even removing some biologically unrealistic assumptions. One common criticism on the model is that alleles at different loci are assumed at linkage equilibrium or a high level of recombination, which translates into the independence of nucleotide sites. For mathematical simplicity, the distribution of site frequency spectrum is assumed to be stationary after the divergence of two species and thus the model is called a time-independent or time-homogeneous PRF model. Also, the selective effects of replacement mutations within any particular gene is assumed to be fixed and only varies from one gene to another and such a model is called fixed effects model. The original formulation also stipulates that the effective population sizes of the ancestral and daughter species are the same.====Many authors have put effort on refining the basic PRF model with more general biological settings (Bustamante et al., 2001, Boyko et al., 2008, Williamson et al., 2005, Sawyer et al., 2007, Abel, 2009, Maximum likelihood and bayesian, 2003, Huerta-Sanchez et al., 2008, Wakeley, 2003, Williamson et al., 2004, Zhu and Bustamante, 2005). To mention a few, since the likelihood ratio test derived from the original PRF model (Hartl et al., 1994) is not robust to the assumption of linkage equilibrium (Bustamante et al., 2001) Zhu and Bustamante proposed a composite likelihood method to correct for the bias in estimates using simulations with a specified recombination rate (Zhu and Bustamante, 2005). To relax the fixed effect assumption, Sawyer et al. proposed a random effect model in which the selective intensity of arising mutations within a genetic locus was assumed to have Gaussian distribution with mean (but not variance) changing between genes (Sawyer et al., 2003). This random effect model was applied to 91 genes in two ==== species, ==== and ==== to estimate quantitatively the distribution of selective effects among loci and the fraction of amino acid replacement between species that are selectively neutral or nearly neutral (Sawyer et al., 2007). Simulation studies have shown that the time-homogenous models tend to overestimate the speciation time (Abel, 2009) and later, a time-dependent PRF model was developed by explicitly accounting for the time since the divergence of the two species into the model using the method of diffusion approximation to discrete time discrete state Markov chains (Amei and Sawyer, 2010, Amei and Sawyer, 2012).====However, all models mentioned above ignored the change in effective population size over evolution process and it can cause a confounding factor in inferring natural selection on DNA polymorphism (Fay et al., 2002, Eyre-Walker, 2002, Begun et al., 2007, Keightley and Eyre-Walker, 2007). Williamson et al. presented a population size change model to simultaneously make statistical inference of selection and demographic factors (Williamson et al., 2005). Their model assumes that a species experienced an abrupt change from the ancestral population size to current size at some moment during the evolution. A maximum likelihood approach incorporating the ratio of the two population sizes were applied to a large dataset of 301 sequenced human genes collected from 90 individuals and the results revealed strong evidence of population expansion under common negative selection on replacement mutations. Boyko et al. extended Williamson’s model to infer the distribution of fitness effects given a non-stationary demographic history (Boyko et al., 2008). Their studies are based on site frequency spectrum data from single population and simulation results have shown that site frequency spectrum polymorphism data may generate strongly biased estimates of selection parameters even for minor deviation from the model assumption of genic selection (Williamson et al., 2004). Gutenkunst et al. proposed a diffusion based method to compare different demographic models based on the joint distribution of allele frequencies in multiple populations, but their model did not make inference about other genetic parameters such as mutation rate and selection coefficient (Gutenkunst et al., 2009).====In this paper, we explore a modified time-dependent Poisson random fieldmodel to depict the DNA site polymorphisms within and between two closely related species while accounting for the differences in population sizes. The resulting non-constant population size time-inhomogeneous PRF model and its sampling formulas for multi-loci data are implemented in a hierarchical Bayesian framework. Numerical evaluation of the diffusion equations involved in the model is computational intense and we developed an efficient parallel scheme by linking ==== code with ====. Results from simulation studies and application of the model to a set of 91 genes in African population of ==== are presented and discussed.",Inference of genetic forces using a Poisson random field model with non-constant population size,https://www.sciencedirect.com/science/article/pii/S0378375818302428,18 March 2019,2019,Research Article,312.0
Chang Ming-Chung,"Graduate Institute of Statistics, National Central University, Taiwan","Received 28 January 2018, Revised 28 February 2019, Accepted 7 March 2019, Available online 15 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.002,Cited by (0),"Given limited resources for conducting follow-up trials, the inability to separate aliased ==== effects hinders the ubiquitous practicality of regular ====, we show that our method can yield desirable model fittings and reliable de-aliasing results.","Two-level fractional factorial designs are widely used in industrial investigations owing to their run-size economy. In general, a factorial design is called ==== if it can be constructed via defining equations and ==== otherwise; see Wu and Hamada (2009) for a comprehensive introduction. Although implementing nonregular designs offers the advantages of run-size flexibility and accommodation of nonorthogonal models among others (Xu et al., 2009), regular designs remain attractive to experimenters owing to their elegant design structures, which facilitate data analysis. For example, it is convenient to use a half-normal plot or Lenth’s method (Lenth, 1989) to determine significant factorial effects without adopting any other model selection techniques.====Two factorial effects in a regular design are either (fully) aliased or orthogonal to each other. Consequently, if an effect that has a significant impact on the response is aliased with another effect in a regular design, then de-aliasing these effects is impossible without further assumptions, domain knowledge, or additional information from follow-up experiments. A commonly used assumption is the ==== (Wu and Hamada, 2009 p. 172). If this assumption holds, three-factor and higher-order interactions are often regarded as negligible and only two-factor interactions need to be de-aliased from the main effects.====Wu (2015) proposed the concept of using ==== for de-aliasing: a pair of aliased two-factor interactions is transformed into relevant conditional main effects and model selection is then performed to select significant effects. Accordingly, a real dataset named ==== was successfully analyzed by Wu (2015). Inspired by Wu (2015), and Su and Wu (2017) developed a systematic de-aliasing method by restricting the fitted models to be orthogonal.====Effect aliasing makes it difficult or even impossible to understand a system. Hence, it is crucial to correctly identify, or at least not to miss any truly active effects, when the purpose of experimentation is to explain the mechanism of a process. In this regard, the approach of Su and Wu (2017) may lead to incorrect interpretations, as will be shown in this paper. Moreover, frequentist approaches always have some limitations in resolving de-aliasing problems owing to multicollinearity. Our objectives are to propose a Bayesian de-aliasing strategy for regular designs by adopting the Bayesian framework proposed by Mitchell et al. (1995), and to show through numerical studies that this method can yield desirable model fittings and reliable de-aliasing results.====In addition to de-aliasing, domain knowledge sometimes suggests the use of conditional main effects for modeling underlying systems, such as sliding level experiments (Hamada and Wu, 1995). Mak and Wu (2018) introduced a novel method for selecting main effects and a set of conditional main effects. Mukerjee et al. (2017) addressed the problem of designing efficient experiments under models with general conditional factorial effects. The strategy proposed in this paper can be applied to such experiments. We present a brief discussion on this application.====The remainder of this paper is organized as follows. Section 2 provides a brief introduction to conditional main effects and presents three motivating examples. Section 3 describes the proposed Bayesian de-aliasing strategy as well as a simulation study on its performance. In addition, the three above-mentioned motivating examples are revisited. Finally, Section 4 concludes this paper. Supplementary material, including some theoretical/numerical details, is also provided.",De-aliasing in two-level factorial designs: A Bayesian approach,https://www.sciencedirect.com/science/article/pii/S0378375819300254,15 March 2019,2019,Research Article,313.0
Aoki Satoshi,"Graduate School of Science, Faculty of Science, Kobe University, Japan","Received 26 October 2018, Revised 7 March 2019, Accepted 8 March 2019, Available online 15 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.003,Cited by (3), designs with strength 2 and orthogonal ==== designs with strength 3 by a computational algebraic software.,"Applications of Gröbner basis theory to various problems of statistics arises in early 1990s. One of the first works in this developing field, a computational algebraic statistics, is given by Pistone and Wynn (1996), where the Gröbner basis theory is applied to the identifiability problem in the design of experiments. After this work, various algebraic approaches to the problems in the design of experiments are presented by researchers both in the fields of algebra and statistics. A theory of the indicator function of fractional factorial designs is one of the early results in this branch.====The indicator function is first introduced by Fontana et al. (2000) for two-level fractional factorial designs. In Fontana et al. (2000), based on the results of Pistone and Wynn (1996), one-to-one correspondence between the design and its indicator function is shown. This correspondence enables us to translate various statistical concepts to algebraic concepts, i.e., various results on the fractional factorial designs can be interpreted to the structure of their indicator functions. For example, aberration and resolution are important concepts in design of experiments, and there is a well-established history starting with Box and Hunter (1961) for two-level fractional factorial designs. An important contribution of Fontana et al. (2000) is to characterize these concepts as the structure of the indicator functions.====To illustrate the motivation of this paper, we glance at the arguments of Fontana et al. (2000) by examples. Note that the necessary definitions on the designs and indicator functions will be given in Section 2. Fig. 1 shows examples of two-level fractional factorial designs. We code the levels of each factor as ==== according to Fontana et al. (2000). For each design, each row of the table shows the combination of the levels of the factors ====’s for each experimental run, and each column corresponds to each factor. For example, the design ==== is a fractional factorial design for ==== two-level factors ====, composed of ==== points in ====, ====. In the field of design of experiments, ==== is known as a regular fractional factorial design with the defining relation ====. On the other hand, the design ==== is an example of nonregular designs. For details on the regularity of designs, see Jeff Wu and Hamada (2009) for example.====The indicator functions of ==== and ==== are given as follows, respectively. ====We see, for example, ==== for ==== points in ====, and ==== for the other ==== points not in ====. The indicator function of the design of ==== two-level factors, ==== has a unique polynomial representation of the form ====where ==== and ====. As is shown in Fontana et al. (2000), the set of the coefficients ==== has all the information of the corresponding design. For example, we see the following facts from the coefficients of the indicator functions ==== and ==== for ==== and ====.====In other words, statistical concepts such as aberration and resolution can be related to the structure of the corresponding indicator functions directly for two-level designs. In particular, the structure of the indicator function of regular two-level designs can be characterized by their defining relations, and is fully revealed. See Ye (2003) for details. Another characterization of the indicator function of two-level designs relating the ====-optimality of the design is given by the author in Aoki and Takemura (2009).====In Fontana et al. (2000), these structures of the indicator function are applied to the classification of the design, which is also the object of this paper. The argument of Fontana et al. (2000) is as follows. For the indicator function (1) of two-level designs, the set of the coefficients ==== satisfies a system of algebraic equations ====where the sum ==== is considered under “mod ====” (Proposition 3.7 of Fontana et al. (2000)). Therefore, adding constraints for some orthogonality of the designs to (2), we have a system of algebraic equations having the designs with these orthogonalities as the solutions. For example, for the case of ====, additional constraints ====to (2) yield a system of algebraic equations having all the orthogonal designs with the size ==== as the solution (and ==== corresponds to one of the solutions). In this way, the complete lists of the orthogonal designs for ==== are computed by a computational algebraic software in Fontana et al. (2000). Recall that solving a system of algebraic equations is a fundamental problem where the theory of Gröbner basis is used.====In this paper, we consider generalization of the above argument on two-level designs to general fractional factorial designs. Note that the direct relations between the size and orthogonality of designs and their indicator functions are obtained only for two-level designs. To see this, consider a fractional factorial design of three-level factors ==== displayed in Fig. 2. ==== is a regular fractional factorial design with the defining relation ====. Though ==== is a regular design (and therefore the resolution of ==== is seen in its defining relation), the structure of its indicator function seems complicated as follows. ====There are several approaches to consider the indicator functions of multi-level designs. In Pistone and Rogantin (2008), a complex coding is proposed to generalize the arguments on two-level cases to multi-level cases. For example, instead of ==== above, the three-level factor is coded as ====, where ==== in Pistone and Rogantin (2008). The idea of the complex coding is based on a theory of a harmonic analysis, where the indicator function is viewed as a discrete Fourier transform. Other approach is presented in Cheng and Ye (2004) for the real coefficients field. However, it is better if we can consider ====, the field of rational numbers, as the coefficients field, because algebraic computations are conducted in ==== (or finite fields ====) for standard computational algebraic software. Another approach is a concept of Hilbert basis presented in Carlini and Pistone (2009), where the case of repeated treatments is considered by considering counting functions instead of indicator functions. In this paper, we give generalization of the relations for two-level designs such as (2) and (3) to general multi-level designs for the rational coefficients field ====, and show how to relate the structure of the designs to the structure of their indicator functions.====The construction of this paper is as follows. In Section 2, we give necessary definitions and theorems on the indicator functions. In Section 3, we give the structure of the indicator functions for general fractional factorial designs. We also derive another representation of the indicator functions, namely, contrast representation, to reflect the orthogonality of the designs directly. In Section 4, we use these results to classify ==== and ==== designs with given orthogonalities by a computational algebraic software.",Characterizations of indicator functions and contrast representations of fractional factorial designs with multi-level factors,https://www.sciencedirect.com/science/article/pii/S0378375819300266,15 March 2019,2019,Research Article,314.0
"Marella Daniela,Pfeffermann Danny","Dipartimento di Scienze della Formazione, Università “Roma Tre”, Italy,Central Bureau of Statistics and Hebrew University of Jerusalem, Israel,University of Southampton, UK","Received 25 September 2018, Revised 31 January 2019, Accepted 5 March 2019, Available online 15 March 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.03.001,Cited by (4),"Statistical matching deals with the problem of how to combine information collected in different samples taken from the same target population, but on partly different survey variables. The purpose of this paper is to analyze the statistical matching problem under informative sampling designs, when applying the sample likelihood approach. First, a ","The statistical matching problem consists of combining information collected from different samples drawn from the same population, with only partial overlap between the two samples. Formally, the problem can be described as follows. Let ==== and ==== be two independent samples of sizes ==== and ====, respectively, selected from a population of independent records ====, generated from some joint density ====, governed by a vector parameter ====. The problem of statistical matching is that ==== are not jointly observed in the two samples. Specifically, suppose that only ==== are observed for the units in sample ====, and only ==== are observed for the units in sample ====. See, e.g., Rässler (2002) and D’Orazio et al. (2006b). We assume that the samples ==== and ==== have no units in common, which will generally be the case if the population is sufficiently large and the selection probabilities are small.====The goal of statistical matching is to reconstruct a matched (fused) data set in which each record includes measurements on ====, which users may treat as a “completely” observed data set from a single source. Due to the absence of joint observations of ==== and ==== for given ====, the distribution ==== is generally not identifiable. In order to overcome this problem, two approaches have been considered in the literature. The first approach assumes conditional independence between ==== and ==== given ==== (hereafter CIA); see, e.g., Okner (1972). The CIA has a very important role in statistical matching, the reason being that under the CIA, the distribution of ==== is identifiable and directly estimable from the information provided by the two samples. Appropriateness of the CIA is discussed in several papers. We cite, among others, Sims (1972) and Rodgers (1984). The second approach assumes the existence of external auxiliary information regarding the statistical relationship between ==== and ====; see, e.g., Singh et al. (1993). However, commonly, neither approach is applicable. The CIA is rarely met in practice and relevant external auxiliary information is not often available. The lack of joint observations on the variables of interest implies uncertainty about the model holding for ====. In other words, the sample information provided by ==== and ==== is not sufficient to enable to distinguish among plausible models for ====, resulting in ====. In a parametric setting, the consequence of the identification problem is that only ranges of plausible values of the missing records, obtained from models fitted to the available sample information can be defined. Intervals defined by these ranges are known in the literature as ====. References tackling the problem of assessing the statistical matching uncertainty in the context of independent and identically observations (====) are Kadane (2001), Rubin (1986), Moriarity and Scheuren (2001) and Rässler (2002). Uncertainty in statistical matching in a nonparametric setting under the ==== assumption is considered in Conti et al., 2012, Conti et al., 2013, Conti et al., 2015.====In practice, the ==== assumption is itself questionable, particularly when dealing with sample survey data. The sample selection in survey sampling involves complex sampling designs based on different levels of clustering and differential inclusion probabilities, which could be correlated with the survey variables of interest. This can violate the independence assumption and result in different distributions of the observed data from the distribution holding in the population from which the samples are drawn. See Section 3. Statistical matching in complex sample surveys is studied in Rubin (1986), Renssen (1998) and Conti et al. (2016). Rubin (1986) proposes to compute new sampling weights for all the units in ====. However, this approach is seldom applied since it requires to assess the inclusion probabilities of the units in one sample under the sampling design of the other sample. Renssen (1998) proposes to calibrate the sampling weights in ==== and ==== such that the new weights, when applied to the measured ====-values in the two samples, reproduce the known (estimated) population totals of ====. Next, the author estimates the joint distribution of categorical variables ==== and ==== under the CIA. Conti et al. (2016) deals with the statistical matching problem for complex sample surveys non-parametrically. The authors propose to estimate the distribution function of variables which are not jointly observed based on an iterative proportional fitting algorithm, and show how to evaluate its reliability.====The aim of the present paper is to analyze the statistical matching problem for the case where the sampling processes used to select the samples ==== and ==== are informative for the target variables of interest. As already mentioned, official survey data are usually collected from samples drawn by probability sampling. When the inclusion probabilities are related to the value of the target outcome variable even after conditioning on the model covariates, the observed outcomes are no longer representative of the population outcomes and the model holding for the sample data is then different from the model holding in the population. This, quite common phenomenon, is known in the survey sampling literature as informative sampling. In this case, conventional analysis, which ignores the informative sampling effects may yield large bias and erroneous inference, as illustrated, for example, in the book edited by Skinner et al. (1989). See Pfeffermann and Sverchkov (2009) for discussion of the notion of informative sampling and review of methods to deal with this problem. Returning to statistical matching, knowledge of the sampling designs underlying the selection of the samples ==== and ==== and accounting for them, is crucial for successful matching. This is true even under the simplified CIA framework.====The paper is organized as follows. In Section 2 we summarize briefly the parametric solution to the matching problem under the CIA, for the case where the sampling process is noninformative. Section 3 considers the case of informative sampling under the CIA and defines the corresponding sample likelihood for the statistical matching problem. The use of the sample likelihood enables estimating the corresponding population distributions and to impute the missing values. The conditions under which the sample models are identifiable and estimable from the information provided by the samples ==== and ==== are investigated in Section 4. Section 5 analyses the case of a three-variate normal distribution, investigating the effects of different informative sampling designs on the population model. In Section 6 the CIA assumption is relaxed, and the uncertainty in statistical matching under informative sampling is restudied, illustrated in Section 7, where we again restrict to the three-variate normal case. Section 8 contains results of simulation experiments used to illustrate the theoretical results. We conclude with a brief summary and suggestions for further research in Section 9. All proofs and additional results are deferred to the Supplementary Material.",Matching information from two independent informative samples,https://www.sciencedirect.com/science/article/pii/S0378375818303082,15 March 2019,2019,Research Article,315.0
"Brandes Dirk-Philip,Curato Imma Valentina","Ulm University, Institute of Mathematical Finance, D-89081 Ulm, Germany","Received 13 March 2018, Revised 23 October 2018, Accepted 1 February 2019, Available online 28 February 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.02.001,Cited by (2),We consider a Lévy driven continuous time ==== ==== sampled at ==== which follow a renewal structure independent of ,"The present paper analyzes the distributional limit of sample mean and sample autocovariance function of a Lévy driven moving average process when sampled at a renewal sequence. More precisely, let ==== be a continuous time moving average process of the form ====where ==== is a two-sided ====-valued Lévy process, ====, and ==== a deterministic function, called ====, for which the integral exists.====Moving average processes as in (1.1) are the natural continuous time analogue of discrete time moving average processes, whose distributional limits for sample mean and sample autocorrelation have been studied by Brockwell and Davis (2006), Davis and Mikosch (1998), and Hannan (1976), and many others.====A popular example of a Lévy driven moving average process is given by the Ornstein–Uhlenbeck (OU) process used to model the volatility of a financial asset, see Barndorff-Nielsen and Shephard (2001), or the intermittency in a turbulence flow, see Barndorff-Nielsen and Schmiegel (2007). The OU process is in fact a tractable mathematical model that can adequately describe the fluctuations of the price and the velocity field on different time scales.====The process ==== is infinitely divisible, as shown in Rajput and Rosinski (1989), and strictly stationary meaning that its finite dimensional joint distributions are shift-invariant, i.e. for all ==== and all ==== it holds ====We study a renewal sampling of the process ==== in (1.1). We select a sequence of increasing random times ==== such that ==== almost surely (abbreviated a.s.). More in detail, we assume that ==== is an i.i.d. sequence of positive supported random variables independent of the driving Lévy process ==== and such that ====. We then define ==== by ====and the sampled process ==== via ====We are interested in studying the sample moments of the process ====. We do this for different reasons. First of all, continuous processes are often used in time series analysis because they can be sampled at non-equidistant points in time and therefore provide a model for non-equidistant data which are often available for statistical inference. Secondly, several authors (Cohen and Lindner, 2013, Drapatz, 2017, and Spangenberg, 2015) analyze the asymptotic distribution of the sample mean and sample autocovariance function when ==== is sampled on a lattice ==== for ==== but results for non-equidistant sampling schemes have not yet been shown.====The central limit theorems presented in the paper generalize the results of Cohen and Lindner (2013) at the cost of slightly more restrictive moment conditions. In fact, in the latter, the asymptotic normality of the sample mean and the sample autocovariance function is shown under the assumption of finite second and fourth moments of the driving Lévy process, respectively, whereas we achieve the asymptotic normality by requiring ==== and ====, respectively, to be finite.====As an application of the developed theory, we present a parameter estimation of the mean reverting parameter of a Lévy driven OU process ====sampled at a Poisson process, i.e. a sequence ==== where ==== is a sequence of i.i.d. exponentially distributed random variables. We then compare the efficiency of our estimator with an estimator based on the results of Cohen and Lindner (2013) for an equidistant sampling.====The paper is organized as follows. In Section 2 we give some preliminary results regarding strict stationarity of a process sampled at a renewal sequence and the mixing property that it fulfills. Section 3 is concerned with establishing a central limit theorem for the sample mean of a renewal sampled continuous time moving average process as Section 4 does for the sample autocovariance and sample autocorrelation functions. Finally in Section 5, we show the parameter estimation of a Lévy driven OU process, and Section 6 concludes.",On the sample autocovariance of a Lévy driven moving average process when sampled at a renewal sequence,https://www.sciencedirect.com/science/article/pii/S0378375819300151,28 February 2019,2019,Research Article,316.0
Godichon-Baggioni Antoine,"Institut de Mathématiques de Toulouse, Université Paul Sabatier, 31000 Toulouse, France","Received 6 February 2017, Revised 16 October 2017, Accepted 3 January 2019, Available online 27 February 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.01.001,Cited by (11),"Stochastic gradient algorithms are more and more studied since they can deal efficiently and online with large samples in high dimensional spaces. In this paper, we first establish a ","High Dimensional and Functional Data Analysis are interesting domains which do not have stopped growing for many years. To consider these kinds of data, it is more and more important to think about methods which take into account the high dimension as well as the possibility of having large samples. In this paper, we focus on a usual stochastic optimization problem which consists in estimating ====where ==== is a random variable taking values in a space ==== and ====, where ==== is a separable Hilbert space. In order to build an estimator of ====, an usual method was to consider the solver of the problem generated by the sample, i.e to consider ====-estimates (see Huber and Ronchetti (2009) and Maronna et al. (2006) among others). In order to build these estimates, deterministic convex optimization algorithms (see Boyd and Vandenberghe (2004)) are often used (see Vardi and Zhang (2000), Oja and Niinimaa (1985) in the case of the median), and these methods are really efficient in small dimensional spaces.====Nevertheless, in a context of high dimensional spaces, this kind of method can encounter many computational problems. The main ones are that it needs to store all the data, which can be expensive in terms of memory and that they cannot deal online with the data. In order to overcome this, stochastic gradient algorithms (Robbins and Monro, 1951) are efficient candidates since they do not need to store the data into memory, and they can be easily updated, which is crucial if the data arrive sequentially (see Duflo, 1996, Duflo, 1997, Kushner and Yin (2003a) or Nemirovski et al. (2009) among others). In order to improve the convergence, Ruppert (1988) and Polyak and Juditsky (1992) introduced its averaged version (see also Dippon and Renz (1997) for a weighted version). These algorithms have become crucial to statistics and modern machine learning (Bach and Moulines, 2013, Bach, 2014, Juditsky et al., 2014). There are already many results on these algorithms in the literature, that we can split into two parts: asymptotic results, such as almost sure rates of convergence (Schwabe and Walk, 1996, Duflo, 1997, Walk, 1992, Pelletier, 1998, Pelletier, 2000), and non asymptotic ones, such as rates of convergence in quadratic mean (Cardot et al., 2017, Godichon-Baggioni, 2016a, Bach and Moulines, 2013, Bach, 2014, Nemirovski et al., 2009).====In a recent work, Godichon-Baggioni (2016b) introduces a new framework, with only locally strongly convexity assumptions, in general Hilbert spaces, which allows to obtain almost sure and ==== rates of convergence. In keeping with it, and in order to have a deeper study of the stochastic gradient algorithm as well as of its averaged version (up to a new assumption), we first give the asymptotic normality of the estimates. In a second time, since a Central Limit Theorem is often unusable without an estimation of the variance, we introduce a recursive algorithm, inspired by Gahbiche and Pelletier (2000), to estimate the asymptotic variance of the averaged estimator and we establish its rates of convergence. As far as we know, there was not yet an efficient and recursive estimate of the asymptotic variance in the literature. Finally, two examples of application are given. The first usual one consists in estimating the parameters of the logistic regression (Bach, 2014) while the second one consists in estimating geometric quantiles (see Chaudhuri (1996) and Chakraborty and Chaudhuri (2014)), which are useful robust indicators in statistics. Indeed, they are often used in data depth and outliers detection (Serfling, 2006, Hallin and Paindaveine, 2006), as well as for robust estimation of the mean and variance (see Minsker et al. (2014)), or for Robust Principal Component Analysis (Gervini, 2008, Kraus and Panaretos, 2012, Cardot and Godichon-Baggioni, 2017).====The paper is organized as follows: Section 2 recalls the framework introduced by Godichon-Baggioni (2016b) before giving two new assumptions which allow to get the rate of convergence of the estimators of the asymptotic variance. In Section 3, the stochastic gradient algorithm as well as its averaged version is introduced and their asymptotic normality is given. The recursive estimator of the asymptotic variance is given in Section 4 and its almost sure as well as its quadratic mean rates of convergence are established. Applications, consisting in estimating the logistic regression parameters and in the recursive estimation of geometric quantiles, are given in Section 5 as well as a short simulation study. Finally, the proofs are postponed in Section 6 and in a Supplementary file.",Online estimation of the asymptotic variance for averaged stochastic gradient algorithms,https://www.sciencedirect.com/science/article/pii/S0378375819300011,27 February 2019,2019,Research Article,317.0
"Wang Xiaoguang,Song Lixin,Sun Leyuan,Gao Hang","Dalian University of Technology, Dalian, 116024, China","Received 18 October 2017, Revised 26 January 2019, Accepted 16 February 2019, Available online 25 February 2019, Version of Record 16 May 2019.",https://doi.org/10.1016/j.jspi.2019.02.004,Cited by (8),"The receiver operating characteristic (ROC) curve is a ==== of the relationship between false positive and true positive rates. It is a widely used statistical tool for describing the accuracy of a diagnostic test. In this paper we adopt Bernstein polynomials to construct the ROC curve estimator. The consistency rate of this estimator is studied. We also obtain explicit expressions for the asymptotic bias and variance and show the improvement of the asymptotic mean squared error compared to that of the classical empirical ROC estimator. Furthermore, the weak convergence of the Bernstein ROC process is established. Simulation studies are conducted to compare our proposed estimator with other popular nonparametric ROC estimators. Finally, the proposed method is illustrated by application to a real data set.","The receiver operating characteristic (ROC) curve is used to describe a diagnostic test which predicts presence or absence of a binary trait, often disease. For comprehensive review of the literature, see Zhou et al. (2002), Pepe (2003) and Krzanowski and Hand (2009). We commonly suppose that the independent real random variables ==== and ==== denote the test scores from diseased and non-diseased individuals, respectively, and for a given cutoff point ====, if the test result is greater than ====, we define it as positive. Let ==== and ==== be distribution functions of ==== and ====, respectively. The sensitivity and the specificity of the test are defined as ==== and ====, respectively. Sensitivity is the probability that a truly diseased individual has a positive test result while specificity is the probability that a truly non-diseased individual has a negative test result. The ROC curve is defined as a plot of sensitivity (true positive fraction or TPF) versus one minus specificity (false positive fraction or FPF), or equivalently as a plot of ====against ==== for ====.====Let ====
 ==== and ====
 ==== be samples from the diseased and non-diseased populations, respectively. Several methods have been proposed to estimate the ROC curve nonparametrically. The commonly used nonparametric estimator is the empirical ROC estimator of the form ====where ==== and ==== and ==== denote the empirical cumulative distribution functions of the samples ====
 ==== and ====
 ====, respectively. Hsieh and Turnbull (1996) studied the asymptotic properties of this estimator. However the empirical ROC estimator is not continuous and not very accurate for small sample sizes. Other methods are needed to construct a smooth estimator of the ROC curve.====Lloyd (1998) used the kernel smoothing technique to obtain a smooth ROC curve estimator given by ====where ==== and ==== are standard kernel cumulative distribution function estimators. Lloyd and Zhou (1999) showed that the kernel estimator has better mean squared error properties than the empirical ROC estimator. However the boundary performance of the kernel ROC estimator is a critical issue, especially in practice. In more recent papers, Peng and Zhou (2004) proposed a local linear smoothing ROC curve estimator and Gu et al. (2008) showed a Bayesian bootstrap estimation of ROC curve. Jokiel-Rokita and Pulit (2012) proposed a continuous and strictly increasing nonparametric estimator of the ROC curve, which is in fact the smoothed version of the empirical ROC estimator and converges uniformly to the true ROC curve almost surely. They only presented the uniform consistency.  Pulit (2016) proposed a new smoothing estimator of the ROC curve which is the combination of the empirical cumulative distribution function and the kernel cumulative distribution function estimator.====In this paper we consider the Bernstein ROC estimator defined in terms of Bernstein polynomials. We firstly review several results on Bernstein distribution function estimators, then present the precise definition of the Bernstein ROC estimator. Bernstein polynomials were employed by Babu et al. (2002) to estimate the univariate distribution and density function. Leblanc (2011) proved the pointwise asymptotic normality of the Bernstein distribution function estimator and showed that this estimator outperforms the classical empirical distribution function in terms of the asymptotic variance. In Leblanc (2009) it was shown that the Chung Smirnov property holds for the Bernstein distribution function estimator. We will study both asymptotic properties of the Bernstein ROC estimator and the weak convergence of the Bernstein ROC process.====The Bernstein polynomial of order ==== for the ROC curve is defined as ====with ====the binomial probabilities. By Lorentz (1986), we have, uniformly on ====, ====since ==== is continuous on [0,1]. We propose the following Bernstein estimator of order ==== for the ROC curve: ====where ==== is the empirical ROC estimator (1.1). The order ==== will depend on ==== and we assume that ==== if ====.====The rest of this paper is organized as following. In Section 2, three theoretical results are shown. We obtain the Chung Smirnov consistency rate for the Bernstein ROC estimator in Section 2.1, the asymptotic bias, variance and the asymptotic normality in Section 2.2, and the weak convergence of the Bernstein ROC process in Section 2.3. In Section 3 we report the results of simulation studies and compare the efficiency of the Bernstein ROC estimator with other nonparametric ROC estimators. In Section 4 the Bernstein ROC estimator is applied to a real data set. All proofs are collected in the Appendix.",Nonparametric estimation of the ROC curve based on the Bernstein polynomial,https://www.sciencedirect.com/science/article/pii/S0378375819300187,25 February 2019,2019,Research Article,318.0
"Overgaard Morten,Parner Erik Thorlund,Pedersen Jan","Department of Public Health, Aarhus University, Bartholins Allé 2, Building 1260, DK-8000 Aarhus C, Denmark,Department of Mathematics, Aarhus University, Ny Munkegade 118, DK-8000 Aarhus C, Denmark","Received 25 January 2018, Revised 21 January 2019, Accepted 9 February 2019, Available online 19 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jspi.2019.02.003,Cited by (12),"A ==== using jack-knife pseudo-observations from the Kaplan–Meier estimator, or related estimators, can be biased when censoring times depend on event times or ==== for the censoring distribution. We argue that, under certain assumptions, the pseudo-observation method with pseudo-observations from such estimators will produce consistent and asymptotically normal parameter estimates.","In the time-to-event setting, suppose interest revolves around some feature of an event time ==== and an event type ==== with ==== representing competing events. We denote this feature by ====. Examples include====The objective of the analysis under study is parameter estimation in a model of the conditional expectation of ==== given covariates of the form ====for a particular choice of mean function ====, a set of covariates ====, and the true parameters of interest ====. Primary examples of models are the generalized linear models with ==== for a suitable link function ====. In particular, when the modelled conditional expectation is a conditional probability, or rather a risk, a choice of ==== as the identity, log, or logit would give the ==== parameters the interpretation of risk differences, log risk ratios or log odds ratios.====Given a sample of size ====, and for some suitable choice of vector function ====, for instance with ==== being the derivative of ====, we would estimate the parameters by solving the estimating equation ====basically a generalized estimating equation, but are prevented from doing so if the ====s are not all observed. In this case, the pseudo-observation method suggests substituting ==== for a pseudo-observation ====. The pseudo-observation is the jack-knife pseudo-observation or pseudo-value corresponding to a certain estimator of the expectation ====. The ====th pseudo-observation is ====, where ==== is the estimate of ==== based on the whole sample and ==== is the corresponding estimate based on the sample in which observation ==== has been left out. The estimating equation actually used is thus ====The pseudo-observation method was originally proposed by Andersen et al. (2003). Important insights into the theoretical justification of the method were gained in Graw et al. (2009) and Jacobsen and Martinussen (2016). The approach was studied further in Overgaard et al. (2017), where conditions justifying the method in a general setting were presented. We recapitulate these conditions in Section 3.====By the results of Graw et al. (2009) and Jacobsen and Martinussen (2016), the pseudo-observation method based on the Kaplan–Meier estimator will produce asymptotically normal and unbiased estimates when, in addition to a positivity assumption, we assume that the censoring time is independent of both event time and the covariates in the model. This assumption is called completely independent censorings in Overgaard et al. (2017). The completely independent censorings assumption is an assumption that is more restrictive than assumptions made in other regression techniques in survival analysis and is too strict for important applications. To deal with this problem, pseudo-observations based on more involved estimators than the overall Kaplan–Meier estimator have been suggested. A stratified Kaplan–Meier estimator has been proposed in Andersen and Pohar Perme (2010) and inverse probability weighted estimators, weighting with the inverse probability of observation based on a proportional hazards model of the censoring hazard, were studied in Binder et al. (2014) and Xiang and Murray (2012). The large-sample properties are, however, not established for these approaches.====This paper establishes important large-sample properties of the pseudo-observation method with pseudo-observations based on inverse probability weighted estimators where weights are the inverse probability of observation. This covers the specific case with a proportional hazards model for the censoring distribution, but also a new approach based on an additive hazard model for the censoring distribution. The structure of the additive hazard model means that this includes the ordinary approaches with covariate-independent censorings as well as the stratified pseudo-observation approaches applied to Example 1, Example 2, Example 3, Example 4.",Pseudo-observations under covariate-dependent censoring,https://www.sciencedirect.com/science/article/pii/S0378375819300175,19 February 2019,2019,Research Article,319.0
"Li Wei,Lederer Johannes","School of Mathematical Sciences, Peking University, Beijing, China,Departments of Statistics and Biostatistics, University of Washington, Seattle, WA, USA","Received 29 September 2017, Revised 23 January 2019, Accepted 24 January 2019, Available online 16 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jspi.2019.01.006,Cited by (11),"Feature selection is a standard approach to understanding and modeling high-dimensional classification data, but the corresponding statistical methods hinge on tuning parameters that are difficult to calibrate. In particular, existing calibration schemes in the ==== framework lack any finite sample guarantees. In this paper, we introduce a novel calibration scheme for ====-penalized ====. It is based on simple tests along the tuning parameter path and is equipped with optimal guarantees for feature selection. It is also amenable to easy and efficient implementations, and it rivals or outmatches existing methods in simulations and real data applications.","The advent of high-throughput technology has created a large demand for feature selection with high-dimensional classification data. In gene expression analysis or genome-wide association studies, for example, investigators attempt to select from a large set of potential risk factors the predictors that are most useful in discriminating two or more conditions of interest. The standard approaches for such tasks are penalized likelihood methods (Bühlmann and van de Geer, 2011, Hastie et al., 2015, Bunea, 2008, Ravikumar et al., 2010, Ryali et al., 2010, Wu et al., 2009). However, the performance of these methods hinges on the calibration of tuning parameters that balance model fit and model complexity.====The focus of this paper is the calibration of the ====-penalized likelihood for feature selection in logistic regression. The most widely used schemes for this calibration are based on Cross-Validation (====) (Stone, 1974) or information criteria, including the Akaike’s information criterion (====) (Akaike, 1973), the Bayesian information criterion (====) (Schwarz, 1978), the extended Bayesian information criterion (====) (Chen and Chen, 2012), and the Generalized Information Criterion (====) (Nishii et al., 1984). ====- and ====-based procedures are designed for prediction and thus typically not suited for feature selection (Shao, 1993). In contrast, ====-, ====- and ====-type procedures, see also the recent versions in Wang et al. (2007) and Chen and Chen (2012), are designed primarily for feature selection, and some consistency results for model selection have been derived (Fan and Tang, 2013, Zhang et al., 2010a). Yet another approach based on a permutation idea has been introduced in Sabourin et al. (2015). However, all these methods share the same limitation in that they lack finite sample guarantees. This means that theoretical backup for applications, where samples sizes are always finite, is not available.====In this paper, we introduce a novel calibration scheme based on a testing procedure and sharp ====-bounds. It is easy to implement and computationally efficient, and in contrast to all previous approaches, it is indeed equipped with finite sample guarantees. Our proposal is thus of immediate practical and theoretical relevance.====The remainder of this paper is organized as follows. Section 2 contains our main proposal and the theoretical results. Sections 3 Simulation studies, 4 Real data analysis demonstrate that our method is also a contender in simulations and real data applications. Section 5 contains a brief discussion. The proofs and further simulations are deferred to the Appendix.",Tuning parameter calibration for ,https://www.sciencedirect.com/science/article/pii/S0378375819300060,16 February 2019,2019,Research Article,320.0
"Ishii Aki,Yata Kazuyoshi,Aoshima Makoto","Department of Information Sciences, Tokyo University of Science, Chiba, Japan,Institute of Mathematics, University of Tsukuba, Ibaraki, Japan","Received 22 December 2017, Revised 20 December 2018, Accepted 4 February 2019, Available online 12 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jspi.2019.02.002,Cited by (15), into the first eigenspace and the others. We create a new test procedure on the basis of those high-dimensional eigenstructures. We precisely study the influence of spiked eigenvalues on a test statistic and consider its bias correction so that the proposed test procedure has a consistency property for the size. We also show that the proposed test procedure has preferable properties for the power. We discuss the performance of the test procedure by simulations. We give a demonstration in actual data analyses using microarray data sets.,"In this paper, we consider the equality test of covariance matrices when the data dimension is much larger than the sample size. Suppose we have two classes ====. We define independent ==== data matrices, ====, for ====. We assume that ====, are independent and identically distributed (i.i.d.) as a ====-dimensional distribution with a mean vector ==== and covariance matrix ====. We assume ====. The eigen-decomposition of ==== is given by ====, where ==== having ==== and ==== is an orthogonal matrix of the corresponding eigenvectors. We assume ==== for ====, and ====s are of multiplicity one in the sense that ====Let ==== for ====. Then, ==== is a ==== sphered data matrix from a distribution with the zero mean and identity covariance matrix. Let ==== and ====, for ====. Note that ==== and ====, where ==== denotes the ====-dimensional identity matrix. Also, note that if ==== is Gaussian, ====s are i.i.d. as the standard normal distribution, ====. We assume that the fourth moments of each variable in ==== are uniformly bounded for ====. Also, we assume the following assumption:====This kind of assumption was made by Bai and Saranadasa (1996), Chen and Qin (2010) and Aoshima and Yata (2011). We note that (A-i) naturally holds when ==== is Gaussian. If ====s are independent, (A-i) is met. Another example satisfying (A-i) is the case when ==== has a skew normal distribution. See Remark S4.1 in Aoshima and Yata (2018) for the details. When ==== has a ====-dimensional ====-distribution, the second and third conditions in (A-i) are met while the first condition “====” is not met. However, as the degree of freedom of the ====-distribution becomes large, ==== becomes close to ==== for all ====. See Section 5 for the details.====We consider the equality test of covariance matrices as follows: ====
 Schott (2007) gave a test procedure based on the Frobenius norm when ====. Srivastava and Yanagihara (2010) considered a test procedure by using a Moore–Penrose inverse covariance matrix. Aoshima and Yata (2011) gave a test procedure based on the quantity of ====. They also discussed sample size determination so as to have a prespecified size and power simultaneously. Li and Chen (2012) considered the test problem by using the quantity of ====. The above references discussed asymptotic properties of their test procedures when ==== and ==== under the following eigenvalue condition: ====
 Aoshima and Yata (2018) called (1.2) the “non-strongly spiked eigenvalue (NSSE) model”. On the other hand, Ishii et al. (2016) investigated asymptotic properties of the first principal component and considered the test problem (1.1) when ==== while ====s are fixed under the following eigenvalue condition: ====Note that (1.3) implies the conditions that ==== and ==== as ====. For a spiked model as ====with positive (fixed) constants, ====s, ====s and ====s, and a positive (fixed) integer ====, the condition (1.3) is met when ==== and ====. The condition (1.3) is generalized as====For the spiked model (1.4), (A-ii) is met when ==== for ====. Aoshima and Yata (2018) called (A-ii) the “strongly spiked eigenvalue (SSE) model” and showed that high-dimensional data often have the SSE model. They also provided a method to distinguish between the SSE model and the NSSE model. See Section 5 in Aoshima and Yata (2018). Ishii, 2017a, Ishii, 2017b considered two-sample tests under (1.3) when ==== while ====s are fixed. The SSE model (A-ii) is quite difficult to handle because of the influence of strongly spiked noise. In order to handle huge noise, Aoshima and Yata (2018) created a data-transformation technique for two-sample tests which transforms the SSE model to the NSSE model. In this paper, we give a new test procedure for (1.1) under the SSE model (A-ii) by using a new approach which is different from the data-transformation technique.====The rest of this paper is organized as follows. In Section 2, we investigate the performance of the test procedure by Li and Chen (2012) under a SSE model. We emphasize that one should construct a test procedure by considering eigenstructures of high-dimensional data. We give a modification of their test statistic under (1.3). In Section 3, we consider a test statistic under the SSE model (A-ii). In Section 4, we propose a new test procedure by using the test statistic. We show that the proposed test procedure has consistency properties both for the size and power. In Section 5, we study the performance of the test procedure in numerical simulations. Finally, in Section 6, we give actual data analyses using a microarray data set.",Equality tests of high-dimensional covariance matrices under the strongly spiked eigenvalue model,https://www.sciencedirect.com/science/article/pii/S0378375819300163,12 February 2019,2019,Research Article,321.0
Rukhin Andrew L.,"Statistical Engineering Division, National Institute of Standards and Technology, Gaithersburg, MD 20899, USA","Received 13 April 2018, Revised 7 September 2018, Accepted 10 December 2018, Available online 5 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jspi.2018.12.003,Cited by (6), subset are recommended for practical use.,"In a meta-analytic setting the estimators of the common mean reported by, say, ==== studies, each using possibly its own technique, are to be combined to get a consensus estimate of this parameter. The popular random effects model posits these estimators ====, to have the form ====The parameter of primary interest (focus parameter) is ====, the unknown common mean, the treatment effect in meta-analysis or the reference value in interlaboratory comparisons. The measurement error ==== of the ====th study has zero mean and some positive variance representing the within-study variability; the protocol usually demands that beside their estimates of ====, the participants also report the estimates ==== of this variance (within study uncertainty). In some cases the degrees of freedom associated with these estimators are also required, but it is not assumed here that they are present. The additional noise (between study effect) realized as ==== in (1) is commonly assumed to have zero mean and the so-called heterogeneity variance ====, ====.====In clinical trials involving the comparison of a drug and placebo, the treatment effect is often measured by the number of independent successful outcomes which automatically provides the variance of this effect estimator. However in many other instances the reported uncertainties tend to underestimate the actual standard errors. Indeed, the multitude of unrealistically small standard errors which do not take into account all possible sources of errors, is widely acknowledged in measurement science. In some applications the variances of systematic, laboratory specific errors cannot be estimated by statistical methods, or the observations are too scarce as they are taken close to the detection limit.====The between study effect (termed “hidden errors”, “excess-variance”, or “dark uncertainty”) is sometimes evident; it represents the additional variability which is a fairly common metrological phenomenon. In some studies the degree of heteroskedasticity is quite substantial (e.g. Thompson and Sharp, 1999). Many different procedures to estimate the heterogeneity variance in meta-analysis are reviewed in Higgins et al. (2009), Rukhin (2013) or Veroniki et al. (2016).====However, the assumption that all ====’s have the same dispersion seems to be violated in many collaborative studies where the smallest reported values ==== correspond to the cases which are most deviant from the bulk of data. These aberrant data points unduly influence the consensus estimate of the common mean shifting it closer to their laboratory value. One of the important examples is the collaborative effort to determine the values of fundamental physical constants (e.g. Mohr and Taylor, 2005). In the study of Newton’s gravitational constant in 1998 one of institutes presented an outlying measurement of this constant accompanied by a small uncertainty. The present (2017) estimate of Newton’s gravitational constant (====) obtained after the removal of this data point is about three standard deviations away from the 1998 value!====Many more examples of this kind can be found in Cox (2007) or in Thompson and Ellison (2011). These authors analyzed heterogeneous data sets from BIPM International Key Comparisons (====) where a group of national metrology laboratories reports the measurement results on the same material along with their uncertainty estimates. All these examples make one think that the main reason for a constant variance between-study effect is merely the mathematical convenience of making this additional error quantifiable.====There is a body of work aimed at extending the traditional random effects exemplar for meta-analysis needs. For example, Ohlssen et al. (2007), Lee and Thompson (2008) provide more general models. This paper assumes random effects to be normally distributed with unknown different variances which admit the given positive lower bounds.====By independence of ==== and ====, the variance of ==== cannot be smaller than the variance of ==== which is estimated by ====. In many situations, the estimate ==== is not very reliable, so it is suggested to use it only as mentioned lower bound for the unknown variance of ====. In some cases such bounds can be obtained from physical considerations, archive data or expert judgments. Hoaglin (2016) discusses of possible dangers when ==== is treated as exactly known but the sample size is small.====The restricted and classical maximum likelihood estimators of the heterogeneity variances ==== are derived in Sections 2 Classical maximum likelihood estimator, 3 Restricted maximum likelihood estimator. They lead to procedures to select the model where only a given number ==== of ==== is positive. The conditions under which some variance estimators exceed their fixed lower bounds is discussed in Section 4. The likelihood ratio tests of this wider paradigm and of the traditional random effects model are discussed in Section 6. Noninformative Bayes estimators are developed in Section 5. They are much smoother than the likelihood estimators, produce good confidence intervals for the common mean and are recommended for practical use. Under the heteroskedasticity scenario described in Section 8, the maximum likelihood estimators are compared to Bayes procedures numerically. The confidence intervals based on these procedures outperform notoriously accurate intervals based on higher order asymptotics obtained from the classical random effects model.====The numerical algorithm for the evaluation of the maximum likelihood estimators is provided in the Supplement. The Appendix gives two proofs and some comments about particular cases.",Estimating heterogeneity variances to select a random effects model,https://www.sciencedirect.com/science/article/pii/S0378375818303574,5 February 2019,2019,Research Article,322.0
"Beirlant Jan,Worms Julien,Worms Rym","Department of Mathematics and Leuven Statistics Research Center, KU Leuven, Belgium,Department of Mathematical Statistics and Actuarial Science, University of the Free State, South Africa,Université de Versailles-Saint-Quentin-En-Yvelines Laboratoire de Mathématiques de Versailles (CNRS UMR 8100), F-78035, Versailles Cedex, France,Université Paris-Est, Laboratoire d’Analyse et de Mathématiques Appliquées (CNRS UMR 8050), UPEMLV, UPEC, F-94010, Créteil, France","Received 17 April 2018, Revised 10 December 2018, Accepted 18 January 2019, Available online 4 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jspi.2019.01.004,Cited by (6)," rate where ==== denotes the number of top data used in the estimation, depending on the degree of censoring.","Starting from Beirlant et al. (2007), the estimation of the extreme value index in a censorship framework is of growing interest. Suppose we observe a sample of ==== independent couples ==== where ====The i.i.d. samples ==== and ====, of respective continuous distribution functions ==== and ====, are samples from the variable of interest ==== and of the censoring variable ====, measured on ==== individual items (insurance claims, hospitalized patients, etc.). The variables ==== and ==== are supposed to be independent and, for convenience only, we will suppose in this work that they are non-negative. We will denote by ==== the order statistics associated to the observed sample, and by ==== the corresponding indicators of non-censorship.====Einmahl et al. (2008) presented a general method for adapting estimators of the extreme value index in this censorship framework. Worms and Worms (2016) proposed a more survival analysis-oriented approach restricted to the heavy tail case, while Ndao et al., 2014, Ndao et al., 2016 and Stupfler (1995) extended the framework to data with covariate information. Beirlant et al. (2016) and Beirlant et al. (2018) proposed bias-reduced versions of two existing estimators. See also Brahimi et al., 2015, Brahimi et al., 2016, Brahimi et al., 2018 for other papers on the subject.====In this paper, we propose a new class of estimators that encompasses one of the estimators proposed in Worms and Worms (2016) and propose a novel approach to prove the asymptotic normality of these estimators which was unknown up to now for the case ====. We consider here that the distributions ==== and ==== are heavy-tailed, with positive and respective extreme value indices (EVI) ==== and ====, i.e. ====where ==== and ==== are slowly varying at infinity. Our target is the EVI ====, which we try to recover from our randomly censored observations.====Denoting the distribution function of ==== with ====, by independence of ==== and ==== we readily obtain ====, where ==== and the EVI ==== of ==== is related to those of ==== and ==== via the important relation ====. Further in this paper, we will denote by ==== the crucial quantity ====, which has to be interpreted as the asymptotic proportion of non-censored observations in the tail.====We assume in this work that the slowly varying functions ==== and ==== satisfy the second order condition first proposed by Hall and Welsh (1985). This yields the so called “Hall-type” model, i.e. as ====, ==== where ====, ====, ====, ==== are positive constants and ====, ==== are real constants. Then, setting ====we have, as ====, ====Correspondingly, with ==== (====) the quantile function corresponding to ====, we consider ====, the right-tail function of ====, for which as ====, ====Let us now explain how we build our new family of estimators of ====. For some real number ====, consider the Box–Cox transform ==== for ====, with the case ==== leading to ====. Based on the relation ====valid for ====, and estimating ==== by the Kaplan–Meier estimator ==== defined for ==== by ====we introduce the following class of statistics ====where ==== denotes an integer sequence satisfying ==== and ====. With ==== we thus obtain the estimator ====of ==== which was considered in Beirlant et al. (2018). In fact ==== turns out to be very close to the estimator==== defined in equation (12) of Worms and Worms (2016) based on ideas issued from the so-called Leurgans approach in survival regression analysis. The difference concerns a different way to circumvent the use of ==== at ====: whether using left-limits or deleting ==== as in ====.====Note that the statistics ==== were used in Beirlant et al. (2018) to obtain a bias-reduced version of the estimator ====: ====where ==== denotes the second order parameter of ==== in assumption (1).====Now, it is clear from (5) that we can construct the following estimator of ==== when the tuning parameter ==== is supposed to be larger than ====: ====We will compare these estimators with the pseudo maximum likelihood estimator which was first proposed in the random censoring context by Beirlant et al. (2007) and Einmahl et al. (2008): ====In Beirlant et al. (2018) a small sample simulation study was performed using all those available estimators and it was found that ==== overall shows quite good bias and MSE performance. However, since no results on the asymptotic normality of this estimator were available yet, these authors proposed the use of a bootstrap algorithm to construct confidence intervals. In this paper we prove the asymptotic normality of ==== in the case ====. Hence this paper provides the first complete proof of the asymptotic normality for ==== in case ====, issued from an explicit asymptotic development stated in Theorem 1 of the next section. In the deterministic threshold case, this central limit result (for ====) had already been obtained in Worms and Worms (2018), where a more general competing risks setting was considered, and using a different approach from the present proof.====The restriction ==== is rather restrictive for instance in insurance problems such as those discussed in Beirlant et al. (2018) where heavy censoring appears. The introduction of the class of estimators ==== helps to circumvent this problem when considering ====.====Finally, in the next section, we will see that our results also lead to the statement of the asymptotic normality of the bias-reduced estimator ====, which was not known so far.====Our paper is organized as follows: in Section 2, we state and discuss the asymptotic normality result for ==== and ====. Section 3 is devoted to the proof. Technical aspects of the proof are postponed to the Appendix. In Section 4 we discuss the finite sample behavior of the different estimators ==== with ====, and of ====.",Estimation of the extreme value index in a censorship framework: Asymptotic and finite sample behavior,https://www.sciencedirect.com/science/article/pii/S0378375819300047,4 February 2019,2019,Research Article,323.0
"Sun Wei,Bindele Huybrechts F.,Abebe Ash,Correia Hannah","411 University Blvd. N, MSPB 316, Department of Mathematics and Statistics, University of South Alabama, Mobile AL 36688-0002, United States,221 Parker Hall, Department of Mathematics and Statistics, Auburn University, AL 36849, United States","Received 4 April 2018, Revised 30 September 2018, Accepted 22 January 2019, Available online 1 February 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jspi.2019.01.005,Cited by (4),. An application in ecology is provided and shows that the rank-based regression procedure effectively provides more accurate estimates compared to its least squares counterpart.,"The single-index varying coefficient model (SIVCM) is studied by many researchers due to its flexibility and interpretability. The model has been applied for addressing problems in areas such as finance, ecology, and public health among others. One important feature that makes the SIVCM attractive is its ability to overcome the “curse of dimensionality” often encountered in nonparametric modeling of multivariate data. The SIVCM is defined as ====where ====, with ====, ====, ==== is a ====vector of unknown regression parameters representing the single-index direction; ====, ====, are unknown regression coefficient functions; and conditioning on ====, ==== are independent and identically distributed random errors with finite Fisher information. For model identifiability, it is assumed throughout this paper that the first component of ==== is set at 1. Model (1) includes a class of important semiparametric models known as single-index models by setting ====. This model was first considered by Xia and Li (1999) who proposed estimating ==== via an ====-cross validation approach following ideas of Härdle et al. (1993). They also established the ====-consistency and asymptotic normality of their proposed estimator under some mild conditions. Setting ==== in model (1), Fan et al. (2003) proposed a computationally efficient estimation approach based on a profile least squares (LS) local linear regression, where they also discussed how to select locally significant variables based on the ====-statistic and the Akaike information criterion. Motivated by the “remove-one-component” approach proposed in Yu and Ruppert (2002) and Xue and Pang (2013) provided estimators of ==== and the coefficient functions. In an effort to construct a robust confidence region for ====, Xue and Wang (2012) studied model (1) using an empirical likelihood approach.====However, all the estimation methods above are LS-type methods, which are known to be sensitive to outliers, model contamination, and/or heavy-tailed error distributions. To mitigate the effect of these abnormalities, it is imperative to develop robust and efficient estimation procedures. Yao et al. (2012) proposed a local modal estimation procedure for nonparametric regression models using an EM algorithm. Their estimator was shown to be more efficient compared to ordinary local polynomial estimators when dealing with outliers in the response space and for heavy tailed model error distributions. They also showed that their estimator is as asymptotically efficient as the local polynomial regression estimator under the condition that there are no outliers or the errors are from the normal distribution. The concept of using local modal estimation to obtain robust estimators has been extended to semiparametric partial linear varying coefficient models, single-index models and SIVCMs by Zhang et al. (2013), Liu et al. (2013) and Yang et al., 2014, Yang et al., 2016, respectively. Feng et al. (2012) used the Wilcoxon rank-based method of Hettmansperger and McKean (2011) to produce a robust estimator ==== for the single-index model. Although their simulation study includes categorical predictors, this is not justified theoretically, as their approach relies on taking derivatives with respect to covariates. Very recently, for single-index models, Bindele et al. (2018) proposed a general rank-based approach which addresses the issue found in Feng et al. (2012). For varying coefficient models, Wang et al. (2009) proposed a local rank estimation method which is based on the objective function of Jaeckel (1972). Their approach was shown to have several advantages compared to the LS-type approaches. However, none of the aforementioned works develops a rank-based estimation procedure for the SIVCM (1).====In this paper, we propose a general rank-based (R) estimation procedure for model (1) based on minimization of the rank objective function of Jaeckel (1972). To our knowledge, this is the first work that addresses the estimation of the SIVCM index direction as well as coefficient functions using a rank-based approach. By including the estimation of the single-index direction ==== for the varying coefficients, our approach will extend the work of Wang et al. (2009). As stated in Wang et al. (2009), R estimation has a simple geometric interpretability as the LS approach and it results in robust and more efficient estimators compared to those obtained via many of the classical estimation approaches including the LS and least absolute deviation (LAD) procedures (Hettmansperger and McKean, 2011). For computing the regression estimators, we propose a backfitting type algorithm by iterating between the coefficient functions through local linear procedure and estimating ==== by a one-step R estimation. The local linear estimation of ====, ====, involves bandwidth selection, which is done via a leave-one-out R cross-validation. We demonstrate that the resulting estimators are robust and asymptotically efficient compared to LS estimators when the data contain outliers. The consistency of the proposed estimator is obtained using the stochastic equicontinuity of the rank objective function. The proof for asymptotic normality of the proposed estimator of single-index direction relies on the asymptotic equivalence of the rank gradient function and a statistic that is a sum of independent random variables, which is asymptotically normal via the conditional Lindeberg–Feller CLT (Rao, 2009).====Our consideration of the model (1) and its robust estimation was primarily motivated by an ecological problem involving a high-dimensional vector of environmental predictors with interacting groundfish species. While the use of the SIVCM in ecology has focused on predator–prey models (Fan et al., 2003, Xia et al., 2007), it may also be used to quantify interspecific competition in complex food webs. We consider a subset of fisheries data obtained from the NOAA Marine Ecology Stock Assessment (MESA) Program paired with environmental data from NOAA buoys to understand interactions between groundfish species in relation to changes in environmental variables in the Gulf of Alaska. The data provided a yearly median catch per unit effort (CPUE) for each of three groundfish and yearly summer coefficients of variation for each of seven environmental covariates.====As a preliminary analysis, we performed principal components analysis on the environmental variables and retained the first PC. We then split the data into two at the median of the first PC indicating two environmental regimes. We plotted log-transformed and scaled CPUE values (Phillips et al., 2014) of Pacific halibut versus those of Pacific cod and sablefish which were superimposed with loess fits to detect any nonlinearities (Fig. 1).====It appears that sablefish and Pacific cod tend to prefer different environmental regimes and their relationship with Pacific halibut depends on the environment. In both cases, the first PC captures only about 40% of the environmental variability. So, a varying coefficient model relating Pacific halibut to Pacific cod and sablefish using just the first PC would not be sufficient. Building a varying coefficient model using all the environmental variables is also not realistic as it requires a seven dimensional smoother. This suggests the model ====where ==== is the CPUE of Pacific halibut for ====; ==== are the CPUE of Pacific cod; ====, the CPUE of sablefish; and the matrix ==== is the vector of environmental variables. Robust fitting is required to mitigate the effect of possible outlying points on our fit. We later demonstrate that for these data it is indeed the case that the classical approach is highly affected by the outliers, but the robust method proposed in this paper remains unaffected.====The remainder of the paper is organized as follows: Section 2 presents the estimation procedures for ==== and function ====. The computational algorithm for obtaining the rank-based estimators of ==== and ==== is also provided in Section 2. In Section 3, an extensive Monte Carlo simulation study and an illustrative real data example are presented to demonstrate the advantage of the proposed rank-based estimation method. Section 4 discusses the asymptotic properties of the proposed estimators. A brief conclusion is provided in Section 5. Proofs of some theoretical results are given in the Appendix.",General local rank estimation for single-index varying coefficient models,https://www.sciencedirect.com/science/article/pii/S0378375819300059,1 February 2019,2019,Research Article,324.0
"Branson Zach,Rischard Maxime,Bornn Luke,Miratrix Luke W.","Department of Statistics, Harvard University, United States,Department of Statistics and Actuarial Science, Simon Fraser University, Canada,Graduate School of Education, Harvard University, United States","Received 14 March 2018, Revised 30 September 2018, Accepted 18 January 2019, Available online 28 January 2019, Version of Record 11 March 2019.",https://doi.org/10.1016/j.jspi.2019.01.003,Cited by (19),"One of the most popular methodologies for estimating the average treatment effect at the threshold in a regression discontinuity design is local linear regression (LLR), which places larger weight on units closer to the threshold. We propose a Gaussian process regression methodology that acts as a ==== analog to LLR for regression discontinuity designs. Our methodology provides a flexible fit for treatment and control responses by placing a general prior on the mean response functions. Furthermore, unlike LLR, our methodology can incorporate uncertainty in how units are weighted when estimating the treatment effect. We prove our method is consistent in estimating the average treatment effect at the threshold. Furthermore, we find via simulation that our method exhibits promising coverage, interval length, and mean squared error properties compared to standard LLR and state-of-the-art LLR methodologies. Finally, we explore the performance of our method on a real-world example by studying the impact of being a first-round draft pick on the performance and playing time of basketball players in the National Basketball Association.","Recently there has been a renewed interest in regression discontinuity designs (RDDs), which originated with Thistlethwaite and Campbell (1960). In an RDD, the treatment assignment is discontinuous at a certain covariate value, or “threshold”, such that only units whose covariate is above the threshold will receive treatment. There are many examples of RDDs in the applied econometrics literature: the United States providing additional funding to only the 300 poorest counties for the Head Start education program (Ludwig and Miller, 2007); schools mandating students to attend summer school if their exam scores are below a threshold (Matsudaira, 2008); colleges offering financial aid to students whose academic ability is above a cutoff (Van der Klaauw, 2002); and Medicare increasing insurance coverage after age 65 (Card et al., 2004). The main goal of an RDD is to estimate a treatment effect while addressing likely confounding by the covariate that determines treatment assignment.====One of the most popular methodologies for estimating the average treatment effect at the threshold in an RDD is local linear regression (LLR), which places larger weight on units closer to the threshold. Implementation of LLR is straightforward and there is a wide literature on its theoretical properties. However, recent works have found that LLR can exhibit poor inferential properties – such as confidence intervals that tend to undercover – which has motivated a strand of literature started by Calonico et al. (2014) that modifies LLR to improve coverage and other inferential properties.====Adding to this literature, we propose a nonparametric regression approach that acts as a Bayesian analog to LLR for sharp regression discontinuity designs. Our approach utilizes Gaussian process regression (GPR) to provide a flexible fit for treatment and control responses by placing a general prior on the mean response functions. While GPR has been widely used in the machine learning and statistics literature, it has not previously been proposed for estimating treatment effects in RDDs. Thus, our main contribution is outlining how to use Gaussian processes to make causal inferences in RDDs and assess how such a methodology compares to current LLR methodologies.====In the remainder of this section, we review RDDs and LLR methodologies for estimating the average treatment effect at the threshold. In Section 2, we outline GPR for sharp RDDs and note various analogies to LLR, which builds intuition for implementing our method. In Section 3, we establish that our method is consistent in estimating the average treatment effect at the boundary. In Section 4, we show via simulation that our method exhibits promising coverage, interval length, and mean squared error properties compared to standard LLR and state-of-the-art LLR methodologies. In Section 5, we use GPR on data from the National Basketball Association (NBA) to estimate the effect of being a first-round versus a second-round pick on basketball player performance and playing time, and we find that GPR detects treatment effects that are more in line with previous results in the sports literature than do LLR methodologies. In Section 6, we conclude by discussing extensions to our methodology to tackle problems beyond sharp RDDs.",A nonparametric Bayesian methodology for regression discontinuity designs,https://www.sciencedirect.com/science/article/pii/S0378375819300035,28 January 2019,2019,Research Article,325.0
"Yu Chaoyu,Hoff Peter D.","Department of Biostatistics, University of Washington, Seattle, WA, USA,Department of Statistical Science, Duke University, Durham, NC, USA","Received 6 January 2018, Revised 11 January 2019, Accepted 11 January 2019, Available online 21 January 2019, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2019.01.002,Cited by (1),"In multiple testing scenarios, typically the sign of a parameter is inferred when its estimate exceeds some significance threshold in absolute value. Typically, the significance threshold is chosen to control the experimentwise type I error rate, family-wise type I error rate or the ====. However, controlling these error rates does not explicitly control the sign error rate. In this paper, we propose two procedures for adaptively selecting an experimentwise significance threshold in order to control the sign error rate. The first controls the sign error rate conservatively, without any distributional assumptions on the parameters of interest. The second is an empirical Bayes procedure, and achieves optimal performance asymptotically when a model for the distribution of the parameters is correctly specified. We also discuss an adaptive procedure to minimize the sign error rate when the experimentwise type I error rate is held fixed.","We consider multiparameter inference for the normal means model, ====where ==== and ====. Simultaneous inference for ==== often begins by testing ==== for each ==== at level ====, that is, we reject ==== if ==== exceeds the ==== standard normal quantile, ====. This controls the experimentwise type I error rate to be equal to ====, i.e. ====. A popular method for choosing ==== is the Benjamini–Hochberg (BH) procedure (Benjamini and Hochberg, 1995). The BH procedure is an adaptive method for selecting a value of ==== that will bound the false discovery rate (FDR), which is defined as ====, where ==== is the number of rejections and ==== is the number of false rejections, that is, the number of null hypotheses that are rejected but true. There is a large literature on FDR control, see Efron (2012), Benjamini (2010), Genovese and Wasserman (2004), Storey, 2002, Storey, 2007. However, in many applications it is likely that none of the ====’s are truly equal to exactly zero. For example, in the case where each ==== represents a difference in sample averages between two treatments, Tukey (1991) argued that evaluating if ==== is “foolish” since the effects of two different factors are always different, however minutely. In such cases, Tukey (1962) suggests that a more meaningful task is to judge whether or not there is enough evidence to infer the sign of ====, instead of whether or not it is zero. However, if significance tests are used in this way, then FDR control is inappropriate since it is always zero if there are no true nulls. Instead, the relevant error control is not the FDR, but a sign error rate (Gelman and Tuerlinckx, 2000, Gelman and Carlin, 2014, Owen, 2016).====Benjamini and Yekutieli (2005) showed that the Benjamini–Hochberg algorithm can be used to control the pure directional FDR, defined as the expected proportion of discoveries in which a positive parameter is declared negative or a negative parameter is declared positive. We refer to this procedure as the BY procedure in this paper. Some follow-up work includes Zhao et al. (2015) who used weighted ====-value methods, and Guo et al. (2010) who extended the idea to making multidimensional directional decisions. Weinstein et al. (2013) derived new selection-adjusted confidence intervals by minimizing an objective function comprised of the length of the acceptance region and a penalty term for the magnitude of the observation. They showed in examples that these procedures have correct coverage on selected parameters, and have more power to determine the sign, but they did not assess the sign error rate directly. These procedures also do not utilize information across experiments and so are not adaptive.  Stephens (2017) proposed an empirical Bayes procedure for sign error control to gain more power. However, the focus there was control of the local sign error instead of the sign error rate across experiments.====In Section 2, we discuss the distribution of the sign error proportion (SEP) under a hierarchical model for the ====’s and ====’s, and relate this to a marginal sign error rate (MSER). We then propose an adaptive nonparametric procedure that controls the MSER below a desired threshold regardless of the distribution of the ====’s. This procedure is more powerful than BY procedure in terms of the number of rejections made, and therefore in terms of the number of signs inferred. The power can be further improved if one is willing to assume a parametric model for the distribution of the ====’s. We show that a model-based approach to MSER control can achieve an optimal power asymptotically, if a model for the ====’s is chosen correctly. In Section 3, we numerically compare the nonparametric procedure and parametric procedures to the BY procedure and an oracle MSER control procedure. In Section 4, we discuss an adaptive procedure for the somewhat different task of sign inference subject to fixed experimentwise type I error rate. We show how the acceptance region of a level-==== test of each ==== may be adaptively chosen to minimize the MSER or maximize the power, that is, the number of sign discoveries. A discussion follows in Section 5.==== The reader might find the following list of acronyms, and the locations of their introduction, to be a useful reference: SEP ==== Sign Error Proportion (Section 1 Paragraph 3, written as 1–3 for short); MSER ==== Marginal Sign Error Rate (1–3); LC ==== Loose Control (2.2–3); NLC ==== Non-asymptotic Loose Control (2.2–4); BY ==== Benjamini and Yekutieli (2.2–5); TCO ==== Tight Control Oracle (2.3–2); TCE ==== Tight Control Empirical (2.3–3); ALD ==== Asymmetric Laplace Distributions (2.3–5); TCEA ==== Tight Control Empirical with ALD Distribution (2.3–6); MSDR ==== Marginal Sign Discovery Rate (4–2); UMAU ==== Uniformly Most Accurate Unbiased (4–3).",Adaptive sign error control,https://www.sciencedirect.com/science/article/pii/S0378375819300023,21 January 2019,2019,Research Article,326.0
"Klaassen Chris A.J.,Susyanto Nanang","Korteweg–de Vries Institute for Mathematics, University of Amsterdam, The Netherlands,Department of Mathematics, Universitas Gadjah Mada, Yogyakarta, Indonesia","Received 20 March 2018, Revised 2 November 2018, Accepted 16 December 2018, Available online 26 December 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.12.005,Cited by (0),Assume a (semi)parametrically efficient estimator is given of the Euclidean parameter in a (semi)parametric model. A submodel is obtained by constraining this model in that a ==== of the Euclidean parameter vanishes. We present an explicit method to construct (semi)parametrically efficient estimators of the Euclidean parameter in such equality constrained submodels and prove their efficiency. Our construction is based solely on the original efficient estimator and the constraining function.,"There is an extensive literature about ====, mainly of finite-dimensional parameters. Density estimation under shape restrictions, like monotonicity and convexity, may be viewed as estimation under constraints of infinite-dimensional parameters. See e.g. Groeneboom et al. (2001) and Horowitz and Lee (2017).====We do not consider nonparametric estimation and restrict attention to the estimation of Euclidean parameters within parametric and semiparametric models. Let us first consider parametric models with their natural parameter space and assume that this natural parameter is constrained to a subset of the parameter space and that this subset has a nonempty interior. Typically such subsets are defined by inequalities for the parameter. So-called restricted maximum likelihood estimators have been studied for many such models, and admissible and minimax estimators have been constructed as well, also in the presence of nuisance parameters. An excellent overview of the huge literature on this topic is given in the monograph by Van Eeden (2006) with 27 pages of references. A more concise review is presented by Marchand and Strawderman (2004).====When we study these estimation problems from the point of view of asymptotic statistics, Local Asymptotic Normality plays an important role. Local Asymptotic Normality at a particular parameter value implies that the experiment of estimating the parameter in the neighborhood of the particular value when properly rescaled, as sample size tends to infinity, converges to the experiment of estimating the location parameter of a (multivariate) normal distribution based on one observation, a so-called Gaussian shift experiment. A generalization of this insight stems from Lucien Le Cam; cf. Le Cam (1972). See also Chapter 10 of Cam (1986) and Chapter 9 of Van der Vaart (1998). Consider regular parametric models, as defined e.g. in Section 2.1 of Bickel et al. (1993). The regularity implies that the parameter space is open and that Local Asymptotic Normality holds at every parameter value. Consequently, asymptotically efficient estimators of the parameter exist; see e.g. Section 2.5 of Bickel et al. (1993). As asymptotically efficient estimators are consistent and hence take their values in any neighborhood however small of the true parameter value with probability tending to 1 as sample size increases, any estimator that is efficient within the original model, is efficient in the constrained model as well, at least if the parameter space of the constrained model is again open. In case the restricted parameter set is closed, one has to work with tangent cones in order to study efficient estimation for parameter values at the boundary; see Chapters 2 and 5 of Van der Vaart (1998), and Andrews (1999).====Next, consider semiparametric estimation with restrictions or constraints on the Euclidean parameter that result in parameter spaces with nonempty interior. Here too, for parameter values in the interior typically Local Asymptotic Normality holds, which means that the standard asymptotic efficiency theory holds, i.e., the sequence of experiments indexed by the sample size behaves asymptotically like the experiment of one observation from a normal distribution for which the mean vector has to be estimated and for which there are no restrictions on this mean vector. Again, for parameter values at the boundary of the restricted subset, there are restrictions on the mean vector in the limit experiment, which typically amount to the mean vector being restricted to a cone.====However, if the restrictions on the parameter space of a parametric or semiparametric model lead to a parameter subset with empty interior, asymptotic estimation theory becomes interesting again. If the restrictions lead to a subset that can be reparametrized in such a way that we are dealing with a regular parametric problem, then Susyanto and Klaassen (2017) can be applied. However, there are constraints that result in parameter subsets for which such a reparametrization is impossible. For example, consider bivariate normal distributions for which the mean vectors are known to be on the unit circle, or any other closed curve without self-intersections. As such a closed curve cannot be reparametrized via an open interval, and hence the constrained model cannot be viewed as a regular parametric model, Susyanto and Klaassen (2017) cannot be applied and a different approach has to be developed. This is done in the present paper, which considers parameter subsets that are zero sets of continuous (constraining) functions and consequently closed. On the other hand, stereographic projection of the unit circle from its north pole shows that the unit circle with one point deleted can be parametrized via an open set. However, the unit circle with one point deleted is not closed. This means that the present paper and Susyanto and Klaassen (2017) are complementary in some sense; although many models can be treated by the methods of both of them, there are models that can be treated by one but not the other.====In the present paper, the asymptotic lower bound for the constrained model is obtained by projection of the efficient influence function within the unconstrained model, and an efficient estimator for the constrained model is constructed by correcting the original, efficient estimator for the unconstrained model with an expression involving the derivative of the constraining function at the value of the original estimator. In contrast, Susyanto and Klaassen (2017) obtain the asymptotic lower bound for the constrained model via projection of efficient score functions and they start from a ====-consistent estimator of the underlying parameter in the constrained model, correct it with an expression involving the given efficient estimator for the unconstrained model, and apply the reparametrization function to it in order to obtain their efficient estimator for the constrained model. In the present paper, everything will be done directly to the original parameter subject to equality constraints without reparametrizing it. Despite the fundamental differences between the present paper and Susyanto and Klaassen (2017), their methods yield the same estimator in case the constraining function is linear.====The outline of the paper is as follows. The type of models we consider and the necessary notation will be introduced in Section 2. There we discuss some more of the literature on constrained models, especially on constrained ==== models. In Section 3, we will present a lower bound to the efficient information bound for estimating the parameter of interest within the constrained model. This lower bound will be formulated in terms of the efficient information bound of the original, unconstrained model and the Jacobian of the constraining function. An explicit estimator that is efficient within the constrained model, will be given in Section 4. It attains the lower bound from Section 3, which shows that both this information bound and the estimator are efficient within the constrained model. Examples are discussed in Section 5. Our conclusions are presented in Section 6.",Semiparametrically efficient estimation of Euclidean parameters under equality constraints,https://www.sciencedirect.com/science/article/pii/S0378375818303598,26 December 2018,2018,Research Article,327.0
"Crambes Christophe,Henchiri Yousri","Institut Montpelliérain Alexander Grothendieck (IMAG), Université de Montpellier, France,Université de la Manouba, Institut Supérieur des Arts Multimédia de la Manouba (ISAMM), Tunisie,Université de Tunis El Manar, École Nationale d’ingénieures de Tunis, LR99ES20, Laboratoire de Modélisation Mathématique et Numérique dans les Sciences de l’Ingénieur (ENIT-LAMSIN), B.P. 37, 1002 Tunis, Tunisie","Received 10 May 2018, Revised 8 October 2018, Accepted 13 December 2018, Available online 21 December 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.12.004,Cited by (21),"We are interested in functional linear regression when some observations of the real response are missing, while the functional ","Literature on functional data is really wide, as attested by the numerous books on this subject these last years. The estimation and forecasting theories of linear processes in function spaces are developed in Bosq (2000). A comprehensive introduction to functional data analysis can be found in Ramsay and Silverman (2005). In the focus of Ferraty and Vieu (2006) are nonparametric approaches. Computational issues are explained in Ramsay et al. (2009). Nonparametric statistical methods for functional regression analysis, specifically the methods based on a Gaussian process prior in a functional space are discussed in Shi and Choi (2011). In Horváth and Kokoszka (2012) inferential procedures based on functional principal components are considered. Zhang (2014) mainly focuses on hypothesis testing problems about functional data. Among this, the functional linear model has received a special attention (see Ramsay and Dalzell, 1991, Cardot et al., 1999, Cardot et al., 2003, Cai and Hall, 2006, Hall and Horowitz, 2007, Crambes et al., 2009, Cardot and Johannes, 2010, Yuan and Cai, 2010 for main references).====In this paper, we are interested in the functional linear model ====where ==== is the unknown function of the model, ==== is a real variable of interest, ==== is a centered real random variable representing the error of the model, with finite variance ====, and ==== is a functional covariate belonging to some functional space ==== endowed with an inner product ==== and its associated norm ====. Usually, ==== is the space ==== of square integrable functions defined on some real compact ==== and the corresponding inner product is defined by ==== for functions ====. Without loss of generality, we consider our work on ====. Moreover, we assume that ==== and ==== are independent.====All the previously cited works are devoted to analyze complete data, however, this is not the case in many interesting applications including for example survival data analysis. For this reason, we focus in this work on the problem of missing data (see Little and Rubin, 2002, Graham, 2012 for a wide introduction in the multivariate framework). This subject has been widely studied, in particular the way to impute missing data and the accuracy of this imputation according to the types of missing data: Missing Completely At Random (MCAR), Missing At Random (MAR) and Missing Not At Random (MNAR). Even if this problematic has received a lot of attention in a multivariate framework, it is not the case for the functional data framework. Our objective is to study the problem of combining regression imputation, missing data mechanisms and functional data analysis. As far as we know, few results are available for the moment. In MAR setting, He et al. (2011) have explored this area by developing a functional multiple imputation approach modeling missing longitudinal response under a functional mixed effects model. They developed a Gibbs sampling algorithm to draw model parameters and imputations for missing values. Besides, Ferraty et al. (2013) have considered two kinds of mean estimates of a scalar outcome, based on a sample in which an explanatory variable is observed for every subject while responses are missing (which is the closest to our context). A weak convergence result was proved. In MCAR setting, Preda et al. (2010) have adapted a methodology based on the NIPALS (Nonlinear Iterative Partial Least Squares) algorithm, which provides an imputation method for missing data, which have affected the functional covariates. In MNAR setting, Bugni (2012) adapts a specification test for functional data with the presence of missing observations. His method is able to extract the information available in the observed portion of the data while being agnostic about the nature of the missing observations. In MAR and MCAR setting, Chiou et al. (2014) have recently proposed a nonparametric approach to missing value imputation and outlier detection for functional data. To our knowledge, there is no existing theoretical result in the case of functional linear model under missing assumption operating on the response variable, this problem only being until now the subject of studies in the multivariate framework (see for instance Manski, 1995, Manski, 2003).====We carefully distinguish the missing data problem from a simple prediction problem. Indeed, the missing data mechanism involves a random variable (which indicates whether the response is missing or not) which plays a central role when obtaining our asymptotic results. This random variable and the variable ==== are dependent in the MAR case. This is also highlighted in Ferraty et al. (2013). In this paper, we first propose an imputation method, based on the completely observed cases, to replace missing values in the response of the functional linear model. We get mean square error rates for these imputed values. Secondly, once the database is completed, we are able to estimate the unknown function ==== of the model with the whole sample. This estimator can then be used for predicting other values of the response on a test set.====Combining missing data and functional variables offers a very large field of applications. Among all possible applications, environment is a core issue interesting many people for the future of our planet, in particular in the study of pollution indexes. The dataset we study here deals with temperature curves in some French cities to predict a specific pollution atmospheric index. The atmospheric index is missing in some cities in the northwest of France, for which the corresponding temperature curves (the explanatory variable) are mild, and leads to consider MAR data. The main objective is to get a map of the atmospheric index on the whole French territory.====The rest of the paper is organized as follows. Section 2 introduces the problem of functional linear model under missing assumption operating on the response variable and formulates our main results of the imputation method and of the mean square error for prediction of a new observation using the complete dataset. A simulation study is performed in Section 3. An environmental data illustration is presented in Section 4. Some preliminary lemmas, which are used in the proofs of the main results, are collected in Section 5.",Regression imputation in the functional linear model with missing values in the response,https://www.sciencedirect.com/science/article/pii/S0378375818303586,21 December 2018,2018,Research Article,328.0
"Wang Dongliang,Wu Tong Tong,Zhao Yichuan","Department of Public Health and Preventive Medicine, SUNY Upstate Medical University, United States,Department of Biostatistics and Computational Biology, University of Rochester, United States,Department of Mathematics and Statistics, Georgia State University, United States","Received 29 November 2016, Revised 2 November 2018, Accepted 3 December 2018, Available online 15 December 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.12.001,Cited by (9),The current penalized regression methods for selecting ====.,"The proportional hazards models, also known as the Cox regression models (Cox, 1972, Cox, 1975), are widely used to identify the risk factors given time to event data. Optimal model selection and estimation procedures are desired to correctly select the true risk factors and yield efficient estimators for regression coefficients. The model selection techniques for linear regression models, such as best-subset selection, stepwise deletion, Akaike information criterion (AIC) and Bayesian information criterion (BIC), can be easily extended for the Cox models. Faraggi and Simon (1998) and Ibrahim et al. (1999) discussed Bayesian variable selection and Sauerbrei and Schumacher (1992) proposed a bootstrap procedure for the Cox model. From the vantage point of penalized regression, Tibshirani (1997) and Fan and Li (2002) extended the least absolute shrinkage and selection operator (LASSO) and the smoothly clipped absolute deviation (SCAD) methods, firstly proposed in Tibshirani (1996) and Fan and Li (2001), respectively. Zhang and Lu (2007) investigated the adaptive LASSO method, where the penalty associated with each coefficient is weighted by its magnitude. Both the LASSO and the adaptive LASSO methods have a convex form for the ease of global optimization, but the adaptive LASSO also enjoys the oracle property as the SCAD method did. All the above mentioned penalized likelihoods for the sparse Cox models are defined upon the partial likelihood function. Many other studies were related to penalized likelihood estimations, including Zucker and Karr (1990), Gu (1996) and Goeman (2010). More recently with the motivation to handle ultra high-dimensional data, novel statistical methods for censored data have been developed from a variety of perspectives, such as quantile regression by He et al. (2013), accelerated failure time models by Xia et al. (2016) and survival impact index by Li et al. (2016).====The goal of this paper is to investigate the penalized empirical likelihood method for the Cox model, as a competitor to the penalized partial likelihood. Since empirical likelihood (EL) was proposed by Owen, 1988, Owen, 2001, empirical likelihood methods have been investigated well from many statistical perspectives. With regards to the model selection, Variyath et al. (2010) investigated the properties of the information criteria with the empirical likelihood. Tang and Leng (2010) proposed penalized empirical likelihood methods for the mean vector estimation and linear regression with the divergent number of dimensions, in conjunction with appropriate penalty functions. Leng and Tang (2012) extended their work for more general estimation equations with growing dimensionality. Other work about the impact of growing dimensionality on the inference of empirical likelihood can be found in Hjort et al. (2009), Chen and Van Keilegom (2009), and Chen et al. (2009). For right-censored data, Zhao and Huang (2007) proposed the bias-corrected empirical likelihood for the accelerated failure time model based on estimated influence functions. Motivated by Zhao and Huang (2007), Wu et al. (2015) developed the penalized empirical likelihood method for the censored accelerated failure time model. For the Cox’s proportional hazards model, Hou et al. (2014) investigated the performance of the penalized empirical likelihood estimator via the bridge penalty. Their empirical likelihood was derived with known baseline hazard function using the method in Qin and Jing (2001), which makes their approach not suitable or applicable in practice. In this paper, we aim to develop the penalized empirical likelihood (PEL) method for sparse Cox model from the following two vantage points: (1) a bias-corrected empirical likelihood, proposed in Sun et al. (2009), Zhao and Jinnah (2012) and Zheng and Yu (2013) with a plug-in cumulative baseline hazard function, is evaluated in order to facilitate the application of the method to real data, and; (2) more general requirements regarding the penalty function to maintain the sparsity and asymptotic normality of the coefficient estimators are explored, so that penalty functions such as LASSO and SCAD can be applied with no any concerns.====In the rest of this paper, we study the property of the bias-corrected empirical likelihood in conjunction with appropriate penalty functions for parameter estimation and model selection. In Section 2.1, we define a penalized empirical likelihood after providing some preliminary results about the Cox model. We investigate the asymptotic properties of the penalized empirical likelihood in Section 2.2, and discuss the optimization and tuning procedure in Section 2.3. Results from extensive simulation studies are presented in Section 3 to evaluate the asymptotic properties of the existing methods, the bias-corrected empirical likelihood and penalized empirical likelihood. We illustrate the proposed method by re-analyzing a well-known primary biliary cirrhosis (PBC) data set in Section 3.2, followed by the discussions in Section 4. The proofs of theorems are given in the Appendix.",Penalized empirical likelihood for the sparse Cox regression model,https://www.sciencedirect.com/science/article/pii/S0378375818303550,15 December 2018,2018,Research Article,329.0
"Cao Ming-Xiang,Park Junyong,He Dao-Jiang","School of Mathematics and Statistics, Anhui Normal University, Wuhu 241000, China,Department of Mathematics and Statistics, University of Maryland Baltimore County, 1000 Hilltop Circle, Baltimore, MD 21250, USA","Received 30 October 2017, Revised 2 November 2018, Accepted 6 December 2018, Available online 14 December 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.12.002,Cited by (5),"In this paper, the ","In many contemporary applications, high and ultrahigh dimensional data are increasingly available, such as molecular biology, genomics, fMRI, finance and transcriptomics. A common feature for high and ultrahigh dimensional data is that the data dimension is larger or much larger than the sample size, so called “large ====, small ====” phenomenon where ==== is the data dimension and ==== is the sample size. In high dimensional settings, classical methods may be invalid, or not applicable at all. Hence, there has been a growing interest in developing testing procedures which are better suited to deal with statistical problems in high dimensional settings. Testing hypotheses in high dimension is one of important issues in high dimensional data which has attracted a great deal of attention in recent decades. In two sample testing in high dimension, there have been numerous studies such as Bai and Saranadasa (1996), Srivastava and Du (2008), Srivastava (2009), Srivastava et al. (2013), Chen and Qin (2010), Aoshima and Yata (2011),  Park and Ayyala (2013), Feng et al. (2015), Zhou and Kong (2015), Ma et al. (2015), Ghosh and Biswas (2016) and Zhao and Xu (2016). For multivariate analysis of variance (MANOVA), see Fujikoshi et al. (2004), Schott (2007), Srivastava (2007), Cai and Xia (2014), Cao and Xu (2015) and Zhang et al. (2017). More specifically, when there are ==== groups and ==== represent ==== random samples from the ====th group with unknown mean vector ==== and positive definite covariance matrix ==== for ====, it is of interest to test ====In particular, when all covariance matrices are homogeneous such as ====, testing (1) is known as MANOVA. On the other hand, Cao (2014) and Hu et al. (2017) recently proposed the same test statistic to test (1) when covariance matrices are not necessarily homogeneous. This is also known as the ==== sample Behrens–Fisher (BF) problem which does not require ====. For the ==== sample BF problem, Cao (2014) and Yamada and Himeno (2015) also proposed the same test statistic except for estimators of covariance matrix. The homogeneity of covariance matrices is a strong condition in practice. In fact, it is not straightforward to verify the homogeneity of covariance matrices especially in high dimensional data. Therefore, unless there is any strong evidence supporting the homogeneity of covariance matrices, it is natural to allow different covariance matrices in practice.====The main goal of this paper is to propose a new test statistic in the ==== sample BF problem. It will be shown that the proposed test behaves differently from existing tests such as Hu et al. (2017) and Yamada and Himeno (2015) when sample sizes are unbalanced. We will discuss such differences between the proposed test and the test in Hu et al. (2017) through both theoretical and numerical comparisons under a variety of situations. We observe that the proposed test has some advantage in power compared to Hu et al. (2017) in many cases through theoretical and numerical comparisons. We also demonstrate through numerical studies that the test in Yamada and Himeno (2015) fails in controlling a given level of size in some cases. Overall, our proposed tests have advantage in controlling a size and obtaining power compared to existing test statistics.====The remainder of the paper is organized as follows. Section 2 first presents conditions of statistical model. Some notations used throughout the paper are defined and assumptions are also announced for the theoretical study. In Section 3, we give the new test statistic and investigate its asymptotic behavior under ==== and ====. Theoretical comparisons and numerical studies on the proposed tests and the existing tests are carried out in Section 4. Section 5 gives a real data example. Concluding remarks are presented in Section 6.",A test for the ,https://www.sciencedirect.com/science/article/pii/S0378375818303562,14 December 2018,2018,Research Article,330.0
"Bodnar Taras,Mazur Stepan,Podgórski Krzysztof,Tyrcha Joanna","Department of Mathematics, Stockholm University, SE-10691 Stockholm, Sweden,Department of Statistics, Örebro University School of Business, SE-70182 Ö,rebro, Sweden,Department of Statistics, Lund University, SE-22007 Lund, Sweden","Received 15 December 2017, Revised 12 November 2018, Accepted 12 November 2018, Available online 7 December 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.11.003,Cited by (9),In this paper we derive the finite-sample distribution of the estimated weights of the tangency portfolio when both the population and the ,"The fundamental goal of portfolio theory is to optimally allocate investments between different assets. The mean–variance optimization is a quantitative tool which allows the making of this allocation through considering the trade-off between the risk of portfolio and its return.====The foundations of modern portfolio theory have been developed in Markowitz (1952), where a mean–variance portfolio optimization procedure was introduced. In this, investors incorporate their preferences towards the risk and the expectation of return to seek the ‘best’ allocation of wealth. This means the portfolios are selected to maximize anticipated profit subject to achieving a specified level of risk or, equivalently, to minimize variance subject to achieving a specified level of expected gain. The risk aversion strategy in the absence of risk free assets (bonds) leads to the minimal variance portfolio. This has to be changed in the presence of bonds and the tangency portfolio is a component for a portfolio that hedges stocks with bond investment or, for higher returns levels, borrows to invest in the stocks. Nowadays, the mean–variance analysis of Markowitz remains an important though basic tool for both practitioners and researchers in the financial sector. Therefore, having a complete understanding of the tangency portfolio properties under all realistic conditions is of great importance for any financial strategist.====To implement these optimal portfolios in practice, the inverse of covariance matrix of asset returns needs to be estimated. Traditionally, the sample covariance matrix has been used for this purpose under the assumption of non-singular true (population) covariance matrix. However, the problem of potential multicollinearity and strong correlations of asset returns results in clear limitations in taking such an approach due to latent singularity or near singularity of the population covariance. In addition, the number of assets in a portfolio can be larger than the number of observations and creates another cause for singularity. There is a wide literature concerning cases with very high-dimensional datasets and a small sample size relative to dimension, see Pruzek (1994) and Stein et al. (1972) among many others. However, consequences of this particular problem for the portfolio theory have not been addressed until recently. Singular population covariance matrix and small sample size relative to the portfolio size were first discussed in Bodnar et al. (2016) and later results were extended in Bodnar et al. (2017). In the last paper, the authors analyzed the ==== for small sample and singular covariance matrix. Here we tackle properties of the ==== (TP) under these two singularity conditions. We derive a stochastic representation of the tangency portfolio weights estimator as well as a linear test for the portfolio weights. We also establish the asymptotic distribution of the estimated portfolio weights under a high-dimensional asymptotic regime. Theoretical results are applied to an illustrative example based on 440 actual weekly stock returns.====We conclude this introduction with pointing out importance and some difficulties in practical handling of the singularity and small samples size problem. Firstly, the singularity of the data typically cannot be observed due to observational or computational noise. As far as the authors are aware, a detection problem or formal statistical tests for singularity have not been thoroughly studied in the literature and deserve a separate treatment. Secondly, the issue of non-normality and time dependence in the singular data will affect the accuracy of the results based on the normality assumption. Unfortunately, we are not aware of any statistical test for normality under the singularity assumption and developing a methodology for such a test also deserves a separate study. Here, we heuristically assume that averaging over time blocks that are long enough brings us to independence and normality of the data. We estimate the rank of the covariance matrix using a test developed by Nadakuditi and Eldeman (2008), where the singularity problem was discussed from the signal processes perspective. We use the method to verify that the rank of the covariance matrix is less than the portfolio size.====The rest of the paper is structured as follows. In Section 2, we discuss the problem of the estimation of the TP weights and derive a very useful stochastic representation for the estimated weights of this portfolio which fully characterizes their distribution. This stochastic representation is used to establish the distributional properties of the estimated TP weights, including their mean vector and covariance matrix in Section 2.2 and in the derivation of a statistical test on the TP weights in Section 2.3. In Section 3, the high-dimensional asymptotic distribution of the estimated weights is present and a high-dimensional asymptotic test on the TP weights is derived. In Section 4, the results of the simulation study are presented, while the empirical findings are summarized in Section 5. Several additional theoretical results are formulated and proved in the supplementary material.",Tangency portfolio weights for singular covariance matrix in small and large dimensions: Estimation and test theory,https://www.sciencedirect.com/science/article/pii/S0378375818303458,7 December 2018,2018,Research Article,331.0
"Dong Junyi,Yu Qiqing","Math and Statistics Department, St Ambrose University, Davenport, IA 52803, United States,Department of Mathematical Sciences, SUNY, Binghamton, NY 13902, United States","Received 5 July 2018, Revised 15 November 2018, Accepted 17 November 2018, Available online 23 November 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.11.004,Cited by (2),"Let ==== be the response variable with the joint cumulative distribution function ====. Given a random sample from ====, in order to analyze the data based on a certain proportional hazards (PH) model, say ====, one needs to test the null hypothesis ====: ==== first. The existing tests to achieve this task make use of the residuals and are invalid in certain situations, such as when ==== is not from any PH model. To overcome this disadvantage, we propose a valid model checking test of ====. This test is called the marginal distribution (MD) test. We give the theoretical justification of the MD test. The simulation study suggests that the MD test is always valid, whereas the existing tests may be invalid and they are often unlikely to reject the wrong PH model assumption when they are not valid.","An important step in regression analysis is to check whether a data set fits the selected regression model; otherwise, the analysis is meaningless. In this paper, we focus on the test of the proportional hazards (PH) model assumption (Cox, 1972). We propose a class of new model checking tests, called the marginal distribution (MD) test. The MD test is an extension of the MD plot proposed by Yu et al. (2017). Like the MD plot, the MD test actually is applicable to most common regression models including, among others, the PH model, the linear regression model, and the generalized linear model.====Let ==== be the response variable and ==== be the ==== covariate vector. Let ==== be a censoring variable which is independent of ====. Let ==== and ====. Let ====, i ==== 1, …, n, denote a right-censored (RC) random sample from ====. A common estimator of ==== based on the RC observations is the non-parametric maximum likelihood estimator (NPMLE), which is also called the Kaplan–Meier estimator (KME). Let ==== denote the joint cumulative distribution function (CDF) of ====. Let ====, ====, and ==== denote the conditional CDF, the conditional survival function, and the conditional hazard function of ==== given ====, respectively. Let ==== be the baseline survival function.====The PH model specifies that the conditional hazard function ====where ==== is an unknown baseline hazard function, ==== and ==== is a ==== unknown coefficient vector (Cox, 1972). An extension to Model (1) is the model with time-dependent covariates and/or time-varying regression coefficients (see, for example, Grambsch and Therneau, 1994). The PH model with time-dependent covariates replaces the time-independent covariate ==== in (1) by the value of the covariate at time ====, ====. The PH model with time-varying coefficients assumes that the regression coefficient is a function of time ====, that is, ====.====For the time-dependent covariate PH (TDPH) model, Kalbfleisch and Prentice (1980) further distinguish the external time-dependent covariates from the internal ones. In this paper, for simplicity, we mainly focus on a class of the PH models with the external time-dependent covariate ====Let ==== be the collection of all possible joint CDFs ====. To test a PH model assumption, let ==== be a PH model specified by (2) with a given covariate ==== and an unknown coefficient parameter ====. Ideally, a test of a model ==== should set the null hypothesis and the alternative hypothesis as ====However, for the convenience in deriving the rejection region of the test, all existing tests of the PH model do not set ==== as in (3) (Therneau and Grambsch, 1990).====Residuals are commonly used to test the PH model assumption. For example, Therneau and Grambsch (1990) proposed a test based on the weighted Schoenfeld residuals and Lin et al. (1993) suggested to test a PH model assumption by the cumulative sum of the Martingale residuals. For instance, Grambsch and Therneau (1994) proposed a goodness of fit test based on weighted residuals. Their residuals method defines another parameter space ====, where ==== is a given ==== diagonal matrix with diagonal elements ====, ====, and ==== is a ==== unknown coefficient vector. This method can be used to test the null hypothesis as in (3) against the alternative hypothesis ==== (Grambsch and Therneau, 1994). More details will be given in Section 2 (see, for instance, (4) and (5)). Let ==== be a statistic based on the residuals method. As summarized in Therneau and Grambsch (1990), “Interestingly, nearly all of the tests for proportional hazards that have been proposed in the literature are ==== tests (as in (6.3) see p.132) and differ only in their choice of the time transform G” (here ==== and ====). That is, the existing residuals methods all need to impose the condition that ==== and test ==== vs. ====. If ==== does not belong to ====, then the residuals methods are invalid because the derived distribution of the test statistic based on ==== is no longer true. When the residuals methods are invalid, our simulation study suggests these tests often do not reject the wrong null hypothesis ==== with probability (w.p.) greater than ====.====In this paper, we propose a new method to test the null hypothesis in (3). This test makes use of the marginal distribution of the response variable ==== and, therefore, is called the marginal distribution test or the MD test. The MD test allows ==== or ==== to be arbitrary, thus it is the first valid test to test the hypothesis in (3) when ==== specifies a PH model.====This paper is organized as follows. In Section 2, we discuss the drawbacks of the existing residuals tests in details. In Section 3, we explain the MD test for testing the model assumption in ====. In Section 4 we provide simulation results to compare the MD test to the existing residuals method and in Section 5 we give the application of the MD test to the real-world data set. Conclusions are given in Section 6.",Marginal distribution test for checking proportional hazards model assumption,https://www.sciencedirect.com/science/article/pii/S0378375818301241,23 November 2018,2018,Research Article,332.0
Katayama Shota,"Department of Industrial Engineering and Economics, Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo 152-8550, Japan","Received 12 May 2017, Revised 4 November 2018, Accepted 7 November 2018, Available online 16 November 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.11.001,Cited by (2)," is not robust against outliers since it involves the least squares. To handle outliers, a two-stage procedure is proposed; at the first stage an initial estimator is calculated and then it is improved at the second stage by iteratively solving a sparse regression problem with reducing outlier effects. This procedure includes not only a random error but also a computational error. The convergence performance for the final estimator is investigated in both computational and statistical perspectives.","Accurate estimation for coefficients is a fundamental problem in recent high-dimensional linear regression analysis. Sparse regularizations such as least absolute shrinkage and selection operator (Lasso; Tibshirani, 1996), smoothly clipped absolute deviation (SCAD; Fan, 1997) and minimax concave penalty (MCP; Zhang, 2010b) achieve success in this problem. However, the usual regression with such regularizations has poor accuracy when data are corrupted by some outliers since the squared ==== norm used for fitting the data to the model is sensitive to the outliers. The effect of outliers needs to be reduced or excluded to attain accurate estimates.====In this paper, the linear regression model with outliers is considered as ====where ==== is a response vector, ==== is an ==== explanatory matrix with ==== element ==== and ====th row vector ====, ==== is a coefficient vector of interest, ==== is a vector corresponding to outliers and ==== is an error vector which is drawn form zero-mean sub-Gaussian distribution with a parameter ====, that is, ====for all ==== and ====. For properties of sub-Gaussian distribution, see for example Vershynin (2012). Each column of ==== is scaled to have ==== norm of ====. In order to match the orders of the coefficients of ==== with those of ====, we set the coefficient of ==== to ====. The model (1.1) is also studied by She and Owen (2011), Nguyen and Tran (2013), Witten (2013) and Katayama and Fujisawa (2017). The goal is to obtain an accurate estimates of ====.====For high-dimensional data, it is naturally assumed that the coefficient vector ==== is sparse since only a few explanatory variables would be relevant to the responses in many practical situations. Furthermore, ==== would be sparse since its non-zero elements mean “outliers”. Under these sparsity constraints, a two-stage procedure to obtain an estimate of ==== is proposed in this paper. The first stage roughly estimates ==== and ==== by Lasso regularization to obtain an initial estimate for ====. The second stage improves it by iteratively solving a sparse regression problem with reducing the outlier effects. At that stage, various regularizations for ==== such as Lasso, SCAD and MCP can be used.====The first stage adopts an idea from Nguyen and Tran (2013), where both ==== and ==== are regularized by Lasso penalty. They have provided an error rate of resulting estimator. The main contributions of this paper are in the second stage, which are the followings:====Most statistical theories for high-dimensional regression are depending on the symbol “argmin”, and hence ignoring an optimization process inside it. Thus, evaluating computational and statistical errors simultaneously becomes to be an important topic. Recently, without outliers, that is, ==== in (1.1), Zhang (2010a), Agarwal et al. (2012), Wang et al. (2014), Loh and Wainwright (2015) and Loh (2017) have studied it. Katayama and Fujisawa (2017) have accommodated outliers with allowing non-convex regularization (thresholding), but only dealt with Lasso regularization for the coefficients and partially relied on the “argmin” symbol. This paper extends their results to include non-convex regularizations for the coefficients without total reliance on the “argmin” symbol. We provide a unified estimation error on various regularizations for coefficients and outliers. It should be noted that She and Owen (2011) have considered an optimization algorithm to obtain ==== and ==== in (1.1) with allowing non-convex penalties for them, but no error rates have been considered.====Throughout the paper, for any set ====, we define the cardinality as ==== and define the complement as ====. For any vector ====, the ==== norm is defined as ==== and the ==== and ==== norms are defined as ==== and ====, respectively. Given a set ====, we denote ==== and for any matrix ====, we denote ==== and ====. We use the symbol ==== with or without a subscript to denote a constant whose value may vary with situations. The sentence “with high probability” means that a random variable satisfies a property with probability going to one as the sample size and/or the dimension increase.",Computational and statistical analyses for robust non-convex sparse regularized regression problem,https://www.sciencedirect.com/science/article/pii/S0378375818303434,16 November 2018,2018,Research Article,333.0
"Kalyagin Valery A.,Koldanov Alexander P.,Koldanov Petr A.,Pardalos Panos M.","National Research University Higher School of Economics, Laboratory of Algorithms and Technologies for Network Analysis, Bolshaya Pecherskaya 25/12, Nizhny Novgorod, Russia,University of Florida, Department of System Engineering, 401 Weil Hall, P.O. Box 116595, Gainesville, FL 32611-6595, USA","Received 29 May 2018, Revised 5 October 2018, Accepted 7 November 2018, Available online 16 November 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.11.002,Cited by (1),A Gaussian ,"A Gaussian graphical model graphically represents the dependence structure of a Gaussian random vector. For undirected graphical model this dependence structure is associated with simple undirected graph. This graph is given by its adjacency matrix, which is symmetric matrix with ==== entries, where zero means conditional independence and one means conditional dependence of random variables. It has been found to be an effective method in different applied fields such as bioinformatics, error-control codes, speech and language recognition, and information retrieval (Jordan, 2004, Wainwright and Jordan, 2008). One of the main question in graphical models is how to recover the structure of a Gaussian graph from observations and what are statistical properties of associated algorithms. This problem is called the Gaussian graphical model selection problem (GGMS). A comprehensive survey of different approaches to this problem is given in Drton and Perlman, 2007, Drton and Maathuis, 2017.====One of the first approach to GGMS for undirected graphs was based on covariance selection procedures (Dempster, 1972, Edwards, 2000). GGMS problem is still popular in our days (Meinshausen and Bühlmann, 2006, Khare et al., 2015, Liang et al., 2015, Peng et al., 2009, Rajaratnam et al., 2008, Ren et al., 2015). Several selection algorithms are used for large dimensional graphical models. Some approaches to GGMS are related with multiple hypotheses testing (Drton and Perlman, 2004, Drton and Perlman, 2007). Measures of quality in multiple testing include the FWER (family wise error rate), k-FWER, FDR (false discovery rate), and FDP (false discovery proportion) (Lehmann and Romano, 2005). Procedures with control of such type errors for GGMS are investigated in Drton and Perlman (2007). However, general statistical properties such as unbiasedeness and optimality for a finite sample size these procedures are not well known.====A procedure for GGMS is assessed by counting the errors of two types: false edge inclusions and false edge exclusions. In this paper we consider the GGMS problem within the framework of multiple decision theory initiated by Lehmann (1957). We measure the quality of statistical procedures using a risk function with additive losses, which is a simple way to take into account both types of error. A procedure is said to be optimal in a class of procedures if it has the smallest risk in this class. In this paper we are looking for procedures that are optimal. We restrict our attention to procedures that are unbiased. We use a general concept of unbiasedeness which is related with the loss function. We combine the tests of a Neyman structure for individual hypotheses with simultaneous inference and prove that the obtained multiple decision procedure is optimal in the class of unbiased procedures.====The paper is organized as follows. In Section 2 we present basic definitions and notation. Section 3 describes the multiple decision framework for the GGMS problem and gives the representation for the risk function. In Section 5, we construct the uniformly most powerful unbiased (UMPU) tests of a Neyman structure for individual hypotheses, and in Section 6, we combine individual tests in a multiple decision procedure and prove its optimality. Finally, in Section 7, we discuss the obtained results.","Loss function, unbiasedness, and optimality of Gaussian graphical model selection",https://www.sciencedirect.com/science/article/pii/S0378375818303446,16 November 2018,2018,Research Article,334.0
"Son Won,Lim Johan","The Bank of Korea, 67, Sejong-daero, Jung-gu, Seoul, 04514, Republic of Korea,Department of Statistics, Seoul National University, 1, Gwanak-ro, Gwanak-gu, Seoul, 08826, Republic of Korea","Received 24 October 2017, Revised 14 June 2018, Accepted 7 October 2018, Available online 25 October 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.10.003,Cited by (2),"The path algorithm of the fused lasso signal approximator is known to fail in finding change points when monotonically increasing or decreasing blocks exist in the mean vector. In this paper, we first understand why the standard path algorithm by Hoefling (2010) fails in the primal optimization problem. We then propose a modified path algorithm for the consistent recovery of the change points and study its properties theoretically and numerically.","In sequential data, occasional abrupt changes are observed. These sudden transitions may occur in the mean level, slope, volatility or anywhere else in the distribution. In this paper, we focus on detecting multiple change points in mean values and propose a modified path algorithm for fused lasso signal approximator (FLSA). The proposed algorithm guarantees consistent recovery of true change points under the assumption of diminishing noise level.",Modified path algorithm of fused Lasso signal approximator for consistent recovery of change points,https://www.sciencedirect.com/science/article/pii/S0378375818303380,May 2019,2019,Research Article,335.0
"Golzy Mojgan,Carter Randy L.","Biostatistics and Research Unit, School of Medicine, University of Missouri, Columbia, MO, United States,Department of Biostatistics, State University of New York at Buffalo, NY, United States","Received 2 January 2018, Revised 4 October 2018, Accepted 5 October 2018, Available online 21 October 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.10.002,Cited by (1),"In this paper, we propose an extension of the frailty modeling to analyze the recurrent events data. We proposed a class of models, based on nonlinear mixed effects modeling, that takes into consideration of the between subject heterogeneity in the model. We propose an estimation method for estimating parameters of the proposed model and their standard deviations. The estimating equations are shown to give consistent estimates under commonly satisfied ","Recurrent events arise quite frequently in many disciplines such as biomedical (recurrence of hospitalization, tumors, epileptic seizures, etc.), public health (recurrence of an infectious type of disease, influenza, etc.), geology (the next hurricane, earthquake, etc.), economics (successive recessions, drop in the stock market by some percentage point, etc.), and engineering (failure of a mechanical system). Because of their prevalence in wide areas of disciplines, it is important to develop statistical models for analyzing their recurrent behavior. In this section, we introduce the basic tools for the analysis of recurrent event.",Generalized frailty models for analysis of recurrent events,https://www.sciencedirect.com/science/article/pii/S0378375818303355,May 2019,2019,Research Article,336.0
"Pumi Guilherme,Valk Marcio,Bisognin Cleber,Bayer Fábio Mariano","Mathematics and Statistics Institute, Universidade Federal do Rio Grande do Sul, 9500, Bento Gonçalves Avenue, 91509-900, Porto Alegre, RS, Brazil,Departamento de Estatística and LACESM, Universidade Federal de Santa Maria, Santa Maria, RS, Brazil","Received 2 October 2017, Revised 13 April 2018, Accepted 3 October 2018, Available online 10 October 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.10.001,Cited by (21),In this work we introduce the class of beta autoregressive fractionally integrated moving average models for continuous random variables taking values in the continuous unit interval ,"In this work we are interested in time series whose values are restrained to a continuous interval of the real line, say ====, where ====. Without loss of generality we consider series in the unit interval ====. One typical broad case is when the time series represent rates and proportions observed over time. Building over the works of Zeger and Qaqish (1988), Benjamin et al. (2003) and Ferrari and Cribari-Neto (2004), Rocha and Cribari-Neto (2009) introduces the class of beta autoregressive moving average models (====ARMA), which can be viewed as a specialization of the generalized autoregressive moving average models (GARMA) (Benjamin et al., 2003) for beta distributed variates. Applications of the ====ARMA model spam over several areas, such as medicine (Zou et al., 2010), online monitoring (Guolo and Varin, 2014), neuroscience (Wang, 2012), among many others.====Let ==== be a process of interest and, aiming towards prediction and the use of partial likelihood inference, let ==== denote the ====-dimensional vector of (exogenous random) covariates at time ==== and any non-random component up to time ====, to be considered in the model (a possible intercept will be considered in the model separately). Let ==== denote the ====-field generated by the past and present (when known) explanatory variables and possibly past values of the response variable, if they are included in the model. In this framework, the ====-field ==== represents all the observer’s knowledge about the model up to time ====, with a possible addition of predetermined variables at time ====.====Inference in the context of ====ARMA process (Rocha and Cribari-Neto, 2009) is conducted using a conditional likelihood approach, which only allows for deterministic covariates to be introduced in the model. In this work, we adopt the more general approach of partial likelihood, which allows for ==== to contain deterministic covariates, as in the conditional likelihood approach, but also enables the inclusion of (time dependent) random covariates, as well as any type of interaction or a mixture of these. For further details on partial likelihood inference we refer the reader to Cox (1975), Wong (1986) and Jacod, 1987, Jacod, 1990. For details on partial likelihood in time series following generalized linear models, we refer to Fokianos and Kedem (2004) and Kedem and Fokianos (2002) and references therein.====This work is concerned with an observation-driven model in which the random component followsa conditional beta distribution, parameterized as (Ferrari and Cribari-Neto, 2004): ====for ====, ====, and ====, where ==== and ====. We note that ==== is a precision parameter in the sense that the greater the ====, the smaller the variance of the distribution. The systematic component follows the usual approach of GLM with an additional dynamic term. Let ==== be a twice differentiable monotonic one-to-one link function for which the inverse link is of class ==== (the class of twice continuously differentiable functions in ====). Consider the additive specification ====where ==== are the coefficient related to the covariates and ==== is the linear predictor. The particular form of ==== is discussed in several papers Benjamin et al., 2003, Rocha and Cribari-Neto, 2009, Fokianos and Kedem, 2004. In ====ARMA models (Rocha and Cribari-Neto, 2009), ==== is assumed to follow an ARMA==== process of the type ====where ==== and ==== and ==== and ==== denote the order and coefficients of the autoregressive and moving average parts of the model, respectively, and ==== denotes an error term. When ==== is non-random, this is the ====ARMA model of Rocha and Cribari-Neto (2009).====An advantage of specification (2) is that even though the conditional mean is transformed, it is actually ==== that is being modeled. In some applications it is common to model ==== and then transform estimates back by applying ==== which can be problematic (Jensen’s inequality, delta method, etc.). Observe that the time series part of the ====ARMA model can only accommodate short range dependence, structure that may not be enough in certain situations. In this work we propose a generalization of the ====ARMA model of Rocha and Cribari-Neto (2009) by allowing ==== to accommodate long-range dependence.",Beta autoregressive fractionally integrated moving average models,https://www.sciencedirect.com/science/article/pii/S0378375818303288,May 2019,2019,Research Article,337.0
"Cai Quan,Wang Suojin","Department of Statistics, Texas A&M University, College Station, TX, USA","Received 3 June 2017, Revised 18 December 2017, Accepted 20 September 2018, Available online 4 October 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.011,Cited by (3),We study generalized partially linear single-index models for ==== in this article. We propose a method to efficiently estimate both the parameters and the nonparametric single-index function in generalized partially linear single-index models when subjects are observed or measured over time. The proposed estimation approach is more flexible and more general in that we can model both categorical response and transformation-necessary response such as heavy-tailed variable with multiple ==== results and an empirical data analysis that support our new method.,"Semiparametric models are flexible in statistical modeling with their advantages of incorporating both the parametric and nonparametric components. It is popular in economics, biomedical science and many other research fields. The generalized partially linear single-index model (GPLSIM) is one of semiparametric models proposed by Carroll et al. (1997). Suppose we have an univariate response variable ==== and possibly multi-dimensional covariates ==== and ====. GPLSIM has the form ====where ==== and ==== are possibly multi-dimensional parameters associated with predictors ==== and ==== respectively, ==== and ==== are the realizations of ==== and ==== respectively, and ==== is an unknown function, referred to as the single-index function hereafter. Besides, ==== is assumed to be a known monotonic and differentiable link function. The goal is to estimate the parameters ==== and ==== and the single-index function ==== in (1). GPLSIM is the generalized model of several types of models. When the single-index function is the identity function, it becomes the generalized linear model (Nelder and Baker, 2004). When there are no covariates ====, it becomes the generalized single-index model (Ichimura, 1993). When ==== is one-dimensional, it becomes the generalized partially linear model (Chen and Shiau, 1994). When the link function ==== is the identity function and ==== is continuous, it becomes the partially linear single-index model Carroll et al., 1997, Chen et al., 2015. Therefore, efficient estimation of GLSIM is of major interest in that it can unify the estimation of several important models above and has broad applications.====Partially linear single-index models (PLSIM) have been studied extensively for independent and identically distributed (i.i.d.) data. In the i.i.d. case, researchers have proposed a variety of methods for the mean function estimation. Carroll et al. (1997) proposed PLSIM and GPLSIM and the maximum quasi-likelihood method to estimate the parameters as well as the single-index function. Yu and Ruppert (2002) proposed the penalized spline estimation approach for PLSIM. Zhu and Xue (2006) provided the empirical likelihood confidence regions for parameters of this model. To extend the work of Liang and Zeger (1986) on generalized linear models for longitudinal data, Liang et al. (2010) proposed the profile estimation method for the parameters and single-index function for PLSIM and obtained the semiparametrically efficient parameter estimators. Xia et al. (1999) and Xia and Härdle (2006) studied the theoretical properties of estimators and extended the conditions for estimating PLSIM.====However, there is limited research on GPLSIM for dependent or correlated data, especially in the case of longitudinal/clustered data. There is some related literature discussing the problem of mean function estimation in longitudinal data. Lin and Carroll, 2000, Lin and Carroll, 2001 studied nonparametric function estimation for clustered data under various settings. They showed that for the profile-kernel GEE method, the estimated parameters in partially linear models are not semiparametrically efficient and the most efficient estimators are obtained by ignoring the dependence and undersmoothing the nonparametric function. Wang (2003) proposed the marginal kernel generalized estimation equation (GEE) method for the mean function in nonparametric regression to account for within-subject correlation. Wang et al. (2005) extended this method to generalized partially linear models and showed that the parameter estimators reach the semiparametric efficiency bound under some mild conditions. Huang et al. (2007) proposed spline-based additive models for partially linear models and Cheng et al. (2014) extended the results to generalized partially linear additive models. They also showed that the parameter estimators are semiparametrically efficient. Li et al. (2010) proposed a bias-corrected block empirical likelihood inference for partially linear single-index models and provided confidence regions for the parameters. Chen et al. (2015) proposed a unified semiparametric GEE analysis for partially linear single-index models for both sparse and dense longitudinal data. Cai and Wang (2018) discussed how to efficiently estimate the parameters and the single-index function in partially linear single-index models for longitudinal data. The main objective of this paper is to extend the work of Cai and Wang (2018) to GPLSIM and propose efficient estimators in a generalized approach.====The remainder of the paper is organized as follows. In Section 2, the main problem is defined and the estimation method is introduced. In Section 3, the main theoretical results are given whose proofs are given in the Appendix. Simulation studies are performed in Section 4 to demonstrate the methodology and a real data analysis is given in Section 5 to apply the proposed method. Some concluding remarks are given in Section 6.",Inferences with generalized partially linear single-index models for longitudinal data,https://www.sciencedirect.com/science/article/pii/S0378375818303008,May 2019,2019,Research Article,338.0
"Hansmann Matthias,Kohler Michael","Fachbereich Mathematik, Technische Universität Darmstadt, Schlossgartenstr. 7, 64289 Darmstadt, Germany","Received 8 August 2017, Revised 23 August 2018, Accepted 21 September 2018, Available online 3 October 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.013,Cited by (1),Motivated by an application in the context of experimental fatigue tests we study the problem of estimating conditional ,"The main goal of the paper is to consider conditional quantile estimation from data that contains additional measurement errors in the dependent variable that arise from an empirical application in the context of experimental fatigue tests. Instead of starting with a formal description of the problem, we will first present a motivating example and continue with the mathematical setting afterwards.",Estimation of conditional quantiles from data with additional measurement errors,https://www.sciencedirect.com/science/article/pii/S0378375818303070,May 2019,2019,Research Article,339.0
"Zhao Yanyan,Zou Changliang,Wang Zhaojun","Institute of Statistics and LPMC, Nankai University, China","Received 2 January 2017, Revised 2 August 2018, Accepted 21 September 2018, Available online 3 October 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.012,Cited by (1),"Lack-of-fit checking for ==== is essential in reducing misspecification. However, for massive data sets which are increasingly prevalent, classical tests become prohibitively costly in computation and their feasibility is questionable even with modern parallel computing platforms. Building on the ==== strategy, we propose a new nonparametric testing method, that is fast to compute and easy to implement with only one tuning parameter determined by a given time budget. Under mild conditions, we show that the proposed test statistic is asymptotically equivalent to that based on the whole data. Benefiting from using the sample-splitting idea for choosing the smoothing parameter, the proposed test is able to retain the type-I error rate pretty well with ==== and achieves adaptive rate-optimal detection properties. Its advantage relative to existing methods is also demonstrated in numerical simulations and a data illustration.","Regression problems containing from tens of thousands to millions of response observations are now commonplace. Sometimes such large data sets require completely new modeling approaches, but sometimes existing model classes are appropriate, provided that they can be made computationally feasible. Although our computing power has also been advancing steadily, the surge of massive data presents challenges to statisticians in terms of data storage, computation and statistical analysis. This paper considers the problem of checking the agreement between a pre-specified parametric model and massive data sets, using scalable nonparametric tests, and in the context in which smoothing parameters must be determined as part of model testing.====Suppose we have a sequence of independent observations ==== coming from a population ====, in which the unknown regression function ==== is assumed to be smooth. To justify the use of a parametric model, a specification test on the functional form of the regression is needed. Given a parametric family of known real functions ====, the null and alternative hypotheses can be described as ====where ==== denotes the parameter space. With any prediction method, an inextricably associated problem is to assess the validity of a given model for an observed data, and this is particularly important for the statistical analysis of massive data sets. As mentioned by Fan et al. (2014), massive data sets are often collected from multiple sources by different experimental methods. A natural next step is to check whether a parametric model is appropriate for describing the data sets with different sources. If the hypotheses are not rejected, the processing procedure for model fitting can be simplified by using some aggregation mechanism (e.g. Lin and Xi, 2011).====This hypothesis testing problem has been widely studied in the literatures. An intuitive approach is based on generalizations of the Kolmogorov–Smirnov or Cramer–von Mises statistics to measure the distance between the estimators under the null and alternative models, see Härdle and Mammen (1993), Fan et al. (2001), Neumeyer and Van Keilegom (2010), González-Manteiga and Crujeiras (2013) and the references therein. Delgado (1993), Kulasekera, 1995, Kulasekera and Wang, 1997, Neumeyer and Dette (2003) and Pardo-Fernandez et al. (2007) considered the use of empirical processes. Dette (1999) considered the approach based on differences of variance estimators. Stute (1997) and Van Keilegom et al. (2008) proposed methods based on the residual. Zheng (1996) proposed a consistent test of functional nonlinear regression models by combining the methodology of the conditional moment test (Bierens, 1990) and nonparametric estimation techniques. There is a growing consensus that an optimal measure does not exist unless a specific alternative is given. In this paper, we focus on smoothing-based tests on account of their advantages compared to tests based on empirical processes under those “singular” local alternatives, refer to Fan and Li (2000) for detailed discussion. Zheng (1996)’s test statistic is preferred since it presents no asymptotic bias. See Zhang and Dette (2004) for a power comparison of some types of nonparametric regression tests.====Current testing methods for (1) are reasonably efficient and robust when applied to data sets containing up to a few tens of thousands of observations. However, these methods usually tend to become too computation or memory intensive and much beyond this point, so larger data sets that contain hundreds of thousands or millions of data points are out of reach. There are several difficulties associated with this task. First, many existing test methods are in quadratic forms Härdle and Mammen, 1993, Zheng, 1996, then the computational complexity is typically ====, which can become too large to handle. Second, due to the computation barriers in massive data, the asymptotic conclusions based on the whole data sets are not tried and true ways for inference. Some recent computationally scalable methods are able to alleviate this issue to certain degree based on resampling methods Kleiner et al., 2015, Wang et al., 2018. But we should try our best to avoid imposing unnecessary demands on the computation without power gain. Third, the performance of smoothing-based tests depends upon the choice of smoothing parameters, i.e., the bandwidth in kernel smoothing. It is widely acknowledged that the optimal parameter for nonparametric estimation is generally not optimal for testing (Hart, 1997). Selection of smoothing parameters for optimal power is an open problem; see Kulasekera and Wang (1997) and Gao and Gijbels (2008) for choosing the bandwidth through maximizing power function, Zhang (2004) for cross-validation-based selection, and Horowitz and Spokoiny (2001) and Guerre and Lavergne (2005) for a hybrid method that combined a sequence of values of the smoothing parameter. From a statistical point of view, there is a great need for new methods that are theoretically sound and remain computationally feasible even for massive data sets.====Here we shall show how simple strategies for constructing test statistics can be used to avoid intensive computations in the model checking context. More importantly, we show how to adapt smoothing parameter selection in this setting. Our technique developed in this paper is based on the combination “divide-and-conquer” and sample-splitting strategies (Wasserman and Roeder, 2009). To address the computational issue associated with massive data sets, we first partition the massive data sets into ==== subsets, each having the same sample size ====, while ==== varies with ====. A test statistic is built based on each subset, then we aggregate the ==== statistics by taking an average. This approach solves insurmountable task separately on different processors and then combines the results to the sub-problems to get the final solution to the original problem. Meanwhile, this strategy can reduce the computing time mainly because sub-problems are processed in parallel. The split and conquer approach is intuitive, and has been widely used in many fields relevant to different problems. For general nonlinear estimating equations, Lin and Xi (2011) proposed a linear approximation of the estimating equations with the Taylor expansion at the solution in each subset and combined the results to all subsets via a weighted average. Zaremba et al. (2013) used the idea to solve two sample test problems. When either the sample size or dimension is large, Chen and Xie (2014) applied the divide-and-conquer method to select variables in generalized linear models. Battey et al. (2018) combined the divide-and-conquer algorithm with high dimensional hypothesis testing and estimation. Zhang et al. (2013), Zhao et al. (2016) and Xu et al. (2016) considered the divide-and-conquer algorithm as an approach in nonparametric regression function estimation for massive data. For lack-of-fit test in big data, Zhao et al. (2017) integrated split and conquer approach into Zheng (1996)’s nonparametric test statistic as well as a data-driven bandwidth selection procedure. However, the integration approach therein can lead to type-I error inflation easily. To address the issues of determining the smoothing parameters and maintaining the type-I error, we suggest to randomly split the observations into two halves. In the first half of observations we find an “optimal” smoothing parameter by a simple criterion. In the second half we perform a lack-of-fit test with asymptotic theories. This data-splitting strategy helps us retain the type-I error rate pretty well.====A key question is how the performance of the proposed procedure is, when compared to the existing tests using the entire data set all at once. We focus on statistical issues such as convergence rate and asymptotic behaviors. We show in theory that our test is adaptive rate-optimal, i.e., it is able to detect alternatives of unknown smoothness approaching the null hypothesis at the fastest possible rate when the sample size grows (see Spokoiny (1996)). Simulation studies validate the theory and show that our method is computationally very efficient. The remainder of this article is organized as follows. In Section 2, we present the new test statistic and its theoretical properties. Section 3 provides extensive simulation studies and Section 4 contains a real data example as an illustration. Section 5 concludes with some remarks, and theoretical proofs are delineated in the Appendix. Some technical details and additional numerical studies are provided in the Supplementary Material.",A scalable nonparametric specification testing for massive data,https://www.sciencedirect.com/science/article/pii/S0378375818303069,May 2019,2019,Research Article,340.0
"Sabzikar Farzad,McLeod A. Ian,Meerschaert Mark M.","Department of Statistics, Iowa State University, Ames, IA 50011, United States,Department of Statistics, Western University, Canada,Department of Statistics and Probability, Michigan State University, East Lansing, MI 48823, United States","Received 5 March 2018, Revised 9 August 2018, Accepted 17 September 2018, Available online 26 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.010,Cited by (19),"The ARTFIMA model applies a tempered fractional difference to the standard ARMA time series. This paper develops parameter estimation methods for the ARTFIMA model, and demonstrates a new R package. Several examples illustrate the utility of the method.","The autoregressive tempered fractionally integrated moving average (ARTFIMA) time series model was introduced in Meerschaert et al. (2014), see also Sabzikar et al. (2014). The ARTFIMA model extends the TFI model from Giraitis et al. (2000), who note that tempered fractionally integrated times series exhibit ====: Their covariance function resembles long range dependence for a number of lags, depending on the tempering parameter, but eventually decays exponentially fast. In this paper, we develop the mathematical foundation for ARTFIMA parameter estimation, and present a new R package ==== to fit data. Several examples from finance, geophysics, turbulence, and climate illustrate the fitting procedure, and the utility of the ARTFIMA model.====There are three reasons to consider the ARTFIMA model. The first is mathematical. The ARFIMA model without tempering is challenging to analyze, since the covariance function is not summable. The ARTFIMA model, however, has a summable covariance function, and hence its properties can be derived using standard methods. Since the tempering parameter can be made as small as we like, the ==== ARTFIMA model can fit data that is usually modeled using the ARFIMA model.====The second reason is statistical. The spectral density function of the ARFIMA model is asymptotically a power law that diverges as the frequency approaches zero. However, in many applications, data fit this power law model only up to a low frequency cutoff, after which the observed spectral density remains bounded. A classical example is turbulence: According to the Kolmogorov model, the spectral density is proportional to ==== in the inertial range, where ==== is the frequency. Since the observed power spectrum in actual turbulence data typically remains bounded as ====, the ARTFIMA model provides a ====. See Meerschaert et al. (2014) for additional discussion.====The third reason is conceptual. The ARTFIMA model with any order of tempered fractional differencing is a stationary time series. It has now become common in many applications to model long range dependence using either an ARFIMA model or a fractional Brownian motion. A fractional Brownian motion with Hurst index ==== satisfies the Kolmogorov model, as does an ARFIMA with ====. Both of these models have stationary increments. However, typical applications involve data that most scientists believe form a stationary sequence, see Molz et al. (2004) for additional discussion. The ARTFIMA model provides a more suitable ==== time series model for such data.",Parameter estimation for ARTFIMA time series,https://www.sciencedirect.com/science/article/pii/S0378375818302829,May 2019,2019,Research Article,341.0
"Zhang Dongxue,Xia Zhiming","School of Mathematics, Northwest University, Xi’an, Shaanxi, China","Received 11 September 2017, Revised 13 September 2018, Accepted 13 September 2018, Available online 24 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.008,Cited by (4),"This paper proposes two types of weighted-averaging estimators of coefficients in segmented linear regressions with a possible threshold. We construct an approximate Mallows criterion which can be looked as an average of limit cases with corresponding threshold effect zero and infinite. Following Hansen (2007) we propose to weightedly average two estimators of coefficients separately with and without threshold effect where weights can be selected by minimizing the Mallows criterion. Further we construct another type of Mallows criterion which is an estimate of the squared error from the model average fit and is used to obtain the new weights for averaging. Under the second Mallows criterion, we find that the new weights make Mallows Model Average (MMA) estimator to be asymptotically optimal in the sense of achieving a lower squared error. Specially in the case of a possible abrupt change, new weights are proved to tend to either one or zero under the true model with only one threshold or tend to ==== under the true model without a threshold. Numerical results demonstrate that the proposed MMA estimator performs better under different complicated changes and does choose the true model under one possible abrupt change.","In the past two decades applied and theoretical econometrics literature has witnessed a growing interest in the threshold models which specify that the sample can be divided into groups according to the magnitude of a threshold variable ====. Formal threshold models appear in the econometrics literature, such as the Threshold Autoregressive (TAR) model of Tong, 1983, Tong, 1995, Abel and Eberly (1994). In no time, threshold models are quite popular in researching economical nonlinearities including the exploration for asymmetries in persistence in the U.S. GNP by Potter (1995), the analysis of nonlinearities in unemployment rates (Koop and Potter, 1999) and so on.====There exist many techniques developed for threshold regression. For instance, Hansen (1997a) proposed in a single-threshold model, the parameter difference of two regimes is called the threshold effect. Applied economists habitually test their models for the presence of threshold effect, typically using the Durlauf and Johnson (1995) and Hansen, 1997b, Hansen, 1997c test. Naturally a threshold model may be an appropriate choice if there is evidence of the threshold effect, which means that they use a pretest estimator which becomes a restricted one under no threshold effect or an unrestricted one under threshold effect. However pretest estimators generally do not have good sampling properties (Hansen, 2009).====An alternative to pretest estimator is model averaging which leads to more robust performance. A model average estimator is a weighted average of estimates obtained from different models. Within the past few years, there are many excellent model averaging papers including Hansen (2007), Zhang et al., 2013, Zhang et al., 2014, Lu and Su (2015), Zhang et al. (2016), etc. Especially, Hansen (2009) investigated averaging of linear regressions with a possible structural break and Gao et al. (2018) proposed a frequentist model averaging approach for threshold model specifications. To our knowledge, there are no averaging papers of linear regressions with a possible threshold.====From a computational standpoint, the threshold model is equal to the change-point model Csörgó and Horváth, 2011, Zou et al., 2014, Xia and Qiu, 2015 with ====. Then we can take suggestions from the change-point literature. Hansen (2009) investigated three jump magnitude estimators including pretest, Mallows selection, and Mallows averaging estimators in segmented linear model with a possible structural break. We find that above-mentioned estimators can also be constructed for estimating coefficients in a linear regression model with a possible threshold. Further we concentrate on constructing MMA estimator of coefficients under another type Mallows criterion (Mallows, 1973) which is consistent according to Zhang (2015). We show that this estimator performs better in the sense of achieving a lower squared error and chooses the true model.====The plan of the paper is as follows. The next section presents the model. In Section 3, we construct four weighted-averaging estimators of coefficients in the model and find some asymptotic properties of them. Numerical studies are then presented in Section 4 and verify our proposed theorems are correct. Section 5 concludes our paper. All proofs are left to Appendix A.",Weighted-averaging estimator for possible threshold in segmented linear regression model,https://www.sciencedirect.com/science/article/pii/S0378375818302763,May 2019,2019,Research Article,342.0
"Cheng Qianshun,Yang Min","University of Illinois at Chicago, United States","Received 21 February 2018, Revised 30 August 2018, Accepted 5 September 2018, Available online 22 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.007,Cited by (6),"Experiments with multiple objectives form a staple diet of modern scientific research. Deriving optimal designs with multiple objectives is a long-standing challenging problem with few tools available. The few existing approaches cannot provide a satisfied solution in general: either the computation is very expensive or a satisfied solution is not guaranteed. A novel algorithm is proposed to address this literature gap. We prove convergence of this algorithm, and show in various examples that the new algorithm can derive the true solutions with high speed.","Experiments with multiple objectives form a staple dietof modern scientific research. For example, in a neurological stimulus–response experiment (Rosenberger and Grill, 1997), the main interests were in estimation of ==== (the lethal dose that causes death for 25% of a study population), ====, and ====. A design that is efficient for the estimation of one of these dose levels is unlikely to be efficient for the others. In an example with four objectives, Clyde and Chaloner (1996) showed that an optimal design for one of the objectives reached efficiencies of only 7%, 10%, and 39% for the others.====There are several approaches for finding efficient designs when multiple objective functions are considered. One approach, which is popular in the optimization literature, is to establish a Pareto front or boundary. Pareto front approach aims to account for all criteria simultaneously by developing a set of Pareto optimal designs and investigate the trade-offs between different designs. For example, Kao et al. (2012) used a modified non-dominated sorting genetic algorithm to obtain a Pareto boundary in the context of event-related fMRI experiments; Cao et al. (2015) proposed a framework for comparing algorithm-generated Pareto fronts based on a refined hypervolume indicator. The graphical presentations they used are effective for two or three criteria, but may not for larger number of criteria. In this paper, we will not pursue this approach any further.====A second approach would use a compound optimality criterion that is a weighted sum of the individual objective functions. An attractive feature is that, for given weights, the compound criterion maintains the concavity property if the separate objective functions possess this property. This property is critically important for applying the celebrated equivalence theorem, which enables verification whether a given design is indeed optimal. With this approach, the weight assigned to each objective function is pre-specified. Then the design found is optimal according to the newly constructed weighted objective function. However, the choice of weights is the main difficulty with this approach; it does in general not have a meaningful interpretation.====The third approach is the constrained optimization approach. It formulates the optimality problem as maximizing one objective function subject to all other objective functions satisfying certain efficiencies. The constrained optimization approach provides a clear and intuitive interpretation to the multiple objective design problem, making it become one of the popular approaches for finding multiple objective optimal design.====On the flip side, in contrast to the compound optimality approach, with the constrained approach there is no “equivalence theorem” that allows a user to verify whether a solution is indeed optimal. Fortunately, there is a relationship between the two approaches. Based on the Lagrange multiplier theorem, Clyde and Chaloner (1996) generalized a result of Cook and Wong (1994) and showed the equivalence of the constrained optimization approach and the compound optimality approach. A numerical solution for the constrained design problem can be derived by using an appropriate compound optimality criterion. In fact, almost all numerical solutions for constrained design problems use this strategy. But the major challenge is how to find the corresponding weights for a given constrained optimality problem.====There are two approaches in the literature using this relation: the grid search approach and the sequential approach. For the grid search approach, the number of grid points increases exponentially with the number of objectives, and can be huge even for a moderate number of objectives. For example, with four objectives and a grid size of 0.01 for each dimension of weights, the total number of grid points is well beyond 170 ==== ====000. Since the best design must be found for each of these, the grid search will become very quickly computationally infeasible as the accuracy increases. And with three objectives, Huang and Wong (1998) proposed a sequential approach for finding the weights. The basic idea is to consider the objective functions in pairs and sequentially add more constraints. While this seems to have given reasonable answers in their examples, there lacks theoretical justification. Consequently this approach will generally not yield satisfactory solution even for the three-objective optimal design problems.====Other approaches are also available. Mikulecka (1983) proposed the idea of hybrid design and algorithm to numerically find the optimal design based on hybrid design settings, which can be regarded as trying to optimize the compound optimal design problem while meeting one constraint criteria. Vandenberghe et al. (1998) proposed an interior-point method to solve determinant maximization problem with linear matrix inequality constraints, which can be used to solve some of the constrained optimal design problem. Harmon and Benkova (2017) proposed the Barycentric algorithm specific for computing D-optimal size- and cost-constrained designs of experiments. Mandal et al. (2005) considered constructing constrained optimal designs with equality constraints and Sagnol and Harman (2015) focused on finding optimal designs with system of linear constraints on weight vectors of design points. The approach proposed by Sagnol and Harman (2015) theoretically can solve the proposed problem if the primary and secondary criteria meet certain requirements. However, technically it is very challenging to derive a practical algorithm based on that. Thus they are not discussed here.====The goal of this paper is to propose a novel algorithm of deriving the optimal design of a given constrained optimality problem through finding the weights in the corresponding compound design. Consistency of the algorithm is proved. The performance of the new algorithm is demonstrated by comparing with the grid search approaches and sequential approaches. In 2016, a short version of this work (Cheng et al., 2016) is presented and published in the 11th International Workshop in Model-Oriented Design and Analysis. In that version, we skipped all the proofs and a few examples. And this paper includes all the proof details and more examples.====This paper is organized as follows. In Section 2, we introduce the set up and necessary notation. Characterization and convergence properties are presented in Section 3. The implementation of the algorithm, as well as the computational cost discussion is in Section 4. Applications to three examples with different number of constraints, and comparisons with grid search and sequential approach are shown in Section 5. Section 6 provides a brief discussion. For the space limit, we put all the proofs and some of the examples in the Appendix.",On multiple-objective optimal designs,https://www.sciencedirect.com/science/article/pii/S0378375818302623,May 2019,2019,Research Article,343.0
"Shi Chenlu,Tang Boxin","Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, British Columbia V5A 1S6, Canada","Received 23 November 2017, Revised 23 June 2018, Accepted 12 September 2018, Available online 22 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.009,Cited by (3),-optimal foldover supersaturated designs can be obtained using ====-optimal supersaturated designs. Further optimization of ====-optimal foldover supersaturated designs is carried out by first minimizing the maximum correlation and then minimizing the frequency of the pairs of columns that attain the maximum correlation. Both theoretical and computational results are presented on the construction of optimal foldover supersaturated designs.,"Supersaturated designs are useful in screening experiments where the primary goal is to identify a few significant factors from a large number of potential factors. As the number of factors in a supersaturated designs is at least as large as its run size, the main effects cannot be estimated simultaneously. The analysis of such designs relies on the assumption of effect sparsity (Box and Meyer, 1986) that only a few factors are active. As supersaturated designs cannot be made orthogonal, a natural way of constructing such designs is to minimize some measure of nonorthogonality, with the ==== being the most popular criterion. Earlier work on supersaturated designs includes Booth and Cox (1962), Lin (1993),Wu (1993), Nguyen (1996), Tang and Wu (1997) and Cheng (1997). More recent work is given in Butler et al. (2001), Bulutoglu and Cheng (2004), Das et al. (2008), Phoa et al. (2009), Marley and Woods (2010), and Jones and Majumdar (2014).====Explicitly or implicitly, this body of work on supersaturated designs assumes the absence of all interaction effects. The reasoning is fairly natural. Since there is even not enough degrees of freedom to estimate all main effects, it seems pointless to allow the existence of significant interactions when considering supersaturated designs. It makes sense to assume that only a few factors are active in screening experiments where a large number of factors are studied — this is the sparsity argument for the use of supersaturated designs. But it does not seem always appropriate to assume that these active factors do not interact with each other.====In this paper, however, we allow the possibility that some of the active factors can potentially interact with each other, which has never been done before to the best of our knowledge. This scenario calls for new supersaturated designs. It turns out that foldover supersaturated designs are well suited for the job. As main effects are all orthogonal to two-factor interactions in a foldover design, the existence of active two-factor interactions will not affect the identification of active main effects. This implies that foldover supersaturated designs provide a class of designs that are robust to two-factor interactions. To analysesuch a design, one can first examine main-effect-only subset models that the design supports to identify active factors. Potential two-factor interactions among active factors will not affect the identification of active factors. One can then consider those models containing the main effects and some two-factor interactions of the active factors. This analysis strategy for foldover designs has been used by many authors, perhaps with the first being Miller and Sitter (2001).====Section 2 introduces foldover supersaturated designs and examines their ====-optimality. It turns that ====-optimal foldover supersaturated designs can be obtained from ====-optimal supersaturated designs, which were recently studied by Jones and Majumdar (2014). We also present an example to illustrate the potential benefits of using foldover supersaturated designs. In Section 3, we further optimize ====-optimal foldover supersaturated designs by first minimizing the maximum correlation and then the frequency of the pairs of columns attaining the maximum correlation. Two methods are presented and small designs are tabulated for practical users. Section 4 concludes the paper.",Supersaturated designs robust to two-factor interactions,https://www.sciencedirect.com/science/article/pii/S0378375818302775,May 2019,2019,Research Article,344.0
Sørensen Helle,"Department of Mathematical Sciences, University of Copenhagen, Denmark","Received 29 September 2016, Revised 19 January 2018, Accepted 4 September 2018, Available online 21 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.002,Cited by (2),None,"Occurrences of diseases or other events are often registered over time, for example as daily, weekly or monthly counts during some period. Apart from bare surveillance it may be of interest to examine the potential association between occurrence and one or more covariates measured in the same period, or to study trends and seasonal patterns. Hence, regression models for counts are needed that take into account the time series structure of the data.====There is a vast literature for Gaussian time series, but Gaussian methods are only appropriate when counts are of reasonable size. For small counts, corresponding to rare events, the Poisson distribution is a natural starting point, but simple log-linear Poisson regression models must be accommodated to incorporate serial correlation and overdispersion. In the following, let ==== and ==== denote the time series of covariate vectors and outcomes. Each ==== is thought of as the realization of a random variable ====, and we are thus interested in the distribution of ==== given ====.====Two types of models are often distinguished: observation-driven models and parameter-driven models Cox, 1981, Zeger, 1988. They differ in their accessibility, both regarding interpretation and statistical inference. Briefly, observation-driven models introduce correlation over time by specifying the conditional distribution of ==== given past values of ==== and covariates. The maximum likelihood principle is directly applicable Davis et al., 2005, Fokianos et al., 2009, Davis et al., 2003, but interpretation of regression coefficients is more subtle. Parameter-driven models, on the other hand, introduce a latent process and use a conditional log-linear Poisson model for the covariate-response relationship given the latent process. Interpretation of the regression coefficients from the simple Poisson regression is thereby maintained, however it comes at the expense of a more difficult estimation problem.====Since our aim is to understand associations between counts and covariates, we prefer parameter-driven models. More specifically, let ==== be an unobserved process and assume that, conditional on ====, the random variables ==== are independent with ==== Poisson distributed with mean ====. As an example, ==== could be the number of cases of a disease, ==== could be the usage of antibiotics, and ==== could represent an underlying and unobserved risk process incorporating unobserved environmental variables (Hay and Pettitt, 2001). With brief notation, and if ==== denotes parameters that determine the distribution of ====, the likelihood for ==== is given by ====where arguments tell whether ==== denotes a marginal, simultaneous or conditional density. The integral is ====-dimensional and does not have an explicit solution, so maximum likelihood estimation is not readily possible.====Several estimation approaches have been suggested in the literature, starting from Zeger (1988) who used an iterative scheme with quasi-likelihood estimation for ==== (for fixed ====) and method-of-moment estimation for ==== (for fixed ====). Later, focus changed to approximations to the likelihood: Chan and Ledolter (1995) used the EM algorithm and computed the expectation in the E-step by Markov chain Monte Carlo (MCMC) simulation, Durbin and Koopman (1997) used MCMC simulation for an approximating linear Gaussian model in a more general state space setting, and Davis and Rodriguez-Yam (2005) and Jung et al. (2006) used importance sampling techniques. The papers just mentioned rely on approximations of the complete likelihood. A simpler alternative is to use composite likelihood methods (Varin et al., 2011). In particular, Davis and Yau (2011) considered pseudo likelihoods corresponding to pairs of counts of at most lag ==== (consecutive pairwise likelihood, CPL, of order ====). Parameter-driven models are hierarchical generalized linear models in the sense of Lee and Nelder (1996), so ====-likelihood methods could be applied; see Lee and Nelder (2001b) for applications to longitudinal and spatial data. Bayesian analysis was presented by Hay and Pettitt (2001).====The method in this paper consists of two steps, both relying on composite likelihoods. First, the marginal density is used to estimate the regression coefficients. This corresponds to a working assumption of independence and could also be interpreted as CPL of order zero (if “pairs of lag zero” is interpreted as single observations). It can be carried out with standard software for generalized linear mixed models. Second, parameters determining the distribution of ==== are estimated via another composite likelihood, either derived from the simultaneous distribution of ==== successive counts or derived from the conditional distribution of ==== given the past ==== observations. The latter corresponds to a working assumption that ==== is Markov of order ====. Both pseudo likelihoods are computed by MCMC simulations, but only require simulation of ==== and ====, respectively. If ==== is an AR(1) process, as it is often assumed, then the pseudo likelihoods corresponding to ==== and ==== can be used. The marginal variance of ==== can be estimated as part of either first or second step.====Step 1 provides consistent estimators for the regression parameters of interest, so at first sight step 2 with estimation of nuisance parameters seems unnecessary. However, step 1 does not provide valid standard errors or confidence intervals, so they are computed by parametric bootstrap, for which we need a full data generating model. Since ==== is estimated for each bootstrap sample it is of great importance that our routine in step 1 is fast and safe. An important part of our numerical studies consists of examining whether the bootstrap confidence intervals have the appropriate coverage, even in cases where the dependence structure is estimated with large uncertainty.====In summary, the aim of the paper is threefold: (1) Comparison of successive and conditional likelihood estimators for ====; (2) validation of coverage for bootstrap confidence intervals for regression coefficients; (3) comparison of our estimators, in particular for ====, to those from other methods. Estimation of beta is simple and valid, and our numerical experiments with an AR(1) model for ==== show that (1) estimators obtained from successive and conditional likelihood are almost identical; (2) confidence intervals have acceptable coverage rates – also for misspecified models – except when the correlation parameter, denoted ====, in the latent AR(1) process is close to 1; and (3) successive and conditional likelihood compare well to other methods for estimation of the regression coefficients, but underestimates ==== when ==== is large.====The rest of the paper is organized as follows: Estimation procedures are described in detail in Section 2 and tested on simulated data in Section 3. In Section 4 the methods are applied to two datasets known from the literature. Concluding remarks are given in Section 5.","Independence, successive and conditional likelihood for time series of counts",https://www.sciencedirect.com/science/article/pii/S0378375818302556,May 2019,2019,Research Article,345.0
"Schnell Patrick M.,Bose Maitreyee","Division of Biostatistics, The Ohio State University College of Public Health, 1841 Neil Ave, Columbus, OH 43210, United States,Department of Biostatistics, University of Washington School of Public Health, 1705 NE Pacific St, Seattle, WA 98195, United States","Received 29 September 2017, Revised 13 April 2018, Accepted 4 September 2018, Available online 18 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.004,Cited by (2)," between columns of the fixed effect and reparameterized random effect ====, as well as individual observations. We also present spectral parameterizations of ANOVA-like models, intrinsic conditional ","Linear mixed models (LMMs) are routinely fit to data, but we have little understanding of how exactly the data drive the fits. When the linear mixed model includes covariates, the fixed effects and the random effects compete with each other to capture the variation in the data. It is not always clear in what way the random effects contribute to the fitted values, particularly when the random effects covariance matrix, say ====, is non-trivial (e.g., ==== has spatial structure). One way to better understand this contribution is by reparameterizing the random effect component into a canonical form with independent and identically distributed random effects and investigating the corresponding canonical predictors (columns of the reparameterized design matrix).====These kinds of reparameterizations are useful for several reasons. The reparameterization leads to tools for understanding linear mixed model fits in detail and provide a way of developing diagnostics for those fits. Reparameterizing the model often also means simpler model formulations and easier algebraic calculations than the standard form, which in turn makes it easier to develop intuition about how these model fits behave the way they do. The spectral decomposition allows us to visualize certain kinds of trends and features present in the data and how heavily they are weighted in the fit. We can then interpret phenomena like confounding between fixed and random effects, and develop tools to avoid them. For example, Reich et al. (2006) use a spectral reparameterization of the conditional autoregressive (CAR) model for areal spatial data to examine spatial confounding, and Reich and Hodges (2008) use the reparameterization to illuminate the mechanism by which data inform estimates of variance parameters in linear models with two variance components. Additionally, the reparameterization involves diagonal covariance matrices which are faster to invert; this aids in computation when we maximize the restricted likelihood to get estimates for the model parameters.====This paper considers questions arising from a desire to understand how the random effects contribute to the fitted values in LMMs. Are the design matrix columns in the reparameterization interpretable; how do they look and behave? We summarize a literature search on known results, as well as develop further knowledge about balanced multi-way ANOVA models and general theory. This will lead to a more general understanding of the canonical representation of the random effects used in work such as Reich et al. (2006). Hodges (2013, Chapter 5) develops the canonical representation of spatial models such as the intrinsic autoregressive (ICAR) model, smoothed ANOVA, and two-dimensional splines, showing via data analysis examples how the reparameterization helps develop intuition about the behavior of model fits.====The structure of this paper is as follows. In Section 2 we introduce the spectral parameterization for linear mixed models, and describe a class of models for which the parameterization is invariant with respect to variance parameters. In Section 3 we provide a geometric interpretation of confounding of fixed effects by random effects via reparameterization. In Section 4, we derive as an example spectral parameterizations for the balanced crossed random effects model. In Section 5 we provide some diagnostic tools derived from the theory of Sections 2 The spectral parameterization, 3 Confounding of fixed effects by random effects, and give a constructed data example using a balanced crossed random effects model and a real data example using an intrinsic autoregressive model. Appendices catalog spectral parameterizations of some additional models.",Spectral parameterization for linear mixed models applied to confounding of fixed effects by random effects,https://www.sciencedirect.com/science/article/pii/S037837581830257X,May 2019,2019,Research Article,346.0
"Hyodo Masashi,Onobuchi Akihiro,Kurakami Hiroyuki","Department of Mathematical Sciences, Graduate School of Engineering, Osaka Prefecture University, 1-1 Gakuen-cho, Naka-ku, Sakai, Osaka 599-8531, Japan,Department of Medical Innovation, Osaka University Hospital, 2-2, Yamadaoka, Suita, Osaka, 565-0871, Japan","Received 9 January 2018, Revised 6 August 2018, Accepted 5 September 2018, Available online 15 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.006,Cited by (0),"In this study, we present a multivariate test for population bioequivalence. Recently, Chervoneva et al. (2007) proposed a multivariate generalization of the criteria for testing univariate population bioequivalence by obtaining an approximate test based on an ","This study proposes two new tests that can be used to exhibit the equivalence of two drug formulations that are used in several pharmaceutical applications. Generally, new prescription drugs are compared with the existing drugs that are already approved. This may be conducted by drug companies, among others, while making significant changes to their manufacturing processes or while evaluating their general product marketing. For example, generic drugs are indispensable for demonstrating the bioequivalence of legal pharmaceuticals. When two medicinal products (e.g., a new product and a generic medicine) contain an identical active ingredient, they are biologically equivalent if the rates and amounts of drugs entering the systemic circulation are equivalent, i.e., the drugs are absorbed in equal concentrations and amounts. Specifically, they are assumed to be biologically equivalent if both the area under the blood concentration–time curve (====) and the maximum blood concentration (====) remain identical for both the formulations. If this is true, the new drug can be safely sold.====Bioequivalence is verified using hypothesis tests that are based on several different equivalence assessments. These assessments are often multivariate, even though they are not necessarily treated as such. A good introduction to multivariate equivalence tests can be observed in Chervoneva et al. (2007). There are two general approaches to investigate multivariate equivalence. One approach focuses on the mean vector difference by considering the variance–covariance matrix as a nuisance parameter and has been investigated by Brown et al., 1995, Brown et al., 1997, Berger and Hsu (1996), Munk and Pfluger (1999), Wang et al. (1999), and Tamhane and Logan (2004). The other approach, which is a natural extension of the univariate method, is based on a criterion that simultaneously handles the covariance matrices and mean vectors. With respect to the univariate case, Sheiner (1992) and Schall and Luus (1993) are observed to propose a criterion that simultaneously compares the means and variances. More recently, Chervoneva et al. (2007) extended this approach to population bioequivalence in the multivariate case and proposed an approximate test that used an unbiased estimator of the linearized criteria. Although this test’s type I error probability is often observed to be less than the significance level, its approximation accuracy is not good in case of small samples. Thus, our study’s contribution is to propose a more accurate approximate test. To improve the approximation accuracy, it is important to investigate the unbiased estimator’s distribution; therefore, we obtain new explicit representations of the characteristic function and asymptotic distribution of the unbiased estimator. Further, we apply these asymptotic results to propose new approximate tests. We additionally obtain the theoretical asymptotic power function in an explicit manner and investigate the local powers of our tests and prove that they are asymptotically unbiased.====The rest of this paper is organized as follows. In Section 2, we introduce the approach of Chervoneva et al. (2007). Then, in Section 3, we present asymptotic results for the unbiased estimator of linearized bioequivalence criteria and propose two new tests. In Section 4, we demonstrate the validity of our proposed tests via Monte Carlo simulations and apply them to real data. Finally, we present our conclusions in Section 5 and provide some essential proofs in the Appendix.",An approximate multivariate asymptotic expansion-based test for population bioequivalence,https://www.sciencedirect.com/science/article/pii/S0378375818302611,May 2019,2019,Research Article,347.0
"Ahmadi-Javid Amir,Moeini Asghar","Department of Industrial Engineering & Management Systems, Amirkabir University of Technology, Tehran, Iran,School of Mathematics and Statistics, The University of Melbourne, Melbourne, Australia","Received 30 December 2017, Revised 22 August 2018, Accepted 4 September 2018, Available online 14 September 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.09.001,Cited by (8),"This paper studies a class of distributions supported on generalized balls and spheres that are extensions of ==== ball and sphere can be represented based on each other. It is observed that the proposed distributions over generalized ==== spheres, the distribution over a generalized ","Uniform random variate generation over bounded sets has found many applications in different fields. Although the approximate uniform sampling methods over various subsets of Euclidean spaces are well studied in the literature (e.g., Vempala, 2005, Kiatsupaibul et al., 2011, Mete and Zabinsky, 2012, Hörmann et al., 2013, Byrne and Girolami, 2013, Rhee et al., 2014 and Dieker and Vempala, 2015), there are only a few known special sets over which random generation can be done exactly (e.g., Rubinstein, 1982, Moeini et al., 2011, Diaconis et al., 2013 and Ahmadi-Javid and Moeini, 2016). In general, these methods are not efficient unless they are developed for special sets that are either defined based on symmetric functions (e.g., Euclidean balls, Euclidean spheres, or standard simplexes) or obtained by an affine transformation from one of the former sets (e.g., non-standard simplexes or Euclidean ellipsoids)—a function is called ==== if it remains unchanged by any ==== of its variables (Macdonald, 1995).==== balls and spheres are important sets, which are defined based on ==== symmetric polynomials (see Definition 2.1). Various efficient exact sampling methods are established to generate samples uniformly distributed over ==== balls and spheres, with respect to the Lebesgue and cone measures, respectively. One may refer to the papers by Song and Gupta (1997), Liang and Ng (2008), Harman and Lacko (2010), and Lacko and Harman (2012) for a review. Moreover, some asymptotic results on distributions over ==== balls and spheres are presented by Stam (1982), Diaconis and Freedman (1987), Rachev and Ruschendorf (1991), Khokhlov (2006), Spruill (2007), Zeng (2014), Kim and Ramanan (2015), Gantert et al. (2015), Alonso-Gutiérrez et al. (2016), and Kabluchko et al. (2017). Distributional characterizations and extensions are given in Gupta and Song, 1997a, Gupta and Song, 1997b, Vignat and Plastino (2005), Hashorva et al. (2007), Sinz and Bethge (2010), Arellano-Valle and Richter (2012), and Richter (2017). Other related geometric and measure-theoretic results can be found in Schechtman and Zinn, 1990, Schechtman and Zinn, 2000, Naor and Romik (2003), Barthe et al. (2005), Naor (2007), Barthe et al. (2010), Richter (2009), Jiang. (2015), Zou and Xiong (2016), Böröczky and Henk (2016), Chen et al. (2017), and Bianchi et al. (2017).====This paper investigates a class of distributions supported on flexible extensions of ==== balls and spheres that are specified by general power-sum polynomials. These extensions are here called ==== ==== ====, and defined based on power-sum polynomials whose exponents can be different positive numbers. From a theoretical perspective, this paper extends the knowledge on (non-approximate) uniform generation to more complicated sets. Our results can also be a basis for developing broad classes of distributions using the approach applied to define spherical and elliptical distributions (Fang et al., 1990), ====-spherically symmetric distributions (Gupta and Song, 1997a), ====-generalized normal distributions Goodman and Kotz, 1973, Gupta and Song, 1997b, Sinz et al., 2009, ====-norm generalized Dirichlet distributions (Hashorva et al., 2007), or other distributions supported on simplexes Favaro et al., 2011, Ongaro and Migliorati, 2013.====Another possible application of our results is to construct flexible uncertainty sets in analysis and control of uncertain systems (Tempo et al., 2013), or in robust optimization (Bertsimas et al., 2004) whenever the uncertain parameters have different tendencies to take large or small values.====The outline of the paper is as follows. Section 2 provides required definitions and preliminaries, and introduces the two classes of distributions studied in this paper. Section 3 explicitly derives the formulas of the joint, marginal, and conditional distributions corresponding to the proposed distributions. Section 4 presents several stochastic representations and numerically compares the sampling schemes developed based on them. Section 5 assesses conditions under which the proposed distributions over generalized ==== spheres are uniform with respect to the associated cone measures, and then discusses uniform sampling from generalized ==== spheres with respect to the Hausdorff measure. Section 6 provides applied insights on the usage of generalized ==== balls for modeling uncertainty sets. Section 7 concludes the paper.",Uniform distributions and random variate generation over generalized ,https://www.sciencedirect.com/science/article/pii/S0378375818302544,14 September 2018,2018,Research Article,348.0
"Bloznelis Daumantas,Claeskens Gerda,Zhou Jing","ORStat and Leuven Statistics Research Center, KU Leuven, Naamsestraat 69, 3000 Leuven, Belgium,Department of Business Administration, Inland Norway University of Applied Sciences, Telthusveien 12, 2450 Rena, Norway","Received 21 November 2017, Revised 2 July 2018, Accepted 4 September 2018, Available online 14 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.003,Cited by (8)," in high-dimensional linear models and compare its performance to the composite quantile estimator in both low- and high-dimensional settings. We also assess the effect on efficiency of using equal weights, theoretically optimal weights, and estimated optimal weights for combining the different quantiles. None of the estimators dominates in all settings under consideration, thus leaving room for both model-averaged and composite estimators, both with equal and estimated optimal weights in practice.","For low-dimensional linear regression models, ordinary least squares (OLS) estimation is the common approach. Under standard assumptions, OLS provides the minimum-variance (a.k.a. best) estimator in the class of linear unbiased estimators. However, it may misbehave when the error distribution has heavy tails. This motivated the seminal Koenker and Bassett (1978) paper that introduced quantile regression as a robust alternative to OLS. Unsurprisingly, robustness does not come at zero cost; the quantile estimator is relatively less efficient than its OLS counterpart for certain light-tailed distributions such as the Gaussian. Efforts to find robust yet efficient estimators have persisted. Koenker (1984) considered weighted composite quantile regression (weighted CQR) and weighted model-averaged quantile regression (weighted MAQR) as more efficient alternatives to the regular single-quantile estimator. He showed that both estimators are more efficient than the single-quantile one, and that both achieve the same lower bound of asymptotic variance, given a suitable choice of weights that depend on the error distribution. The origins of MAQR can be found already in Koenker and Bassett (1978), while the idea of CQR was proposed by R.V. Hogg in 1979; see Koenker, 1984, Koenker, 2005. The literature has continued expanding on the composite quantile regression (see Zou and Yuan, 2008, Bradic et al., 2011, Jiang et al., 2012, Jiang et al., 2014). Meanwhile, the model-averaged quantile regression has garnered little attention (with a recent exception of Zhao and Xiao, 2014), although it is computationally cheaper than CQR, and the difference in the computational cost becomes prohibitive when the number of quantiles employed is larger than about ten.====For high-dimensional models, only the composite estimator has been considered (Bradic et al., 2011). We introduce its model-averaged counterpart, obtain optimal weights for the different quantiles under a given error distribution, and compare CQR and MAQR in terms of asymptotic relative efficiency and finite-sample performance. We also draw attention to the fact that when the error distribution is unknown, theoretically optimal weights are unavailable. They need to be estimated from the data and thus become random variables. Therefore, optimality results for plug-in versions of the theoretically optimal weights may change. This is similar in spirit to the forecast combination puzzle (e.g. Claeskens et al., 2016 and references therein) where estimated optimal weights in forecast combinations may yield poorer results than equal weights. Moreover, the asymptotic distributions of the CQR and MAQR estimators under estimated optimal weights are less straightforward to obtain than under fixed weights. We examine in simulations whether estimated optimal weights or equal weights perform better in practice.====In Section 2 we first review the composite and model-averaged linear quantile estimators in low-dimensional regression models. We contribute with a theoretical comparison of equal weights and optimal weights for both types of estimators. Section 3 proceeds with composite and model-averaged estimation in high-dimensional regression models under a sparsity assumption. We obtain the limiting asymptotic distribution of a high-dimensional model-averaged quantile estimator and use the distribution to propose a vector of optimal weights. A simulation study in Section 4 and a data example in Section 5 illustrate the estimators’ performance in practice.",Composite versus model-averaged quantile regression,https://www.sciencedirect.com/science/article/pii/S0378375818302568,May 2019,2019,Research Article,349.0
"Yamamoto Hirotaka,Hirao Masatake,Sawa Masanori","Daifuku Co., Ltd., 3-2-11 Mitejima, Nishiyodogawa-ku, Osaka 550-0012, Japan,Department of Information and Science Technology, Aichi Prefectural University, 1522-3 Ibaragabasama, Nagakute, Aichi 480-1198, Japan,Graduate School of System Informatics, Kobe University, 1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501, Japan","Received 25 August 2017, Revised 11 July 2018, Accepted 4 September 2018, Available online 14 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.09.005,Cited by (1),"In this paper we establish a construction of Euclidean 9-designs (i.e., the fourth order rotatable designs) on the unit ball. A classical, popular approach for this is to use the corner vectors of the hyperoctahedron such as the vertices, the midpoints of the edges, the ==== of the faces and so on. As an improvement of this, we propose to use the corner vectors of the hyperoctahedral group, plus their “internally dividing points”. We give a classification of Euclidean 9-designs on two spheres, and several examples of the fourth order optimal rotatable designs in low dimensions.","Let ==== be a probability measure on the unit ball ====, which we call a ==== on the unit ball.====A design ==== defines the positive semi-definite inner product ====on the vector space ==== of all polynomials of degree at most ==== in ==== variables.====Let ====, where ==== form a basis of ==== and ====. We denote the transpose of ==== by ====. The information matrix of a design ==== is defined as follows: ====We say that a design ==== is ==== ==== if ==== is nonsingular, and hereafter restrict our attention to designs ==== of degree ====.====Among many optimality criteria, we are particularly concerned with the ====, which is commonly used in practice. We say that a design is ==== (or ==== for short) if the design maximizes the determinant of the information matrix.====Here, we give the concept of Euclidean design introduced by Neumaier and Seidel (1988). Let ==== be a nonnegative integer, and ==== be a finite subset in ==== with a positive weight function ====. For ==== with ====, we denote by ==== the concentric sphere with radius ====, i.e., ====. Let ==== and ====. Let ==== be the surface measures on ====, respectively. Let ====. We use the convention that ==== for ====. We also use ==== and ====.====A D-optimal design with finite support for the ====th degree polynomial regression on the unit ball is well-known to be rotatable. Namely, the ====th order optimal rotatable design on the unit ball is a Euclidean ====-design with ====, or ====; see also Neumaier and Seidel (1992) and Bannai and Bannai (2006). Moreover, such a design is supported by ==== spheres including the surface of ====; we regard a half as the origin if ==== is a half integer. Thus, one of the important problems is to construct Euclidean designs supported by “suitably” weighted concentric spheres.====There have been many articles about constructions of Euclidean designs. Among them, we focus on a construction method based on the corner vectors associated with the ==== ====; see, e.g., Kiefer (1960), Farrell et al. (1967), Pesotchinsky (1978), Gaffke and Heiligers, 1995a, Gaffke and Heiligers, 1995b, Gaffke and Heiligers, 1995c, Gaffke and Heiligers, 1998, Bajnok (2007) and Hirao et al. (2014). Here, ==== is the group of all permutations and sign changes of the coordinates of a vector in ====.====We note that, Euclidean ====-designs consisting of only corner vectors have degree ==== (cf. Bajnok (2007) and Nozaki and Sawa (2012)). Thus, in this paper, as an extension of the corner vector construction, we propose to use the corner vectors ==== and their internally dividing points ====, which enable us to find various Euclidean ====-designs. This is an extension of a construction of Euclidean designs on the unit sphere, proposed by Sawa and Xu (2014).====In the next section, we review some basic terminology and explain, in detail, our idea of constructing Euclidean ====-designs supported by one or two concentric spheres. In Section 3, we present classifications of Euclidean ====-designs on two concentric spheres and several examples of the fourth order optimal rotatable designs on the unit ball. In Section 4, we give a proof of our main theorem (see also Theorem 2.5 in the next section).",A construction of the fourth order rotatable designs invariant under the hyperoctahedral group,https://www.sciencedirect.com/science/article/pii/S0378375818302581,May 2019,2019,Research Article,350.0
"Fischer Aurélie,Mougeot Mathilde","Laboratoire de Probabilités, Statistique et Modélisation, Université Paris Diderot, 75013 Paris, France","Received 26 December 2016, Revised 25 May 2018, Accepted 22 August 2018, Available online 5 September 2018, Version of Record 26 November 2018.",https://doi.org/10.1016/j.jspi.2018.08.001,Cited by (8),"In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction for a new observation ==== previously calibrated using a training data set. Mojirsheibani proposes to compute the prediction associated to a new observation by combining selected outputs of the training examples. The output of a training example is selected if some kind of consensus is observed: the predictions computed for the training example with the different machines have to be “similar” to the prediction for the new observation. This approach has been recently extended to the context of regression in Biau et al. (2016).====In the original scheme, the agreement condition is actually required to hold for all individual estimators, which appears inadequate if there is one bad initial estimator. In practice, a few disagreements are allowed ; for establishing the theoretical results, the proportion of estimators satisfying the condition is required to tend to 1.====In this paper, we propose an alternative procedure, mixing the previous consensus ideas on the predictions with the ==== computed between entries. This may be seen as an alternative approach allowing to reduce the effect of a possibly bad estimator in the initial list, using a constraint on the inputs.====We prove the consistency of our strategy in classification and in regression. We also provide some numerical experiments on simulated and real data to illustrate the benefits of this new aggregation method. On the whole, our practical study shows that our method may perform much better than the original combination technique, and, in particular, exhibit far less variance. We also show on simulated examples that this procedure mixing inputs and outputs is still robust to high dimensional inputs.","Given the growing number of available statistical estimation strategies, trying to combine several procedures in a smart way is a very natural idea, and so, an abundant literature on aggregation of estimators for different types of statistical models has sprung up in recent years. A widespread aggregation method consists in building a linear or a convex combination of a bunch of initial estimators (see, for instance, Catoni (2004), Juditsky and Nemirovski (2000), Nemirovski (2000), Yang, 2000, Yang, 2001, Yang, 2004, Györfi et al. (2002), Wegkamp (2003), Audibert (2004), Bunea et al., 2006, Bunea et al., 2007a, Bunea et al., 2007b, and Dalalyan and Tsybakov (2008). Observe that the model selection approach, which aims at selecting the best estimator in the list, is also linked to the same goal (see, for example, the monograph by Massart, 2007).====In this paper, we focus on the question of combining estimators for two kinds of statistical procedures, pertaining to the supervised learning framework: classification, which consists in assigning to an observation a label, out of a finite number of candidates, and regression, whose aim is to associate real outputs to entries.====Beside the usual linear aggregation and model selection methods, a quite different point of view has been introduced by Mojirsheibani (1999) for classification. The idea consists in an original combination method, which is non linear in the initial estimators and is based on a consensus concept. More specifically, in this combining scheme, an observation is considered to be reliable for contributing to the classification of a new query point if all initial classifiers predict the same label for both points. Then, the label for the query point is estimated thanks to a majority vote among the labels of the observations which have been retained this way. Note that more regular versions, based on smoothing kernels, have also been developed (Mojirsheibani, 2000). A numerical comparison study of several combining schemes is available in Mojirsheibani (2002b), and recently, a variant of the method has been proposed in Balakrishnan and Mojirsheibani (2015).====This strategy has just been adapted in the regression framework by Biau et al. (2016). In this context, an observation is used in the combination step if all the initial estimators predict a similar value for the observation and the new point: the difference between both predictions is required to be less than some prespecified threshold. Then, the new prediction by the combined estimator is the average of the outputs corresponding to the selected entries. Note that the functional data framework has also been considered, by Cholaquidis et al. (2016).====In the classification case as well as in the regression one, when the initial list contains a consistent estimator, it can be shown that the combined estimator inherits this consistency property. Let us mention that the techniques of proof, and consequently, the assumptions made in both situations, are quite different. For instance, the number of initial estimators is expected to tend to infinity with the sample size in Mojirsheibani (2000) whereas it is fixed in Biau et al. (2016).====It is worth pointing out that, in both contexts described above, the condition for an observation to be reliable is in principle required to be satisfied for all estimators. In a further paper, Mojirsheibani (2002a) notes that this rule may seem too restrictive and proposes to allow a few disagreements (typically, a single one). The resulting classifier is still consistent provided that the number of initial classifiers keeps tending to infinity after removing those with disagreement. Similarly, in Biau et al. (2016), this unanimity constraint is relaxed in practice by demanding that the distance condition for keeping an observation is true at least for a certain proportion ==== of the estimators (for example, ====). The theoretical results remain true provided that ==== tends to 1.====Here, our purpose is to investigate a new approach, based on distances between observations, which also aims at reducing the effect of a possibly bad initial estimator. Roughly, choosing a kernel point of view, we will propose a combined estimator with weights constructed by mixing distances between entries with distances between predictions coming from the individual estimators. Our motivation for introducing such a strategy is the intuition that, taking advantage of the efficiency of the consensus idea of Mojirsheibani (1999) and Biau et al. (2016), without for all that forgetting the information related to the proximity between entries, shall help improving the prediction, especially in the presence of an initial estimator that does not perform very well.====Our modified rule will be shown to be consistent under general assumptions. In particular, the combined estimator may perfectly be consistent even if the list of initial estimators does not contain any consistent estimator. We also conduct numerical experiments, both on simulated and real data, which demonstrate the benefits of our strategy, with respect to the original combining method and the individual estimators.====The paper is organized as follows. The new combined estimator is defined in Section 2. Then, the main theoretical results are stated in Section 3. Section 4 is devoted to numerical experiments with simulated and real examples. A few perspectives are presented in a brief conclusive paragraph, Section 5. For the sake of clarity, proofs are postponed to Section 6.",Aggregation using input–output trade-off,https://www.sciencedirect.com/science/article/pii/S0378375818302349,May 2019,2019,Research Article,351.0
"Afendras Georgios,Markatou Marianthi","Department of Biostatistics and Jacobs School of Medicine and Biomedical Sciences, University at Buffalo, United States","Received 8 December 2017, Revised 18 July 2018, Accepted 19 July 2018, Available online 4 August 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.005,Cited by (27),"An important question in cross-validation (CV) is whether rules can be established to allow optimal sample size selection of the training/test set, for fixed values of the total sample size ====. We study the cases of repeated train–test CV and ====-fold CV for certain decision rules that are used frequently. We begin by defining the ","Advances and accessibility to technology for capturing and storing large amounts of data has transformed many scientific fields. Yet, for many important scientific questions very large data sets are not available. In these cases, when learning algorithms are used, resampling techniques allow the estimation of the generalization error, an important aspect of algorithmic performance.====The generalization error is defined as the error an algorithm makes on cases that the algorithm has never seen before; it is important because it relates to the algorithm’s prediction capabilities on independent data. The literature includes both, theoretical investigations of risk performance of machine learning algorithms as well as numerical comparisons.====Estimation of the generalization error can be achieved via the use of resampling techniques, and in particular via CV. Estimates of the generalization error are used in both, performance evaluation of computational algorithms as well as selection of an algorithm for a given problem (i.e. model selection). Important work in comparing classifiers, from the field of machine learning, includes Dietterich (1998), Demšar (2006), Garcia and Herrera (2008), Kohavi (1995). In the field of Biostatistics van de Wiel et al. (2009) develop an inference framework for the difference in errors between two prediction procedures. Similarly, model selection strategies for machine learning algorithms involve the numerical optimization of a criterion that is very often based on an estimate of the generalization error (Cawley and Talbot, 2010, p. 2087). In the statistics literature model selection via CV has been studied by Shao (1993), Yang (2006).====Carrying out inference on equality of generalization errors requires insight into the variance of the estimator of the generalization error. Nadeau and Bengio (2003) provide estimators for the variance of the repeated train–test CV estimator of the generalization error, while Bengio and Grandvalet (2003–2004) address estimators of variance in ====-fold CV. Furthermore, Markatou et al. (2005) propose a moment approximation-based estimator of the same CV estimator of the generalization error, and compare this estimator with those provided by Nadeau and Bengio. Other relevant work on variance estimation includes Wang and Lindsay (2014), Wang and Lindsay (2017), Fuchs et al. (2013), Markatou et al. (2011).====Selecting the size of the training set or the number of folds in ====-fold CV and understanding the effect of this selection on the generalization error, its bias and its variance, is of interest to many areas of scientific investigation. Examples include pattern recognition and machine learning Highleyman (1962), Fukunaga and Hayes (1989), Raudys and Jain (1991), Guyon et al. (1998), Kearns (1997), statistics Berger and Pericchi (2004), Cabras et al. (2015), remote sensing Zhen et al. (2013), Jin et al. (2014), biostatistics and bioinformatics Dobbin and Simon (2005), Dobbin and Simon (2011), Dobbin et al. (2008), Popovici et al. (2010), Shao et al. (2013) among others. Hall and Robinson (2009) show that “bagging CV” based on training samples of size ==== significantly improves mean integrated squared error performance in kernel density bandwidth selection. The reason of the interest shown in the problem of training sample size selection is because it impacts the accuracy of the estimators of the generalization error. Arlot and Celisse (2010, p. 69, Sect. 10.2 and p. 70, Sect. 10.3) discuss explicitly the importance of this problem. Even if performance comparisons of algorithms are based on surrogate to the generalization error quantities, such as ranks, inaccurate estimates of the generalization error provide inaccurate surrogate quantities and therefore tests based on these quantities provide incorrect results. In this paper, we offer an analysis of the problem of training sample size selection when the total sample size is fixed and interest centers on carrying out inference on the generalization error. The analysis is complex, and for this reason we consider three kinds of problems to cover a good range of possible applications. These include the case where the decision rule is the sample mean, linear regression or ridge regression and classification via logistic regression or via support vector machines. The strategy we follow establishes a general framework that has the potential to address the issue of optimal selection of the training (and hence test) set sample size and selection of the number of folds in ====-fold CV. Within this framework we refer to work by Burman (1989), Burman (1990) to establish rules for optimal selection and study the role of the bias–variance trade-off. In our context, the number of predictors is considerably smaller than ====.====Optimality is defined in terms of minimizing the variance of the test set error, and we show that this action is equivalent to minimizing the mean squared error of the CV estimators of the generalization error.====Our results are based on the following two observations. First, the sampling scheme used in repeated train–test and ====-fold CV imposes a uniform distribution on the training sets; Lemma 1 in Appendix A provides all necessary tools for establishing our results. Secondly, in Section 2.3, Proposition 1 provides useful results that establish the mathematical foundations of this work. These results have not previously appeared in the literature.====We show that for large samples the variance term of the error estimate of the generalization error is highly relevant, while the bias term plays a secondary and often unimportant role. For small samples, both bias and variance terms are relevant. We present two novel methods for obtaining the resampling size ==== of the repeated train–test (====) CV estimator of the generalization error Nadeau and Bengio (2003), Markatou et al. (2005) that are based on the variance of the aforementioned estimator. We discuss a general framework that can be followed to obtain optimal training set sample size or optimal number of folds ==== of CV estimators of the generalization error for any classification algorithm and illustrate it using several decision rules. We study the impact of data distribution and elucidate the role of the loss function in selecting the optimal training set sample size and the number of folds in ====-fold CV, by constructing and using in the respective problems two new loss functions (modified squared and double squared error loss). Associated empirical results are presented in Tables 6 and 7 of the online supplement, while the literature includes results mainly for the squared error loss.====The paper is organized as follows. Section 2 presents a brief discussion of relevant literature, establishes notation and offers a useful result that serves as a foundation of the proposed optimality rules. Section 3 presents the proposed optimality rules and results using various decision rules. Two methods for obtaining the resampling size of the ==== CV estimator are also discussed. Section 4 presents simulation results, while Section 5 contains discussion and recommendations.",Optimality of training/test size and resampling effectiveness in cross-validation,https://www.sciencedirect.com/science/article/pii/S0378375818301514,March 2019,2019,Research Article,352.0
"Du Xingqi,Ghosal Subhashis","Department of Statistics, North Carolina State University, 5109 SAS Hall, Campus Box 8203, Raleigh, NC 27695, USA","Received 29 August 2017, Revised 16 May 2018, Accepted 28 July 2018, Available online 3 August 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.009,Cited by (4),We consider a ,"Finding structural relations in a network of random variables ==== is a problem of significant interest in modern statistics. The intrinsic dependence between variables in a network is appropriately described by a graphical model, where two nodes ==== are connected by an edge if and only if the two corresponding variables ==== and ==== are conditionally dependent given all other variables. If the joint distribution of all variables is multivariate normal with precision matrix ====, the conditional independence between the variable located at node ==== and that located at node ==== is equivalent of having zero at the ====th entry of ====. In a relatively large network of variables, generally conditional independence is abundant, meaning that in the corresponding graph edges are sparsely present. Thus in a Gaussian graphical model, the structural relation can be learned from a sparse estimate of ====, which can be naturally obtained by a regularization method with a lasso-type penalty. Friedman et al. (2008) and Banerjee et al. (2008) proposed the graphical lasso (====) estimator by minimizing the sum of the negative log-likelihood and the ====-norm of ====, and its convergence property was studied by Rothman et al. (2008). A closely related method was proposed by Yuan and Lin (2007). An alternative to the graphical lasso is an approach based on regression of each variable on others, since ==== is zero if and only if the regression coefficient ==== of ==== in regressing ==== on other variables is zero. Equivalently this can be described as using a pseudo-likelihood obtained by multiplying one-dimensional conditional densities of ==== given ==== for all ==== instead of using the actual likelihood obtained from joint normality of ====. The approach is better scalable with dimension since the optimization problem is split into several optimization problems in lower dimensions. The approach was pioneered by Meinshausen and Bühlmann (2006), who imposed a lasso-type penalty on each regression problem to obtain sparse estimates of the regression coefficients, and showed that the correct edges are selected with probability tending to one. However, a major drawback of their approach is that the estimator of ==== and that of ==== may not be simultaneously zero (or non-zero), and hence may lead to logical inconsistency while selecting edges based on the estimated values. Peng et al. (2009) proposed the Sparse PArtial Correlation Estimation (====) by taking symmetry of the precision matrix into account. The method is shown to lead to convergence and correct edge selection with high probability, but it may be computationally challenging. A weighted version of ==== was considered by Khare et al. (2015), who showed that a specific choice of weights guarantees convergence of the iterative algorithm due to the convexity of the objective function in its arguments. Khare et al. (2015) named their estimator the CONvex CORrelation selection methoD (====), and proved that the estimator inherits the theoretical convergence properties of ====. By extensive simulation and numerical illustrations, they showed that ==== has good accuracy for reasonable sample sizes and can be computed very efficiently.====However, in many situations, such as if multiple characteristics are measured, the variables ==== at different nodes ==== may be multivariate. The methods described above apply only in the context when all variables are univariate. Even if the above methods are applied by treating each component of these variables as separate one-dimensional variables, ignoring their group structure may be undesirable, since all component variables refer to the same subject. For example, we may be interested in the connections among different industries in the US, and may like to see if the GDP of one industry has some effect on that of other industries. The data is available for 8 regions, and we want to take regions into consideration, since significant difference in relations may exist because of regional characteristics, which is not possible to capture using only national data. It seems that the only paper which addresses multi-dimensional variables in a graphical model context is Kolar et al. (2014), who pursued a likelihood based approach.====In this article, we propose a method based on a pseudo-likelihood obtained from multivariate regression on other variables. We formulate a multivariate analog of ====, to be called ====, because of the computational advantages of ==== in univariate situations. Our regression based approach appears to be more scalable than the likelihood based approach of Kolar et al. (2014). Moreover, we provide theoretical justification by studying large sample convergence properties of our proposed method, while such properties have not been established for the procedure introduced by Kolar et al. (2014).====The paper is organized as follows. Section 2 introduces the ==== method and describes its computational algorithm. Asymptotic properties of ==== are presented in Section 3. Section 4 illustrates the performance of ====, compared with other methods mentioned above. In Section 5, the proposed method is applied to two real datasets on gene/protein profiles and GDP respectively. Proofs are presented in Section 6 and in the Appendix A Proof of the lemmas, Appendix B Auxiliary results.",Multivariate Gaussian network structure learning,https://www.sciencedirect.com/science/article/pii/S0378375818301745,March 2019,2019,Research Article,353.0
"He Lei,Yue Rong-Xian","Department of Mathematics, Shanghai Normal University, Shanghai, 200234, China,Scientific Computing Key Laboratory of Shanghai Universities, Shanghai, 200234, China","Received 23 January 2018, Revised 30 May 2018, Accepted 21 July 2018, Available online 29 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.008,Cited by (7),-optimal designs are investigated. A few examples are presented for illustration and the relative efficiency comparisons between the second-order least squares estimator and the ordinary least squares estimator are discussed via the new criterion.,"The ordinary least squares estimator (OLSE) has been widely used to estimate the unknown parameters in both linear and nonlinear regression models, since it has many best-known (asymptotic) statistical properties (see Seber and Wild (1989)). In particular, this estimator is most efficient if the random errors in the model are normally distributed. However, there exist more efficient estimators than the OLSE if the random error distribution is not normal or asymmetric. For example, the maximum likelihood estimator can be employed based on a known distribution for the error term. It is noted that the exact distribution of the error term is often unknown in practice. In this situation, Wang and Leblanc (2008) proposed the second-order least squares estimator (SLSE) that is another asymptotically more efficient than the OLSE if the third moment of the random error is nonzero (i.e., the error distribution is asymmetric). Also, this estimator does not rely on a certain error distribution and satisfies the strong consistency and asymptotic normality under general regularity conditions. In this paper, we consider the problem of optimal designs for regression model with the asymmetric errors.====It is well known that optimal design of experiment is a useful tool to collect data for improving the efficiency of the statistical analysis substantially, and the existing literature on optimal experimental designs focuses on minimizing some convex function of the covariance matrix of the OLSE, in which Kiefer’s ====-optimality criteria have been studied extensively by numerous authors, see, for example, Fedorov (1972), Pukelsheim (1993) and Berger and Wong (2009) among others. Recently, however, optimal designs based on the SLSE have been paid more and more attention in the literature. Gao and Zhou (2014) made a seminal work of optimal designs under the SLSE and proposed new optimality criteria under the frameworks of the ====- and ====-optimality criteria, which are minimizing the determinant and the trace of the asymptotic covariance matrix of the SLSE of the parameters of interest, respectively. Transformation invariance and symmetry of the new ====-optimality criterion were also investigated in Gao and Zhou (2014). Bose and Mukerjee (2015) gave the results of further research on this direction that contain the equivalence characterizations of these new criteria and an available numerical algorithm. After that another algorithm for finding optimal designs based on the SLSE was established in Gao and Zhou (2017) by using convex optimization techniques and moment theories. Moreover, mention may be made to Yin and Zhou (2017) who investigated optimal designs problems for regression models using the SLSE, especially including the transformation invariance of the ====-optimality criterion on a discrete design space.====In the present paper, we are concerned with the optimality aspect under the SLSE with respect to the ====-optimality criterion proposed by Dette (1997). The ====-optimality criterion minimizes the volume of the Bonferroni rectangular confidence region of the regression parameters. It has been well recognized (Kutner et al., 2005) that the Bonferroni simultaneous rectangle is an uncomplicated confidence region and can be used in such situation where the analyst wishes to make simultaneous inferences about the regression parameters with a given overall confidence level. In fact, this volume is proportional to the square root of product of the diagonal elements of the inverse of the information matrix. Such an optimality criterion not only has a nice statistical interpretation, but also satisfies an extremely useful invariance property which allows an easy calculation of optimal designs on many linearly transformed design spaces. So far the ====-criterion has been applied to a variety of models, such as multiresponse models (Liu and Yue, 2013), multi-factor models Liu et al. (2014a), Liu et al. (2016), random coefficients regression models (Liu et al., 2014b) and so on. The main objective of this paper is to extend the ====-optimality criterion stated in Dette (1997) under the SLSE and derive several results, including a characterization of the ====-optimality criterion.====The remainderof this paper is organized as follows. In Section 2, a new expression of the ====-optimality criterion under the SLSE is proposed and the corresponding equivalence theorem is derived. Several invariance properties of the ====-optimal designs based on the SLSE are studied in Section 3. Two examples are presented for illustration in Section 4, and finally, a discussion of the results is given in Section 5.",-optimality criterion for regression models with asymmetric errors,https://www.sciencedirect.com/science/article/pii/S0378375818301654,March 2019,2019,Research Article,354.0
Hirose Masayo Y.,"Institute of Statistical Mathematics, 10-3 Midori-cho, Tachikawa, Tokyo, Japan","Received 30 December 2016, Revised 18 July 2018, Accepted 18 July 2018, Available online 26 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.006,Cited by (5),"The empirical best ====In this paper, we therefore seek an adequate class of general adjusted maximum-likelihood methods that simultaneously achieve the three desired properties of MSE estimation. To establish that the investigated class does so, we reveal the relationship between the general adjusted maximum-likelihood method for the model variance parameter and the general functional form of the second-order unbiased MSE estimator, maintaining strict positivity. We also compare the performance of several MSE estimators in our investigated class and others through a ==== study. The results show that the MSE estimators in our investigated class perform better than those in others.","In recent decades, there has been high demand for reliable statistics on smaller geographic areas and sub-populations where large samples are not available. Considering the limited number of observations, a direct design-based estimator (direct estimator) is not reliable for such “small areas”—as they are called. Even in such a situation, an explicit model-based approach can achieve more accurate estimates by borrowing strength from related areas. Accordingly, a methodology based on this approach has been developed for small-area estimation. For a comprehensive overview of small-area estimation, refer to Rao and Molina (2015).====The Fay–Herriot model (Fay and Herriot, 1979), in particular, is widely used as an aggregated level model for small-area inference as follows:====For ====
               ====The level-1 model takes into account the sampling distribution of the direct estimator ==== for the ====th small area. The true small-area mean for the ====th area, denoted by ====, is linked to the area-specific auxiliary variables ==== in the level-2 model. In practice, the coefficient vector ==== in ==== and the model variance parameter ==== in this linking model are unknown. The assumption of the known ==== often follows from the asymptotic variances of the transformed direct estimates (Efron and Morris, 1975) or from empirical variance modeling (Fay and Herriot, 1979). This model can be rewritten as a specific linear mixed model: ====where ==== and ==== are mutually independent with the normality assumption ==== and ====. It is well known that, among all linear unbiased predictors ==== of ====, the best linear unbiased predictor (BLUP) yields the minimum mean squared prediction error (MSE), which is defined as ====, where the expectation is defined with respect to the joint distribution of ==== and ==== under the Fay–Herriot model (1). We give the form of BLUP as follows: ====where ==== is called the shrinkage factor, which can shrink toward ==== from the direct estimator ==== with ====, ==== and ====.====Since ==== is unknown in practice, the following empirical best linear unbiased predictor (EBLUP) of ==== is generally used for small-area inference (==== is replaced with its consistent estimator, ====, in ====): ====where ==== and ====. Hereafter, the consistent estimator ==== also denotes an even-translation-invariant estimator for all ==== and ==== that achieve unbiasedness in the EBLUP, as in Kackar and Harville (1981). Such conditions are satisfied by several estimators of the model variance parameter ====, which are obtained by methods of moments estimators Fay and Herriot (1979), Prasad and Rao (1990) and standard maximum likelihood methods such as the profile maximum likelihood method (PML) and the residual maximum likelihood method (REML). In particular, the REML estimator of ==== is preferred in terms of its higher-order asymptotic accuracy for large ====. Let ==== denote the REML estimator of ====, obtained as ====where the residual likelihood function is ====and ====
            ====As mentioned above, an EBLUP is widely used as an efficient estimator based on a specific linear mixed model. It would also be quite important to measure the MSE of EBLUP as its uncertainty. For small-area inference, its MSE needs to be estimated with high accuracy since it generally depends on an unknown parameter even if the true MSE can be derived in a closed form. Given a consistent estimator of an unknown model variance parameter, the MSE of EBLUP is always larger than that of the best linear unbiased prediction (BLUP) estimator under certain conditions (Kackar and Harville, 1984). In most small-area applications, sufficient accuracy cannot be achieved by ignoring this difference, which is of the order of ==== for large ==== (the number of areas). Moreover, the naive MSE estimator, a consistent estimator substituted for the model variance parameter ==== in the MSE of BLUP, lacks second-order unbiasedness for sufficient asymptotic accuracy in small-area estimation with large ====. Therefore, several second-order unbiased MSE estimators, with some bias correction, are suggested in place of the naive estimator. A pioneer work by Prasad and Rao (1990) proposed such a second-order unbiased MSE estimator based on Taylor linearization, which used a method of moment estimator of the model variance parameter ====. Subsequently, MSE estimators have been developed adopting other estimators of unknown model parameter via the Taylor linearization method. Datta and Lahiri (2000) and Das et al. (2004) proposed MSE estimators of EBLUP using the standard likelihood method to estimate the model variance estimator ====. Datta et al. (2005) also suggested an MSE estimator using another method for the moment estimator of ====, proposed in Fay and Herriot (1979). MSE estimators of EBLUP have been constructed through not only the Taylor linearization method but also resampling methods—a bootstrap and jackknife method. The bootstrap MSE estimator was first introduced in a small-area context by Butar and Lahiri (2003) and extended by Hall and Maiti (2006). Another resampling method, the Jackknife-type MSE estimator, was developed by Jiang et al. (2002), Jiang et al. (2016) and Chen and Lahiri (2008).====However, the estimation methods for ==== mentioned above could cause zero estimates although such estimates are unreasonable in the context of small-area estimation. Such estimates of ==== could cause additional estimation problems: an over-shrinking problem in estimating the shrinkage factor ==== and occurrence of unreasonable MSE estimates as if the MSE of EBLUP is only caused by variabilities of the estimators of ==== and ====. Incidentally, Bell (1999) has reported zero estimates of PML and REML for four consecutive years for the Fay and Train (1997) model of 5- to 17-year-old poverty state rates in U.S. To avoid such zero estimates, Li and Lahiri (2010) proposed a specific adjusted maximum likelihood method based on the frequentist approach. Lahiri and Li (2009) put forward the concept of generalized maximum likelihood of unknown variance components for a general linear mixed model. Their class includes not only the well-known PML and REML estimators but also the specific adjusted maximum likelihood estimators of Li and Lahiri (2010). The main purposes of the Li–Lahiri adjusted maximum likelihood estimators were to avoid the over-shrinking problem associated with the PML and REML estimators of an unknown variance parameter and to make the parametric bootstrap method for constructing an interval of ==== more efficient. Although MSE estimation was not the main focus of Li and Lahiri (2010), they also outlined how to obtain a second-order unbiased estimator of MSE. These methods therefore did not take into account the strict positivity of the MSE estimator of EBLUP. In contrast, one of the MSE estimators proposed in Yoshimori and Lahiri (2014) provides strict positivity in the estimators of both model variance parameter ==== and MSE of EBLUP simultaneously, while maintaining its second-order unbiasedness, by developing a specific adjusted maximum-likelihood method. Hirose and Lahiri (2018) achieved another specific adjusted maximum likelihood method successfully, satisfying not only the three desired properties in terms of MSE estimation but also an additional property in terms of the shrinkage factor ====. As already described, the adjusted maximum likelihood method has been developed for different purposes in terms of point, MSE, or interval estimation.====Let ====, ====, and ==== denote the respective estimators proposed in Li and Lahiri (2010), Yoshimori and Lahiri (2014), and Hirose and Lahiri (2018), given as ====where ====, and ==== are adopted from their specific adjustment factors, ==== for ====, ==== for ==== with ====, and ==== for ==== with an additional adjustment factor ==== satisfying Conditions A2–A3 given in Appendix A. Incidentally, Li and Lahiri (2010) and Yoshimori and Lahiri (2014) also proposed other estimators, obtained by substituting a profile maximum likelihood function such as ==== for ==== in (2).====Then, a relevant question that arises is, what type of adjusted maximum likelihood method can achieve the three desired properties of MSE estimation? To answer this question, this paper proposes, in Section 3, a general class of adjusted maximum likelihood methods to achieve the desired asymptotic property while maintaining strict positivity in terms of estimation of the variance parameter and the MSE of EBLUP. In the same section, we also provide a theorem to choose both a suitable adjusted maximum-likelihood method for a specified functional form of the second-order unbiased and strictly positive MSE estimator and a suitable functional form of the second-order unbiased and strictly positive MSE estimator for a specified adjusted maximum-likelihood method. We also show that our class includes the methods proposed in Yoshimori and Lahiri (2014) and Hirose and Lahiri (2018). Moreover, we also provide a new naive MSE estimator with second-order unbiasedness via the specific adjusted maximum-likelihood method included in our class. Section 4 presents a performance comparison among certain MSE estimators, including ours. Our simulation also shows that the MSE estimates of Li and Lahiri (2010) could be negative, especially when the model variance ==== is very small relative to the sampling variances ====. We, however, point out that Li and Lahiri (2010) stressed confidence interval and not the MSE as the measure of uncertainty for their EBLUP.====The regularity conditions and all technical proofs are deferred to the Appendix A Conditions, Appendix B Proof of. Incidentally, this paper focuses on MSE estimation based on Taylor linearization, which is a non-computer-intensive method and therefore tractable in practice.",A class of general adjusted maximum likelihood methods for desirable mean squared error estimation of EBLUP under the Fay–Herriot small area model,https://www.sciencedirect.com/science/article/pii/S0378375818301526,March 2019,2019,Research Article,355.0
Wiens Douglas P.,"Department of Mathematical and Statistical Sciences; University of Alberta, Edmonton, Alberta, Canada T6G 2G1","Received 26 February 2018, Revised 25 June 2018, Accepted 21 July 2018, Available online 26 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.007,Cited by (4),"In a previous article (Wiens, 1991) we established a maximin property, with respect to the power of the test for Lack of Fit, of the absolutely continuous uniform ‘design’ on a design space which is a subset of ====. We show that these designs are supported on the entire design space. They are in general not uniform for fixed ====, but are asymptotically uniform as ====. Several examples with ==== fixed are discussed; in these we find that the approach to uniformity is very quick.","In Wiens (1991), henceforth referred to as [W], we studied the uniform ‘design’, as applied to design spaces ==== that are subsets of ==== – intervals, hypercubes, etc. – with positive Lebesgue measure. We call such design spaces ====, to distinguish them from the finite, discrete design spaces considered in this article.====The uniform design on ==== is the absolutely continuous measure, with constant density ====. Of course such a design must be approximated in order to implement it in an actual experiment. A contribution of [W] was that, in a sense made precise there and detailed in Section 2 , the uniform design possesses an optimality property in the class of all designs on ==== – it maximizes the minimum power of the standard F-test for Lack of Fit ( ====) of a fitted linear regression model, with the minimum taken over a broad class of alternatives.====The theory in [W] has been adapted to justify the use of discrete uniform designs in numerous applications in the sciences. For its application to drug combination studies see the series of papers Tan et al. (2003), Fang et al. (2008), Tan et al. (2009), Fang et al. (2009) and Fang et al. (2015). The ideas in [W] have gained traction in the theory of artificial neural networks – see Zhang et al. (1998) – and reduced support vector machines – see Lee and Huang (2007). The theory has been extended to nonparametric regression models – Xie and Fang (2000) – and, also allowing for heteroscedasticity, by Biedermann and Dette (2001) and Bischoff and Miller (2006).====The continuous nature of ==== in this context has been controversial. Indeed Bischoff (2010) argues that it allows for classes of alternative regression models – as used both in [W] and in Biedermann and Dette (2001) – that are too broad for the optimality property to be asymptotically meaningful (when the continuous uniform design is viewed as the limit of discrete uniform designs); he proposes a restricted interpretation.====That the richness of classes of alternatives as in [W] makes discrete designs inadmissible was noted in Wiens (1992, p. 355), where we state ‘ ====’ This remains our view. Nonetheless, in this article we suggest an alternate approach that we feel is less controversial. We take a ==== design space ==== – here ==== can be arbitrarily large, allowing for at least a close approximation of the space of interest in an anticipated application. We obtain exact designs for small values of ==== – these are non-uniform – and show that the maximin designs are asymptotically uniform, as ====. Theory and examples show that this limit is approached very quickly.====In the next section we outline the mathematical framework, provide a reduction of the maximin problem to a simpler minimax problem, and prove the asymptotic optimality of the uniform design. Some solutions with ==== fixed are given in Section 3. Proofs are in the Appendix. The computing code is available from the author’s personal web site.",Maximin power designs in testing lack of fit,https://www.sciencedirect.com/science/article/pii/S0378375818301642,March 2019,2019,Research Article,356.0
"Benhaddou Rida,Pensky Marianna,Rajapakshage Rasika","Department of Mathematics, Ohio University, United States,Department of Mathematics, University of Central Florida, United States","Received 6 March 2017, Revised 22 April 2018, Accepted 14 July 2018, Available online 23 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.004,Cited by (10),"In the present paper we consider the problem of estimating a three-dimensional function ====-risk when ==== belongs to a three-dimensional Laguerre–Sobolev ball and demonstrate that the wavelet-Laguerre estimator is adaptive and asymptotically near-optimal in a wide range of Laguerre–Sobolev spaces. We carry out a limited simulations study and show that the estimator performs well in a finite sample setting. Finally, we use the technique for the solution of the Laplace deconvolution problem on the basis of DCE Computerized Tomography data.","Consider an equation ====where ====, ==== and ==== is the three-dimensional Gaussian white noise such that ====Here and in what follows, ==== denotes the indicator function of a set ====. Formula (1.1) can be viewed as a noisy version of a functional Laplace convolution equation. Indeed, if ==== is fixed, then (1.1) reduces to a noisy version of the Laplace convolution equation ====that was recently studied by Abramovich et al. (2013), Comte et al. (2017) and Vareschi (2015).====Eq. (1.1) represents a white-noise version of the Laplace convolution equation which corresponds to the observational version of the equation ====where ====
               ====
               ====
               ==== are equispaced on the interval ====, ==== and ==== and ==== are standard normal variables that are independent for different ==== and ====. If ==== and ==== are large, then Eq. (1.1) serves as an “idealized” version of Eq. (1.3). This result is rigorously proved in the case of the Gaussian regression model (see, e.g. Brown and Low (1996)), and it is well known that it holds for a large variety of settings. Abramovich et al. (2013) studied a one-dimensional ==== version of Eq. (1.3). It follows from the upper and lower bounds in their paper that the correspondence between Eqs. (1.2) and the one-dimensional version of Eq. (1.3) holds with ==== where ==== (since ====).====Comte et al. (2017) also studied solution of Eq. (1.3) in the case of ==== and rigorously investigated the implications of the fact that observations are taken on the finite interval ==== rather than on the positive part of the real line. They showed that the latter leads to a much more involved mathematical arguments. On the other hand, Vareschi (2015) considered Eq. (1.2) and, building upon an earlier version of Comte et al. (2017), derived the lower and the upper bounds for the error in the white noise version of the Laplace deconvolution problem. Our paper can be regarded as an extension of Vareschi’s (2015) results to the case when Laplace convolution equation has a spatial component and the function of interest is anisotropic, i.e., may have different degrees of smoothness in different directions. Therefore, our objective is to show how utilizing the spatial smoothness of the unknown function ==== leads to its more precise recovery.====Our study is motivated by the analysis of Dynamic Contrast Enhanced (DCE) imaging data. DCE imaging provides a non-invasive measure of tumor angiogenesis and has great potential for cancer detection and characterization, as well as for monitoring, ====, the effects of therapeutic treatments (see, e.g., Bisdas et al. (2007), Cao (2011), Cao et al. (2010) and Cuenod et al. (2011)). The common feature of DCE imaging techniques is that each of them uses the rapid injection of a single dose of a bolus of a contrast agent and monitors its progression in the vascular network by sequential imaging at times ====, ====. This is accomplished by measuring the pixels’ gray levels that are proportional to the concentration of the contrast agent in the corresponding voxels. At each time instant ====, one obtains an image of an artery as well as a collection ==== of measurements for each voxel ====. For example, in the case of a CT scan, ==== are the Hu units which represent the opacity of the material to X-rays. The images of the artery allow to estimate the so called Arterial Input Function, ====, which quantifies the total amount of the contrast agent entering the area of interest. Comte et al. (2017) described the DCE imaging experiment in great detail and showed that the cumulative distribution function ==== of the sojourn times for the particles of the contrast agent entering a tissue voxel ==== satisfies the following equation ====Here the errors ==== are independent for different ==== and ====, ====, a positive coefficient ==== is related to a fraction of the contrast agent entering the voxel ==== and ==== is the time delay that can be easily estimated from data. The function of interest is ==== where the distribution function ==== characterizes the properties of the tissue voxel ==== and can be used as the foundation for medical conclusions.====Since the Arterial Input Function can be estimated by denoising and averaging the observations over all voxels of the aorta, its estimators incur much lower errors than those of the left hand side of Eq. (1.4). For this reason, in our theoretical investigations, we shall treat function ==== in (1.4) as known. In this case, Eq. (1.4) reduces to the form (1.1) that we study in the present paper. If one is interested in taking the uncertainty about ==== into account, this can be accomplished using methodology of Vareschi (2015).====Laplace deconvolution equation (1.2) was first studied in Dey et al. (1998) under the assumption that ==== has ==== continuous derivatives on ====. However, the authors only considered a very specific kernel, ====, and assumed that ==== is known, so their estimator was not adaptive. Abramovich et al. (2013) investigated Laplace deconvolution based on discrete noisy data. They implemented the kernel method with the bandwidth selection carried out by the Lepskii’s method. The shortcoming of the approach is that it is strongly dependent on the exact knowledge of the kernel ====. Recently, Comte et al. (2017) suggested a method which is based on the expansions of the kernel, the unknown function ==== and the observed signals over Laguerre functions basis. This expansion results in an infinite system of linear equations with the lower triangular Toeplitz matrix. The system is then truncated and the number of terms that are kept in the series expansion of the estimator is controlled via a complexity penalty. One of the advantages of the technique is that it considers a more realistic setting where ==== in Eq. (1.2) is observed at discrete time instants on an interval ==== with ==== rather than at every value of ====. Finally, Vareschi (2015) derived a minimax optimal estimator of ==== by thresholding the Laguerre coefficients in the expansions when ==== is unknown and is measured with noise.====In the present paper, we consider the functional version (1.1) of the Laplace convolution equation (1.2). The study is motivated by the DCE imaging problem (1.4). Due to the high level of noise on the left hand side of (1.4), a voxel-per-voxel recovery of individual curves is highly inaccurate. For this reason, the common approach is to cluster the curves for each voxel and then to average the curves in the clusters (see, e.g., Rozenholc and Reiß (2012)). As the result, one does not recover individual curves but only their cluster averages. In addition, since it is impossible to assess the clustering errors, the estimators may be unreliable even when estimation errors are small. On the other hand, the functional approaches, in particular, the wavelet-based techniques, allow to denoise a multivariate function of interest while still preserving its significant features.====The objective of this paper is to solve the functional Laplace deconvolution problem (1.1) directly. In the case of the Fourier deconvolution problem, Benhaddou et al. (2013) demonstrated that the functional deconvolution solution usually has a much better precision compared to a combination of solutions of separate convolution equations. Below we adopt some of the ideas of Benhaddou et al. (2013) and apply them to the solution of the functional Laplace convolution equation. Specifically, we assume that the unknown function belongs to an anisotropic Laguerre–Sobolev space and recover it using a combination of wavelet and Laguerre functions expansion. Similar to Comte et al. (2017), we expand the kernel ==== over the Laguerre basis and ====, ==== and ==== over the Laguerre-wavelet basis and carry out denoising by thresholding the coefficients of the expansions, which naturally leads to truncation of the infinite system of equations that results from the process. We derive the minimax lower bounds for the ====-risk in the model (1.1) and demonstrate that the wavelet-Laguerre estimator is adaptive and asymptotically near-optimal within a logarithmic factor in a wide range of Laguerre–Sobolev balls. We carry out a limited simulation study and then finally apply our technique to recovering of ==== in Eq. (1.4) on the bases of DCE-CT data.====Although, for simplicity, we only consider the white noise model for the functional Laplace convolution equation (1.1), the theoretical results can be easily generalized to its observational version (1.3) by following Comte et al. (2017). However, as it is evident from Comte et al. (2017), the latter will lead to much more complex calculations and will make the paper very difficult to read while adding very little to the paper conceptually. For this reason, in the present paper, we avoid this extension.====The rest of the paper is organized as follows. In Section 2, we describe the construction of the wavelet-Laguerre estimator for ==== in Eq. (1.1). In Section 3, we derive the minimax lower bounds for the ====-risk for any estimator of ==== in (1.1) over anisotropic Laguerre–Sobolev balls. In Section 4, we demonstrate that the wavelet-Laguerre estimator is adaptive and asymptotically minimax near-optimal (within a logarithmic factor of ====) in a wide range of Laguerre–Sobolev balls. Section 5 presents a limited simulation study followed by a real data example in Section 6. The proofs of the statements of the paper are placed in Section 7. Finally, Section 8 provides some supplementary results from the theory of banded Toeplitz matrices.",Anisotropic functional Laplace deconvolution,https://www.sciencedirect.com/science/article/pii/S0378375818301472,March 2019,2019,Research Article,357.0
"Filippou Panagiota,Kneib Thomas,Marra Giampiero,Radice Rosalba","Department of Statistical Science, University College London, Gower Street, London WC1E 6BT, UK,Chairs of Statistics and Econometrics, Georg-August-Universität Göttingen, Humboldtallee 3, 37073 Göttingen, Germany,Department of Economics, Mathematics and Statistics, Birkbeck, University of London, Malet Street, London WC1E 7HX, UK","Received 17 October 2017, Revised 13 March 2018, Accepted 5 July 2018, Available online 20 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.002,Cited by (5), function in the ==== package ====.,"When the researcher is interested in modelling more than one response, univariate regression will not yield valid inferences if there is residual dependence between the outcomes conditional on covariates. The case of trivariate models has been discussed in the literature in various contexts. For example, Loureiro et al. (2010) assessed the effect of parental smoking habits on their children’s smoking habits by estimating a three-equation probit regression model, whereas Zhong et al. (2012) evaluated the safety of a treatment and identified an optimal dose by jointly modelling the probabilities of toxicity, efficacy, and surrogate efficacy given a specific dose. Król et al. (2016) examined the response to a treatment on patients with metastatic colorectal cancer by analysing simultaneously three outcomes: a longitudinal marker, a set of recurrent events, and a terminal event. A mixture of powers copula-based approach to model jointly three binary and discrete outcomes was employed by Zimmer and Trivedi (2006), whereas Zhang et al. (2015) developed a Bayesian algorithm to estimate trivariate probit-ordered models affected by double sample selection.====This paper contributes to the literature by introducing a generalization of the trivariate additive probit model. Specifically, we extend and therefore enhance the model proposed by Filippou et al. (2017) by allowing (i) the link functions to be virtually derived from any parametric distribution and (ii) the model’s association parameters to depend on several types of covariate effects (such as linear, nonlinear, random, and spatial effects). The first extension allows for the use of link functions other than probit. In particular, the additional link functions implemented for this work are the logit and complementary log–log which are used extensively in numerous disciplines, including the medical and social sciences. In clinical research logit models are widely employed as they provide direct information about which treatment has the best odds of benefiting a patient, for instance. Complementary log–log models have important applications in survival analysis where they can, for example, provide a clear insight into the relative reduction of risk for death or progression. Extension (ii) is of some relevance since it can help to gain insights into the way the residual association between the responses is modified by the presence of covariates. As will be further elaborated in the paper, the practical success of such extensions depends on the use of a computationally efficient and theoretically tractable parametrization for the model’s correlation matrix as well as the availability of the analytical score and Hessian of the proposed model’s log-likelihood which are not trivial to derive.====To the best of our knowledge, the two proposed developments have not been considered in the context of trivariate (or more generally, multivariate) binary response regression models. Note also that, despite we have focused on the trivariate case, the model’s formulation in Section 2 could be easily extended to the multivariate context as would be in principle the proposed estimation framework. Finally, it is worth pointing out that our proposal may be regarded as an extension of the bivariate regression approaches introduced by Marra and Radice (2017a), Klein and Kneib (2016) and Radice et al. (2016) as well as of the popular generalized additive models (GAMs) and GAMs for location, scale and shape of Wood (2017) and Rigby and Stasinopoulos (2005). In summary, the two main contributions of this paper are to extend the model introduced by Filippou et al. (2017) as detailed above and to make the new developments available via the ==== function from the ==== package ==== (Marra and Radice, 2017b).==== Section 2 introduces the proposed model, Section 3 describes the log-likelihood and Section 4 provides the key details on parameter estimation. The proposal is empirically evaluated in a simulation study, presented in Section 5, and then applied to a case study in Section 6, where the interest is in modelling jointly three adverse birth binary outcomes in North Carolina. Section 7 concludes the paper.",A trivariate additive regression model with arbitrary link functions and varying correlation matrix,https://www.sciencedirect.com/science/article/pii/S0378375818301332,March 2019,2019,Research Article,358.0
Kroll Martin,"Universität Mannheim, Germany","Received 8 August 2017, Revised 9 May 2018, Accepted 9 July 2018, Available online 19 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.003,Cited by (7)," is the realization of a Poisson random variable with parameter ====. The aim is to estimate the functional parameter ==== from independent or weakly dependent observations ==== in a random design framework.====First we determine upper risk bounds for projection estimators on ==== under mild conditions. In the case of Sobolev ellipsoids the obtained rates of convergence turn out to be optimal.====The main part of the paper is devoted to the construction of adaptive projection estimators of ==== via model selection. We proceed in two steps: first, we assume that an upper bound for ==== is known. Under this assumption, we construct an adaptive estimator whose dimension parameter is defined as the minimizer of a penalized contrast criterion. Second, we replace the known upper bound on ==== by an appropriate plug-in estimator of ====We illustrate our theoretical findings by a short simulation study and conclude by indicating directions of future research.","We consider the non-parametric estimation of a regression function ====defined on some Polish space ==== from observations ==== where, conditional on ====, the ==== are independent and Poisson distributed with parameter ====. The covariates ==== are drawn from some strictly stationary process ====, and we will consider the two cases where either (i) the ==== are independent, or (ii) some adequate condition on the dependence of the underlying process ==== is satisfied. Although we will also provide minimax theoretical results for the non-parametric estimation problem at hand, our focus will be on the adaptive estimation of ====, that is, the construction of estimators that depend only on the observations but not on any structural presumptions concerning the regression function.====Regression models with count data, i.e., non-negative and integer-valued response, are of interest in a wide range of applications, for instance in economics (Winkelmann, 2008), quantitative criminology (Berk and MacDonald, 2008), and ecology (Ver Hoef and Boveng, 2007). The ==== model introduced above is the most natural example of such a count data regression model. Other models with count data response include models based on the negative binomial distribution which can also deal with ====. Such more advanced models will not be considered in this paper. Most of the work in the area of count data regression has been devoted to parametric models, see for instance the monograph (Cameron and Trivedi, 1998) for a comprehensive overview of methods. Let us just mention some examples: the paper (Diggle et al., 1998) gives an application of a Poisson regression model in a geostatistical context. It provides a fully parametric approach and suggests MCMC techniques for fitting a model to the given data. The paper (Carota and Parmigiani, 2002) introduces a semi-parametric Bayesian model for count data regression and applies it as a prognostic model for early breast cancer data. The article (Nakaya et al., 2005) considers geographically weighted Poisson regression for disease association mapping.====Despite its potential utility in many applications, non-parametric Poisson regression has hardly been studied from a theoretical point so far. One possible approach is to apply the so-called Anscombe transform (Anscombe, 1948) to the data and treat the data as if they were Gaussian. Another approach would be to consider the generalized linear model representation of Poisson regression and allow for varying coefficients Hastie and Tibshirani (1993), Fan and Zhang (1999). Recent work has also considered the Poisson regression model in a high-dimensional framework using the LASSO and the group LASSO (Ivanoff et al., 2016). Another interesting reference is Fryzlewicz (2008): in this paper the author considers a very general model with Poisson regression as a special case. In contrast to our model, only regression with deterministic design is considered (note that the distinction between independent and weakly dependent covariates considered by us is not possible in the model with deterministic design). Moreover, the automatic choice of the smoothing parameter is not addressed from a theoretical point of view in Fryzlewicz (2008) whereas this is the major topic of our contribution. Finally, let us mention the work (Besbeas et al., 2004) where an extensive simulation study for count data regression using wavelet methods was performed. That paper contains also further references to Bayesian methods in the context of count data regression. In this paper, we study adaptive non-parametric Poisson regression via the model selection approach. To the best of our knowledge, this approach has not been used for non-parametric Poisson regression so far (in a parametric framework, however, the recent paper (Kamo et al., 2013) considers a model selection approach via a bias-corrected AIC criterion).====Note that a characteristic feature of the non-parametric Poisson regression model is the fact that it naturally incorporates heteroscedastic noise. Besides work on regression in presence of homoscedastic errors (see for instance Baraud, 2000), there already exists research that considers model selection techniques in regression frameworks containing heteroscedasticity (Saumard, 2013). However, in Saumard (2013) the observations are of the form ====where ==== is the unknown regression function to be estimated, the residuals ==== have zero mean and variance one, and the function ==== models the unknown heteroscedastic noise level. Note that this model does not contain the Poisson regression model to be considered in this paper as a special case.====Our paper is also more general with regard to another aspect: we do not exclusively stick to the case that the covariates ==== are independent but also consider the more general case that the covariates are weakly dependent which seems to be more realistic at least in some real world scenarios. For instance, when studying clutch sizes of bird eggs that are modelled via count data models (that usually go beyond the Poisson model studied here due to over-dispersion of the data) in ornithology (Ridout and Besbeas, 2004), the covariates (e.g., temperature) are not independent when data are collected over a period of time. Concerning mathematical methodology, we will model this by imposing throughout conditions on the decay of the so-called ====-mixing coefficients. The class of time series with ====-mixing coefficients is sufficiently large to be of interest for applications and includes stationary vector ARMA processes (Mokkadem, 1990) or even more general autoregressive processes of the form ==== under mild conditions on the functions ==== and ==== Neumann and Thorarinsdottir (2006), Doukhan (1994). Our methodological approach is mainly based on fundamental results from the article (Viennet, 1997) that have also been exploited in a wide variety of other statistical problems: in Baraud et al. (2001) and Asin and Johannes (2016) the authors consider the non-parametric estimation of a regression function in case of ====-mixing covariates. The paper (Lacour, 2008) considers adaptive estimation of the transition density of a particular hidden Markov chain under the assumption that the hidden chain is ====-mixing. From a methodological point of view our approach was also inspired by the recent work (Asin and Johannes, 2016). However, in contrast to that paper, we build our construction of adaptive estimators on the model selection technique from Barron et al. (1999) only, whereas (Asin and Johannes, 2016) combines the model selection approach with a more recent technique due to Goldenshluger and Lepski (2011).====Let us sketch the organization and summarize the main contributions of the paper. In Section 2 we introduce notations and the general methodology used in the paper. Section 3 is devoted to the case of independent observations: we derive a general minimax upper bound and a matching lower bound over Sobolev ellipsoids. We then consider adaptive estimation of the regression function via model selection which has not been addressed before in the literature. We first consider an estimator based on the a priori knowledge of an upper bound on the regression function (Section 3.3.1), and then, inspired by the approach in Comte (2001), put some effort to develop an estimator that does not depend on this assumption (Section 3.3.2). The risk bound of the adaptive estimators is deteriorated by a logarithmic factor only in comparison with the minimax optimal rate. In Section 4 we extend the findings from Section 3 to the weakly dependent case. The proofs in this case are more demanding than in the independent case, but the results are essentially the same. Subsequent to our theoretical findings, we provide a short simulation study in Section 5. In Section 6 we conclude and discuss perspectives for future research. All proofs are deferred to the Appendix A Proofs of Section , Appendix B Proofs of Section , Appendix C Technical lemmata, Appendix D Bernstein inequality, Appendix E Concentration inequalities.",Non-parametric Poisson regression from independent and weakly dependent observations by model selection,https://www.sciencedirect.com/science/article/pii/S0378375818301356,March 2019,2019,Research Article,359.0
"Sarkar Sanat K.,Chen Aiying,He Li,Guo Wenge","Temple University, United States,Sanofi Pasteur, United States,Merck Research Laboratories, United States,New Jersey Institute of Technology, United States","Received 17 November 2017, Revised 30 May 2018, Accepted 4 July 2018, Available online 19 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.07.001,Cited by (1),"-values satisfying a positive dependence condition both between and within stages is given. This group sequential BH is adapted to the proportion of true nulls in two different ways, resulting in the proposal of two adaptive group sequential BH. While one of these adaptive procedures is theoretically shown to control its FDR when the ====-values are positively dependent between but independent within stages, the other one’s FDR control is assessed through simulations. Comparative performance studies of the proposed procedures in terms of FDR control, power, and proportion of sample saved carried out through extensive simulations provide evidence of superior performance of the proposed adaptive procedures.","In many modern scientific investigations, such as those in gene and protein expression studies where thousands of genes are tested for possible association with some disease condition, and in pharmacogenetics research where genetic contributions are studied in evaluating safety and efficacy of drugs, questions are investigated through large-scale, long-term follow up studies. For economic benefits and safety reasons, data are often accrued sequentially in these studies allowing interim analyses to be performed for making early decisions. Statistical analyses of data in these studies often involve simultaneous testing of a large number of hypotheses, making multiple testing in a sequential framework involving multiple stages a frequently arising statistical problem. This brings newer challenge for developing large-scale multiple testing method that is applicable to a sequential setting with multiple stages and controls an appropriate error rate such as the false discovery rate (FDR), which is the expected proportion of false rejections out of the total number of rejections.====The notion of FDR has been introduced by Benjamini and Hochberg (1995), along with a powerful and easy-to-use method, known as the BH method, that controls the FDR when multiple hypotheses are simultaneously tested in a non-sequential or single-stage setting. Given a set of ==== null hypotheses ==== to be simultaneously tested using their respective ====-values ====, the level ==== BH method is a step-up method with the critical constants ====, ====; that is, it rejects ==== for all ==== such that ====, where ====, provided the maximum exists, otherwise, it rejects none, with ==== being the ordered values of the ====’s. Benjamini and Hochberg (1995) showed that the FDR of their method is less than or equal to ====, where ==== is the number of true null hypotheses, and hence the FDR is controlled at ====, when the ====-values are independent. Later on, Benjamini and Yekutieli (2001), Finner and Roters (2001), Sarkar (2002), and Storey et al. (2004) proved that the BH method’s FDR under independence of the ====-values is actually exactly equal to ====. Benjamini and Yekutieli (2001) and Sarkar (2002) further showed that the FDR of the BH method is less than or equal to ==== under a form of positive dependence condition that is shared by ====-values in many multiple testing situations; see also Finner et al. (2009) and Sarkar (2008). The BH method, as a single-stage multiple testing method, has gained much popularity because of its applicability to a wide variety of scientific investigations and other desirable theoretical properties. Benjamini and Hochberg (2000) suggested the idea of adapting it to the data in an attempt to tighten its FDR control by estimating ==== from the data and incorporating the estimate into it. They, of course, did not offer any theoretical proof of its ultimate FDR control. Several such adaptive versions of the BH method utilizing a wide variety of estimators for ====, with theoretical proofs of their FDR control being offered only under independence, have been put forward in the literature. See, for example, Storey (2002), Storey et al. (2004), Benjamini et al. (2006), Gavrilov et al. (2009), Blanchard and Roquain (2009), and Sarkar (2008).====Papers dealing with FDR control in multi-stage statistical experiments involving simultaneous testing of multiple hypotheses do exist in the literature prior to this work.====Benjamini and Yekutieli (2005) introduced a two-stage procedure in the context of quantitative trait locus (QTL) mapping analysis, where the BH procedure is applied at each stage using the data available only at that stage and the hypotheses rejected at the first stage are further tested at the second stage. With ==== being the number of true nulls and ==== and ==== being chosen differently for the first and second stages respectively, it controls the FDR at level ==== under the same positive dependence condition for which the BH procedure is known to control the FDR.====Zehetmayer and coauthors in a sequence of papers considered the problem of controlling the FDR in the context of gene association or gene expression studies under a sequential setting when the test statistics are normally and independently distributed. Zehetmayer et al. (2005) considered a two-stage design where the total number of observations is fixed with certain fraction of these observations allocated to the first stage and the remaining observations distributed among the hypotheses whose first stage ====-values are less than or equal to a prefixed futility boundary. They defined a sequential ====-value and derived an estimation based approach to controlling the FDR following Storey (2002). Specifically, an estimate of the FDR is derived using the sequential ====-values and a rejection threshold is chosen so that this estimate is less than or equal to a nominal level ====. The hypotheses whose sequential ====-values are less than or equal to this rejection threshold are then rejected. This approach was later extended by Zehetmayer et al. (2008) to control the FDR under multi-stage designs with fixed stagewise sample sizes as well as under multi-stage designs where the overall number of observations is fixed and at each stage a pre-specified fraction of observations is evenly distributed among the selected hypotheses according to some futility boundaries. Zehetmayer and Posch (2012) further assessed the following selection methods used at the first stage in a two-stage design — (i) the hypotheses whose first stage ====-values are less than or equal to some prefixed futility boundary are selected; (ii) a prefixed number of most significant hypotheses are selected; and (iii) the hypotheses rejected by the BH procedure in Benjamini and Yekutieli (2005) at some fixed level ==== are selected — with simulation studies showing that the FDR is controlled in all these scenarios. Zehetmayer et al. (2015) further proposed a sample size reassessment procedure controlling the FDR under a two-stage design. Based on the data available at the first stage, they derived an asymptotic expression of a selected power measure and determined a sample size for the second stage so that the power of the FDR controlling procedure is at some specified value. Their simulation results showed that their sample size reassessment procedure controls the FDR despite the data dependent choice of sample size.====Victor and Hommel (2007) also considered extending the BH procedure. Focusing on a two-stage adaptive design, they derived a method using a generalized definition of sequential ====-value allowing for both early rejection and early acceptance at the interim analysis. They noted, however, that unlike in single testing where a futility boundary can often be determined based on sample size considerations, the choice of futility boundaries in multiple testing is often challenging since the joint distribution of the underlying test statistics is rarely known.====By utilizing sequential ====-values, Malek et al. (2017) proposed a sequential conversion method to transform a fixed sample multiple testing procedure controlling some type I error rate, such as the FDR, to a sequential multiple testing procedure that still controls the same error rate. Specifically, their method applies the fixed sample multiple testing procedure, for example, the BH method, to the sequential ====-values at each stage and allows an early rejection once sufficient evidence has accumulated against the null hypothesis.====Sarkar et al. (2013) extended the BH method and its adaptive version from single- to a two-stage adaptive design setting. More specifically, they considered screening the null hypotheses sequentially at the first stage as being rejected or accepted subject to certain boundaries on the FDR across all hypotheses and testing the remaining null hypotheses at the second stage having combined their ====-values from the two stages using some combination function. These methods were theoretically proved to control the FDR under the assumption that the pairs of first and second-stage ====-values across all hypotheses are independent and those which correspond to the null hypotheses are identically distributed as a pair (====) satisfying the ====-clud property of Brannath et al. (2002). Bartroff and Song (2013) further considered the problem of developing a multi-stage FDR controlling procedure and developed such a procedure by appropriately adjusting the BH critical values at each stage, and assumed independence of the ====-values across the hypotheses to prove its FDR control. However, in most studies involving group sequential design, the ====-values are rarely independent across hypotheses, just as in the case of fixed sample design, and the underlying dependence structure can often be characterized by assuming a positive dependence condition. Therefore, there seems to be an urgent need for developing a multiple testing procedure in a group sequential framework that controls the FDR under positive dependence across hypotheses and effectively identifies the false null hypotheses.====In this paper, we propose such a procedure, which we call the Group Sequential BH (GSBH) procedure. It extends the original BH method from single to multiple stages in a group sequential setting and uses the alpha spending function approach of Lan and DeMets (1983), which is widely used to set group sequential boundaries that control the Type I error rate while allowing flexibility in the number and the timing of the interim looks. With ==== being the fraction of information observed up till ====th interim look, an alpha spending function ==== describes the rate at which the total error rate ==== is spent and it satisfies ==== at the beginning of trial and ==== at the end of trial (Proschan et al. (2006)). In GSBH, we consider allocating to each analysis stage a nominal error level based on some alpha spending function. Then, at each stage, we apply, at the corresponding nominal level, a step-up procedure based on the ====-values that are associated with the active hypotheses (those not rejected in the previous stages) and BH type critical values. We show that the proposed GSBH procedure controls the FDR at level ==== under Assumption 1, Assumption 2 defined in Section 2. These two assumptions considered together can be viewed as an adaptation of the positive dependence condition typically assumed for the BH method to control its FDR, from single to multiple stages. We also propose two adaptive versions of the GSBH procedure. While the first of these adaptive GSBH procedures uses at each stage a standard estimate of ==== obtained from the data available at the first stage, the second does the same at each stage but based on the data available up to that stage. We offer a theoretical proof of the first adaptive GSBH procedure’s control of FDR when the ====-values are independent across hypotheses but positively dependent across stages. For the second proposed adaptive GSBH, however, we provide simulation evidence of its FDR control under the same assumption about the ====-values.====We carry out extensive simulation studies to compare the performance of the three proposed procedures against the BH, which is the ideal one to use had the data been available across all stages. The performance comparisons are made in terms of the FDR control, the average power, the expected proportion of saved samples, and the FNR (the expected proportion of false hypotheses that are accepted out of the total number of accepted hypotheses) in situations where the underlying test statistics are either independent or positively dependent. We perform these studies using the following two alpha spending functions, ==== and ====, where ==== and ==== are respectively the cumulative distribution function and the upper ====th percentile of the standard normal distribution, ==== is the total error rate, and ==== is the information fraction. These two alpha spending functions approximate, respectively, the rejection boundaries of Pocock (1977) and O’Brien and Fleming (1979) for group sequential tests with equal group sizes. The Pocock (1977) rejection boundaries are constant across stages, while O’Brien and Fleming (1979) rejection boundaries change across stages allowing easier rejections at later stages. The outputs of these two alpha spending functions are presented in Table 2 for a four-stage design with equal allocation of the total sample size across the 4 stages and with the total error rate ==== chosen as 0.025. A real data set is also used to demonstrate applications of our proposed procedures under this same design.====The paper is organized as follows. With some concepts and assumptions given in Section 2, we present our proposed group sequential BH procedure and its adaptive versions in Section 3. The numerical findings from simulation studies are given in Section 4, and a real data application is presented in Section 5. Some concluding remarks are made in Section 6 and proofs of all results are given in the Appendix.",Group sequential BH and its adaptive versions controlling the FDR,https://www.sciencedirect.com/science/article/pii/S0378375818301320,March 2019,2019,Research Article,360.0
"Clairon Quentin,Brunel Nicolas J.-B.","School of Mathematics and Statistics, University of Newcastle, UK,ENSIIE & LaMME, Université Paris Saclay, France","Received 6 September 2017, Revised 26 April 2018, Accepted 25 June 2018, Available online 10 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.06.005,Cited by (5),"We address the problem of parameter estimation for partially observed linear ====. Estimation from time series with standard estimators can give misleading results because estimation is often ill-posed, or the models are misspecified. The addition of a forcing function ====, that represents uncertainties in the original ODE, can overcome these problems as shown in Clairon and Brunel (2017). A general regularized estimation procedure is derived, that corresponds to an ====consistency. A significant improvement is the avoidance of the estimation of initial conditions thanks to a profiling step. Consequently, we can deal with more elaborated penalties and also provide a profiled semiparametric estimation procedure in the case of time-varying parameters. Simulations and real data examples show that our approach is generally more accurate and more reliable than reference methods when the ","Ordinary Differential Equation (ODE) models are widely used in life and engineering sciences in order to predict, understand or control real-life systems. ODE modeling is relatively simple, computationally efficient and easy to understand. It can describe the main mechanisms of complex systems and provide reasonable approximations of stochastic dynamics. ODE models are standard in population dynamics and epidemiology (Ellner and Guckenheimer, 2006), virology (Nowak and May, 2000), or in the description of gene regulation networks Marbach et al. (2012), Wu et al. (2014). A model takes usually the form ====, where ==== is a vector field, ==== is the state in ====, and ==== is an unknown parameter.====We focus on the estimation problem of partially observed systems modeled by a linear ODE. At random times ====, we have ==== observations ==== in ==== which are realizations of the random variables ====where ==== is the true trajectory at ====, ==== is a random noise and ==== is an observation matrix of size ====. We assume that there is a true parameter ==== belonging to a subset ==== of ====, such that ==== is the unique solution of the linear ODE ====for ==== with initial condition ====. We want to estimate ==== and ==== from the data ====. The solution of (2) for a given ====, and initial condition ==== is denoted ==== and so ====.====Maximum likelihood estimation of ==== (MLE) is feasible for fully parametric models Li et al. (2005), Walter and Pronzato (1997), Pronzato (2008). In this work, however, we will avoid parametric assumptions on ====, meaning that Nonlinear Least Squares (NLS) estimation is a natural starting point. Here, the estimator ==== is obtained by minimizing the residuals sum of squares ====simultaneously on ==== and ====. ODE models are often sufficiently regular so that we can obtain consistent, asymptotically normal and efficient estimators (see Wu (1981) or van der Vaart (1998) (chapter 5)). In some circumstances however, the use of NLS for ODE model parameters can lead to difficulties over and above the computational burden of repeated ODE integration and nonlinear optimizations. Indeed, there are intrinsic difficulties that makes estimation an ill-posed inverse problem Engl et al. (2009), Stuart (2010):====
               ====An additional difficulty can arise from misspecification of the assumed ODE for ====. In such cases, NLS is known to produce biased parameter estimates and state predictions (Brynjarsdottir and O’Hagan, 2014). Despite the difficulty of quantifying the effect of misspecification, recent work has shown that taking into account the model error can significantly improve the quality of estimation Brynjarsdottir and O’Hagan (2014), Tuo and Wu (2015).====Alternative statistical estimators have been developed for regularizing the inverse problem or coping with misspecified models Ramsay et al. (2007), Brunel et al. (2014), Brunel and Clairon (2015), Gugushvili and Klaassen (2012). In this work, we focus on methods that minimize criteria of the form ====, where ==== is an approximation of the solution of (2). In practice, these criteria are smoother than ==== and take into account a possible discrepancy between data, the assumed model and the real model. A common approximation method is Generalized Profiling (GP, also denoted Generalized Smoothing in Ramsay et al. (2007) based on a B-splines fitted with a model-based penalty. In a different way, the Tracking estimator is based on an approximation ====, which is a solution of a perturbed ODE ==== where ==== is a perturbation in ====, see Brunel and Clairon (2015), Clairon and Brunel (2017). The estimation problem is then to identify the parameter ==== that needs the smallest perturbation ====, such that the solution ==== is close to the data, the perturbation magnitude being measured by its ==== norm ====. In order to benefit from a continuous-time setting and to remove some noise, the original data (====) are pre-smoothed and replaced by a nonparametric smoother ==== (typically splines or local polynomials). The computation of ==== is carried out by using tools of (continuous-time) optimal control, which lead to feasible and efficient estimator even when the model is badly-conditioned. The estimation procedure derived in Clairon and Brunel (2017) is based on repeated solving of ==== boundary value problems which can be time consuming. As a consequence of the Pontryagin Maximum Principle, the estimation of the unknown initial condition ==== is necessary even if this is of no interest per se. This complexity might reduce the applicability of Tracking to relatively small ODE models. Here, we specialize the Tracking estimator to the case of partially observed linear systems with unknown initial conditions. This is a considerable improvement on the case studied in Brunel and Clairon (2015), which was restricted to fully observed linear systems with known initial conditions.====We show how to profile efficiently on unknown initial conditions, and we consider also model perturbations ==== that are not necessarily small but smooth i.e. with a small norm ====, but with a possibly large norm ====. As we show, the ability to deal with various penalties and types of perturbations is linked to the ability to profile over the initial conditions. We illustrate the interest of our approach by deriving semiparametric estimation procedures in various cases, emphasizing the advantage of profiling over the nuisance parameters compared to the general method.====As in Brunel and Clairon (2015), we use the fact that the optimal control problem defining our statistical estimator can be solved by the linear–quadratic theory (Sontag, 1998). We show that the Tracking estimator minimizes a profiled criterion ==== related to a value function that is computed recursively with the Riccati equation and the famous deterministic Kalman filter (Sontag, 1998). As a consequence, we obtain a feasible algorithm in which all the quantities of interest are tractable. In particular, we can derive an estimate of ==== without estimating either the initial condition ====, or the states ====.====When the model is assumed to be well-specified, we prove that the corresponding estimators are root==== ==== consistent and asymptotically normal. We derive then a consistent estimate for the state, including the hidden values, while considering the problem of parameter identifiability. For tackling the identifiability problem, we derive a tractable criterion inspired from control theory. Moreover, we consider the use of more general penalties such as ==== or the Sobolev norm ====, and we show that enforcing regularity for ==== can be beneficial for parameter estimation.====In the next section, we introduce the criterion ==== that defines the parametric estimator ====, we discuss the influence of the choice of penalty for ====, and we derive an estimator for the state. In Section 3, we investigate the existence and regularity of the criterion ====. In Section 4, we focus on the particular case of well-specified models. We show that ==== and the state estimator are consistent under regularity assumptions and that we reach the ==== ==== rate when the nonparametric estimator ==== is a regression spline. We then compare the Tracking Estimator with NLS and GP in two ill-posed models for several statistical settings. We show that the Sobolev penalty ==== improves parametric estimation, that the simultaneous estimation of ==== and ==== dramatically changes the difficulty of the statistical problem, and that profiling is highly beneficial in that case. In Section 5, we focus on estimation under model misspecifications and use simulations to illustrate the benefits of our method. Moreover, we adapt our methodology to the estimation of a time-varying parameter by using an appropriate penalty. In Section 6, we apply the method to two real datasets, one on enzyme-diffusion and one on microbiotal population dynamics.",Tracking for parameter and state estimation in possibly misspecified partially observed linear Ordinary Differential Equations,https://www.sciencedirect.com/science/article/pii/S0378375818301095,March 2019,2019,Research Article,361.0
"Zhang Qiao-Zhen,Dai Hong-Sheng,Liu Min-Qian,Wang Ya","School of Statistics and Data Science & LPMC, Nankai University, Tianjin 300071, China,Department of Mathematical Sciences, University of Essex, Wivenhoe Park, Colchester, CO4 3SQ, UK,State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System (CEMEE), Luoyang, 471003, China","Received 31 August 2016, Revised 25 June 2018, Accepted 26 June 2018, Available online 3 July 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.06.006,Cited by (2),"-optimal augmented design would minimize the error variances of the parameter estimators of secondary factors. In addition, a blocking factor will be involved to describe the mean shift between two stages. Simulation results show that the method performs very well in certain settings.","Screening is the first phase of an experimental study on systems and simulation models. Its purpose is to eliminate negligible factors so that efforts may be concentrated upon just the important ones (active factors). Using a supersaturated design (SSD) whose run size is not enough for estimating all the main effects may be considered when a large experiment is infeasible in practice. SSDs were introduced by Box (1959), but not studied further until the appearance of the work by Lin (1993) and Wu (1993). Many developments in the area have taken place over the last two decades. For further details, please refer to Georgiou (2014), Sun et al. (2011) and the references therein.====The analysis of SSDs is challenging due to the inherent non-full rank nature of the design matrix and the fact that the columns of the model matrix are correlated. As a result, the effects of different factors are aliased with one another making it very difficult to identify the active factors correctly. Methods to overcome these problems include regression procedures, such as forward selection (Westfall et al., 1998), stepwise and all-subsets regression (Abraham et al., 1999), partial least squares methods [38], [37], shrinkage methods, including SCAD (Li and Lin, 2002) and Dantzig selector (Phoa et al., 2009) and Bayesian methods [3], [9], [10], [20]. Readers can refer to Salawu et al. (2015) and Georgiou (2014). However, different methods may give different results and no method is infallible.====If we want to clarify or confirm initial results and guide the next phase of experimentation, adding follow-up runs to the initial design is a useful way. As a matter of fact, performing extra experimental runs is the only data-driven way to break confounding patterns and to disentangle confounded effects. Suppose an SSD of ==== runs and ==== 2-level factors, denoted by SSD====, has been run and now the experimenter can afford ==== more runs to resolve ambiguities, the target is to find the best way to augment the original design to reduce uncertainty and get the most information out of the final SSD====. Gupta et al. (2010) considered the problem for 2-level SSDs firstly: ====-optimal designs (proposed by Booth and Cox, 1962) are augmented with additional runs to create a new class of “extended ====-optimal” designs. Then Gupta et al. (2012) extended the method to ====-level designs. Suen and Das (2010) also used a similar approach to add or remove one row from an existing ====-optimal design to make a new ====-optimal design. Qin et al. (2015) studied the optimality of the extended design generated by adding few runs to an existing ====-optimal mixed-level SSD and their paper covers the work of [18], [17] as two special cases. All of these methods, however, did not consider using the information from the initial analysis and design when adding runs. Gutman et al. (2014) proposed an SSD augmentation strategy using the Bayesian ====-optimality criterion, they considered the information gained from the initial design, SSD====, as a prior, and constructed the final SSD==== to reduce the error variances of the parameter estimators under the Bayesian paradigm.====When adding runs to fractional factorial designs, two optimality criteria, ====-optimality and ====-optimality, are often used. The ====-optimal design approach would be applied if the experimenters emphasize precise estimation for the “subset” of the experimental factors. Kiefer and Wolfowitz (1961) defined a design as ====-optimal if it minimizes the determinant of the normalized covariance sub-matrix of estimators of the chosen model parameters while treating the other parameters as nuisance parameters. The use of ====-optimality designs would result in increased power since the parameters of interest are estimated more precisely [2], [8]. In this paper, we will combine the ====-optimality criterion with the Bayesian technique to propose an alternative approach, which is different from the Bayesian ====-optimal augmentation in two aspects: the principle of factor classification and the optimal criterion.====The next section reviews the relevant background firstly, then we propose the new algorithmic augmentation strategy for SSDs using information from the initial runs in Section 3. Section 4 compares the performance of the Bayesian ====-optimal augmented designs with the Bayesian ====-optimal augmented designs by several highlighting examples. Some concluding remarks are provided in Section 5.",A method for augmenting supersaturated designs,https://www.sciencedirect.com/science/article/pii/S0378375818301137,March 2019,2019,Research Article,362.0
"Wang Rui,Xu Xingzhong","School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 100081, China,Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing 100081, China","Received 2 August 2017, Revised 27 February 2018, Accepted 16 June 2018, Available online 23 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.06.003,Cited by (7),The strength of ,"Suppose ==== are ====-variate independent and identically distributed (i.i.d.) random vectors with mean vector ==== and covariance matrix ====. In this paper, we consider the problem of testing the hypotheses ====A classical test statistic for hypotheses (1) is Hotelling’s ====, defined as ====, where ==== is the sample mean vector and ==== is the sample covariance matrix. Under normal distribution, Hotelling’s ==== is the likelihood ratio test and enjoys desirable properties in fixed ==== setting. See, for example, Anderson (2003). However, Hotelling’s test cannot be defined when ==== due to the singularity of ====. In a seminal paper, Bai and Saranadasa (1996) considered two sample testing problem and proposed a statistic by removing ==== from Hotelling’s ==== statistic. They studied the asymptotic properties of their test statistic when ==== tends to a positive constant. Many subsequent papers generalized the idea of Bai and Saranadasa (1996) to more general models [16], [3], [18]. The critical values of existing high dimensional tests mostly rely on the asymptotic normality of the test statistics. We call it asymptotic method. However, if ==== has spiked eigenvalues, the asymptotic normality of the test statistics is not be valid (Katayama et al., 2013). In this case, the asymptotic methods cannot satisfactorily control the level.====The randomization test method is a tool to determine the critical value for a given test statistic. The idea of randomization tests dates back to Fisher (1935). See Romano (1990) for a general construction of randomization test. Its strength is in that the resulting test procedure has exact level under mild condition. There are many papers concerning the theoretical properties of randomization tests for fixed ==== case. See, for example, Romano (1990), Zhu (2000) and Chung and Romano (2016). In high dimensional setting, randomization tests are widely used in applied statistics [17], [6], [10]. However, little is known about the theoretical properties of the randomization test in high dimensional setting.====In this paper, we consider the following randomization method. Suppose ==== is certain test statistic for hypotheses (1). Let ==== be i.i.d. Rademacher variables (====) which are independent of data. The randomization test rejects the null hypothesis when ==== is greater than the ==== quantile of the conditional distribution of ==== given ====, and accepts the null hypothesis otherwise, where ==== is the significant level and the ==== quantile of a distribution function ==== is defined as ====. In fixed ==== setting, it is well known that randomization tests consume much more computing time than the asymptotic method, which historically hampered the use of randomization tests. The goal of this paper is to show that in high dimensional setting, randomization tests can be computationally feasible and have desirable statistic properties. Inspired by the work of Bai and Saranadasa (1996) and Chen and Qin (2010), we propose a randomization test for hypotheses (1). We give an implementation of the proposed randomization test, which is computationally feasible. We also investigate the asymptotic behavior of the test procedure. Our results show that even if the null distribution of ==== is not symmetric, the randomization test is still asymptotically exact under fairly general assumptions. Hence the test procedure is robust. In particular, the proposed test can be applied to situations where the asymptotic method is not valid. We also derive the asymptotic power function of the proposed test. To the best of our knowledge, this is the first work which gives the asymptotic behavior of randomization tests in high dimensional setting. A simulation study is carried out to examine the numerical performance of the proposed test and compare with the asymptotic method and the bootstrap method. Compared with its competitors, the proposed test reduces the size distortion while still possesses reasonable test power.====The rest of the paper is organized in the following way. In Section 2, we propose a randomization test and give a fast implementation. In Section 3, we investigate the asymptotic behavior of the proposed test. The simulation results are reported in Section 4. The technical proofs are presented in Appendices.",A feasible high dimensional randomization test for the mean vector,https://www.sciencedirect.com/science/article/pii/S0378375818300879,March 2019,2019,Research Article,363.0
"Ou Zujun,Zhang Minghui,Qin Hong","College of Mathematics and Statistics, Jishou University, Jishou 416000, China,College of Mathematics and Statistics, Central China Normal University, Wuhan 430079, China","Received 19 October 2017, Revised 4 March 2018, Accepted 11 June 2018, Available online 20 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.06.002,Cited by (6),"-discrepancy of triple design is expressed by the wordlength pattern of its initial design, and a tight lower bound of the wrap-around ====-discrepancy of triple design is obtained. An efficient method for constructing uniform minimum aberration designs is proposed based on the projection of triple design. These constructed designs have better properties, such as minimum aberration and lower discrepancy, than existing uniform designs, and are recommended for use in practice.","Fractional factorial designs are widely used in various area such as manufacturing, pharmaceutics, sciences and engineering. They are often chosen by the minimum aberration criterion (Fries and Hunter, 1980) and its extension, generalized minimum aberration criterion (Xu and Wu, 2001) and minimum generalized criterion (Ma and Fang, 2001).====Chen and Cheng (2006) discussed the method of doubling for constructing two-level fractional factorial designs, in particular, those of resolution IV. Suppose that ==== is an ==== matrix with two distinct entries, 1 and –1. Then the double of ==== is the ==== matrix ====Doubling plays an important role in the construction of maximal designs. Xu and Cheng (2008) developed a general complementary design theory for doubling. One can refer to Hu and Zhang (2009) and [12], [13] for more details about doubling. From the definition of double design ====, both ==== and ==== can be regarded as the level permutation of ====, and ==== is just the orthogonal combination of all possible level permutations of ====. We may wonder whether the doubling process by orthogonal combination of all possible level permutations of two-level design could be extended to three-level design.====For designs with more than two levels, level permutation of one or more factors can alter their geometrical structures and statistical properties (Cheng and Ye, 2004). Recently, by considering all possible level permutations, Tang et al. (2012) proposed a novel construction method for three-level uniform designs under uniformity criterion measured by the centered ====-discrepancy, Tang and Xu (2013) and Xu et al. (2014) extended their results to multi-level designs under uniformity criterion in terms of the centered ====-discrepancy and wrap-around ====-discrepancy, respectively. Furthermore, in Zhou and Xu (2014), this idea has been generalized into fractional factorial designs with any number of levels and any discrepancy defined by a reproducing kernel. Tang and Xu (2014) studied level permutations for regular fractional factorial designs in order to improve their efficiency for screening quantitative factors.====The current research is inspired by the idea of doubling in the sense of orthogonal combination of all possible level permutations of its initial design. A method of tripling for three-level design, which triples both the run size and number of factors of initial three-level design, is proposed by combining all possible level permutations of its initial design in this paper. The wrap-around ====-discrepancy of triple design is expressed by the wordlength pattern of its initial design, and a tight lower bound of the wrap-around ====-discrepancy of triple design is obtained. Therefore, triple designs of generalized minimum aberration designs tend to have good uniformity under the wrap-around ====-discrepancy. Based on these theoretical results, an efficient method for constructing uniform minimum aberration designs (Tang et al., 2012) is proposed by the projection of triple design. These constructed designs have better properties, such as minimum aberration and lower discrepancy, than existing uniform designs, and are recommended for use in practice.====The paper is organized as follows. In Section 2, some notations and preliminaries are included. Section 3 gives the concept of triple design and the uniformity of triple design under the wrap-around ====-discrepancy is studied, the wrap-around ====-discrepancy of triple design is expressed by the generalized wordlength pattern of its initial design, and a tight lower bound of the wrap-around ====-discrepancy of triple design is obtained. In Section 4, an efficient method for constructing uniform minimum aberration designs is proposed by the projection of triple design, numerical results show that the constructed designs have better properties, such as minimum aberration and lower discrepancy, than existing uniform designs. Finally, some conclusions are given in Section 5.",Tripling of fractional factorial designs,https://www.sciencedirect.com/science/article/pii/S0378375818300740,March 2019,2019,Research Article,364.0
"Yang Guangren,Yao Weixin,Xiang Sijia","Department of Statistics, School of Economics, Jinan University, Guangzhou, 510632, PR China,Department of Statistics, University of California, Riverside, CA 92521, USA,School of Data Sciences, Zhejiang University of Finance & Economics, Hangzhou, Zhejiang, 310018, PR China","Received 22 October 2017, Revised 10 April 2018, Accepted 10 April 2018, Available online 18 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.04.004,Cited by (0)," in multivariate nonparametric regressions with non-Gaussian responses, including continuous, binary or count data. In this paper, we propose a fast and efficient feature screening method for GAMs with ultrahigh dimensional ====. We provide some theoretical justifications for our screening method and establish the sure screening property. We further examine the finite sample performance of the proposed screening procedure and compare it with some existing methods via ====. Three real data examples are used to illustrate the effectiveness of the new method.","Generalized linear models (GLMs) have been well studied in the literature, and variable selection via penalized likelihood has been developed for GLMs with high-dimensional covariates. With modern data gathering devices and vast data storage space, ultrahigh-dimensional data have been collected in various research areas such as proteomics studies, finance, tumor classification and biomedical imaging. Direct applications of variable selection based on penalized likelihood may not perform well for ultrahigh dimensional data due to the algorithmic stability, computational cost and statistical accuracy. Fan and Lv (2008) proposed a sure independence screening (SIS) procedure for linear models using Pearson correlation coefficient as the marginal utility and further established the sure screening property of their procedure under Gaussian linear model framework. Fan and Song (2010) extended SIS to GLMs with NP-dimensionality. Hall and Miller (2012) proposed a feature screening procedure for transformation of linear models using generalized correlation. Fan et al. (2009) proposed a SIS procedure for generalized linear models based on marginal likelihood estimates. Xia et al. (2016a), Xia et al. (2016b) considered the variable screening with dichotomous response data under generalized varying coefficient models. More details about these marginal feature screening procedures can be found in the recent review paper on feature screening by Liu et al. (2015).====In many applications, it may be too restrictive to assume the effect of all covariates be captured by a simple linear form. Unlike GLMs, generalized additive models (GAMs) introduced by Hastie and Tibshirani (1986), Hastie and Tibshirani (1990) allow for greater flexibility by modeling the linear predictor as a sum of nonparametric functions of each covariate. GAMs include GLMs as special cases and have less bias. In addition, after fitting a GAM, we can also test whether a GLM is well-specified. GAMs have gained popularity by addressing the curse of dimensionality in multivariate nonparametric regressions with non-Gaussian responses, including continuous, binary or count data. In this paper, we propose a fast and efficient feature screening procedure for GAMs with ultrahigh dimensional covariates. We provide some theoretical justifications for our screening method and establish the sure screening property. A greedy version of the proposed feature screening method is also provided to further improve the performance. We examine the finite sample performance of the proposed screening procedure and compare it with some existing methods via Monte Carlo simulations. We further illustrate the effectiveness of the proposed procedure by applications of three real data examples.====Many penalized methods have been proposed to select significant components for additive models when the response is continuous. Cui et al. (2013) proposed method that is a combination of penalized regression spline approximation and group variable selection in additive models. Huang et al. (2010) proposed a two-step approach to select and estimate the nonzero components simultaneously in additive models when ==== is fixed. It used the group Lasso in the first stage and the adaptive group Lasso in the second stage. Meier et al. (2009) considered the problem of estimating a high-dimensional additive model when ====. Ravikumar et al. (2009) studied a new class of methods for high dimensional non-parametric regressions and classifications, namely the sparse additive models. Xue (2009) considered a penalized polynomial spline method for additive models when ==== is fixed. The method can select significant components and estimate non-parametric additive function components simultaneously. More recently, Fan et al. (2011) suggested several closely related variable screening procedures in sparse ultrahigh dimensional additive models. However, as far as we know, none of the above methods can be directly applied to binary or count data for ultrahigh dimensional setting. Our paper tries to fills this gap and makes additive models much more applicable.====The rest of this paper is organized as follows. In Section 2, we introduce a new feature screening procedure for ultrahigh-dimensional GAMs, and study its theoretical properties. Effective algorithms for the procedure are presented in Section 3. In Section 4, we use Monte Carlo studies and three real data examples to demonstrate the finite sample performance of the new procedure. Some discussion and conclusion remarks are given in Section 5. Technical proofs are deferred to the supplement file.",Sure independence screening in ultrahigh dimensional generalized additive models,https://www.sciencedirect.com/science/article/pii/S037837581830065X,March 2019,2019,Research Article,365.0
"Giurcanu Mihai,Presnell Brett","Department of Public Health Sciences, University of Chicago, 5841 S Maryland Ave, Room R325, Chicago, IL, 60637, USA,Department of Statistics, University of Florida, 225 Griffin-Floyd Hall, Gainesville, FL, 32611, USA","Received 10 November 2016, Revised 29 May 2018, Accepted 30 May 2018, Available online 14 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.05.007,Cited by (5),"We study the consistency of the standard (non-parametric) ====, the ==== in practice.","The least absolute shrinkage and selector operator (LASSO) was proposed by Tibshirani (1996) as an estimation and variable selection procedure for regression models. The LASSO is a penalized least-squares estimation method which combines the favorable properties of variable selection and ridge regression. Since the publication of the paper of Tibshirani (1996), many similar estimators have been proposed in the literature. The fused LASSO was proposed by Tibshirani et al. (2005) for regression models in which the predictors can be ordered in a meaningful way. Motivated by the fact that the LASSO does a poor job in simultaneously selecting groups of highly correlated predictors, Zou and Hastie (2005) proposed the elastic net by combining the ==== penalty of the LASSO with the ==== penalty of the ridge regression.====There has also been a focus on estimators with the ==== (Fan and Li, 2001, p. 1353), i.e., estimators that identify the zero regression coefficients with probability tending to one and with the same limiting distribution as of the least-squares estimator (LSE) obtained under the additional assumption that the set of non-zero regression coefficients were known in advance. These include the smoothly clipped absolute deviation (SCAD) estimator (Fan and Li, 2001), the Bridge estimator Frank and Friedman (1993), Knight and Fu (2000), the adaptive LASSO (ALASSO) of Zou (2006), and the thresholding least-squares estimator (TLSE) of Giurcanu (2016), among others.====It is well known that fixed parameter consistency does not fully describe the asymptotic behavior of non-regular estimators and of their bootstrap distributions. Specifically, Beran (1997) established that, typically, the bootstrap distributions of non-locally asymptotically equivariant estimators, such as the Hodges and Stein estimators, are inconsistent over the set of superefficiency points. Samworth (2003) showed in a simulation study that, in the case of the Hodges and Stein estimators, although the standard bootstrap is inconsistent over the set of superefficiency points, it performs better than the ==== bootstrap in small neighborhoods of this set even though the ==== bootstrap is pointwise consistent. Similarly, Pötscher and Leeb (2009) proved an impossibility result which shows that pointwise consistent distribution function estimators are not uniformly consistent for some LASSO-type estimators.====In this paper, we derive the exact asymptotic behavior of bootstrap distributions of the Bridge and the ALASSO. Similar results can be obtained for other estimators with the oracle property, such as the SCAD and the TLSE. Generally, there are two ways to center the bootstrap distributions (Efron, 1979), either at the sample version of the parameter (substitution principle) or at the corresponding estimator (plug-in principle). We show that when the regression parameter is sparse, i.e., when some regression coefficients are zero, then the substitution standard bootstrap distribution converges ==== to a random distribution, and thus, it is inconsistent, while the plug-in standard bootstrap is consistent. We also show that the ==== bootstrap (Bickel and Freedman, 1981) and the oracle bootstrap (Giurcanu, 2012) distributions are consistent. The oracle bootstrap is a particular version of the empirical-likelihood bootstrap (Giurcanu and Presnell, 2018) used in conjunction with non-regular estimators. We further study the large sample local behavior of LASSO-type estimators and of their bootstrap distributions. Our results show that when some regression coefficients are “very small” (in magnitude), then the substitution standard bootstrap is inconsistent and that the plug-in standard bootstrap, the ==== bootstrap, and the oracle bootstrap are consistent. Furthermore, when some regression coefficients are “moderately small”, then none of the bootstrap methods is consistent. Finally, when some “small” regression coefficients are “moderately large”, then all bootstrap methods are consistent. We also identify some additional difficulties for the ==== bootstrap in this situation.====We conclude this section with an outline. In Section 2, we review the oracle properties of the Bridge and the ALASSO. In Section 3, we present the asymptotic properties of the standard bootstrap, the ==== bootstrap, and the oracle bootstrap distributions. In Section 4, we describe the large sample local behavior of the estimators and of their bootstrap distributions. In Section 5, we present the results of a simulation study of the finite sample local behavior of the ALASSO and its bootstrap distributions. In Section 6, we perform a data analysis of a prostate cancer data set to illustrate an application of ALASSO and bootstrap inference in practice. The proofs of theoretical results can be found in an online supplementary material.",Bootstrapping LASSO-type estimators in regression models,https://www.sciencedirect.com/science/article/pii/S0378375818300648,March 2019,2019,Research Article,366.0
Ogihara Teppei,"The Institute of Statistical Mathematics, 10-3 Midori-cho, Tachikawa, Tokyo 190–8562, Japan,PRESTO, Japan Science and Technology Agency, Japan,School of Multidisciplinary Sciences, SOKENDAI (The Graduate University for Advanced Studies), Japan","Received 21 June 2017, Revised 7 March 2018, Accepted 6 June 2018, Available online 13 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.06.001,Cited by (1),We study the ==== of Bayes-type estimators and ==== and convergence of moments for Bayes-type estimators with general loss functions.,"The theory of random fields of likelihood ratios is a powerful tool to investigate the asymptotic behavior of Bayes-type estimators. This theory was initiated by Ibragimov and Has’minskiĭ (1972), Ibragimov and Has’minskiĭ (1973), Ibragimov and Has’minskiĭ (1981), who applied it to statistical models of regular independent and identically distributed (i.i.d.) observations and Gaussian white-noise models. After that, Kutoyants applied Ibragimov–Has’minskiĭ theory to other statistical models, including diffusion-type and point processes. See Kutoyants (1984), Kutoyants (1994) for the details. Yoshida (2006), Yoshida (2011) introduced polynomial-type large deviation inequalities and gave a scheme to obtain asymptotic properties of the ====-estimator and the Bayes-type estimator under certain moment conditions for the contrast function and its derivatives. This scheme can be applied to many classes of statistical model and gives the consistency, asymptotic (mixed) normality and convergence of moments for quasi-maximum likelihood estimators and Bayes-type estimators. See Yoshida (2006), Yoshida (2011) for an application to ergodic diffusion processes, Ogihara and Yoshida (2011) for ergodic jump diffusion processes, Masuda (2010) for Ornstein–Uhlenbeck processes driven by heavy-tailed symmetric Lévy processes, Uchida and Yoshida (2013) for diffusion processes observed in a fixed interval, Ogihara and Yoshida (2014) for diffusion processes with nonsynchronous observations.====One of the most important motivations for the study of quasi-maximum likelihood estimators and Bayes-type estimators is that these estimators are asymptotically efficient in several models. For statistical models of regular i.i.d. observations, we can obtain minimax theorems for estimation errors, and hence can define the concept of asymptotic efficiency of estimators. Since the maximum likelihood estimator and the Bayes estimator attain minimax bounds, these estimators are asymptotically efficient. See Ibragimov and Has’minskiĭ (1981) for details. We also have the asymptotic efficiency of quasi-maximum likelihood estimators and Bayes-type estimators for some statistical models of diffusion processes with discrete observations. Jeganathan (1983) extended the minimax theorems to statistical models that satisfy the local asymptotic mixed normality (LAMN) property. Moreover, Gobet (2001) proved the LAMN property for models of diffusion processes observed in a fixed interval and the estimators proposed in Genon-Catalot and Jacod (1994) have asymptotic minimal variance. Gobet (2002) proved local asymptotic normality for statistical models of ergodic diffusion processes, and Ogihara (2015) gives the LAMN property and asymptotic efficiency of the quasi-maximum likelihood estimator and the Bayes-type estimator proposed in Ogihara and Yoshida (2014) for diffusion processes with nonsynchronous observations in a fixed interval.====Convergence of moments is another important asymptotic property used in the study of asymptotic expansions of estimators and information criteria. Yoshida (2006), Yoshida (2011) applied polynomial-type large deviation inequalities to the Bayes-type estimator with a quadratic loss function. This estimator can be obtained as a ratio of certain integrals with respect to the parameter if the loss function is quadratic, and hence its asymptotic behavior can be specified by using polynomial-type large deviation inequalities. Along another line, Ibragimov and Has’minskiĭ (1981) considered the asymptotic behavior of Bayes-type estimators with a wider class of loss functions. Though their results apply mainly to models of i.i.d. observations, we can apply their ideas to models satisfying polynomial-type large deviation inequalities. This allows us to derive asymptotic properties of Bayes-type estimators with a wider class of loss functions, which is the subject of this paper.====In this paper, we extend the results of asymptotic normality and convergence of moments of a Bayes-type estimator with a quadratic loss function given in Yoshida (2011) to a Bayes-type estimator with a general loss function. By changing the loss function, we can set a large penalty when the estimator is far from the true parameter value. Then, the Bayes-type estimator becomes sensitive to tail risk. Conversely, by reducing penalty for large errors, we obtain an estimator which is tolerant of such risk compared to the case of the quadratic loss function. If the loss function is quadratic, the Bayes-type estimator can be written as a ratio of some integrals with respect to parameters, as seen at the beginning of Section 5. In this case, the integrals can be estimated by dividing the domains of integration into ring domains and using tail probability estimates, such as ==== below. However, we cannot use this approach to obtain asymptotic results for a Bayes-type estimator with a general loss function because we do not have an expression with integrals ratio. Here, we apply the idea used for the derivation of Theorem 8.2 in Ibragimov and Has’minskiĭ (1981) and prove the asymptotic equivalence of the estimation error of the Bayes-type and ====-estimators. By doing this, we obtain the asymptotic distribution of the estimation error of the Bayes-type estimator if we have the asymptotic distribution of the ====-estimator. In particular, we see that the asymptotic distribution for the Bayes-type estimator does not depend on the loss function. These results can be applied to ergodic diffusion processes, diffusion processes observed in a fixed interval, ergodic jump diffusion processes and diffusion processes with nonsynchronous observation; and we obtain asymptotic (mixed) normality and convergence of moments for Bayes-type estimators with general loss functions. We focus on the application to ergodic diffusion processes and ergodic jump diffusion processes in this paper.====This paper is organized as follows. In Section 2, we present the theory of random fields of likelihood ratios and polynomial-type large deviation inequalities, and we state our main results. Section 3 is devoted to the application of the main results to ergodic diffusion processes. The application to ergodic jump diffusion processes is studied in Section 4. The proofs are collected in Section 5.",On the asymptotic properties of Bayes-type estimators with general loss functions,https://www.sciencedirect.com/science/article/pii/S0378375818300661,March 2019,2019,Research Article,367.0
"Loonis V.,Mary X.","Insee, Division des Méthodes et des Référentiels Géographiques, Paris, France,Modal’X, UPL, Univ Paris Nanterre, F92000 Nanterre, France","Received 24 March 2017, Revised 23 May 2018, Accepted 27 May 2018, Available online 5 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.05.005,Cited by (1),"In this article, recent results about point processes are used in sampling theory. Precisely, we define and study a new class of sampling designs: determinantal sampling designs. The law of such designs is known, and there exists a simple selection algorithm. We compute exactly the variance of linear estimators constructed upon these designs by using the first and second order ","The goal of sampling theory is to acquire knowledge of a parameter of interest ==== using only partial information. The parameter ==== is a function of ====, usually the sum or the mean of the ====’s. This is done by means of a sampling design, through which a random subset ==== is observed, and the construction of an estimator ==== of ==== based on this random sample. The properties of the sampling design are thus of crucial importance to get “good” estimators. In practice, the following issues are fundamental: simplicity of the design (in terms of its definition, theory and/or drawing algorithm), knowledge of the first and, possibly, second order inclusion probabilities, control of the size of the sample, effective construction, in particular with prescribed unequal probabilities, statistical amenability (consistency, central limit theorem, …), low Mean Square Error (MSE)/Variance of specific estimators based on the design.====In this article, we introduce a new parametric family of sampling designs indexed by Hermitian contracting matrices, ====, that addresses all these issues. Section 2 gives their definition and probabilistic properties. In particular, it is shown that for this family, inclusion probabilities are known for any order. Section 2 also provides a sampling algorithm. Section 3 studies the statistical properties of linear estimators of a total. It gives algebraic and geometric formulas for the ==== which provide necessary and sufficient conditions for obtaining a perfectly balanced determinantal sampling design. In addition, we give asymptotic theorems and concentration inequalities. Section 4 provides effective constructions of fixed size determinantal sampling designs with fixed first order inclusion probabilities. Optimization problems and algorithms are then discussed in Section 5, and applied on a real data set. While the use of determinantal processes allows to derive directly statistical results in the field of survey sampling from well known results in point process theory (Definition 2.1, Theorems 2.1, 3.3, 3.5, Algorithm 2.1), innovative results can nevertheless be found: Theorems 3.1, 3.2, 4.1, 4.4, 5.1, or Algorithms 5.1 to 5.5. We also make connections with other theories (frame theory or semidefinite optimization).",Determinantal sampling designs,https://www.sciencedirect.com/science/article/pii/S0378375818300533,March 2019,2019,Research Article,368.0
"Liu Guanfu,Li Pengfei,Liu Yukun,Pu Xiaolong","School of Statistics and Information, Shanghai University of International Business and Economics, Shanghai 201620, China,Department of Statistics and Actuarial Sciences, University of Waterloo, Waterloo, ON, Canada N2L 3G1,,School of Statistics, East China Normal University, Shanghai 200241, China","Received 30 November 2016, Revised 3 May 2018, Accepted 21 May 2018, Available online 5 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.05.003,Cited by (6),"We provide a general and rigorous proof for the strong consistency of ==== of the cumulative distribution function of the mixing distribution and structural parameter under finite mixtures of location-scale distributions with a structural parameter. The consistency results do not require the parameter space of location and scale to be compact. We illustrate the results by applying them to finite mixtures of location-scale distributions with the component density function being one of the commonly used density functions: normal, logistic, extreme-value, or ====. An extension of the strong consistency results to finite mixtures of multivariate ==== is also discussed.","Suppose we have an independent and identically distributed (====) sample ==== from the following finite mixture model: ====Here ====, the component density function, is assumed to come from a location-scale distribution family, namely, ==== with ==== and ==== being the location and scale parameters, respectively. The positive integer ==== is called the order of the mixture model, ==== with ==== and ==== are called the mixing proportions, and ==== is called the cumulative distribution function of the mixing distribution. The parameter ====, appearing in all ==== component density functions, is called a structural parameter, and Model (1) is called a finite mixture of location-scale distributions with a structural parameter. Note that ==== includes unknown ==== and ==== parameters. Hence, ==== covers all the unknown parameters in (1). In this paper, we investigate the strong consistency of the maximum likelihood estimator (MLE) of ==== under Model (1).====Finite mixtures of location-scale distributions with a structural parameter have many applications. They play an important role in medical studies and genetics. For example, Roeder (1994) applied the finite normal mixture model with a structural parameter to analyze sodium–lithium countertransport activity in red blood cells. Finite mixtures of logistic distributions and of extreme value distributions with a structural parameter are widely used to analyze failure-time data. For instance, a finite mixture of logistic distributions with a structural parameter was used by Naya et al. (2006) to study the thermogravimetric analysis trace. A finite mixture of extreme value distributions with a structural parameter was found to provide an adequate fit to the logarithm of the number of cycles to failure for a group of 60 electrical appliances (Lawless, 2003; Example 4.4.2). More applications can be found in McLachlan and Peel (2000) and (Lawless, 2003).====The maximum likelihood method has been widely used to estimate the unknown parameters in finite mixture models (McLachlan and Peel, 2000; Chen, 2017). The consistency of the MLE under finite mixture models has been studied by Kiefer and Wolfowitz (1956), Redner (1981), and Chen (2017). As pointed out by Chen (2017), the results in Kiefer and Wolfowitz (1956) require that ==== can be continuously extended to a compact space of ====. This turns out to be impossible because ==== is not well defined at ====. To make the results in Kiefer and Wolfowitz (1956) applicable to our current setup, we must constrain the parameter ==== to be in a compact subset of ====. The consistency results in Redner (1981) require even more restrictive conditions: the parameter space for ==== must be a compact subset of ====; see Chen (2017) for more discussion. It is worth mentioning that Bryant (1991) established the strong consistency of the estimators obtained by the linear-optimization-based method. His result can be viewed as a generalization of the classical consistency result for MLE. However, it requires that the parameter space be closed and that ==== be equal to the true order of the mixture model. By utilizing the properties of the normal distribution, Chen (2017) proved the strong consistency of the MLE under finite normal mixture models with a structural parameter without imposing the compactness assumption on the parameter space. To the best of our knowledge, general consistency results for the MLE of ==== under Model (1) are not available in the literature except for the normal mixture model.====Because of the importance of finite mixtures of location-scale distributions with a structural parameter, it is necessary to study the consistency of the MLE of the underlying parameters, ====, under Model (1). The goal of this paper is to provide a general and rigorous proof of this consistency. In Section 2, we present the main consistency results. We emphasize that we do not require the parameter space of ==== to be compact. The detailed proofs are given in Section 3. Section 4 illustrates the consistency results by applying them to Model (1) with ==== being one of the commonly used component density functions: normal, logistic, extreme-value, or ====. An extension of the consistency results to finite mixtures of multivariate elliptical distributions is discussed in Section 5.",On consistency of the MLE under finite mixtures of location-scale distributions with a structural parameter,https://www.sciencedirect.com/science/article/pii/S037837581830051X,March 2019,2019,Research Article,369.0
"Sommer Andreas,Steland Ansgar","Institute of Statistics, RWTH Aachen University, Aachen, Germany","Received 12 January 2018, Revised 23 May 2018, Accepted 26 May 2018, Available online 2 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.05.006,Cited by (6),"The evaluation of a set of objects, e.g. items of a production lot, in terms of a random measurement, such that the decision is statistically designed to control the ==== of false decisions considered as a function of the fraction of measurements falling below a threshold, can be conducted by acceptance sampling procedures. These methods are typically studied for the quality control problem to accept or reject a lot of produced items. This paper provides an extension of the acceptance sampling methodology to a multi-stage framework where a lot is inspected at several ","Acceptance sampling procedures decide whether a lot should be accepted or rejected in terms of the fraction of low-quality items, by conducting a statistically designed hypothesis test based on a preferably small random sample. Both error probabilities, namely the acceptance of a “bad” lot (consumer’s risk) and the rejection of a “good” lot (producer’s risk), are controlled. Clearly, the scope of such procedures goes beyond its historical motivation, namely one-time quality control of incoming lots in mass production, and covers any situation where a population needs to be assessed by drawing a random sample in such a way that the risk of a false acceptance or rejection is considered and controlled as a function of the fraction of non-conforming items.====The present paper studies, for the class of ====-type test statistics, the acceptance sampling methodology for a multi-stage inspection setting under a panel-type sampling design. Our work is mainly motivated by quality control of photovoltaic system. Here newly fabricated photovoltaic modules installed in a solar power plant may undergo changes of their electrical properties, see Steland (2015) for some background. Further, ageing and degradation due to quality issues or damages may result in performance losses. This calls for a multi-stage acceptance procedure, where an initial test is followed by later inspections. But the costs of drawing independent random samples at each time point are too high, since the modules of such systems are spread over a large area, and thus motivate to develop procedures which allow for a panel like sampling design. Clearly, acceptance sampling procedures addressing these issues have a wide scope of applicability ranging from general industrial quality control to health monitoring and group sequential designs in drug assessment and clinical trials, see e.g. Jennison and Turnbull (2000).====Acceptance sampling by variables has been thoroughly studied for normally distributed data by Liebermann and Resnikoff (1955) and Bruhn-Suhr and Krumbholz (1991). These approaches are, however, not robust when the assumption of a normal distribution is violated, see Kössler and Lenz (1997). Another approach is presented in Kössler (1999), which covers continuous distributions with medium and long tails. Here the fraction of nonconforming items is estimated based on approximating the tail of the underlying distribution by a Pareto law, which has to rely on a small number of order statistics. Multiple dependent sampling plans by variables has been studied by Balamurali and Jun (2007) for lot-by-lot acceptance. This approach is somewhat related to the present work, when interpreting a lot-by-lot sampling from a production process as a multi-stage evaluation of the production lot where at each stage different items are sampled. In Balamurali and Jun (2007) a kind of uncertainty interval is defined and the lot is accepted, if the previous ==== lots were accepted, but rejected otherwise. The basic idea of such conditional sampling schemes dates back to WorthamBaker and Baker (1976), where for attributes it has been proposed to defer the decision after having seen the next ==== lots. Our approach, however, differs from this stream of research in that for each lot the decision statistic explicitly takes into account the values of the previous statistics, and thus the information how the underlying quality evolved, and the sample size taken from each lot is optimized and not held fixed as in Balamurali and Jun (2007).====In many applications, especially in photovoltaics, historical data are available and can be used for the construction of nonparametric sampling plans. For such settings, Steland and Zähle (2009) generalized the classical variable sampling plans for Gaussian data, where the test statistic is based on the sample means, to arbitrary distribution functions with finite fourth moment. For applications to quality control in photovoltaics we refer to Steland and Hermann (2010) and in Meisen et al. (2012), where the latter paper also discusses the effect of a shift in mean between the distributions of the historical data set and the incoming lot. A two-stage acceptance sampling procedure is considered in Steland (2015), where the author derives sampling plans for one time of reinspection under a nonparametric model. Moreover, an improved quantile estimator based on Bernstein–Durrmeyer polynomials and especially designed for the application in photovoltaics is presented in Pepelyshev et al. (2014).====Our approach, which extends the two-stage procedure of Steland (2015) to a general multi-stage framework, is as follows. We assume a nonparametric location model where the fraction of nonconforming items, ====, also called quality level, is given by the probability that the quality measurement of interest falls below a specification limit, ====. High quality is now defined by requiring ==== for some preassigned threshold AQL (acceptable quality level), whereas low quality is defined by ==== for some threshold ==== (rejectable quality level). The decision statistic used at each stage ==== (inspection time point) to evaluate the lot is based on a weighted average of all sample means ====, ====, up to time ====. The decision to accept or reject the lot at each stage can be either controlled by chosen stage-wise (conditional) consumer’s and producer’s error probabilities or, alternatively, by selecting global consumer and producer risks, i.e. the procedure’s type I and type II error rates, for which one then determines appropriate stage-wise error probabilities. The acceptance sampling procedure is conducted sequentially: Starting at time ==== one proceeds to the next stage, only if the inspection continues (i.e. the lot is accepted at this stage). Otherwise, the procedure stops and the lot is rejected. Given this general design, we determine explicit formulas for the asymptotically optimal sampling plan ==== for each stage ====, consisting of the stage-wise minimal sample sizes, ====, and associated control limits, ====, for the decision function. Those plans depend on quantile estimates calculated from the historical sample of size ====, which, however, converge to the true quantities if ==== converges to ====. Further, they rely on a normal approximation, which is asymptotically valid, if the error probabilities converge to ==== (or ====), such that the sample sizes ==== converge to ====. We call a procedure with those properties asymptotically optimal.====The sampling design studied here substantially goes beyond the common design of a sample of independent and identically distributed measurements. As in Steland (2015), our procedure allows a panel like sampling design by relying on observations from items (statistical units) which were already sampled at previous time points. When focusing on designs where the sample sizes ==== are non-increasing, the items drawn at time ==== form a ==== from which at later stages subsamples are drawn and measured to obtain the sample of quality measurements. This approach can substantially reduce the sampling costs.====In our approach the number of inspections, ====, is fixed and the inspection scheme is given by acceptance sampling plans, which can be calculated in advance before inspections start. We provide explicit formulas and propose an algorithm for calculating the plans, which substantially improves upon simple numerical optimization of the operating characteristic as discussed in Steland (2015).====In a simulation study we investigate the finite-sample properties of the procedure. We investigate numerically how to select the weights determining the influence of the test statistics calculated at previous inspection times and study the accuracy of the procedure in terms of real error probabilities. Here some deficiencies arise for which we propose some simple but effective modifications for practical analyses.====The rest of the paper is organized as follows. In Section 2 we describe in detail the general multi-stage acceptance sampling methodology and the proposed acceptance rejection procedure. The required asymptotic theory justifying the approach is provided in Section 3. In particular, we provide Gaussian approximations for the (conditional) operating characteristics, solve the constrained minimization problem to minimize sequentially the sample sizes, under the associated 2-point condition and establish the consistency as well as asymptotic optimality of the sampling plans. Further, we provide the asymptotic distribution of the stopping time associated to the multi-stage procedure and show that the plans are asymptotically normal. Section 4 reports about simulations which investigate the statistical properties of the new procedures. Proofs are provided in Section 5.",Multistage acceptance sampling under nonparametric dependent sampling designs,https://www.sciencedirect.com/science/article/pii/S0378375818300545,March 2019,2019,Research Article,370.0
"Sang Yongli,Dang Xin,Zhao Yichuan","Department of Mathematics, University of Louisiana at Lafayette, Lafayette, LA 70504, USA,Department of Mathematics, University of Mississippi, University, MS 38677, USA,Department of Mathematics and Statistics, Georgia State University, Atlanta, GA 30303, USA","Received 11 August 2017, Revised 24 May 2018, Accepted 27 May 2018, Available online 2 June 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.05.004,Cited by (17),"-values of the tests. Simulation studies show that our methods are competitive to existing methods in terms of coverage accuracy and shortness of confidence intervals, as well as in terms of power of the tests. The proposed methods are illustrated in an application on a real data set from UCI Machine Learning Repository.","The Gini correlation has been used in a wide range of fields since proposed in 1987 (Schechtman and Yitzhaki, 1987). In the field of economic data analysis, the Gini correlation enables us to test whether an asset increases or decreases the risk of the portfolio (Schechtman and Yitzhaki, 1999), and can be used to build the relationship between the family income and components of income (Schechtman and Yitzhaki, 1987); in plant systems biology, the Gini correlation can compensate for the shortcomings of popular correlations in inferring regulatory relationships in transcriptome analysis (Ma and Wang, 2012); it has also been widely used in all branches of modern signal processing (Xu et al., 2010).====Let ==== and ==== be two non-degenerate random variables with continuous marginal distribution functions ==== and ====, respectively, and a joint distribution function ====. Then two Gini correlations are defined as ====to reflect different roles of ==== and ==== The representation of Gini correlation ==== indicates that it has mixed properties of those of the Pearson and Spearman correlations, and thus complements these two correlations Schechtman and Yitzhaki (1987), Schechtman and Yitzhaki (1999), Schechtman and Yitzhaki (2003). The two Gini correlations in (1) are not symmetric in ==== and ==== in general. The equality of the two Gini correlations can be involved in many procedures in Economics. For example, it can be applied to determine the similarity in two popular methodologies for constructing portfolios, the MV and MG (Schechtman et al., 2007), and the equality of the two Gini correlation between the return on each asset and the return on the portfolio is the necessary condition of the statement that all the Security Characteristic curves are linear (Schechtman et al., 2007), that is, a rejection of the hypothesis on the equality of Gini correlations is a rejection of the assumption that all the Security Characteristics curves are linear. Therefore, to understand the Gini correlation and to test the equality of the two Gini correlations are essential. In the paper, we develop a procedure to estimate the Gini correlation and to test the equality of the two Gini correlations. To the best of our knowledge, there is no nonparametric approaches to infer the Gini correlations.====As a nonparametric method, the empirical likelihood (EL) method introduced by Owen (1988), Owen (1990) has been used heuristically for constructing confidence intervals. It combines the effectiveness of likelihood and the reliability of nonparametric approach. On the computational side, it involves a maximization of the nonparametric likelihood supported on data subject to some constraints. If these constraints are linear, the computation of the EL method is particularly easy. However, EL loses this efficiency when some nonlinear constraints are involved. To overcome this computational difficulty, Wood et al. (1996) proposed a sequential linearization method by linearizing the nonlinear constraints. However, they did not provide the Wilks’ theorem and stated that it was not easy to establish. Jing et al. (2009) proposed the jackknife empirical likelihood (JEL) approach. The JEL method transforms the maximization problem of the EL with nonlinear constraints to the simple case of EL on the mean of jackknife pseudo-values, which is very effective in handling one and two-sample ====-statistics. Wilks’ theorems for one and two-sample ====-statistics are established. This approach has attracted statisticians’ strong interest in a wide range of fields due to its efficiency, and many papers are devoted to the investigation of the method, for example, Liu et al. (2015), Peng (2012), Feng and Peng (2012), Wang and Zhao (2016), Wang et al. (2013), Li et al. (2016) and Li et al. (2011) and so on. However, theorems derived in Jing et al. (2009) are limited to a simple case of the ====-statistic but the Gini correlation cannot be estimated by a ====-statistic, which does not allow us to apply the results of Jing et al. (2009) directly. However, it can be estimated by a functional of multiple ====-statistics (Schechtman and Yitzhaki, 1987). Due to this specific form of the Gini correlation, we propose a novel ====-statistic type functional and a JEL-based procedure with the ====-structured estimating function is applied for the Gini correlation. And this approach may work for making an inference about some difference functions of multiple ====-statistic structure with nuisance parameters involved.====In the test ====where ====, the natural empirical estimator ==== of ==== is a function of 4 dependent ====-statistics. Based on ====-statistics theorem, ====, will, after appropriate normalization, have a limiting normal distribution. However, the asymptotic variance is complicated to calculate. In the present paper, by proposing a new ====-statistic type functional system, we avoid estimating the asymptotic variance to do the test. However, only a part of parameters are being interested. When only a part of parameters are of interest, Qin and Lawless (1994) proposed to use a profile empirical likelihood method which is also an important tool to transform nonlinear constraints to some linear constraints by introducing link nuisance parameters. However, the profile EL could be computationally costly. Hjort et al. (2009) proposed to reduce the computational complexity by allowing for plug-in estimates of nuisance parameters in estimating equations with the cost that the standard Wilks’ theorem may not hold. Li et al. (2011) proposed a jackknife plug-in estimate in terms of a function of interested parameters so that their EL still have standard chi-square distributions. However, we cannot take advantage of their method since the parameters of interest in this paper are estimated by solving estimating functions with ====-statistics structure. We cannot apply theoretical results of the profile JEL method in Li et al. (2016), either. Li et al. (2016) developed a JEL-based inferential procedure for general ====-structured estimating functions. It requires the condition that kernel functions are bounded both in the sample space and in the parameter space. Under merely second order moment assumptions, we establish the Wilks’ theorem for the jackknife empirical log-likelihood ratio for ====. The computation is also easy since a simple plug-in estimate of the nuisance parameter is used.====It is often of considerable interest to compare the Gini correlations from two independent populations. For instance, Lohweg et al. (2013) constructed adaptive wavelets for the analysis of different print patterns on a banknote and made it possible to use mobile devices for banknote authentication. After the wavelet transformations, there are four continuous variables: variance, skewness, kurtosis and entropy of wavelet transformed images. It is natural to ask what are correlations of each pair of the above variables. Are there any differences between the Genuine banknotes and Forgery banknotes? One of the main goals of this paper is to develop the JEL method for comparing the Gini correlations for independent data sets.====The remainder of the paper is organized as follows. In Section 2, we develop the JEL method for the Gini correlations. The JEL method for testing the equality of Gini correlations is proposed in Section 3. In Section 4, we consider the JEL method for comparing Gini correlations for two samples. Following the introduction of methods in each section, simulation studies are conducted to compare our JEL methods with some existing methods. A real data analysis is illustrated in Section 5. Section 6 concludes the paper with a brief summary. All proofs are reserved to the Appendix.",Jackknife empirical likelihood methods for Gini correlations and their equality testing,https://www.sciencedirect.com/science/article/pii/S0378375818300521,March 2019,2019,Research Article,371.0
"Dauxois J.-Y.,Gasmi S.,Gaudoin O.","Institut de Mathématiques de Toulouse; UMR5219, Univ. Toulouse; CNRS, INSA, F-31077 Toulouse, France,Univ. of Tunis, ENSIT, OMAD, 1008 Bab Menara, Tunisia,Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France","Received 7 November 2017, Revised 16 May 2018, Accepted 19 May 2018, Available online 28 May 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.05.002,Cited by (4),"The aim of this paper is twofold. First a new imperfect maintenance model is introduced. This model is an extension of Finkelstein’s Geometric Failure Rate Reduction model, using the modification proposed by Bordes and Mercier for extending the Geometric Process. Second, based on the observation of several systems, the semiparametric inference in this model is studied. Estimators of the euclidean and functional model parameters are derived and their ==== is proved. A simulation study is carried out to assess the behavior of these estimators for samples of small or moderate size. Finally, an application on a real dataset is presented.","The basic assumptions on maintenance effectiveness for the failure process of repairable systems are known as minimal maintenance (or As Bad As Old effect) and perfect maintenance (or As Good As New effect). In the minimal maintenance case, each repair leaves the system in the same state as it was before failure and the obtained process is a nonhomogeneous Poisson process (NHPP). In the perfect maintenance case, each repair is perfect and leaves the system as if it were new, the obtained process is then a renewal process (RP). In practice, it is well known that the reality is between these two extreme cases. A standard maintenance action is better than minimal but not necessarily perfect. This leads to the notion of imperfect maintenance for repairable systems.====Many imperfect maintenance models have been proposed. Some of the most famous models are the model of Brown and Proschan (1983) (BP), Kijima’s virtual age models (Kijima, 1989) and the Geometric Process (GP) model (Lam, 1988). An extension of this last model has been recently proposed by Bordes and Mercier (2013). Kijima’s models of types I and II have been modified by Baxter et al. (1996) for more realistic interpretations. Classes of imperfect repair models based on an arithmetic reduction of failure intensity (ARI) or an arithmetic reduction of virtual age (ARA) have been proposed by Doyen and Gaudoin (2004). The ARA and ARI models are very flexible and have been widely used. The idea of using a geometric reduction instead of an arithmetic reduction has been proposed in Doyen and Gaudoin (2004) but has been deepened only very recently in Doyen et al. (2017). The first geometric reduction of intensity model has been introduced by Finkelstein (2008) under the name of Geometric Failure Rate Reduction (GFRR) model. Since the GFRR model suffers from the same drawbacks as the GP model, the first contribution of this paper is to extend the GFRR model, using the idea proposed by Bordes and Mercier (2013) for extending the GP model.====For agingsystems undergoing imperfect corrective maintenances (CM) after failures, it is important to be able to improve system reliability by performing preventive maintenances (PM, see for instance Liao et al. (2010)). A usual sequential maintenance policy consists in replacing the system by a new one after ==== maintenances. This policy can be found for instance in Nakagawa (1988) and Zhang et al. (2013). In these papers, all the model parameters are considered to be known and the objective is to determine an optimal value of ====. In the present paper, we assume that a more general sequential maintenance policy is used and our aim is to estimate the model parameters.====It has to be noted that the question of statistical inference is not frequently treated in the literature on imperfect maintenance. Basically, the parameters are estimated using a fully parametric maximum likelihood approach. Some exceptions are Lam (1992), Peña and Hollander (2004) and Beutner et al. (2017). The first one studied non-parametric inference in the GP model. The second one introduced a general model which takes into account covariates, factors of heterogeneity and virtual age. The semiparametric inference for this model has been studied by Peña et al. (2007) when the virtual ages are supposed to be known. The last one studied the semiparametric inference for ARA models. In this paper, we study the semiparametric inference for the extended GFRR model.====The remainder of this paper is organized as follows. Section 2 briefly reviews some existing imperfect maintenance models involving a reduction of age or intensity. Section 3 introduces the new extended GFRR model with the proposed sequential maintenance policy. Section 4 develops the estimation procedure of the euclidean and functional parameters of this model. The study of their asymptotic behaviors is done in Section 5. Experimental results based on simulated data are presented in Section 6. In Section 7, the model is applied to the Norsk Hydro ammonia plant dataset introduced by Bunea et al. (2003). Finally, Section 8 draws some conclusions and prospects.",Semiparametric inference for an extended geometric failure rate reduction model,https://www.sciencedirect.com/science/article/pii/S0378375818300508,March 2019,2019,Research Article,372.0
"Wang Congcong,Zhao Qianqian,Zhao Shengli","School of Statistics, Qufu Normal University, Qufu 273165, China","Received 10 October 2017, Revised 6 February 2018, Accepted 7 May 2018, Available online 28 May 2018, Version of Record 26 September 2018.",https://doi.org/10.1016/j.jspi.2018.05.001,Cited by (11),"In practice, fractional ","Fractional factorial (FF) designs are widely used in industrial and agricultural experiments because they can reduce the number of experimental times and lower the costs. When an FF experiment is performed, it is required that the experimental runs are completely randomized. However, it is sometimes impractical to perform the experimental runs in a completely random order since it is very difficult or expensive to change or control the levels of some factors. Then, a fractional factorial split-plot (FFSP) design, which involves a two-phase randomization, may represent a practical design option to meet the special demands.====Suppose we wish to run an experiment with ==== factors, each at two levels. However, among them, there are ==== factors, whose levels are very difficult or expensive to be changed. The levels of the remaining ==== factors are easy to be changed. The hard-to-change factors are called ==== (WP) ==== and the relatively easy-to-change factors are called ==== (SP) ====. Obviously, the factors no longer have the same status.====Minimum aberration (MA) was proposed to choose optimal two-level designs by Fries and Hunter (1980). Bingham and Sitter (1999a) gave a construction method of the MA FFSP design and tabulated a catalog of MA two-level FFSP designs with 8 and 16 runs. Bingham and Sitter (1999b) discussed the impact of randomization restrictions on the choice of FFSP designs and developed theoretical results on MA FFSP designs. Yang et al. (2009) extended the results of Bingham and Sitter (1999b) to multi-level designs. Bingham and Sitter (2001) listed MA FFSP designs with up to 32 runs. Since FFSP designs do not have the interchangeability between WP factors and SP factors, there frequently exist several non-isomorphic FFSP designs which have MA. Mukerjee and Fang (2002) explored a criterion of minimum secondary aberration (MSA), called MSA-FFSP criterion, which significantly narrows the class of competing non-isomorphic MA FFSP designs and hence often yields the unique optimal one. Ai and Zhang (2006) constructed MSA-FFSP designs in terms of consulting designs. Cheng and Tsai (2009) proposed a general and unified approach to the selection of regular FF designs, which can be applied to experiments that are unblocked, blocked or have a split-plot structure.====Montgomery (2012, p. 629) mentioned an experiment in which the factors affecting uniformity in a single-wafer plasma etching process were investigated. The experimenter’s objective was to minimize the uniformity response. Three factors on the etching tool were relatively difficult to change from run to run: ====
               ==== electrode gap, ====
               ==== gas flow, and ====
               ==== pressure. Two other factors were easy to change from run to run: ====
               ==== time and ====
               ==== radio frequency power. The experimenters used an FFSP design with factors ====, ====, and ==== in the whole plots and factors ==== and ==== in the subplots. Half-normal plots of the effects (Montgomery, 2012, p. 632) showed that the main effects of the factors ====, ==== and ==== and the ==== and ==== interactions were large. The two-factor interaction graphs indicated that the treatment combination ==== high, ==== low, and ==== low or ==== low, ==== high, and ==== high would produce low levels of the uniformity response. Note that in an FFSP experiment, the whole plots were run in random order, but once a whole plot was set up, the subplot runs were made in random order. Therefore, the WP factors were more important than the SP factors in this experiment since the selection of the levels of ==== affected that of ====.====The above example shows real context that the WP factors are more important than the SP factors and certain two-factor interactions are important. When some background knowledge suggests that certain two-factor interactions are potentially important, the clear effect criterion (Wu and Chen, 1992) can be used to select the optimal designs. Yang et al. (2006), Zi et al. (2006), Zhao and Chen (2012a), Zhao and Chen (2012b), Wang et al. (2015) and Yuan and Zhao (2016) investigated the FFSP designs under the clear effect criterion.====This paper considers the background that the WP factors are more important than the SP factors. In FFSP designs, two-phase randomization results in two sources of errors in analysis of variance, the WP error term and SP error term. Usually, the former is larger than the latter. This implies that the power to detect significant effects in data analysis is not the same for the two kinds of factors. Consequently, if the experimenter has some prior information that the WP factors are possibly more significant than the SP factors, we need a new criterion to select FFSP designs.====Tichon et al. (2012) considered five scenarios for more flexible split-plot design choices. They proposed an extended word length pattern to deal with both regular and nonregular designs and constructed optimal orthogonal split-plot designs with 12, 16, 20, and 24 runs through searches over the complete catalogs of orthogonal arrays. The setting “WP factors are more important than SP factors” in this paper is one of the five scenarios they considered. This paper proposes a new WP-MA criterion for regular designs and gives the theoretical construction methods which can be used to construct all designs (including the designs of large run size) with small number of defining words.====The rest of the paper is organized as follows. Section 2 proposes the definition of the WP-MA criterion and introduces some useful results. Section 3 is devoted to constructing WP-MA FFSP designs. Examples are included to illustrate the construction methods. Section 4 gives a conclusion. All proofs are given in Appendix A. The WP-MA FFSP designs with 8, 16, and 32 runs are tabulated in Appendix B.",Optimal fractional factorial split-plot designs when the whole plot factors are important,https://www.sciencedirect.com/science/article/pii/S0378375818300491,March 2019,2019,Research Article,373.0
"Barczy Mátyás,Ben Alaya Mohamed,Kebaier Ahmed,Pap Gyula","MTA-SZTE Analysis and Stochastics Research Group, Bolyai Institute, University of Szeged, Aradi vértanúk tere 1, H-6720 Szeged, Hungary,Laboratoire De Mathématiques Raphaël Salem, UMR 6085, Université De Rouen, Avenue de L’Université Technopôle du Madrillet, 76801 Saint-Etienne-Du-Rouvray, France,Université Paris 13, Sorbonne Paris Cité, LAGA, CNRS (UMR 7539), Villetaneuse, France,Bolyai Institute, University of Szeged, Aradi vértanúk tere 1, H-6720 Szeged, Hungary","Received 2 January 2017, Revised 7 January 2018, Accepted 7 February 2018, Available online 26 May 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.02.002,Cited by (6),We study ====.  We prove strong consistency and ,"Parameter estimation, especially studying asymptotic properties of maximum likelihood estimator (MLE) of drift parameters for Cox–Ingersoll–Ross (CIR) and Heston models is an active area of research mainly due to the wide range of applications of these models in financial mathematics.====The present paper gives a new contribution to the theory of asymptotic properties of MLE for jump-type Heston models based on continuous time observations. Concerning related works, due to the vast literature on parameter estimation for Heston models, we will restrict ourselves to mention only papers that investigate the very same types of questions. For a detailed and recent survey on parameter estimation for Heston models in general, see the Introduction of Barczy and Pap (2016).====Overbeck (1998) studied MLE of the drift parameters of the CIR process based on continuous time observations, which is also called square root process or Feller process. Ben Alaya and Kebaier (2012), Ben Alaya and Kebaier (2013) made a progress in MLE for the CIR process, giving explicit forms of joint Laplace transforms of the building blocks of this MLE as well.====The original Heston model (see Heston, 1993) takes the form ====where ==== is the price process of an asset, ==== is the rate of return of the asset, ==== is the so-called long variance (long run average price variance, i.e., the limit of ==== as ====), ==== is the rate at which ==== reverts to ====, ==== is the so-called volatility of the volatility and ==== is the correlation between the driving standard Wiener processes ==== and ====. Barczy and Pap (2016) investigated the Heston model with another parametrization.====In this paper we study a jump-type Heston model (also called a stochastic volatility with jumps model, SVJ model) ====where ==== is a purely non-Gaussian Lévy process independent of ==== with Lévy–Khintchine representation ====where ==== and ==== is a Lévy measure concentrated on ==== with ====. Here, let us recall that the Lévy process ==== has finite variation on each interval ====, ====, if and only if ====, see, e.g., Sato (1999, Theorem 21.9). We point out that the assumption ==== and the assumption in question on the support of the Lévy measure ==== assure that ==== for all ==== (see Proposition 2.1), so the process ==== can be used for modeling prices in a financial market. For a good survey on jump-type Heston models, pricing and hedging in these models, see Runggaldier (2003). In fact, the model (1.2) is quite popular in finance with the special choice of the Lévy process ==== as a compound Poisson process. Namely, let ====where ==== is a Poisson process with intensity ====, ==== is a sequence of independent identically distributed random variables having no atom at zero (i.e., ====), and being independent of ==== as well. We also suppose that ====, ====, ==== and ==== are independent. One can interpret ==== as the jump size of the logarithm of the asset price. Then ====has the form (1.3) with ==== being the distribution of ==== and ====. Moreover, ==== takes the form ====for ====, see (2.1). We note that the SDE (1.2) with the Lévy process ==== given in (1.4) has been studied, e.g., by Bates (1996, equation (1)), Bakshi et al. (1997, equations (1) and (2) with ====), by Broadie and Kaya (2006, equations (30)–(31)) (where a factor ==== is missing from the last term of equation (30)), by Runggaldier (2003, Remark 3.1, with ====) and by Sun et al. (2017, equation 1 with ====). Bates (1996), Bakshi et al. (1997) and Broadie and Kaya (2006) have chosen the common distribution of ==== as a normal distribution. Bakshi et al. (1997) used this model for studying (European style) S&P 500 options, e.g., they derived a practically implementable closed-form pricing formula. Broadie and Kaya (2006) gave an exact simulation algorithm for this model, further, they considered the pricing of forward start options in this model. Sun et al. (2017) have chosen the common distribution of ==== as a normal distribution, a one-sided exponential distribution or a two-sided distribution, and they applied the Fourier–cosine series expansion method for pricing vanilla options under these jump-type Heston models.====The aim of this paper is to study the MLE of the parameter ==== for the model (1.2) based on continuous time observations ==== with ====, starting the process ==== from some deterministic initial value ==== supposing that ====, ====, ==== and the Lévy measure ==== are known. Here we stress that under these assumptions, the underlying statistical space corresponding to the parameters ==== is identifiable, however it would not be true for the statistical space corresponding to the parameters ====. We call the attention that the MLE in question contains stochastic integrals with respect to ====. We prove that, for all ====, ==== is a measurable function (i.e., a statistic) of ====, by providing a sequence of measurable functions of ==== converging in probability to ====, see Remark 2.4 (note that this sequence depends on ==== and ==== as well). Further, it turns out that ==== for all ====, and the parameters ==== and ==== are also measurable functions of ====, see Remark 2.5. Hence, for the calculation of the MLE in question, one needs only the sample ====, the parameter ==== and the Lévy measure ==== (==== and ==== are needed for the reconstruction of ====). Though we do not need to estimate the parameters ==== and ====, it is worth mentioning that the market microstructure effects may cause serious damage to the approximation of ==== and ==== given in Remark 2.5 and to the MLE of ==== in case of high-frequency observations as in Zhang et al. (2005). This type of question can be another interesting topic for future research.====The paper is organized as follows. In Section 2, we prove that the SDE (1.2) has a pathwise unique strong solution (under some appropriate conditions), see Proposition 2.1, we recall a result about the existence of a unique stationary distribution and ergodicity for the process ==== given by the first equation in (1.2), see Theorem 2.2. In Proposition 2.3, we derive a Grigelionis representation for the process ====. Further, one can prove that for all ====, ==== and ==== are measurable functions of ====, and one can justify why we do not estimate the parameters ==== and ====, see Remarks 2.4, 2.5 and our arXiv version ( Barczy et al., 2016, Remarks 2.5 and 2.6). Section 3 is devoted to study the existence and uniqueness of the MLE ==== of ==== based on observations ==== with ====. In Proposition 3.2, under appropriate conditions, we prove the unique existence of ====, and we derive an explicit formula for it as well, see (3.8). In Remark 3.5, we describe the connection with the so called score vector due to Sørensen (1991) and the estimating equation due to Luschgy (1992), Luschgy (1994) leading to the same estimator. In Section 4, we prove that the MLE of ==== is strongly consistent if ==== with ====, and weakly consistent if ==== with ====, see Theorem 4.1 and Remark 4.2, respectively. Section 5 is devoted to investigate the asymptotic behavior of the MLE of ====. In Theorem 5.1, provided that ==== with ====, we show that the MLE of ==== is asymptotically normal with a usual square root normalization ====, but as usual, the asymptotic covariance matrix depends on the unknown parameters ==== and ====, as well. To get around this problem, we also replace the normalization ==== by a random one (depending only on the sample, but not on the parameters ====, ==== and ====) with the advantage that the MLE of ==== with the random scaling is asymptotically ====-dimensional standard normal. Theorem 5.2 is a counterpart of Theorem 5.1 in some sense. Namely, provided that ==== with ====, we derive two limit theorems for the MLE ==== with mixed normal limit distributions. First, we have a non-random scaling, but for ==== instead of the usual scaling ==== we have ====; and then we have a random scaling as well. We point out that, surprisingly, the limit distributions in Theorem 5.1, Theorem 5.2 do not depend on ==== (roughly speaking, they do not depend on the jump part). From a practical point of view, a natural question can occur, namely, how one can decide whether Theorem 5.1, Theorem 5.2 can be applied (if yes, then which one), since one does not know the product ==== of the unknown parameters ==== and ==== in advance. To answer this question, one can build up a probe for testing the null hypothesis ==== against some alternative hypothesis, e.g., ====. In Section 6 we present some numerical illustrations of our limit theorems. We close the paper with Appendices, where we prove Proposition 2.1, Proposition 2.3 (Appendix A Proof of, Appendix B Proof of), we recall a limit theorem for continuous local martingales for studying asymptotic behavior of ==== (Appendix C), and a version of the continuous mapping theorem (Appendix D), and we give an explicit formula for the non-normal but mixed normal density function of the limit distribution of ==== as ==== in Theorem 5.2 (Appendix E).====We call the attention that in both cases ==== and ====, the CIR process ==== has a unique stationary distribution and is ergodic, nevertheless, in case ==== the asymptotic limit distribution of the MLE of ==== is normal, while in case ==== it is mixed normal. The interesting point is that we have an ergodic case with an asymptotically mixed normal (but non-normal) limit distribution. The main difference between the two ergodic cases is that ==== if ====, but ==== if ====.",Asymptotic behavior of maximum likelihood estimators for a jump-type Heston model,https://www.sciencedirect.com/science/article/pii/S037837581830048X,January 2019,2019,Research Article,374.0
Zhou Quan,"Department of Statistics, Rice University, Houston, TX 77005, USA","Received 11 October 2017, Revised 16 April 2018, Accepted 26 April 2018, Available online 7 May 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.04.003,Cited by (1),We consider the ==== of a cell in a ==== ==== as the fixed marginal totals tend to infinity. The asymptotic order of the cell variance is derived and a useful diagnostic is given for determining whether the cell has a Poisson limit or a Gaussian limit. There are three forms of Poisson convergence. The exact form is shown to be determined by the growth rates of the two smallest marginal totals. The results are generalized to contingency tables with arbitrary sizes and are further complemented with concrete examples.,"This work considers the asymptotic distribution of a cell in a ==== contingency table as the fixed marginal totals tend to infinity. The literature on this problem has been documented under various names: “the coupon collector problem”, “capture–recapture”, “the committee problem”, “matrix occupancy”, “random allocation”, and “allocation by complexes” ( Barbour et al., 1992, Sec. 6.4). The reader is encouraged to consult Holst (1986) and Stadje (1990) for historical accounting of these problems.====The present work borrows the framework and terminology of the coupon-collector problem. Consider ==== distinct coupons and ==== coupon collectors operating independently and let the ====th collector collect ==== distinct coupons. Let ==== denote the set of the collectors. For each set ====, we are interested in the number of coupons that are collected by ==== and by no others. These counts may be summarized in an ====-way ==== contingency table. Let ==== denote the count in the cell ====, where ==== and ==== indicates that a coupon is collected by collector ====. This contingency table must satisfy ==== and ====, for ====, where the marginal total ==== is treated as fixed. For the case of two collectors, the 2 ==== 2 contingency table is shown in Table 1.====We consider the distribution of an arbitrary cell under the following asymptotic conditions:====
               ====Under (A1)–(A4), each cell can be treated equivalently up to relabelling of rows and columns. Therefore, without loss of generality, it suffices to consider one cell. Henceforth our analysis shall concern the cell ====, where ====, i.e. the number of the coupons that are collected by all collectors.====To the best of our knowledge, the first complete analysis of all the possible asymptotic limits of ==== is due to Vatutin and Mikhailov (1983). The authors showed that ==== has either a normal or a Poisson limit depending on whether ==== converges (see Theorem 1). This was accomplished by verifying that its generating function has only real roots (see also Kou and Ying, 1996). Alternative proofs for this problem and its variants are given in Kolchin et al. (1978, Chap. VII), Holst (1980), Mitwalli (2002), Harris (1989), and Cekanavicius et al. (2000). See Smythe (2011) for an extension to the case in which ==== are random. See Lareida et al. (2017) for a more recent application of these results.====
               ====
            ====In Section 2, we calculate the asymptotic order of ====. This provides a useful diagnostic for determining whether the limiting distribution of ==== given by Theorem 1 is normal or Poisson. In Section 3, we show that the exact form of Poisson convergence is determined only by ==== and ====. Section 4 generalizes the results of Sections 2 Asymptotics of the cell variance, 3 Poisson convergence to contingency tables of arbitrary size.",Asymptotics of multivariate contingency tables with fixed marginals,https://www.sciencedirect.com/science/article/pii/S0378375818300363,January 2019,2019,Research Article,375.0
"Sohrabi Maryam,Zarepour Mahmoud","Department of Mathematics and Statistics, University of Ottawa, Canada","Received 30 May 2017, Revised 9 April 2018, Accepted 10 April 2018, Available online 2 May 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.04.001,Cited by (1),) processes. The innovations are assumed to be in the domain of attraction of a symmetric stable law with index ,"Consider the autoregressive process of order ==== (AR(====)) ====where ==== is the backward operator and ====The errors ==== in (1.1) form a sequence of independent and identically distributed (i.i.d.) random variables in the domain of attraction of a symmetric stable law with index ====. The model (1.1) is referred to as non-stationary autoregressive time series if the characteristic polynomial ==== has at least one root on the boundary of the unit circle.====It is well known that the unit root tests are particularly an important tool to classify if a time series is stationary or non-stationary. When economic variables are non-stationary, estimates may generate a spurious model unless they are cointegrated. A unit root test can be used for cointegration of two processes. The analysis of unit-root processes and cointegrated time series is likely to be the one of the most important and controversial topics in econometrics in the last few decades. The case where innovations are in the domain of attraction of the Gaussian distribution has received considerable attention in cointegration literature; see for example Engle and Granger (1987) and Park and Phillips (1988). However, many empirical studies show that heavy-tailed and asymmetrically distributed samples are frequently observed in economic and, especially, financial time series. In these cases, the Gaussian models are not applicable. Paulauskas and Rachev (1998) develop the asymptotic theory for econometric cointegration processes under the assumption of infinite variance innovations with different tail indices.====The problem of conducting asymptotic inference for time series with unit roots has been a challenging topic of interest for some time. In cases where errors (innovations) have finite variance, Dickey and Fuller (1979), and Phillips and Perron (1988) provide the asymptotic theory for the least squares (LS) estimates in an AR(1) process with one unit root. Chan and Wei (1988) study the large sample theory for a non-stationary autoregressive AR(====) model when the innovations form a sequence of martingale differences with respect to an increasing sequence of ====-fields ====.====With infinite variance innovations, Chan and Tran (1989) consider the Dickey–Fuller test when the errors are in the domain of attraction of a stable law. Phillips (1990) extends the results of Chan and Tran (1989) to find the limit theory of the parameters in an AR(1) process with weakly dependent errors in the domain of attraction of a stable law. Since both the Dickey–Fuller and Phillips–Perron statistics are based on LS estimation, they do not take advantage of the heavy tails of the innovations and can exhibit rather poor power performance. Thus, it is important to consider estimation and inference procedures that are robust to departures from finite variance condition. One way to achieve robustness is the use of the ====-estimate method. With an appropriate choice of a loss function, ====-estimates have a number of desirable properties when the errors are heavy tailed. Knight (1989) considers the asymptotic behavior of the LS estimates and ====-estimates for the random walk model. The results establish that self-normalized ====-estimates are asymptotically normal and their rate of convergence is higher than the LS estimates. Davis et al. (1992) mention that ====-estimates are more appropriate when the distribution of innovations are heavy-tailed. This follows from the fact that ====-estimates give less weight to the outliers. Samarakoon and Knight (2009) develop a class of unit root tests based on ====-estimates in an AR process with a unit root derived by infinite variance innovations. Also see Konev and Le Breton (2000) for a sequential procedure for a weighted least-squares estimator for a general AR==== process with infinite variance errors.====Chan and Zhang (2012) obtain the limiting distribution for the LS estimates of the parameters for unstable AR(====) processes, with i.i.d. innovations in the domain of attraction of a stable law. They show that the limiting distribution of the LS estimate is a function of integrated stable processes. However, for model (1.1) with unit roots when ==== is a sequence of random variables with infinite variance, a complete theory on a more efficient estimating technique is still missing in the literature.====In this paper, we consider an important class of unstable autoregressive time series models with many practical implications. An example of these time series are seasonal models, where ==== may have several real and complex conjugate roots on the unit circle. We derive the asymptotic distribution of ====-estimates for the parameters in an unstable AR(====) process, where the innovations are in the domain of attraction of a stable law with index ====. Our results show that, similar to the previous cases, ====-estimates have higher asymptotic rate of convergence than LS estimates. This paper is organized as follows. Section 2 provides some necessary preliminary concepts. In Section 3, the limiting distribution of ====-estimates in an AR(====) model is presented and Section 4 consists of our simulation study. Due to the complexity of the limiting distributions a brief discussion and a bootstrap simulation scheme are presented in Section 5. We summarize our results in Section 6 and finally, the proof of the main theorem is outlined in Appendix.",Asymptotic theory for M-estimates in unstable AR(,https://www.sciencedirect.com/science/article/pii/S037837581830034X,January 2019,2019,Research Article,376.0
"Azaïs Romain,Genadot Alexandre","Inria Nancy –Grand Est, Team BIGS and Institut Élie Cartan de Lorraine, Nancy, France,Université de Bordeaux, IMB, Institut Mathématiques de Bordeaux, INRIA Bordeaux Sud-Ouest, Team: CQFD, 351 cours de la Libération, 33405 Talence Cedex, France","Received 31 March 2017, Revised 12 April 2018, Accepted 13 April 2018, Available online 27 April 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.04.002,Cited by (1),Assume that you observe trajectories of a non-diffusive non-stationary process and that you are interested in the average number of times where the process crosses some threshold (in dimension ,"We consider a random but non-diffusive trajectory ==== that models some physical or biological phenomenon. In order to set the ideas down, ==== may be issued from a piecewise deterministic Markov process (PDMP) (Davis, 1993) but our theoretical results will be established for a more general class of stochastic models. In this paper, we are not interested in the estimation of the parameters that govern the underlying model that has generated the trajectory ==== but in a functional of this trajectory. Indeed, in numerous situations, the main quantity of interest is related to the average number of crossings of some level (in dimension ====) or hypersurface (in dimension ====) within a given time window. For instance, if ==== models the cumulative exposure to some food contaminant (Bertail et al., 2008), the toxic effects depend on the time spent beyond a critical threshold. In reliability, ==== may describe the size of a crack in a certain material (Abdessalem et al., 2016). Exceeding a dangerosity threshold may lead to rupture and thus to a dramatic event. Unfortunately, the trajectory ==== is often observed on a discrete temporal grid which time step size is imposed by the measuring devices. In this context, some crossings may be missed in such a way that the crude Monte Carlo-type estimator that only counts the number of observed crossings generally underestimates the theoretical quantity of interest. However, modern datasets often contain more information than the location at the measure time: they may provide the instantaneous velocity. This is typically the case when one studies spatial trajectories captured by a GPS device like the terrestrial and marine movements of lesser black-backed gulls investigated in Garthe et al. (2016a). This additional information is crucial because it may give a clue on the likelihood of an unobserved crossing between two successive measure times. The main objective of this article is to show that one can take into account the velocity measurement to better estimate the average number of crossings.====In the present article, we deal with piecewise smooth processes (PSP’s) as defined in Borovkov and Last (2012). They form a very general class of non-diffusion stochastic processes composed of deterministic trajectories following some differential equation punctuated by random jumps at random times. PDMP’s are a particular case of PSP’s because of the particular link between the inter-jumping times and the deterministic path that ensures the Markov property to hold. It should be noted that we do not impose any kind of Markov assumption in this paper. Kac–Rice formulae give a concise relation between the average number ==== of crossings of ==== with a hypersurface ==== within the time window ==== and some features of the underlying stochastic model. They have already been stated for non-stationary one-dimensional PSP’s (Dalmao and Mordecki, 2015) and for stationary multidimensional PSP’s (Borovkov and Last, 2012). These papers Borovkov and Last (2012), Dalmao and Mordecki (2015) and our approach are different and complementary. In the applications (typically spatial trajectories captured by GPS), the stochastic process of interest ==== is often both multidimensional and non-stationary. In Theorem 2.10, we establish under mild conditions the following Kac–Rice formula for multidimensional non-stationary PSP’s, ====where ==== is the velocity of the deterministic motion, ==== is a field of unit normals of ====, ==== denotes the density of ==== and ==== stands for the Hausdorff measure (see Remark 2.2). The strategy developed to state (1) also gives a fresh look at Kac–Rice formula for one-dimensional processes. Consequently, we also provide a new proof of this formula for one-dimensional PSP’s in Corollary 2.7. In addition, we investigate in Corollary 2.8, Corollary 2.12 the Euclidean-mode setting often used in applications of hybrid Markov models (for instance in Abdessalem et al., 2016), which has never been studied from that perspective in the literature to the best of our knowledge.====The distribution ==== appearing in the Kac–Rice formula (1) is generally unknown but can be estimated (for example by kernel methods) from a dataset of trajectories observed within the time window ====. In a wide range of applications, the deterministic motion is assumed to be known because it has been postulated by scientific laws, in particular in physical or biological models. This allows us to propose a new strategy for estimating the average number of crossings ====. If ==== denotes an estimate of ====, the number of crossings ==== can be approximated by the plug-in estimator ====In the simulation study presented in Section 4, we show that ==== better estimates ==== than the Monte Carlo estimator that only returns the empirical mean of observed crossings within the interval ====, in particular when the time step size is large. In the real data application given in Section 5, we investigate the spatial trajectories of lesser black-backed gulls studied in Garthe et al. (2016a). We do not assume any model for the velocity ==== but we directly estimate the scalar product ==== appearing in (1) from instantaneous velocity measurements provided in the dataset (Garthe et al., 2016b). Estimated Kac–Rice formulae allow us to approximate the average depth of marine and terrestrial trips of the birds within a one-day window, which helps to describe their daily habits.====The paper is organized as follows. Section 2 is devoted to Kac–Rice formulae for non-stationary PSP’s. The theoretical framework is presented in Section 2.1, while results for one-dimensional (multidimensional, respectively) processes are given in Section 2.2 (Section 2.3, respectively). The proofs of the results stated in Section 2 have been deferred until Appendix A. Section 3 is dedicated to the statistical inference procedures. The proofs of the estimation results established in this section have been deferred until Appendix B. A simulation study on PDMP’s is provided in Section 4 through three examples: stationary one-dimensional telegraph process in Section 4.1, piecewise deterministic simulated annealing in Section 4.2 and non-stationary two-dimensional telegraph process in Section 4.3. Finally, real data experiments are investigated in Section 5.",Estimation of the average number of continuous crossings for non-stationary non-diffusion processes,https://www.sciencedirect.com/science/article/pii/S0378375818300351,January 2019,2019,Research Article,377.0
"Barati Fahimeh,Talebi Hooshang","Department of Statistics, University of Isfahan, Isfahan, 81746, Iran","Received 21 April 2017, Revised 22 March 2018, Accepted 22 March 2018, Available online 31 March 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.03.007,Cited by (2),"Main effect plus one plans (MEP.1) search for a non-zero interaction effect and estimate it in addition to estimate all main effects. A new series of MEP.1 with ==== runs has been constructed and given in this research for ==== runs, the D-efficiency of new proposed designs is increasing in ====.","Screening experiments are running to identify the active factors. The main effect plans (MEP) are commonly used to screen the active factors, assuming that 2-factor and higher order interactions are all zero. Although, the sparsity and hierarchical principles support this assumption, there may exist a small number of low order non-negligible interaction effects. Under such a scenario, the main effect estimates are biased. That is, ignoring the existent non-zero interactions misleads to come up with the fair estimate for the main effects and hence, the right identification of the active factors. For instance, Wu and Hamada (2009, pp 424–431) reanalyzed the real data of the Cast Fatigue experiment which was analyzed by Hunter et al. (1982). The data were obtained from the 12-run Plackett–Burman design for screening the active factors from a list of seven candidate factors. By identifying 2 active factors, Hunter et al. (1982) fitted a model including 2 main effects and came up with ====. Using a new method of analyzing the data, Wu and Hamada found out that the actual significance is due to a 2-factor interaction (====) which is partially aliased with one of the main effects that was currently included into the model. Replacing the ==== for the main effect in the model, they came up with ====. More going into details of analyzing the data, Wu and Hamada (2009) tended for further support to the existence of the ==== instead of the main effect aliased with. This example addresses the importance and existence of the possible non-zero interactions and needs to estimate such interaction effects in addition to the estimation of the main effects; however, the main effect screening plans are disable to do such a job. The higher resolution plans comprise a large number of runs to estimate the required effects simultaneously. It should be noted that the non-zero interactions are often not known ====. So, the problem is to identify such interactions and estimate them in addition to the main effects. Srivastava (1975) introduced the search design to solve the problem with a reasonable number of runs. The number of runs in search designs is much smaller than those in the usual high resolution designs. In general, let ==== be a ==== vector of unknown factorial effects which the experimenter is interested to estimate, ==== be a ==== vector of unknown factorial effects which one has a partial information about, and ==== be a ==== vector of observations. Consider the search linear model ====where ==== is the random error vector, ==== and ==== is the identity matrix of order ====. Let at most, ==== effects in ==== be non-zero, but which one is non-zero is unknown. A necessary condition for a design to meet the property of being a search design is given by Srivastava (1975) as ====where ====, ====, are all possible ==== submatrices of ====.====For a ==== factorial experiment a search design is called main effect plus ==== plan (MEP.====), if ==== is the set of the general mean and all main effects (====). By parsimony principle ==== is very small in comparing to ====, hence it is rational to restrict ==== to 2- and 3-factor interactions and let all 4- and higher order interactions be zero, i.e. ====.====Construction of MEP.====’s has been considered by several authors, as reviewed by Ghosh et al. (2007). Recently Esmailzadeh et al. (2011) and Talebi and Jalali (2014) presented the MEP.==== with economical number of runs for odd and even ====, respectively.====In this research, a new series of MEP.==== is presented for ==== factorial experiments for all odd prime powers ====, ====, with ==== runs. Although designs in the new series have the same number of runs as Esmailzadeh et al. (2011) it will be shown that they are more efficient in estimating the factorial effects.====In Section 2, some notations are presented. We introduce the new series of MEP.==== through the matrix ==== whose structural format plays a key role to propose the new designs. Some matrix characteristics for 2-, 3- and 4-column submatrices of ==== are given in Section 3 and then the main result is presented in Section 4. The efficiency of the new MEP.====s is addressed in Section 5.",A new efficient MEP.1 series for ,https://www.sciencedirect.com/science/article/pii/S0378375818300338,January 2019,2019,Research Article,378.0
"Sun Shaun Zheng,Lockhart Richard A.","Department of Mathematics and Statistics, University of the Fraser Valley, Abbotsford, BC, Canada V2S 7M8,Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, BC, Canada V5A 1S6","Received 25 October 2016, Revised 21 March 2018, Accepted 22 March 2018, Available online 31 March 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.03.006,Cited by (3),We show that the locally most powerful tests of uniformity on the circle given by Beran have ,"Suppose we have a sample of observations located on the circumference of a circle and want to test if these are randomly distributed with no preferred directions. For example, in a study of habitat perceptions in tropical butterflies, conducted by Cardoso et al. (2017), butterflies were transported from their home habitat to an open field a certain distance away and then released. The butterflies’ flying directions were recorded as points on a unit circle. A test of randomness in the directions can then be used to decide if butterflies can perceive direction.====Testing for randomness around a circle (also called testing for uniformity) is therefore an important problem in the field of directional data. Many tests have been developed for this purpose. Among them are a class of tests, introduced by Beran (1968), Beran (1969), which are locally most powerful invariant for testing uniformity around the circle against a certain parametric alternative. Many classical tests belong to this class. Examples include Rayleigh’s test, Watson’s ==== test (Watson, 1961), Ajne’s test (Ajne, 1968), Rothman–Rao’s test Rothman (1972), Rao (1976) and more recently, Pycke’s test (Pycke, 2010).====In Contreras et al. (2017), Bayesian priors are used to study the average power of goodness-of-fit tests over a band of alternative distributions around the null distribution. The band has a sample-size dependent width chosen so that distances between alternative distributions in the band and the null are large enough to be detectable but small enough not to be obvious; that is, the priors are supported on contiguous neighbourhoods of the null. The Neyman–Pearson Lemma is then used to find an asymptotically (Bayes) optimal test.====In this paper we define Bayes average power of tests of uniformity on the circle using priors on the family of alternative densities. We re-interpret Beran’s result to show that his tests are, for fixed sample size ====, locally optimal for Bayes power for a certain parametric prior distribution. Then we describe general non-parametric priors on the alternative derived by treating the log density as a stochastic process. We show that Beran’s class provides tests which are locally most powerful Bayes against the non-parametric alternative represented by our priors. Finally we interpret the results of Contreras et al. to show that tests in Beran’s class are asymptotically optimal in a non-local framework.====Our proposals are focused on the frequency theory properties of tests but they should be compared to truly Bayesian methods in goodness-of-fit. Nonparametric Bayesian goodness-of-fit testing considers the null hypothesis that a distribution belongs to a finite dimensional parametric model. This parametric model is embedded in a larger family whose number of parameters can grow as the sample size grows or is infinite so that the alternative is infinite-dimensional (nonparametric).====A common Bayesian testing procedure is to assign a prior on the null and alternative and compute the Bayes factor. Verdinelli and Wasserman (1998) developed a nonparametric Bayes factor for goodness-of-fit and examined the consistency of the procedure; their prior is closely connected to ours. See Delampady and Berger (1990), Berger and Pericchi (1996) and Kass and Raftery (1995) for more goodness of fit procedures based on Bayes factors. Relatively speaking, the literature on nonparametric Bayesian goodness of fit is still small; see Tokdar et al. (2010) for a review paper on this subject.====In Section 2 we describe Beran’s results. In Section 3 we define Bayes optimal tests and locally most powerful Bayes tests and re-interpret Beran’s result as showing that his tests are locally most powerful Bayes for a wide variety of priors. In Section 4 we discuss briefly the non-local asymptotic version of these results described in detail in Contreras et al. (2017). In Section 5 specific tests fitting in this framework are described. Section 6 has discussion and final remarks. Proofs are in Section 7.",Bayesian optimality for Beran’s class of tests of uniformity around the circle,https://www.sciencedirect.com/science/article/pii/S0378375818300326,January 2019,2019,Research Article,379.0
"Gaffke N.,Schwabe R.","University of Magdeburg, Faculty of Mathematics, PF 4120, D-39016 Magdeburg, Germany","Received 19 December 2016, Revised 13 March 2018, Accepted 17 March 2018, Available online 27 March 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.03.005,Cited by (6),"Given a linear regression model and an experimental region for the independent variable, the problem of finding an optimal approximate design calls for minimizing a convex ==== over a ",None,Quasi-Newton algorithm for optimal approximate linear regression design: Optimization in matrix space,https://www.sciencedirect.com/science/article/pii/S0378375818300314,January 2019,2019,Research Article,380.0
"Kim Hyungwoo,Wu Yichao,Shin Seung Jun","Department of Statistics, Korea University, 145 Anam-ro, Seongbuk-gu, Seoul, 02841, South Korea,Department of Mathematics, Statistics and Computer Science, University of Illinois at Chicago, 851 S. Morgan Street Chicago, IL 60607, USA","Received 22 March 2017, Revised 2 February 2018, Accepted 5 March 2018, Available online 20 March 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.03.001,Cited by (6),"Sufficient dimension reduction (SDR) has recently received much attention due to its promising performance under less stringent model assumptions. We propose a new class of SDR approaches based on slicing conditional ====: quantile-slicing mean estimation (QUME) and quantile-slicing variance estimation (QUVE). Quantile-slicing is particularly useful when the ==== is more efficient to capture underlying model structure than the response itself, for example, when ==== exists in a regression context. Both simulated and real data analysis results demonstrate promising performance of the proposed quantile-slicing SDR estimation methods.","In high-dimensional data analysis, it is often a primary goal to reduce the dimensionality of data without losing much information of interest. The well-known principal component analysis (PCA) is a canonical example. In the regression context, PCA fails to exploit information about association between the response and predictors. Penalization-based variable selection methods such as LASSO (Tibshirani, 1996) or SCAD (Fan and Li, 2001) can be regarded as another type of dimension reduction. However, many variable selection methods rely on stringent parametric assumptions which may often be unrealistic in practice.====Sufficient dimension reduction (SDR) has received much attention in statistical community. In a regression framework, SDR reduces the predictor dimension by seeking a matrix ==== that satisfies ====where ==== and ==== are the univariate response and ====-dimensional predictor, respectively. The SDR model (1) is quite flexible since it does not impose any type of link function on the relationship between ==== and ====. Yet SDR preserves information about association between ==== and ====, which differs from PCA. The space spanned by the columns of ====, denoted as ====, is called dimension reduction subspace (DRS). DRS is not unique and thus not identifiable. So is ====. To impose identifiability, we define the central subspace, denoted by ====, as the intersection of all DRSs that satisfy (1). It is shown that ==== exists uniquely under mild conditions (Cook, 1996). We finally assume that ==== to make ==== (or more precisely ====) an identifiable target in SDR. The dimension of ====, ==== is called the structural dimension and is another important quantity of interest to be estimated from the data.====Since the seminal work of sliced inverse regression (SIR, Li, 1991) and sliced average variance estimation (SAVE, Cook and Weisberg, 1991) both of which are based on inverse moments, a variety of SDR methods have been developed. Li and Wang (2007) proposed the directional regression based on empirical directions of ==== that generalizes the idea of inverse-moment. The inverse-moment-based methods often require to slice the support of ====, and the selection of slices may affect the finite sample performance. To tackle this issue, Cook and Zhang (2014) proposed a fusing method and Zhu et al. (2010) develop a cumulative slicing estimation. Li et al. (2005) proposed an alternative method for SDR called contour regression, and this motivates the principal support vector machine (Li et al., 2011), a unified framework to handle both linear and nonlinear SDR.====The SDR model (1) can be viewed as a semi-parametric model for the conditional distribution function of ==== given ====, denoted by ====. Xia et al. (2002) proposed the minimum average variance estimation (MAVE) which recovers ==== by estimating the conditional expectation, ==== which is the same as ==== under (1). Motivated by MAVE, related variants have been developed. See for example, Xia (2007), Wang and Xia (2008), and Yin et al. (2011). Zhu and Zeng (2006) proposed an estimator of ==== by estimating the gradient of ==== using the Fourier transformation. Kong and Xia (2014) exploited the gradient of quantile function, instead of ====, and proposed the adaptive composite quantile outer product of gradients method. Ma and Zhu (2012) derived the space of influence functions of the estimator of ====, and Ma and Zhu (2013) further proposed an efficient estimator of ==== and established its asymptotic properties. Recently, Huang and Chiang (2017) proposed an alternative semi-parametric estimator for SDR that can estimate ==== and ==== simultaneously.====In practice, data often display heteroscedastic variance which can be of scientific importance. Note that the SDR model (1) requires the conditional independence only. In principal it has no difficulty to encompass underlying heteroscedasticity in the data. However, most of SDR methods are designed to focus primarily on conditional mean relation and can be inefficient to identify such heteroscedasticity, as illustrated by a toy example coming up next. See Kong and Xia (2014) and Wang et al. (2018) for difficulties in SDR with heteroscedasticity.====The quantile regression is a popular alternative to the conventional mean regression when homoscedastic error assumption is violated. The quantile regression seeks the ====th conditional quantile of ==== denoted by ==== that satisfies ====for a given quantile level ====. Notice that the conditional distribution of ==== possesses all the information about ====. By stacking together all the conditional quantile functions of ==== at different quantile levels, we define ==== as a function of the quantile level ====. The stacked conditional quantile function ==== contains complete information about the conditional distribution of ====.====It can be shown that the stacked conditional quantile function ==== contains same amount of information on ==== as ==== does. Namely, ==== where ==== denotes the central subspace for the “regression” of ==== on ==== and is defined accordingly. This motivates us to develop a new SDR approach that slices conditional quantiles of ==== instead of the response ==== itself to estimate ====. Finally two versions of estimators based on quantile-slicing are developed: QUantile-slicing Mean Estimation (QUME) and QUantile-slicing Variance Estimation (QUVE).====In practice, ==== is an unknown quantity and should be inferred from the data. Toward this, we propose to use the kernel quantile regression (KQR, Takeuchi et al. (2006), Li et al. (2007)). The KQR is a flexible nonparametric method showing promising performance in high-dimensional data due to the use of the kernel trick (Zhang, 2002). The KQR solution as a function of ==== is piecewise linear in ==== (Takeuchi et al., 2009). This enables us to estimate the stacked quantile function ==== completely from a finite sample.====As a simple illustration, we consider a toy example of simple regression with heteroscedastic error: ====, where ==== and ==== are independent of each other. Notice that ==== where ==== and ==== and hence ==== and ==== are the two sufficient predictors, ==== for mean and ==== for variance of the response. We apply SIR and QUME to a random sample of size 100 generated from this simple model. Fig. 1 depicts the estimated ==== by SIR (red plane with a finer mesh) and QUME (back plane with a coarser mesh) on the three dimensional predictor space. The table at the bottom-right corner reports the distance between true and estimated ==== in terms of the criterion defined in (10). One can see that QUME outperforms SIR for identifying ====. In particular, SIR shows insufficient accuracy to identify the direction associated with ==== that controls variance of the response.====The rest of article is organized as follows. In Section 2, we first show the equivalence between ==== and ====, and then propose two versions of quantile-slicing scheme, QUME and QUVE. In Section 3, the finite-sample implementation of QUME and QUVE via solution paths of the KQR is described in details. Additional issues including estimation of the structural dimension and tuning parameter selection in KQR are discussed in Section 4. Illustration to both simulated and real data are presented in Sections 5 Simulation, 6 Application to AIDS data, respectively. Concluding remarks are in Section 7. All the proofs are relegated to the Appendix.",Quantile-slicing estimation for dimension reduction in regression,https://www.sciencedirect.com/science/article/pii/S0378375818300272,January 2019,2019,Research Article,381.0
Tsao Min,"Department of Mathematics, Southern University of Science and Technology, 1088 Xueyuan Avenue, Nanshan District, Shenzhen, Guangdong, 518055, China,Department of Mathematics & Statistics, University of Victoria, Victoria, British Columbia, Canada V8W 3R4","Received 20 July 2017, Revised 11 January 2018, Accepted 8 March 2018, Available online 20 March 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.03.003,Cited by (6), of these parameters and uncover ,"Strongly correlated predictor variables appear often in data sets from observational studies such as social and medical studies. Such variables generate a multicollinearity problem for the (ordinary) least-squares regression. To describe the problem, consider linear model ====where ==== is an ==== vector of observations, ==== a known ==== design matrix, ==== an unknown ==== vector of regression parameters, and ==== an ==== vector of random errors with mean zero and variance ====. Suppose the first ==== variables ==== are strongly correlated (====). Then, the resulting multicollinearity problem manifests through unusually large variances of least-squares estimators ====, ====, ====
               ==== for ====, ====, ====
               ====, rendering the estimators unreliable. This also makes it difficult to make inference and predications using the estimated model. Discussions about this type of multicollinearity can be found in many books on linear models, ====, Draper and Smith (1998), Belsley et al. (2004), and Montgomery et al. (2013). In practice, the problem is often handled by modified least-squares methods, such as Ridge regression (Hoerl and Kennard, 1970) and principal component regression. However, these methods are more complicated than the least-squares regression in terms of implementation, inference and interpretation. One of the objectives of this paper is to explore the use of the least-squares estimates for handling the consequences of multicollinearity, instead of avoiding it through more complicated alternatives, thereby making better use of the least-square regression in the presence of multicollinearity.====Although it is widely known that multicollinearity causes poor estimation of ====, ====, ====
               ==== in the least-squares regression, we conducted a comprehensive search of the literature but found no discussion about its impact on the estimation of linear combinations of ====, ====, ====
               ====. Silvey (1969) studied the estimation of linear combinations of all parameters ==== but his results are not concerned with such an impact. To study this impact and to make the problem well defined, we consider ====where ==== is any ==== vector satisfying ====. We call set ==== the class of ==== of the ==== strongly correlated variables, each ==== a ==== and each vector ==== a ====. We choose constraint ==== instead of ==== because it allows ==== to include commonly used weighted averages such as ====, even though it is technically more difficult to handle as it is non-smooth. We say a group effect is ==== if its minimum variance unbiased linear estimator has a variance that is smaller than or comparable to the error variance ====.====Individual parameters ==== are special group effects in ==== but they are not estimable. We are interested in finding estimable effects in ====; as none of the underlying parameters is estimable, such estimable effects are of theoretical interest. More importantly, the estimable effects are useful for inference and estimation concerning ====, and for knowing when accurate predictions can be made with the estimated model. For example, if ==== is estimable and its estimated value ==== is significantly different zero, then we may reject the null hypothesis ==== and conclude that one or more parameters in this group are not zero. Also, the unknown parameters ==== satisfy ====which is a constraint that reduces the parameter space for the ==== variables from ==== to essentially a line in ====. Such a dimension reduction can be used for estimating the unknown parameters. The set of weight vectors ==== associated with the estimable effects also defines the region in the space of the ==== strongly correlated variables over which accurate predictions can be made using the least-squares estimated model.====To study estimable group effects in ====, we first construct a uniform model containing a single group of strongly correlated predictor variables with a uniform correlation structure. For this model, the level of multicollinearity can be quantified and its impact on estimation of group effects is mathematically tractable. Under this model, we find an optimal effect that benefits from multicollinearity in that the variance of its minimum-variance unbiased linear estimator actually decreases when the level of multicollinearity increases; that is, this effect can be more accurately estimated when the level of multicollinearity goes higher. It is rather surprising that such a linear combination of the parameters exists as these parameters themselves are not estimable at high levels of multicollinearity. Further, this optimal effect can be substantially more accurately estimated than a similar effect of a group of uncorrelated variables. The optimal effect has a simple interpretation as a ==== of the underlying parameters. At any given level of multicollinearity, all estimable effects under the uniform model are located around it.====With the knowledge that multicollinearity can be an advantage in estimating group effects, we look for estimable effects of strongly correlated variables in a general linear model (1) which may contain more than one group of such variables with unspecified correlation structures, as well as not strongly correlated variables. For such a general model, multicollinearity becomes difficult to quantify, which makes it difficult to formulate the problem of finding estimable effects analytically. Although in principle it is possible to find such effects numerically for any group of strongly correlated variables, the computational effort required may be prohibitive. To avoid these difficulties, we make use of insights gained from the uniform model and generalize the variability weighted average effect to a group of strongly correlated variables in model (1). The (generalized) variability weighted effect is easy to compute and always accurately estimated in our numerical examples. It provides a simple means for finding other estimable effects in (1) as these are also located around it.====The focus of this paper is on finding estimable effects, but we will briefly discuss how such effects may be used to ensure prediction accuracy of the least-squares estimated model. More applications are given in Tsao (2017), and one of these is a constrained local regression method that uses the variability weighted average effect to estimate the underlying parameters of strongly correlated variables. This method complements the Ridge regression and other penalized least-squares methods such as Lasso (see, ==== Hastie et al., 2015) in that it is a local method for estimating parameters of only strongly correlated variables; the ordinary least-squares estimates of other parameters are unchanged.====The rest of this paper is organized as follows. In Section 2, we introduce the uniform model under which we reduce ==== to a subclass ====; this subclass is only “====th” the size of ==== but it contains all effects that can be most accurately estimated. We then find the optimal variability weighted average effect through ==== and give a characterization of all estimable effects under the uniform model using this effect as a reference point. In Section 3, we go beyond the uniform model and generalize the variability weighted average effect to a group of strongly correlated variables in model (1). We give numerical examples demonstrating the remarkable accuracy at which it is estimated under severe multicollinearity. We also briefly discuss the application of estimable effects in the prediction accuracy problem of the least-squares estimated model. We conclude with a few remarks in Section 4. All proofs of theorems and corollaries are given in the Appendix.",Estimable group effects for strongly correlated variables in linear models,https://www.sciencedirect.com/science/article/pii/S0378375818300296,January 2019,2019,Research Article,382.0
"Liu Zhifan,Liu Chunling,Sun Zhihua","University of Chinese Academy of Sciences, Beijing, 100049, China,The Hong Kong Polytechnic University, Kowloon, Hong Kong,Key Laboratory of Big Data Mining and Knowledge Management of CAS, Beijing, 100049, China","Received 30 May 2017, Revised 5 February 2018, Accepted 5 March 2018, Available online 19 March 2018, Version of Record 4 June 2018.",https://doi.org/10.1016/j.jspi.2018.03.002,Cited by (3),"In this paper, we consider the adequacy check of the varying-coefficient model when ","The varying-coefficient model is a popular semiparametric model, which takes the form: ====where ==== is a scalar response variable, ==== is a ====dimensional predictor and ==== is scalar. The functional coefficient ==== is unknown and the model error ==== satisfies ====. Model (1.1) was introduced by Hastie and Tibshirani (1993) and further studied extensively. See Fan and Zhang (2000), Fan and Zhang (2007), Wang et al. (2008), Park et al. (2015) and the references within.====In this paper, we consider the adequacy check of the varying-coefficient model with errors in covariates. That is, we aim to test ====against the alternative hypothesis that ==== is not true when the variable ==== is not observed for some reasons, such as measurement error. Instead of ====, its surrogate ==== is observed. We assume that a ====-dimensional auxiliary variable ==== is available to remit ====. And the unobservable true variable ====, the observed surrogate variable ==== and the auxiliary variable ==== are of the following relationship: ====Actually, the error structure (1.3) is a special case of the additive error model because (1.3) is equivalent to the model that ==== with ====. We further assume that ==== but allow for an unknown covariance of the error variable ====. So we aim to investigate the adequacy of the following model ====
            ====This type of measurement error was also discussed in Zhou and Liang (2009), Zhao and Xue (2010), Sun et al. (2015) and Zhang et al. (2017). We present two examples, which are similar to Examples 1–2 in Zhou and Liang (2009), to illustrate the rationality of Model (1.4).====
               ====
            ====
               ====
            ====Let ==== be an i.i.d. sample from the population ====. For Model (1.3), if the measurement error is ignored, it can be validated that the naive estimator of the coefficient function, denoted by ====, will be biased. Let ==== be the naive estimator of the model error ==== for the ====th subject. It can be decomposed into three parts: ====for ====. We can validate that the first and third terms, ==== and ====, have zero expectations. However, the expectation of ====
 does not converge to zero in that the expectation of ==== is not equal to zero. Actually, the term ==== acts just as a deviation function. This causes the naive test to tend to reject the null hypothesis even if it is true. We conduct some simulation studies in Section 4, which show that the naive method yields empirical sizes larger than 0.5 in many scenarios. A reasonable test should be able to control Type I error. This motivates us to develop a model checking method for (1.2) based on the calibration of the measurement error.====The estimation of the regression models with errors in covariates has been studied extensively. See Liang et al. (1999), Carroll et al. (2006), Ma et al. (2006) and Li and Greene (2008), among others. However, the lack-of-fit test of regression models with measurement error has not received enough attention that it deserves. Sporadic researches can be found in the literature: Hall et al. (2007), Ma et al. (2011), Koul and Song (2009) and Sun et al. (2015). For the model checking problem (1.2), we first calibrate the model error and then build an empirical process (EP) test with simple indicator (SI) weighting function, which has many merits. First, it is consistent; second, it is free from the nonparametric smoothing of the estimated model error; third, it can detect the alternative hypothetical model converging to the null hypothetical model at the parametric rate. More details of the EP test with SI weighting function can refer to Zhu and Ng (2003), Sun et al. (2009), Ma et al. (2014) and Xu and Zhu (2015), among others.====The rest of paper is organized as follows. In Section 2, we calibrate the model error and develop an empirical process test. The asymptotic properties of the test statistic are rigorously studied in Section 3. In Section 4, simulation studies and real data analyses are conducted to validate the performance of the proposed test. The proofs of the main results are presented in the Appendix.",Consistent model check of errors-in-variables varying-coefficient model with auxiliary variable,https://www.sciencedirect.com/science/article/pii/S0378375818300284,January 2019,2019,Research Article,383.0
"Baek Jong-Il,Park Sung-Tae","Division of Mathematics and Informational Statistics, Institute of Basic Natural Science, Wonkwang University, IkSan 570-749,South Korea,Division of Business, and Business and Economic Research Institute, Wonkwang University, IkSan 570-749, South Korea","Available online 24 October 2018, Version of Record 7 February 2019.",https://doi.org/10.1016/j.jspi.2018.10.004,Cited by (0),"This article has been retracted: please see Elsevier Policy on Article Withdrawal (====). This article has been retracted at the request of Editors.====The article is a duplicate of a paper that has already been published in the Journal of Theoretical Probability (2010), 23: 362–377, ====. One of the conditions of submission of a paper for publication is that authors declare explicitly that the paper is not under consideration for publication elsewhere. As such this article represents a severe abuse of the scientific publishing system. The scientific community takes a very strong view on this matter and apologies are offered to readers of the journal that this was not detected during the submission process.",None,Retraction notice to “Convergence of weighted sums for arrays of negatively dependent random variables and its applications” [J. Statist. Plann. Inference 140(9) (2010) 2461–2469],https://www.sciencedirect.com/science/article/pii/S0378375818303422,24 October 2018,2018,Research Article,388.0
