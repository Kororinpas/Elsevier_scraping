name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
"Das Chaudhury Ratul,Bhattacharya Sukanta","Monash Digital Impact Lab, Monash Business School, Monash University, Clayton, 3800, Victoria, Australia,Department of Economics, University of Calcutta, Kolkata 700050, West Bengal, India","Received 2 July 2022, Revised 5 May 2023, Accepted 10 May 2023, Available online 20 May 2023.",https://doi.org/10.1016/j.mathsocsci.2023.05.001,Cited by (0),"We model the situation where a borrower can choose to acquire costly information about the outcome before implementing a risky project. The borrower is resource-constrained and faces a trade-off between incurring the cost of information or putting effort into the project. We provide novel insights about the type of project the borrower chooses and identify the conditions under which the borrower acquires information. We characterize the ==== conditions for the ==== charged by a socially-motivated as well as a profit-motivated lender. We find that if the ==== is high, the borrower is likely to choose riskier projects and acquire information about the outcome. If capital is moderately expensive for the lender, even the socially-motivated lender charges a higher interest and makes a positive profit. This provides an alternate explanation for the prevalence of high-interest rates in the rural credit market, despite the presence of socially-motivated lenders.","A typical decision maker is not always certain about the outcome of their actions and often seeks advice from suitable experts with relevant knowledge about the outcome. The outcome of an action is pertinent in informed decision-making. A graduate student, for example, turns to their career counselor about the chance of success before applying for a job. An investor consults a financial advisor regarding the appraisal of a potential investment portfolio. An electoral candidate consults with their political advisor who holds specialized information about the local voter characteristics and the likelihood of success for implementing a particular strategy and its potential impact on electoral outcomes. Prior information regarding the outcome can save the decision maker the effort of choosing a strategy with an undesirable outcome.====In this article, we develop a stylized model to study whether a decision-maker (DM) seeks expert advice to resolve the uncertainty involving the outcome of a risky project. In other words, if the information is inexpensive but not costless, is it always beneficial for the DM to collect information regarding the outcome of the investment? If the DM can collect information about the outcome of a risky project, then what is the level of riskiness of the investment project they choose? We study a reduced-form model where the expert’s opinion is costly but not strategic. The cost can be manifested as a value for time and research put in by the expert to acquire information and impart unbiased objective reports about the project outcome.====In developing countries, farmers have an informational disadvantage, and providing useful information about price uncertainty can resolve the issue (Mitra et al., 2018). The incidents of crop failure, excess production, bad weather, etc., indicate that uncertainty plays a role in determining the outcome of an investment. Scientists and consultants (henceforth, ====) possess specialized information regarding the aforementioned factors that determine the outcome of the project. This motivates us to introduce expert advice in our analysis. Expert advice is analogous to ==== about the project. The need for credit in the rural market is moderated through informal lenders, in exchange for a high rate of interest. However, rural credit with joint liability lending and peer monitoring has not been fully effective in alleviating the problem of high interest (Dehejia et al., 2012, Maitra et al., 2017). Besides informal lenders, the market for rural credit is dominated by socially-motivated lenders like microfinance and not-for-profit organizations. We provide an explanation as to why socially-motivated lenders charge high-interest rates. The analysis of a market where prior information is available about the outcome of risky projects is an issue to ponder over for policy purposes.====To further explain our results, consider the main features of our model. We model a sequential game with a lender as the first mover. The lender decides the rate of interest. The individually liable borrower chooses an investment project based on an increasing level of risk to return and faces a resource constraint. The borrower can seek expert advice about the project outcome. However, seeking information to learn the ‘future’ is costly and sunk in nature. This introduces a trade-off, pay the expert fee to learn about the outcome of the risky project (==== or ====) or save on the expert’s fee and take the gamble.==== Ceteris paribus, acquiring information about the outcome of a risky project reduces the uncertainty of repayment. If the cost of acquiring information is low and the interest rate is above a threshold, the borrower chooses riskier projects and collects information about the project outcome. Alternatively, a low rate of interest induces the borrower not to seek information about safer projects. We demonstrate that if the cost of information goes down, the likelihood of collecting information goes up.====We show that a socially-motivated lender maximizing total surplus charges an interest higher than the breakeven rate under certain conditions. The intent of most microfinance lenders is to alleviate poverty. Our formulation of the socially-motivated lender allows for the possibility where the lender not only cares about their own payoff but also the borrower’s payoff. When the lender only breaks even, the lender’s objective function boils down to the payoff of the borrower. Although, we allow for the possibility of the lender to earn more than the breakeven rate. Firstly, we find the optimal interest rate in the setting with socially-motivated lenders. Our formulation provides an explanation for the prevailing high-interest rate among socially-motivated lenders. Later, we then contrast our results with that of a profit-motivated lender.==== Developing economies have a long-standing history of moderation of the credit demands by informal lenders for a high rate of interest (Ghatak, 1999, Dehejia et al., 2012, Maitra et al., 2017). Traditional models in microfinance focused on it as a method of poverty reduction (Morduch, 1999, Karlan et al., 2014). Eventually, the focus shifted towards developing more efficient lending models in the absence of collateral. Mechanisms like peer monitoring and joint liability loans were introduced (Varian, 1990, Stiglitz, 1990, Ghatak, 1999). Initially, joint liability lending was used to induce screening and peer monitoring to deal with adverse selection (Laffont and N’Guessan, 2000, Ghatak, 2000, Gangopadhyay et al., 2005) and both ex-ante and ex-post moral hazard (Stiglitz, 1990, Ghatak and Guinnane, 1999, Bhole and Ogden, 2010). Ghatak (2000) emphasized endogenous group formation through local information and self-selection. In the absence of collateral, joint liability with positive matching has the potential to improve welfare. In the presence of social capital, peer monitoring reduces the rate of interest charged on loans (De Aghion and Gollier, 2000). In a fairly similar setting, Laffont and N’Guessan (2000) highlight that prior knowledge about the other borrowers increases efficiency in lending.====The success of most microfinance institutions in developing nations can be credited to the tight-knit societies. Besley and Coate (1995) highlight the repayment game in presence of strategic default through social sanctions. However, research and experiments in developing countries about credit rationing and peer monitoring provide evidence of the downsides of joint liability lending (De Aghion and Morduch, 2000, Maitra et al., 2017). A significant portion of the literature focus on strategic defaults and the alternatives to joint liability lending (Rai and Sjöström, 2004, Karlan and Zinman, 2011). Studies show that the rate of repayment and welfare is unaffected by individual or joint liability lending in places with strong social collateral (Madajewicz, 2011, Giné and Karlan, 2014, De Quidt et al., 2016). However, in our work, we suppress the impact of social capital. This motivates us to consider individual liability lending without collateral.====Farmers suffer from informational asymmetries regarding prices. Researchers suggest that providing useful information and developing a communication network can resolve the issue (Mitra et al., 2018). This motivates us to include the option of seeking expert advice in models of individual liability lending. A significant portion of the information collection literature focus on learning, efficiency, or welfare (Larcker and Lys, 1987, Bergemann and Välimäki, 2002, Colombo et al., 2014).==== Similarly, models on expertise focus primarily on the biased and unbiased expert benefiting from the outcome (Crawford and Sobel, 1982, Krishna and Morgan, 2001, Schnakenberg, 2015). We study a reduced version of expert advice where the information acquisition is costly but the expert is truthful and has no stake in the outcome. However, we explicitly incorporate the incentive to seek expert advice for ex-ante state verification in a credit market set-up.====In this article, we tie together the literature on individual liability lending with that of information acquisition. A salient feature of our model is that the interest rate is endogenously determined. This feature is absent in models of information acquisition or information sharing in the credit market (Barlevy and Veronesi, 2000, Hauswald and Marquez, 2006, Van Nieuwerburgh and Veldkamp, 2010, Karapetyan and Stacescu, 2014).====The rest of the paper is organized as follows. The following section outlines the main results of our analysis followed by the conclusion. In Section 2, the first subsection highlights the borrower’s problem. The borrower is given the choice to invest in a risky project and the opportunity to learn about the project outcome ex-ante. Section 2.2 features the optimization problem of a socially-motivated lender who maximizes total surplus. In Section 2.3, we consider the optimization problem of a profit-motivated lender and contrast our results from the previous subsection.",When to seek expert advice? A simple model of borrowers with limited liability,https://www.sciencedirect.com/science/article/pii/S0165489623000483,Available online 20 May 2023,2023,Research Article,0.0
"Lee Ki-Dong,Choi Kangsik","Faculty of Economics & Commerce, Keimyung University, Republic of Korea,Graduate School of International Studies, Pusan National University, Republic of Korea","Received 13 September 2022, Revised 25 April 2023, Accepted 29 April 2023, Available online 9 May 2023, Version of Record 6 June 2023.",https://doi.org/10.1016/j.mathsocsci.2023.04.004,Cited by (0),"Using the export-rivalry model, we examine the welfare impact of two tariff regimes when firms’ vertical structure is endogenously determined via the strategic behavior of each firm. First, with discriminatory tariffs, if the degree of imperfect ","The most favored nation (MFN) clause is so important that it is the first article of the General Agreement on Tariffs and Trade (GATT) that governs trade in goods. In the World Trade Organization (WTO) regime, the successor to GATT, the MFN clause has played an important role in bringing about multilateral trade liberalization. The MFN nondiscrimination rule is justified in terms of world production efficiency, as it eliminates inefficiencies arising when a country imposes different tariff rates on the same goods imported from different trading partners. There are numerous papers that study the superiority of a uniform tariffs, as compared with a tariff discrimination.==== However, the interests of individual trading countries do not necessarily coincide with global gains from the MFN clause. This is especially true when the country’s tariff policy is the consequence of a strategic trade policy. Our central objective is to show the different effects of vertically related market between uniform and discriminatory tariffs on the economy with the assumption that governments pursue a rent-extracting strategic trade policy.====There are primary empirical studies that international trade affects the organizational form of firms. Alfaro et al. (2016) investigated the empirical validity of the productivity gains and loss from integration by studying the organizational effects of trade policy, which provides a source of price variation that is exogenous to firms’ ownership decisions. They also found that trade policy provides a source of exogenous price variation to assess this prediction: higher tariffs should lead to higher prices and, therefore, to more integration. Moreover, the empirical evidence using establishment-level data from the U.S. Census in Bai (2021) showed that firms’ internal specialization or diversification plays a significant role in shaping the impact of international trade shocks.==== So far, the evidence on the implications of these models is sparse when considering international trade between uniform and discriminatory tariffs.====On the theoretical side, Brander and Spencer (1985) demonstrated that the importer prefers to impose differentiated tariffs when the exporters differ in production cost, and the tariff. In canonical papers on the welfare comparison of uniform and discriminatory tariffs, both Gatsios (1990) and Hwang and Mai (1991) showed that the home country prefers discriminatory tariff to uniform tariff by imposing a higher (lower) tariff on the low-cost (high-cost) firm (see also (Liao and Wong, 2006, Hashimzade et al., 2011a, Hashimzade et al., 2011b)). On the other hand, Choi (1995) examined the importing country’s choice between two tariff regimes, focusing on the impact of short-run discriminatory tariffs on exporters’ long-run choice of technology (or capacity). In particular, Choi (1995) analyzed that the importing country is better off with a uniform tariff while the foreign duopolists are better off when the importing country pursues a discriminatory tariff. Most of the existing literature, such as Saggi, 2004, Saggi, 2009, Ozerturk and Saggi (2005) and Hashimzade et al., 2011a, Hashimzade et al., 2011b, support the argument that uniform tariff is superior to discriminatory tariff in terms of global welfare, because tariff discrimination will result in cost asymmetry caused by imposing a higher (lower) tariff on the low-cost (high-cost) firm.====However, Saggi and Yildiz (2005) showed that tariff discrimination can be welfare preferred to MFN globally when inefficient exporters are merged and the cost disadvantage of the merged unit relative to competing exporters is of intermediate magnitude. Recently, Din et al. (2016) examined the superiority of MFN vs. tariff discrimination in global welfare by taking into account the cross ownership between exporters. Thus, given cross ownership of financial interests and linear demand, the government of the importing country will impose a lower (higher) tariff on the low-cost (high-cost) firm and the global welfare under tariff discrimination will be higher than that under MFN.====These studies contribute to the literature by providing an in-depth understanding of the welfare implications of two different tariff regimes, especially when firms compete in an oligopolistic market. However, since the above studies overlook the vertical structure in the market as exogenously given, they ignore the interaction between the vertical structure and trade policies in the welfare analysis of the tariff regime. This study sheds light on the fact that the existing literature on the superiority of MFN vs. tariff discrimination has paid relatively little attention to the endogenous choice of exporters’ vertical structure. Reflecting that diverse vertical market structures are a common phenomenon in the international market, the literature exploring the validity of trade policies for the upstream and downstream markets has been growing.====In an oligopolistic market, the strategic advantages of vertical separation are well known. Strategic vertical separation is discussed in papers on vertical relations between manufacturers and distributors (e.g., (Bonanno and Vickers, 1988, Gal-Or, 1990, Rey and Stiglitz, 1995, Buehler and Schmutzler, 2008)). Most of the literature on vertical separation in distribution channels presumes a “domestic” market and focuses on symmetric vertical market structures. Thus, in equilibrium, these studies on strategic vertical relations obtain symmetric market structures, except for Buehler and Schmutzler (2008).====Recent theoretical work has embedded models of firms into market settings to study how firms’ boundary choices are affected by market conditions. In particular, market thickness, demand elasticities, and terms of trade in supplier markets may have an impact on firms’ vertical integration decisions (e.g. (McLaren, 2000, Grossman and Helpman, 2002, Legros and Newman, 2008)). Legros and Newman (2013) is the first paper to point out that product prices can have a causal impact on integration decisions. In particular, Ziss (1997) has examined the choice between vertical integration and vertical separation by domestic and foreign upstream manufacturers. Although he used a three-country export rivalry model following Brander and Spencer (1985), he did not consider the effects of importing country’s strategic tariffs on exporters nor was he able to derive exporters’ endogenous vertical structure including an asymmetric structure. It has been pointed out that international competition including trade policies can have a significant effect on firms’ vertical structures. The international competition can affect productivity via firms’ organization choices such as their vertical integration intensity (Acemoglu et al., 2010, Alfaro et al., 2016, Aghion et al., 2006, Conconi et al., 2012). For instance, vertical integration might improve coordination between different stages of production but at the same time accompany administrative costs that are not directly related to output and product price. Then a price-taking firm will choose to integrate only if the benefits in terms of increased productive overweigh the cost of integrating (Alfaro et al., 2016).====Considering the growing competition among firms and the trade barriers in a global market framework, it is somewhat naive to apply the research results for the domestic market to the global market. In fact, in the real world, numerous firms compete in the international market in a diverse vertical structure. Examples of industries where vertical separation is a key feature of their organizational structure include aircraft, computers, and audio/video systems (Matsushima and Mizuno, 2013). Despite widespread vertical separation, there are many industry- and firm-level examples of vertical integration in reality. In the electronic devices industry, Samsung pursues like many other Asian producers, such as NEC Corporation or SONY Corporation, vertical integration strategy in the sense that it controls much of its value chain. Although Samsung relies on vertical integration as its competitive advantage, Apple, the biggest rival in the market, still purchases billions of dollars’ worth of components from the outside upstream manufacturers. And what is important is that trade friction is particularly frequent in these industries. In fact, both the United States and Europe have actively used antidumping tariff and other trade restrictions for imports from Japan as well as from Korea in these industries. The above examples imply that the economic analysis to determine the firm’s vertical structure in an international oligopoly needs to take into account the trade policies in particular.====The present paper presents the following findings. First, with discriminatory tariffs, if the degree of imperfect substitutability is sufficiently low, the rent-shifting effects of vertical separation becomes less important and firms chooses vertical integration to enjoy lower tariffs, and vice versa if the degree of imperfect substitutability is sufficiently high. However, an asymmetrical vertical structure, i.e., low (high)-cost firm chooses vertical separation (integration), emerges only in the uniform tariffs. Thus, the existence of firms’ diverse vertical structure between tariff systems can arise. Second, unlike the traditional results, the prisoner’s dilemma situation arises in the determination of firms’ vertical structure even in the Bertrand competition between firms. Although firms are both better off if they choose vertical separation, each firm pursuing rational self-interest chooses vertical integration instead of vertical separation in the discriminatory tariffs. Third, when the endogenously determined vertical structure varies depending on the tariff system, then each country’s preference for the tariff system contrasts with the traditional view. If both firms choose vertical integration in the discriminatory tariffs but vertical separation in the uniform ones, both exporters prefer the uniform tariffs as far as cost differential between firms is not large; and the global welfare is greater in the discriminatory than in the uniform tariffs. Especially, the latter is striking considering that many countries have been pursuing the application of MFN rule under the auspices of the GATT/WTO which involves non-discrimination or symmetric treatment for all trading partners.====The rest of this paper is organized as follows. Section 2 describes the basic model. Sections 3 Discriminatory tariffs and vertical structure, 4 Uniform tariffs and vertical structure examine the market equilibrium for each firms’ vertical structure in the discriminatory tariffs and uniform tariffs, respectively, and then analyzes the firms’ choosing of the vertical structure. A welfare comparison between the two tariff regimes will be examined in Section 5, and Section 6 provides our concluding remarks.",Optimal tariffs with endogenous vertical structure: Uniform versus discriminatory tariffs,https://www.sciencedirect.com/science/article/pii/S0165489623000392,9 May 2023,2023,Research Article,1.0
"Li Changying,Li Youping,Zhang Jianhu","School of Economics, Shandong University, 27 Shanda South Road, Jinan, Shandong 250100, China,School of Business and the Oliver Hart Research Center of Contracts and Governance, East China University of Science and Technology, 130 Meilong Road, Shanghai 200237, China","Received 15 August 2022, Revised 25 April 2023, Accepted 25 April 2023, Available online 5 May 2023, Version of Record 18 May 2023.",https://doi.org/10.1016/j.mathsocsci.2023.04.003,Cited by (0),"We analyze the effect of targeted informative advertising on firms’ incentive to improve product quality and the welfare implications. We find that, compared with mass advertising, targeted advertising results in (i) a decreased incentive to invest in R&D unless the cost of advertising is sufficiently low, (ii) a lower mark-up, net of product quality, being charged to consumers, and (iii) a smaller (larger) proportion of uninformed consumers when the cost of advertising is low (high). The firms may earn higher or lower profits, but consumers are usually better off due both to the lower net mark-up and to improved product-consumer match. Under certain conditions though, the negative impact of more uninformed consumers dominates and leads to reduced consumer and total welfare.","Digital tracking of consumer behavior by online platforms, search engines and shopping websites enables businesses to provide product information to their targeted consumers. It is estimated that ad spending on digital media worldwide was over $450 billion dollars in 2021, accounting for 60.9% of total media ad spending.==== Targeted advertising can also be through offline media. For example, a firm can mail catalogues to residents in certain geographical regions or to those who previously made a purchase.====By reducing the wastage of sending advertisements to unlikely buyers, targeted advertising is usually considered more effective than traditional mass advertising. Despite the savings, its effect on firm profits has been found to be more complicated as this means of advertising changes also the way how firms compete, both in advertising and in pricing. A largely neglected aspect in existing studies, however, is the firms’ innovation strategies. In this paper, we incorporate R&D investment into the decisions of the firms, and provide an integrated analysis of the implications of targeted informative advertising.====Strategies on product, price and promotion, three of the marketing mix, are of critical importance to the success of businesses. They are also interconnected as the choice of one affects the choices of the others. For example, the amount a firm invests in R&D to improve product quality, and the price it charges, depend on how well the product is known to consumers. A shift from mass to targeted advertising thus changes also a firm’s R&D and pricing decisions. In a monopoly framework, Esteban et al. (2006) compare the effects of different advertising strategies on product quality and price. Better targeting in advertising certainly benefits a monopolist whose quality and price choices are made solely to maximize own profit. But when there is competition and the firms act strategically, the indirect effects of one’s strategies on the rival play an important role and the equilibrium outcome is less obvious.====We employ a differentiated duopoly model in which the firms play a two-stage game. They first conduct R&D to improve product quality and then make their advertising and pricing decisions. We find that, unless the cost of advertising is sufficiently low, the incentive to invest in R&D is lower with targeted advertising than with mass advertising. The firms balance a direct effect of higher product quality on profit and two indirect effects, via the reduction of the rival’s advertising intensity and product price. Depending on the cost of advertising, they may earn higher or lower profits. And endogenous R&D investment expands the range in which ad targeting makes the firms better off, relative to a framework without R&D choices.====On the other hand, targeted advertising leads to a lower net mark-up—mark-up​ minus product quality—being charged to the consumers, in addition to better product-consumer match. However, the total effect on consumer welfare depends also on the number of informed and uninformed consumers in equilibrium. We find that the firms spend more (less) on advertising, and fewer (more) consumers remain uninformed about the products, when the cost of advertising is low (high). Consumer surplus is usually higher with targeted advertising, unless the negative impact of more uninformed consumers dominates the two positive effects.====The widespread adoption of targeted advertising in various industries has attracted considerable attention among economists and marketing researchers. That ad targeting helps raise firm profit in a monopoly market seems to be obvious.==== But with competition, the way how firms advertise affects how they compete in the market and can have an ambiguous effect on their payoffs. Iyer et al. (2005) develop a duopoly model in which consumers are divided into loyal customers and comparison shoppers, and find that targeting increases equilibrium profits by reducing wasted advertising. This is often the case even when the firms have asymmetric loyal market shares, as shown by Arnold et al. (2022). Galeotti and Moraga-González (2008) analyze Bertrand competition with segmented market and targeted advertising, and show that the firms may obtain positive profits in the symmetric equilibrium. In these models, the firms either advertise to a group of consumers and get them informed, or not. When the intensity of advertising is a continuous choice, as in the differentiated duopoly model of Brahim et al. (2011), the equilibrium profits under targeted advertising may be lower relative to mass advertising.==== With the inclusion of R&D decisions, our result is largely consistent, although endogenous quality investment expands the range in which ad targeting makes the firms better off.====Important from a policy perspective but often ignored in the literature is the impact of targeted advertising on consumer welfare.==== It is complicated as consumer surplus depends not only on the price consumers pay but also on what product they buy and whether they are informed of the product in the first place. Johnson (2013) analyzes the implications of firms’ increasing ability in ad targeting and finds that, while firms generally benefit from improved targeting, consumers may not.==== In his model, the pricing decisions of the firms are ignored. Arnold et al. (2022) consider the firms’ decisions in both advertising and pricing and find generally ambiguous welfare results. They focus on mixed-strategy equilibria due to the nonexistence of pure-strategy equilibrium. With endogenous product quality, which too affects consumer welfare, we provide a comprehensive analysis of the implications of targeted advertising under strategic interactions.====Our paper also adds to the large body of literature on R&D incentives in oligopoly markets.==== Market structure, such as price or quantity competition with homogeneous or differentiated products, significantly influences a firm’s incentive to invest in R&D which in turn affects firm profits. It has been long recognized that the joint optimization of product quality, advertising and price is “of greater practical interest” to a firm (Dorfman and Steiner, 1954), and the importance of both R&D and advertising expenditures on firm performance has been well documented in empirical studies (e.g., Erickson and Jacobson, 1992, Chauvin and Hirschey, 1993). This paper combines the two, along with the pricing decisions of the firms, in a game-theoretic framework.====The rest of the paper is organized as follows. In Section 2, we set up the duopoly model with product, price and promotion choices. We solve the model in Section 3 under the traditional means of mass advertising. In Section 4, the case of targeted advertising is analyzed and the equilibrium outcomes are compared with the case of mass advertising. Section 5 concludes the paper. All proofs are presented in the Appendices.",Targeted advertising with R&D rivalry,https://www.sciencedirect.com/science/article/pii/S0165489623000380,5 May 2023,2023,Research Article,2.0
Jacobs Arthur,"Department of Economics, Ghent University, Ghent, Belgium","Received 20 September 2022, Revised 23 March 2023, Accepted 24 March 2023, Available online 7 April 2023, Version of Record 14 April 2023.",https://doi.org/10.1016/j.mathsocsci.2023.03.007,Cited by (0),"I explore the effects of capital-augmenting technical change (CATC) in a task-based production setting where untapped automation opportunities exist. I contribute to the literature by showing analytically that CATC is a convenient modeling approach to automation, of which the labor market implications match the empirical literature. In my setting, CATC lowers the labor share of income even in the face of strong capital–labor complementarity. The intuitive explanation for this result is that the standard effect of CATC is more than fully offset by a contraction in the set of non-automated tasks, executed by labor. Furthermore, I show that CATC increases the wage rate unambiguously.","Many authors have used capital-augmenting technical change (CATC) as a core modeling instrument to automation in contexts where the share parameters of the production factors are fixed (e.g., Basso and Jimeno, 2021, Nordhaus, 2015, Sachs and Kotlikoff, 2012, Stähler, 2021). However, it has been noted that the implications of this approach are not consistent with the empirical findings regarding the impact of automation on the labor share (Acemoglu and Restrepo, 2018a). Empirical studies on the labor market impact of robotics find that the induced rise in labor productivity outstrips the induced rise in wages such that the labor share of income falls as a result of robot adoption (Acemoglu et al., 2020, Acemoglu and Restrepo, 2020, Dauth et al., 2017, Graetz and Michaels, 2018). Besides the evidence based on robotics data, there are convincing indications that automation technologies as a whole have strongly contributed to the lowering of the labor share (Autor and Salomons, 2018, Bergholt et al., 2022, Eden and Gaggl, 2018, Guimarães and Gil, 2022). In standard production settings, however, CATC will only decrease the labor share of income if the elasticity of substitution between capital and labor is larger than one. This assumption is untenable given that the bulk of empirical estimates point to an elasticity of substitution lower than one (e.g., Chirinko, 2008, Gechert et al., 2022, Knoblach et al., 2020, Oberfield and Raval, 2021). In a simple set-up with fixed share parameters for capital and labor, CATC is thus not an adequate modeling approach to automation since it cannot simultaneously acknowledge that automation lowers the labor share and that capital and labor are gross complements in the aggregate production function.====This inconsistency in the CATC approach to automation can be remedied somewhat by distinguishing between traditional capital and automation capital and only regarding the latter as a gross substitute for labor. This method has been used by Basso and Jimeno (2021), Cords and Prettner (2022) and Jacobs and Heylen (2021) for instance. In these frameworks, however, the effect of the automation process on the distribution of income across production factors is still governed by the elasticity of substitution between labor and (one type of) capital. The task-based approach put forward by Acemoglu and Restrepo, 2018a, Acemoglu and Restrepo, 2018b represents a more fundamental alternative to the CATC approach, since automation in these models takes the form of a contraction in the set of tasks which are executed by labor. From the aggregate production function point of view, this represents a fall in the share parameter of the labor input. This modeling approach guarantees that automation lowers the labor share of income, in accordance with the empirical literature.====Crucial for this study is that two distinct situations can arise in this task-based approach to automation. In the first situation, firms are technologically constrained in their choice whether to produce a task using capital or labor. In this scenario, all “automation opportunities” have been fully used by firms in the sense that capital — and not labor — is used for the production of any task for which capital and labor are perfect substitutes. In other words, all “technologically automated tasks” have also been “actually automated”. In the second situation, firms are not constrained by technology in their choice whether to produce a task using capital or labor. In this case, there exist tasks where capital can perfectly substitute for labor, but for which firms prefer to make use of labor since it is more cost-effective. As a result, firms can react to changes in the cost-effectiveness of capital vis-à-vis labor (due to CATC, for instance) by expanding or contracting the set of automated tasks. This second situation of “untapped automation opportunities” is my focus here.====In this work, I study the effects of CATC in a simple task-based model in the context where untapped automation opportunities exist. Under these conditions, the share parameters of capital and labor in the aggregate production function are no longer fixed, but they are a function of the relative factor cost. In this context, I show analytically that — under quite general conditions — CATC lowers the labor share of income, regardless of the value of the elasticity of substitution between capital and labor. Intuitively, this is because CATC will have two distinct labor share effects in this framework. First, the standard effect whose sign is entirely determined by the elasticity of substitution between capital and labor. Second, CATC will also lead firms to re-evaluate which tasks are more cost-effectively produced with capital than with labor and this leads to a contraction in the set of non-automated tasks. This second, so-called displacement effect always lowers the labor share and I show that this effect dominates regardless of the value of the elasticity of substitution. From this perspective, I argue that CATC in the presence of untapped automation opportunities is a convenient modeling approach to automation with credible empirical implications for the labor share. Furthermore, I show that the effect of CATC on the wage rate is also twofold in this setting. First, CATC raises the effective execution of already automated tasks and this increases the demand for the execution of non-automated tasks (and thus for labor). Second, it leads to a contraction in the set of non-automated tasks and this displacement effect lowers labor demand. I show that the productivity effect dominates irrespective of the parameterization such that CATC always increases the wage rate. This is fully in accordance with the recent empirical findings of Gregory et al. (2022) who find that the negative displacement effect of routine-replacing technical change is more than fully compensated by countervailing mechanisms stemming from productivity gains. To the best of my knowledge, this is the first time that the effects of CATC on the labor share and the wage rate are studied in this setting.==== The paper to which mine is most closely related is that of Martinez (2021).====I argue that the context of untapped automation opportunities is a highly relevant one. First, note that it is somewhat uneconomic to assume that production firms face a hard constraint and cannot respond to the rising effectiveness of existing automation technologies by further automating the production process. Of course, imposing that firms are technologically constrained in terms of the set of automated tasks only implies that there can be no endogenous response at the level of the production firm which makes use of the automation technology. There can still be a reaction to changing factor costs or productivities at the level of the development of new automation technologies. Acemoglu and Restrepo (2018b) proceed in this way and thus make the technological constraint to automation itself endogenous. However, there are reasons to suppose that it is not only the development of new automation technologies that can respond to changing factor costs or productivities, but also the degree to which firms make use of existing automation technologies.====First, there is the strong positive correlation between the labor cost and robot density across countries, both worldwide and for Europe specifically (Cséfalvay, 2020). Differences in the automation technology frontier are not a good explanation for these cross-country differences, since robotics technology is commercially available everywhere in Europe. This cross-country variation is more readily explained by differences in the adoption of existing automation technologies prompted by the differences in labor costs and thus the cost-effectiveness of these technologies. At least with respect to robotics, this suggests that the situation in which production firms have untapped automation opportunities — which they can pursue as the cost effectiveness of automation technology relative to labor improves — is quite commonplace. Second, there is some casuistic evidence that insufficient cost-effectiveness in particular is an important reason for firms not to automate. Based on 26 interviews with managers (and government and union representatives) in the South African apparel industry, Parschau and Hauge (2020) find that an insufficiently strong business case for the investment is a crucial reason for why automation technology is not used in more instances.====The remainder of this paper is structured as follows. In the second section, I set out the model. In the third section, I analytically derive the effects of CATC on the labor share. In the fourth section, I derive the effects of CATC on the wage rate. The fifth section concludes.",Capital-augmenting technical change in the context of untapped automation opportunities,https://www.sciencedirect.com/science/article/pii/S0165489623000355,7 April 2023,2023,Research Article,3.0
Ohtake Kensuke,"Center for General Education, Shinshu University, Matsumoto, Nagano, 390-8621, Japan","Received 30 November 2022, Revised 15 February 2023, Accepted 2 April 2023, Available online 5 April 2023, Version of Record 13 April 2023.",https://doi.org/10.1016/j.mathsocsci.2023.04.001,Cited by (0),"Two spatial equilibria, agglomeration, and dispersion, in a continuous space core–periphery model, are examined to discuss which equilibrium is socially preferred. It is shown that when transport cost is lower than a critical value, the agglomeration equilibrium is preferable in the sense of the Scitovszky criterion, while when the transport cost is above the critical value, the two equilibria cannot be ordered in the sense of the Scitovszky criterion.","Charlot et al. (2006) discuss the welfare economic properties of agglomeration and dispersion equilibrium in the two-regional core–periphery model,==== which has been introduced by Krugman (1991).==== One of their main results is that agglomeration can be preferred over dispersion in the sense of the Scitovszky criterion (Scitovszky, 1941) if the transport cost is sufficiently low.====In this paper, we apply the method devised by Charlot et al. (2006) to Krugman’s core–periphery model, which is characterized in particular by the CES function and iceberg transport cost,==== in a continuous periodic space==== and compare the agglomeration and dispersion equilibrium. From the perspective of self-organization, the periodic space assumption is made to focus purely on the self-organizing mechanisms inherent in economy and to eliminate spatial asymmetries caused by some boundary conditions as stated by Krugman (1996, pp. 22–23). In fact, there are many theoretical studies of the core–periphery model in some periodic or symmetric space such as Krugman (1993), Mossay (2003), Anas (2004), Tabuchi and Thisse (2011), Castro et al. (2012), Akamatsu et al. (2012), Ikeda et al. (2012), Ikeda et al. (2014), and Gaspar et al. (2018). Our study is then an attempt to discuss the welfare property, which is free from any specific spacial asymmetries, of the self-organizing mechanism. We prove that even in a periodic continuous space, also similar to Charlot et al. (2006), agglomeration is preferred in the sense of the Scitovszky criterion under sufficiently low transport cost.====Let us describe the model setup we use. The geographical space is assumed to be a circle ==== of which the radius is ====. The industry consists of two sectors: manufacturing under the monopolistic competition and agriculture under the perfect competition. The manufacturing produces various differentiated goods with the increasing returns, while the agriculture produces one variety of homogeneous good with the constant returns. The supply of each variety of the manufacturing goods is normalized to be ==== as in Fujita et al. (1999, Chapter 4, (4.33)). There are two different types of workers for each sector. Manufacturing workers whose total population is ==== are assumed to move around ==== in search of higher real wages, while agricultural workers whose total population is ==== are assumed to be unable to move. The transportation of the manufacturing goods incurs the iceberg transport cost. That is, to deliver one unit of any one variety of the manufacturing goods, ==== units of it must be shipped from ==== to ====. Meanwhile, the transportation of the agricultural good does not incur any transport cost.====The model which we use is described as follows.====
 ====with an initial condition ====. For any function ==== on ====, we denote the integration of ==== over any subset ==== by ====, where ==== is a line element.==== The functions ==== and ==== represent the income, manufacturing nominal wage, manufacturing price index, and manufacturing real wage at time ==== in region ====, respectively. Each of the nominal wage of the agricultural workers and the price of the agricultural good is assumed to be one. The manufacturing population density is given by ==== at ==== in ====, and the function ==== must satisfy ====. Similarly, the agricultural population density is given by ==== in ====, and the function ==== must satisfy ====. The iceberg transportation is assumed by ====, where ====. Here, ==== denotes the shorter distance between ====. The parameter ==== denotes the consumer’s preference for manufacturing variety; the closer ==== is to one, the stronger the degree to which consumers prefer diversity of the manufacturing goods. Since the parameters ==== and ==== often appear in the form ====, it is convenient to introduce ====in the following.====Any ==== corresponds one-to-one to a certain ==== as ====, so it is convenient for specific calculations to use ==== as the coordinates put into ====. Then, when ==== and ====, the shorter distance ==== between them is computed by ==== where ==== denotes the absolute value.",Agglomeration and welfare of the Krugman model in a continuous space,https://www.sciencedirect.com/science/article/pii/S0165489623000367,5 April 2023,2023,Research Article,4.0
Dehez Pierre,"Center for Operations Research and Econometrics (CORE-LIDAM), University of Louvain, Voie du Roman Pays 34, 1348 Louvain-la-Neuve, Belgium","Received 10 August 2022, Revised 20 March 2023, Accepted 23 March 2023, Available online 29 March 2023, Version of Record 4 April 2023.",https://doi.org/10.1016/j.mathsocsci.2023.03.006,Cited by (0),.,"We consider situations where different actors have a common target to reach, knowing the probabilities with which actors will succeed in attaining the target. The question concerns the determination of the share of each actor in the collective probability of success defined as the probability that at least one actor attains the target. The resulting shares can then eventually be used for allocative purposes. This could be applied to various domains: financial, legal, military, research... Budget allocation for a missile interception system or for vaccine research units are two examples. Another example is the division of damage in an uncertain environment.==== And actors could also be factors, like for instance the pathologies that lead particular diseases or the circumstances that lead to a plane crash.====Hou et al. (2018) have translated this problem into a transferable utility game assuming that a coalition succeeds if at least one of its members reaches the target. They then apply the Shapley value to allocate the collective probability of success among the players. They propose an axiomatization of the Shapley value which turns out to be incorrect.====The probability game introduced by Hou et al. (2018) is concave and its Shapley value allocates to a player a share in the collective probability of success that is ==== to his individual probability of success. Furthermore, the coefficient of proportionality is anonymous, being defined by a player-independent and symmetric function. We show that, together with symmetry (equal treatment of equals), proportionality characterizes the Shapley value on the class of probability games.====We consider other scenarios. Assuming that a coalition is successful if at least one of its members succeeds while the outside players ==== fail, the associated game is dual to the probability game as defined by Hou et al. This game is then convex and, the Shapley value being a self-dual allocation rule, it leads to the same allocation. Assuming that a coalition succeeds if ==== the member with the highest probability of success acts and reaches the target, the associated game is an ====. It is a concave game whose Shapley value is well known and for which proper axiomatizations exist. We do not retain the game that results from assuming that a coalition succeeds if ==== its members succeed. The associated game is subadditive but its Shapley value may contain negative components.====Probability games have a connection with the existing literature on sharing the revenue that results from joint actions aiming at increasing the collective probability of success. For instance, Pickard et al. (2011) report on an experience of social mobilization where large groups of agents are recruited in order to contribute to the completion of a particular task in a minimum time. Hougaard et al. (2022) study models of evolving hierarchies where agents invest in recruiting additional agents in order to realize a particular task, like for instance the social mobilization just mentioned, mining in blockchains (Shrijvers et al., 2017), multilevel marketing (Emek et al., 2011) or prediction markets (Arrow et al., 2008).====The paper is organized as follows. Probability games and their duals are introduced in Section 2. Probability games have a nonempty core. It is the object of Section 3. Harsanyi dividends associated with probability games are defined and used in Section 4 to construct the Shapley value. Its axiomatization is the object of Section 5. Section 6 is devoted to the remaining scenarios and the last section offers concluding remarks.",Sharing a collective probability of success,https://www.sciencedirect.com/science/article/pii/S0165489623000343,29 March 2023,2023,Research Article,5.0
"Diss Mostapha,Dougherty Keith,Heckelman Jac C.","Université de Franche-Comté, CRESE, F-25000 Besançon, France,University Mohamed VI Polytechnic, Africa Institute for Research in Economics and Social Sciences (AIRESS), Rabat, Morocco,University of Georgia, USA,Wake Forest University, USA","Received 13 September 2022, Revised 26 February 2023, Accepted 20 March 2023, Available online 27 March 2023, Version of Record 8 April 2023.",https://doi.org/10.1016/j.mathsocsci.2023.03.004,Cited by (0),"We use Ehrhart polynomials to estimate the likelihood of each three-candidate social ranking produced by pairwise majority rule assuming an even number of voters and the Impartial Anonymous Culture condition. We then calculate the ==== winner. Finally, we determine the weak ==== efficiency of various voting rules. We prove that Baldwin, Nanson, Copeland, and ranked pairs are weak Condorcet efficient, as is Borda unless there are no ties. Simulations show that among the rest, Dowdall is typically the most efficient rule for small voter groups and anti-plurality the least.","In developing his landmark theory on social welfare functions, Arrow (1963) placed great importance on the concept of rationality. He assumed individuals would be rational in their personal preferences, which required, among other things, that each voter’s personal ranking of the candidates be transitive. Arrow’s analysis was motivated by the potential for pairwise majority rule (PMR) to generate intransitive social rankings, a possibility first presented by Condorcet (1785). Namely, although a majority may prefer ==== to ====, and ==== to ====, it is still possible for a majority to prefer ==== to ====. Thus, a society may not appear rational despite everyone in society being individually rational.====The benefits to transitivity are two-fold. First, transitivity presents a natural way to order all the candidates under consideration which guarantees there will be at least one top-ranked candidate. Second, a transitive social ranking avoids a cycle in social preferences which can result in paradoxical rankings in which one candidate is ranked both above and below another candidate. In this paper, we equate Arrovian rationality with transitivity of the social ranking (Stearns, 1995). By definition, transitivity cannot be violated unless there are at least three candidates to be ranked. We limit our attention to three-candidate elections as is often done in the social choice literature (e.g., Tideman and Plassmann, 2014, Gehrlein et al., 2016, Miller, 2017, Dougherty and Heckelman, 2020).====When there is an odd number of individuals in the group, and all individuals register preferences on every pairwise vote (i.e., no indifference or abstention), ties are not possible. As such, if there are only three candidates and an odd number of voters, Arrovian rationality implies the presence of a strong Condorcet winner (sCW) and strong Condorcet loser (sCL), where a sCW (sCL) is a candidate that wins (loses) all of its pairwise comparisons under majority rule. When there are an even number of voters, however, ties are possible under PMR and there can be a sCW without a sCL, and vice versa.====For the three-candidate case and an odd number of voters, Gehrlein and Fishburn (1976) derive a closed-form expression for the probability that a sCW exists when each combination of potential collective voter preferences are equally likely. This preference distribution has since been come to be called the Impartial Anonymous Culture (IAC) condition====
 Gehrlein (2002) summarizes a derivation originally presented by Lepelley (1989) to determine the probability of a sCW for even-sized groups using the IAC condition. Yet for an even number of voters, transitivity can still occur in the absence of either a sCW or sCL. Lepelley’s formula correctly determines the probability of a sCW, or the existence of a ==== best candidate under PMR. Yet it underestimates the probability for PMR to be transitive and therefore overestimates the probability of cycles.====As we show below, transitivity only implies the existence of a weak Condorcet winner (wCW) who never loses any pairwise votes but does not necessarily win each of them.====
 Gehrlein, 2002, Gehrlein, 2006 builds on the earlier studies by using algebraic reduction techniques to derive probabilities for the existence of (at least one) wCW.====The algebraic derivations by Gehrlein and Fishburn (1976), Lepelley (1989) and Gehrlein, 2002, Gehrlein, 2006 require multiplying four or five summations together which is inelegant and cumbersome, and require separate proofs for odd and even numbers of voters. More recently, Ehrhart polynomials have been used to confirm the same probabilities for the existence of a sCW assuming IAC holds. As far as we know, no one has yet used this method to deduce the probability of transitivity, or the probability of the existence of at least one wCW. However, wCWs can also exist in the absence of transitivity, so we determine these probabilities separately. Because transitivity is the only one of Arrow’s (1963) conditions that PMR can violate, determining the probability of a transitive social ranking indicates the probability that PMR will successfully adhere to all of Arrow’s criteria (Dougherty and Heckelman, 2020).====A voting rule is weak Condorcet consistent (wCC) if the existence of one or more wCWs always results in the rule selecting a wCW, and a non-wCW is selected only when there are no wCWs to select. Subsequently, a rule’s weak Condorcet efficiency (wCE) reflects the proportion of times it selects only wCWs when at least one exists.==== Clearly, a wCC rule is perfectly wCE.====When ties are not considered, for example when there are an odd number of voters or the group size is quite large, there can be (at most) only one wCW, and the wCW is also the sCW. Thus, wCC is tantamount to selecting the sCW when one exists. In this case, the wCW is unambiguously better than the other candidates it defeats, according to the Condorcet criterion. Yet when ties are possible, there can be more than one wCW. If the pairwise social ranking is transitive but does not have a sCW, then a wCW must tie another candidate who is also a wCW and both of these candidates either defeat or tie the remaining candidate. Either way, the wCWs tie against each other and perform the same against a common opponent. Thus for transitive social rankings, majoritarian principles indicate a wCW is better than any non-wCW but no better than any other wCW. Yet that conclusion does not extend to social rankings which are intransitive. In this case, a unique wCW is ==== a sCW, and must tie another candidate that is defeated by the candidate the wCW defeats. Thus, the wCW does not unambiguously perform better than, or even at least as well as, both of the other candidates because the wCW only ties the one candidate that the other defeats. We also note that when an intransitive social ranking includes two wCW (which therefore must tie each other), one wCW ==== unambiguously better than the other in the sense that the intransitivity arises only if one wCW is able to defeat a candidate the other wCW only ties. Thus, when society fails Arrovian rationality, wCC is not necessarily a beneficial trait for a voting rule to possess. As such, we define our measure of wCE as the proportion of times the winner(s) come(s) exclusively from the set of wCW when the social ranking is transitive. As in the case without ties, all rankings involving intransitive cycles are ignored. Because wCW can differ from sCW only when ties occur, and the Condorcet efficiency of many rules has already been heavily studied for cases without ties, we focus on the study of wCE of ten distinct rules in small even-sized groups of up to 20 voters so that ties are unsurprising.====The paper is organized as follows. In Section 2 we describe the basic framework. We also determine in three-candidate elections the probability of a transitive social ranking and the probability for the existence of at least one wCW, separately. In Section 3 we determine the wCE of ten distinct voting rules computationally. The final section presents our conclusions.",When ties are possible: Weak Condorcet winners and Arrovian rationality,https://www.sciencedirect.com/science/article/pii/S016548962300032X,27 March 2023,2023,Research Article,6.0
Pradelski Bary S.R.,"Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, France","Received 2 February 2022, Revised 28 December 2022, Accepted 14 March 2023, Available online 21 March 2023, Version of Record 22 March 2023.",https://doi.org/10.1016/j.mathsocsci.2023.03.003,Cited by (0),". We introduce a new heuristic of social influence where agents are influenced by the ==== generates qualitatively different dynamics than the classic model of social influence through ====. Moreover, the processes leave characteristic footprints; ==== favors more extreme outcomes than ====.","A fundamental question about aggregate behavior of groups is how transitions between seemingly stable states occur almost instantaneously after long lags of little change. Social influence has been found to play an important role in such transitions. This behavioral model describes the process in which individuals have a preference for conformity and are influenced by the actions of others in a group, where the strength of this influence varies across agents. Examples include voting, fashions and fads, stock market rallies and opinion extremization and polarization. Consider voting in a two-party system, where individuals receive representative polls prior to the final election and are thus influenced in their final decision by the proportion of voters that vote for one or the other party, that is, the ====. The ==== heuristic has been extensively studied. However, it fails to capture social influence that occurs from the repeated choice of others, rather than solely from a single act of adoption. This is the theme of this paper, which introduces and studies the novel ==== heuristic of social influence.====Social influence through ==== naturally occurs in many environments. Consider, for example, the proportion of smartphone owners using instant messaging software, e.g., WhatsApp versus Telegram, to communicate with each other. An agent’s decision which application to use is likely influenced by its repeated use to interact with others, rather than the fact that other agents have installed the application on their device. Hence an agent considers the historical proportion of times he was contacted through WhatsApp versus Telegram, that is, the ====. Note that the differentiation between social influence via ==== and ==== has not gone unnoticed in marketing departments. Hardware providers such as Apple focus their reporting and marketing effort on the number of products sold (====). By contrast, software providers, such as WhatsApp or Telegram, focus their analysis and messaging on the frequency of usage of their products (====).====The contribution of this paper is to introduce the novel heuristic of social influence via ==== and identify the equilibria of the process and study their stability in a stochastic environment. Note that the tools and results for the ==== model are known and are included for completeness, the novel contributions are with regard to the ==== model. We find that the long-run behavior of these seemingly similar processes differ, and we elaborate on the differences. We show that each process leaves a characteristic footprint – ==== favors more extreme outcomes than ====. Thus, the current limitation in the literature to not differentiate between the two heuristics (dependent on the application at hand) is not only of formal interest but results in substantially different predictions and possible policy implications. Finally, we show how to discriminate empirically between ==== and ====, allowing empiricists and practitioners to evaluate which process is at work.",Social influence: The ,https://www.sciencedirect.com/science/article/pii/S0165489623000318,21 March 2023,2023,Research Article,7.0
Pham Ngoc-Sang,"EM Normandie Business School, Métis Lab, France","Received 5 June 2022, Revised 18 February 2023, Accepted 6 March 2023, Available online 11 March 2023, Version of Record 20 March 2023.",https://doi.org/10.1016/j.mathsocsci.2023.03.002,Cited by (0), productive government spending may prevent economic recession and promote economic growth. We also investigate the issue of optimal dividend taxation and the role of dividend taxation on the asset price bubble.,"The interplay between the financial market and the production sector is an important issue in economics. On the one hand, the financial market having financial frictions may amplify the macroeconomic impacts of exogenous changes (Kiyotaki and Moore, 1997), and, in some cases, it is considered as a source of economic recession. On the other hand, Le Van and Pham (2016) point out that the key factor to prevent recessions is the productivity of the production sector and that financial assets may be beneficial to the productive sector by providing financial support for the purchase of physical capital.====Note that the productivity of firms in the above papers is exogenous and they did not investigate the role of taxation. While there is an extensive literature on capital and labor income taxation,==== few papers focus on the impacts of dividend taxation on economic growth and asset price bubble. Motivated by these observations, we propose a policy to enhance the firms’ productivity: the government sets a tax on financial asset dividends and then uses the tax revenue to finance productive government spending which in turn improves the productivity of firms.==== We then investigate the impacts of this policy on the economic growth and asset price bubble as well as the issue of optimal dividend taxation.====To address these questions, we construct an infinite-horizon general equilibrium model with heterogeneous consumers, a firm, and a government. In this economy, there are a long-lived asset and a good (which can be consumed or/and used to produce). If consumers buy the long-lived asset, they may resell it after receiving exogenous dividends (in term of consumption good). This asset is similar to the Lucas tree (Lucas, 1978) or security (Santos and Woodford, 1997) or stock (Kocherlakota, 1992). We will call it the ====. The government taxes the dividends on the financial asset and then spends the tax revenue on financing productive projects (for example, public infrastructure) that improve the firm’s productivity. This kind of endogenous growth is in the spirit of Barro (1990). The representative firm maximizes its profit by choosing its capital demand. Consumers maximize their intertemporal utility by choosing their allocation of consumption, capital stock and financial asset holding. They can also borrow by selling financial asset but there is a borrowing constraint: the repayment cannot exceed a fraction of their (physical) capital income.====Our contribution is three-fold. First, we explore the role of productive government spending (which is financed by dividend taxation) on economic recessions and growth. We say that an economic recession appears if the aggregate capital stock used for production falls below some critical level. We prove that recessions occur at infinitely many dates if the firm’s productivity is too low. However, when the government employs the above policy, the productivity of firms is improved. By consequence, economic recessions can be prevented and we may have economic growth in the long run. This happens if the governance quality is good and the size of dividend is high. By contrast, when these conditions are violated (for example, when the tax revenue were used for wasteful government spending), the economy cannot escape from recession.====These findings contribute to the endogenous growth theory. The added-value is that our results are obtained in a general equilibrium model with heterogeneous agents and borrowing constraints, which raises technical difficulties that methods in the standard optimal growth theory (Le Van and Dana, 2003, Acemoglu, 2009) are no longer applicable. It should be noticed that our results hold for any equilibrium, including recursive ones. Although some authors (Acemoglu and Jensen, 2015, Datta et al., 2018) study comparative statics of recursive equilibria, intertemporal equilibria in our paper maybe not recursive, and therefore their methods cannot be directly applied in our framework.====Our second contribution concerns the optimal dividend taxation. This question matters because when the government increases the tax rate (====) on dividends, the net dividends decrease but the production level may increase. Hence, the total amount of good may decrease or increase. So, it would be important to study the optimal dividend taxation to grasp this trade-off. In this respect, we assume that the government maximizes the total consumption of households at the steady state by choosing the tax rate. If the productivity of firms or the effect of the productive government spending are high, the government should choose the highest feasible tax rate on dividends. By contrast, if these factors are low, the government should apply the lowest tax rate. In the intermediate case for productivity, the size of dividend and the effect of the productive government spending, the optimal level of dividend taxation can be explicitly computed as a function of these three factors. We show that the optimal dividend tax rate is increasing in the governance quality and the firm’s productivity, but decreasing in the size of dividend.====This result is closely related to Barro and Sala-i Martin (1995) (Section 4.4.1) where they also study the interplay between productive government services and economic growth. They assume the production function of firms depend on the productive government services (which are equal to a proportion of the output). While this spending is financed by the dividend taxation in our paper, it is financed by a lump-sum tax in Barro and Sala-i-Martin. Another difference is that Barro and Sala-i-Martin focus on the case where the government maximizes the rate of growth of the balanced growth path. With specific setups,==== they find that the optimal rate of tax equals the output elasticity of the productive government services. By contrast, the optimal dividend tax in our paper depends on several factors and we can provide comparative statics as mentioned above.====Our third contribution is about the impact of dividend taxation on asset prices and bubbles. Following Santos and Woodford (1997), we say that an asset price bubble arises if, at equilibrium the fundamental value (i.e., the sum of discounted values) of asset dividends (after-tax) exceeds the asset’s equilibrium price. Although there is a large literature on the non-existence of rational bubble in general equilibrium models,==== few examples of bubbles of assets having positive dividends have been provided. We present a model where there may be a continuum of equilibria with bubble. Asset price bubbles may exist if endowments of agents fluctuate over time. Indeed, with such a fluctuation, at any date there is at least one agent who needs to buy asset (even when the asset price exceeds the fundamental value) because this agent has to transfer her wealth from this date to the next date (this is the only way she can smooth consumption because she is prevented from borrowing).==== Our paper is different from Le Van and Pham (2016) because the asset’s fundamental value in our model is not monotonic in dividends while, in Le Van and Pham (2016), it is monotonic. The reason is that we introduce a dividend taxation which makes the real returns and discount factors in our example depend on dividends through productive government investment. Interestingly, we show that asset bubbles are more likely to arise when dividend taxes increase. The intuition is that if such taxes increase, then the after-tax dividends decrease, making the asset’s fundamental value decrease and lower than the asset price.====The paper is organized as follows. Section 2 presents the model and provides some basic equilibrium properties. Section 3 investigates the role of dividend taxation on recessions and economic growth. Section 4 studies the optimal dividend taxation. Section 5 considers the role of dividend taxation on asset bubbles. Section 6 concludes. Formal proofs are gathered in Appendix.",Intertemporal equilibrium with physical capital and financial asset: Role of dividend taxation,https://www.sciencedirect.com/science/article/pii/S0165489623000240,11 March 2023,2023,Research Article,8.0
"Gaumont Damien,Badra Yassine,Kamburova Detelina","Université Paris-Panthéon-Assas, CRED, Paris, France, and TEPP,Institute of Mathematics and Informatics, Bulgarian Academy of Sciences, Bulgaria","Received 15 June 2022, Revised 17 February 2023, Accepted 20 February 2023, Available online 5 March 2023, Version of Record 13 April 2023.",https://doi.org/10.1016/j.mathsocsci.2023.02.005,Cited by (0),"This article specifies the conditions on the utility function and on the cost function for which congestion or production overcapacity emerge in a market when information is perfect and prices are flexible. Consumers are of two types: Some appreciate a large production market, some do not. A typology underlines the shape of utility functions and cost functions which generate an optimal production overcapacity or an optimal congestion at the pure strategy ","In this theoretical paper on network goods, we demonstrate, on the one hand, that positive consumption externalities do not necessarily lead to market congestion. On the other hand, negative consumption externalities do not necessarily entail market production overcapacity. Therefore, the shapes of both utility and cost functions are central in our theory.====In this paper, we consider consumers who are sensitive to the market size. As usual in economic literature, consumption externalities are considered to be positive or negative if for instance the consumer’s marginal utility increases or decreases with the market size. We show that a given consumer may buy more on large network markets in pure strategy Nash equilibrium, while he may buy more on small network markets at the welfare equilibrium. We point out that when the market size varies, their utility also varies, but not necessarily in the same direction. In that sense the consumer’s type is endogenous. When considering every impact of positive or negative consumption externalities, four possible cases may arise.====We depart from the literature by making no assumptions about the sign of the consumption externalities which are arbitrary functions from the modeling point of view. A negative (positive) consumption externality thus corresponds to a decreasing (increasing) marginal utility with respect to the demanded quantity which is a decreasing (an increasing) function of the market size. In other words, this means that the inverse demand is a decreasing (increasing) function of the market size. In this setting, it turns out that the sign of the externality is endogenous. We point out that it is relevant to consider its sign after calculating the pure strategy Nash equilibrium. Indeed, this sign can change depending on the neighborhood in which the equilibrium is reached.====In the remaining of the paper, we say that consumption externalities are positive or negative in equilibrium if the marginal utility function increases or decreases in the neighborhood of the equilibrium. Depending on the shape of the ====-concave utility function, either the market is balanced or not.====The additive separable shape of the utility function chosen by Katz and Shapiro (1985) helps understand why in equilibrium consumers’ expectations are fulfilled. Similarly, in Amir and Lazzati (2011), the existence of a symmetric equilibrium is due to some properties of the inverse demand function which are direct consequences of the shape of the consumer’s utility function. Moreover, Amir et al. (2021) investigate the viability problem with an argumentation in the direction of technological progress and exogenous entry of firms. They obtain the surprising result that ’monopoly leads to the highest prospects for viability’ p 1207.====We consider a monopoly and we study the conditions on the utility and the cost functions for which consumers’ expectations are fulfilled or not. As mentioned in Amir et al. (2021), “the comparison of consumer surplus requires a more restrictive condition on demand. ==== A theoretical investigation at hand appears needed as a way to gauge the well-foundedness of the conventional view, which was based on a number of observed cases and stylized facts but little formal analysis”. In the single network industry case, our paper explores the role of both the consumption externalities’ sign and the cost function’s shape on the optimal solutions of the market.====After the general model, we present an example with quadratic utility function and an interaction cost function for the monopoly. Depending on the values of the parameters, either the consumer’s expectations are fulfilled or not. Moreover, whatever the consumer’s type, it is shown that the social planner chooses the same network capacity assumption the market would have chosen, but provides the consumers with more consumption in the case where their expectations are fulfilled. Depending on the values of parameters, the social planner may choose either production overcapacity or congestion in the single network industry.====Extension in the direction of the emergence of an excess of supply of goods on non-network market is provided. Replacing the size of the network by the display of goods allows us to understand the emergence of permanent unsold stocks of goods. Traditional models explain temporary excess of supply by considering demand a random variable, (Prescott, 1975, Bryant, 1980, Lucas and Woodford, 1993, Eden, 1990 and  Dana, 1993), (Rey and Tirole, 1986, Deneckere et al., 1996, Khan and Thomas, 2007, Scarf, 1960, Schutte, 1984). However, a permanent unsold stocks of goods cannot be considered a random variable. For that reason, the literature introduces fixed prices under certainty to generate excess of supply, Kawasaki et al. (1983), Carlton (1986), Mathewson and Winter (1987),  Shaffer (1991). At the macroeconomic level, fixed prices result in excess of supply on the goods market and excess of supply on the labor market, resulting in short term unemployment, Benassy (1975), and Grandmont (1977). Another argument is unobserved consumer’s preferences, Lazear (1986). The main problem with those approaches is that in the long run it is hard to assume that prices will not be adjusted. Consequently, there is no convincing theory of permanent unsold stock of goods, i.e., a persistent gap between sales and warehouse inventories, as observed on many markets, Den Haan (2013) and Dudin, Nazarov and Yakupov (2015, p 264) (Dudin et al., 2016). Our general setting provides a theoretical framework for explaining such observed facts without fixed price or random variables.====The paper is organized as follows. Section 2 is devoted to the model. Section 3 presents the solution of the model. Section 4 exposes an example applied to Network goods. Section 5 discusses and extends the results before Section 6 concludes.","Market-dependent preferences, positive and negative network effects and welfare",https://www.sciencedirect.com/science/article/pii/S0165489623000185,5 March 2023,2023,Research Article,9.0
Fernandes Marcos R.,"University of São Paulo, Department of Economics, Brazil","Received 21 October 2021, Revised 22 February 2023, Accepted 23 February 2023, Available online 2 March 2023, Version of Record 17 March 2023.",https://doi.org/10.1016/j.mathsocsci.2023.02.007,Cited by (0)," of emergence of the less-biased consensus when people are connected and have different priors is difficult. Hence, I used simulations to analyze its determinants and found three main results: (i) some network topologies are more conducive to consensus efficiency, (ii) some degree of partisanship enhances consensus efficiency even under confirmation bias and (iii) open-mindedness (i.e. when partisans agree to exchange opinions with opposing partisans) might inhibit efficiency in some cases.","People form opinions on various economic, political, social, and health issues based on information from both the media and people they trust (e.g. friends, coworkers, family, experts, etc.). This information acquisition process usually occurs when the issue discussed has no clear-cut ==== or ==== distinction or when the available information cannot be easily understood. Consulting people’s opinions, in this case, is an appealing and easy way to gather information. For many people, social networks then become primary tool to stay informed. Thus, understanding how beliefs depend on how agents perceive and process information is vital. In this study, I examine how opinions are affected by confirmation bias in a networked environment.====In psychology, confirmation bias denotes the interpretation of evidence in ways consistent with existing beliefs (Nickerson, 1998, Molden and Higgins, 2008). This can be done in different ways, like restricting attention to favored hypothesis, disregarding evidence that could falsify the current worldview or overvaluing positive confirmatory instances. In all cases, people restrict attention to a single hypothesis and fail to carefully consider alternatives.====In social psychology, people interpret evidence when they are ambiguous (i.e. when evidence is conflicting). People may misinterpret scientists and experts after ambiguous announcements. Simonovic and Taber (2022) highlighted that when WHO declared the COVID-19 outbreak a global pandemic in 2020, experts did not precisely understand the extent and nature of the health risks or how disease transmission can be prevented. Hence, WHO provided ==== to the public on whether wearing a mask was necessary. Other medical authorities also provided conflicting recommendations to the public regarding medicines and vaccines’ efficacy. Conflicting evidences may have even contributed to people making their own assessment about the problem.====While friends may help people to aggregate information in some cases, in other cases, people may expose themselves to others who that rely on their own worldview to derive information from ambiguous evidence. In these cases, efficient aggregation of information is not guaranteed, and I investigate how opinions are influenced by people’s biases.====To analyze this phenomenon, I consider a society where agents are interested to learn the underlying state ====. For instance, the underlying state ==== might represent the efficacy of a new vaccine (e.g. from 0 to 1). All agents have prior beliefs about the vaccine’s efficacy and observe a sequence of public signals, one at each date ====. Public signals may be (i) informative or (ii) ambiguous. Informative signals are binary variables indicated as ==== if state on the right side of the 0–1 spectrum are more likely (i.e. if vaccine’s efficacy is high) and ==== if the states on the left side of the 0–1 spectrum are more likely (i.e. if vaccine’s efficacy is low). Hence, as signals realization does not convey full information on the underlying state, agents can only learn the true state (vaccine’s efficacy) asymptotically. This is in the spirit of ongoing learning, where information accumulates through experience. In the case of ambiguous signals, agents are allowed to interpret these signals using a fairly general randomization rule proposed by Fryer et al. (2019) that accounts for confirmation bias. Hence, the interpretation of the ambiguous signal received at time ==== is influenced, to a greater or lesser extent, by the likelihood of 0 and 1 at time ==== (see more details below). This captures situations wherein people feel impelled to explain ambiguous evidence about a particular issue.====As in Jadbabaie et al. (2012), besides learning from public signals, agents exchange information through a social network. At the beginning of every period ====, the public signal is realized. Thus, each agent first ==== signals (if ambiguous) using the randomization rule, ==== the signal and ==== the Bayesian posterior (opinion and precision). Every agent then sets their ==== opinions and precisions to be a linear combination of the Bayesian posterior opinions and precisions computed with the interpreted signal and opinions and precisions of friends (e.g., formal definition of neighbors in Section 3.1) they met in the period before. Social connectivity among agents remains fixed over time and strong connectivity is assumed (i.e., all agents are exposed to all other agents either through a directed or undirected path in the social networks).====Hence, despite the level of ambiguity and both in the case of a single individual or a connected society, only two types of opinions can emerge, and both are biased: left- and right-biased opinions. However, one type of opinion is less biased than the other depending on the underlying state. Less-biased opinion is only guaranteed to emerge under a favorable combination of sufficiently low ambiguity and sufficiently pronounced states. If this condition holds, I show that the less-biased opinion is attained with probability 1. Moreover, long-run learning is not attained even if people are impartial when they interpret ambiguous signals (i.e., when interpreting evidence uniformly at random instead of using their own opinions). Those results contrast with those by Rabin and Schrag (1999) and Fryer et al. (2019), who suggest that long-run learning occurs with a positive probability and that impartiality helps in learning the state. Furthermore, both the network effect presented here and signals realization, reinforce the interpreting dispute (====) as people may have their own interpretation biases reinforced or attenuated by other agents.====Finally, confirming the probability of emergence of the less-biased consensus analytically is difficult, and I use Monte Carlo simulations to show its determinants. The presence of partisan agents (i.e., agents with skewed initial priors) in societies suffering from confirmatory bias have two main effects. (i) When the degree of partisanship is low, partisanship helps to counter the realization of initial misleading signals (e.g., realization of a 0 when ====. Thus, low partisanship increases the odds of reaching less-biased consensus. (ii) When the degree of partisanship is high, partisans exacerbate misinterpretation of signals. Thus, high partisanship reduces the odds of reaching less-biased consensus. Moreover, I also show that open-mindedness of partisan agents (i.e., when partisans agree to exchange opinions with partisans with polar opposite beliefs) might reduce the odds of reaching less-biased consensus in some network structures.====While this work does not generalize theoretical results for other conjugate families and numerical results for other network structures, both methods and cases explored are sufficiently general to capture important aspects of real-world networks. In every period, public signals realized and observed by all agents may represent information reported by sources including media outlets and international organizations. The level of ambiguity of the informational content reported by them, measured by a parameter ====, represents the fraction of instances where a signal simultaneously conveys two conflicting meanings and agents feel impelled to interpret them. Parameters of the signal interpretation function dictate the interpretation behavior of every agent.====This work is structured as follows. Section 2 provides a brief literature review and highlights contributions. Section 3 describes a framework for updating beliefs when agents communicate over social networks with ambiguous signals and present main theoretical results. Section 4 describes a simulation exercise when priors heterogeneity (partisanship) is assumed. Section 5 concludes the study. Moreover, six appendices are available. Appendix A Beta-Bernoulli model and likelihood function of interpreted signals, Appendix B Beta distribution: Mode, Mean, Median contain primitives of the Beta-Bernoulli conjugate family employed in this work. Appendix C contains proofs of auxiliary results, while Appendix D presents proofs of main results. Appendix E Tests concerning differences among proportions, Appendix F Probit regression model - Robustness show simulation statistics and present regression robustness.",Confirmation bias in social networks,https://www.sciencedirect.com/science/article/pii/S0165489623000203,2 March 2023,2023,Research Article,10.0
"Chollete Lorán,de la Peña Victor,Klass Michael","Jack Welch College of Business and Technology, Department of Finance, 3135 Easton Turnpike, Fairfield, CT 06825, United States of America,Columbia University, Department of Statistics, 1255 Amsterdam Avenue, New York, NY 10025, United States of America,University of California, Berkeley, Department of Statistics, 367 Evans Hall, Berkeley, CA 94720, United States of America","Received 17 June 2020, Revised 27 February 2023, Accepted 27 February 2023, Available online 2 March 2023, Version of Record 17 March 2023.",https://doi.org/10.1016/j.mathsocsci.2023.02.008,Cited by (0),"How much does it cost a decisionmaker to base her payoff on interdependent, biased information sources? This question is relevant in economics, ","We analyze a statistical model where a decisionmaker faces multidimensional information sources, whose dependence is unknown or uncertain. The decisionmaker has to find ways to address the multivariate ambiguity that results. We offer two main contributions. First, we develop new inequalities that bound the payoffs from concave functions of the dependent variables. The inequalities provide a benchmark for the decisionmaker who is unaware of dependence. Unlike previous research, our inequalities have a bound that is universal (applies to all types of dependence), exact, and easy to calculate. Second, we apply this approach to risky asset investment, and to a voting model where we illustrate the impact of dependent signals on the probability of voting for Brexit.====An important question guiding our work is the following: How much does it cost a decisionmaker to base her payoff on interdependent, biased information sources? Statistically, this question may be posed as: what is the price of independence? The term ‘price of independence,’ denoted ====, signifies the maximal payoff that a decisionmaker could achieve using dependent signals, relative to independent signals. In Section 3 we show that ==== may be expressed as the sup over all possible joint distributions, once you know the marginals. Our approach assists the decisionmaker who worries about potentially overvaluing information from dependent signals. The main Theorem in Section 3 shows that given a concave payoff function, the maximal value is roughly 50% more than if the signals were independent.====Unknown dependence is relevant in many economic environments: among voters who share the same political affiliation; across assets during a financial downturn; and among citizens who have similar likelihood of pathogen exposure. In these environments, a researcher may need to know how much the interdependence costs her, relative to a case where the individuals are truly independent. First, if a pollster analyzes a voter survey while unaware that voters are from a similar political party, this induces unknown bias into the analysis. Second, if assets tend to face increased dependence during financial downturns, an investor that does not account for this will mistakenly believe her asset portfolio is diversified and can potentially lose large sums of money. This situation occurred during the subprime mortgage crisis of 2007. Third, as shown in the ==== pandemic, an epidemiologist interested in assessing the threat level in a certain locale would benefit by knowing the potential impact of dependence across individuals in that population.====The above considerations relate to both ambiguity and unawareness. In the preceding examples, ambiguity arises because the joint probability distribution is uncertain.==== The researcher understands that there may be dependence across individual citizens and asset classes, but is not able to quantify the dependence or probabilities. Such uncertain probabilities have been studied in decision settings by Schmeidler (1989) and Gilboa and Schmeidler (1989), among others. The preceding examples may also reflect unawareness, where the researcher has no conception that there could be hidden dependence. Situations of unawareness have been studied by Karni and Vierø (2013) and Schipper (2014), among others. In the current paper we will focus on ambiguity implications, although similar considerations can apply in settings of multidimensional unawareness.====The term dependence refers to comovement of random variables, and is used in several ways in economics and statistics. In Fig. 1 we illustrate dependence with Kendall’s tau, denoted ====, which measures the difference between concordance and discordance.==== We utilize ==== because it measures dependence between random variables in a way that is robust to rescaling, and unlike the linear Pearson correlation, does not confuse marginals with the dependence structure. Experimental evidence suggests that although individuals tend to estimate average dependence fairly well, they incorrectly estimate high or low dependence (Clemen et al., 2000). We therefore require a measure like ==== that can apply overall and in the tails of the distribution. Our main result does not depend on the particular measure of dependence that we utilize.====. We build on three areas of research: political economy, economic theory, and statistics. Regarding political economy, Berg (1994) studies the effect of voter dependence on collective decisional efficiency. Dittrich et al. (2006) develop a model to capture dependence in judges’ comparison of objects based on multidimensional attributes. Alonso-Villar and del Río (2010) study segregation measures that apply to local subgroups within a larger overall population, and demonstrate the connection between local and overall segregation measures. In recent years, several papers study the impact of correlations on decisionmaking. Levy and Razin (2015) examine the effect on policy outcomes when voters neglect correlations. They characterize conditions on the distribution of preferences under which induce higher vote shares for optimal policies and better information aggregation. Ortoleva and Snowberg (2015) explore the implications of overconfidence on voting behavior, where voters underestimate the correlation of their signals. The authors show that overconfidence can result in ideological extremeness and larger voter turnout. They find empirical support for their predictions on US electoral data. Ellis and Piccione (2017) illustrate that improper perception of correlations has a role in individual choice. The agents choose assets, and may misperceive the joint returns. Maximizing subjective expected utility using agents’ incorrect beliefs might lead to misvaluing of assets. Levy and Razin (2020) provide a framework for how sophisticated decision makers combine multiple sources of information to form predictions, under multivariate ambiguity. They characterize a tradeoff between correlation neglect and an information-based correlation bound, which affects choice behavior in nontrivial ways. This literature has made successful inroads in the study of correlation perception on choice and voting. The above research is generally couched in terms of linear correlation, which does not capture general dependence such as comovement at the distribution’s tails. This latter consideration would be relevant in studies of polarization and extreme partisanship. We add to this strand of research by incorporating a decoupling technique that provides the researcher with a benchmark for payoffs, even in the face of arbitrary dependence.====Regarding economic theory, our work is related to research on interdependence and unawareness. Meyer and Strulovici (2012) study how to order interdependence in economic settings. They analyze five orderings of interdependence. The authors assess when the orderings can be meaningfully ranked, depending on the dimension of the variables involved. We add to this research by providing tools based on decoupling, which apply regardless of the dependence structure. In research on unawareness, recent papers have provided insights on how decisionmakers deal with vague or unprecedented situations. Vierø (2012) studies the implications of contract design, when the relevant contract parties face environments that are difficult to describe in standard economic terms. Karni and Vierø (2013) and Karni and Vierø (2017) extend a standard Bayesian state space framework to accommodate decisionmaking under unawareness. Schipper (2014) uses a preference-based approach to characterize unawareness, and proves its equivalence to epistemic notions of unawareness. We add to this literature by discussing unawareness of multivariate dependence, and by providing a statistical tool, decoupling, which can help to ameliorate the effects of such unawareness.====Regarding the statistics literature, we build on two main papers. de la Peña (1990) analyzes situations where functions of nonnegative dependent random variables have to be compared to their conditionally independent counterparts. The author proves that for concave or convex functions, there exists a constant that bounds the relevant dependent quantity. In related work, Makarychev and Sviridenko (2018) analyze a similar problem in the context of optimization with diseconomies of scale. These latter authors develop the constant of de la Peña (1990), and show that it can be related to the moments of a Poisson distribution. Other literature on decoupling is summarized in the monograph by de la Peña and Giné (1999). This line of research is important in statistics, because it provides a means of relating complex multidimensional variables of arbitrary dependence, to variables that are closely related (with common marginals) but which are conditionally independent. The constant of de la Peña (1990) holds for any class of dependence, however the author does not provide a specific number. The bound of Makarychev and Sviridenko (2018) can be computed, however it is not constant, since it depends on another distribution. We add to this insightful literature, by providing the exact value of the universal bounding constant. Furthermore, unlike previous papers that focused on statistical and engineering settings, we apply the bound to risky asset investment and the political economy of voting.",The price of independence in a model with unknown dependence,https://www.sciencedirect.com/science/article/pii/S0165489623000215,2 March 2023,2023,Research Article,11.0
"Barros Fernando,Bertolai Jefferson,Carrijo Matheus","FEARP/USP, Brazil,LEMC-FEARP/USP, Brazil,DCM-FFCLRP/USP, Brazil","Received 20 March 2022, Revised 16 February 2023, Accepted 17 February 2023, Available online 26 February 2023, Version of Record 7 March 2023.",https://doi.org/10.1016/j.mathsocsci.2023.02.004,Cited by (0)," of a coordination game among the network’s nodes: the so called miners. Equilibrium analysis demands allowing miners to secretly update their accounting, i.e., to privately build multiple blocks of transactions and to deviate from the longest chain rule. We formalize such reasoning by proposing an ==== coordination game inspired on the ==== design. In particular, by proposing a model that explicitly tells apart mining costs related to energy consumption from those related to computational capacity, we are able to study how symmetric equilibrium existence depends on well known parameters, like the average time for updating accounting records and the rewards collected from mining (accounting) activities. It is shown that the (off-equilibrium) possibility of double spending makes the attractiveness of the equilibrium strategy a decreasing function of the average time for updating accounting records.","Decentralized accounting management is what differentiates cryptocurrencies from other forms of digital money, like demand deposits accessible through debit cards (Berentsen and Schär, 2018). From its very nature, the management of a cryptocurrency’s accounting system demands coordination among those responsible for updating it: the network nodes, usually also referred as the ====. The update process must be coordinated in order to avoid multiple versions for the state of accounting records, which would imply the network to split into multiple new networks (usually referred as ====) and, therefore, multiple new cryptocurrencies.====In addition to organizing ideas around the problem of accounting coordination, we further explore cryptocurrencies’ coordination solution by proposing an ==== coordination game that incorporates the main features of the Bitcoin design.==== In particular, two features related to the fact that miners are able to privately update their accounting records emerge as key ingredients: miners are able to both secretly build multiple blocks of transactions and refuse to immediately adopt a new proposed accounting state.====Delayed settlement of payments is another key feature implied by the Bitcoin’s design that our model takes into account. Payment settlement is usually not a relevant matter when traditional media of exchange are the payment instrument: transactions are almost instantaneously settled when buyer and seller use money or demand deposit as payment instruments. This is not true for exchanges mediated with cryptocurrencies based on Proof-of-Work protocols, like the Bitcoin. Because reliance on Bitcoin digital records increases with time, such transactions are usually associated with delayed delivery of goods and services as an strategy to protect seller from buyer ==== its currency.====In reality, secret mining and longest chain rule are central features of Bitcoin’s network. As discussed in Narayanan et al. (2016) and Antonopoulos (2017), longest chain rule is at the heart of blockchain consensus and double spending attacks require the ability to secretly mine an alternative version of the blockchain until the seller delivers the good or service. In this sense, it would be valuable to know how these two features shape equilibrium existence and double spending incentives by means of simple models of mining competition on Proof-of-Work based cryptocurrencies. Keeping the model’s simplicity is attractive in the sense that it allows for embedding the mining competition model in workhorse economic models, as Chiu and Koeppl (2019) successfully did.====Our model for the accounting coordination game provides a contribution towards this objective. In particular, by explicitly telling apart mining costs related to energy consumption from those related to computational capacity, we are able to study how symmetric equilibrium existence depends on well known parameters, like the average time for updating accounting records and the rewards collected from mining (accounting) activities. Also, equilibrium analysis shows that the (off-equilibrium) possibility of double spending makes the attractiveness of the equilibrium strategy a decreasing function of the average time for updating accounting records.====Our paper can be seen as a contribution to an emerging literature in the blockchain economics.====
 Cong and He (2019), for example, shows how blockchain based smart contracts can mitigate informational asymmetry and improve welfare and consumer surplus by enhancing entry and competition. Biais et al. (2019), in its turn, is closer to our work in the sense that equilibrium properties of a mining game are studied for a Proof-of-Work (PoW) based cryptocurrency. They establish that “mining blocks on the longest chain” composes a Markov perfect equilibrium. Also, they argue that the blockchain protocol is a coordination game with multiple equilibria. Specifically, it is shown that equilibria with forks (a coordination failure) can emerge from information delays and software upgrades. Ewerhart (2020) shows that the longest-chain rule constitute a pure-strategy Nash equilibrium in a finite-time mining game. However, he build some examples showing that longest-chain rule is not a subgame perfect equilibrium. Departing from costly managed cryptocurrencies, Saleh (2020) studies a mining game intended to model the accounting management of a Proof-of-Stake (PoS) cryptocurrency. Equilibrium conditions are established under which PoS protocol generates consensus in appending blocks to the longest chain.====Our paper differs from Biais et al. (2019), among other things, because we explicit model ==== secret mining behavior. While in Biais et al. (2019) miners choose which blockchain follow (adopt), in our environment miners have the option to hide valid blocks in order to create their own longer blockchain. Similarly to Saleh (2020), we show that low rewards can induce an equilibrium where miners coordinate on updating the longest chain. According to Saleh (2020), low rewards for updating blockchains powered by PoS technology induce an equilibrium with no forks (miners append blocks only to the longest chain) because two opposing effects emerge when a miner adds a block to a shorter branch on the blockchain. A low block reward, in terms of coins on that branch, is received at the same time that the value of all coins falls.====In addition to this introduction, this paper is organized in three sections. In Section 2, we organize concepts on cryptocurrencies around the problem of accounting coordination and develop our benchmark accounting coordination game. In Section 3, the possibility of double spending is introduced in the accounting coordination framework. Also, the benchmark model is extended to incorporate this possibility. Section 4 concludes with some final remarks. Proof and auxiliary results are presented in Appendix A and a description of the numerical strategy for computing equilibrium condition is presented in Appendix B.",Cryptocurrency is accounting coordination: Selfish mining and double spending in a simple mining game,https://www.sciencedirect.com/science/article/pii/S0165489623000173,26 February 2023,2023,Research Article,12.0
Scalzo Vincenzo,"Department of Economics and Statistics (DISES), University of Naples Federico II, via Cinthia 21, 80126 Napoli, Italy","Received 31 August 2022, Revised 21 February 2023, Accepted 21 February 2023, Available online 24 February 2023, Version of Record 26 February 2023.",https://doi.org/10.1016/j.mathsocsci.2023.02.006,Cited by (0),"In this paper, we consider non-cooperative games where each player has an other-regarding behavior. In particular, we study the ","Non-Cooperative Game Theory usually assumes that the players behave in a selfish way: each player looks at those strategies that maximize own preference relation in response to the strategies chosen by the opponents. This behavior gives rise to the well-known Nash equilibrium concept. However, this behavior and Nash equilibrium do not always allow to get the best outcomes for the players. For instance, one can look at the well known Prisoners’ Dilemma recalled below, where ==== means “confess”, ==== means “deny” and the elements of the bimatrix are the years of detention (here the players aim to minimize the payoff functions): ====The strategy profile ==== is the only one Nash equilibrium, but a best outcome for each prisoner can be obtained with the strategy profile ====. Nevertheless, the strategy profile ==== can be identified as an equilibrium of the given one-shot game if each player has an ====. More precisely, assume that each prisoner chooses for the best of the opponent and believes that the opponent behaves similarly. So, the prisoners have the strategy profile ==== as an equilibrium point. In particular, ==== satisfies the definition of ==== introduced by Schouten et al. (2019) (in the setting of ====-person games, the unilateral support equilibrium coincides with a previous equilibrium concept due to Berge, 1957, Zhukovskii, 1994 and called ====).====The other-regarding behavior in non-cooperative games and unilateral support equilibrium find motivation in the stream of economic thought where the individuals are assumed to be not selfish: for instance, let us refer to Civil Economy (see Bruni and Zamagni, 2017).==== Moreover, if one looks at general models of interacting individuals, the other-regarding behavior can be considered as a good practice. For example, we refer to the Hume aphorism (1740), that we quote as given in Becchetti and Cermelli (2018): “Your corn is ripe today; mine will be so tomorrow. It is profitable for us both, that I should labour with you today, and that you should aid me tomorrow. I have no kindness for you, and know you have as little for me. I will not, therefore, take any pains upon your account; and should I labour with you upon my own account, in expectation of a return, I know I should be disappointed, and that I should in vain depend upon your gratitude. Here then I leave you to labour alone: You treat me in the same manner. The seasons change; and both of us lose our harvests for want of mutual confidence and security”. The same question has been resumed by Salukvadze and Zhukovskiy (2010), which noted that “playing the Berge equilibrium does not make individuals more altruistic or fair; instead, they are simply better economists, since respecting this social practice enables them to reach social efficiency”.====In this paper, we give new existence results for the unilateral support equilibrium in the setting of discontinuous games. First, we consider games where players’ preferences are not necessarily complete or transitive (non-ordered preferences); then, we deal with players with payoff functions. The results we give have been obtained in the spirit of a recent body of literature on the existence of Nash equilibria: see Nassah and Tian (2016) and Scalzo (2019). In particular, we give necessary and sufficient conditions for the existence of unilateral support equilibria and introduce two sets of conditions: (i) the ==== and ====; (ii) the ==== and ====. Under standard compactness assumptions, we obtain two different results: ====a) ====; ====b) ====. Moreover, we study stability properties of unilateral support equilibria when perturbations on players’ behavior and players’ characteristics occur. More precisely, first we consider players that partially defect in supporting the others. We allow players that are willing to support the others – in the sense of the unilateral support equilibrium concept – but choosing on strict subsets of own strategies; in this case, players aim to get some gain by renouncing to a subset of own strategies. We give conditions on games such that, if one considers a sequence of subsets of strategy profiles ==== of a game that converges to the set of strategy profiles and if one takes a sequence ==== such that ==== is the unilateral support equilibrium corresponding to the set ====, then the sequence ==== has a cluster point which is a unilateral support equilibrium of the game. Then, we consider the stability of unilateral support equilibria with respect to small changes of players’ preference relations. In particular, we identify complete metric spaces ==== of games such that the mapping ====, which associates each game ==== with the set of unilateral support equilibria of ====, is non-empty valued and upper semicontinuous on ==== and lower semicontinuous on a dense subset of ====. The lower semicontinuity of ==== corresponds to a property called ==== (see Fort, 1951 for the essential stability of fixed points and Wu and Jiang, 1962 for the essential stability of Nash equilibria). We define three complete metric spaces: ====, ==== and ====. The space ==== includes games with non-ordered preferences, while ==== and ==== deal with games having discontinuous payoff functions. The spaces ==== and ==== are defined by the games that satisfy the uniform support convexity property and, respectively, two strengthening of the single support property, while ==== is the space of support transfer quasi-concave games that satisfy a strengthening of the support transfer continuity.====The paper is organized as follows. Section 2 gives the setting, the definition of unilateral support equilibrium and examples with economic meaning. Section 3 deals with existence results in discontinuous games with either non-ordered preferences and payoff functions. Section 4 presents stability results. The comparison between our results and the previous ones on the unilateral support equilibrium and Berge equilibrium is given in Section 5.",Existence and stability results on the unilateral support equilibrium,https://www.sciencedirect.com/science/article/pii/S0165489623000197,24 February 2023,2023,Research Article,13.0
"Dastidar Krishnendu Ghosh,Jain Sonakshi","Jawaharlal Nehru University, India,Sri Venkateswara College, University of Delhi, India","Received 6 September 2022, Revised 11 February 2023, Accepted 14 February 2023, Available online 20 February 2023, Version of Record 26 February 2023.",https://doi.org/10.1016/j.mathsocsci.2023.02.003,Cited by (1),This paper analyses the impact of favouritism and ,"Public institutions as well as state-owned enterprises need to procure goods and services to carry out their responsibilities and duties. Public procurement is a key economic activity of governments that represents a significant percentage of the Gross Domestic Product (GDP) generating huge financial flows. In OECD countries, public procurement is estimated to account for 12% of GDP. In many non-OECD countries, that figure is probably higher.====An effective procurement system plays a strategic role in governments for avoiding mismanagement and waste of public funds. Public procurement is one of the government activities which is most vulnerable to corruption. In addition to the volume of transactions and the financial interests at stake, corruption risks are exacerbated by the complexity of the process, the close interaction between public officials and businesses, and the multitude of stakeholders. There is a lot of literature around corruption in public procurement.====Besides corruption, it is favouritism that puts the public procurement in peril. It may be noted that favouritism has been a part of human society, and it has existed within the social structure for millennia. Most government organisations have diversified stakeholders. Besides satisfying the voters, the ruling political party often favours some specific groups of people—industrialists, people belonging to a certain caste or social group etc.====Favouritism and corruption coexist in almost all developing economies. Corruption thrives on the lack of commitment to basic morality and professional ethics. Favouritism thrives either on ==== (same caste, family, same community etc.) or on ==== (favouring politically connected actors in return for some favour). For instance, there are evidences to demonstrate that African political leaders have a tendency to favour members of their own ethnic group.====This paper tries to theoretically analyse the effects of both favouritism and corruption in public procurements in an emerging economy. In our exercise there are two firms: one is the favoured one while the other is not. The firms compete in a procurement auction to win the contract for building a public good. The lowest bidder is declared the winner. The firm that wins the contract needs to supply a good that meets a certain quality standard, failing which its payment would be withheld. There is corruption in the system: if the measured quality falls short of the minimum stipulated one, the winner can pay a bribe to inflate the reported quality. The same amount of bribe will inflate the reported quality of the favoured firm by a higher magnitude as compared to the firm which is not favoured.====We focus on favouritism based on kinship. This kind of favouritism is common in an emerging economy like India. For example, if the owner (manager) of a firm has the same caste as the officer in the government department (who is in charge of giving the final approval for the contract), then such an owner (manager) is often provided some unfair advantage.====It may be noted that in our story corruption and favouritism are intertwined. The favoured firm is required to pay bribe to escape punishment but the amount of bribe it needs to pay is lower than the other firm (which is not the favoured one). To the best of our knowledge no other paper in the literature has taken this approach.====We show that such favouritism induces inefficient outcomes, reduces competition and results in lower expected equilibrium quality. The favoured firm also earns a higher payoff. This indicates some form of crony capitalism. The welfare effects of favouritism are ambiguous and we illustrate our results with an example.",Favouritism and corruption in procurement auctions,https://www.sciencedirect.com/science/article/pii/S0165489623000161,20 February 2023,2023,Research Article,14.0
"Powers Robert C.,Wells Flannery","Department of Mathematics, University of Louisville, Louisville, KY, 40292, USA","Received 25 May 2022, Revised 21 December 2022, Accepted 1 February 2023, Available online 2 February 2023, Version of Record 9 February 2023.",https://doi.org/10.1016/j.mathsocsci.2023.02.001,Cited by (0),"Campbell and Kelly (2015) proved that, for ==== alternatives and ==== is a multiple of four and ==== is equal to three. We prove their characterization does hold in this case and in the process we give another characterization of majority rule.","Our model of social choice starts with ==== alternatives and ==== individuals. Each individual provides a strict ranking of the alternatives and these strict rankings are listed as an ordered ====-tuple called a profile. A social choice function takes any profile as input and outputs a single winning alternative. Given a profile of linear orders, an alternative ==== is the strict Condorcet alternative if, for any other alternative ====, there are more individuals that rank ==== above ==== than ==== above ====. In other words, ==== beats any other alternative by majority rule. It is well known that for some profiles of linear orders there is no strict Condorcet alternative. This situation is avoided by restricting the domain of a social choice function to the set of all profiles of linear orders where the strict Condorcet alternative exists. This is the Condorcet domain and we will denote it by ====. The social choice function defined on ==== that always outputs the strict Condorcet alternative is called majority rule or the Condorcet rule and it is denoted by ====.====The characterization of majority rule given in this paper is based on the conditions of neutrality, unanimity, and strategy-proofness. A social choice function satisfies neutrality if the social output does not depend on the labeling of the alternatives and unanimity is the requirement that if every individual ranks an alternative ==== first, then the social output should be ====. The condition of strategy-proofness is the requirement that an individual cannot gain a better social outcome if she inputs a ranking other than her true ranking. Majority rule and dictatorships are rules that satisfy neutrality, unanimity, and strategy-proofness. To be clear, a social choice function is called a dictatorship if there is an individual, the dictator, such that the social output is always the dictator’s first ranked alternative. If a social choice function is not a dictatorship, then it is said to be non-dictatorial. Our main theorem states that for any integers ==== and ====, a rule ==== defined on the Condorcet domain ==== satisfies unanimity, neutrality, strategy-proofness and is non-dictatorial if and only if ==== is the Condorcet rule ====. As we explain next, our characterization of majority rule is basically an extension of a well-known theorem due to Campbell and Kelly.====Campbell and Kelly (2003) (edited in Campbell and Kelly, 2016) proved that for any ==== integer ====, majority rule is the only social choice function defined on the Condorcet domain ==== that is non-dictatorial, strategy-proof and has full range. A social choice function has full range, or is said to be surjective, if every alternative is realized as a social output. The first step in the proof of their theorem is to show that if a rule ====, defined on ====, is strategy-proof and has full range, then ==== satisfies unanimity. It is clear that if a social choice function satisfies unanimity, then it has full range whereas the converse is not true. Therefore, we can formulate a weaker version of the Campbell and Kelly theorem: a social choice function ==== defined on ==== is non-dictatorial, strategy-proof, satisfies unanimity, and the number ==== of individuals is odd if and only if ==== is the Condorcet rule. We can compare the main theorem of this paper with this weak version of the Campbell and Kelly theorem. Specifically, Campbell and Kelly’s requirement of ==== being odd can be dropped if we add the neutrality condition. So, under the oddness of ==== assumption, our theorem is actually weaker than Campbell and Kelly’s theorem. The situation changes if ==== is even. In this case, Merrill (2011) gave an example of a social choice function ==== defined on the Condorcet domain ==== such that ==== is non-dictatorial, strategy-proof, satisfies unanimity, and ====. So, for ==== even, the neutrality assumption is a necessary condition for our theorem. Moreover, we will give an example of a social choice function defined on ==== that is not the Condorcet rule and yet this rule is non-dictatorial, strategy-proof, neutral, and has full range.====Campbell and Kelly (2015) also considered neutrality as well as the condition of anonymity. Anonymity is the requirement that a social choice rule does not depend on the identities of the individuals. Campbell and Kelly (2015) proved that for any integers ==== and ====, the Condorcet rule ==== is the only rule defined on the Condorcet domain ==== that satisfies anonymity, neutrality, and strategy-proofness. Moreover, they were able to show that their characterization of ==== holds for ==== alternatives whenever ==== for ====. They left as an open question whether their result holds for ==== and ==== (see Remark 3 in Campbell and Kelly, 2015). As a consequence of our main result, we are able to show that it does hold.",Another strategy-proofness characterization of majority rule,https://www.sciencedirect.com/science/article/pii/S0165489623000148,2 February 2023,2023,Research Article,15.0
"Banerjee Swapnendu,Chakraborty Somenath","Department of Economics, Jadavpur University, Kolkata 700032, India","Received 22 May 2021, Revised 6 January 2023, Accepted 20 January 2023, Available online 26 January 2023, Version of Record 1 February 2023.",https://doi.org/10.1016/j.mathsocsci.2023.01.005,Cited by (0),"We, first with discrete outcomes and continuous effort, characterize the optimal contracts when a spiteful principal interacts with an other-regarding agent. We show that the optimal wage is weakly decreasing in principal’s spitefulness. When the principal and the agent have exactly opposite other-regarding preferences, under certain situations, we get back the self-regarding benchmark optimal contract. Next with continuous outcomes and general functional forms, we characterize the optimal wage schedule and get back the ==== result when the principal and the agent have exactly opposite other-regarding preferences.","Spiteful behaviour existed in human beings from times immemorial.==== It can stem from envy Wobker (2015), Nickerson and Zenger (2008), Canen and Canen (2012) or more generally from social comparison.==== One might feel a loss in her wellbeing when she observes that others around her has achieved more compared to her (can be consumption, material payoffs or other forms of achievements, fame etc.) even if she has an increased payoff/achievement compared to an earlier reference point and this can lead to the person becoming spiteful.==== Thus spiteful behaviour can originate from a sense of relative under-achievement although there can be various other factors like cultural, behavioural and/or genetic.====There are various ways in which spite is conceived in the literature. One is the ‘willingness of an individual to incur a cost to one-self in order to inflict harm on others even in the absence of any direct benefit’ (Fehr and Fischbacher, 2005, Smead and Forber, 2013. Another is ‘desire to reduce another’s material payoff for the mere purpose of increasing one’s relative payoff” (Fehr et al., 2008. In this paper we define spite a la (Fehr et al., 2008), i.e., the ‘desire to reduce another’s material payoff for the mere purpose of increasing one’s relative payoff”.====The target of this paper is to flesh out some of the contract theoretic implications of having a spiteful principal in a single agent hidden action framework. We conduct the analysis in two parts. First we consider continuous effort but discrete outcomes and work with specific functional forms for the sake of closed form solutions. Next we generalize our structure by assuming continuous effort and outcomes with general functional forms. Specifically, we study the interaction of a spiteful principal with an other-regarding agent. The principal is spiteful a la (Fehr et al., 2008). The principal derives additional pleasure from being ahead. The agent when inequity-averse, suffers utility loss from being behind. We show that the optimal wage falls with increased spitefulness of the principal. When the principal and the agent have opposite other-regarding preferences, the optimal wage will depend on the relative strength of the principal’s and the agent’s other-regardingness. For lower outside option of the agent, if the degree of spitefulness of the principal is exactly equal to the degree of inequity aversion of the agent, the optimal wage will be exactly equal to the self-regarding benchmark.==== For higher outside option such that the agent’s participation constraint binds, the principal will optimally offer (weakly) lower success wage vis-a-vis the self-regarding benchmark. When the principal’s spite exceeds the agent’s other-regarding parameter, optimal success wage is (weakly) lower compared to the self-regarding benchmark and this holds irrespective of participation constraint binding or not. In the closed form model, throughout, we assume effort to be continuous. We model other-regarding preferences in line with the distributional approach a la Fehr and Schmidt (1999) and Neilson and Stowe (2003). As we proceed we provide alternative specifications of comparing payoffs and examine the possible changes in the optimal contractual structure. Interesting to note is that when the principal factors in the effort cost (of the agent) while comparing payoffs, the self-regarding benchmark is not achieved even with exactly opposite other-regarding preferences of the principal and the agent. This is worth pointing out. Finally, we propose a model with general other-regarding functions and continuous output and efforts, a la Englmaier and Wambach (2010) and show that optimal wage indeed falls with increased spite of the principal. We provide detailed characterization of the optimal wage schedule and also show that the sufficient statistics result holds when the principal and the agent have exactly opposite other-regarding preferences. We discuss our contribution compared to the closely related papers by Banerjee and Sarkar (2017) and Englmaier and Wambach (2010) below.====Till date the only paper that specifically addressed optimal contracting with an other-regarding principal is Banerjee and Sarkar (2017). First, in terms of specification, Banerjee and Sarkar (2017) focus on discrete efforts whereas our closed form analysis focuses on continuous efforts. Second, Banerjee and Sarkar (2017) show that the optimal contract differs considerably vis-a-vis the self-regarding benchmark only when the principal is sufficiently ‘inequity-averse’, otherwise not. On the contrary we show that the optimal contract differs considerably vis-à-vis the self-regarding benchmark even when the principal is status-seeking or spiteful. Third, contrary to Banerjee and Sarkar (2017), we provide detailed characterization of the optimal contract for all possible combinations of principal’s spite and agent’s other-regardingness and compare it with the self-regarding benchmark. Finally and importantly, in our structure we get back the self-regarding benchmark when the principal and the agent have exactly equal and opposite other-regarding preferences. This is a crucial distinction of our analysis with Banerjee and Sarkar (2017) which can be attributed to our effort specification. Discrete effort throws up interesting discontinuities in optimal contracts whereas continuous effort provides a smooth optimal contract schedule. Exactly opposite other-regarding preferences leading to the self-regarding benchmark is an artefact of the smooth continuous effort specification which was absent in Banerjee and Sarkar (2017).====The crucial distinction of our general structure with that of Englmaier and Wambach (2010) is that we consider a spiteful principal who is ahead and ‘enjoys’ being ahead. We show that with contractible efforts and risk-neutral agent optimal linear contract with slope ==== holds certainly for a linearly spiteful principal. With risk-averse agents an increase in principal’s spite reinforces the agent’s risk-aversion effect and the slope of the wage schedule becomes closer to zero. With non-contractible effort and risk-neutral agent, on the contrary, principal’s spite reinforces the risk-neutrality effect leading to extreme incentive provision at the optimal. Similarly under non-contractibility with a risk-averse agent, principal’s spite reinforces the incentive effect and the slope of the wage schedule gets increased. Thus under non-contractibility, principal’s spite reinforces the incentive effect, nullifying the impact of the agent’s inequity concern to some extent. Interestingly in an important addition to their results, we show that the sufficient statistics result holds when the principal and the agent have exactly opposite other-regarding preferences. This, in essence, supports our conjecture that we get back the self-regarding benchmark when the principal and the agent have exactly opposite other-regarding preferences, a result we get in our closed form model as well.====It is worth mentioning that in their working paper version (Englmaier and Wambach, 2005) do briefly discuss the interaction of an inequity-averse principal and an inequity-averse agent. They show that the principal’s and the agent’s fairness concerns work in the same direction, and therefore incorporating an inequity-averse principal will reinforce their results (proposition 1 to 8 in their paper) since their original problem does not change qualitatively with an inequity-averse principal. To differentiate our work, on the contrary, we focus on a principal who is spiteful and is ahead. Spitefulness of the principal works in the opposite direction of agent’s inequity concern and provide us with additional insights in characterizing the optimal wage schedule. This is a crucial difference that separates our contribution from that of Englmaier and Wambach, 2005, Englmaier and Wambach, 2010. We carry out our analysis in a single principal–agent framework; multiple agent case is kept for future research.====Various experimental evidences have proved the existence of other-regarding preferences in behavioural decision making (Guth et al., 1982, Camerer (2011).==== Relaxing the self-regarding hypothesis is crucial for contract theory since the aim is to design appropriate incentives, and therefore people’s attitude towards other’s wellbeing as well as her own wellbeing is crucial for incentive design.====Few recent papers have addressed the issue of incorporating other-regarding (or social) preferences into contract theory.==== Apart from Englmaier and Wambach (2010) the paper that addressed other-regarding preferences in hidden action framework was Itoh (2004). He, in a discrete effort framework, looked into the interaction between a self-regarding principal and an other-regarding agent and showed that the principal is in general worse-off the more other-regarding the agent is. Although Itoh (2004) briefly mention other-regarding principal, he did not analyse it in detail. Dur and Glazer (2008) use a principal–agent model with contractible effort to study profit-maximizing contracts when a risk-averse worker envies her employer. They show that with an envious agent a self-regarding principal can optimally offer a profit sharing contract even when the effort is contractible. Englmaier and Leider (2012) incorporate reciprocal preferences in line with Rabin (1993), Dufwenberg and Kirchsteiger (2004) and Falk and Fischbacher (2006) into a moral hazard framework and derive properties of the optimal contract and implications for organizational structure but with a self-regarding principal.====The rest of the paper is organized as follows: In Section 2 we analyse the interaction between a spiteful principal and an other-regarding agent with continuous effort and binary outcomes. In Section 3 we analyse in detail the interaction of a spiteful principal and an other-regarding agent in a general structure with continuous effort and outcomes and general functional forms. In Section 4 we provide concluding remarks and throw some light on possible future works.",Optimal incentive contracts with a spiteful principal: Single agent,https://www.sciencedirect.com/science/article/pii/S0165489623000136,26 January 2023,2023,Research Article,16.0
"Vragov Roumen,Smith Vernon","QCC, CUNY Business Department, 222-05 56th Ave, Bayside, NY, 11364, United States of America,Chapman University Dale E. Fowler School of Law George L. Argyros School of Business and Economics, 1 University Dr, Orange, CA 92860, United States of America","Received 14 December 2021, Revised 27 November 2022, Accepted 7 January 2023, Available online 13 January 2023, Version of Record 21 January 2023.",https://doi.org/10.1016/j.mathsocsci.2023.01.002,Cited by (0),"The recent birth and proliferation of online digital community platforms have led to an increased interest in more efficient voting mechanisms than simple yes–no majority voting. Quadratic voting and the Compensation election have shown some promise in the laboratory, however some gaps still remain in our understanding of the strategic features of these mechanisms. Using proof by example and the properties of infinitely divisible distributions, we develop a procedure for generating parameterizations of these voting mechanisms that admit Pure Strategy Bayes–Nash equilibria (PSBNE). We illustrate the equilibrium bid (or voting) functions graphically and derive their ====.","Eguia and Xefteris (2021) & Goeree and Zhang (2017) provide a thorough review of currently known referendum mechanisms that can achieve higher efficiency than simple “yes-no” majority voting. Out of these mechanisms Quadratic voting (see Posner and Weyl, 2017) and the Compensation election (see Oprea et al., 2007) have shown the most promise in the laboratory.==== Both of these mechanisms allow voters to express the intensity of their preferences and to receive compensation after the voting process is complete. Under Quadratic voting the compensation comes in the form of a rebate that is determined by the sum of the votes of the remaining participants, and under the Compensation election participants receive compensation equal to their vote in case the alternative they vote for is not chosen.==== Advances in technology have removed many of the technical hurdles to applying these mechanisms in practice but some theoretical issues remain. The pure-strategy symmetric Bayes–Nash equilibrium (PSBNE) bid function in the Compensation election is still unknown and there is currently no proof that PSBNE of Quadratic voting exists when the outcome function is discontinuous or there are more than two participants. This article identifies a set of parameterizations of the Compensation election that admit a pure strategy Bayes–Nash equilibrium using proof by example. Even though the proof applies only to these limited parameter spaces, it does not require continuity of the outcome function, compactness of the participants’ types or large electorates. The approximate equilibrium voting function of the Compensation election is derived in those cases and a method that one can use to generate more parameterizations allowing an equilibrium is described. The method is then applied to Quadratic voting but in a narrower fashion. The work here extends the theory of Bayesian games and is timely since one of the mechanisms has already been used twice in practice (Coy, 2019, Tang, 2019).====The article is organized in the following way: Section 2 provides the theoretical background; Section 3 formally describes the Compensation election mechanism, analytically derives the PSBNE voting function in the trivial case of two voters, and describes the complexities involved in deriving the PSBNE voting function when there are more than two voters; Section 4 describes the procedure used to generate model parameterizations using a Normal distribution as a “seed”, offers a proof of the validity of the procedure, and illustrates some of the generated examples; Section 5 provides conditions that can extend the examples further to other distributions; Section 6 applies the procedure to Quadratic voting; and Section 7 outlines the conclusions, limitations, and directions for future research.",A method for identifying parameterizations of the Compensation election and Quadratic voting that admit pure-strategy equilibria,https://www.sciencedirect.com/science/article/pii/S0165489623000021,13 January 2023,2023,Research Article,17.0
Bucci Alberto,"International Center for Economic Analysis (ICEA), Wilfrid Laurier University, Waterloo, Canada,Department of Economics, Management, and Quantitative Methods (DEMM), University of Milan, Milan, Italy,Center for Economic and International Studies (CEIS), University of Rome-‘Tor Vergata’, Rome, Italy","Received 2 March 2022, Revised 5 October 2022, Accepted 4 January 2023, Available online 10 January 2023, Version of Record 21 January 2023.",https://doi.org/10.1016/j.mathsocsci.2023.01.001,Cited by (1),"To better understand the long-term consequences and the possible channels (mainly, technological progress and human capital accumulation) through which a declining population may eventually sustain long-run economic growth and innovation in advanced economies, we build an endogenous, non-scale growth model with horizontal R&D activity and ====. Population growth and per capita human capital accumulation are linked to each other by means of what in the paper is identified as the ‘multiplicative effect’ of population. As long as the strength of this effect exceeds a given positive threshold, population growth and per-capita income growth can be negatively correlated. This means that a negative rate of population growth can sustain a positive economic growth rate in the long run.",None,Can a negative population growth rate sustain a positive economic growth rate in the long run?,https://www.sciencedirect.com/science/article/pii/S016548962300001X,10 January 2023,2023,Research Article,18.0
"Altun Ozan Altuğ,Barlo Mehmet,Dalkıran Nuh Aygün","Faculty of Arts and Social Sciences, Sabancı University, Istanbul, 34956, Turkey,Department of Economics, University of Maryland, 3114 Tydings Hall, 7343 Preinkert Dr., College Park, MD 20742, USA,Department of Economics, Bilkent University, Ankara, 06800, Turkey","Received 24 December 2021, Revised 4 August 2022, Accepted 6 December 2022, Available online 15 December 2022, Version of Record 23 December 2022.",https://doi.org/10.1016/j.mathsocsci.2022.12.002,Cited by (1),"We study Nash implementation under complete information with the distinctive feature that the planner knows neither individuals’ state-contingent preferences (payoff states) nor how they correspond to the states of the economy on which the social goal depends. Our main question is whether or not the planner can extract only the essential information about individuals’ underlying preferences and simultaneously implement the given social goal. Our setup is especially relevant when the planner cannot use mechanisms asking for the full revelation of the payoffs states due to privacy and political correctness concerns or non-disclosure and confidentiality agreements. In economic environments with at least three individuals, we show that the planner may Nash implement a social goal while extracting only the essential information about the payoff states from the society whenever this goal has standard ","In the implementation problem, a planner (she) is responsible for the decentralization of a social goal that depends on information that she seeks to elicit from the society via a mechanism. Her foresight of individuals’ behavior is crucial for the design of such mechanisms. The standard approach assumes that the planner knows how the ====, on which the social goal depends, correspond to the individuals’ payoff characteristics (====). In this context, the seminal works of Maskin (1999, circulated since1977), Moore and Repullo (1990), and Dutta and Sen (1991) provide characterizations of social goals that admit mechanisms the equilibria of which coincide with a given goal under complete information.====The critical difference of our setup is that the planner does not have any information about the connection between individuals’ state-contingent preferences and the states of the economy. Still, she is tasked with implementing a given social goal.====In a nutshell, we analyze full implementation under rationality and complete information with the distinctive feature that the planner observes neither the realized state of the economy (determining the social goal) nor individuals’ associated state-contingent preferences (payoff states) and does not know how the states of the economy and the payoff states are related. We investigate the scope of the essential information about individuals’ payoffs the planner needs to elicit to implement the given social goal. This is especially relevant when planners have to refrain from using mechanisms inquiring directly about individuals’ rankings of alternatives (payoff states) due to privacy and political correctness concerns or non-disclosure and confidentiality agreements.====As a concrete example, the planner could be an ==== (such as the IMF, the World Bank, or the UNICEF) authorized to implement a development policy in a country by choosing the ‘appropriate’ policy alternative depending on the state of that country’s economy. The stakeholders in the country’s welfare know the state of the economy (e.g., the level of corruption) and have a ranking of the feasible policy options (depending on the level of corruption). To foster synergies and promote joint decisions, the international organization aims to decentralize its policy decisions, taking a mechanism design approach. However, the agency knows that elicitation of the minimal essential information from the country’s stakeholders is critical; asking the native stakeholders’ rankings of all policy alternatives may not be feasible because many of them may not wish to reveal seemingly ‘problematic’ information about their country’s state to a ‘foreign power’ since they may get punished publicly by being branded as a ‘comprador’.====Alternatively, the planner could be an ==== (e.g., McKinsey Implementation (McKinsey, 2018)) responsible for implementing a given policy contingent on the information about the financial and operational state of a firm that its client has acquired through either a merger or an acquisition. The policy calls for a choice from a set of feasible alternatives based on the state of the newly acquired firm on which its employees’ rankings of these alternatives depend. The implementation consulting agency uses a mechanism design approach to decentralize the policy decision. In such cases, the inquiries about the payoff states of the acquired firm’s employees may need to be restricted. Because these employees may not be at liberty to reveal all their knowledge due to non-disclosure or confidentiality agreements as their rankings of policy alternatives may disclose information detrimental to the previous owner.====, we establish that if the planner knows that a social choice correspondence (SCC) is implementable by a mechanism in Nash equilibrium, then she infers that there is a profile of sets ==== with this SCC without necessarily knowing the full specification of sets that appear in this profile. Therefore, the knowledge of the existence of a rational-consistent profile constitutes the minimal information pertinent to the association between individuals’ preferences and the states of the economy in conjunction with the Nash implementability of the given SCC. Moreover, the existence of a profile rational-consistent with the SCC is equivalent to the well-known Maskin monotonicity of this SCC.==== Consequently, our necessity result also highlights that if the revelation of a rational-consistent profile is not admissible on the grounds of privacy, political correctness, and non-disclosure or confidentiality agreements, then implementation is not ====.====Our ==== and the main result is that with at least three individuals, if the planner knows that the ==== is ====, the implementation is feasible as discussed in the previous paragraph, and one of the individuals (whose identity is not necessarily known to the planner and the other individuals) is a ====, then she infers the following: The given SCC is Nash implementable by a mechanism that elicits the essential information concerning rational-consistency from the society unanimously. That is, if the planner knows that the SCC possesses a rational-consistent profile of sets and inquiring about it is admissible, then she no longer needs to know the association between the payoff states and the states of the economy to identify a rational-consistent profile of sets and to implement the SCC. She can simply ask the individuals for the minimal essential information, knowing that all announce the same profile of rational-consistent sets, and design the mechanism using this profile.====The ==== assumption requires that agents’ choices are not perfectly aligned: for any alternative and any payoff state, there exist two individuals who do not choose that alternative in that state from the set of all alternatives. Therefore, it demands that there is some weak form of disagreement in the society at every payoff state.====We attain the notion of ==== by modifying ==== of Dutta and Sen (2012) so that it involves only announcements of profiles of sets. In that regard, we restrict attention to mechanisms that involve each agent announcing a profile of sets. A ==== of the SCC, then, is an individual who strictly prefers an action that consists of the announcement of a profile rational-consistent with this SCC coupled with some messages to another action that involves announcing an inconsistent profile and the same messages, whenever both actions deliver this individual’s most preferred alternatives among those he can sustain via unilateral deviations. Thus, a sympathizer is not a snitch or an informer because he does not feel obligated or inclined to truthfully reveal the realized state of the economy or the associated payoff state. In essence, sympathy relates only to the truthful revelation of the association between the states of the economy and the payoff states. Further, a sympathizer of a given SCC does not have to be inclined to truthfully reveal a rational-consistent profile of sets associated with another SCC. So, a sympathizer serves the planner as a guide and can be thought of as a proponent of the policy the planner aims to implement.====In our initial example with the international agency, a sympathizer is a native stakeholder who ‘believes’ in the policy the agency is tasked to implement. This individual is inclined to reveal a profile of choice sets such that in every state of the economy, individuals’ choices from the corresponding sets are aligned with the agency’s policy. Such a profile discloses only the essential information needed for implementation while revealing some partial admissible information about associated payoff states. So, in the context of our example with the consulting agency, the agency can decentralize its policy as long as the revelation of the sympathizing employee of the acquired firm (who is enthusiastic about climbing up the ladder in the new organization) does not violate his non-disclosure or confidentiality agreements with the previous owner.====The mechanism that we employ in our main (sufficiency) result differs from the canonical mechanism in a particular manner: It asks every individual a profile of sets, the realized state of the economy, an alternative, and an integer. The distinctive feature is that the ====—alternatives that an individual attains by unilateral deviations—associated with the situation in which all agents announce the same state of the economy and an alternative socially optimal at that state are determined according to the announced profiles of sets as long as profile announcements of all but one agree. Thus, the planner needs to know neither the payoff states nor individuals’ state-contingent lower contour sets. The presence of a sympathizer ensures that in equilibrium, all agents announce the same rational-consistent profile of sets. As a result, the identity of the sympathizer is not disclosed in any Nash equilibrium of our mechanism.====The existence of a rational-consistent profile is at the core of the Nash implementability of the given SCC. Yet, the planner, who does not know how states of the economy and payoff states are related, cannot identify/verify this central condition on her own. To extend our sufficiency result to a setting where the planner draws the inference of existence of rational-consistency by herself, we provide the following result: The planner deduces the existence of a profile rational-consistent with the given SCC whenever she knows that this SCC possesses a Maskin monotonic extension to the set of all payoff states even if she does not know the full specification of this extension.====We extend our analysis and results to the behavioral domain (by allowing but not insisting on violations of WARP) in Appendix A.==== We also consider extensions of our sufficiency result to noneconomic environments using the behavioral version of the no-veto property and continuing to work with three or more individuals. In the resulting behavioral setting, we attain another version of our sufficiency result by replacing the economic environment assumption with ==== and having at least two ==== the identities of whom are privately known to themselves, but not to the planner.====Our paper is closely related to the literature on implementation with partial honesty, pioneered by Dutta and Sen (2012).==== Their construction assumes that at least one of the individuals has a preference for honesty. To formulate this, individuals’ preferences on alternatives are extended to messages when dealing with mechanisms that involve the announcement of a payoff state. A partially honest individual is assumed to strictly prefer a message involving the announcement of the ‘true’ payoff state when none of his deviations make him strictly better off. Then, that study shows that all SCCs satisfying the no-veto property can be implemented in Nash equilibrium whenever the society contains at least three individuals, one of whom, whose identity is privately known only by himself, is partially honest. This sufficiency result does not need Maskin monotonicity. On the other hand, sympathy for an SCC involves an inclination toward the truthful revelation of profiles of sets with this SCC but not truthful announcements of the realized states of the economy and the associated payoff states. That is why sympathy for an SCC differs from partial honesty. Indeed, unlike many papers on implementation with partial honesty, we need a Maskin monotonicity type of requirement to extract information about the states of the economy. In Section 5, we analyze the relation between sympathy and partial honesty in detail.====Another related paper is Barlo and Dalkıran (2021) which studies “suitable notions of implementation [under incomplete information] for environments in which planners do not observe all the data on individuals’ choices and are partially informed about the association of individuals’ preferences with states of the economy”. That article differs from the current paper in three folds. In that paper, ==== the planner has missing data on individuals’ choices and hence is not completely ignorant; ==== there are no sympathizers and/or partially honest individuals in the society to help the planner; ==== the equilibrium notion, while related to Nash equilibrium, is different.====The rest of the paper is organized as follows. We present the preliminaries in Section 2 and a motivating example in Section 3. Our main result is in Section 4. Section 5 displays the relation between sympathy and partial honesty. Section 6 contains a result on the inference of the existence of a rational-consistent profile, while Section 7 concludes. In the Appendix, Appendix A presents a behavioral formulation, Appendix B contains our analysis of noneconomic environments, and Appendix C includes the proofs.",Implementation with a sympathizer,https://www.sciencedirect.com/science/article/pii/S0165489622000919,15 December 2022,2022,Research Article,19.0
"Ayouni Mehdi,Gabuthy Yannick","BETA (CNRS, University of Strasbourg, University of Lorraine), 13, Place Carnot, C.O. 70026, 54035 Nancy, France,Public Economics Group, University of Marburg, Am Plan 2, 35037 Marburg, Germany,CESifo, Munich, Germany,EconomiX, Paris, France","Received 1 December 2020, Revised 2 December 2022, Accepted 2 December 2022, Available online 5 December 2022, Version of Record 19 December 2022.",https://doi.org/10.1016/j.mathsocsci.2022.12.001,Cited by (0), information sharing may outperform a regime ==== information sharing according to the criteria of expected social costs and fairness considerations.,None,Asking for information prior to settlement or trial when misrepresentation of evidence is possible,https://www.sciencedirect.com/science/article/pii/S0165489622000907,5 December 2022,2022,Research Article,20.0
"Maublanc François,Rouillon Sébastien","CY Cergy Paris Université, CNRS, Thema, France,Université de Bordeaux, BSE, UMR CNRS 6060, France","Received 21 May 2021, Revised 10 November 2022, Accepted 11 November 2022, Available online 1 December 2022, Version of Record 14 December 2022.",https://doi.org/10.1016/j.mathsocsci.2022.11.001,Cited by (0),"We study multiple-prize contests where the number of prizes to be awarded is a random variable. We identify conditions under which a unique symmetric ==== exists. We compare the equilibrium efforts according to different probability distributions of the number of prizes. Considering multi-prize contests proposed so far in the literature, we show that each player’s effort decreases with the average number of prizes (first-order stochastic dominance) and may increase or decrease with the risk in the number of prizes (second-order stochastic dominance) depending on the contest technology adopted.","Sisak (2009) surveys the literature on multiple-prize contests.==== She shows that this framework can be relevant in many situations, taking examples from rent-seeking activities, patents and R&D races, licenses, labor markets, sports and so on. Sisak (2009) classifies the literature along two main dimensions, based on the specification of the contest success function (Tullock versus fully discriminating contest success function) and on the adoption of single versus multiple efforts (the contestants exert an overall effort for all prizes or can allocate it more specifically to a sub-group of prizes). The central finding is that with risk-neutral and symmetric contestants, a contest designer aiming at maximizing the aggregate effort should always prefer allocating a single prize rather than splitting it into several smaller prizes. However, dividing the prize can be optimal in situations with risk aversion and asymmetric players.====Surprizingly, the case of multiple-prize contests with an uncertain number of prizes has never been investigated, although this is a natural assumption and an immediate extension of the literature just surveyed. Yet, such situations happen in everyday life. For example, investors lobbying a public authority for a right to set up in a local market (driving school, fishing license, gas station, supermarket, etc.) do not have perfect information about how many licenses will eventually be granted. Likewise, entrepreneurs affiliating with a franchise (McDonalds, Pizza Hut, Ubereats, etc.) will have to share the market with other franchisees, whose number is not known with certainty ====. Regarding competition in Science, the number of research grants or prizes to be awarded is rarely announced in advance when researchers apply for funds or participate in scientific challenges.====If contests with an uncertain number of prizes are common, could uncertainty change the participants’ incentives to exert effort? In a recent paper, Balafoutas and Sutter (2019) answer in the affirmative. They present an experiment where subjects compete for an uncertain number of prizes in an all-pay auction. They find that men (resp. women) exert more (resp. less) effort when the number of winners is known with certainty. To the best of our knowledge, this is the only paper considering a tournament with an uncertain number of prizes. The purpose of the present paper is to provide a first attempt to fill the gap.====We study a contest between symmetric and risk-neutral players, involving a given amount divided into several prizes whose number is a random variable with a known probability distribution. We consider the class of symmetric and homogeneous contest success functions, to generalize all prize allocation rules considered so far in multi-prize contests (Berry, 1993, Chowdhury and Kim, 2014, Clark and Riis, 1996, Fu et al., 2014). Following Malueg and Yates (2006), this allows to nicely characterize pure-strategy symmetric Nash equilibria. We determine the effect of prize uncertainty on the contestants’ equilibrium effort.====We apply our results to the multi-prize contests proposed in the literature (Berry, 1993, Chowdhury and Kim, 2014, Clark and Riis, 1996, Fu et al., 2014). We find that the equilibrium effort of the contestants increases when the uncertain number of prizes gets smaller in the sense of first-order stochastic dominance. In other words, having larger chances to win a prize of lower value creates less incentives to exert effort. We also show that if the uncertain number of prizes gets more risky in the sense of second-order stochastic dominance, the aggregate effort can both increase or decrease, depending on the prize allocation mechanism at stake.====Our paper is also related to Münster, 2006, Lim and Matros, 2009, Myerson and Wärneryd, 2006 and Kahana and Klunover (2015), who extend the literature to contests with incomplete information concerning the number of contestants. They show that the (====) aggregate effort in a contest with population uncertainty is smaller than its counterpart in a contest with population certainty and the same ==== number of contestants.====Finally, this paper also contributes to the literature from the technical point of view. Our results rely on the properties (monotonicity and concavity) of a ==== function induced by the contest design. We show how to extend it to a ==== function, by using the ==== (or ====) function (Abramowitz and Stegun, 1964). This greatly eases the analysis of its properties, which are then obtained from first-order and second-order derivations.====The rest of the paper is organized as follows. Section 2 sets out the general model and characterizes symmetric pure-strategy Nash equilibria. Section 3 states conditions revealing the role of uncertainty on the equilibrium behavior of contestants before applying them to well-known multi-prize contests in the literature. Section 4 concludes. The proofs are given in the Appendix.",Contests with an uncertain number of prizes with a fixed total value,https://www.sciencedirect.com/science/article/pii/S0165489622000816,1 December 2022,2022,Research Article,21.0
"Carvajal Andrés,Thereze João","University of California, Davis, United States of America,EPGE-FGV, Brazil,Princeton University, United States of America","Received 4 February 2022, Revised 10 August 2022, Accepted 15 November 2022, Available online 26 November 2022, Version of Record 9 December 2022.",https://doi.org/10.1016/j.mathsocsci.2022.11.002,Cited by (0),"We study the interaction between insurance and financial markets. Individuals who differ only in risk have access to insurance contracts offered by a monopolist and can also save through a competitive market. We show that an equilibrium always exists in that economy and identify an externality imposed on the insurer’s decision by the endogeneity of prices in the financial market. We argue that, because of that externality and in contrast to the case of pure contract theory, equilibrium always exhibits under-insurance even for the riskiest agents in the economy and may even exhibit pooling. Importantly, the externality does not disrupt the single crossing property of the economy.","In this paper, we couple a version of the standard monopolistic insurance model to a competitive financial market. In an economy that presents idiosyncratic risk, a monopolist charges a premium in the present in exchange for idiosyncratic state-contingent promises of coverage in the future. At the same time, a financial market that is complete with respect to aggregate shocks is available to the agents. Individuals with different risk distributions simultaneously choose a portfolio of financial assets and whether to buy an insurance contract, and which. At equilibrium, these choices must be individually rational, and asset prices must guarantee that the financial markets clear. The monopolist understands this, and optimally chooses a menu of insurance contracts that is self-enforcing, taking into account how this menu impacts the equilibrium of the financial market and, therefore, the insurees’ willingness to pay for insurance.====The motivation for this study is that abstracting away from any other relation or trade occurring in the economy may sometimes render the classical insurance theory analysis invalid. The considerations that explain the behavior of, say, the insurance market for domestic appliances would likely be insufficient to understand the effects of a social security reform.==== In particular, when the insurance industry is considered in isolation, its relation with other financial markets is overlooked: on one hand, the analysis will ignore that the distribution of wealth in the society is endogenous, as individuals react to different insurance coverages by purchasing different financial portfolios; on the other, it will ignore that the prices of financial assets affect consumers’ willingness to pay for insurance.====Our main result is that the equilibrium menu of contracts always displays under-insurance of ==== agents, even the riskiest ones, which differs from the usual lessons obtained in partial equilibrium. Moreover, the equilibrium menu can pool agents of different riskiness together, which is in stark contrast with the basic results of contract theory.====We then identify the driving force behind these results, which we dub the ====. Suppose that the firm decides to increase the coverage for any insuree type. This has two effects. One is direct: the contract that insuree signs becomes more attractive. But under general equilibrium, there is also an indirect effect: when that type of agent receives more coverage, their need for savings decreases and, consequently, so does the price of savings. This price drop affects ==== the agents in the economy, making the financial assets relatively more attractive and potentially making some agents opt-out of insurance altogether. In other words, the change in coverage for one type of insuree affects the participation decision of all others. Because the insurance company understands this mechanism, the optimal menu takes the market effect into account.====The market effect is conceptually different from other sources of inefficiency and pooling in the contracting literature. It consists of an externality that operates through the individual rationality constraint, rather than through incentives. It makes separation costly for the monopolist by making it more expensive to ensure the participation of individuals in a potential separating menu. This conclusion differentiates our result from the literature that deals with the failure of the Spence–Mirrlees condition in mechanism design. Additionally, our result is not a consequence of countervailing incentives and type-dependent reservation utilities either. Rather, in our model the outside options do depend on private information, on the specific contract each agent is offered and, what is new, on the whole set of contracts posted by the firm. In fact, the externality is generated by the endogeneity of all agents’ reservation utilities to the whole menu of contracts, which is in itself a consequence of individual decisions being linked together by the financial market.",Insurance contracts and financial markets,https://www.sciencedirect.com/science/article/pii/S0165489622000889,26 November 2022,2022,Research Article,22.0
"Yang Yongjie,Dimitrov Dinko","Chair of Economic Theory, Saarland University, Saarbrücken, Germany","Received 19 December 2021, Revised 5 November 2022, Accepted 20 November 2022, Available online 25 November 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.mathsocsci.2022.11.003,Cited by (1),"We study group control for consent rules in the setting of group identification restricted to domains of consecutive qualifications. In particular, these domains are equivalent to the ====-matrix corresponding to an input opinion profile being either row circular or column circular. By utilizing two ILPs, we show that these decision problems, being ","The process of group formation can be modeled in several ways. In this paper we use the setup of group identification models formally introduced in the seminal work of Kasher and Rubinstein (1997). The task therein is to construct a well-defined group of people who share a given identity, a specific property, or a certain qualification. Specifically, individuals are assumed to have diverse views concerning who is qualified in a given society and a rule transforms these views into a well-defined group of socially qualified individuals. Thus, group identification models can be seen as voting systems where the individuals are both voters and candidates.====The social rules studied in the literature mainly differ with respect to the level of social intervention they allow in one’s own decisions. The most suitable way for modeling the different types of such an intervention was suggested in Samet and Schmeidler (2003), where the class of consent social rules was introduced and axiomatically characterized. Each rule in this class is defined by means of two positive integers ==== and ====. Specifically, if an individual qualifies herself, then this individual is socially qualified if and only if there are at least ==== other individuals who also qualify her. On the other hand, if the individual disqualifies herself, then this individual is socially disqualified if and only if there are at least ==== other individuals who also disqualify her. Clearly then, ==== or ==== refer to self-determination in the sense that one’s own opinion about oneself suffices for one’s social qualification (====) or disqualification (====). The case of ==== defines the well studied in the literature liberal rule for social qualification (cf. Cho and Ju, 2020, Dimitrov, 2011, Kasher and Rubinstein, 1997, Miller, 2008).====In this paper, we focus on the manipulability of a consent rule. One possibility to analyze this issue in group identification models is to first formally introduce individual preferences into the setup (cf. Cho and Saporiti (2020)). We have chosen to follow a rather different approach rooted in the literature on computational social choice (cf. Bartholdi et al. (1992)). More specifically, we assume that an external authority organizing the aggregation process attempts to achieve strategic results by including or excluding individuals from this process (cf. Faliszewski and Rothe, 2016, Meir et al., 2008). We formulate the corresponding attempts as decision problems, where in each of them the authority focuses on a fixed subset of individuals and its goal is to make all individuals in that subset socially qualified.====In other words, any such decision problem is described by instances, each consisting of an input and a Yes–No question. Once a problem has been formulated, one may next ask how fast the problem can be correctly solved by an algorithm. Generally, the running time of an algorithm increases as the size of problem instances increases. An algorithm is said to be efficient if its running time is bounded above by a polynomial function of the size of the instance. The class of problems that admit at least one efficient algorithm is denoted by ====. ==== is the class of all decision problems such that if the answer to a problem instance is “YES”, then there exists a certificate (aka. “YES”-witness) of polynomial length so that, in polynomial time, an algorithm accepts the certificate as a proof for a “YES” answer. The class ==== contains ==== and it is generally accepted that ==== and ==== are different classes of problems (cf. Garey and Johnson, 1979, Tovey, 2002). A problem is ====-hard if every problem in ==== is polynomial-time reducible to it.====As we already have shown in Yang and Dimitrov (2018) for the case of consent rules without any opinion restrictions, the decision problem when including individuals is ====-hard if ==== and ====, and the decision problem when excluding individuals is ====-hard if ==== and ====. In other words, it is rather difficult for an external authority to control the outcome of a consent rule in the above mentioned way. What we show in the present paper is that, under consecutive opinions with respect to qualification or disqualification, the aforementioned decision problems become polynomial-time solvable. These domains correspond to the prevalent domains of binary matrices (representing the opinions of individuals) being row or column circular (Hsu and McConnell, 2003). Our results are based on integer linear programming (ILP). However, we also derive combinatorial algorithms for consent rules where ==== and for more restricted domains. These special cases where ==== may be appropriate when considering the right of an individual to drive a car in a public domain (cf. Samet and Schmeidler (2003)): a social consent is required at least to some extent when one wishes to drive, while the absence of a wish is usually enough to disqualify the corresponding individual.====The rest of the paper is organized as follows. In Section 2 we formally introduce the class of consent rules and the domain restrictions we focus on. In Section 3, we introduce the two ILPs used to derive our polynomial-time solvability results. Then, we present the ILP-based algorithms for group control problems restricted to the aforementioned specific domains in Section 4. After this, we present combinatorial algorithms for consent rules where one of ==== and ==== is equal to one in Section 5. We summarize our results and point out some important topics for future research in Section 6.",Group control for consent rules with consecutive qualifications,https://www.sciencedirect.com/science/article/pii/S0165489622000890,25 November 2022,2022,Research Article,23.0
"Camacho Franklin,Fonseca-Delgado Rigoberto,Pino Pérez Ramón,Tapia Guido","School of Mathematical and Computational Sciences, Yachay Tech University, Urcuquí, Ecuador,Centre de Recherche en Informatique de Lens (CRIL), CNRS and Université d’Artois, Lens, France","Received 24 November 2021, Revised 22 October 2022, Accepted 22 October 2022, Available online 7 November 2022, Version of Record 17 January 2023.",https://doi.org/10.1016/j.mathsocsci.2022.10.003,Cited by (1),The problem of finding envy-free allocations of ,"The resource allocation problem has been widely studied in mathematics and economics for almost a century,  (Steinhaus, 1948, Steinhaus, 1949, Nash, 1950, Aziz et al., 2018, Caragiannis et al., 2019, Camacho et al., 2020). The main elements in the problem are agents and resources. The goal is to distribute (or allocate) the resources among the agents in a “good manner”. The agents can represent individuals, objects, government institutions, among others, depending on the application. The set of resources or goods to be distributed can be divisible or indivisible. In general, these sets are considered finite. In this work, we study the problem of allocating indivisible resources among a group of agents, with the aim of satisfying both the group and each individual in the best possible way.====This topic has many applications, for instance in solving divorce disputes, dividing an inheritance, sharing apartment rents, or even assigning household chores. In the last decade, there has been a considerable interest in the computational aspects of this problem. In particular, in Artificial Intelligence and more specifically in MultiAgent Systems, these problems are studied and renamed MultiAgent Resource Allocations (MARA) problems (Aziz et al., 2016, Chevaleyre et al., 2017).====Finding a correct distribution of resources consists of distributing all the resources among the agents fairly and efficiently. To establish efficiency and some criterion of fairness, it is necessary to consider the preferences that each agent has over resources. In general, these preferences over resources are established through additive utility functions.====Traditionally, fairness is established through properties such as envy-freeness or proportionality. However, there are situations where it is impossible to find allocations that meet any of these properties. Thus, other weaker versions of fairness, such as envy-free up to one good (Budish, 2011) or proportionality up to one good (Conitzer et al., 2017) are considered. Although there are results that, under certain conditions, guarantee the existence of allocations with some fairness property (see Caragiannis et al. (2019)), finding them is a computationally complicated problem (de Keijzer et al., 2009). Just considering fairness may not be enough, because it could imply loss in group satisfaction.====Efficiency, also known as Pareto efficiency or Pareto optimality, is related to the group satisfaction by an allocation. One way to find efficient allocations is through social welfare functions.  Caragiannis et al. (2019), showed that, under additive utility functions for goods, it is always possible to find an allocation that is Pareto optimal and envy-free up to one good. Actually, they prove that some of the allocations that maximize the Nash social welfare are Pareto optimal and envy-free up to one good. Unfortunately, finding allocations which maximize the Nash welfare is also an NP-hard problem (Ramezani and Endriss, 2010) (see also Aziz et al. (2016)).====Searching allocations which maximize the utilitarian social welfare is in general a more tractable problem from the computational point of view and, because of that, commonly used. A well-known result is that, under additive utility functions, allocations that maximize this social welfare are Pareto optimal (see Theorem 1), although the converse is not true. Moreover, allocations that maximize this social welfare do not always satisfy fairness properties. In Example 2, an allocation that is Pareto optimal and does not maximize utilitarian social welfare is proposed; besides, we find, in this example, that no allocation that maximizes utilitarian social welfare is envy free up to one good.====When we consider additive utility functions, it is possible to define a procedure that allows to find all the allocations which maximize utilitarian social welfare, see Camacho et al. (2020). Moreover, finding these allocations is a computationally tractable problem. Actually, in this work we propose a very simple algorithm in polynomial time, for scenarios of additive utilities, that finds an allocation which maximizes the utilitarian social welfare for goods as well as for chores.====Unlike the allocations that maximize the Nash social welfare which are EF====, the allocations that maximize the utilitarian social welfare are not, in general, EF====. Moreover, there are additive scenarios in which the property EF==== fails for every allocation which maximizes the utilitarian social welfare (see Example 2). Indeed, in this example we can see that Nash and utilitarian social welfares are independent.====Finding an allocation that maximizes the utilitarian social welfare and satisfies the EF==== criteria is an NP-complete problem when the number of agents is greater than or equal to three (see Aziz et al. (2021)). However, for some specialized scenarios, there are very simple algorithms in polynomial time which find EF====, and even EFX allocations. That is the case of identical utilities (see Barman et al. (2018)) and others like bivaluated utilities ( Ebadian et al. (2022)) or binary utilities (Barman et al., 2018) (see Section 6).====In this work, we consider a class of additive functions, called generalized binary utility functions. This class is more general than the classes of binary and identical utilities. Intuitively, each resource has a market-price; each agent either does not want the resource at all, or wants it and values it by its market price. In the framework of these utility functions, the following results are established:====Moreover, we propose a basic algorithm in ==== which finds, under additive utility functions, an allocation that maximizes the utilitarian social welfare (Theorem 6).====The rest of this work is organized as follows. Section 2 introduces the concepts and problems studied in this paper. Section 3 is devoted to a characterization of Pareto Optimality in our particular scenarios. In Section 4, we propose a very simple and tractable algorithm for maximizing the utilitarian social welfare and we study its justice properties in the case of generalized binary utilities. In Section 5, we give a slight generalization of our generalized binary scenarios and prove that most of the results obtained in the previous sections do not hold for this new class of scenarios. Section 6 contains a comparison of our work with other works using specialized scenarios for tackling the problems of fairness and efficiency. We conclude in Section 7 with some final remarks. The proofs, our detailed algorithm and some detailed examples can be found in Appendix.",Generalized binary utility functions and fair allocations,https://www.sciencedirect.com/science/article/pii/S0165489622000798,7 November 2022,2022,Research Article,24.0
Mamada Robert,"College of Humanities and Social Sciences, Grand Canyon University, Phoenix, AZ 85017, USA","Received 13 January 2022, Revised 5 October 2022, Accepted 6 October 2022, Available online 14 October 2022, Version of Record 22 October 2022.",https://doi.org/10.1016/j.mathsocsci.2022.10.002,Cited by (0),"Spence’s theory of signaling shows that signals can resolve adverse selections in job markets. Instead of signals, I consider what Spence calls indices and show that if the costs of knowing the indices are not nil, they could play the role of resolving adverse selection in sequential ==== games. I also show that by incorporating costs that are measured by the mutual information of indices, adverse selection can be resolved if the costs fall within a certain range.","This study reconsiders the market for lemons by referring to what Spence (1973) calls “indices” (observable and unalterable attributes of the players) and shows that the market would not collapse if the cost of learning the indices fell within a certain threshold.====Since the seminal study by Akerlof (1978) was published, many studies have investigated and attempted to resolve adverse selection. For instance, some studies attempt to resolve the adverse selection of used cars (lemons) by introducing the certification of quality and licensing (Heinkel, 1981, Leland, 1979, Mishra et al., 2020, Sultan, 2010, Viscusi, 1978). Others investigate empirical data on used car markets and inquire whether the lemons problem indeed occurs (Bond, 1982, Emons and Sheldon, 2002, Pratt and Hoffer, 1984, Sultan, 2008). Meanwhile, researchers have attempted to resolve the lemons problem by extending the theory of games with asymmetric information (Adriani and Deidda, 2009, Bester and Ritzberger, 2001, Kim, 1985, Levin, 2001, Rose, 1993, Voorneveld and Weibull, 2011, Wilson, 1980). Many of these efforts to resolve adverse selection rely on the concept of signaling, which was first introduced by Spence (1973).====However, Spence mentioned indices. According to Spence, indices are observable and unalterable attributes of the players. Consequently, players cannot do much with these indices. Thus, in his signaling model, Spence argues that indices themselves play no part, and even if indices influence the outcome of the game, it must be through their interactions with the signaling mechanism. Consequently, to the best of my knowledge, researchers have rarely paid attention to the indices and their possible influence on the games of asymmetric information.====However, in our society, there are many indices such as gender, race, age, religion, and national origin, and people seem to be judged by some of these indices when they apply to schools, jobs, and other social/economic activities. For example: Kimura (2020) showed that the gender as an index successfully predicts one’s educational attainment. Consequently, the indices themselves can play a role in the outcome of games with asymmetric information.====Beyond job markets, indices can play a pivotal role in investment. Watanabe (2005) reports that in 19th century Japan, when farmers requested a businessman, named Zenjirō Yasuda, to invest in their rice fields, he would first visit the Shintō shrine and the Buddhist temple of the village to see how well they were taken care of. Only if he found the shrine and temple to be well-maintained, he would negotiate with the farmers. Yasuda recognized the correlation between the quality of the shrine and temple as an index of the diligence of farmers, and, hence, the profitability of rice fields. As this episode suggests, indices could be a tool for us to decide on various investment opportunities, and we should be able to find episodes similar to this case in today’s economic and financial world.====Now, the indices considered by Spence (1973) and Kimura (2020) can be obtained without any cost; that is, they consider one’s gender, and this index is easy to obtain without any cost. However, what if the indices were costly? For instance, consider the used car market. In general, it is difficult to determine which car is a lemon. But what if there were correlation between the quality of, let us say, cars’ air conditioners and the quality of the cars? Here, my assumption is that cars’ air conditioners are hard to manipulate unless one is an auto mechanic; that is, the quality of air conditioners is an index. Let us assume that the cars with “good” air conditioners have “low” probability of being lemons while the cars with “bad” air conditioners have “high” probability of being lemons. In this case, the quality of air conditioners can serve as an index, and knowing the index can reveal information about the quality of the car. My assumption is that knowing the quality of air conditioners is costly, and I postulate that this cost, which is proportional to the reduction in uncertainty after inspecting the index, could resolve adverse selection if the cost falls within a certain range.====The problem of adverse selection (the lemons problem) has been modeled and studied by sequential Bayesian games (dynamic games with incomplete information), and this paper follows that tradition. Thus, several assumptions are commonly shared by the standard model and the model in this study. For instance, this paper assumes that when the seller brings her car to the buyer, the buyer does not know whether that particular car is “good” or “bad” while the seller fully knows the quality of her car as well as the quality of its air conditioner. Additionally, at the beginning of the game, Nature moves first and assigns a certain probability distribution to each type of car. Here, there is a difference between the standard model and the model used in this study. In the standard model, the buyer forms a belief that is consistent with the probability distribution from which Nature makes its choice. However, in current model, there is an additional index that the buyers can acquire to update their beliefs. In real markets, buyers often try to refine their beliefs so that they have a better guess about the quality of the car. At that moment, the amount of information obtained from inspecting the air conditioner is measured by its mutual information, and the cost of mutual information is incorporated into the payoffs of the players. This is where the model in this study is different from the standard model.====Economic analyses by using information theory have been conducted by several economists who propose and use the concept of “rational inattention” (Matějka and McKay, 2015, Sims, 2003). This concept concerns the effects of the cost of obtaining information for players to make decisions. When the information needed to make a decision is expensive to obtain, players may not attempt to obtain complete information because doing so is costly. Instead, players are satisfied with incomplete information and make decisions based on it. In the case of the market for lemons, the buyer could try to obtain as much information as possible by inspecting only the car itself, which could be costly. Thus, the buyer might “stop” inspecting the car at some point, and she might decide whether she should buy the car based on such incomplete information. However, the model proposed in this paper (inspecting the index) is likely to be less costly and can extract more information about the quality of the car because of the correlation between the index and the quality of the car.====The remainder of this paper is organized as follows. Section 2 formally introduces the model analyzed in this study. In Section 3, I analyze the game (the sequential Bayesian game with refinement of the buyer’s belief), as mentioned above. Section 4 concludes the paper.",The market for lemons and information theory,https://www.sciencedirect.com/science/article/pii/S0165489622000786,14 October 2022,2022,Research Article,29.0
"Costa Matheus,Riella Gil","School of Public Policy and Government, Getulio Vargas Foundation, Brazil","Received 13 May 2021, Revised 5 October 2022, Accepted 6 October 2022, Available online 13 October 2022, Version of Record 28 October 2022.",https://doi.org/10.1016/j.mathsocsci.2022.10.001,Cited by (0),"Given a complete, though not necessarily transitive, preference relation, we propose a family of choice representations inspired by the king-chicken procedure, according to which an alternative ==== is chosen among a set of alternatives ==== if, for every other alternative ==== in ====, either ==== is preferred to ==== or there is another alternative ==== in ==== such that ==== is preferred to ====, and ==== is preferred to ====. We generalize this process by allowing the path from ==== to ==== to include more than one alternative ==== and fully characterize the choice correspondences that can be achieved through it. Two of the most relevant tournament solutions, the ==== and the ====, are special cases of this generalized king-chicken choice procedure, so this work improves previous results that have appeared in the choice theory literature by delivering axiomatizations for those models in generic (not necessarily finite) choice spaces.","Complete and not necessarily transitive binary relations, usually called tournaments, are a matter of great relevance in the context of the social choice literature. As discussed in Miller (1977), tournaments represent the outcome of pairwise majority voting when all individual voters have complete and transitive preferences over the set of alternatives under consideration. Since the lack of transitivity may lead to cycles in the majority preference, a problem known as the Condorcet (1785) Paradox, there may exist no maximal element in the consideration set. The implication is that which outcome is chosen will depend strongly on the voting process applied.====The study of nontransitive rationalization is also relevant in the context of individual choice. Both in cases where the individual has multiple selves, understood as multiple preferences over the same set of alternatives, or when the alternatives themselves vary along multiple attributes, the individual choice procedure may give rise to the same voting paradox mentioned above.====Several tournament solution concepts have been proposed seeking to narrow down which alternatives in the feasible set could, or should, be deemed as possible winners. Among them, two are of special relevance: the uncovered set and the top-cycle choice rules.====In this paper, we investigate the ====, also known as ==== solution, and fully characterize the choice correspondences that can be represented by it. Under the generalized king-chicken procedure, the winning alternatives, for a given ====, are those that beat any other available alternative in at most ==== steps. We show that this procedure encompasses both the (deep) uncovered set and the top-cycle tournament solutions when ==== and ====, respectively.====Our characterization is based on two known axioms, Sen’s Gamma, proposed in Sen (1971), and Tournament Consistency, discussed in Smith (1973) and Fishburn (1977), and two axioms depending on ====, (k+1)-Bounded Beta Plus, an adaptation of the Beta Plus axiom from Bordes (1976), and (k+1)-Bounded Weakened Chernoff, which is a version of a postulate that has appeared in Lombardi (2008). Considering previous characterizations of choice correspondences that represent the uncovered set and the top-cycle rules proposed in the literature, we believe that the one presented here has some advantages. Firstly, it builds a bridge between these two well-known solution concepts, and, secondly, as far as we know, it is the first general characterization for these concepts that accommodates the possibility of infinite choice problems.====We dedicate the remainder of this section to presenting some of the related literature and a discussion on the relevance of the generalized king-chicken procedure. In the second section, we present the setup that will be used for our results, while in the third we state the main theorem and the axioms supporting the general king-chicken representation. In Section 4, we explore the special cases of ====, ====, ====, and relate these representations to some other solution concepts that have previously appeared in the literature.",King-chicken choice correspondences,https://www.sciencedirect.com/science/article/pii/S0165489622000774,13 October 2022,2022,Research Article,30.0
"De Marco Giuseppe,Romaniello Maria,Roviello Alba","Department of Management and Quantitative Sciences, University of Naples Parthenope, Via Generale Parisi 13, Napoli 80132, Italy,Center for Studies in Economics and Finance, University of Naples Federico II, Italy,Department of Economics, University Campania Vanvitelli, Corso Gran Priorato di Malta, Capua, Italy","Received 16 January 2022, Revised 17 July 2022, Accepted 29 September 2022, Available online 8 October 2022, Version of Record 22 October 2022.",https://doi.org/10.1016/j.mathsocsci.2022.09.005,Cited by (0),"Psychological games aim to represent situations in which players may have belief-dependent motivations. In this setting, utility functions are directly dependent on the entire hierarchy of beliefs of each player. On the other hand, the literature on ","Psychological games have been introduced to understand how emotions, opinions and intentions of the decision makers can affect a game. In the pioneering paper by Geanakoplos et al. (1989), this goal is tackled by assuming that players may have belief-dependent motivations.==== More precisely, in a psychological game, each player’s payoff depends on his hierarchy of beliefs; that is, it depends not only on what every player does but also on what he thinks every player believes, on what he thinks every player believes the others believe, and so on. Geanakoplos et al. (1989) present an equilibrium concept for this class of games, based on the idea that the entire hierarchy of beliefs of each player must be correct in equilibrium; moreover, they provide an existence result for this notion of equilibrium.====There is another strand of literature that focuses on the issue ==== in classical strategic form games as it is well known that players may have ambiguous (or imprecise) beliefs about opponents’ strategy choices. The classical Nash equilibrium concept is based on two ideas: the first is that each player best responds to the beliefs he has about his opponents’ strategy choices; the second is that his beliefs are correct, that is, every player believes with probability 1 that his opponents follow their equilibrium strategies. In the equilibrium concepts for games under strategic ambiguity already studied in the literature, each player best responds to the beliefs he has about his opponents’ strategy choices but these beliefs are now ambiguous (or imprecise), that is, they can take the form of a capacity or of a set of probability distributions (see for instance Dow and Werlang, 1994, Eichberger and Kelsey, 2000, Lehrer, 2012, Riedel and Sass, 2013, Battigalli et al., 2015 and De Marco and Romaniello (2015) and references therein). In particular, in the concept of ==== (Lehrer, 2012), ambiguity stems from the actual strategies chosen and has a specific structure as Lehrer (2012) states: ==== From the mathematical point of view, this kind of ambiguous beliefs of a player takes the form of a ==== (or ====) that associates with every given profile of opponents’ strategies (that represents the one actually played), a set of profiles of opponents’ strategies that are indistinguishable (to the player) from the given one. In fact, they provide the same expected value for every random variable in a given set, which, in turn, is exogenous and characterizes the information available to the player. So, the set-valued map describes the first-order beliefs that the player perceives to be consistent with the actual play of his opponents, given the information he has and whatever is the actual play. In De Marco and Romaniello (2013) this approach has been slightly generalized by taking into account arbitrary set-valued maps that allow to study other perturbations of correct beliefs. Finally, it is worth noting that the equilibrium notions under strategic ambiguity give back the Nash equilibrium concept in case of no ambiguity; moreover, some limit results provide conditions that guarantee the convergence of sequences of equilibria of ambiguous games to the equilibria of the unambiguous games when ambiguity converges to zero (see De Marco and Romaniello (2013) and references therein).====It is clear that beliefs about opponents’ strategy choices can be regarded as first-order beliefs; from this perspective, the literature on strategic ambiguity substantially looks at games in which first-order beliefs are ambiguous. However, in case payoffs depend explicitly on higher-order beliefs, it is possible that players perceive ambiguity regarding the entire hierarchy of beliefs. For instance, it might be the case that partially specified probabilities (or any kind of partial knowledge) appear directly in the second (or higher) order beliefs; else, in case some player (say John) has partially specified first-order beliefs, then every John’s opponent, having correct second-order beliefs about John’s first-order beliefs, has a set-valued belief as a natural consequence. As strategic ambiguity has been shown to affect equilibria in classical games, the natural question is in which way ambiguous beliefs affect psychological Nash equilibria. This is the key motivation of the present work; we combine these two relevant aspects of strategic interactions: psychological payoffs and ambiguous beliefs. More precisely, we look at the issue of ambiguity in the framework of psychological games by taking into account ambiguous hierarchies of beliefs and adapting the model of psychological games of Geanakoplos et al. (1989) to the ambiguity framework. The idea is that beliefs might be ambiguous (or imprecise) in equilibrium. More precisely, the function that maps strategy profiles to the ==== hierarchies of beliefs, that is used in the classical definition of psychological Nash equilibria, is now replaced by a set-valued map (called ====), that maps strategy profiles to the subsets of those hierarchies of beliefs that players perceive to be consistent with the corresponding strategy profile. Ambiguous belief correspondences provide a general tool to handle ambiguous hierarchies of beliefs and can cover several specific cases such as partially specified probabilities or perturbations of the correct belief function. This will be shown by different examples.====Following the standard approach, agents are assumed to have a pessimistic attitude towards ambiguity as they are endowed with the classical ==== preferences to compare ambiguous alternatives.==== From the mathematical point of view, such ==== preferences (see Gilboa and Schmeidler (1989)) correspond to the maximization (with respect to the strategy of the corresponding player) of a marginal function computed along the graph of the ambiguous belief correspondence whose values, in turn, depend on the entire strategy profile. The equilibrium concept we introduce here, called ====, appears to be the natural generalization of the psychological Nash equilibrium notion in Geanakoplos et al. (1989). We give an existence result for this equilibrium notion that is naturally based on the continuity properties of the ambiguous belief correspondences. We provide also different examples in order to better illustrate this new concept of equilibrium: they will show that even a little (infinitesimal) amount of ambiguity may alter significantly the equilibria of the game. However, the way in which the set of equilibria changes is not unequivocally determined but depends on the specific model. In fact, a first example shows that the set of equilibria might remain unaltered after the introduction of ambiguity, while, in a second example, the set of psychological equilibria under ambiguity is disjoint from the set of classical psychological equilibria. In a further example, ambiguity produces an equilibrium selection, that is, the set of psychological Nash equilibria under ambiguity is a proper subset of the (classical) psychological Nash equilibrium set. In contrast, in the last example, the set of psychological equilibria enlarges when ambiguity (represented by partially specified probabilities) is introduced.====The issue of equilibrium selection that arises from the example previously mentioned relates this work with another relevant strand of literature that concerns the classical theory of refinements of Nash equilibria.==== These equilibrium concepts are based on properties of stability with respect to some kind of perturbations: roughly speaking, an equilibrium is stable if a game nearby has an equilibrium nearby. In the seminal paper by Selten (1975), the ==== concept selects equilibria that are stable with respect to the possibility that players believe that their opponents can make (infinitesimal) mistakes playing their equilibrium strategies: each equilibrium strategy should be ==== to the best reply against perturbed expectations about opponents’ behavior, if the perturbation is small enough. In Geanakoplos et al. (1989) it is considered a notion of trembling hand perfect psychological equilibrium, that is constructed by perturbing the strategies as in Selten (1975) and keeping the hierarchies of beliefs fixed along the perturbation and equal to those that are correct given the unperturbed strategies. In the perfect equilibrium concept considered in Battigalli and Dufwenberg (2009), strategies are perturbed in the same way but now hierarchies of beliefs are perturbed accordingly, as equilibrium beliefs are determined by the strategies via their consistency condition. In the present paper we look at the problem of stability of psychological equilibria from another perspective as perturbations concern the entire hierarchy of correct beliefs and, as the literature on strategic ambiguity suggests, they (can) take the form of sets of hierarchies. However, our approach has an underlying problem that concerns understanding in which way ambiguous beliefs should converge to correct beliefs so that sequences of psychological equilibria under perturbation converge to psychological equilibria of the unperturbed game. We give a general limit theorem that tackles this issue. Then, we show how to construct selection criteria for classical psychological equilibria based on ==== of the correct belief function.====The paper is organized as follows: Section 2 presents the model of psychological games under ambiguity and the equilibrium concept. The examples mentioned above are presented in Section 3. Section 4 focuses on the equilibrium existence theorem. The problem of stability of psychological Nash equilibria with respect to ambiguous trembles is studied in Section 5. Section 6 concludes.",Psychological Nash equilibria under ambiguity,https://www.sciencedirect.com/science/article/pii/S0165489622000750,8 October 2022,2022,Research Article,31.0
Neumann Berenice Anne,"University of Trier, Department IV, Universitätsring 19, 54296 Trier, Germany","Received 25 October 2021, Revised 20 July 2022, Accepted 29 September 2022, Available online 6 October 2022, Version of Record 18 October 2022.",https://doi.org/10.1016/j.mathsocsci.2022.09.006,Cited by (0),"Mean field games allow to describe tractable models of dynamic games with a continuum of players, explicit interaction and heterogeneous states. Thus, these models are of great interest for socio-economic applications. A particular class of these models are games with finite state and action space, for which recently in Neumann (2020a) a semi-explicit representation of all stationary equilibria has been obtained. In this paper we investigate whether these stationary equilibria are stable against model perturbations. We prove that the set of all games with only essential equilibria is residual and obtain two characterization results for essential stationary equilibria.","Mean field games have been introduced independently by Lasry and Lions (2007) and Huang et al. (2006) as a game theoretic model for stochastic games in continuous time with a continuum of players. The main feature of these games is that the players do not observe the other players’ behaviour individually, but only its distribution. These games allow for tractable models of the interaction of a continuum of players with explicit interaction (in contrast to the classical assumption in general equilibrium theory that “prices mediate all social interaction”) as well as heterogeneous states (in contrast to representative agent models).====This led to a large variety of economic applications (see Gomes et al., 2015, Guéant et al., 2011, Caines et al., 2017). In particular, applications with finite state and action space where the dynamics of the individual player are given by a continuous time Markov chain have been considered. These include Kolokoltsov and Bensoussan (2016), Kolokoltsov and Malafeyev (2017), Guéant (2009) as well as Besancenot and Dogguy (2015) and the focus in all these applications lied in the analysis of stationary equilibria. Recently, also a formal model with finite state and action space has been introduced in Doncel et al. (2019), where existence of dynamic equilibria and the relation of these equilibria to Nash equilibria of associated ====-player games are considered. In Neumann (2020a) then the existence of stationary equilibria as well as several tools for the computation of these equilibria has been considered.====We remark that stationary equilibria are of interest for several reasons: The computation of dynamic equilibria is impossible for an infinite time horizon, namely even the underlying individual control problem is intractable (Neumann, 2020b), and for a finite time horizon it is equivalent to solving a forward–backward system of differential equations, which are notoriously intractable (Belak et al., 2021). However, we see (at least in an example) that stationary equilibria are under certain conditions limit objects of dynamic equilibria if the time horizon is large (Kolokoltsov and Malafeyev, 2018). Moreover, we observe that stationary equilibria are limit points of a partially rational learning rule, the myopic adjustment process (Neumann, 2020b).====Now that results regarding existence and computation of stationary equilibria are available and that there is evidence that stationary equilibria are a sensible prediction of agents’ behaviour in these games, a natural next step is to understand what happens to equilibria if the game is slightly perturbed. More precisely, we are interested in essential equilibria, which are equilibria such that any perturbed game that is close to the original game has an equilibrium close to the considered equilibrium of the original game. This notion has been introduced by Wen-Tsün and Jia-He (1962) in the context of normal form games with finite strategy spaces and has also been considered for static games with infinite strategy spaces (Yu, 1999, Carbonell-Nicolau, 2010, Scalzo, 2013), static population games (Correa and Martínez, 2014) and Markov perfect equilibria (Doraszelski and Escobar, 2010).====In the context of mean field games the question whether equilibria are stable given model perturbations has, up to the best knowledge of the author, not been considered so far. The main reason for this is that, except for the case of linear quadratic models, diffusion-based mean field equilibria cannot be determined explicitly, but it is only possible to characterize them as solutions of a forward–backward system of PDEs or SDEs. However, understanding the effect of a possibly discontinuous change (due to jumps in the Hamiltonian) of the right-hand side on the solution is complex. In this paper, the key to analyse the effect of small perturbations is that for the considered mean field game model a semi-explicit characterization of the equilibria is available.====This paper introduces essential stationary equilibria for mean field games with finite state and action space. We prove that the set of essential games, which are all those games where all equilibria are essential, is residual. The proof follows the classical line of argument, namely, we show that the equilibrium correspondence is upper hemicontinuous and that games with only essential equilibria are the points of continuity of this map. The classical theorem of Fort (1949, Section 7) then yields the desired result. Furthermore, we provide criteria to identify essential equilibria. The first is a simple and classical consequence of the genericity statement, namely that unique equilibria are essential. The second is non-standard and relies heavily on the results on equilibrium computation. More precisely, we obtain that equilibria with a deterministic equilibrium strategy are essential when the equilibrium distribution is well-behaved (for example unique or an essential fixed point of an associated map).====Let us conclude the introduction by relating our considerations of essential equilibria to those in traditional game theory: The first two results we obtain (genericity of essential equilibria and that unique equilibria are essential) are standard results also in traditional game theory and also the general idea of the proof using the equilibrium correspondence is usually applied to prove these results in traditional game theory. Yet, there is an important difference between the traditional approach and our approach — the form of equilibria. Here, we consider a pair ====, where ==== is a distribution over states and ==== is a strategy such that ==== is a best response to ==== and ==== is a stationary point of the dynamics given ====. This latter condition is not present in traditional game theory, hence, the proofs have to be modified accordingly and results for which no traditional analogue exists (i.e. the second result) are obtained. In light of the approach in Doraszelski and Escobar (2010) to associate a stationary Markov perfect equilibrium to an equilibrium in a one-shot game and thereafter prove genericity relying on this one-shot game, let us finally mention that due to the condition that ==== is a stationary point of the dynamics given ==== this approach is also not applicable in our setting.====The remainder of the paper is structured as follows: In Section 2 we introduce the considered model and define the notion of an essential equilibrium. In Section 3 we prove that the set of all games with only essential stationary equilibria is residual and that games with a unique stationary equilibrium are essential. In Section 4 we then introduce the second essentiality criterion and in Section 5 we discuss, how these criteria can be applied in examples presented in the literature.",Essential stationary equilibria of mean field games with finite state and action space,https://www.sciencedirect.com/science/article/pii/S0165489622000762,6 October 2022,2022,Research Article,32.0
"Guo Xiaoli,Ryvkin Dmitry","Shanghai Institute of International Organization and Global Governance, Shanghai University of Finance and Economics, Shanghai, PR China,Department of Economics, Florida State University, Tallahassee, FL 32306-2180, USA","Received 3 January 2022, Revised 22 September 2022, Accepted 23 September 2022, Available online 3 October 2022, Version of Record 18 October 2022.",https://doi.org/10.1016/j.mathsocsci.2022.09.003,Cited by (0)," ends up in the superior steady state, even if technology adoption is very difficult for its members. When the open group learns from multiple autonomous groups, the advantage of intergroup herding is more salient. However, if population is bounded, or if adopting the inferior technology generates a negative externality on the other group, intergroup herding becomes less beneficial.","Herding, or learning from popularity, is an important feature of decision making. Under bounded rationality, imitation of beneficial practices is a “simple heuristic” (Gigerenzer and Goldstein, 1996) that has evolutionary advantages for humans and other social animals (Simon, 1990, Surowiecki, 2005). Herding can also emerge as a fully rational response in the form of “information cascades” (Bikhchandani et al., 1992). It has generated convergence of behavior in political movements and other areas such as financial decisions, adoption of innovative technology, fashion and fads (e.g., Lohmann, 1994, McAllister and Studlar, 1991, Morton et al., 2015, Devenow and Welch, 1996, Bikhchandani and Sharma, 2000, Geroski, 2000).====Learning from popularity through imitation of others’ actions can occur within as well as between various social groups.==== Indeed, because ingroup–outgroup bias is ubiquitous in the human society (e.g., Sherif, 1966, Tajfel and Turner, 1979), people often prefer to engage in ==== herding by learning from insiders and downplaying or completely ignoring the knowledge and experience of outsiders (e.g., Katz and Allen, 1982, Buttelmann et al., 2013). For example, people herd along party, ethnic, religious or ideological lines when they decide which political candidates to vote for, what TV shows to watch, or whether to practice social distancing or wear a mask during a pandemic. Yet, ==== herding, whereby people ignore the boundaries of groups and take the behavior of outsiders as a reference, is also a familiar behavioral pattern. For example, as the social meanings of some behaviors change due to law (Lessig, 1995) or meaning entrepreneurs (Granovetter, 1978), actions that have positive meanings are more likely to be imitated, no matter which group they used to symbolize.====Does it pay off to be open to experiences of outsiders? In this paper, we study herding in the context of choosing between two possible actions, or ==== – a status quo technology that is easy to adopt, and a new technology that is harder to adopt. It is not clear ==== which of the two technologies is beneficial. We ask whether intergroup herding is more efficient than intragroup herding in this environment. This is a nontrivial question because, on the one hand, learning from outsiders exposes one to more information; on the other hand, if the other group is converging to an inferior choice, imitating it can be detrimental.====As a motivating example, consider mask wearing during the COVID-19 pandemic, a highly politicized and polarized issue in the United States and around the globe. When COVID-19 first broke out, people had very limited knowledge of the virus and obviously herded along party and ideological lines in their responses. Numerous polls and studies demonstrate that political conservatism predicted lower levels of concern about the virus threat and lower adherence to mask wearing and social distancing guidelines (e.g. Bird and Ritter, 2020, Druckman et al., 2020, Gollwitzer et al., 2020, Gadarian et al., 2021).====Several reasons may account for why people displayed intragroup herding behaviors at least at the early stage of the pandemic. First, the US society was already highly polarized before COVID-19 – conservatives and liberals had different and often opposing opinions on many social, political, and economic issues. Deep-rooted intergroup distrust naturally extended to the public’s responses to the pandemic. Second, polarization of the elites intensified the ingroup–outgroup bias in the public. Conservative elites had publicly downplayed the virus threat, mask wearing, and science. States led by a Republican governor adopted mask mandates much slower, if at all (Adolph et al., 2020). Third, protective behaviors were also subject to peer pressure: To feel belonged, individuals often take cues from their friends, neighbors, and communities, who tend to share similar values and opinions and adopt similar behaviors. Indeed, most people are followers instead of initiators, and make decisions after observing the behaviors and consequences on those around them (e.g. Clark and Kemp, 2008, Orji et al., 2015).====When the pandemic became severe, some of the above conditions began to weaken, the social meanings of mask wearing gradually changed, and intergroup herding seemed to emerge. First, differences in reported social distancing were mediated by divergent perceptions of the health risk posed by COVID-19 (Rothgerber et al., 2020). Conservative counties that exhibited less physical distancing had subsequent higher COVID-19 infection and fatality growth rates (Gollwitzer et al., 2020). Threat undermined partisan reasoning – the effects of political polarization on people’s attitudes about the pandemic and their actions declined in areas with high caseloads (Druckman et al., 2020). Second, as active cases and deaths increased dramatically, more states led by a Republican governor issued mask mandates (Adolph et al., 2020), and more conservative elites began to admit the risk of COVID-19 and showed up wearing a mask. Third, accumulating experimental evidence demonstrates that wearing masks can protect others (e.g. Eikenberry et al., 2020, Prather et al., 2020), and theoretically it protects those wearing them as well (e.g. Miller et al., 2013, Gandhi et al., 2020). Independent from the mandates, people wearing masks were perceived as more prosocial (De Vries, 2013, Betsch et al., 2020), and mask wearing was perceived as a social contract where compliant individuals reward each other and punish non-compliant others (Korn et al., 2020, Betsch et al., 2020).====Intra- and intergroup herding can occur in numerous economic, political and social settings, such as voting, political movements, consumer behavior, recycling, vaccinations, or the spread of innovative technology. In this paper, we use a model of population dynamics to theoretically explore the costs and benefits of intergroup herding. Specifically, we ask when intergroup herding increases an open group’s likelihood of convergence to a superior steady state, or promotes the group’s long-term prosperity. We measure prosperity by a “group size” or, more generally, fitness variable that evolves logistically following the standard approach to population dynamics (Miekisz, 2008, Hastings, 2013). We posit a society composed of two groups we refer to as ==== and ====. The members of the autonomous group rely on internal experience exclusively; whereas the members of the open group learn from both insiders and outsiders.==== Each member in either group chooses from two options, or “technologies”, in every period over an infinite horizon. One option is superior to the other, which, however, is unobservable to the individuals. Instead, an individual makes a decision based on two clues – a private signal received in the current period and the popularity of the two options in the previous period. The two options are asymmetric in that adopting the inferior option is easy, while switching from the inferior option to the superior one is difficult, which models the adoption of a new social norm or adhering to restrictions such as mask wearing or social distancing. The two groups may be characterized by different propensities, or levels of difficulty, in adopting the new technology, and this heterogeneity plays a key role. For example, it may be easier for liberals to adhere to government-imposed restrictions because they generally trust the government more than conservatives.====We analyze the evolution of technology adoption and population sizes of the two groups and focus on two nontrivial cases. In the first case, the autonomous group converges to the inferior steady state, while the open group would have converged to the superior steady state had it been autonomous. Can the open group “escape” from being dragged towards the inferior state by the autonomous group? We find that when the quality of the private signal is sufficiently high or it is relatively easy for individuals to switch to the superior technology, the open group manages to accumulate a good amount of members who adopt the technology early on. This accumulation mitigates the negative influence of the autonomous group as it converges to the inferior steady state, which helps the open group to get back on track. We conclude that intergroup herding can be harmful if the open group is not sufficiently resilient.====In the second nontrivial case, the autonomous group converges to the superior steady state and the open group would have converged to the inferior state if it were autonomous. Here, we show that intergroup herding ==== saves the open group, even when it is very difficult for its members to switch to the superior technology on their own. The reason lies in that as the autonomous group converges to the superior option, its population share, or relative fitness, in the society keeps growing, and its positive influence on the open group is, therefore, increasing over time. When this influence becomes sufficiently strong, the open group will eventually recover, regardless of how poorly its members performed initially. As a result, the open group also succeeds in the long term.====The main takeaway message from this basic model is that, because the positive and negative effects of intergroup herding are asymmetric, its benefits outweigh the costs in a wide range of scenarios. While there is some level of risk in imitating a group that makes bad choices, rather restrictive conditions are required for such imitation to have a long-term negative effect on a group that makes good choices otherwise. In contrast, a group making bad choices always benefits in the long term from imitating an outside group making good choices.====We consider three extensions of the basic model, which relax some of the assumptions and generate richer dynamics. In the first extension, the open group learns from multiple autonomous groups. If all autonomous groups converge to the inferior state, the escape condition for the open group becomes more stringent; however, it is sufficient for at least one autonomous group to be successful for the open group to succeed as well. Similar to the two-group case, this result is driven by the effect of technology adoption on populations. In the second extension, population is bounded even if a group converges to the superior state, and hence the autonomous group’s influence on the open group becomes limited. In this case, the open group’s adoption rate may converge to an interior level due to the negative (positive) influence of an unsuccessful (successful) autonomous group. In the third extension, adopting the inferior technology imposes a negative externality on the population of the other group. If the autonomous group converges to the superior state, this externality disappears, and the open group can always be saved. If, however, the autonomous group fails, the open group may converge to intermediate adoption because of the negative externality. Thus, if population growth is bounded or in the presence of externalities, the benefits of intergroup herding may be limited.",When is intergroup herding beneficial?,https://www.sciencedirect.com/science/article/pii/S0165489622000737,3 October 2022,2022,Research Article,33.0
"Matsui Akihiko,Murakami Megumi","Faculty of Economics, University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan,Department of Economics, Northwestern University, 2211 Campus Drive, Evanston, IL 60208, USA","Received 24 November 2021, Revised 24 August 2022, Accepted 30 August 2022, Available online 29 September 2022, Version of Record 15 October 2022.",https://doi.org/10.1016/j.mathsocsci.2022.08.004,Cited by (0),"We study deferred acceptance algorithm (DA) with retrade by formulating a two-stage model where DA is played in the first stage, and a decentralized market opens in the second. Both non-monetary and monetary retrades are considered. Perfect market equilibrium (PME) is defined: market equilibrium prevails in the second stage both on and off the path, and ==== is played in the first stage game induced by the second-stage markets.====In the economies with no money, the stable and ==== allocation is the truthful PME allocation if and only if the priority structure is acyclical. A pure PME object allocation is unique if and only if the priority structure is unreversed. In the economies with money, an efficient PME allocation exists if and only if the minimum demand across tangible objects exceeds a certain threshold. A pure PME object allocation is always unique.","Mechanism with renegotiation has been studied extensively since the seminal paper by Maskin and Moore (1999). The motivation behind these studies lies in our understanding that no matter how fine a mechanism may be constructed, agents continue to play the game outside the mechanism. If the agents know the possibilities of renegotiation, then they take them into account and behave in a different manner from the situation where they do not know such possibilities. Then some desirable properties of mechanisms, such as strategy-proofness, may no longer hold.====Some analyses focus on a specific mechanism and induce more concrete results.  Hafalir and Krishna (2008) consider asymmetric auctions with resale. The reason that they incorporate resale in auctions may be summarized as follows. In an asymmetric situation, a first-price auction may result in inefficient allocations. This inefficiency creates bidders’ incentives for post-auction retrade. In addition, the seller may not be able to prevent bidders from engaging in the retrade even if resale was deemed disadvantageous. They show, among others, that the revenue equivalence result between first-price and second-price auctions no longer holds.====Similar post-mechanism transactions can be seen when the original mechanism is a matching algorithm instead of an auction. For example, after allocating office spaces by using a matching algorithm, we observe some post-algorithm transactions of the spaces in some universities, ====, the Department of Economics of the University of Tokyo as well as that of Northwestern University.==== The post-matching transactions are not governed by any planner and are carried out in a decentralized manner between the occupants of the spaces.====There are at least two reasons why retrade possibilities should be considered explicitly. The first one is positive. Suppose that post-matching transactions are inevitable. Knowing this possibility, players may have different incentives and misreport their preferences. One of the questions is how the possibility of retrade may distort the allocation achieved in the mechanism.====The second reason is normative. Suppose that the authority has sufficient resources to ban or promote decentralized transactions. Suppose further that the authority wishes to promote decentralized transactions only when it strictly improves welfare. If the allocation determined by the algorithm is Pareto optimal, there is no room for retrade. A possible question is under what conditions retrade strictly improves welfare.====To be more specific, we study deferred acceptance algorithm (DA) with retrade. For the mechanism of the first stage, we use a priority-based indivisible goods allocation model due to Balinski and Sönmez (1999) and Abdulkadiroğlu and Sönmez (2003). As for retrade in the second stage, we introduce a competitive market, both with and without money. Note once again, that this stage of retrade is not part of a mechanism. In this sense, we do not study or propose a two stage mechanism. We revisit this issue in the literature review. Among various mechanisms, we choose DA because DA is known to have some desirable properties such as stability and strategy-proofness. The purpose of the present paper is to investigate the conditions under which these properties of DA are robust against retrade.====If retrade is possible, some properties of DA no longer hold. Consider the truth-telling strategy. If retrade is prohibited as in the existing literature, then the truth-telling strategy is a dominant strategy, ====, the participants have no incentive to deviate from it under any circumstance. However, this is not the case if they can retrade. Indeed, the truth-telling strategy may not be a dominant strategy, and the truth-telling strategy profile may not constitute a Nash equilibrium, either.====Consider the following example of office allocation. There are three players, 1, 2, and 3, and three offices, ====, ====, and ====. The priority structure satisfies ====. The utility function ==== of player ==== is given by Table 1.1. No monetary transaction is allowed.====In this example, the DA outcome under the truth-telling strategy profile is ====. This outcome is Pareto optimal. However, if retrade is possible, Player ==== has an incentive to deviate from the truth-telling strategy, putting ==== on the top of the list. Through this deviation, Players 1, 2, and 3 obtain ====, ====, and ====, respectively, in DA. After this assignment, Player ==== can exchange ==== for ==== with Player 3, and the final allocation after retrade becomes ====.====As illustrated by the above example, the strategy-proofness of DA no longer holds, and the outcome may not be stable: these properties of DA are not robust against retrade. When we introduce the market as a Pareto-improving institution, we cannot take it for granted that a stable allocation is achieved by the truthful reports of the preferences of all the players; rather, the players may strategically misreport their preferences in DA in the presence of opportunities for retrade. In this sense, our analysis on a matching with retrade is different from a simplistic analysis where an outcome in DA is realized in isolation and then improved upon by a decentralized market.====The present paper sheds new light on the study of DA by proposing to evaluate DA based on its robustness against retrade. In the literature, DA has been evaluated based on stability, strategy-proofness, group strategy-proofness, and so on. Existing papers have been concerned with how and when allocations are distorted by unilateral or group deviations. We share the basic concern with the literature on DA because we are concerned with how and when allocations are manipulated in the presence of the future retrade. In this sense, we add a new dimension to the evaluation of DA.====In bold strokes, the present analysis can be described as follows. There are a finite number of indivisible objects and players. There is an object type called the null object. An object type other than the null object is called a tangible object type. Each tangible object type has a limited amount of capacity, called quota. The null object has a sufficient amount of quota.====The first stage is DA. The participants simultaneously submit ordered lists of object types. Each object type is endowed with a priority over the players. The objects are assigned through DA based on priority as well as the submitted lists. Each participant obtains at most one tangible object. The rest obtain the null object.====The market opens in the second stage. The objects assigned in the first stage become the endowments of the second stage. Unassigned tangible objects are no longer available. The players trade objects in the market. Both non-monetary and monetary markets are considered.====The players’ payoffs are determined by the indivisible object and money, if available, held at the end of the second stage. The utility functions of the players are quasi-linear. The payoffs from the objects are generic. In particular, the players have strict preferences over the set of object types. The null object induces zero value for all the players.====As an equilibrium concept, perfect market equilibrium (PME) is defined. PME requires that a market equilibrium should be realized in each second-stage market, and that a Nash equilibrium is played in the first stage game induced by these market equilibrium outcomes. When the players truthfully report their preferences in a PME, such a PME is called a truthful PME. A PME in which the players take pure strategies is called a pure PME. An allocation achieved on the path of PME is called a PME allocation.====The present paper focuses on the analysis of PME. We show the existence of PME first and then examine the conditions under which some properties of DA are robust against retrade. We study the economies with no money and with money separately.====In the economies with no money, the main question is when the two properties, Pareto optimality and the stability, of PME allocations are attained. An allocation is ==== if no player cannot be better off without reducing the payoff of other players. In DA without retrade, the outcome under the truth-telling strategy profile is Pareto optimal if and only if the priority structure is acyclical (Ergin, 2002), while in DA with retrade, any pure PME allocation is always Pareto optimal.====An allocation is ==== if every player prefers an object to her own object only if the object is assigned to the player who has higher priority than her at the object. In DA without retrade, the outcome under the truth-telling strategy profile is always stable (Gale and Shapley, 1962). On the other hand, stability does not necessarily hold if retrade is possible. Two conditions play important roles in relating PME allocations to stability. First, a priority structure is ==== if there are three distinct players ====, ====, and ==== such that ==== has higher priority than ==== at some object type, and ==== in turn has higher priority than ==== at the same object type, while ==== has higher priority than ==== at another object type (Ergin, 2002). A priority structure is ==== if it is not cyclical. The second condition is defined as follows. Two distinct players are said to be ==== if the priority of each of these two players is higher than any other player at every object type. We then say that a priority structure is ==== if all the players except for the top-two players have the same priority order across the object types.====We obtain two equivalence results, which are roughly stated as follows. First, the truthful PME allocation is stable if and only if a priority structure is acyclical. Second, the unique pure PME allocation is stable if and only if a priority structure is unreversed.====In the economies with money, the question is when PME allocations recover efficiency. An ==== allocation is defined as the allocation that maximizes the sum of the players’ utility values among all the feasible object allocations.==== It is shown that the efficient object allocation is uniquely achieved in a pure PME if and only if the minimum demand for each object exceeds a certain threshold.====This paper shares the basic motivation with the literature on mechanism design with renegotiation (Maskin and Moore, 1999, Maskin and Sjöström, 2002). Questions therein include the robustness of the properties of mechanisms against renegotiation. Some analyses focus on a specific mechanism and induce more concrete results. Hafalir and Krishna (2008) consider a two-stage model where an auction is held in the first stage, and the players who obtain the objects through the auction trade them in the second stage. It is shown that the revenue equivalence result for first- and second-price auctions no longer holds. Analogously, the present paper considers a two-stage model where DA is used in the first stage, which is followed by retrade in the second stage. We then find some (necessary and sufficient) conditions under which Pareto optimality, efficiency, and stability are guaranteed in the presence of retrade.====Given the above motivation, we adopt widely used models both for the stage of the centralized mechanism and for that of the decentralized market. The first stage of the present model is based upon the literature on DA, which is adopted from Gale and Shapley (1962), Balinski and Sönmez (1999), Abdulkadiroğlu and Sönmez (2003), Dubins and Freedman (1981), Roth (1982), and Roth and Sotomayor (1989). The model of the present paper formulates the first stage as a game. This formulation is taken from Sotomayor (2008). It enables us to study how the behavior of the participants changes, in anticipation of future retrade.====Both our analysis, especially the model with no money, and existing papers have been concerned with two properties, Pareto optimality and manipulability. First, the literature on DA has been concerned with when Pareto optimal allocations are achieved. Ergin (2002) shows the equivalence between the Pareto optimality of the allocation induced by DA and the acyclicity of the priority structure. The present paper shows that Pareto optimality is always achieved. Second, the literature on DA has been concerned with when allocations are manipulated by unilateral or group deviations. Ergin (2002) shows the equivalence between the group strategy-proofness of DA and the acyclicity of the priority structure. Kojima (2011) relates acyclicity to the immunity of manipulation in a single-stage mechanism. Our first main result states that the truthful PME exists if and only if the priority structure is acyclical. Furthermore, our second main result states that the unique pure PME allocation exists and coincides with the truthful PME allocation if and only if the priority structure is unreversed.====The second stage of the present model is based upon the literature on markets with indivisible objects. In proving the existence of market equilibrium, the present analysis directly uses Shapley and Scarf (1974) for the non-monetary economy and Kaneko and Yamamoto (1986) for the monetary one. In the economy with no money, Shapley and Scarf (1974) show the existence of competitive equilibrium by way of Gale’s top-trading cycle (TTC). In this sense, Gale proposes TTC in the context of a decentralized market in which an initial endowment is given exogenously. Therefore, Gale’s TTC is property-right-based. It is conceptually and technically different from the centralized TTC that allocates goods without property rights.====Abdulkadiroğlu et al. (2020) compare centralized priority-based mechanisms and finds that priority-based TTC is justified envy minimal. In their model, no one holds an endowment at the beginning. The priority-based TTC mechanism directly assigns a Pareto optimal allocation without property rights. In contrast, the present model uses DA to assign property rights first, and then a decentralized market opens in a decentralized manner. Section 3.4 provides an example that shows the difference between the priority-based TTC and DA with retrade.====There are no property rights in Pápai (2000), either. Some individuals initially have multiple objects while others may have nothing. Everyone consumes at most one object. At each stage, TTC runs and people and objects in formed trading cycles leave the algorithm. When an individual who has multiple objects leaves the algorithm, the remaining objects are “inherited” by somebody still in the algorithm. A hierarchical exchange rule determines a new endowment allocating inherited objects. The remaining people are matched with the objects through TTC, given a new endowment at each stage. Thus, the centralized mechanism of TTC together with the hierarchical exchange rule iteratively operates in their model. In our model, nobody has any object at the beginning. DA assigns objects and induces property rights. Each player obtains at most one object. The objects that are not assigned in DA are no longer available for retrade. Thus, there is no “inheritance” governed by the central mechanism in our model.====In the economy with money, Kaneko and Yamamoto (1986) prove the existence of competitive equilibrium by proving the non-emptiness of core under non-transferable utility. Wako (1984) shows that a strong core is inside the set of competitive equilibria and demonstrates the conditions under which a strong core exists.====The present model, especially the monetary part of the model, is related to the literature on property rights’ assignment with resale. First of all, it is related to the Coase theorem (see Coase (1960)). In the present context, the Coase theorem implies that irrespective of the assignment of property rights, the market leads to an efficient allocation. Jehiel and Moldovanu (1999) consider assignment with resale and show that the assignment of property rights is irrelevant if there are resale processes. These papers show that a market is a device that attains an efficient allocation. In contrast, we show that in the economy with money, efficiency is attained if there is sufficient demand for the objects. If the demand is insufficient, however, some objects may remain unassigned, and inefficiency is induced.====In terms of game-structure, our model is related to the literature of dynamic matching. However, since we do not intend to study or propose a centralized mechanism, the motivation of our model is different from theirs. In bold strokes, what dynamic matching models are concerned with is the situation where objects are sequentially, rather than simultaneously, available, and therefore, a one-shot mechanism cannot properly allocate objects. Bloch and Cantala (2013), Kurino (2014), and Kadam and Kotowski (2018) use the same matching algorithm over multiple periods.==== Andersson et al. (2018) construct a two-stage model of school choice in which different centralized mechanisms may be used in different stages. In contrast to these models, the present model considers a situation in which a centralized mechanism to allocate property rights to individuals is followed by a decentralized market that the central authority cannot control. Note that aside from preferences, priority determines the first stage allocation, ====, property rights, while the property rights in turn determine the second stage allocation. In this sense, our work is conceptually different from the literature on dynamic matching models.====The rest of the paper is organized as follows. Section 2 presents the two-stage model and introduces notation and definitions, including the solution concept of perfect market equilibrium (PME). Section 3 investigates the economies with no money. Section 4 analyzes the economies with money. Section 5 concludes the paper. Some proofs and examples are relegated to appendices.",Deferred acceptance algorithm with retrade,https://www.sciencedirect.com/science/article/pii/S0165489622000701,29 September 2022,2022,Research Article,34.0
"Wang X. Henry,Zhao Jingang","University of Missouri-Columbia, United States of America,University of Saskatchewan, Canada","Received 16 April 2022, Revised 22 August 2022, Accepted 16 September 2022, Available online 26 September 2022, Version of Record 12 October 2022.",https://doi.org/10.1016/j.mathsocsci.2022.09.002,Cited by (0), and ====.,"Industrial organization theory has strongly influenced the regulation of horizontal mergers. Competition law and policy have drawn particularly heavily on insights from Cournot (1838) in establishing merger guidelines (Carlton, 2010, Farrell and Shapiro, 2010a, Farrell and Shapiro, 2010b, Nocke and Whinston, 2013). One of the reasons for the reliance on Cournot models is that Bertrand (1883) are not as tractable and many theoretical problems in Bertrand models remain unsolved. This is unfortunate because although Bertrand models probably suit some industry circumstances better than others, and several important results in Cournot models do not extend to Bertrand models. Thus, competition policies and merger guidelines based primarily on insights from Cournot models may be suboptimal or even possibly wrong.====It is known that, in linear symmetric models, Bertrand mergers always increase both the insiders’ and the outsiders’ profits at the expense of consumers, even without generating any cost-savings. It is also known (Motta, 2004) that two-firm mergers will increase consumer surplus and social welfare if merger synergies are sufficiently strong. This paper advances Motta’s study (2004) in three directions: (====) it studies ====-firm mergers in asymmetric linear Bertrand models, (====) it studies non-arbitrary synergy or cost savings measured by the largest difference between insiders’ marginal costs, and (====) it provides a nearly complete picture about the merger effects in asymmetric linear Bertrand models with one efficient firm and (====) identical inefficient firms.====It shows that a Bertrand merger’s main effects are determined by two critical levels for the cost savings (in increasing order, see Table 1): (====) If a merger’s cost saving is small and below the first critical level, including symmetric models, it raises both the insiders’ and the outsiders’ profits and reduces consumer surplus; (====) if the cost saving is between the first and second critical levels, the merger only raises its own profits and reduces not only consumer surplus but also each outsider’s profits; and (====) if the cost saving is sufficiently large and above the second critical level, it raises both own profits and consumer surplus but reduces outsiders’ profit.====The most striking result of this paper is the discovery of Bertrand mergers with moderate cost-savings (i.e., the middle column in Table 1) that benefit the insiders at the expenses of both the consumers and the outsiders. Such Bertrand mergers, even if very small with just two insiders, could have severe anti-competitive effects when they harm all the parties involved except the insiders themselves. Such severely anti-competitive Bertrand mergers are also intriguing because they can never happen in the popular models of Farrell and Shapiro (1990) and Nocke and Schutz (2018) in which consumer surplus and the outsiders’ profits always move in opposite directions.====The rest of the paper is organized as follows: Section 2 provides a brief review of the relevant merger literature; Section 3 introduces the Bertrand model; Section 4 presents analytical tools and preliminary results such as the pre- and post-merger equilibria; Section 5 shows how merger synergies and merger sizes affect merger incentives; Section 6 shows how merger synergies and merger sizes affect consumer surplus and total welfare; and Section 7 concludes with a discussion about implications on competition policy. The proofs are provided in the Appendix.",Merger effects in asymmetric and differentiated Bertrand oligopolies,https://www.sciencedirect.com/science/article/pii/S0165489622000725,26 September 2022,2022,Research Article,35.0
"Ferrasse Jean-Henry,Neerunjun Nandeeta,Stahn Hubert","Aix-Marseille Univ, CNRS, M2P2, Marseille, France,Aix-Marseille Univ, CNRS, AMSE, M2P2, Marseille, France,Aix-Marseille Univ, CNRS, AMSE, Marseille, France","Received 19 April 2022, Revised 18 July 2022, Accepted 2 September 2022, Available online 6 September 2022, Version of Record 23 September 2022.",https://doi.org/10.1016/j.mathsocsci.2022.09.001,Cited by (0),"We analyze the integration of intermittent renewables-based technologies into an electricity mix comprising of conventional energy. Intermittency is modeled by a contingent electricity market and we introduce demand-side flexibility through the retailing structure. Retailers propose diversified electricity contracts at different prices, but in an insufficient number to cover intermittent production. These delivery contracts are modeled similarly to numeraire assets. We study the competitive equilibrium of the state-contingent wholesale electricity markets and the delivery contract markets. We also provide an analysis linking the delivery contracts to social welfare. Finally, we discuss the conditions under which changing the delivery contracts improve penetration of renewables and increases welfare. These provide useful insights for managing intermittency and achieving renewable capacity objectives.","The integration of renewables into the electricity mix is widely accepted as having a significant role to play in decarbonizing the electricity industry. Most particularly, the International Energy Agency reports on the deployment of energy sources as wind and solar helping in increasing renewables-based electricity. Still, their share in the global electricity mix remains quite modest, around 10% in 2021 (IEA 2021). A persisting obstacle to the adoption of these renewables lies in their variable and uncertain nature, collectively referred as ==== (see Perez-Arriaga and Batlle 2012, Verzijlbergh et al. 2017).====Electricity from renewables varies significantly with natural and uncontrollable conditions.==== These render the production process from renewable technologies not only intermittent but also inflexible, technically termed as ====.==== The integration of renewable electricity adds a new source of intermittency on the grid; demand intermittency is a long-existing phenomenon that is mainly managed with investment in dispatchable power plants==== (IEA 2011). Hence, intermittency from renewables challenges the imperative of the electricity industry to constantly balance electricity supply and demand. Disruptions in this balance have both technical and economic impacts.==== Contrary to demand intermittency, this new source of intermittency still needs to be tackled.====In the above context, introducing flexibility in the electricity market is recognized as a solution to managing renewables intermittency (Cochran et al. 2014, Eurelectric 2014, IEA 2011, IEA-ISGAN 2019). Flexibility can be implemented upstream of the market so that supply follows demand. For example, it can be through existing or new flexible power plants, storage capacities (Benitez et al. 2008, Green and Vasilakos 2012, Pommeret and Schubert 2022, Sioshansi 2011) and interconnection (Abrell and Rausch 2016, Yang 2022). It can also be developed downstream through demand-side flexibility that requires demand to follow supply. In this paper, we focus on the latter.====The main objective of our work is to provide an approach to manage intermittent supply by means of demand flexibility through the retail market. The importance of demand flexibility was first highlighted with the question of optimal investment in production capacities to meet intermittent demand. Borenstein and Holland (2005) and Joskow and Tirole (2007) study one weakness of the electricity market to derive optimal investment programs. They point out the disconnection between wholesale prices that vary with electricity provision and retail tariffs such as the flat tariff that do not reflect these changes.==== Hence, consumers are unaware of varying wholesale market conditions including those where expensive power plants are run to meet peak demands. In a framework where demand is intermittent, Borenstein and Holland (2005) and Joskow and Tirole (2007) suggest implementing time-varying tariffs==== to improve efficiency of the market in terms of capacity investment. In practice, time-varying tariffs that have been developed include Time-of-Use (ToU), Critical Peak Pricing (CPP), Variable CPP, and Real-Time-Pricing (RTP). These tariffs are able to shape demand thereby reducing peaks==== and investments in expensive peaking power plants used only for a few hours during a year (IRENA 2019). Since these retail contracts can stabilize demand intermittency, we propose to tap into them and/or use more sophisticated ones to now adjust demand according to intermittent supply. Nevertheless, the contracts should not be too sophisticated, especially for households that do not necessarily react to non-linear electricity prices (see Ito 2014 and Shaffer 2020 for empirical evidence).====Can retail contracts be designed to unlock demand flexibility and ease the integration of intermittent renewable technologies? As part of their work, Ambec and Crampes (2012, 2021) and Rouillon (2015) address this question by studying the optimal investment in renewable capacity in a framework with intermittent supply. While Ambec and Crampes (2012) consider that consumers can use either a flat retail tariff or one that varies with the availability of the intermittent source of energy, Ambec and Crampes (2021) and Rouillon (2015) consider that there is a mix of both consumers. In this paper, we study a more general situation where retailers propose a large set of flexible delivery contracts.====To investigate the above question, we have in mind a theoretical framework of capacity investment and electricity production with two types of energy source: an intermittent, non-dispatchable source such as wind or solar and a non-intermittent, dispatchable source such as nuclear or fossil fuel. We propose that electricity production due to the integration of the intermittent renewable technology depends on conditions such as weather (e.g. “with”or “without”the intermittent source) or times of day (night, dawn, daytime, and dusk). We refer to these conditions as states of nature. Electricity production is therefore state-contingent. We consider that the wholesale market is a contingent one where contingent electricity is offered at contingent prices. We further propose that retailers introduce demand flexibility in the retail market through diversified electricity delivery contracts that are supplied at different prices. These allow consumers to choose their optimal electricity consumption based on their flexibility. The diversity of the delivery contracts is depicted through what we call ====. Their structure is similar to the asset structure in the incomplete market theory (see, for instance, Magill and Quinzii 2002). These base delivery contracts can well generate contracts with flat and time-varying tariffs, but one can also think of more complex contracts depending on weather conditions or on the pressure on the wholesale markets. In this paper, we provide a general model for the structure of the base state-contingent electricity delivery contracts.====In a competitive setting, we study the equilibrium of the state-contingent wholesale and different retail markets where the optimal investment in intermittent renewable capacity is endogenous. We assume that the structure of the base state-contingent delivery contracts is not rich enough to generate retail delivery contracts that allow perfect adjustment of demand to variations in supply. This lack of richness can be explained, for instance, by variations in conditions and thereby electricity supply at a level of granularity that is too fine to incite a response from consumers. Nevertheless, we show that the electricity market equilibrium and social welfare are constraint efficient. The constraint is induced by the limited number of base delivery contracts that constrains electricity allocations. We are also able to determine the conditions under which changing the base delivery contracts improves (i) welfare, (ii) the degree of integration of the renewable capacity, and (iii) both. Ultimately, we find that it is impossible to find a change in the base delivery contracts structure that both increases investment in renewable capacity and reduces the production of conventional electricity in each state of nature. Nevertheless, it may occur that the average conventional production decreases.====The rest of the article is organized as follows. Section 2 presents the theoretical framework and the main assumptions. In Section 3, we describe the electricity contract markets and derive some useful properties of electricity demand. Section 4 studies the state-contingent electricity supply and describes the market equilibrium. Section 5 provides a welfare analysis. In Section 6, we study the impact of changing the base delivery contracts on social welfare, investment in intermittent renewable capacity, and conventional production. Section 7 concludes. Technical proves are relegated to the appendix.",Intermittency and electricity retailing: An incomplete market approach,https://www.sciencedirect.com/science/article/pii/S0165489622000713,6 September 2022,2022,Research Article,36.0
Bu Nanyang,"SILC Business School, Shanghai University, Shanghai, China,Business School, University of Technology Sydney, Sydney, Australia","Received 16 December 2021, Revised 21 June 2022, Accepted 7 August 2022, Available online 17 August 2022, Version of Record 5 September 2022.",https://doi.org/10.1016/j.mathsocsci.2022.08.002,Cited by (0),"We study the problem of assigning objects. There may be multiple copies of each object. Each agent is assigned at most one. Monetary transfer is not allowed. We require a rule to be fair and efficient. We introduce an axiom that is a natural weakening of ====. We call it “bounded no-envy”. It states that for each pair of agents who receive objects of the same rank in their respective preferences, each of them should find his assignment at least as desirable as the other’s. ==== is compatible with ====. In particular, the immediate acceptance rules (a.k.a. the “Boston mechanisms”) satisfy both. Our main result is that the immediate acceptance rules are the only rules satisfying ====, and ====.","We consider a group of agents who collectively own a set of “objects”, and each has a strict preference relation over them. We call the collection of preference relations the “preference profile”. There is a number of copies of each object, which we refer to as the “supply” of it. A “problem” is a pair of a preference profile and a supply vector. At an “allocation”, each agent receives at most one object. A rule maps each problem to an allocation. Our objective is to find “fair” and “efficient” rules.====A standard fairness property is “no-envy”, the requirement that no agent should ever prefer any other’s assignment to his own. A standard efficiency property is “Pareto efficiency”. Consider a problem and the allocation chosen for it by a rule; there should be no other allocation that “Pareto dominates” it, i.e., each agent is at least as well off and some agent is better off. A mild form of efficiency is “weak non-wastefulness”, the requirement that there should be no remaining copy of an object that an agent prefers to his assignment. Disappointingly, no-envy is not compatible with weak non-wastefulness, let alone Pareto efficiency. Since no rule satisfies no-envy and Pareto efficiency, to attain one of the two properties, we need to weaken the other.====Kesten and Yazıcı (2012) insist on no-envy. They introduce the “unrestricted fair rule”, which they show to be the Pareto dominant one among all rules that satisfy no-envy. However, it is clear from the discussion above that the unrestricted fair rule is not weakly non-wasteful. No-envy is demanding here because the resources to be allocated are indivisible: if all agents’ preference relations are the same, then it may not be feasible to assign to each of them his top object; as a consequence, an agent who is not assigned his top object will prefer the assignment of another who does.====We insist on Pareto efficiency, and hence we need to weaken no-envy. We appeal to the simple idea of imposing no-envy conditional on ranks: for each pair of agents, if their assignments’ ranks in their respective preferences are the same, then each of them should find his assignment at least as desirable as the other’s. We refer to the above requirement as “bounded no-envy”. Bounded no-envy and Pareto efficiency are compatible. Indeed, the “immediate acceptance rules” a.k.a. the “Boston mechanisms”, satisfy both.====It is worth mentioning that our problem is not the “school choice” problem. In school choice, the only and key difference is that each object is equipped with a priority order over the agents. We refer to the collection of the priority orders as the “priority profile”. The immediate acceptance rule, which is defined on the basis of the priority profile, is widely used in practice (Abdulkadiroğlu and Sönmez, 2003).==== For our model, the priority profile is not a primitive. Yet, we want to adapt it to our setting. We do so by letting a priority profile be a characteristic of a rule. Since each rule is indexed by a priority profile and there are more than one priority profiles, we use the plural form and say “rules”.====We are not the first to propose a requirement that resorts to objects’ ranks in agents’ reported preferences. Kojima and Ünver (2014) introduce two such requirements. The first is called “favoring higher ranks”. It states that if an agent prefers some object to his own assignment, then all copies of the first object have been assigned to agents who rank it at least as high as he does. Favoring higher ranks implies both bounded no-envy and Pareto efficiency. The second is called “rank-respecting invariance”, which is a weakening of “Maskin monotonicity”. Consider a problem and the allocation chosen by a rule. Let a preference profile be such that each agent’s lower contour set of his initial assignment either expands or remains the same. Maskin monotonicity requires that the same allocation should be chosen for the new problem. Rank-respecting invariance is defined with an additional hypothesis: under the new problem, for each agent, the set of agents who are “competing” for his initial assignment either shrinks or remains the same.====We also consider solidarity and consistency requirements. The first is “resource monotonicity”, the requirement that if supplies increase, each agent should end up at least as well off as he was initially. Next is “bilateral consistency”: suppose all but two agents leave with their assignments; then, when distributing among the remaining two agents the remaining objects, the rule should still assign to each of them what he received initially.====Kojima and Ünver (2014) characterize the immediate acceptance rules: they are the only ones satisfying favoring higher ranks, rank-respecting invariance, resource monotonicity, and consistency. Favoring higher ranks is an important axiom of welfare implications. We show that for each problem, favoring higher ranks alone characterizes the set of allocations chosen by the immediate acceptance rules (Theorem 2).====Our main result is that the immediate acceptance rules are the only ones satisfying bounded no-envy, weak non-wastefulness, rank-respecting invariance, resource monotonicity, and bilateral consistency (Theorem 3).==== As compared to Kojima and Ünver’s characterization, ours dispenses with favoring higher ranks, and includes bounded no-envy and weak non-wastefulness; also, we replace consistency with bilateral consistency. Bounded no-envy and favoring higher ranks are logically related (discussed in Sections 3.2 Two axioms based on ranks, 3.3 Solidarity and consistency). Nevertheless, we present a direct proof via the “Elevator Lemma” (Thomson, 2010).====Our characterization is a complement to Kojima and Ünver’s (2014). They are concerned about the welfare property (favoring higher ranks). We show that besides efficiency, the immediate acceptance rules also satisfy certain degree of fairness (bounded no-envy). Kojima and Ünver also consider a more general setting that under a rule, some agent is not qualified to receive some object, i.e., the agent is not assigned the object for any problem. By incorporating this possibility, they characterize a larger family of rules that are counterparts of the immediate acceptance rules. Their setting allows for more applications at the cost of efficiency.==== Since we insist on Pareto efficiency, we implicitly assume that all agents are qualified for all objects.",A new fairness notion in the assignment of indivisible resources,https://www.sciencedirect.com/science/article/pii/S0165489622000622,17 August 2022,2022,Research Article,37.0
"Kirsch Werner,Toth Gabor","FernUniversität in Hagen, Germany,IIMAS-UNAM, Mexico City, Mexico","Received 10 February 2022, Revised 16 July 2022, Accepted 1 August 2022, Available online 6 August 2022, Version of Record 19 August 2022.",https://doi.org/10.1016/j.mathsocsci.2022.08.001,Cited by (0),"We analyse optimal voting weights in two-tier voting systems. In our model, the overall population (or union) is split in groups (or member states) of different sizes. The individuals comprising the overall population constitute the first tier, and the council is the second tier. Each group has a representative in the council that casts votes on their behalf. By ‘optimal weights’, we mean voting weights in the council which minimise the democracy deficit, i.e. the expected deviation of the council vote from a (hypothetical) popular vote.====We assume that the voters within each group interact via what we call a local collective bias or common belief (through tradition, common values, strong religious beliefs, etc.). We allow in addition an interaction across group borders via a global bias. Thus, the ==== of each voter depends on the behaviour of all other voters. This correlation may be stronger between voters in the same group, but is in general not zero for voters in different groups.====We call the respective voting measure a Collective Bias Model (CBM). The ‘simple CBM’ introduced in Kirsch (2007) and in particular the Impartial Culture and the Impartial Anonymous Culture are special cases of our general model.====We compute the optimal weights in the large population limit. Those optimal weights are unique as long as there is no ‘complete’ correlation between the groups. In this case, we obtain optimal weights which are the sum of a common constant equal for all groups and a summand which is proportional to the population of each group. If the correlation between voters in different groups is extremely strong, then the optimal weights are not unique. In fact, in this case, the weights are essentially arbitrary. We also analyse the conditions under which the optimal weights are negative, thus making it impossible to reach the theoretical minimum of the democracy deficit. This is a new aspect of the model owed to the correlation between votes belonging to different groups.","We study voting in two-tier voting systems. Suppose the population of a state or union of states is subdivided into ==== groups (member states for example). Each group sends a representative to a council which makes decisions for the union. The representatives cast their vote (‘aye’ or ‘nay’) according to the majority (or to what they believe is the majority) in their respective group. Since the groups may differ in size, it is natural to assign different voting weights to the representatives, reflecting the size of the respective group. When a parliament such as the House of Representatives in the U.S. is elected, usually the country is subdivided into a number of districts of roughly equal population, each of which votes on a representative for a single seat. This procedure is feasible within a country but may not be possible in other situations. Even in the U.S., no effort has been made to divide the states and reassemble them into districts of roughly equal size so that each of them could have the same number of senators without giving rise to questions whether that is the ‘right’ way to determine the number of senators. It is even less likely that sovereign countries – such as the members of the United Nations or the European Union – would be willing to submit to being divided into districts of equal size. Thus, it is not possible in practice to circumvent the question of how to assign voting weights to groups of different sizes.====To determine these weights is the problem of ‘optimal’ weights. How should the weights be assigned? One objective studied in the literature is to minimise the democracy deficit, i.e. the deviation of the council vote from a hypothetical referendum across the entire population. The democracy deficit was first studied for binary voting (the same setting which is considered in the present article) by Felsenthal and Machover (1999). Later on, it was also analysed in other settings by several authors (see e.g. Felsenthal and Machover, 1998, Kirsch, 2007, Zyczkowski and Słomczyński, Kirsch and Langner, Langner, 2012, Maaser and Napel, 2012, Toth, 2020). Other notions of optimal weights are based on welfare considerations or the criterion of equalising the influence of all voters belonging to the overall population. In the latter vein, we find the seminal article by Penrose (1946), where the square root law was first established as the assignation rule for voting weights that equalises the probability of each voter’s being decisive in a two-tier voting system under the assumption of stochastically independent voting. Other contributions to the study of optimal voting weights under welfare and influence frameworks can be found in Maaser and Napel, 2012, Beisbart and Bovens, 2007, Koriyama et al., 2013, Kurz et al., 2017. Correlated voting across groups was also analysed by Kaniovski and Zaigraev in Kaniovski and Zaigraev (2011).====Suppose the overall population is of size ====, whereas the group size is ====, where the subindex ==== stands for the group ====. Let the two voting alternatives be encoded as ====, ==== for ‘aye’ and ==== for ‘nay’. The vote of voter ==== in group ==== will be denoted by ====.====Each group casts a vote in the council:====The (representative of) group ==== votes ‘aye’ if there is a majority in group ==== on the issue in question. Each group ==== is assigned a weight ====. The weighted sum ==== is the council vote. The council vote is in favour of a proposal if ====. Weights ==== together with a relative quota ==== constitute a weighted voting system for the council, in which a coalition ==== is winning if ====We will exclusively consider the majority rule with ==== in this article.====It is reasonable to choose the voting weights ==== in the council in such a way, that the raw democracy deficit ====is as small as possible. For a given set of weights, each configuration of all ==== votes induces a certain raw democracy deficit. It is immediately clear that in general there is no choice of weights which makes this variable small uniformly over all possible distributions of Yes-No-votes across the overall population. All we can hope for is to make it small ‘on average’, more precisely we try to minimise the expected quadratic deviation of ==== from ====.====To follow this approach, we have to clarify what we mean by ‘expected’ deviation, i.e. there has to be some notion of randomness underlying the voting procedure.====While the votes cast are assumed to be deterministic and rational, obeying the voters’ preferences which we do not model explicitly, the proposal put before them is assumed to be unpredictable, i.e. random. Since each yes/no question can be posed in two opposite ways, one to which a given voter would respond ‘aye’ and one to which they would respond ‘nay’, it is reasonable to assume that each voter votes ‘aye’ with the same probability they vote ‘nay’.====This leads us to the following definition:====The simplest and widely used voting measure is the ====-fold product of the measures ====which models independence between all the individual votes ====. In this special case, known as the ==== (see e.g. Garman and Kamien, 1968, Gehrlein and Lepelley, 2017, or Kurz et al., 2021), we have ====This article treats the class of voting measures called the ==== (or common belief model, CBM) which extends the Impartial Culture considerably by allowing correlations both between voters in the same group as well as correlations across group borders. We introduce and discuss the CBM in Section 3.====Once a voting measure is given, the quantities ====, ====, ====, and the raw democracy deficit are random variables defined on the same probability space ====.====Now we can define the concept of democracy deficit which is a measure of how well the council votes follow the public opinion:====Note that the democracy deficit depends on the voting measure. It is also worth pointing out that the democracy deficit is a differentiable function of the council weights. This facilitates the analysis required to find the optimal weights.====Instead of minimising the democracy deficit, we could ask the question of how to minimise the probability that the binary council decision differs from the decision made by a referendum. This would be a less strict criterion in the sense that for a favourable public opinion of 51%, a 51% percent vote in the council and a 100% vote would both be considered equally good. However, one could argue that a 100% vote in the council would not be a good representation of public opinion. In fact, the 49% minority might feel they are not represented in the council at all, giving rise to populist anti-elite sentiment among the minority. We argue that adjusting the voting outcomes in the council in such a way that they follow the popular opinion as closely as possible is a worthwhile goal.====If we multiply each weight by the same positive constant and keep the relative quota ==== fixed, we obtain an equivalent voting system. If the weights ==== minimise the democracy deficit ====, then the (equivalent) weights ==== minimise the ‘renormalised’ democracy deficit ==== defined by ====It is, therefore, irrelevant whether we minimise ==== or ==== as long as ====. In this article, we will compute optimal weights as ==== tends to infinity. As a rule, in this limit the minimising weights for ==== will also tend to infinity, it is therefore useful to minimise ==== with an ====-dependent ==== to keep the weights bounded. A particularly convenient choice is to normalise the weights ==== in such a way that ====.====The rest of this paper is organised as follows: as a first step, in Section 2, we recall the CBMs with independent groups studied in the past and give an example of a CBM with correlated voting across group boundaries. Then, we formally define the CBM and give several more examples in Section 3. In Section 4, we discuss the problem of determining the optimal weights in order to minimise the democracy deficit. Section 5 contains the results concerning the large population behaviour of CBMs. In Sections 6 Optimal weights, 7 Optimal weights for tight correlations, we calculate the optimal weights in the large population limit. Then, Section 8 discusses the optimal weights for some specific models introduced earlier, such as additive and multiplicative models. Sections 6 Optimal weights, 7 Optimal weights for tight correlations, 8 Specific models contain the main results of this article concerning the optimal weights for a large set of CBMs. The second part of our analysis of optimal weights concerns their non-negativity. Under independence of the groups, the optimal weights are always strictly positive. Thus, this is a new aspect owed entirely to the relaxation of the independence assumption and not previously analysed in the literature. Section 9 deals with the problem of negative optimal weights and conditions that rule them out. Section 10 presents an extension of the CBM with non-identical group bias distributions. Finally, Section 11 concludes the paper.",Collective bias models in two-tier voting systems and the democracy deficit,https://www.sciencedirect.com/science/article/pii/S0165489622000610,6 August 2022,2022,Research Article,38.0
"Tramontana Fabio,Ventura Marco","Sapienza University of Rome, Italy,London South Bank University, UK,GRANEM, University of Angers, France,DESP, University of Urbino, Italy","Received 19 April 2022, Revised 21 July 2022, Accepted 26 July 2022, Available online 3 August 2022, Version of Record 13 August 2022.",https://doi.org/10.1016/j.mathsocsci.2022.07.003,Cited by (0),"We introduce a ==== to model the complex interaction between COVID-19 and economic activity. The model introduces some novelties not accounted by SIR-like models. The equilibrium of the system is an unstable focus, with fluctuations having increasing size and periodicity. Numerical simulations of the model produce waves which reproduce the pandemic dynamics. In observing the stylized facts linking economics and pandemic and stating related reasonable assumptions, we obtain a Lotka–Volterra co-dynamics. This outcome is confirmed by extensive simulations. The outcomes obtained qualitatively replicate some important stylized facts deepening the knowledge about the role of some parameters in their origin and eventually in their shaping.","Over the recent months scientists, researchers and policy makers have devoted considerable efforts to study and face complex and unprecedented problems raised by the COVID-19 outbreak. Among the many, one unsolved issue is the contrasting interplay between pandemic and economy. The trade-off between halting the virus spread and halting economy has been largely debated taking the issue directly and/or indirectly. Among the former we find articles dealing with optimal macroeconomic policies (====, ====, ====), distributional consequences of macroeconomic policies (====), real business cycles and epidemiology models (====, ====) and the literature therein, just to cite some of the most representative works of this copious strand of the literature. Among the latter, we find studies focused on the heterogeneous effects produced by shelter-in-place policies (====, ====, ====), the economic determinants of timing and intensity of the policy reaction to pandemic (====), and the cost of strict policy measures, such as the lockdown. In turn, this cost has been declined in terms of job losses (====), changes to consumers behaviour (====), fairness and cooperation (====), uncertainty and expectations (====), externalities (====) and GDP (====).====This paper moves from this premise and provides a model for the formal description of the interplay between economic activity and COVID-19 diffusion. Until few years ago, epidemiology and economics have long operated in distinct silos (====). The modelling of the interactions between the spread of a pandemic and economic activity is a recent topic, emerged as a consequence of the COVID-19 pandemic.==== Most of these models are compartmental models, originated from the work of ====, where economic variables have been inserted into a SIR (Susceptible–Infected–Removed) model or into one of its numerous augmented versions (SEIR models, with the addition of Exposed; SEIRD models, with also Deceased, and so on).====We move from some empirical examples to show that some features of the interplay between economy and COVID-19 resemble the classical dynamics obtainable by a Lotka–Volterra type model. For this reason, we present a new theoretical model from which several interesting results emerge. First, our model leads to a waves-shaped dynamics for the pandemics. This is very close to the empirical evidence of the sequence of waves, defined as an increasing number of infected individuals followed by a decrease of such a number (for a formal definition of waves, see ====).====Third, the dynamics of the two variables obtained from stylized facts and reasonable assumptions make our model consistent with a Lotka–Volterra prey–predator model. This outcome is confirmed also from the simulations. With respect to the existing literature this is a novelty because while some models assume the Lotka–Volterra co-dynamics (====, ====) our analysis attains it as a result of the preliminary analysis of the considered economic-pandemic context.====Fourth, from an empirical viewpoint we show that economic activity and new cases seem to be strictly related in the initial phase of the pandemic with waves of increasing amplitude. This is attributable to the fact that the virus can still affect a quite large number of ==== in a preliminary phase when no restrictions were into place and in absence of vaccines. The model captures this fact. As time goes by, the introduction of vaccines changes the whole story, so that governments do not need any stronger intervention to limit the spreading of the virus; it follows that the co-evolution of economics and epidemics intrinsically changes. We give full account for these aspects by showing that a structural break occurred in the real world time series. Furthermore, one unpleasant characteristic of COVID-19 outbreak is that a recovered individual can be affected again. In terms of stylized facts, this feature works as if recovered do not exist. On this particular stylized fact, our model can be of great usefulness to describe the inception of the pandemic. Interestingly, the outcomes obtained qualitatively replicate some important stylized facts deepening knowledge about the role of some parameters in their origin and eventually in their shaping.====The remainder of the paper proceeds as follows. Section ==== presents a motivating example showing evidence for the US case and refers to the Appendix for further evidence about other countries. Section ==== outlines the dynamical system model capable of accounting for the empirical features emerged in Section ====. Section ==== provides a study of the map, Section ==== elaborates on the economic implications of the results and finally Section ==== concludes.",The complex interplay between COVID-19 and economic activity,https://www.sciencedirect.com/science/article/pii/S0165489622000609,3 August 2022,2022,Research Article,39.0
Schweizer Urs,"Department of Economics, University of Bonn, Kaiserstr. 1, 53113 Bonn, Germany","Received 25 January 2022, Revised 9 June 2022, Accepted 25 July 2022, Available online 30 July 2022, Version of Record 15 August 2022.",https://doi.org/10.1016/j.mathsocsci.2022.07.002,Cited by (0),". The multiplier of the incentive constraint if understood as shadow price allows for an intuitive explanation of the results. As a second step, a comparative statics analysis with respect to the incentive threshold is provided. Surprisingly enough, the relation between the threshold and minimum error costs need not be monotonic.",None,Negligence rules coping with hidden precaution,https://www.sciencedirect.com/science/article/pii/S0165489622000592,30 July 2022,2022,Research Article,40.0
Wang Xingtang,"Institute of Studies for the Great Bay Area, Guangdong University of Foreign Studies, Guangzhou, PR China","Received 30 December 2021, Revised 15 July 2022, Accepted 18 July 2022, Available online 26 July 2022, Version of Record 6 August 2022.",https://doi.org/10.1016/j.mathsocsci.2022.07.001,Cited by (0),"In this paper, we introduce input price discrimination into vertical product differentiation model to analyze the impact of input price discrimination on social welfare. We find that the input price discrimination improves the supply of high-quality products in the market, and lead to the increase of ====. The effect of input price discrimination on social welfare is influenced by the pricing contract of input. When the price of input is determined by upstream firm, the input price discrimination reduces social welfare. While the input price discrimination increases social welfare when the price of input is determined by bargaining of upstream and downstream firms.","Input price discrimination means that the upstream firm selling the same input at different prices to the downstream firms. Input price discrimination is prevalent in many markets. For example, in oil, steel, chips, engines and other fields. There are many reasons for the input price discrimination, such as the difference in demand and production cost in the downstream market. Under different reasons, input price discrimination has different effects on social welfare. Upstream firm set lower input prices in price-sensitive markets and higher input price in price-insensitive downstream markets (Varian, 1985, Ireland, 1992). Such price discrimination may lead to new markets for products and services, leading to improved social welfare. If the production costs of downstream firms are different, the upstream firm will set higher input price for the downstream firms with high production efficiency and low production costs, and such input price discrimination will bring about the decline of social welfare (DeGraba, 1990, Yoshida, 2000). The reason is that input price discrimination makes the production capacity of firms with high production efficiency transfer to firms with low production efficiency, resulting in the decline of social welfare.====The literature has long debated the welfare effects of price discrimination in input markets. The classic papers of Katz (1987) and Yoshida (2000) have even demonstrated that allowing input price discrimination reduces welfare, because efficient firms then face higher input prices and outputs are reallocated to less efficient firms. Since then, this result has been extensively examined and shown to be not robust in various contexts, for if nonlinear pricing is feasible (Inderst and Shaffer, 2009), in the presence of input substitutability (Inderst and Valletti, 2009), if there are multiple markets (Arya and Mittendorf, 2010), if entry is possible (Herweg and Müller, 2012, Dertwinkel-Kalt et al., 2016), if downstream firms have bargaining power (O’Brien, 2014), if downstream firms produce quality differentiated products (Brito et al., 2019), if firms can invest in acquiring alternative inputs (Akgün and Chioveanu, 2019), if there is upstream R&D (Pinopoulos, 2020), in the presence of cost convexity (Chen, 2022) and vertical shareholding (Lestage, 2021). Many scholars have found that input price discrimination has a negative impact on social welfare (Valletti, 2003, Villas-Boas, 2009). Villas-Boas (2009) find that a ban on input price discrimination has a positive effect on social welfare, but the reduction of cost difference and competition will reduce the welfare effect. However, some scholars believe that input price discrimination leads to improved social welfare (Kim and Sim, 2015, Pinopoulos, 2020, Chen, 2022). In the case of upstream R&D investment, a ban on input price discrimination is detrimental to social welfare. The above studies mostly assume that downstream firms produce the same quality of product. What is the social welfare effect of input price discrimination when downstream firms produce different product quality? Brito et al. (2019) find that if the quality gap is sufficiently high between the two downstream firms, input price discrimination will lead to the improvement of social welfare.====Under vertical market structure, the influence of input price contract has been widely concerned. For example, scholars have analyzed the mechanism of input pricing contract in the comparison of competition modes (Alipranti et al., 2014, Basak and Mukherjee, 2017) and the formulation of managerial delegation contract (Wang and Wang, 2021). When the upstream firm completely decides the price of inputs, the decision-making of the upstream firm is aimed at maximizing its own profit. When the bargaining is introduced in the two-part tariff contract, the price of inputs is determined with the goal of maximizing joint profits, and then the joint profits are divided according to the bargaining power. Under different input pricing mechanisms, will the impact of input price discrimination on the market change? This is the key question to be answered in this paper.====In the vertical product differentiation model, we introduce input price discrimination and analyze the impact of input price discrimination on social welfare under different input pricing contracts. We find that the input price discrimination improves the supply of high-quality products in the market, and lead to the increase of consumer surplus. The effect of input price discrimination on social welfare is influenced by the pricing contract of input. When the price of input is determined by upstream firm, the input price discrimination reduce social welfare. While the input price discrimination increases social welfare when the price of input is determined by bargaining of upstream and downstream firms.====The article most closely related to this paper is Chen (2017) who consider two common contracts: linear and non-linear price contracts. However, there are the following differences with Chen (2017). The first is the difference in terms of research thrust. We mainly analyze how the difference in pricing contracts of inputs affects the result of price discrimination of upstream firm. Secondly, in terms of model setting. Chen (2017) assumes that there is a fixed marginal cost difference between the two downstream firms, while we assume that the marginal production cost of the downstream firms is related to quality. In the nonlinear pricing contract, we assume that upstream and downstream firms determine the price of input through bargaining which is different from Chen (2017) who considers that the upstream monopolist offers each downstream firm a take-or-leave-it price contract.==== Finally, in the conclusion of the article, Chen (2017) finds that when the upstream firm charges a unit wholesale price for input product, discriminatory pricing could be socially desirable, while if a two-part tariff is feasible, then banning price discrimination could increase the social welfare when the cost difference is small. The conclusion of our paper is quite different from Chen (2017). We find that the change of price contract of inputs changes the impact of input price discrimination on social welfare.====This paper is organized as follows. Section 2 formulates our basic model and considers that the price of input is determined by the upstream firm. In Section 3, we analyze the case that the price of input is determined through Nash bargaining. Section 4 concludes this paper.","Input price discrimination, pricing contract and social welfare",https://www.sciencedirect.com/science/article/pii/S0165489622000580,26 July 2022,2022,Research Article,41.0
Sokolov Denis,"15 Lake Shore Court 3, Brighton, MA 02135, Boston, USA,Boston College, USA","Received 3 September 2020, Revised 24 April 2022, Accepted 14 June 2022, Available online 26 June 2022, Version of Record 12 July 2022.",https://doi.org/10.1016/j.mathsocsci.2022.06.005,Cited by (0),"In this paper, we introduce a new form, the ","A central problem of cooperative game theory is how to fairly divide a coalition’s total payoff among all its members once it has been formed. Shapley (1953a) proposed a prominent solution, now referred to as the ====. It is based on three natural axioms: ====, where only carrier players (i.e., those important for the game) can receive non-null payoffs and they get all the worth of the grand coalition; ====, where two players playing the same role get the same payoff; and ====, where a solution is considered to be a linearly-separable operator. Surprisingly, only these three axioms lead to a unique division of the grand coalition payoff among participants. The derived Shapley value is applicable to the following class of cooperative games: superadditive ==== (CFF) games (von Neumann and Morgenstern, 1944) with a set of agents ====, where for each subset ==== of ==== (coalition), a real number (i.e., wealth) is assigned.====However, to make the CFF framework more realistic, some underlying assumptions can be relaxed. While the superadditivity should stay for the grand coalition to form, some externalities across coalitions should be introduced, allowing us to reproduce a coalition’s bargaining power conditional on the outside structure (Maskin, 2016, Maskin, 2003). Indeed, the level of coordination among some groups of players can affect the outcomes for the other players in many settings, such as the interactions between political parties, oligopolistic markets, international trade, and public goods production (free-rider problem). The desired augmented model for cooperative games was proposed by Thrall and Lucas (1963). Here, instead of assigning a single wealth to every coalition ====, the authors suggested giving wealth to any pair of a coalition ==== and including ==== partition ==== of the set of all players ====. Such a pair ==== is named an ==== and a new class of TU-games: ==== (PFF) games. In a PFF game, the wealth of a given coalition ==== depends on how the other players outside ==== are coordinated.====Maintaining superadditivity, the initial issue of a fair division of the grand coalition wealth persists. An extension of the Shapley value for a cooperative environment that allows for externalities should be able to capture the additional value of a player coming from her role outside of the coalition under question. Indeed, now the same coalition may have different worths depending on how the rest of the players have united. Note that the classical Shapley value cannot account for this factor.====Unfortunately, based on the more general set-up, there appears to be no unique approach for developing an analog of the Shapley value for PFF games. The first attempt to do so was made by Myerson (1977a), where the author formulated three simple axioms and derived the corresponding value.==== Since then, a great deal of work has been done on this issue. Hart and Kurz (1983) proved, using an axiomatic approach, that the ==== (Owen, 1977) is a reasonable extension of the Shapley value based on the partition framework (not PFF in full sense). Moreover, they proposed a number of different notions for stability of formed coalitions based on various external players’ strategies, like the core and the domination. Pham Do and Norde (2002) presented a different formula for calculating the Shapley value for PFF games, using four basic axioms and a linear decomposition in unanimity games. Grabisch and Funaki (2012) introduced an alternative approach to this issue, involving the explicit use of processes and different coalition formation scenarios. For convenience, Macho-Stadler et al. (2019) provided a complete summary and discussion of all the approaches existing at that time to extend the Shapley value based on the PFF setting. The authors highlighted three general approaches to address this issue: axiomatic (Myerson, 1977a), through marginal contributions (de Clippel and Serrano, 2008), and non-cooperative (Macho-Stadler et al., 2006). Skibski et al. (2018) presented one of the most recent extensions, where they naturally obtained an axiomatization for the unique value from a specific stochastic coalition formation process.====Notwithstanding, as we observed, neither the CFF nor PFF settings allow a player to participate in more than one coalition. Given that in real life we often encounter such an opportunity, a multiple membership feature should be introduced (Maskin, 2003, Ray, 2007). Assuming in general that people tend to gather together to deal with a certain set of issues, we divided the field papers relevant to this study into two major classes: ==== and ====. ==== papers cover settings where one player may participate in many coalitions across issues; however, within any particular issue, the player may join only one coalition. On the other hand, ==== studies assume there is only one general issue to solve and that a participant may be a member of multiple coalitions. Both approaches have yielded a rich variety of models: ==== games (Nax, 2014), ==== games (Diamantoudi et al., 2015, Le Breton et al., 2013), ==== (Gayer and Persitz, 2016) and ==== games (Assa et al., 2015), among others, for ==== cases; and ==== games (Myerson, 1980), ==== games (Agbaglah, 2014a, Agbaglah, 2014c, Agbaglah, 2016) and ==== games (Albizuri et al., 2006, Albizuri, 2010), among others, for ==== cases. These papers provide reasonable real-life examples, define new aforementioned generalized TU-game settings, and extend the concepts of the Shapley value and the core on them, as well as aiding the study of endogenous coalition formation processes.==== New extensions of the Shapley value can determine an additional value of a player based on her simultaneous contributions to multiple coalitions (on top of her value accounting for externalities).====Our approach falls into the ==== basket, but with the following novelty: unlike existing models, after expressing a given collection of subsets of ==== as an undirected graph (players-nodes, where there is an edge between two players if and only if they are from the same subset), we treat only the maximal fully interconnected sub-graphs as coalitions. It follows from the insight that, if a set of agents ==== is fully interlinked, it is logical to assume that they all need to sign only one contract to determine all the relationships among them. That is why we also do not allow for sub-coalitions inside a coalition. Furthermore, Agbaglah (2016) showed that for each such social graph, there exists a cover function bargaining game, whose subgame perfect Nash equilibrium outcome coincides with its set of all maximal fully interconnected sub-graphs. This result enforces our insight and justifies the novelty introduced above.====The main goal of this paper is to formally introduce the aforementioned concept and to develop a corresponding extension of the Shapley value on the basis of the Myerson value through an axiomatic approach. To our knowledge, this is the first paper that constructs an extension of the Myerson value for ==== overlapping coalition cooperative games through three independent axioms.==== The key ingredient for the suggested solution is the notion of the restriction of a cooperative game to its carrier, given that this object plays a central role in our extension of the ==== axiom: ==== axiom. Combined with classical ==== and ==== axioms, they yield a unique value.====The rest of the paper is structured as follows. In Section 2 we summarize the CFF and PFF concepts and introduce one that is more general: the ==== (CQFF). In Section 3 we derive a value for CQFF games using an axiomatic approach. Section 4 provides a toy economic example and discusses the results. The concluding Section 5 describes some features of the proposed concept and offers avenues for future research. Appendix A contains omitted proofs. Appendix B gives basic and well-known definitions, axioms, and results related to the values for CFF and PFF games. Appendix C provides a detailed algorithm for constructing a restricted game. Finally, Appendix D presents one additional special case of an economic example.",Shapley value for TU-games with multiple memberships and externalities,https://www.sciencedirect.com/science/article/pii/S0165489622000579,26 June 2022,2022,Research Article,42.0
D’Agata Antonio,"Department of Political and Social Science, University of Catania, Italy","Received 31 December 2020, Revised 14 April 2022, Accepted 14 June 2022, Available online 25 June 2022, Version of Record 6 July 2022.",https://doi.org/10.1016/j.mathsocsci.2022.06.003,Cited by (0),"This paper introduces two new boundary conditions that ensure the existence of a ==== in ==== violating Walras’ Law and homogeneity of degree zero of the excess demand function. The two conditions are more general than those adopted by the existing literature. The existence of a ==== with free disposal is also considered. As a by-product of our analysis, a refinement of the celebrated Uzawa’s equivalence Theorem and extensions of the Hartman–Stampacchia and Poincaré–Miranda Theorems are also provided.","The existence of a Walrasian equilibrium is usually proven by assuming that the excess demand function satisfies Walras’ Law on the good markets, homogeneity of degree zero and a boundary condition, in addition to the usual continuity property. The boundary condition generally considered (henceforth, the 0-boundary condition) requires that the excess demand for some goods whose price tends to zero must eventually become infinitely large (see, e.g., Debreu, 1982, p. 724). Grandmont (1977) introduces a weaker 0-boundary condition. It requires that the value of the excess demand for price vectors close to the boundary of the unit simplex is positive when calculated in terms of a pre-determined price vector in this set (see also Neuefeind, 1980). Grandmont’s boundary condition has been recently generalized by assuming that the positivity condition holds true just for some price vector in the unit simplex (Hüsseinov, 1999). The conditions of homogeneity and Walras’ Law impose very strong restrictions on the underlying economic model, like the absence of money and price-independent preferences (Arrow and Hahn, 1971, Balasko, 2003, Patinkin, 2008). Discarding these conditions is challenging because of the consequent unboundedness of the price set that rules out the application of standard fixed point theorems.====Chichilnisky and Kalman (1978) and Schulz (1985) have proven the existence of a Walrasian equilibrium without homogeneity and Walras’ Law in a monetary economy by retaining the usual 0-boundary condition. To deal with the unboundedness issue, they rely on a new condition (henceforth, the ====-boundary condition) requiring that the excess demand of some goods must eventually become negative when some price tends to infinity. However, the 0- and the ====-boundary conditions are easily violated in economies with price-dependent preferences. This occurs, for example, in economies with status goods because the excess demand for these goods is directly related to their prices (Scitovszky, 1944, Leibenstein, 1950).==== The particular behavior of the excess demand for status goods suggests that the 0-boundary and the ====-boundary conditions could still be useful by considering the excess ====, rather than the excess demand functions for these goods. This procedure hints at a more general strategy in searching for a Walrasian equilibrium, which consists in verifying the validity of the 0- and ====-boundary conditions for ==== function whose generic ====th element is either the excess demand or the excess supply function of good ==== (henceforth, any function of this kind is called a d/s-excess function). As it will be shown in the next section, this strategy is not always successful, although it enlarges the opportunities for obtaining a Walrasian equilibrium.==== Consequently, we need more general boundary conditions than the extant 0-boundary and ====-boundary conditions for the excess demand function or, more broadly, for the d/s-excess functions when Walras’ Law and homogeneity are discarded.====This paper introduces two new boundary conditions for d/s-excess functions defined on unbounded price sets. These conditions generalize the extant 0-boundary and ====-boundary conditions. Roughly, the condition replacing the 0-boundary condition, i.e., the “avoiding cone” (AC) condition (Fonda and Gidoni, 2016), requires that there exists a d/s-excess function such that for price vectors ==== close to the boundary of the price set, the value of the d/s-excess function calculated at some price ==== is greater than its value calculated at the current price ====. The AC condition extends Grandmont-type 0-boundary conditions to unbounded price sets when Walras’ Law and homogeneity are not satisfied. The condition replacing the ====-boundary condition, i.e., the “no exceptional sequence” (NSE) condition (Smith, 1984), ensures that the relevant d/s-excess function does not always point “outward” the non-negative part of balls with integer radii when the price vector tends to infinity. It will be shown that the AC condition, together with the NES condition, ensures that there exists a ==== convex subset of the price set such that the relevant d/s-excess function satisfies the AC condition in this set and that this condition is sufficient to ensure the existence of a zero of the d/s-excess function in that compact subset. The existence of a Walrasian equilibrium with free disposal is shown to be guaranteed only under the NES condition. We show that our results include the existence results currently available in the literature; we also present examples of economies whose Walrasian equilibrium (with or without free disposal) is explained by our results but cannot be explained by the extant results.====The paper is organized as follows. The next section summarizes the extant results on the existence of Walrasian equilibria in economies violating homogeneity and Walras’ Law. It also presents examples of economies having a Walrasian equilibrium and violating the conditions posited by these results. Section 3 offers some preliminary mathematical results that will be useful in the subsequent sections. Section 4 provides the main results that generalize those presented in Section 2 and shows that these results are able to explain the existence of a Walrasian equilibrium in the examples in that section. Section 5 gathers some final remarks. The Appendix contains some additional mathematical and economic results that are not directly related to our main issue but may be of intrinsic interest. In particular, it provides a refinement of the celebrated Uzawa’s equivalence theorem (Uzawa, 1962).",Walrasian equilibrium without homogeneity and Walras’ Law,https://www.sciencedirect.com/science/article/pii/S0165489622000555,25 June 2022,2022,Research Article,43.0
Romaniega Sancho Álvaro,"Instituto de Ciencias Matemáticas, Consejo Superior de Investigaciones Científicas, 28049 Madrid, Spain","Received 6 February 2022, Revised 26 May 2022, Accepted 8 June 2022, Available online 16 June 2022, Version of Record 25 June 2022.",https://doi.org/10.1016/j.mathsocsci.2022.06.002,Cited by (0),The ,None,On the probability of the Condorcet Jury Theorem or the Miracle of Aggregation,https://www.sciencedirect.com/science/article/pii/S0165489622000543,16 June 2022,2022,Research Article,44.0
"Gonzalez Stéphane,Rostom Fatma Zahra","UJM Saint-Étienne, GATE UMR 5824, F-42023 Saint-Étienne, France,Muséum National d’Histoire Naturelle, Centre d’Écologie et des Sciences de la Conservation (CESCO UMR 7204), France,Institut de la Transition Environnementale (Alliance Sorbonne Université), France","Received 11 May 2021, Revised 7 May 2022, Accepted 19 May 2022, Available online 8 June 2022, Version of Record 17 June 2022.",https://doi.org/10.1016/j.mathsocsci.2022.05.003,Cited by (0),"The article explores the implications of natural resource scarcity in terms of global cooperation and trade. We investigate whether there exist stable international long-term agreements that take into account the disparities between countries in terms of geological endowments and productive capacity, while caring about future generations. For that purpose, we build an original cooperative game framework, where countries can form coalitions in order to optimize their discounted consumption stream in the long-run, within the limits of their stock of natural resources. We use the concept of the strong sequential core that satisfies both coalitional stability and time consistency. We show that this set is nonempty, stating that an international long-term agreement along the ==== will be self-enforcing. The presented model sets out a conceptual framework for exploring the fair sharing of the fruits of global economic growth.","The nexus between trade and mineral resource extraction is crucial to the understanding of wealth creation dynamics. Early stages of mankind are named after the materials from which tools and weapons were made of: the Bronze Age, which arose fully around 3000 BCE, was preceded by the Chalcolithic or Copper Age. The Near East was the “kernel of the Age of Metals”, but was poorly endowed with these materials. Therefore, the “valley urban societies” had to exchange with the “barbarians” (in the technical archaeological sense), before trade expanded from the Near East to Europe (Goody, 2012). Modern resource extraction and trade are of much higher orders of magnitude. Industrialization is characterized by the construction of infrastructures in the sectors of heavy industry, energy, housing, transport and communication, and therefore is inevitably associated with an increase in the consumption of raw materials. The development of trade, necessary to meet an ever-growing demand, is directly linked to the institutional implementation of free trade agreements in free trade areas.====The debate on the “trade-environment divide” (Esty, 2001) is mostly focused on pollution issues (Copeland and Taylor, 1994). The interaction between trade and materially sustainable growth (Dupuy, 2014) was studied by Asheim (1986) who looked at the effects of opening economies on the Hartwick rule. Strategic game theory approaches were also used to explore finite resources issues (Van Long, 2011), and the relationship with trade was studied with two countries (Kagan et al., 2015, Tamasiga and Bondarev, 2014). However, and despite the call of some game theorists to develop cooperative game theory (Samuelson, 2016), the literature on material sustainability neglected to treat the issue through cooperative lenses. Trade can indeed be analyzed through three concepts: “pure competition”, “coalitional power” and “fair division”. Shapley and Shubik (1969) provide a static market game for the three varieties of solution and show that, under certain assumptions, the outcomes converge. In a distinct stream of literature of cooperative games, the question of renewable and common-pool resources was studied (Funaki and Yamato, 1999), mostly for fishery or river sharing problems (Ambec and Sprumont, 2002, Béal et al., 2013). However, these models do not take into account the dynamic aspects of the issue.====The aim of this article is to show that there exist stable international long-term agreements that take into account the disparities between countries in terms of geological endowments and productive capacity, while caring about future generations. In this context, countries can form coalitions and look for the best agreement to optimize their allocation. Members can break an alliance according to their interest, at any point of time. Dynamic cooperative game theory is the most fitted framework to tackle this kind of issues, but the literature on the application of such games is quite limited. Some dynamic cooperative approaches were used to tackle environmental issues, especially fisheries (Munro, 1979) and pollution (Jørgensen and Zaccour, 2001, Hoofe, 2019). The question of non-renewable resource exhaustibility was raised by d’Albis and Ambec (2010), who study cooperative allocations among overlapping generations depleting a natural resource over an infinite future. However, the authors focus on intergenerational allocations and do not include any international aspect in their model.====In order to find such long-term international agreements, the twin issues of coalitional stability ==== time consistency have to be handled at once. First, it is a question of designing a policy that no coalition of the present generation has an incentive to refuse. Traditionally, this issue is tackled by the concept of the core, which is the set of allocations which cannot be improved upon by any coalition of agents. However, this notion does not deal with the dynamic issues relevant to the intergenerational sharing of the benefits generated from resource extraction. It must therefore also ensure that at no point in time the policy initiated then departs from the one originally planned. Recent papers tackle the issue of dynamic core concepts (Predtetchinski et al., 2004, Kranich et al., 2005, Lehrer and Scarsini, 2013), but the first studies date back to the 70s with the introduction of core concepts for production economies (Boehm, 1974, Becker, 1982), as well as the notion of trust in a monetary economy (Gale, 1978). Becker and Chakrabarti (1995) introduced the recursive core solution concept, designed for the specific case of capital accumulation models. They defined it as the set of allocations for which no coalition can improve upon its consumption stream at any time given its accumulation of assets up to that period. This concept is a particular case of the most demanding dynamic solution concept, the strong sequential core, which captures “those situations in which at each stage the grand coalition is formed, its worth is distributed among the players and no coalition has a profitable deviation” knowing that “coalitions are allowed to deviate at any stage of the game” (Kranich et al., 2005). This concept has the strong property to satisfy both coalitional stability and time consistency. It is empty-valued for large classes of economies, as argued by Habis and Herings (2011) who compare various dynamic core concepts. In the present article, we question the emptiness of this solution concept in the case of open and resource-dependent capitalist economies.====The novelty of our approach is to model the geographical heterogeneity of natural endowments that results from geological processes, as well as the geographical heterogeneity of capital and technological endowments that results from historical processes. For that purpose, we settle a cooperative game where countries can form coalitions in order to optimize their discounted consumption stream in the long-run, within the limits of their stock of natural resources. Trade is viewed as a market cooperative TU-game à la Shapley and Shubik (1969) where non-renewable natural resource inputs are exchanged in such a way as to maximize the total value to be shared among the coalition. This game is cast into a traditional Ramsey-type model of intergenerational equity, where the Bellman’s dynamic programming framework is used. An international long-term policy is translated into:====In other words, the coalition decides its extraction and investment plan, and deduces the amount of resources it wishes to trade. It then distributes consumption goods to its members. Note that this definition of a policy is larger than the traditional sense in the field of macroeconomics. Here, it adds to the action the notion of sharing the fruits of economic growth.====This paper addresses the underexplored topic of how to cooperate in a dynamic environment. We show that there exists a unique optimal path of extraction and investment for each country in a coalition ====, which could be interpreted as quotas proposed by a social planner. As traded quantities are also unique, we conclude that there exists a unique optimal path for collective production in a coalition ====. We then can build an intertemporal cooperative game where the worth of ==== is the discounted sum of consumption – corresponding to the remaining part of production that is not invested – along this optimal path. We demonstrate that the strong sequential core associated to this game is nonempty. This strong result states that an international long-term agreement along the optimal path will be self-enforcing. The countries, as rational agents, will stick to this agreement, as no other coalition could offer them a better outcome, at any point in time. Furthermore, we show that this strong sequential core contains an infinity of elements, opening an avenue for the research of a fair distribution from both intergenerational and international points of view.====The article is organized as follows. The following section presents our formal apparatus and details the reasoning of the model. In the third section, our main results and their interpretation are provided. The last section is a discussion on the model and the research avenue it opens. All proofs are available in Appendix A.",Sharing the global outcomes of finite natural resource exploitation: A dynamic coalitional stability perspective,https://www.sciencedirect.com/science/article/pii/S0165489622000427,8 June 2022,2022,Research Article,45.0
"Meyer Seth A.,Pomplun Jessica,Schill Joshua","St. Norbert College, United States of America,Marquette University, United States of America","Received 14 August 2021, Revised 7 May 2022, Accepted 20 May 2022, Available online 30 May 2022, Version of Record 9 June 2022.",https://doi.org/10.1016/j.mathsocsci.2022.05.004,Cited by (0),"The well-studied phenomenon of present bias describes how people over-weight costs and rewards incurred in the present. This leads people to put off portions of a task until later, which can cause them to complete tasks in inefficient ways. Using the ==== of Kleinberg and Oren (2014) to study present bias, previous work has considered agents who are unaware of the effects of their present bias (naive agents) and those who are aware of its effects on the entire task (sophisticated agents). In this paper we use the executive functions of working memory and cognitive flexibility from the psychology literature to introduce a new way of understanding the decision making of these agents which is particularly appropriate for large or complicated tasks. This allows us to define and analyze a new kind of partially sophisticated agent who uses sophisticated reasoning on only part of a task at a time. The main result of this paper shows that in the worst case the performance on a task for each of these different types of agents fits a simple formula which depends only on their present bias and the number of plans the agent needs to make to complete the task. We also define a new commitment device, where an external agent can assist in the completion of a task, and show that the formula still holds with an appropriate modification.","People do not always complete tasks with the minimum amount of effort. Even when the underlying cost of accomplishing a task does not change, a person attempting that task may change their strategy partway through completing it or even abandon the attempt entirely. This is called time inconsistent planning and it is a well studied area of research in behavioral economics (e.g., see Wong, 2008 for a study). In particular, people overestimate costs and rewards which are incurred in the present (or equivalently, underestimate costs which occur in the future) and so they base their plan for how to accomplish a task on how much work they “feel like” the plan will require. This can lead to suboptimal behavior, as putting off work to the future can cause a greater total cost to be incurred than was necessary.====Complicating this situation is the fact that people can sometimes anticipate their own time inconsistency and build that into their planning process. Reasoning of this form is referred to as sophisticated and usually, though not always, leads to better performance on the task at hand. This paper aims to model the behavior and efficiency of agents who attempt long or complicated tasks and are able to use sophisticated reasoning on only a portion of the upcoming task. We call reasoning of this form partially sophisticated and compare and contrast it to other notions of partial sophistication in the literature.====To formalize the process of completing a task, we use the model proposed by Kleinberg and Oren in Kleinberg and Oren (2014), which assumes that any task can be broken up discretely into intermediate stages, and an agent must decide how to transition from the start to the end by moving from one stage to the next. Further, every stage transition has a non-negative cost associated with it, and the agent’s goal is to move from the start to the end while accruing the least total cost. Note that costs are generalized in the sense that part of the cost could be effort, part could be anxiety about the task not yet being finished, part could be monetary, etc., but we assume that all costs associated with a particular transition are represented by a single number. At each stage, we assume that the agent considers each strategy they could use to complete the task and selects the strategy with the lowest perceived cost, keeping in mind that their present bias will cause the perceived costs to be greater than the true costs.====To be precise, a task is modeled as a directed acyclic graph where a source vertex ==== represents the initial state of the task when no progress has been made, a sink vertex ==== represents the state where the task is completed, and each other vertex in the graph represents an intermediate state where some action (or inaction) has been taken since the initial conditions. Each one-way state transition is represented by a directed edge where the cost of the transition is viewed as a weight on the edge. The goal of the agent is then to follow a directed path from ==== to ==== which accrues the least amount of total cost, defined as the sum of the edge weights along the chosen path.====We assume that an agent attempting this task has a consistent present bias factor, denoted by a positive number ==== which is greater than 1, and the agent evaluates the perceived cost of each path from their current location to ==== by multiplying the cost of the first edge by ==== and then adding the costs of the other edges in the path. Thus, the agent starts at ==== by computing the perceived cost of all paths to ====, they take the first edge of the path with the lowest perceived cost, and then they recompute the perceived costs and repeat this until they reach ====. This can be seen as a simplification of the quasi-hyperbolic discounting model (see Frederick et al., 2002 for more information on the full model). In particular, ==== is equal to ==== and ==== is equal to ==== in the standard framing of quasi-hyperbolic discounting.====This simplification makes analysis of the model much more tractable and we can judge the performance of the agent by looking at the path they actually take and the optimal or minimal cost path. This is called the ==== ==== of a particular path from ==== to ====, and is computed as the cost of that path divided by the cost of the optimal path. It allows for comparison between different types of agents, in particular naive agents who reconsider their path at each vertex with no ability to predict how their present bias will affect their future choices (this is described above), sophisticated agents who have full knowledge of how they will make decisions in the future, and partially naive agents as defined by O’Donoghue and Rabin (see, e.g., O’Donoghue and Rabin, 1999 or O’Donoghue and Rabin, 2001) who use sophisticated reasoning but with only an estimate of their bias in the future. In Kleinberg et al. (2016), Kleinberg, Oren, and Raghavan compute the worst possible performance of all three different kinds of agents on these task graphs.====In recent years there have been a number of papers addressing different aspects of this model. For instance, Albers and Kraft (2017) allow present bias to vary and incentivizes the agent with penalty fees while Gravin et al. (2016) describe agents whose bias is drawn from a probability distribution at each step. Fomin and Strømme (2020) find simple motivating subgraphs through deleting edges and vertices while the agent changes their plan at most ==== times throughout the graph. Instead of changing the graph or the agent’s present bias, Saraf et al. (2020) introduce a second agent and shows that competition makes the agent more efficient. Tang et al. (2017) address the open questions from Kleinberg and Oren (2014) and the inefficiencies of intermediate rewards and deleting edges.====In this paper we consider modifications of the model to better apply it to complicated tasks that take place over a long period of time, such as completing all the assignments in a semester long course. Previous work with this model has assumed that agents can consider every possible path through the entire task graph at every stage, but for large task graphs this is unrealistic or very expensive (e.g., see Bolton and Faure-Grimaud, 2009 which models the cost of planning future actions as the time taken to understand that portion of the decision space). Motivated by the psychology literature on executive functions, we introduce two new notions of partial sophistication which generalize the behavior of naive and sophisticated agents to cases where the agent can only anticipate the effects of their present bias a fixed number of steps in the future. This allows us to resolve similar questions to previous work which has looked at worst case problems involving sophistication and results in more natural worst case bounds than the standard definition of partial sophistication. In particular, the worst case results for previously defined partially sophisticated agents are essentially the same as those for naive agents, while the bounds for our partially sophisticated agents vary predictably from the sophisticated bounds to the naive bounds as the level of sophistication decreases.====We start by reviewing the behavior of naive and sophisticated agents in Section 2. In Section 3 we review relevant material concerning executive functions. We then discuss how to relax the assumption that agents can plan on the entire task graph and define the relevant parameters. We then build on these definitions in Section 4 where we formally define two new agents who exhibit partial sophistication and contrast them with the definition of O’Donoghue and Rabin. In Section 5 we review previous results on the worst case cost ratios from Kleinberg et al. (2016) and determine the corresponding results for our new agents. In Section 6 we then introduce a new commitment device for large graphs which changes the cost of a decision. These results are collected in Section 7 and the main result of this paper is Theorem 7.1, which shows that, in several different circumstances, the worst case cost ratio for an agent is the product of the cost of each decision they make. We end in Section 8 with a summary and suggestions for future work.",Present bias in partially sophisticated and assisted agents,https://www.sciencedirect.com/science/article/pii/S0165489622000439,30 May 2022,2022,Research Article,46.0
"Mukherjee Arijit,Zeng Chenhang","Nottingham University Business School, Nottingham, UK,Wenlan School of Business, Zhongnan University of Economics and Law, 182 Nanhu Ave, Wuhan 430073, China","Received 9 June 2021, Revised 4 May 2022, Accepted 6 May 2022, Available online 24 May 2022, Version of Record 3 June 2022.",https://doi.org/10.1016/j.mathsocsci.2022.05.002,Cited by (0),"We show the implications of sunk investments for social efficiency of downstream-entry in a bilateral ====. The possibility of socially excessive entry increases as the percentage of non-sunk investments increases. If there are no sunk investments or bargaining for the input prices occurs before investments as in the “ex-ante bargaining”, entry is always socially excessive. These results hold under both two-part tariff and linear input pricing.","Social desirability of entry is an important concern for the policy makers, which attracted significant attention in academia.==== The influential “excess-entry theorem” derived by Mankiw and Whinston (1986) and Suzumura and Kiyono (1987) justifies the anti-competitive entry regulation policies adopted by many countries. However, the result of excessive entry, created by the “business-stealing” effect,==== has been challenged by Ghosh and Morita, 2007a, Ghosh and Morita, 2007b, Basak and Mukherjee (2016), Cao and Wang (2020), and Chen et al. (2021). It is shown in a vertical structure that entry can be socially insufficient and therefore, requires pro-competitive policies. The driving force for insufficient entry is the resulting “business-creation” effect.====The papers considering vertical relationships provide important insights, but they are restrictive for at least two reasons. First, they consider that the fixed costs incurred by the entrants are completely sunk at the time of input price determination, which may not be the case always. Fixed costs may have both sunk and non-sunk parts. For example, Varian (1999) uses the term “quasi-fixed costs” to mention avoidable or non-sunk fixed costs, which are the fixed costs that are independent of the amount of outputs but are incurred only if there is a positive output. Several other authors, such as Baumol and Blinder (1997), Carleton and Perloff (2000), Hall and Lieberman (1998), Mankiw (1998), Pindyck and Rubinfeld (1998), Samuleson and Marks (1999) and Tirole (1988), also discussed the existence of sunk and non-sunk fixed costs. See, Wang and Yang (2001) for a discussion on this issue.====Hence, the entire fixed costs may not be sunk at the time of bargaining for the input prices. For example, if the bargaining is not successful, firms may rent or sell the buildings and machineries to recover at least part of the fixed costs they incurred before the bargaining for the input prices. In the extreme case, there may be no sunk fixed costs or bargaining for the input prices may occur before the investments undertaken by the entrants, as in the case of ex-ante bargaining by Ulph and Ulph, 1994, Ulph and Ulph, 1998.==== Industries such as postal delivery and car rental will serve as representative examples of low sunk costs industries since most of their costs can easily and quickly be recovered. By contrast, industries with the highest sunk costs tend be those with the greatest barriers to entry and biggest startup costs. Examples include the film industry and the pharmaceutical industry. We will discuss more on this later.====Second, the papers considering social efficiency of entry in a vertical structure assume linear input pricing while non-linear two-part input pricing is often observed in a wide range of industries. Empirical evidence includes Bonnet and Dubois (2010) on food retailing, Borenstein and Davis (2012) on natural gas, and Lakdawalla and Sood (2013) on health insurance, etc.==== Theoretically, two-part tariff contracts are widely assumed in the literature on vertical markets, such as in Tirole (1988), Nocke and White (2007), and Chen and Riordan (2007), to name but a few.====We use a bilateral oligopoly framework (see, e.g., Ghosh and Morita, 2007b)==== to analyze social desirability of downstream-entry by introducing the above-mentioned two new aspects: sunk and non-sunk fixed costs at the time of bargaining for the input prices and two-part input pricing contracts. We show that the possibility of socially excessive entry increases as the percentage of non-sunk investments increases. If there are no sunk investments or bargaining for the input prices occurs before investments as in the “ex-ante bargaining” (Ulph and Ulph, 1994, Ulph and Ulph, 1998), entry is always socially excessive.====Hence, our analysis suggests that the anti-competitive entry regulation policy, which has been employed in a number of countries, is more likely to be relevant in a vertical structure as the percentage of non-sunk investments by the entrants increases, thus maintaining the policy implications of Mankiw and Whinston (1986) in this situation. However, the policy implications of Ghosh and Morita (2007b) are more likely to be relevant as the percentage of sunk investments by the entrants increases.====To understand the reasons for our results, let us first see the implications of two-part tariff contracts. Since the fixed-fee under the two-part tariff contract helps the upstream firm to extract rent from the downstream firm, it can encourage the upstream firm to charge a per-unit input price that is lower than the upstream firm’s marginal cost of production, which is in line with Vickers (1985). The lower per-unit input price helps to create a strategic advantage in the product market by stealing market share from the competing firm. Hence, on the one hand, the lower per-unit input price under two-part tariff contract helps to increase the outputs of the downstream firms by reducing their marginal costs, which tends to increase welfare. On the other hand, if the profit of the downstream firm is positive ex-post bargaining, which depends on the sunk investment, as discussed below, a higher bargaining power of the upstream firms helps to reduce the net profits of the downstream firms by allowing the upstream firms to extract more rents from the downstream firms through higher fixed-fees, which, in turn, reduces the downstream firms’ incentive for entry. In this situation, a sufficiently higher bargaining power of the upstream firms reduces the downstream firms’ incentive for entry significantly, and creates the possibility of insufficient entry.====Now see the implications of sunk entry costs and to understand it clearly, consider the two extreme cases of completely sunk costs (i.e., the case of “ex-post bargaining”) and completely non-sunk costs (i.e., the case of “ex-ante bargaining”). If the fixed costs are completely sunk at the time of input price bargaining, the downstream firms earn positive profits ex-post bargaining. Hence, the fixed-fees under two-part tariff allow the upstream firms to extract rent from the downstream firms, which results in a “business-creation” effect for the upstream sector but reduces the profitability of the downstream firms and therefore, reduces the incentive for downstream-entry. The magnitude of the “business-creation” effect increases with the bargaining power of the upstream firms, and may lead to insufficient entry when upstream firms have sufficiently high bargaining power.====In contrast, if there are no sunk investments or there is “ex-ante bargaining”, i.e., the fixed costs are incurred after bargaining for the input prices, the net profits of the downstream firms are zero ex-post bargaining, regardless of upstream bargaining power. Hence, the fixed-fees under two-part tariff contracts do not help the upstream firms to extract rents from the downstream firms. In this situation, the upstream firms do not earn positive profits in the free entry equilibrium since the net profits of the downstream firms are zero in the subgame of bargaining and production. That is, downstream-entry does not generate the well-known “business-creation” effect for the upstream sector. In this situation, as in Mankiw and Whinston (1986), the “business-stealing” effect of entry leads to excessive entry in the downstream sector.====Hence, it is intuitive that if there are positive sunk and non-sunk fixed costs at the time of bargaining for the input prices, as the percentage of non-sunk fixed costs increases, it reduces the downstream firms’ profitability after input price determination, which, in turn, reduces rent extraction by the upstream firms and therefore, the extent of “business-creation” effect for the upstream sector. Hence, a higher percentage of non-sunk fixed costs makes excessive entry more likely.====We further conduct a robustness check by considering bilateral bargaining over a linear wholesale price as that used in the literature. We find that the above results hold under both two-part tariff and linear input pricing. If the profit of the downstream firm is positive ex-post bargaining, a higher bargaining power of the upstream firms helps them to extract more rents from the downstream firms through higher linear pricing. In this situation, a significantly higher bargaining power of the upstream firms helps to create insufficient entry. Hence, our analysis shows how the degree of sunk costs and the form of input pricing contracts affect social efficiency of entry of the final goods producers in a bilateral oligopoly with input price bargaining.====The remainder of the paper is organized as follows. Section 2 describes the basic model. Section 3 derives the results. Section 4 provides an illustration of our results with a linear demand. Section 5 conducts a robustness check with the consideration of a linear wholesale price. And Section 6 concludes. We organize most of the proofs and claims in an Appendix.",Social desirability of entry in a bilateral oligopoly—The implications of (non) sunk costs,https://www.sciencedirect.com/science/article/pii/S0165489622000415,24 May 2022,2022,Research Article,47.0
Ha-Huy Thai,"Université Paris-Saclay, Univ Evry, EPEE, 91025, Evry-Courcouronnes, France,TIMAS, Thang Long University, Viet Nam","Received 3 December 2020, Revised 25 April 2022, Accepted 25 April 2022, Available online 6 May 2022, Version of Record 9 June 2022.",https://doi.org/10.1016/j.mathsocsci.2022.04.003,Cited by (0),"This article considers optimization problems under two Rawls criteria. The first criterion is the classical Rawls criterion from the literature and the second one is based on the maximin criteria with multiple discount factors presented in Chambers and Echenique (2018). Although these criteria are different, they have the same optimal value and solution.","Consider the following classical question: given a stock of renewable resources, what would be the best inter-temporal exploitation of it, considering the welfare of both current and future generations?====The discounted utilitarian criterion, which is widely used in economic dynamics research is criticized for its weak weighting parameters for generations in the distant future. The evaluation of each utility stream is quasi-determined by a finite number of generations. This raises the concern that following this criterion, the economy does not leave enough resources for future generation. As an illustration, we have the comment by Ramsey (1928), “discounting the interests of future people is ethically indefensible and arises merely from the weakness of imagination”.====In his classical work “Theory of Justice”, Rawls (1971) assumes that if one is hidden behind a ====, with a total lack of information about the conditions into which one will be born, the economic agent would choose the maximization of the least favored generation. Precisely, given inter-temporal consumption streams, the agent’s evaluation criterion of the inter-temporal utility streams would be ====where ==== is the utility of the ====th generation, given ==== as the consumed resource.====This article considers Rawls’s question in another light: the economic agent may be ambiguous about what a “good” discount factor is for evaluating utility streams. The agent’s set of possible discount factors belongs to ====. With a total lack of information, this set takes all possible values of discount factors and becomes the whole set ====. For a given consumption stream ====, the agent would evaluate it as====
 ====This criterion can also be seen as an application of Rawls’s spirit to a configuration in which disagreements exist between people in the economy about how to discount the future. The economic agent chooses the discount factor that gives the worst evaluation. In maximizing the economy under this condition, the economic agent maximizes the worst scenario.====This was not the only reason that urged us to conduct this study. In recent decades, a large body of literature in decision theory has arisen, enlarging the world of  Savage (1954), where the famous ==== is not satisfied. The seminal contribution of Gilboa and Schmeidler (1989) considered the behavior under which the economic agent, facing ambiguities, maximizes the worst scenario. This work is the source of numerous contributions not only in decision theory but also in inter-temporal axiomatization.==== Applying the idea of Gilboa and Schmeidler (1989) in the context of this article, assume that the economic agent must choose a time discounting system to evaluate the inter-temporal consumption streams. Behind the ====, every time discounting system is possible. The set of ==== possible time discounting systems is ====Hence, the criterion under ==== is ==== which is the first Rawls criterion.====Now, assume that the economic agent is just ambiguous about which discount factor must be chosen to evaluate consumption streams. In other words, the set of possibilities reduces to the set of time discounting systems satisfying the usual properties such as ====, and ====. In  Chambers and Echenique (2018), this set is described as follows: ====We obtain the aforementioned criterion, which can be regarded as the second Rawlsian criterion.====This raises the question of what the behavior of the economy is under the Rawls criteria. The first Rawls criterion is well studied in seminal contributions by Arrow, 1973, Solow, 1974, and  Calvo (1977). The behavior of the economy depends strongly on the initial stock. If the stock of a renewable resource is below the ==== (the stock allowing a maximal level of constant consumption), the optimal exploitation strategy is to ensure that the stock remains constant over time. In the case of an abundant stock of resources, that is higher than the ====, there is an infinite number of solutions, and every optimal path converges decreasingly to this level.====The purpose of this work is to study the same question under the second Rawls criterion. First, we prove that there is a lower bound for the speed of convergence to the steady state of Ramsey–Cass–Koopmans models, even in cases where the discount factor is very close to ====. Based on this result, we show that economies under the two Rawlsian criteria have the same value function. The solutions coincide if the stock of a resource is low (below the ====). In this case, the optimal choice is to ensure that the stock remains constant over time. For the case where the resource is abundant (higher than the ====), the solution under the first criterion is also a solution under the second one.====This article is organized as follows. Section 2 introduces the two Rawlsian problems, describes the main properties of the first problem and solves the second one. Section 3 discusses the different criteria studied in the literature. Proofs are given in Appendix.",A tale of two Rawlsian criteria,https://www.sciencedirect.com/science/article/pii/S0165489622000312,6 May 2022,2022,Research Article,48.0
"Minagawa Junichi,Upmann Thorsten","Faculty of Economics, Chuo University, 742-1 Higashinakano, Hachioji, Tokyo 192-0393, Japan,Helmholtz-Institute for Functional Marine Biodiversity at the University of Oldenburg (HIFMB), Ammerländer Heerstraße 231, 23129 Oldenburg, Germany,Carl von Ossietzky Universität Oldenburg, Germany,Bielefeld University, Germany","Received 28 February 2022, Accepted 12 April 2022, Available online 22 April 2022, Version of Record 9 June 2022.",https://doi.org/10.1016/j.mathsocsci.2022.04.001,Cited by (0),"For an optimal commodity taxation problem under a consumption target with many consumer types, Minagawa and Upmann (2021) prove the existence of an optimal solution under three assumptions on (i) utility, (ii) welfare, and (iii) total demand. In this note we prove that the assumption on utility is sufficient for the stipulated property on total demand, so that the third assumption can be dropped from the existence result.","Minagawa and Upmann (2021) present an optimal commodity taxation problem under a consumption target with many consumer types and prove the existence of an optimal solution for this problem under assumptions on utility, welfare, and total demand functions. They show that the optimal solution obeys a taxation rule that is contrary to the inverse-elasticity rule: Higher prices should be charged for commodities with ==== price elasticities of demand. In this note we show that the assumptions on utility function postulated by Minagawa and Upmann (2021) are sufficient for the stipulated assumption on total demand, so that the latter can be dispensed with from the existence result. In this way, we provide a concise version of their existence result.",The generalised anti-inverse elasticity rule: A concise result,https://www.sciencedirect.com/science/article/pii/S0165489622000294,22 April 2022,2022,Research Article,49.0
Wang Yuntong,"Department of Economics, University of Windsor, Windsor, Ontario, Canada N9B 3P4","Received 1 November 2021, Revised 16 March 2022, Accepted 23 March 2022, Available online 31 March 2022, Version of Record 14 April 2022.",https://doi.org/10.1016/j.mathsocsci.2022.03.003,Cited by (0),This paper considers the river sharing problem with incomplete information. We ask whether the efficient allocation of water in a river network is possible when agents have ,"In a large variety of markets, buyers and sellers interact directly with each other through network connections, and the various prices that are determined for each individual pair of buyers and sellers in the market equilibrium depend partially on the positions of the buyers and sellers in the network. Also, the allocation of the goods between the buyers and sellers in the equilibrium must satisfy the relevant network restrictions. Generally, these markets belong to the family of the so-called ==== in economics, which in recent years have received increasing attention from researchers (Sharkey, 1995, Kranton and Minehart, 2001, Jackson, 2010).====The river sharing problem is an important example of the network models in resource allocation, and has been extensively studied in recent years (Béal et al., 2013). In practice, water trading along river networks has been commonly used in water management in many parts of the world to resolve water shortage problems (Dinar and Wolf, 1994, Giannias and Lekakis, 1997, Young et al., 2000, Chong and Sunding, 2006).====The literature on the river sharing problem mostly focuses on the ==== model. Ambec and Sprumont (2002) consider the river sharing problem on a linear river and define the ==== solution based on the cooperative game theory. Along this line, recent contributions include Ambec and Ehlers (2008), Van den Brink et al. (2014), Houba et al. (2014), and Ansink et al. (2017), among others. An alternative approach, the so-called market-based approach, has also been considered. In this approach, it is assumed that riparian states participate in water trading voluntarily. Recent contributions to this literature include Lekakis (1998), Weber (2001), Wang (2011), and recently, Abraham and Ramachandran (2021).====There are three important aspects in the river sharing problem. The first is the number of agents involved. The more the number of agents, the more complex the problem. The second is the structure of the river network. A linear network is simpler than a ====. For example, a multi-source tree-structured river network is a nonlinear river network. The third is whether we have ==== on the utility functions of the agents. A river sharing problem with incomplete information is different from a river sharing problem with complete information.====In the cooperative game theory approach, the number of agents in a river sharing problem can be any positive integer. However, in the market-based approach, a river network with many agents would be more complex than a network with fewer agents. For example, Ansink et al. (2017) show that water trading between agents along a linear river may not emerge if there are four or more agents. They show that the potential benefits of water trade may not be sufficient to make all agents in the river cooperate and sign an agreement. In other words, expected water trading may not take place if there are four or more agents.==== In real life, most river networks are tree-like nonlinear networks. It has been long recognized that river networks exhibit features of ==== (Mandelbrot, 1983), as small basins have a statistically similar structure with larger basins (Tarboton et al., 1988, LaBarbera and Rosso, 1989).====I argue that the most important aspect in the river sharing problem is the informational assumption on the agents’ utility functions. Following Kilgour and Dinar (2001), Ambec and Sprumont (2002), it has been customary to assume the complete information about the utility functions of the agents in the river sharing problem. However this assumption may not always be practical. Imagine a situation in which agents have various optimal demands, called satiation points, for water and these optimal demands are only known to themselves privately. Then the efficient allocation of water may be problematic if the allocation rule depends on agents truthfully reporting their optimal demands. The river sharing problem with incomplete information has rarely been considered in the literature.====With the above observations, in this paper we consider the river sharing problem of the following type: (1) The number of agents is less than four; (2) Agents are connected by a nonlinear network; and (3) Agents have satiation points and these satiation points are private information. Our main question is: Is efficient water allocation possible between three agents in a nonlinear river network with incomplete information?==== To be specific, we focus on the question of whether efficient water trading is possible in a three-agent river network that has two upstream agents and one downstream agent, as illustrated in Fig. 3 in Section 5.====We will proceed in three steps. First, we introduce a trading model on a tree network with incomplete information. We hope that this model is general enough for other network trading problems with incomplete information. For example, in supply chain management, suppliers have private information about their inventories and they may not report this information truthfully between relevant suppliers. Next, we turn to the river sharing problem with incomplete information. Finally, we focus on the river sharing problem in the three-agent river network as mentioned in the previous paragraph.====We take a mechanism design approach. We consider ==== mechanisms that ask agents to report their satiation points to a mechanism designer who then determines a feasible allocation of water and a list of monetary transfers between the agents. A mechanism is called an ==== if it is incentive compatible (truth-telling of the satiation points), individually rational (participating in the mechanism is at least as good as not participating), budget balanced (all monetary transfers are made between the agents themselves), and Pareto efficient (water is efficiently allocated between the agents).====We first provide a necessary and sufficient condition that guarantees the existence of an efficient trading mechanism for the river sharing problem in general (Proposition 1). Then we apply this result to our three-agent river network problem. We show that if each upstream agent’s endowment of water is sufficiently larger than the downstream agent’s endowment, then there exists an efficient trading mechanism (Theorem 1). We also construct an efficient trading mechanism in Theorem 1.====Theorem 1 is in contrast with Cramton et al. (1987). Under the linear utility assumption, Cramton et al. (1987) show that efficient trading mechanisms exist if the initial endowments of the agents are approximately symmetric. However, their result does not apply to the river sharing problem. In the river sharing problem, agents’ utility functions are often nonlinear (e.g., strictly concave), and agents in different geographic locations have very different (asymmetric) endowments of water (water rich or water poor).====The intuition behind Theorem 1 can be explained as follows. Under the nonlinear utility function, efficient allocation of water requires a positive amount of trading between all the agents. When the downstream agent’s (the buyer’s) initial endowment of water is less than his satiation point and the upstream agents’ (sellers’) endowments of water are more than their satiation points, the downstream agent would have no incentive to underreport his satiation point (valuation) and the upstream agents would have no incentive to over-report their satiation points. This is because the upstream agents’ and the downstream agent’s marginal utilities at their respective asymmetric initial endowments are separated apart and their valuation intervals would not overlap, and as a result, efficient trading is warranted.====This paper is related to Ambec and Sprumont (2002) and Ambec and Ehlers (2008), where both papers consider a linear river model with complete information. This paper is related to Myerson and Satterthwaite (1983) and especially Cramton et al. (CGK) (1987). Unlike CGK, here the goods, water, can only go in one direction, and buyers and sellers must be connected through a network. Another difference from CGK is that we consider ====. This paper is also related to Kranton and Minehart (2001), in which sellers each own one unit of an ==== good and buyers own none, and there are at least as many buyers as sellers. In addition, Kranton and Minehart consider linear utility. This paper is closely related to Lu and Wang (2010). In Lu and Wang, there is no restriction on who can be a seller or a buyer, and there is no network structural restriction on the way that the goods can be transferred between the agents. Moreover, here the utility function is different from Lu and Wang (2010). We assume that agents’ utility function is strictly decreasing after the satiation level is reached. In Lu and Wang (2010), utility function is non-decreasing on the entire domain.====The organization of the paper is as follows. In Section 2, we define the network trading model with incomplete information and the river sharing problem with incomplete information in particular. In Section 3, we discuss the efficient trading mechanisms that we consider for the river sharing problem. In Section 4, we discuss the efficient trading mechanisms under the quadratic utility function. In Section 5, we provide the main result of the paper. In Section 6, we conclude the paper.",The river sharing problem with incomplete information,https://www.sciencedirect.com/science/article/pii/S0165489622000270,31 March 2022,2022,Research Article,50.0
Choi Kangsik,"Graduate School of International Studies, Pusan National University, Republic of Korea","Received 21 May 2021, Revised 16 October 2021, Accepted 18 October 2021, Available online 18 March 2022, Version of Record 29 March 2022.",https://doi.org/10.1016/j.mathsocsci.2021.10.009,Cited by (1)," with the M-form, ==== is always greater than that with the U-form, while social welfare crucially depends on whether the excess burden of taxation is higher than a certain threshold. Ultimately, social welfare and ","Over the past few decades, there has been an interest in mixed oligopoly settings in which private profit-maximizing firms compete with public ones. Pioneering works by De Fraja and Delbono (1989) and Beato and Mas-Colell (1984) on mixed oligopolies employed game-theoretic analyses of public and private firms. State-owned enterprises often implement the government’s public policies, such as providing social goods to society at the expense of welfare loss—the implementation of public policies carries shadow costs (or an excess burden of taxation). Various theoretical studies have examined the effect of the shadow cost of public funds in a mixed oligopoly, and they have recently yielded new policy implications (Capuano and De Feo, 2010, Wang and Chen, 2011, Matsumura and Tomaru, 2013, Matsumura and Tomaru, 2015, Sato and Matsumura, 2019a, Sato and Matsumura, 2019b).====Although these theoretical studies have focused on the excess burden of taxation, little attention has been paid to the investigation of organizational forms [i.e., multidivisional form (M-form) and unitary form (U-form)],==== and managerial delegation within public firms competing against private firms in a multiproduct market.==== In other words, the effects of different incentives for public and private firms’ managers based on their internal organization and social welfare need to be analyzed.====According to Kawasaki (2021), “In the real-world economy, a public firm may provide multiple services. For example, in Japan, the national government supplies research and education services through universities, junior colleges and technical colleges, and local governments may provide both bus and streetcar services, as does the Kagoshima City local government. Private firms may also provide one or more of the same services as the public firm”. In terms of the M-form and the U-form, a similar case is that central and local governments’ decisions on privatization are discussed, for example, by Qian and Roland (1998) using the soft budget constraint model. However, this topic is rarely discussed in the literature on mixed oligopolies. Examples of differentiated local goods include museums, tourist facilities, and non-profitable hospitals. Airports and ports, among other things, may be more important. There are private, privatized, state-owned, and local government-owned airports. Those may be related to a differentiated mixed duopoly in a multi-city model in which a local public firm operates in one city and a private firm in the other. This can be interpreted as the M-form. However, in the centralized government, the regional (or national) government owns a public firm that maximizes the welfare of entire region, which can be interpreted as the U-form.==== As Laffont (2005, chapter 7) suggested, the question of multiregulation has many dimensions and the need to show clear understanding of the pros and cons of decentralization of regulation institutions.====Theoretically, Tirole (1994) suggested that the compartmentalization of responsibilities (i.e., delegation) across different government ministries or agencies can serve a useful function by placing the onus of creating, disclosing, and defending the information necessary for decision-making on the party particularly interested in the related dimension of policy. Although public and private firms differ in terms of ownership, there is extensive evidence that they both adopt managerial compensations based on financial performance (e.g., Lambert and Larcker, 1995, Ballou and Weisbrod, 2003). For example, Brickley and Van Horn (2002) reported that the turnover and compensation of public firm CEOs are significantly related to the return on assets.====Given these observations, we assume that both public and private firms adopt performance-based compensation schemes. Specifically, by introducing the excess burden of taxation, we theoretically analyze managerial delegation through the endogenous choice of U- and M-forms when multiproduct firms exist in mixed duopoly markets. A multiproduct firm can reallocate output within firms, and product mixing in firms is a ubiquitous practice. The prevalence of multiproduct firms has led to the application of multiproduct oligopoly models in policymaking. We found that the endogenous organizational form between public and private firms crucially depends on whether the excess burden of taxation is higher than a certain threshold. For the tax role, we discuss the following. If there is an excess burden of taxation, the profit of the public firms might be more valuable for welfare than the consumer surplus. This is because the government can use the profits of the public firms or the revenue from selling the stocks of the public firms for tax reduction in other markets, thereby reducing the deadweight loss in the markets. Thus, we introduce the excess burden of taxation in our analysis.====As Meade (1944) first highlighted, the government must resort to distortionary taxes on income, capital, or consumption in the absence of lump-sum transfers. In other words, if the government raises ====, society pays ====$1 in the absence of lump-sum tax instruments. Parameter ==== is usually known as the excess burden of taxation.==== In the literature on mixed oligopolies, Capuano and De Feo (2010) examined the welfare effect of privatization when the government considers the distortionary effect of rising funds through taxation.====Additionally, using Capuano and De Feo (2010) framework, Matsumura and Tomaru (2013) investigated the relationship between optimal subsidy schemes and privatization neutrality. They demonstrated that privatization affects the optimal subsidy rate unless the excess burden of taxation is zero.====
 Sato and Matsumura (2019a) indicated that the relationship between the excess burden of taxation and the optimal privatization policy is non-monotone. Its effects crucially depend on whether the excess burden is higher than a certain threshold. Moreover, Sato and Matsumura (2019b) examined the equilibrium privatization policy in the presence of the excess burden of taxation, wherein the government faces a time-inconsistency problem because the government cannot commit to avoid changing the future degree of privatization.====Furthermore, the strategic use of decentralized devices in oligopolies has recently been investigated from two main perspectives. The first stream of the literature has focused on the use of managerial contracts. The pioneering works of Vickers (1985), Fershtman and Judd (1987), and Sklivas (1987) found that the firm owner could enjoy strategic advantages through properly designed managerial delegation contracts based on sales and profits (the so-called FJSV delegation contract).==== The second stream, including Corchon (1991), Baye et al., 1996a, Baye et al., 1996b, Barcena-Ruiz and Espinosa (1999), Ziss (1999), and Gonzalez-Maestre (2000), has focused on the behavior of multidivisional firms with or without allowance for proper managerial contracts.====Meanwhile, the notion of strategic delegation in which the FJSV contract within a public and/or private firm is considered has been extensively analyzed in the context of a mixed oligopoly. Barros (1995) investigated the managerial delegation of public and private firms as strategic variables in a mixed duopoly, which can improve social welfare. White (2001) extended the analysis of Barros (1995) by demonstrating that only private firms hire managers under Cournot competition in an equilibrium.==== However, Barcena-Ruiz (2009) found that public and private firms hire managers under Bertrand competition, and welfare is lower if both types of firms hire managers than if neither does. Extending Barros’ setting, Heywood and Ye (2009) revealed that the optimal incentive contract for a public firm might increase or decrease welfare depending on the number of private firms and the exact nature of costs. Chirco et al. (2014) examined partial delegation implying that only market decisions are delegated to managers, while the choice of price and quantity remains in the firm owner’s hands.====Barcena-Ruiz and Espinosa (1999) and Ho and Sung (2014) examined multiproduct duopolists with managerial delegation. Barcena-Ruiz and Espinosa (1999) demonstrated that firms provide corporate incentives (i.e., U-form) when their goods are substitutes, and divisional incentives (i.e., M-form) when they are complements in a pure duopoly market (i.e., privatization) under Cournot and Bertrand competition.==== In contrast, although Ho and Sung (2014) treated the multiproduct mixed duopoly, they demonstrated that the owner of a public firm might overcompensate or undercompensate the manager when providing multiple services without considering organizational forms. However, this leads to managers behaving less aggressively in the profitable market. In the absence of the delegation problem, Dong et al. (2018) and Barcena-Ruiz and Garzon, 2005, Barcena-Ruiz and Garzon, 2017 analyzed whether state holding corporations with multiproduct firms should be privatized. They illustrated that the decision to merge depends on the degree to which goods are substitutes and on the percentage of government-owned shares in the multiproduct firm.====There are three ways our framework differs from that of previous studies, such as Barcena-Ruiz and Espinosa (1999), Barcena-Ruiz and Garzon, 2005, Barcena-Ruiz and Garzon, 2017, Dong et al. (2018) and Ho and Sung (2014). First, except for Ho and Sung (2014), previous studies on mixed oligopolies have examined both delegation and the endogenous choice of delegation from the viewpoint of a single product firm. However, they overlooked the endogenous choice of organizational forms between private and public firms. We focus on the endogenous determination of organizational forms while allowing for managerial delegation between private and public firms. Second, most studies on mixed oligopolies ignored the effects of the U-form and M-form organizational structures. We fill this research gap by comparing mixed duopolies with privatization as the introduction of multiproduct firms has non-trivial effects on organizational forms and social welfare. Finally, going beyond the studies mentioned above, we incorporate the excess burden of taxation into a mixed duopoly, and compare the optimal organizational structures in equilibrium. The result implies that social welfare and consumer surplus are always lower under privatization than in a mixed duopoly.====There may be concerns that the introduction of the organizational form is an oversimplification. However, it allows us to analyze how different incentives for public and private firms’ managers change their optimal internal organization and social welfare when comparing a mixed duopoly with privatization. Moreover, the optimal organizational structure between public and private firms is affected by the excess burden of taxation. The results of this study considering the excess burden of taxation provide a more realistic basis for policy implications. Our results are crucial for evaluating the impact of welfare after the privatization of public firms. Incorporating the excess burden of taxation into a mixed duopoly when public firms are assumed to be less efficient than private firms, Capuano and De Feo (2010) showed that privatization never increases welfare from the viewpoint of a single-product firm. Unlike the finding of Capuano and De Feo (2010), we demonstrated that, when the excess burden of taxation is relatively low or high in some countries, we can probably emphasize policy implications regarding country differences when considering the endogenous choice of organizational forms.",Organizational form and excess burden of taxation in a multiproduct mixed oligopoly,https://www.sciencedirect.com/science/article/pii/S0165489622000257,18 March 2022,2022,Research Article,51.0
"Crettez Bertrand,Nessah Rabia,Tazdaït Tarik","Université Panthéon-Assas, Paris II, CRED, EA 7321, 21 Rue Valette - 75005 Paris, France,IESEG School of Management, 3 rue de la Digue 59000 Lille, France,LEM-CNRS 9221, 3 rue de la Digue, 59000 Lille, France,CIRED, CNRS, EHESS, Ecole des Ponts, Nogent sur Marne, France","Received 12 April 2021, Revised 28 July 2021, Accepted 29 July 2021, Available online 17 March 2022, Version of Record 29 March 2022.",https://doi.org/10.1016/j.mathsocsci.2021.07.006,Cited by (1),"We propose a new notion of coalitional equilibrium, the strong hybrid solution, which is a refinement of Zhao’s hybrid solution. It is well suited to study situations where people cooperate within coalitions but where coalitions compete with one another. In the strong hybrid solution, as opposed to the hybrid solution, the strategy profile assigned to each coalition is strongly ====. We show that there exists a strong hybrid solution whenever preferences are partially quasi-transferable.","To study cooperation in strategic non-transferable utility games, one can rely on the concepts of coalitional equilibria. Those concepts differ in the way outsiders are assumed to react to the formation of a deviating coalition. The three traditional concepts of coalitional equilibria, all introduced in Aumann (1959), include the ====-core, the ====-core (Aumann, 1961, Aumann, 1959) and the strong Nash equilibrium (Aumann, 1959). An action profile is in the ====-core if no coalition can guarantee a higher payoff for each of its members by choosing another action, independently of the actions taken by the other players. On the other hand, an action profile is in the ====-core if no deviating coalition can guarantee a higher payoff for each of its members, because coalition outsiders have a dominant blocking strategy. In a strong Nash equilibrium, no coalition can guarantee a higher payoff for each of its members assuming that coalition outsiders stick to their equilibrium strategies.====Scarf (1971) gave existence results of ====-core in games with nontransferable utilities, which were later extended for local ==== and ====-cores by Yano (1990). Kajii (1992) showed the non-emptiness of the ====-core without assuming that agents’ preference relations are transitive or complete. Zhao, 1999a, Zhao, 1999b obtained general existence results for ==== and ====-cores in games with transferable utility. More recently, Uyanik (2015) addressed the existence of ====-core in discontinuous games. In addition, Yang, 2017a, Yang, 2018 generalized Kajii’s theorem to games with infinitely many players, Yang (2017b) considered the essential stability of ====-core and Yang and Meng (2017) tackled the Hadamard well-posedness of the ====-core.==== Martins-da Rocha and Yannelis (2011) established the existence of the ====-core in games where agents’ preferences are non-ordered and with infinite dimensional strategy spaces. Askoura et al. (2013) analyzed the ex-ante ====-core in games with uncertainty while Noguchi (2018) focused on the ====-cores in games with asymmetric information.====Yet, the coalitional equilibrium concepts initially defined by Aumann are probably too demanding. They require that ==== conceivable coalition should not break away from a prealably agreed strategy profile. As a consequence, the class of games for which the ====-core and the ====-core exist is not large. In that connection, it is worthy to note that in many “real-world” situations not all the coalitions are relevant (for instance, it is doubtful that certain countries can ever cooperate, so different are their views on the issue at hand, be it for international trade, environmental protection and so on). To put it another way, some coalition can form more easily than others.====Zhao (1992) is a major attempt to study coalition games with restricted formation of coalitions. Specifically, he introduces the ====, a solution concept pertaining to the case where there is cooperation inside coalitions, but competition between coalitions. The main issue here is the internal stability of the coalitions belonging to a ==== coalition structure.====Consider for instance Catalogna in Spain, Corsica in France, or Scotland in Great Britain. In each of these cases, a region (that is, a subcoalition of a given coalition) contemplates breaking away from an existing coalition (a Member State of the European Union). But to the best of our knowledge, none of these regions endeavors to join, or merge, with ==== member of the European Union (that is, another coalition).====In relation to the remark above, the hybrid solution is a strategy profile that is stable against deviations by any subcoalition of a coalition belonging to a given partition of the players. More precisely, any subcoalition of a given coalition ==== can be blocked by ==== decisions taken by the outsiders in the coalition, assuming that all the other players (in the other coalitions) stick to their strategies. Here, blocking means that a subcoalition cannot choose a strategy such that the payoff obtained by any of its members is strictly greater than what he obtains with the hybrid solution, when the outsiders in the coalition minimize his gain.====Zhao (====) showed the existence of the hybrid solution and Zhao, 1999a, Zhao, 1999b extended the notions of TU ==== and ==== cores to the hybrid solution.==== Existence of a version of the hybrid solution for games with non-ordered preferences, infinite dimensional action spaces (namely Hausdorff topological vector spaces) and infinitely many players was studied in Yang and Yuan (2019). In their approach, however, the definition of blocking is slightly different from Zhao’s. It is actually similar to the definition used in the ====-core.====This paper introduces a new coalitional equilibrium concept, namely the strong hybrid solution, that applies for any given coalition structure ====. This definition furthers the approach of Yang and Yuan (2019) in the sense that it introduces yet another definition of blocking. Here, we propose to say that an alternative strategy chosen by a subcoalition is blocked by the outsiders in the coalition if it cannot guarantee a payoff at least as great as the reference one for each of its members, and strictly higher than the reference one for at least one of its members, independently of the actions taken by all the other players (like in the hybrid solution, we assume that the players outside the coalition stick to their strategies). To wit, because coalition members are assumed to ====, there is no reason why they would not collectively choose an alternative strategy profile that does no good nor bad for all of them, and strictly benefits to at least one member. With this definition, one can show that the set of strong hybrid solution is a subset of the Yang and Yuan’s hybrid solutions, which are themselves a subset of Zhao’s hybrid solutions. Hence the assertion that the notion of strong hybrid solution is a refinement of the hybrid solution. Our main contribution is to provide an existence result for the strong hybrid solution. This existence result is obtained under the assumption that preferences are partially quasi-transferable.====The remainder of the paper is organized as follows. Section 2 sets out formal definitions, establishes some properties for the strong hybrid solution and compare it with alternative ones. Section 3 establishes our existence result for the strong hybrid solution. Section 4 studies the condition of partially quasi-transferable preferences in two economic models. Section 5 provides some concluding remarks.",On the strong hybrid solution of an n-person game,https://www.sciencedirect.com/science/article/pii/S0165489622000245,17 March 2022,2022,Research Article,52.0
"Bonifacio Agustín G.,Juarez Noelia,Neme Pablo,Oviedo Jorge","Instituto de Matemática Aplicada San Luis, Universidad Nacional de San Luis and CONICET, San Luis, Argentina,RedNIE, Argentina","Received 22 October 2021, Revised 28 January 2022, Accepted 2 March 2022, Available online 9 March 2022, Version of Record 17 March 2022.",https://doi.org/10.1016/j.mathsocsci.2022.03.001,Cited by (4),"In a many-to-many matching model in which agents’ preferences satisfy ==== and the law of aggregate demand, we present an algorithm to compute the full set of stable matchings. This algorithm relies on the idea of “cycles in preferences” and generalizes the algorithm presented in Roth and Sotomayor (1990) for the one-to-one model.","In many-to-many matching models, there are two disjoints sets of agents: firms and workers. Each firm wishes to hire a set of workers and each worker wishes to work for a set of firms. Many real-world markets are many-to-many, for instance, the market for medical interns in the UK (Roth and Sotomayor, 1990), the assignment of teachers to high schools in some countries (35% of teachers in Argentina work in more than one school). A matching is an assignment of sets of workers to firms, and sets of firms to workers, so that a firm is assigned to a worker if and only if this worker is also assigned to that firm. In these models, the most studied solution is the set of stable matchings. A matching is stable if all agents are matched to an acceptable subset of partners and there is no unmatched firm–worker pair, both of which would prefer to add the other to their current subset of partners.==== In their seminal paper, Gale and Shapley (1962) introduce the Deferred Acceptance (====, from now on) algorithm to show the existence of a stable matching in the one-to-one model. This algorithm computes the optimal stable matching for one side of the market. Later, the ==== algorithm is adapted to the many-to-many case by Roth (1984).====In this paper, we present an algorithm to compute the full set of many-to-many stable matchings. In the one-to-one model, beginning from a stable matching and through a procedure of reduction of preferences, Roth and Sotomayor (1990) define a “cycle in preferences” that allows them to generate a new matching, called a “cyclic matching”, that turns out to be stable.==== They present an algorithm that, starting from an optimal stable matching for one side of the market and by constructing all cycles and its corresponding cyclic matchings, computes the full set of one-to-one stable matchings (see Irving and Leather, 1986, Gusfield and Irving, 1989, Roth and Sotomayor, 1990 for more details). The purpose of our paper is to extend Roth and Sotomayor’s construction to a many-to-many environment.====Our general framework assumes substitutability on all agents’ preferences. This condition, first introduced by Kelso and Crawford (1982), is the weakest requirement in preferences to guarantee the existence of many-to-many stable matchings. An agent has substitutable preferences when she wants to continue being matched to an agent of the other side of the market even if other agents become unavailable. Given an agent’s preference, Blair (1988) defines a partial order over subsets of agents of the other side of the market as follows: one subset is Blair-preferred to another subset if, when all agents of both subsets are available, only the agents of the first subset are chosen.==== When preferences are substitutable, the set of stable matchings has a lattice structure with respect to the unanimous Blair order for any side of the market.====In addition to substitutability, we require that agents’ preferences satisfy the “law of aggregate demand” (====, from now on).==== This condition says that when an agent chooses from an expanded set, it selects at least as many agents as before. Under these two assumptions on preferences, the set of stable matchings satisfies the so-called Rural Hospitals Theorem, which states that each agent is matched with the same number of partners in every stable matching. Substitutability of preferences and ==== ensure that suitable generalizations of the concepts of “cycle” and “cyclic matching” can be defined. To do this, given a substitutable preference profile and two stable matchings that are unanimously Blair-comparable (for one side of the market), we define a “reduced preference profile” with respect to these two stable matchings and show that this profile is also substitutable and satisfies ====. Next, we adapt Roth and Sotomayor’s notion of a cycle for our reduced preference profile and use this many-to-many notion of a cycle to define a cyclic matching. This new matching turns out to be stable not only for this reduced preference profile but also for the original preference profile. With all these ingredients we can describe our algorithm as follows. Given a preference profile, by the ==== algorithm compute the two optimal stable matchings, one for each side of the market. Pick one side of the market, say the firms’ side, and obtain the reduced preference profile with respect to the firms’ optimal and the workers’ optimal stable matchings. In each of the following steps, for each reduced preference profile obtained in the previous step, compute: (i) each cycle for this profile, (ii) its corresponding cyclic matching, and (iii) the reduced preference profile with respect to this cyclic matching and the worker optimal stable matching. The algorithm stops in the step where all the cyclic matchings computed are equal to the worker optimal stable matching. The firms’ optimal stable matching together with all the cyclic matchings obtained by the algorithm encompass the full set of stable matchings.====Several papers calculate the full set of stable matchings in two-sided matching models. McVitie and Wilson (1971) are the first to present an algorithm that computes the full set of one-to-one stable matchings. This algorithm starts at the optimal stable matching for one side of the market and then, at each step, breaks some matched pair and applies the ==== algorithm to the new preference profile in which the broken matched pair is no longer acceptable. This algorithm is generalized by Martínez et al. (2004) to a many-to-many matching market in which agents’ preferences satisfy substitutability. However, we provide an example that shows that the algorithm in Martínez et al. (2004) has an error: it stops before computing all stable matchings. We also give an intuition of why this happens.====Following the lines of Irving and Leather (1986) and Roth and Sotomayor (1990), Bansal et al. (2007) extend the notion of cycle to a many-to-many matching model in which each agent has a strict ordering over individual agents of the other side of the market.==== Among other results, they use cycles to compute the full set of stable matchings. Eirinakis et al. (2012) revise and improve the algorithm presented in Bansal et al. (2007). Moreover, they extend the algorithm for a model in which agents’ preferences satisfy the “max–min criterion”. This criterion establishes that agents rank stable matchings in a responsive manner. However, their assumptions are more restrictive than substitutability over subsets of agents and ====. For a many-to-one matching model with strict orderings over individual agents (Cheng et al., 2008), using the notion of cycles introduced by Bansal et al. (2007), show that broad classes of feasibility and optimization stable matching problems can be solved efficiently.====A different approach to compute the full set of stable matchings is presented by Dworczak (2021). For a one-to-one model, they generalize the ==== algorithm allowing both sides of the market to make offers in a specific order. The paper proposes a generalized ==== algorithm with “compensation chains” and proves that: (i) for each order of the agents, the algorithm obtains a stable matching, and (ii) each stable matching can be obtained as the output of the algorithm for some order of the agents.====Our paper is organized as follows. In Section 2 we present the preliminaries. The reduction procedure of preferences is presented in Section 3. Section 4 contains the definition of a cycle in preferences together with the algorithm that computes the many-to-many stable set. Concluding remarks are gathered in Section 5, where the error in Martínez et al. (2004) is discussed. All proofs are relegated to Appendix.",Cycles to compute the full set of many-to-many stable matchings,https://www.sciencedirect.com/science/article/pii/S0165489622000154,9 March 2022,2022,Research Article,53.0
"Crettez Bertrand,Morhaim Lisa","Université Panthéon-Assas, Paris II, CRED, EA 7321, 21 Rue Valette - 75005 Paris, France","Received 6 May 2021, Revised 6 December 2021, Accepted 24 February 2022, Available online 4 March 2022, Version of Record 15 March 2022.",https://doi.org/10.1016/j.mathsocsci.2022.02.003,Cited by (0),We study the valorization of ,"We study the valorization of a cryptocurrency in an overlapping generations model where agents have the choice between cryptocurrency and central bank currency. We pay attention to three features of cryptocurrency: it is a risky asset (because of frauds and hacks), which one acquires by bearing transaction costs, and its supply reaches its upper limit in a finite time (e.g., Bitcoin). We consider both the standard case with linear–quadratic utility functions and the more general case where utility functions are not specified. In both cases, the only source of uncertainty in the model is the hack risk. This source of uncertainty, however, is compatible with deterministic currency prices. Concentrating on these prices, we find that the effect of an increase in cryptocurrency on the currency prices depends on the transaction costs that individuals bear to get cryptocurrency. In particular, when these costs are convex non-increasing in the quantity of cryptocurrency, the cryptocurrency price may increase with this quantity. Otherwise, when the transaction costs are either increasing or concave non-increasing, the cryptocurrency price decreases with its supply. We also show that the cryptocurrency and the central bank currency prices always move in opposite directions.====This paper contributes to a burgeoning literature devoted to the economics of cryptocurrency. Biais et al. (2018) present an overlapping generations equilibrium model of cryptocurrency pricing and confront it to data on bitcoin transactional benefits and costs. Their model emphasizes that the fundamental value of the cryptocurrency is the stream of net transactional benefits it will provide, which depend on its future prices. The link between future and present prices implies that returns can exhibit large volatility unrelated to fundamentals. Consistent with the model, estimated transactional net benefits explain a statistically significant fraction of bitcoin returns.==== Also in an OLG setting, Garratt and Wallace (2018) study the indeterminacy of the equilibrium exchange rates between two currencies by introducing storage costs for the central bank currency and a risk of currency crash for the cryptocurrency. Their conclusion is that little can be said about the relative value of several cryptocurrencies. Yet another OLG model is studied in Saleh (2018). He contrasts Proof-of-work blockchains with Proof-of-burn ones.==== Saleh shows that Proof-of-burn blockchains result in less price volatility than Proof-of-work’s.====Schilling and Uhlig (2019) also obtain an indeterminacy result albeit in a model with two types of agents who have infinite decision-horizon (they also obtain conditions under which there is no speculation in the cryptocurrencies).==== In a search setting, Fernández-Villaverde and Sanches (2019) extend the model by Lagos and Wright (2005) to multiple currencies and study the stability and welfare implications of the private supply of money alongside a government-backed currency.==== Fernandez-Villaverde and Sanches show that the existence of an upper bound on the available supply of each brand makes privately issued money in the form of cryptocurrencies consistent with price stability in a competitive environment. Table 1 compares the main approaches used in the literature.====This paper builds on Biais et al. (2018) but differs from it by studying the effects of a change in the quantity of cryptocurrency on the equilibrium currency prices and by assuming that cryptocurrency reaches its upper-limit in finite time. We also advance the study of the effect of a hack risk on the currency prices.==== While the magnitude of the hacking risk may be thought of as second order relatively to the large current volatility of cryptocurrency prices, we believe that this risk is likely to be a major concern in the event that cryptocurrencies are used on a large scale.====The remaining part of the paper unfolds as follows. Section 2 lays out our version of an OLG model with cryptocurrency and a risk of hacking. Section 3 is devoted to the study of the equilibrium. More precisely, because the policy setting is stable (the currency growth rates are constant) we believe that it is relevant to study equilibria where price expectations are stable too. This is why we first concentrate on the steady-state equilibrium that occurs when the cryptocurrency supply reaches its upper-limit. We illustrate this equilibrium with an example in Section 4. Section 5 briefly studies the interim dynamics between the date at which the cryptocurrency is introduced and the date at which its supply reaches its upper-limit. The interim prices are obtained by a backward induction argument from the steady state prices and the changes in the supplies of crypto- and central bank currency. Section 6 briefly concludes the paper.",General equilibrium cryptocurrency pricing in an OLG model,https://www.sciencedirect.com/science/article/pii/S0165489622000142,4 March 2022,2022,Research Article,54.0
Knoblauch Vicki,"Department of Economics, University of Connecticut, 1 University Place, Stamford, CT 06901, USA","Received 20 June 2021, Revised 19 February 2022, Accepted 21 February 2022, Available online 28 February 2022, Version of Record 11 March 2022.",https://doi.org/10.1016/j.mathsocsci.2022.02.002,Cited by (1),". Four examples launch an exploration of the relationships between lexicographic complexity and intransitivity and between lexicographic complexity and incompleteness. Two examples of 1–1 lexicographic representations exhibit a strong lexicographic flavor in that, for each of the two examples, any two distinct reorderings of its coordinate functions result in representations of distinct binary relations.","Debreu (1954) observed that lexicographic preferences on Euclidean ====-space with ==== cannot be represented by a utility function. Chipman (1960) reacted to this and introduced lexicographic representations of complete and transitive preferences via an ordered set of utility functions. More recently, Mandler (2020) studied lexicographic representations of such preferences by means of binary criteria.====The present paper considers asymmetric relations on finite sets of alternatives; completeness and transitivity are not assumed. All such relations have ternary lexicographic representations (Tversky, 1969, Manzini and Mariotti, 2012). As a prelude to the discussions, an illustration is presented. Let ==== be a set of alternatives and let ==== be the relation on ==== defined by ====. Hence, there is a cycle of three alternatives, and ==== is not connected to these alternatives. Next, let ==== and let ==== denote the singleton binary relation ==== on ====. An example of a map that lexicographically represents the relation ==== is ==== such that ====, where ==== if ==== if ==== and ==== otherwise. Hence, ==== The map is one-to-one; i.e., ==== entails ====. Let ==== denote the lexicographic ordering on ====. For each ==== in ====, one can check that ==== if and only if ====. In short, the map ==== is a 1–1 lexicographic representation of the relation ====. The Cartesian product ==== is said to be the ==== of ====. In this example, there are four alternatives and the map ==== has ==== as target set. However, the map ====, defined by ====. is another 1–1 lexicographic representation of ==== and has ==== as its target set.====The representation ==== exhibits an efficiency of representation that is the topic of this paper; in this study we look for the minimal number of factors in the Cartesian-product target set ==== needed in order to obtain a 1–1 lexicographic representation for a given asymmetric binary relation. This minimal number is said to be the ==== of the asymmetric binary relation.==== The lexicographic complexity concept has multiple uses in the study of asymmetric binary relations on finite sets:====(1) it provides a measure of the ease or difficulty of representing an asymmetric binary relation simply, that is, lexicographically using ternary criteria;====(2) it provides a measure of the non-representability by a utility function of an asymmetric binary relation;====(3) it further informs us about utility representability in that it possesses surprising and surprisingly similar relationships with intransitivity and incompleteness, two properties of binary relations on finite sets that prevent utility representation. The paper is organized as follows. Section 2 introduces the formal framework, proves a general representation result, and presents two examples. The asymmetric relations in these two examples and in the six examples in later sections find their origins in round-robin tournaments, sports league standings, majority-wins, head-to-head voting and rankings of cars in an automotive magazine.====Section 3 establishes upper and lower bounds for the lexicographic complexity of an asymmetric binary relation as a function of the cardinality of the set of alternatives. Three more examples are presented and the lexicographic complexity is calculated for each of the binary relations in the five examples in Sections 2 Lexicographic representation of asymmetric binary relations on finite sets, 3 Lexicographic complexity.====Section 4 adds three more examples to the mix. Together these eight examples provide some intuitions about the relationships between lexicographic complexity, intransitivity and completeness. Results provided for ==== relations (for which all alternatives are related to the same number of other alternatives) confirm these intuitions.====Section 5 discusses lexicographic flavor (the property of a representation for which any two distinct re-orderings of co-ordinate functions result in distinct representations) and a conjecture is formulated on the minimum lexicographic complexity for homogeneous asymmetric binary relations.====Section 6 consists of a short review of related studies.",The lexicographic complexity of asymmetric binary relations,https://www.sciencedirect.com/science/article/pii/S0165489622000130,28 February 2022,2022,Research Article,55.0
Harrison-Trainor Matthew,"Department of Mathematics, University of Michigan, Ann Arbor, Michigan 48109, United States of America","Received 9 April 2021, Revised 10 January 2022, Accepted 12 January 2022, Available online 10 February 2022, Version of Record 19 February 2022.",https://doi.org/10.1016/j.mathsocsci.2022.01.002,Cited by (1),"In an election in which each voter ranks all of the candidates, we consider the head-to-head results between each pair of candidates and form a labeled directed graph, called the margin graph, which contains the margin of victory of each candidate over each of the other candidates. A central issue in developing voting methods is that there can be cycles in this graph, where candidate ==== defeats candidate ====, ==== defeats ====, and ==== defeats ====. It is known that such cycles are unlikely to occur. Under the Impartial Culture assumption, in a random election with three candidates and a very large number of voters there is a 91.23% chance of avoiding a cycle. By studying the geometry of the space of random elections, we give a mathematical explanation of why this is the case. Our main result is that margin graphs that are more cyclic in a certain precise sense are less likely to occur. This connects the probabilistic study of voting methods to Zwicker’s analysis of Condorcet’s paradox in terms of cycles and cuts.","The Condorcet paradox is a situation in social choice theory where every candidate in an election with three or more alternatives would lose, in a head-to-head election, to some other candidate. For example, suppose that in an election with three candidates ====, ====, and ====, and three voters, the voter’s preferences are as follows:====There is no clear winner, for one can argue that ==== cannot win as two of the three voters prefer ==== to ====, that ==== cannot win as two of the three voters prefer ==== to ====, and that ==== cannot win as two of the three voters prefer ==== to ====.====More formally, fix a set ==== of voters and a set ==== of candidates. Let ==== be the set of all linear orders on ====; we think of such a linear order as a ranking of the candidates. A ==== is a map ====, mapping each voter ==== to a ranking ==== of the candidates; we call ==== voter ====’s ====. So a profile is exactly the data we might get from an election. We write ==== if voter ==== prefers candidate ==== to ==== in the profile ====.====Given a profile ====, the ==== of one candidate ==== over another ==== is the margin of victory/loss of ==== over ==== in a direct comparison: ====If ==== we say ==== is ==== to ====. We will always consider the number of voters to be odd so that for every pair of candidates, one is majority preferred to the other. We can construct a labeled directed ==== ==== whose vertices are the candidates, and with an edge from ==== to ====, labeled with ====, exactly when ==== is majority preferred to ====. For example, the margin graph of the profile described above is ====
 Debord (1987) showed that any labeled tournament (with the weights of all edges of the same parity) is the margin of some profile. When we forget the margins of victory, we obtain a tournament which we call the ==== ==== of ====, e.g., ====The Condorcet paradox occurs when there is a cycle in the margin graph of a profile, so that there are candidates ==== such that ==== is majority preferred to ====, ==== to ====, and so on, and ==== is majority preferred to ====. If there is no cycle in the margin graph of a profile ====, then the margin graph is just a linear ordering of the candidates, and it is plausible that the winner should be the greatest candidate according to this ordering. A central problem of voting theory is to come up with a voting method—formally a function mapping each profile ==== to a set of winning candidates (or sometimes to a ranking of all the candidates)—which deals as well as possible with cycles in the margin graph.====Thus an important area of research has been to identify how often there occur cycles in the margin graph, both in historical situations and theoretically under various assumptions on the voters. We cite some references analyzing historical situations: [35] argues that various amendments in the Agricultural Appropriation Act of 1953 in the US House of Representatives formed a cycle; Bjurulf and Niemi (1978) found similar situations in the Swedish parliament; Stensholt (1999) found a cycle in a decision by the Norwegian national assembly; see also Deemen (2014), Kurrild-Klitgaard (2001), and Truchon (1998).====In this paper we take the more theoretical point of view where we assume that voters fill out their ballot randomly according to some probability distribution, and we consider the probability of a paradox occurring. There is also a large literature of results here. The book by Gehrlein (2006) is an excellent reference for what is known, as well as the books by Gehrlein and Lepelley (2011) and Gehrlein and Lepelley (2017) and the recent collection (Diss and Merlin, 2021).====To begin, we must make an assumption about the probability distribution of ballots for each voter. We assume for the rest of the paper that each voter is equally likely to pick any of the ==== linear orders on the ==== candidates, an assumption referred to in the literature as the Impartial Culture (IC) condition.====An important probability is the probability ==== with ==== voters and ==== candidates of having a Condorcet winner—a single candidate who is majority preferred to each other candidate. Such a candidate can be argued to be a clear winner, and an important class of voting methods, the Condorcet methods, select such a candidate as the winner. Using increasingly sophisticated methods, Sevcik (1969), DeMeyer and Plott (1970), Niemi and Weisberg (1968),  Garman and Kamien (1968), Gehrlein and Fishburn, 1976, Gehrlein and Fishburn, 1979 and Gehrlein (1999) calculated these probabilities for small numbers of voters and candidates. These values are included in Table 1. One can also calculate other related probabilities, such as the probability ==== of having a transitive majority graph. Gehrlein, 1988, Gehrlein, 1989 calculated these values for various ==== and ====.====Of course many elections in the real world have millions of voters. Guilbaud (1952) was the first to compute for three candidates the probability ==== of having a Condorcet winner. Niemi and Weisberg (1968) and Garman and Kamien (1968) calculated values of ==== for various numbers ==== of candidates (up to ====). Some of these probabilities are shown in Table 1 as well. For five or more candidates, these are numerical approximations of an integral for which no closed form expression is known. Gehrlein and Fishburn (1978) obtained expressions for ==== and ==== in terms of a single non-elementary integral. They also computed other probabilities such as that of being transitive or having a Hamiltonian cycle.====This paper is about this limiting case as the number of voters goes to ====. Consider the case of three candidates. There are eight possible tournaments on three candidates, six of which are transitive, and two of which are cycles (i.e., instances of the Condorcet paradox) (See Box I).====If each of these was equally likely to be the outcome of a random election, then the probability of a Condorcet paradox would be 25%. Instead, we see from Table 1 that the probability of a Condorcet paradox is less than 10%. So there is a tendency towards transitivity in a random election. The same pattern continues for more candidates as well.====Why is this the case? We were surprised to find that this question is almost never considered in the literature. Of course the ==== that there is a preference for transitivity, at least for small numbers of candidates, is already well-established by the computations of Table 1. But these are numerical computations that have little explanatory power; knowing that the relevant integral for three candidates evaluates to ==== does not explain ==== this is. For larger numbers of candidates, we only know that a Condorcet paradox is not so likely by the numerical calculation of an integral, which has no explanatory power. There seems to be a gap between our knowledge and our understanding. The main goal of this paper is to fill this gap and explain mathematically why there is a preference for transitivity in the outcomes of random elections.====
 Theorem 5.1 is the main technical result along these lines, but it will take more setup to explain what this theorem says and what it means.====There are two important ideas that we will build off. The first is due to Niemi and Weisberg (1968) and Garman and Kamien (1968) when calculating the values of ====. Fix a number ==== of candidates and consider the distribution of margin graphs produced by a random election with ==== voters. These margin graphs are labeled directed graphs with ==== vertices and ==== edges, and so we can view them as ====-dimensional vectors. We view a negative label as the same thing as a positive label in the other direction, so for example the following two margin graphs are considered to be the same: ====
 As the number ==== of voters go to ====, the Central Limit Theorem says that the distribution of ==== margin graphs approaches a multivariate normal distribution. (To normalize a margin graph, we divide the labels of the margin graph by ====.) The probability density function of this multivariate normal distribution is ====where ==== is an ==== matrix called the covariance matrix. The values ==== are then just integrals of this probability density function over the appropriate regions. Niemi and Weisberg (1968) and Garman and Kamien (1968) computed the matrices ==== for particular values of ==== and used numerical computations of these integrals.====The second main idea originated in Young (1974) and was used more explicitly by Zwicker (1991). Consider a profile ==== and the corresponding margin graph ====. The margin graph is a labeled directed graph, which one can think of as belonging to a vector space of labeled graphs; one adds two labeled graphs by adding the labels edge-by-edge, with the convention that a positive label in one direction is the same as a negative label in the other direction. Every labeled graph can be decomposed, in a unique way, as the sum of a cyclic component and an acyclic component. For example see Box II.====Note that, looking at the edge from ==== to ====, we have ====, using the aforementioned convention that a positive label on an edge ==== is the same as a negative label ====. The left-hand-side is the margin graph corresponding to a single voter who has the preference order ====. On the right-hand-side, the first term is a cyclic component, and the other two terms are acyclic. So even the transitive preferences of a single voter contains a cyclic component, which is masked by the acyclic components. Zwicker noticed that when combining the preferences of many voters, one adds the (transitive) margin graphs of the individual voters together, and it is possible for the acyclic components to cancel out revealing the cyclic components and resulting in Condorcet’s paradox. Zwicker argued that the cyclic component measures the tendency towards a Condorcet paradox (cycle in the margin graph), and that whether or not there is a paradox depends on whether the acyclic component is large enough to mask the cyclic component. This gives an insightful geometric explanation of how cycles (Condorcet’s paradox) can arise from combining seemingly acyclic individual preferences. In general, the vector space of margin graphs decomposes into the direct sum of two orthogonal subspaces, the ====, representing the cyclic component, and the ====, representing the acyclic component.====We now describe the new results of this paper. Fix a number ==== of candidates and consider the distribution of margin graphs (labeled graphs with ==== vertices) produced by a random election with ==== voters. Recall that as the number of voters goes to ====, the distribution of (normalized) margin graphs approaches a multivariate normal distribution with probability density function ====We begin by computing the covariance matrix ====
 (Theorem 4.1) and its inverse ====
 (Theorem 4.2) as a closed form for any number ==== of candidates. The computation of ==== is not essentially new (see Niemi and Weisberg, 1968, who compute a similar covariance matrix) but already the proof that ==== is always invertible for any number ==== of candidates allows us to answer a question of Holliday and Pacuit (a). A special case of this is that every tournament on ==== candidates has a positive probability ==== of occurring as the outcome of a random election with sufficiently many voters. See Section 11.====Now consider the probability density function given above. This is a continuous probability density function. Given a normalized margin graph ====, the value ==== should be interpreted as the relative likelihood that the value of the random variable would be close to ====; that is, the larger ==== is, the more likely that a random election will have a normalized margin graph close to ====. The term determining the size of ==== is ==== which is a quadratic form in ====. One can analyze such a quadratic form in terms of the eigenvalues and eigenspaces of ====. The matrix ==== is acting on the space of normalized margin graphs, which as described above form a vector space. In Theorem 5.1 we determine the eigenvalues and eigenspaces of ====. We show that there are always two eigenvalues, and that their corresponding eigenspaces are the cycle space and cut space that appeared in Zwicker’s work described above! Moreover, the eigenvalue of the cycle space is smaller than that of the cut space. We consider this the main and most surprising result of the paper, and it connects the computational and probabilistic perspective to the geometric perspective of Zwicker.====Perhaps the most valuable consequence of these results is that one gets a geometric intuition for what the probability space of random elections looks like (we will see that it has the shape of a higher-dimensional oblate spheroid). As with any sort of intuition it takes time to develop, and so we leave the development of this to the body of the paper. But we can still describe some of what is going on. Suppose that one has a normalized margin graph ====, and one wishes to determine ====, which measures the relative likelihood that a random election would result in a normalized margin graph close to ====. As in Zwicker’s work, one can decompose ==== into the sum of a cyclic component (in the cycle space) and an acyclic component (in the cut space). Then the value of ==== is completely determined by the sizes of these two components, and the larger the cyclic component is, the smaller ==== is. In particular, if we write ==== where ==== is in the cycle space and ==== is in the cut space, we have ====where the denominator is a fixed constant that depends on ==== but does not involve ====. Note that ==== is always smaller than ====, and so if we fix ====, ==== is maximized when ==== is completely cyclic (i.e., in the cycle space).====Put more qualitatively, these results might be stated: ====Recall Zwicker’s argument that the cyclic component of a margin graph measures the tendency towards a Condorcet paradox. Put together, this explains why there is a tendency away from cycles and the Condorcet paradox, and towards transitivity, in the outcome of a random election: margin graphs with cycles have a larger cyclic component, and hence are less likely to occur.====Much of the second half of the paper considers the following question: Given a particular tournament ====, what is the probability ==== of having that tournament ==== as the majority graph of a random election with a large number of voters ====? This is a problem that does not seem to have been much studied in the literature outside of particular special cases, e.g., ==== transitive (which coincides, up to a factor of ====, with the probability of having a Condorcet winner). In particular, how might we compare ==== for different tournaments ====? Recall that the majority graph just records which candidates beat which other candidates but not the margins, e.g., with four candidates, the majority graph might be: ====
 Ideally what we would like is a simple way to look at the two majority graphs and determine which is the more likely. Naturally, one expects that the more cyclic a majority graph ==== is, the lower ==== is. Unfortunately (and surprisingly!) we are unable to prove this. While we know how to compare the probability density function at different margin graphs, the probability of a majority graph is an integral of this probability density function over an orthant, and such integrals are not well-understood outside of certain special cases (see Ribando, 2006).====Given a tournament ==== on ==== vertices, we can assign a number to ==== which we call the ==== of ====: ====This value differs by a constant (depending on ====) from the following value: for each pair of edges meeting at a common vertex ====, add ==== if the edges are either both into or both out of ====, and subtract ==== otherwise. Linearity is maximized by a linear order and is related, as will be described in Section 8, to the decomposition into cycles and cuts. We conjecture that the more linear a tournament is, the more likely it is to arise as the majority graph of a random election. This conjecture is informed by the geometric understanding of the probability space developed in this paper, and we give an intuitive geometric argument for its truth. We also show that the conjecture is true for random elections conditional on all of the margins of victory being small. In Sections 5 Eigenvalues and eigenvectors, 9 Four candidates, 10 Five candidates we also verify the conjecture for all tournaments on ====, ====, and ==== candidates.",An analysis of random elections with large numbers of voters,https://www.sciencedirect.com/science/article/pii/S0165489622000087,10 February 2022,2022,Research Article,56.0
"Yang Zhe,Song Qingping","School of Economics, Shanghai University of Finance and Economics, Shanghai, 200433, China,Key Laboratory of Mathematical Economics (SUFE), Ministry of Education, Shanghai 200433, China","Received 17 July 2021, Revised 26 January 2022, Accepted 28 January 2022, Available online 4 February 2022, Version of Record 9 February 2022.",https://doi.org/10.1016/j.mathsocsci.2022.01.005,Cited by (0),"In this paper, we establish a generalized game with infinitely many players and pseudo-utilities. We shall define the weak ","Nash (1950) first proved the existence of noncooperative equilibria for normal-form games. Inspired by Nash (1950), Arrow and Debreu (1954) proved the existence of competitive equilibria for a production economy. Later, Shafer and Sonnenschein (1975) established generalized games (abstract economies), where each agent has an open-graph preference correspondence and continuous feasible strategy correspondence. By the fixed point theorem, Shafer and Sonnenschein (1975) gave the existence theorem of noncooperative equilibria for generalized games. Inspired by Shafer and Sonnenschein (1975), Yannelis (1987) analyzed the noncooperative equilibria of generalized games with a continuum of agents. Furthermore, Kim et al. (1989) proved the existence of noncooperative equilibria for generalized games with a measure space of agents and an infinite dimensional strategy space. Generally speaking, the first work on the equilibrium existence of markets with a continuum of agents is Aumann (1966). Later, Greenberg et al. (1979) used a different model to analyze the production economy with a continuum of agents. Recently, the work of Greenberg et al. (1979) was developed by Liu (2017) and Jang and Lee (2020). Moreover, following Aumann (1966), Noguchi (1997) considered a production economy with a continuum of consumers, a continuum of suppliers, an infinite dimensional commodity space and nonordered preferences.====From above articles, we know that the noncooperative equilibrium was studied for games and markets with infinitely (finitely) many players. As far as we know, the cooperative equilibrium was defined in the game theory and market theory in order to analyze the cooperative behavior. In general, the cooperative equilibrium is represented by core, ====-core, ====-core, strong Nash equilibrium, TU ====-core and so on. Scarf (1967) first proved the nonemptiness of nontransferable utility (NTU) core for an NTU cooperative game. Predtetchinski and Herings (2004) developed the work of Scarf (1967) and obtained the necessary and sufficient condition to have a nonempty core for an NTU game. Observe that the work of Scarf (1967) and Predtetchinski and Herings (2004) based the model with characteristic functions. On the other hand, Aumann (1961) first defined the cooperative equilibrium for the strategy form game. Following Aumann (1961), Scarf (1971) used the result of Scarf (1967) to prove the existence of ====-core for normal-form games. Inspired by Scarf (1971), Border (1984) gave the core existence theorem of generalized games with nonordered preferences, and the work of Border (1984) was extended to generalized games with externalities and infinite dimensional strategy spaces by Kajii (1992). Note that the proof techniques of Border (1984) and Kajii (1992) were inspired by Ichiishi (1981). The model of Ichiishi (1981) combined the concepts of noncooperative equilibria and cooperative equilibria. Zhao (1992) used another concept to combine two notions. Zhao (1992) defined the hybrid equilibrium, and Zhao (1996) analyzed the hybrid equilibrium of exchange economies with externalities. Unlike the proof techniques of Ichiishi, 1981, Border, 1984, Kajii, 1992 and Zhao, 1992, Zhao, 1996, Florenzano (1989) defined the group preference of every coalition, and proved the existence of the core for a coalitional production economy with nonordered preferences. The result was developed by Florenzano (1990) and Lefebvre (2001). Moreover, Zhao, 1999a, Zhao, 1999b defined TU ====-core and TU ====-core in normal-form games. Recently, Uyanık (2015) extended the work of Scarf (1971) and Zhao (1999a) to discontinuous games.====So far, there exist two research frontiers on the cooperative equilibria of strategy-form games. The first case is the model with infinitely many players. The second case is the model with incomplete information.====For the model with infinitely many players, Weber (1981) first defined the weak core of an NTU game with a continuum of agents, and gave its existence theorem. Inspired by Weber (1981), Askoura (2011) extended the result to normal-form games with a continuum of agents. Furthermore, Askoura (2017) improved the work by considering the equi-continuity of functions. Moreover, by assuming that the set of players is a compact topological space, Yang, 2017, Yang, 2018 and Yang and Yuan (2019) provided some generalizations of Scarf, 1971, Kajii, 1992 and Zhao (1992) to games with infinitely many players. Following Yannelis (1991a) and Holly (1994), Yang (2020) analyzed the weak ====-core of an exchange economy with a continuum of players and pseudo-utilities. Recently, Yang and Zhang (2021) extended the work of Kajii (1992) to games with a continuum of agents.====For the model with differential information, Yannelis (1991b) first considered the core of exchange economies and normal-form games with differential information. Recently, Askoura et al. (2013) considered the ex ante ====-core of normal-form games with incomplete information. Later, the work of Askoura et al. (2013) was improved by Noguchi, 2014, Noguchi, 2018, and was extended to exchange economies by Askoura (2015).====We shall follow the work on cooperative equilibria with infinitely many players in our paper. We think that there exist the following reasons such that the ====-cores of Scarf (1971) and Kajii (1992) were extended to allow infinitely many players. It is well known that there exist infinitely many and homogeneous agents for the classical perfect market. In a market with finite agents, the set of competitive equilibrium allocations is a proper subset of the core. By the technique of replication economy, Debreu and Scarf (1963) showed that the core of infinite replication economy coincides with the set of competitive equilibrium allocations. Observe that the number of agents is infinite in the infinite replication economy. On the other hand, Aumann (1964) assumed that the set of agents is a measure space, and also gave the coincidence result between the core and the set of competitive equilibrium allocations. In the model of Aumann (1964), the number of players is also infinite. Thus, the assumption of infinitely many players is important to analyze the perfectly competitive market from the purely theoretical viewpoint. To consider the market with infinitely many agents, we have the motivation for extending the work of Scarf (1971) and Kajii (1992) to generalized games with infinitely many players.====The rest of our paper is organized as follows. Section 2 recalls different models of generalized games and gives some remarks and extensions. Section 3 is the main result. Section 4 is the conclusion.",A weak ,https://www.sciencedirect.com/science/article/pii/S0165489622000117,4 February 2022,2022,Research Article,57.0
"Basile Achille,Rao Surekha,Bhaskara Rao K.P.S.","Dipartimento di Scienze Economiche e Statistiche, Università Federico II, 80126 Napoli, Italy,School of Business and Economics, Indiana University Northwest, Gary, IN 46408, United States of America,Department of Computer Information Systems, Indiana University Northwest, Gary, IN 46408, United States of America","Received 6 May 2021, Revised 24 December 2021, Accepted 12 January 2022, Available online 24 January 2022, Version of Record 22 February 2022.",https://doi.org/10.1016/j.mathsocsci.2022.01.001,Cited by (1),"We consider a set of voters who express preferences about two alternatives, indifferences included. We characterize the class of all strategy-proof and anonymous ==== by means of a geometric property related to an integer triangular grid representing the number of voters supporting each of the two alternatives.","Consider a set of voters who have to decide collectively about the implementation of one of two projects based upon the profile of their preference relations between the two alternative possibilities. Assume the voters may declare indifference. The collective choice corresponding to a preference profile, can be seen as assigned by a ==== (scf, for short). Ballots between two alternatives are common in real life. Moreover, it is far from rare that they determine the social choice according to rules “constitutionally”==== ====more sophisticated than just majority. This is especially the case if voters are allowed to express indifference. On the other hand, indifference can arise due to a variety of factors.====For example, think of a university department where, in order to approve a new proposal, at least a certain number of faculties must wish doing it. However, if the number of the ones for approval is not big enough, it is necessary that those who are against are less than a certain threshold. In this scenario indifference can be natural. Either deriving from lack of enough information to evaluate consequences, or from lack of interest, in case the proposal affects aspects of the departmental life which are not of interest for all. Moreover, a proposal can be approved even if the majority is against it.====A scf is ==== if permuting the preference profile of the voters does not change the chosen alternative. A scf is ==== if it makes the misrepresentation of preferences not profitable for the voters.====We study the class of the binary scfs which are anonymous and strategy-proof, and our main result is a geometrical characterization of such a class.====Let us name ==== and ==== the two alternatives. Anonymity implies that the collective choice only depends on the number of voters who prefer ==== and the number of voters who prefer ==== under a preference profile. So, every anonymous scf can be identified with an ====-valued mapping defined over the triangular integer grid composed of the ordered pairs that collect the support, i.e. the number of votes, for each alternative in each possible preference profile. Naturally, every scf partitions the triangular grid in two sets (one of which may be empty): the inverse images of the two alternatives. In other words, a preference profile falls into one of the two sets of the partition and this selects the collective choice.====Our main result is that adding strategy-proofness to anonymity, determines the shape of the sets of the partition. Indeed, they have to be separated by a vertical–horizontal zigzag line whose sequence of corners is properly identified. Equivalently, one can describe the scf in terms of listing a sequence of parameters that tidily express the horizontal and vertical segments of the mentioned grid which are associated to the alternatives. Hence, designing a scf becomes fixing the (unique) proper zigzag line, and it is visually represented.====Our paper is devoted to the ==== of social choice functions. The term representation refers to the identification of a relevant class of scfs (in our case anonymous, and strategy-proof scfs) with a class of functions for whose elements a specific functional form is given. A representation not only provides a characterization result, but also gives something more, i.e. a concrete way of producing all and only the desired scfs. Note that in [2] such representations are called ====.====If indifference is not allowed,  Moulin (1983, Corollary on page 63) provides a representation of the anonymous, strategy proof binary scfs. If indifference is allowed, Lahiri and Pramanik (2020, Theorem 2) provide a representation. In our paper we provide a substantively simpler, intuitive, and geometric representation of such anonymous and strategy proof social choice functions. This representation is unique, showing that the number of such functions increases exponentially with the number of voters.====The remaining sections of the paper are organized as follows. In Section 2, we present the notations and recall the main notions. In Section 3 we introduce dual monotonicity, by means of which the strategy-proofness of anonymous scfs is characterized. Section 4 is devoted to the representation of dually monotone functions and, therefore, of anonymous, strategy-proof binary scfs. Section 5 discusses the relation between our representation results and those obtained by Lahiri and Pramanik. Section 6 briefly concludes.",Geometry of anonymous binary social choices that are strategy-proof,https://www.sciencedirect.com/science/article/pii/S0165489622000075,24 January 2022,2022,Research Article,58.0
"Mahajne Muhammad,Volij Oscar","Univ. Lyon, UJM Saint-Etienne, CNRS, GATE L-SE UMR 5824, F-42023 Saint-Etienne, France,Ben-Gurion University, Beer Sheba, Israel","Received 29 November 2020, Revised 10 August 2021, Accepted 6 December 2021, Available online 13 January 2022, Version of Record 31 January 2022.",https://doi.org/10.1016/j.mathsocsci.2021.12.001,Cited by (2),"We say that a preference profile exhibits pairwise consensus around some fixed preference relation, if whenever a preference relation is closer to it than another one, the Kemeny distance of the profile to the former is not greater than its distance to the latter. We show that if a preference profile exhibits pairwise consensus around a preference relation, then this preference relation coincides with the binary relation induced by the Borda count. We also show that no other scoring rule always selects the top ranked alternative of the preference relation around which there is consensus when such consensus exists.","We study a standard setting of social choice in which there is a set of social alternatives and a group of voters, each of whom has a preference relation over this set. A preference aggregation rule identifies a subset of social preferences for every preference profile. The class of scoring rules is a well-known special class of preference aggregation rules in which each voter assigns a fixed list of ==== scores to the set of ==== alternatives according to their positions in his preference relation and ranks the alternatives according to the sum of their scores. The Borda rule is an instance of a scoring rule in which the successive scores are equidistant. Another preference aggregation rule is the Kemeny rule, which selects those preference relations that minimize the sum of their Kemeny distances to the preferences in the profile.====A minimal requirement of preference aggregation rules is that they satisfy the unanimity axiom. This axiom dictates that whenever there is unanimity on a given preference relation, namely, when all individuals share the same preference relation, the social preference must be this common preference relation. In this paper, we propose to strengthen the unanimity requirement by replacing the concept of unanimity on a preference relation with that of consensus around a preference relation. Roughly speaking, a preference profile exhibits pairwise consensus around some preference relation ==== if whenever a preference relation is closer to ==== than another one, the more in agreement are the preferences in the profile with the former than with the latter. Here, the level of agreement of a preference profile to a given preference relation is measured by the negative of the Kemeny distance. Our strengthened requirement, which we call the consensus property, dictates that whenever a preference profile exhibits consensus around some preference relation, the preference aggregation rule identifies precisely this preference relation as the social preference.====There are several preference aggregation rules that have the consensus property. In particular, the Kemeny rule satisfies it. This paper shows, however, that if we restrict attention to scoring rules, the only one that satisfies the pairwise consensus property is the Borda rule.====Although no scoring rule other than Borda satisfies the pairwise consensus property, one may suspect that the requirement that a preference profile exhibits consensus around some preference relation is strong enough to imply that some other scoring rules agree with Borda on their highest ranked alternatives. We show, however, that this is not the case. Specifically, we show that for any scoring rule other than Borda, there is a preference profile exhibiting consensus around some preference relation whose top-ranked alternative differs from the scoring rule’s top-ranked alternative.====One may wonder whether other scoring rules can be characterized by means of similar notions of consensus properties that differ only in the metric used to define them. We approach this question by showing that when the metric gives sufficiently higher weight to differences in the top-ranked alternatives, then consensus around a preference relation implies that this preference relation is consistent with the plurality rule, although it may not necessarily coincide with it. A similar observation holds for the inverse plurality rule.====Chebotarev and Shamis (1998) survey several existing characterizations of the Borda rule. In particular, we can mention Young (1974) and Nitzan and Rubinstein (1981). It is well known that for some preference profiles, the Kemeny and Borda rules disagree.==== There are several articles that compare the Kemeny and Borda rules in terms of the properties they satisfy or fail to satisfy. Can and Storcken (2013), for instance, propose the axiom of ==== which says that when a voter’s preference is updated towards the current social preference, this social preference remains the social preference under the updated profile. They show that whereas the Kemeny rule satisfies update monotonicity, the Borda rule does not. One can also construct examples in which a profile exhibits consensus around some preference relation ==== but this consensus is upset after one voter updates his preference relation towards ====.  Saari (2006), on the other hand, considers the ====, which dictates that adding or removing preferences that constitute a Condorcet cycle to a given preference profile does not affect the social preferences. Saari (2006) shows that whereas the Borda rule fulfills this requirement, the Kemeny rule does not. Similarly, it can be shown that the addition of preferences that constitute a Condorcet cycle to a given profile which exhibits consensus may well upset this consensus. Nevertheless, we show that whenever there is consensus, the Kemeny and Borda rules coincide and furthermore, are consistent with majority rule.====The paper is organized as follows. Section 2 contains basic definitions. In Section 3 we define the concept of pairwise consensus and prove our main result.",Pairwise consensus and the Borda rule,https://www.sciencedirect.com/science/article/pii/S0165489621001189,13 January 2022,2022,Research Article,59.0
"Kundu Rajendra P.,Kaur Harshil","Jawaharlal Nehru University, India,College of Vocational studies, University of Delhi, India","Received 19 April 2021, Revised 7 October 2021, Accepted 25 December 2021, Available online 6 January 2022, Version of Record 5 February 2022.",https://doi.org/10.1016/j.mathsocsci.2021.12.007,Cited by (1),"In this paper we consider a general class of rules called ==== under which the assignment of liabilities for losses arising out of interactions involving negative externalities can be coupled for some combinations of the levels of nonnegligence of the interacting parties and decoupled for other combinations, and explore the possibility of efficient assignment of liabilities in the presence of decoupling. The main result of the paper establishes that a simple liability assignment rule is efficient if and only if its structure is such that (i) whenever one party is negligent and the other is not then the negligent party is made to bear the full loss and the nonnegligent party bears none; (ii) whenever both parties are negligent they are made to together bear at least the full loss; and (iii) whenever both parties are nonnegligent they are made to together bear at most the full loss. Thus it follows that the assignment of liabilities under an efficient rule has to be coupled only when one party is negligent and the other is not and hence decoupling liability is not inconsistent with efficiency.","The problem of inefficiencies arising out of the presence of negative externalities has been extensively studied in economics and it is well known that imposition of appropriately designed taxes is one solution to the problem.==== The most important among the alternative solutions is the legal remedy of assignment of liabilities between interacting parties. Courts from across the world are routinely required to decide on matters relating to assignment of liabilities for accidental losses due to negative externalities. A variety of rules are used by courts for this purpose. Which of these rules invariably induce the involved parties to make efficient choices is a key question addressed in the economic analysis of law of torts.====Assignment of liabilities under such rules could be coupled in the sense that the liabilities of all interacting parties taken together add up to the total loss or decoupled in the sense that the liabilities of all interacting parties taken together add up to more or less than the total loss.==== While efficiency properties of rules under which the assignment of liabilities is always coupled have been studied extensively the rules under which the assignment is not always coupled have received very little attention. It is generally believed that decoupling can yield better outcomes by invariably achieving efficiency not only in contexts where coupling can do the same but also in some contexts where no coupled rule is efficient.==== Contrary to this, Jain (2012) demonstrates the inconsistency of decoupled liability and efficiency even in contexts for which efficient coupled rules exist. This demonstration, however, is based on the analysis of only a small subclass of the class of all rules under which the assignment of liabilities is not always coupled. Needless to say that the limited applicability of this contrary claim leaves room for further exploration of the possibility of efficient assignment of liabilities in the presence of decoupling and such an exploration assumes its significance from the importance of determining the validity of the general belief about decoupling yielding better outcomes. This paper is a step in that direction. We study the efficiency properties of a class of rules under which the assignment of liabilities is not always coupled and demonstrate that the contrary claim about the inconsistency of decoupling and efficiency is not robust.==== Our results also indicate that the validity of the belief about achieving efficiency by decoupling liability when coupled rules are all inefficient is also questionable.====We exploit the framework which is now standard in the significant literature on the efficient assignment of liabilities.==== In this framework the analysis is usually done in the context of interactions between two risk-neutral parties who are strangers to one another. It is assumed that the loss, in case of accident, falls on one of the parties called the victim. The other party is referred to as the injurer. It is assumed that the expected accident loss from this interaction depends on the care levels of the two parties and the social objective is to minimize the total social costs which are defined as the sum of costs of care of the parties involved and the expected accident losses. The assignment of liabilities for the losses in case of occurrence of accident is usually based on the levels of nonnegligence of the victim and the injurer where nonnegligence of a party is defined with respect to a legally specified total social costs minimizing due care level. If there is a legally specified due care for a party and the party chooses a care level which is less than the due care specified for the party then the party is called negligent, otherwise the party is nonnegligent. The level of nonnegligence of a party is either 0 (indicating that the party is negligent) or 1 (indicating that the party is nonnegligent).==== A rule for the assignment of liabilities specifies the portions of the loss, in case of occurrence of accident, that the victim and the injurer have to bear for every possible combination of their levels of nonnegligence. An application of a rule is simply a complete specification of the possible care levels of the two parties, the expected loss function and the due care levels for each of the two parties. Given a rule and an application, expected costs of parties for every possible configuration of care levels can be defined. It is assumed that the parties know about the rule and the application and choose their care levels simultaneously to minimize their expected costs. Thus an application of a given liability rule is a two-player simultaneous move game of complete information. A rule for assignment of liabilities is efficient if and only if for every game that it can induce there is a Nash equilibrium and every Nash equilibrium is total social costs minimizing. In other words, a rule for assignment of liabilities is said to be efficient if and only if it always induces both parties to choose care levels that minimize total social costs.====The assignment of liabilities corresponding to any combination of the levels of nonnegligence of the interacting parties is said to be ==== if and only if the interacting parties together are made to bear the full loss and is said to be ==== if and only if the interacting parties together bear less or more than the loss. The standard rules used by courts are all examples of rules under which liabilities are coupled for all combinations of levels of nonnegligence of the parties.==== The rule under which the injurer pays tax equal to the harm and the victim bears her loss is an example of a rule under which liabilities are decoupled for all combinations of levels of nonnegligence of the parties. It has been established in the literature that while there are efficient rules under which liabilities are always coupled,==== rules under which liabilities are always decoupled are all inefficient.==== Thus it appears that the notion of decoupled liability is inconsistent with efficiency.====In this paper we consider a very general class of rules called ==== under which the assignment of liabilities can be coupled for some combinations of the levels of nonnegligence of the interacting parties and decoupled for other combinations and explore the possibility of efficient assignment of liabilities in the presence of decoupling. The main result of the paper establishes that a simple liability assignment rule is efficient if and only if its structure is such that (i) whenever one party is negligent and the other is not then the negligent party is made to bear the full loss and the nonnegligent party bears none; (ii) whenever both parties are negligent they are made to together bear at least the full loss; and (iii) whenever both parties are nonnegligent they are made to together bear at most the full loss. Thus it follows that the assignment of liabilities under an efficient rule has to be coupled only when one party is negligent and the other is not and hence decoupling liability is not inconsistent with efficiency.====The paper is organized as follows: The model is presented in Section 2. All definitions and assumptions are stated here and are illustrated with appropriate examples. Section 3 contains the main result of the paper in the form of Theorem 1 and also contains the intermediate results (Proposition 1, Proposition 2, Proposition 3, Proposition 4, Proposition 5, Proposition 6) which are used to prove the theorem. Section 4 concludes the paper with a discussion on the implications of the results.",Efficient simple liability assignment rules: A complete characterization,https://www.sciencedirect.com/science/article/pii/S0165489621001244,6 January 2022,2022,Research Article,60.0
"Aubin Jean-Baptiste,Gannaz Irène,Leoni Samuela,Rolland Antoine","Univ Lyon, INSA Lyon, UJM, UCBL, ECL, ICJ, UMR5208, 69621 Villeurbanne, France,ERIC, EA 3083, Université de Lyon, Université Lumière Lyon 2, 5 Pierre Mendès France, 69596 Bron Cedex, France","Received 14 April 2021, Revised 17 December 2021, Accepted 19 December 2021, Available online 28 December 2021, Version of Record 14 January 2022.",https://doi.org/10.1016/j.mathsocsci.2021.12.006,Cited by (0),"This article aims to present a unified framework for grading-based voting processes. The idea is to represent the grades of each voter on ==== candidates as a point in ==== and to define the winner of the vote using the deepest point of the scatter plot. The deepest point is obtained by the maximization of a depth function. Universality, unanimity, and neutrality properties are proved to be satisfied. Monotonicity and IIA are also studied. It is shown that usual voting processes correspond to specific choices of depth functions. Finally, some basic paradoxes are explored for these voting processes.","Balinski and Laraki (Balinski and Laraki, 2007, Balinski and Laraki, 2014, Balinski and Laraki, 2020) have developed the “theory of measuring, electing and ranking”. The authors show that voting procedures, based on evaluations rather than on rankings, satisfy valuable properties. They propose a voting rule, the majority judgment, based on the medians of the evaluations and prove that it does not fall in the scope of Arrow’s impossibility theorem (Balinski and Laraki, 2007). This property emerges from the information contained in each vote. This framework, based on the grading of the candidates by the voters, gives more nuanced information than a ranking-based setting. The grading model has encountered much interest in the last decades, and alternatives to majority judgment are, for example, approval voting (Brams and Fishburn, 2007) and range voting (Smith, 2000).====This article aims to present a unified framework for grading-based voting processes, study some of their properties, and extend the scope of voting processes. This family of social decision functions is based on the statistical notions of depth functions and their related deepest points. Let us consider in the following that we have ==== voters and ==== candidates. Each voter gives a grade to each candidate. Each voter can then be assimilated to a point in ====, whose coordinates are the grades for each candidate. The set of all voters’ grades can, hence, be seen as a scatter plot. The key idea is to consider the ==== voter in this scatter plot. According to their grades, this innermost (possibly imaginary) voter can be seen as the most representative of all voters. Hence, their preferences should meet a large consensus among the others voters. Therefore, the presented social decision function returns simply the candidate who has the maximum grade of this innermost voter. We refer to any voting process based on the use of such notion of the ==== voter as a “deepest voting” process.====The definition of the deepest point of a scatter plot is a well-known research topic in statistics. It was initiated in 1975 by Tukey, who defined a multivariate median of a given multivariate data cloud (Tukey, 1975). The idea is to introduce a depth function, which associates to each point a value, so that the depth function is maximal at the innermost point of the scatter plot and minimal for outliers. Many notions of data depth have been introduced since then, proposing alternative definitions of the deepest point. See ==== the monographs of Zuo and Serfling (2000a) and Mosler (2013). Depth functions vary regarding their computability and robustness and their sensitivity to reflect the shapes of the data. Our claim is that it can bring an interesting viewpoint for voting processes. The properties of the depths functions are linked with the properties of the associated voting process and some new voting procedures can be proposed, based on different notions of depth functions.====The paper is organized as follows. Section 2 recalls the definition of Balinski and Laraki (2007)’s grading model. In Section 3, we recall the statistical notion of depth function which will help us to determine the innermost voter, knowing that there are many ways of choosing the “center” of a scatter plot. Each way of choosing it corresponds to a new member of the family of social decision functions. We next give the definition of the deepest voting process. We show in Section 4 that main usual conditions on voting processes – non-dictatorship, universality, unanimity – are satisfied by classical depth functions. We also characterize monotonicity and independence to irrelevant alternatives with respect to the behavior of the depth functions. We establish that some usual depth functions such as halfspace or projection, do not lead to monotone voting procedures, which seems a main drawback. Finally, we study some interesting properties for a given family of depth functions in Section 5. This family includes, to our knowledge, all grading-based voting processes, namely, majority judgment, range voting and approval voting. We show that voting processes of this family may suffer from Condorcet winner, Condorcet looser, reinforcement and no-show paradoxes. We provide a discussion about the pertinence of viewing these properties as paradoxes in the grading-based context. All the proofs are given in Appendix.",Deepest voting: A new way of electing,https://www.sciencedirect.com/science/article/pii/S0165489621001232,28 December 2021,2021,Research Article,61.0
de Boisdeffre Lionel,"University of Paris 1 - Panthéon - Sorbonne, 106-112 Boulevard de l’Hôpital, 75013 Paris, France","Received 11 February 2021, Revised 6 October 2021, Accepted 15 December 2021, Available online 24 December 2021, Version of Record 6 January 2022.",https://doi.org/10.1016/j.mathsocsci.2021.12.005,Cited by (1),"We consider a two-period pure-exchange economy, where uncertainty prevails and agents, possibly asymmetrically informed, exchange commodities and securities of all kinds. Consumers’ c====”. A sequential equilibrium obtains when agents expect the ‘true’ price as a possible outcome on every spot market, and elect optimal market-clearing strategies. This so-called ”correct foresight equilibrium” (CFE) is shown to exist whenever agents’ anticipation sets include the minimum uncertainty set. When the CKRMC assumptions are restored, the CFE is shown to lead to an overarching concept of rational expectations equilibrium, which generalizes the classical concepts.","In economies where uncertainty prevails and agents may be asymmetrically informed, the issue of how markets may reveal information and how agents forecast and coordinate on equilibrium prices, is essential and, yet, debated. This problem is traditionally tackled by rational expectations equilibrium models, by assuming, along Radner, 1972, Radner, 1979, that “====”. Under this assumption, also called the rational expectations hypothesis (or REH), a map between the state of the environment (identified to agents’ joint signals when their information is asymmetric) and the equilibrium price is assumed to exist and to be known and agreed upon by all agents. A weaker, but close, rationality condition, presented below, is referred to as the “====” (henceforth, CKRMC).====The current paper drops all the above rationality conditions, as unnecessary to define and prove the existence of sequential equilibria with asymmetric information. It defines a notion of “====” (CFE) instead, which permits to generalize the various concepts of rational expectations equilibria, presented below.====The standard sequential equilibrium, along    Radner, 1972, Radner, 1979, is based on the REH, which has three components. The first one is degeneracy: the standard agent anticipates only one spot price, say ====, in each state of nature, say ====. The second component is agreement: all agents have the same anticipations of the spot prices. The first two components yield a map, ====, relating states to prices, upon which all agents agree. The final component of the REH makes the price expectation, ====, self-fulfilling and market-clearing ex post. That is, agents would actually observe, in each state ====, the very price, ====, that they had anticipated, which would clear their demands. As a consequence, the REH assumes that agents perfectly anticipate a same unique equilibrium price in every state.====The above hypothesis is clear when the states are random, realized at a second period (====), and when agents have the same information about them at a first period (====). Such is the REH of symmetric information in Radner (1972). When information is asymmetric, it unfolds slightly differently. In addition to the exogenous environment of the second period, say ==== (instead of ====), each agent, ====, receives (at ====) an information signal on the future environment, say ====, whose collection, ====, summarizes the joint information of all agents. At ====, the typical agent along the REH infers all relevant information from the observed prices (possibly asset prices) and exchange securities, whose future payoffs depend on the environment. At ====, all uncertainty is removed, assets pay off and agents exchange good(s) on the spot market. Along the REH, the observed prices define a map on the joint signal, ====, upon which all agents agree. Prices are said to be fully-revealing if the relation ==== holds whenever ====. In that case, the typical agent along the REH is assumed to infer (at ====) the realized signal, ====, from simply observing the equilibrium price, ====, and, thus, to infer the joint information of all other agents. Radner (1979) shows, in a particular model of asset trading with finitely many states and one single commodity, that rational expectations equilibrium prices, which are asset prices, exist generically in the economy’s parameter space, that are fully-revealing.====Such generic existence and full revelation properties of the Radner rational expectations equilibrium (henceforth, REE) stem from the joint three conditions of the REH, described above. CKRMC models, in particular Ben-Porath and Heifetz, 2011, Dutta and Morris, 1997 and McAllister (1990), among others, show that if one component of the REH is dropped, these properties change. Instead of meeting the REH, agents having CKRMC infer information from observed prices, a la Radner (1979),==== and only make updated forecasts (i.e., state-price expectations), which clear markets. They have common knowledge that all agents are so ‘====’.====The starting point of CKRMC models is to allow agents “====” (McAllister, 1990), p. 361). The word “====” in CKRMC models refers to the above signal, ====. Such a joint distribution of uncertainty differs from Radner’s (1979). In McAllister (1990), as in Radner (1979), numeraire securities are exchanged at the first period (====), which pay off at the second (====), and consumption of a unique good takes place at both periods. By appropriately restricting the sets of possible priors on the former distribution, McAllister reaches so-called “====” ex post, related to prices, which are consistent with CKRMC. Together with agents’ demands, these (security) prices define rational expectations equilibria, which include and extend Radner’s. Such equilibria satisfy CKRMC only. Along McAllister (1990), they always exist under mild conditions on agents’ information filtrations. These prices coincide with Radner’s whenever agents have a common price function. But equilibrium prices need no longer be unique, in every given state, and agents may have different explanations of how they obtain. CKRMC equilibrium prices also differ from Radner’s by allowing for a continuum of non fully-revealing prices on open sets of economies. And the range of equilibria may decrease with the proportion of informed agents.====The degree of generality of the rational expectations equilibrium varies with models. The new definition identifies equilibria to CKRMC outcomes. Dutta and Morris (1997) ignore the security market and allow for multiple consumption goods. This setting prevents agents to learn information from asset prices or markets along a so-called “====”, a minimal inference process, presented below, that we prefer to Radner’s (1979). There is one effective period (====), during which agents exchange their endowments and consume, possibly uncertain of the realized state, unless prices are fully-revealing. Dutta–Morris defines, for each ====, the ====th agent’s prior belief as (to simplify) a probability distribution, ====, over a set of state-price pairs, ====, that we call “====”. The agent’s preferences follow a V.N.M. expected utility over consumptions in realizable states, given the observed price, ====, the prior, ====, and the signal, ====.====
 Dutta and Morris (1997) distinguish, with increasing generality, the Radner equilibrium (REE), akin to the above, from the common belief equilibrium (CBE), or CKRMC outcome when all priors coincide, and from the belief equilibrium (BE), or CKRMC outcome given arbitrary beliefs.====As standard, Dutta and Morris (1997) restrict the set of priors, so that posteriors be consistent with CKRMC. That is, agents only make forecasts, ====, such that ==== is an equilibrium price, if state ==== is realized. We henceforth refer to such beliefs as “====”. The literature on CKRMC, in general, relies on the assumption that agents have perfect beliefs. In Dutta and Morris (1997), prior beliefs have, moreover, the same support amongst agents. Providing no general existence theorem for its equilibria, Dutta and Morris (1997) focus on the differences between the BE, CBE and REE concepts, and their respective informational properties. The focus is similar in other CKRMC models, which we henceforth call “====” models.====Thus, Ben-Porath and Heifetz (2011) generalize Dutta–Morris’ BE, by allowing agents to be uncertain of other agents’ beliefs. Beliefs and decisions of the generic ====th agent only belong to an agent’s “====”, ====, which is common knowledge. This generalized BE “====” (p. 2615). As the authors show, this milder assumption (than the common knowledge of beliefs) may significantly enlarge the set of equilibria, i.e., of CKRMC outcomes. Yet, it leaves the fundamental assumptions and results of CKRMC models unchanged. To our best knowledge, in all CKRMC models, agents’ forecasts, ====, are restricted to be equilibrium outcomes for some collection of beliefs amongst agents, and agents are always able to make inferences from prices a la Radner (1979). As explained below, the two assumptions are related.====One aim of the current paper is to show that agents with asymmetric information need not have rational expectations, along REH or CKRMC models, to infer information from markets and reach equilibrium. A so-called “====” (CFE) obtains, along Definition 2, when such rationality assumptions are dropped. The full existence of the CFE follows from a condition of “====” amongst agents and Theorem 1 of Section 3. Since the CFE does not require, but does not rule out, the above rationality assumptions, it also serves to construct an overarching rational expectation model and equilibrium, which embed the CKRMC standard ones. Section 5 shows how, by restricting the sets of admissible priors. It clarifies the assumptions underlying the standard rational expectations models, namely, the CKRMC and Radner inference assumptions. It shows that none is necessary to define equilibrium with asymmetric information.====The motivation for dropping rational expectations is thus clear. With symmetric information, rational expectations reduce to agents having ‘====’ of future prices, along Radner (1972). This assumption is criticized along Kurz and Wu’s (1996) comments: “====”. Radner (1982) himself acknowledges that it “====”. Yet, the sequential equilibrium has relied on perfect foresight so far. Under symmetric information, the CFE provides a notion of sequential equilibrium, where agents’ forecasts need no longer be ‘====’, but simply ‘====’ (see below).====Under asymmetric information, rational expectations of CKRMC models are tantamount to agents having perfect beliefs and making the Radner inferences. No argument is provided to explain why and how agents reach perfect beliefs. The sole justification is that CKRMC would summarize a “====” (McAllister, 1990), that all agents would have acquired. Yet, McAllister thus concludes his paper (p. 361): “====”. Thereby, he acknowledges that no formal justification of the CKRMC assumption exists yet.====CKRMC is similarly demanding as the REH. It indeed requires common knowledge of beliefs (or types): if one agent deviates from the perfect forecasts, other agents typically follow (unless beliefs are adapted, assumed to be fixed). Common knowledge is contradicted by the actual privacy of beliefs. Similarly, common knowledge of market clearing would imply to know the supply, demands, behaviours… No typical consumer would manage. The REE and CKRMC inferences are similar. Only one state ==== is realized when agents observe a price ====. However, they need know all other (non observed) prices, ==== for ====, to learn information from price ====. The condition is strong already with the Radner price map, when spot prices are unique. It is still stronger when several prices, on a same spot market, are studied, as in CKRMC models. Thus, rationality along CKRMC and the Radner REE are similarly demanding in terms of agents’ knowledge and reckoning skills.====If the perfect belief assumption has not been formally justified as yet, it is implicitly required to let agents make Radner inferences. Indeed, when a state, ====, is realized, and an equilibrium price, ====, is observed, an agent would only learn from price ====, along Radner (1979), if the support of her belief contains ==== and only consists of equilibrium outcomes. Without their having perfect beliefs, agents would not make the Radner inferences. And the fact that their beliefs fail to be perfect, in the real world, prevents agents to make Radner inferences, on actual markets.====If both the perfect belief and the Radner inference assumptions, which characterize CKRMC models, are unrealistic, is it possible to define a notion of equilibrium without? The current paper answers positively and proposes the CFE as a solution.====To replace the Radner inferences, markets, in the current paper, reveal information via a so-called “====” (see Cornet and De Boisdeffre, 2009 for finite economies with nominal assets, and De Boisdeffre, 2016 for arbitrary assets, forecasts and state spaces). The principle is as follows: if an agent hopes to have an arbitrage opportunity, along some prior forecast of hers, that arbitrage is automatically contradicted by the financial market. That agent, from simply observing the market, may rightly infer that her prior forecast was mistaken. This inference process converges in finitely many steps. It does not require markets to be at equilibrium. It leads financial markets, whether nominal or real, to preclude arbitrage. Such inferences are accessible to anyone. They require no awareness of agents on the economy or on the other agents. Their completion, which results in agents having ‘====’ (instead of ‘====’) beliefs, is a minimal condition of equilibrium.====As the current model does not assume the existence of a Radner price function, agents’ uncertainty needs be represented by sets of forecasts, as in CKRMC models, instead of sets of states. These sets of forecasts are given exogenously at the time of trading, after agents have exhausted their inferences along the no-arbitrage principle. As the model drops rationality assumptions, agents’ beliefs are arbitrary probability distributions over arbitrary sets of forecasts. The supports of beliefs, that are agents’ uncertainty sets, need not be restricted to equilibrium outcomes, as in CKRMC models. This flexibility is consistent with observation. Ex ante, these supports are arbitrary. After agents have completed their inferences, the refined supports are arbitrage-free and, henceforth, fixed. Then, agents, observing prices, elect their strategies, being only aware of their preferences, endowments and beliefs.====Such are agents’ beliefs, inferences and behaviours, when they are unaware of the economy’s and other agents’ characteristics. As above, the current model has two periods (====), finite sets of agents, ====, of states, ====, a price set (for ==== goods), ====. It defines an agent’s belief as a probability distribution over forecasts, ====, and its support, ==== ==== (for ====). The latter are fixed and arbitrage-free at the time of trading (====). A sequential equilibrium is a collection of prices and strategies, which satisfies optimality, anticipation and market-clearing conditions. Equilibrium prices are observed at ==== (in a state denoted ====), and anticipated in each ==== state tomorrow (say, ====). Let ==== be the equilibrium price at ====. For each agent, ====, the equilibrium strategy consists of a consumption plan, or a continuous map, ====, and a portfolio, ====, which summarizes the agent’s (purchase or sale) positions in assets.====Equilibrium prices and strategies satisfy three conditions. First, agents’ beliefs are self-fulfilling, that is, ==== holds, for every ====. Thus, each agent rightly expects (at ====) that any forecast, ====, for ====, may prevail tomorrow (and one will). Second, each strategy, ====, for ====, is optimal in the budget set, only constrained by the observed prices, at ====, and the agent’s belief and endowment. Third, each market, which opens or may open, clears. Namely, the security and spot markets clear at ====, that is, ==== and ==== hold (where ==== is agents’ total endowment at ====). And each spot market, which may open at ====, clears agents’ conditional demands, i.e.,  ==== holds, for each realizable state, ==== (where ==== is agents’ total endowment in state ====).====The above set of self-fulfilling forecasts is ====, where ==== is the equilibrium price. Consistently, consumption plans, conditional to the equilibrium forecasts only, need clear. Tomorrow, exactly one equilibrium forecast will obtain, say ==== if state ==== is realized. So, the definition of equilibrium needs impose no other market-clearing condition than the above, namely, one in each state ====.====This is a major difference with CKRMC models. With perfect beliefs, ==== consumption plans, conditional to ==== posterior forecast, need clear. For instance, in Dutta and Morris (1997, p. 235), translated with slight abuse into the above definitions and notations, the relations ==== hold, for ====, where ====, for every ====, is the common support of agents’ beliefs. Section 5 shows that such demanding market-clearing conditions restrict the model’s possible values of endowments, enable Radner inferences, but are unnecessary to define equilibrium.====Equilibria with asymmetric information are thus defined when agents only know their own characteristics. The existence of such equilibria, or CFE, is now discussed. At a CFE, agents no longer have rational expectations and, yet, face no bankruptcy or unexpected price or event across periods. Ex ante, cautious agents would only rule out forecasts, which their private information contradicts. Typically they would anticipate uncountably many possible forecasts, making the model infinite dimensional. This complexity may explain why standard sequential equilibrium models have always assumed rational expectations, and never explored alternative price inferences to Radner’s. The current paper attempts to fill the gap. It recalls what information markets reveal without Radner inferences. It states a condition, which insures that beliefs are self-fulfilling and equilibria exist, for all assets and signals.====Because they lack CKRMC, agents are shown to face an incompressible uncertainty over the set of admissible forecasts, represented by a so-called ‘====’. Never empty, this set consists of all equilibrium outcomes that might result from all agents’ beliefs, which are private and arbitrary. Such an uncertainty is akin to Kurz and Wu’s (1996) “====”, seen as “====”.====That minimum uncertainty set, or a bigger set, might be inferred, we argue, by a tradehouse or financial institution from observing and treating past data on long time series. Consumers themselves would not have the required computational capacities. But they could build personal beliefs on public institutions’ forecasts. No agent or institution would anticipate equilibrium prices with certainty, because this would typically require to know every agent’s private characteristics and actions ex ante. Only a set of possible market-clearing prices might be inferred or, equivalently, the minimum uncertainty set, possibly endowed with a probability distribution. The precise location of future prices in that set would remain uncertain (see Section 3).====The correct foresight equilibrium (CFE) is thus defined as De Boisdeffre’s (2007) equilibrium, except for agents’ expectations, which need no longer be unique in every state, but form sets containing the true spot prices. The CFE, we argue, reconciles into one concept the sequential and temporary equilibria, which Grandmont (1982) describes as dichotomic. It is sequential, since anticipations are self-fulfilling. It is temporary, since forecasts are exogenously given and not endogenous to the model.====Along Theorem 1, below, whether financial assets be nominal or real and whether agents shared the same beliefs or disagreed about the future, a CFE exists whenever all consumers’ anticipations embed the minimum uncertainty set. The current paper proves this theorem in a model, which drops all forms of rational expectations.====The approach to information transmission and equilibrium we propose seems to model actual agents’ behaviours. Typical consumers have limited awareness, observational capacities and reckoning skills, hence, no CKRMC. They are unaware of the primitives of the economy, make exogenous anticipations and face uncertainty over future prices. They may infer a unique coarsest arbitrage-free refinement of prior expectations from observing trade, along De Boisdeffre (2016). Once reached, this refinement can no longer be improved, as high price volatility vanishes with arbitrage opportunities. Market forces, driven by prices, lead to equilibrium.====The paper is organized as follows: Section 2 presents the model and defines its concept of equilibrium, the CFE. Section 3 states the latter’s main existence property, Theorem 1, and discusses its informational assumption. Section 4 proves the Theorem. Section 5 relates CFE and standard rational expectations equilibria (REE). It clarifies standard REE assumptions and constructs a generalized concept of REE based on CFE. The Appendix proves technical Lemmas.",Dropping rational expectations,https://www.sciencedirect.com/science/article/pii/S0165489621001220,24 December 2021,2021,Research Article,62.0
"Tanaka Masato,Matsui Tomomi","Graduate School of Engineering, Tokyo Institute of Technology, Ookayama 2-12-1, Meguro-ku, Tokyo 152-8552, Japan","Received 26 January 2021, Revised 21 November 2021, Accepted 13 December 2021, Available online 21 December 2021, Version of Record 7 January 2022.",https://doi.org/10.1016/j.mathsocsci.2021.12.002,Cited by (0),"In this paper, we propose a pseudo polynomial size LP formulation for finding a payoff vector in the least core of a weighted voting game. The numbers of variables and constraints in our formulation are both bounded by ====, where ==== is the number of players and ==== is the total sum of (integer) voting weights. When we employ our formulation, a commercial LP solver calculates a payoff vector in the least core of practical weighted voting games in a few seconds. We also extend our approach to vector weighted voting games.","This paper deals with a special class of simple games, called ====, which constitute a familiar example of voting systems. In a weighted voting game, each player has a voting weight (intuitively corresponding to their contribution), and a coalition wins if the sum of its members’ weights meets or exceeds a given threshold.====Over the years, many power indices of voting games have been proposed and studied, such as the Shapley–Shubik index (Shapley and Shubik, 1954), the Banzhaf index (Banzhaf III, 1964), and the Deegan–Packel index (Deegan and Packel, 1978). The problem of computing the players’ power indices has received ample attention (see Matsui and Matsui (2000) for example) and its computational complexity is well-understood (Deng and Papadimitriou, 1994, Elkind et al., 2009, Garey and Johnson, 1979, Matsui and Matsui, 2001, Prasad and Kelly, 1990).====The focus of this work is on the solution concept called the ====, proposed by Maschler et al. (1979), which is a natural relaxation of the core. Elkind et al. (2009) showed some intractability results including coNP-hardness for checking the non-emptyness of the ====-==== of a weighted voting game. They proposed a pseudo polynomial time algorithm to compute a payoff vector in the least core, which is based on the (polynomial time) ellipsoid method and a pseudo polynomial time separation oracle. They also proposed a fully polynomial time approximation scheme (FPTAS) for the value of the least core. The ==== is a unique payoff vector with a lexicographically maximal excess vector, which is contained in the least core (e.g., see Schmeidler, 1969, Elkind et al., 2009). Pseudo-polynomial time algorithms for finding the nucleolus of a weighted voting game are proposed in Elkind and Pasechnik, 2009, Pashkovich, 2021.====In this paper, we propose a pseudo-polynomial size LP “formulation” for finding a payoff vector in the least core of a weighted voting game. The numbers of variables and constraints of our formulation are both bounded by ====, where ==== is the number of players and ==== is the sum of (integer) voting weights. Thus, a polynomial time algorithm for general linear programming problems finds a payoff vector in the least core in pseudo polynomial time. An advantage of our approach is that one can adopt his/her favored software (LP solver) for calculating a payoff vector in the least core. The computational experiment shows the effectiveness and efficiency of the proposed formulation. In the last section, we also extend our approach to vector weighted voting games.",Pseudo polynomial size LP formulation for calculating the least core value of weighted voting games,https://www.sciencedirect.com/science/article/pii/S0165489621001190,21 December 2021,2021,Research Article,63.0
"Barthel Anne-Christine,Hoffmann Eric","West Texas A&M University, 2501 4th Ave, Canyon, TX 79016, USA","Received 24 July 2021, Revised 27 September 2021, Accepted 14 December 2021, Available online 21 December 2021, Version of Record 7 January 2022.",https://doi.org/10.1016/j.mathsocsci.2021.12.004,Cited by (0),"The ==== (IFT) offers a way of deriving a correspondence between the parameter space and the ==== of a game. However, which equilibrium will actually emerge after a parameter change involves a dynamic adjustment process, which may significantly differ from IFT predictions. Utilizing the notion of local uniform ","Given an action space ====, a smooth best response function ====, and an equilibrium ==== at parameter ====, the implicit function theorem (IFT) offers a way of determining the solutions to the system ====so that one can define an equilibrium correspondence ==== locally about ====, where ====. Thus, for small changes from ==== to ====, one can determine the location of a new fixed point ==== by studying the comparative statics derivative ====. However, simply locating fixed points says nothing about whether or not players will actually learn to transition from ==== to ==== after the parameter change. Due to the extensive use of the IFT, it is therefore important to know when such dynamic processes will converge to the IFT equilibrium prediction, or when the two comparative statics predictions are “consistent”.====Our paper establishes both necessary and sufficient conditions for such consistency by further developing and extending recent results on local contraction mappings to parameterized settings. Given this new approach, we first show that in any smooth game with continuous best response functions, comparative statics predictions at a locally contraction stable equilibrium will be consistent. We then show that in the very broad class of games of strategic complements (GSC), local contraction stability is both a necessary and sufficient condition for consistency, which suggests that the notion of local contraction stability cannot generally be weakened. We also demonstrate that in monotone games, the notion of consistency under best response dynamics can be extended to consistency under the much broader class of learning rules known as adaptive dynamics, which include best response dynamics and fictitious play as special cases.====It is straightforward to construct examples in which locally unstable equilibria may in fact be reasonable predictions of the game, but at which the IFT derivative and the resulting adjustment processes significantly differ after a parameter change, as Fig. 1 demonstrates. Here, best responses at an original parameter ==== (solid lines) produce two Nash equilibria, ====, and ====. Best responses after parameter change from ==== to ==== (dotted lines) produces two new equilibria, ====, and ====. Notice that while the IFT prediction at ==== suggests a strictly lower equilibrium ==== after the change in parameter, best response dynamics beginning at ==== after the parameter change converge upwards to the equilibrium ====.====One may suggest that to avoid such inconsistencies, unstable equilibria such as ==== should be refined away due to them being unlikely outcomes of the game. However, instability only implies the existence of ==== initial starting points at which dynamics do not converge back to ====. In fact, any best response dynamic which begins below ==== converges back to ====, and hence by adjusting this example to our liking, we see that the region of starting points at which play will eventually lead back to the unstable equilibrium ==== can be made arbitrarily large. Hence, it is very possible for unstable equilibria to be reasonable outcomes of a game, but nevertheless be points at which the IFT prediction is at odds with subsequent adjustment processes.====Our paper shows that such inconsistencies arise because equilibria such as ==== fail to be locally ==== stable at ====, a slightly stronger notion than local stability. More precisely, we show that the IFT prediction will be consistent with the direction of subsequent adjustment processes at equilibria which are locally contraction stable, and hence determining the sign of the derivative of the former amounts to determining the direction of the latter.====Our results also provides insights into the mechanisms underlying the Correspondence Principle (CP), which has been the focus of much of the recent literature. Developed by Samuelson, 1941, Samuelson, 1947, the CP postulates that the sign of the IFT derivative may be determined by refining away locally unstable equilibria on the grounds of them being unlikely outcomes. The CP has enjoyed mixed success, and an overview of results can be found in Echenique (2008). While Arrow and Hahn (1971) and Echenique (2002) point out that the CP does not yield unambiguous comparative statics in general multidimensional models, Echenique (2004) and Christensen and Cornwell (2018) show that the CP obtains in models which encompass GSC. By showing that the IFT prediction is consistent with dynamics at locally contraction stable equilibria, our results show that the direction of adjustment dynamics lie at the core of the CP.====This paper is organized as follows: Section 2 introduces the basic model, while Section 3 includes the main results. Section 4 extends our analysis to adaptive dynamics in monotone games, Section 5 provides examples. Section 6 concludes with a discussion about the CP.",On dynamic adjustment and comparative statics via the implicit function theorem,https://www.sciencedirect.com/science/article/pii/S0165489621001219,21 December 2021,2021,Research Article,64.0
Courtin Sébastien,"Normandie Univ, Unicaen, CREM, UMR CNRS 6211, France","Received 6 May 2021, Revised 12 November 2021, Accepted 13 November 2021, Available online 19 November 2021, Version of Record 27 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.11.001,Cited by (0),"This work deals with the evaluation of decision power in multi-dimensional rules. Courtin and Laruelle (2020) introduced a decision process that specifies the collective acceptance or rejection of a proposal with several dimensions. The decision process is modeled as follows: (i) There are several individuals. (ii) There are several dimensions. (iii) Each of the individuals expresses a binary choice (“Yes” or “No”) on each dimension. (iv) A decision process maps each choice to a final binary decision (“Yes” or “No”). We extend and characterize six well-known power indices within this context: the Shapley–Shubik index (Shapley and Shubik, 1954), the Banzhaf index (Banzhaf, 1965), the Public good index (Holler, 1982), the Null individual free index (Alonso-Meijide et al., 2011), the Shift index (Alonso-Meijide and Freixas, 2010) and the Deegan–Packel index (Deegan and Packel, 1978).","At university, various subjects are taught throughout different periods. A student in a Master degree in economics (four periods) will take several tests, mostly in Microeconomics, Macroeconomics, Industrial organization, Game theory... The students’ success depends mainly on their various test scores. The final results are also correlated with the students’ progress from one year of study to the next. Can a period be compensated by another one? Are there any compulsory tests? Are the subjects (periods) equally weighted? In a parliament, when a bill is under discussion, the members have to express their choices on various sections of the bill. The decision process is key in the final enforcement of the bill. Are some sections more “important” than others? Do all the sections have to be adopted in order for the bill to be adopted? In different fields, the decision process is not a one-dimensional one==== since individuals express their choices on many questions simultaneously.====In this paper, we follow the model of Courtin and Laruelle (2020) who introduced multi-dimensional rules. The decision process is modeled as follows: (i) There are several individuals. (ii) There are several dimensions. (iii) Each individuals expresses a binary choice (==== or ====) on each dimension. (iv) A decision process maps each choice to a final binary decision (==== or ====) . The university example can be modeled by the following multi-dimensional rules: (i) the individuals are the subject which are supposed to be all taught in the different periods, (ii) the dimensions are the different periods, (iii) the result of the test in each subject and each period gives us the binary choice, (iv) the student does or does not obtain her degree. The associated multi-dimensional rules of the parliament example are such that: (i) the individuals are the members of parliament, (ii) the dimensions are the different sections of the bill, (iii) the vote of each member on each section gives us the binary choice, (iv) the output is dichotomous, i.e. the bill is or is not passed.====As a real-life example, one can present the parliamentary vote of the French government budget. The vote is organized in two steps: expenses are first approved or rejected by ministry after ministry, the search for a majority agreement on the overall expenses and tax revenues being made afterwards. Here the individuals are the members of the parliament, the dimensions are the different ministry, the dichotomous nature of choice refers to a “yes–no” position on each partial budget. The decision process is the following: as a first step, the parliament’s members make their global opinion on each partial budget (dimension) by simple majority and, in the second step, the global opinions of the parliaments members are aggregated by simple majority and the government budget is or is not adopted.====Courtin and Laruelle (2020) studied some of the properties of multi-dimensional rules, such as separability and weightedness. Separability refers to rules that can be decomposed by unidimensional rules, whereas weightedness concerns rules where each binary choice can be weighted.====Our aim, in this paper, is different. We focus on how a given individual can affect the final result. Given a student’s passing grades rule, how is “game theory” an important subject? Is a given member of parliament more influential than another? To answer these questions we use the tools that are power indices. Power indices evaluate the decision power of a given individual or a given dimension. In a single-dimensional context, Holler (1982) divided the different power indices into two categories: private and public. The common factor in the first category is that, once formed, the value of a coalition of individuals is shared among the coalition members even though it was created collectively. The amount of power received by individuals of a given coalition can be different. The second category assumes that the value of the coalition cannot be seen as a “private good” but as a “public good”. This means that once a coalition is achieved, all the individuals must be treated equally regardless of their own or of others’ manner of contribution (with the exception of those who do not contribute). In other words, individuals can have different contributions but share the same amount of power.====In this paper we extend these two categories to multi- dimensional rules. In particular, we consider two well-known private indices, the Shapley–Shubik index (Shapley and Shubik, 1954) and the Banzhaf index (Banzhaf, 1965). We also take into consideration three public indices: the Public good index (Holler, 1982), the Null individual free index (Alonso-Meijide et al., 2011) and the Shift index (Alonso-Meijide and Freixas, 2010). We also extend the Deegan–Packel index (Deegan and Packel, 1978) which is an index behind the private indices and the public indices. A full characterization of each of these six power indices is provided. Our characterization results can be seen as straightforward extensions of characterization results for the one-dimensional framework. Since the beginning of the use of game theory to study the distribution of power in voting systems, a wide collection of studies providing different games, different power indices notions have been developed. We understand our approach as a complementary step in this research line.====This paper is structured as follows. Section 2 introduces the general framework of multi-dimensional rules. Section 3 defines and characterizes the Shapley–Shubik index and the Banzhaf index for multi-dimensional rules, while the Deegan–Packel index and the public index for multi-dimensional rules category are discussed in Section 4. Section 5 concludes.",Evaluation of decision power in multi-dimensional rules,https://www.sciencedirect.com/science/article/pii/S0165489621001116,19 November 2021,2021,Research Article,65.0
Nosratabadi Hassan,"Université catholique de Louvain, Center for Operation Research and Econometrics (CORE), Voie du Roman Pays 34, 1348, Louvain-la-Neuve, Belgium","Received 9 September 2020, Revised 26 August 2021, Accepted 24 October 2021, Available online 6 November 2021, Version of Record 21 January 2022.",https://doi.org/10.1016/j.mathsocsci.2021.10.008,Cited by (0),"We study a model of choice where inferior alternatives may act as ==== that influence the appeal of those that are superior. In contrast to the current literature, the model allows reference points to ==== influence choice. In particular, facing multiple reference points, we assume that the decision-maker uses ====. We show that, as long as inferiors only break ties between superiors rather than reversing the choice between them, the model retains a strong connection to rational choice. More precisely, it is proved that the model is characterized by a “single-step” relaxation of an ==== of the ====.","A ==== is an inferior alternative that influences the choice between those that are superior. This influence – i.e., “the decoy effect” – takes two forms. A decision-maker (henceforth DM) chooses ==== from ====, despite choosing (i) ==== or (ii) ====, from ====. The first instance is normally referred to as “choice reversal” phenomenon, while the second is as “tie-breaking”.==== Needless to say, either form violates the ==== (WARP). This choice anomaly has received a noticeable attention in the literature. Recently, Ok et al. (2015) argue that, since it is indeed WARP that requires the choice in ==== to stay consistent with that of ====, we may relax WARP to incorporate the decoy effect in a broader model where ==== acts as an ==== that increases the appeal of ==== over ====.====While relaxing WARP on the triplet problems where the decoy effect occurs does enrich the model with reference-dependence, it does not directly address the question as to how DM ==== a collection of reference points. In fact, one can contemplate a number of models in regards to this question. One, for example, is where reference points are ordered in terms of their strength and where the strongest prevails. That is, as if the aggregation rule is “dictatorial” in the sense that the strongest reference point dictates as long as it is available. The aggregation behavior may otherwise be more permissive. For example, DM may choose an alternative if she can simply justify it from the lens of a single reference point; in other words, as if each reference point is “pivotal” to her. Of course, more rules are conceivable. Instead of taking a rule as given however, in this paper we ask: ==== To be more precise, is it possible to relax WARP to (i) allow inferiors to endogenously act as reference points, and (ii) attain a manner in which multiple reference points are aggregated? We show that such a practice is possible as long as the model only allows WARP violations of tie-breaking nature rather than choice reversal.====Let us explain how we achieve this goal. We start by formulating an ==== of WARP. We then relax this decomposition ==== on the triplets where the decoy effect may occur. The WARP-consistency on all other problems remains intact. We show that this “single-step” relaxation of WARP, morphs rational choice model into a reference-dependent model where inferior alternatives break indifference between the superiors and where DM uses plurality rule when facing multiple inferiors. To elaborate on the model, consider Fig. 1. DM is shopping from an online platform where she observes the prices and the customer reviews. She faces the set of alternatives described in Panel (a). She does not favor either ==== or ==== since each is clearly inferior. At first glance, ====, and ==== are equally favorable to her. It is known however that, ==== acts as a decoy that makes either ==== or ==== more attractive than ====. Similarly, ==== makes either ==== or ==== more attractive than ====. What if DM faces both ==== and ====? It may be the case that ==== dictates her choice, in which case she chooses ====. Otherwise if ==== dictates, then she chooses ====. These two cases are illustrated by the red arrows in Panel (b). As shown by the blue arrows in this panel however, if both ==== are ==== are pivotal to DM, then she chooses all three alternatives ====. Nonetheless, as illustrated in Panel (c), if ==== and ==== were to ==== influence DM’s behavior, then ==== seems to have an “edge” over both ==== and ====. This is because both ==== and ==== favor ====, which is not the case for either ==== or ====; i.e. only ==== has a plurality.====Formally, the model is as follows: DM has a well-defined preference relation ====. Beyond the preference relation, her behavior is also influenced by a collection of binary ==== ==== defined for each alternative ==== over those that are preferred to ====. The interpretation of ==== may be that “==== makes ==== ==== ====”. In a given choice problem, DM chooses those alternatives that are the most attractive from the prescriptive of a plurality of the reference relations in that problem.====We observe that the preference relation and the reference relations are ==== revealed from the choice observations. For the case of revealed preference, the mechanism here is the well-known one due to Arrow (1959). That is, ==== is uniquely revealed from the choices made over doubletons: ====. This is because of the “forward-looking” nature of referential effects; that is, the influence is from inferior alternatives towards superior ones. Thus, the effect is absent in a doubleton, and the reference-dependent model coincides with rational choice. This forward-looking nature along with the fact that reference relations only break indifference also makes sure that referential effects may at most be due to ==== inferior alternative in a triplet. Consequently, reference relations are further uniquely revealed by extending Arrow’s observation: ====. Importantly, this revelation mechanism is independent of DM’s aggregation behavior. In particular, neither plurality, nor dictatorial, nor any other perceivable rule, do pose any restrictions on this revelation mechanism. This is because the aggregation rule only comes into action when DM faces at least four alternatives.====Building upon this latter point, rational choice is seen a ==== model that only relies on a preference relation which, in turn, is uniquely identified via the choices over doubletons. The reference-dependent model under plurality, on the other hand, is parameterized by the object ==== to which we refer as a ====. In this case, one could uniquely identify this object instead by observing the choice over doubletons and triplets. Thus, we may interpret the model under plurality rule as a ==== reference-dependent model, since it only adds one layer of observational necessity for identification purposes. From the same viewpoint, dictatorial rule is indeed of ====. In that, beyond identifying the preference system, one also needs to use the choice data on quadrupletons to identify the “relative strength” of two reference relations. In words, this paper also contributes to the understanding of reference-dependent choice by providing a manner with which one may contemplate a ==== of dependence.",Reference-dependent choice under plurality rule,https://www.sciencedirect.com/science/article/pii/S0165489621001104,6 November 2021,2021,Research Article,66.0
Bougheas Spiros,"School of Economics, University of Nottingham, University Park, Nottingham NG7 2RD, UK","Received 9 November 2020, Revised 19 August 2021, Accepted 24 October 2021, Available online 3 November 2021, Version of Record 21 January 2022.",https://doi.org/10.1016/j.mathsocsci.2021.10.006,Cited by (5),"We study the formation of networks in environments where agents derive benefits from their neighbours (immediate links) but suffer losses through contagion when any agent on a path that connects them is hit by a shock. We first consider networks with undirected links (e.g. epidemics, underground resistance organizations, trade networks). We find that the only networks that satisfy strong notions of stability are comprised of disjoint subgraphs that are complete. Then, we consider networks with directed links and we find that stable networks can be asymmetric, connected but not completely connected, thus capturing the main features of production and financial networks. We also identify a trade-off between efficiency and stability.","A significant part of the literature on strategic network formation has focused on variants of the ‘connections’ model studied by Jackson and Wolinsky (1996). The common idea of this literature is that being part of a network allows agents to benefit not only from their direct links but also from indirect connections to other agents in the network. In contrast, the costs of network participation in these models are only associated with the creation of new links. However, as Blume et al. (2011) observe in many networks studied in economics and other disciplines the structure of costs and benefits is inverted. As an example from economics they refer to the extensive body of work on issues related to systemic risk in financial networks. In those networks two institutions form a link by signing a loan agreement from which each party derives a benefit. A failure by one institution to meet its obligations inflicts costs not only on the two parties that have signed the agreement but also on other institutions connected to them, directly or indirectly, by other financial agreements.====The above observation motivated Blume et al. (2011) to study the formation of networks with this alternative general payoff structure. They have restricted their analysis to undirected graphs where shocks can travel in either direction along a link. Using stability as a solution concept that allows them to make predictions about which network structures are more likely to form they find stable networks that consist of fully connected disjoint subgraphs (cliques). Such arrangements might be offering a good description of some examples of social networks they mentioned in their paper (e.g. formation of groups that minimize the risk of disease epidemics and the organization of clandestine operations) but definitely less so for financial systems and production networks that have network structures which are connected but incomplete.==== In such networks there is always a path connecting any of the nodes (financial institutions or firms) with every other node, however, not all nodes are directly linked with every other node.====In this paper, we argue that by distinguishing between directed and undirected graphs we can explain such variations in network structures. We begin by analysing the formation of undirected networks in a variant of the Blume et al. (2011) model. As in their model, (a) agents derive a benefit by forming a link, and (b) each agent fails independently with some fixed probability. The difference in the two models is related to the way the costs associated with such failures spread through the network. In their model after a failure each link with some probability becomes alive and shocks can only be transmitted through alive links (the network structure is determined after the shock). In our model all nodes are alive and thus potentially affected by the shock, however, the magnitude of the losses for each node depends on its distance from the one that initially failed. As Blume et al. (2011) we find that when links are undirected there exist stable networks that consist of fully connected disjoint subgraphs. We also go one step further by showing that these are the only types of networks that are Pareto-efficient. The implication of this result is that they also satisfy the notion of ‘strong’ stability (Dutta and Mutuswami, 1997, Jackson and van den Nouweland, 2005).====Then, we turn our attention to directed networks where shocks can be only transmitted along directed paths. The Blume et al. (2011) model by having only a subset of nodes being live it mimics the transmission of shocks in directed networks. However, there is a crucial difference between their model and ours. In their model the paths are exogenously determined but in our model are equilibrium outcomes. The direction of the links are determined by the strategic decisions of agents. We argue that our version is better suited to capture the types of contagion observed in real financial and production networks. In financial networks links stand for loan agreements between financial institutions which are represented by the nodes of the network. There are two types of contagion that have been studied on networks formed by financial links. The first type of contagion is caused by insolvencies and flows from borrowers to lenders. When a financial institution becomes insolvent it is unable to meet its financial obligations to its creditors. The latter lose not only the benefit of their financial relationship with their creditor but they might also not be able to meet their obligations to their own creditors. The second type of contagion, which flows from lenders to borrowers, is caused when a financial institution, out of fear that its borrowers might not be able to repay their loans in the future, interrupts their funding. The crisis unfolds when the affected institutions might not be able to provide funding to their own debtors and eventually as these funding interruptions cascade through the network the market eventually freezes. In production networks the links stand for supply-chain relationships between firms and the direction of the links captures the flow of goods.==== Contagion in such networks can be caused either because of financial factors (lack of trade credit) or because of shocks on production lines.====We establish very mild conditions such that complete directed networks (tournaments) are stable.==== We also identify a trade-off between stability and ==== efficiency. The intuition behind this result is that stability depends on the weakest link while efficiency depends on the average expected payoff. We then derive our main result by demonstrating the existence of stable networks that are not fully connected and where nodes do not belong to a closed path. In such networks some nodes are more connected than others resembling the hierarchical structures of financial and production networks. In the following section, we describe a few examples of undirected and directed networks from economics and other disciplines whose general structure is captured by our model.",Contagion in networks: Stability and efficiency,https://www.sciencedirect.com/science/article/pii/S0165489621001086,3 November 2021,2021,Research Article,67.0
Dubois Marc,"UPR CHROME - Université de Nîmes - CUFR de Mayotte, Département droit-économie-gestion, 8, rue de l’Université BP 53 Iloni, 97660 Dembéni, France","Received 4 March 2021, Revised 30 September 2021, Accepted 21 October 2021, Available online 2 November 2021, Version of Record 17 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.10.004,Cited by (1),"The paper proposes a dominance criterion that assesses whether a seasonal outcome of a sports league is more balanced than another. This criterion is based on a novel third-order ==== defined on finite sets of evenly spaced seasonal points (seasonal grids), called downward seasonal balance (DSB). The DSB criterion makes the same assessments as the well-known Lorenz criterion. However, the converse is not true: The DSB criterion makes assessments even in cases where the Lorenz criterion cannot. The former is then less incomplete than the latter. The assessments of the DSB criterion reflect the unanimity of a class of competitive balance indices. A seasonal outcome is more balanced than another according to the DSB criterion if and only if every index of the class agrees. Such a class is axiomatically characterized so that the indices place at least as much emphasis on the balance between leading competitors as on the balance occurring among the nonleading competitors. An empirical application provides comparisons of seasonal outcomes of the five most competitive soccer leagues in Europe from 2014–2015 to 2018–2019.","Introduced by Rottenberg (1956) and Neale (1964), the so-called ==== is commonplace in the economics of sport. It requires that demand for stadium attendance or broadcast increases with respect to competitive balance, that is balance among competitors. Cairns et al. (1986) decompose competitive balance into three lengths: short-, medium-, and long-term competitive balance. ==== amounts to the balance of strength between two opponents in a game. ==== refers to the balance among participants in a contest during a season. Long-term competitive balance deals with an interval of several seasons. According to Szymanski (2003), the championship race is a distinctive attribute of medium-term competitive balance. This race is determinant in providing empirical support for the uncertainty-of-outcome hypothesis (Jennett, 1984, Pawlowski and Anders, 2012, Scelles et al., 2013, Andreff and Scelles, 2015, Pawlowski et al., 2018; among others). In this sense, this paper aims to measure medium-term competitive balance, emphasizing the championship race. Such a measurement is performed by means of pairwise comparisons of distributions of seasonal points.====As this work focuses on distributional aspects, the concepts and results of the measurement of income inequality can be translated into concepts and results of the measurement of competitive (im)balance. One of the most-used methods for measuring inequality is the Lorenz, or second-order stochastic, dominance criterion.==== Distribution ==== is said to ==== distribution ==== when no part of the Lorenz curve of distribution ==== lies below that of distribution ====. In sports terms, this means that ==== is at least as balanced as ====. Quirk and Fort (1992) and Szymanski and Kuypers (1999) employ the Lorenz criterion to measure long-term competitive balance. Additionally, Michie and Oughton (2004) employ the Lorenz criterion for analyzing medium-term competitive balance.====The normative content of the Lorenz criterion is derived from the higher of two non-intersecting curves obtaining from the lower by a sequence of ====, that is, transfers of points from a stronger to a weaker competitor. For instance, assume a contest has three competitors: ====, ====, and ====. A first hypothetical distribution of points is ====, where ====, the strongest, is awarded ==== points, while the two weakest – ==== and ==== – obtain ==== points. A second hypothetical distribution is ====. Distribution ==== can be obtained from ==== through a progressive transfer of two points from ==== to ====. According to the Lorenz criterion, ==== is at least as balanced as ====. This judgment is consistent with the so-called ==== (Pigou, 1912, Dalton, 1920). In sports terms, this principle requires competitive balance to increase as the result of a progressive transfer. According to the principle of transfer, ==== is more balanced than ====. Dasgupta et al. (1973) show that a first-distribution Lorenz dominates a second distribution if and only if ==== inequality indices that satisfy the principle of transfers state that the first is at least as balanced as the second. This equivalence only stands for pairs of distributions with equal means.====Horowitz (1997), Schmidt and Berri, 2001, Schmidt and Berri, 2002, Utt and Fort (2002), Borooah and Mangan (2012) and Gayant and Le Pape (2017a), among others, propose several inequality indices to measure competitive balance. Moreover, Triguero Ruiz and Avila-Cano (2019) introduce a cardinal index, which is based on the concept of distance. Since these indices satisfy the principle of transfers, their judgment is unanimous when one distribution Lorenz dominates another. For instance, all indices state that ==== is at least as balanced as ====. However, these indices render conflicting judgments when both progressive and ==== (weaker-to-stronger) transfers are needed to convert one distribution into another. Building on the previous example, consider a third hypothetical distribution ====. The well-known Gini coefficient indicates that ==== is as balanced as ====. Atkinson indices (Atkinson, 1970) state that distribution ==== is more balanced than ====. Alternatively, some generalized entropy indices (with parameters higher than ====) state that ==== is more balanced than ====. When the indices that satisfy the principle of transfers are not unanimous, Lorenz curves intersect, and the criterion is inconclusive. With intersecting Lorenz curves, the criterion cannot be used to judge whether ==== is at least as balanced as ====. Moyes (1999) and Aaberge (2009), among others, point out that Lorenz curves often intersect in practice.====To overcome this limitation, the paper proposes three equivalent aspects of the measurement of competitive balance. First, the notion of an increase in balance is associated with a progressive transfer and a ==== (BCT). A composition of two transfers, a BCT is a regressive transfer between weaker competitors and a progressive transfer between stronger competitors. It is equivalent to the UNFACT introduced by Gayant and Le Pape (2017a). A BCT should increase competitive balance because “competition for top positions is considerably more important than competition in the “dull” midfield” (Bundzinski and Pawlowski, 2017, pp. 116–117). Applying concepts of behavioral economics, Pawlowski and Budzinski (2014) show that soccer fans care more about a balanced championship race than about any other balanced subpart of the competition. Second, all indices that satisfy this requirement and the principle of transfers may be used to make a unanimous judgment on two distributions if and only if a distribution can be obtained from the other through a sequence of progressive transfers and/or BCTs. The set for which this unanimity is reached includes the generalized entropy indices (with parameters higher than 2) proposed by Gayant and Le Pape (2017a) and the index of Triguero Ruiz and Avila-Cano (2019).==== In practice, it seems impossible to check whether one of the two above equivalent conditions is fulfilled. For instance, an infinite number of comparisons should be performed to check whether these indices make a unanimous judgment.====The third aspect of the measurement of competitive balance regards an operational method to verify the validity of these conditions. It is shown that a first-distribution downward seasonal balance (DSB) dominates a second distribution if and only if a sequence of progressive transfers and/or BCTs is needed to convert the second into the first distribution.==== The DSB criterion is a novel third-order stochastic dominance quasiorder. It is weaker, and so less incomplete, than the Lorenz criterion. When the Lorenz criterion can be employed to make a judgment, the DSB criterion allows the same judgment, whereas the converse is not true. According to the previous example, ==== DSB dominates ==== because ==== Lorenz dominates ====. Moreover, ==== DSB dominates ==== because ==== can be obtained from ==== through a BCT of one point among ====, ====, and ====.====Fishburn and Willig (1984), Gayant and Le Pape (2017b), and Dubois and Mussard (2019), among others, explore third-, fourth- and higher-order dominance criteria to rank income distributions. Unlike these approaches, this paper proposes a reverse third-order stochastic dominance. Applied to distributions of seasonal points, the classical third-order stochastic dominance is based on the aggregation from lower point levels to larger ones, whereas DSB is defined on the aggregation from larger point levels to lower ones.====In contrast to measuring income inequality, measuring medium-term competitive balance generally includes populations with fewer than 33 competitors, each with an integer-valued number of points. Thus, both Lorenz and DSB criteria are designed to compare distributions of finite numbers of integers (grids). They are easily implementable, as the required data are lists of wins–ties–losses of each competitor. Such data are freely available and do not suffer from bias or measurement errors.====This work performs intranational judgments of seasonal distributions from 2014–2015 to 2018–2019 of the five most competitive soccer leagues in Europe. The Lorenz criterion was conclusive in only 34% of the 50 pairwise comparisons that could to be made, whereas the DSB criterion was conclusive for 66% of these cases. According to these criteria, the competitive balance of the English ====, the German ====, and the Italian ==== globally decreased during the period. Only the competitive balance of the Spanish ==== globally increased from 2014–2015 to 2018–2019. More precisely, the competitive balance of the French ====, the Bundesliga, and Serie A first decreased generally from 2014–2015 to 2016–2017 and later generally increased from 2017–2018 to 2018–2019.====Section 2 introduces the framework. Section 3 presents the second-order stochastic dominance criterion on grids (equivalently, the Lorenz criterion). Section 4 examines the DSB criterion. Section 5 discusses an empirical application, whose results are summarized in Table 5.1. Section 6 concludes the article.",Dominance criteria on grids for measuring competitive balance in sports leagues,https://www.sciencedirect.com/science/article/pii/S0165489621001062,2 November 2021,2021,Research Article,68.0
"Faias Marta,Moreno-García Emma","Universidade Nova de Lisboa, CMA and FCT, Portugal,Universidad de Salamanca, Spain","Received 5 September 2020, Revised 5 October 2021, Accepted 5 October 2021, Available online 26 October 2021, Version of Record 21 January 2022.",https://doi.org/10.1016/j.mathsocsci.2021.10.002,Cited by (1),"We state and analyze a non-cooperative approach to the provision of public goods where agents decide simultaneously not only their private contribution but also their level of utilization which may differ among consumers (contributors and non-contributors). We show that the distribution of intensities of use matters and allows us to deepen the analysis of altruistic behaviors, neutrality results, congestions issues, and further externalities captured within our framework.","Samuelson (1954) defined a public good as one “... ====...” Since then, we say that a public good is characterized by the properties of non-rivalry and non-excludability.====However, the fact that it is impossible to exclude any individual from consuming a good does not imply that everyone makes the same use of it. That is, despite the non-excludability properties of the collective commodity, since its use is not fixed and compulsory, consumers may vary their degree of utilization. Indeed, we observe that not all individuals use public goods with the same intensity and we argue that the distribution of end-users affects the individual preferences and the welfare of society. For instance, some agents may prefer a swimming pool with few people while others prefer a more crowded ambient. Some parents may prefer for their children a school where all the students have certain characteristics while others are indifferent about it or even prefer diversity in the group. On the other hand, when using public goods, and also depending crucially on the distribution of users and levels of utilization, congestion issues might arise that can be analyzed from a point of view based on a common and objective perspective, as it is the case of physical space or capacity restrictions; but also and at the same time individual and subjective perception may also play a role. In this way, this reasoning, based on utilization levels, makes it possible to include the study of goods that are commonly used and are not necessarily pure public goods, in the sense that may be subject to further externalities and, in particular, can present congestion above some degree of utilization.====In this work, the aforementioned ideas lead us to provide a non-cooperative strategic approach to the provision of public goods, where agents decide simultaneously not only their private contributions but also a new variable that captures their level of utilization which may differ among individuals. We claim that the intensity of use matters and outlines the perception that each consumer has of the provided public good given by the sum of all contributions. Moreover, the external characteristics of the end-users and their use distribution may be significant for each consumer and give rise to further externalities.====Each individual has a preference relation, represented by a utility function, defined on the set of vectors that establish: private consumption, perceived public good adjusted for their particular level of use, and profiles of use and contributions of all participants. That is, the intensity of utilization that a consumer decides enters their preferences in two ways. First of all implicitly, as a variable of a function that together with the sum of contributions determines the valuation that the individual assigns to the public good. Second, as an explicit variable in their utility function.====In our game, each player is a consumer endowed with an amount of private good that is used as an input to produce a collective commodity. A strategy for a player is a pair given by her private contribution for the provision of public good and a parameter that specifies her level of use. The voluntary contributions and the distribution of the use intensities lead to an outcome given by the allocation of private commodity and the public good that is generated, taking into account the use and the contributions. Then, each profile determines a vector belonging to the set where the individual preferences relations are defined. Therefore, the payoffs of the game are given by the corresponding utility functions evaluated at the respective outcomes that each strategy profile defines.====After showing the existence of equilibrium for our non- cooperative game, we establish how the scheme we propose leads us to the study of a variety of issues, including altruistic behaviors, free-riding situations, additional externalities that our model can incorporate, and questions on neutrality.====Within our framework, the utilization variable can give reasons to contribute privately to the provision of a public good. This fact allows us to shed light on a variant of altruistic behavior, that we refer to as altruism with self-interest, and differs from the so-called pure and impure altruism in the related literature.====A free-rider is a person who benefits from something without paying for it. In other words, free-riders are those who utilize goods without paying for their use. However, to our knowledge, models involving public goods and free-riding problems implicitly assume that all individuals use such a good and with the same intensity. In contrast to our approach, they do not consider the possibility that consumers can modify the use of a non-excludable commodity as we explicitly do in this work. Thus, the framework we propose incorporating both the contribution and the utilization as variables allows deepening the free-riding concept and a potential measure of its relevance associated with each strategy profile.====Additionally, the analysis of the distributions of users and the aggregate utilization degree motivate the study of further externalities within scenarios where collective commodities are privately provided. To address some of these externalities, we consider situations where the utilization of a public good is given for a set of individuals under some circumstances. For instance, we may think about the use of public hospitals by persons with weak health or the use of public schools by a family with many children, or the use of a highway by people who necessarily have to use it when they go to work. Thus, we provide a variant of the initial game that leads us to identify conditions, based on complementarity or supermodularity properties, under which the larger the originally given utilization of the public good the larger the contribution determined by the best reply functions of each player.====Moreover, the neutrality theorem, that goes back to Warr (1983) concerning the provision of a public good in a voluntary Nash equilibrium shows that if income is redistributed among contributors so that a loser loses no more than her original donation, the equilibrium remains the same. Within our framework, we find conditions to recover a neutrality result for the model we address, and state an example showing the necessity of the required conditions.====We stress that our approach includes existing well-known models as particular cases. The game we provide is an extension of the original one by Bergstrom et al. (1986) to a setting where individuals are free to modify their use of a collective consumption commodity, and we also have Andreoni’s (1990) framework on impure altruism as a particular setting.====On the other hand, an increasing line of research that grows out of the one initiated by Buchanan (1965) defines different general equilibrium models with exchange and club formation with non-anonymous crowding. See, for instance, Conley and Wooders, 1997, Conley and Wooders, 2001), Ellickson et al. (1999), Allouch and Wooders (2008), and the references there. Unlike our aim in the current article, the main question addressed in many of these papers is whether it is possible to get an efficient decentralizing price system in which prices can be based on publicly observable characteristics and their relation with the core. Although we also address related issues within a private provision of public goods setting, our analysis is based on a non-cooperative approach where players contribute in a voluntary way to the provision of a collective commodity and we do not study market-like outcomes.====The remainder of this paper is structured as follows. In Section 2, we present the game on the private provision and use of public goods to be analyzed. We also state an example to illustrate the model. In Section 3, we show an equilibrium existence result for such a game and we present some remarks regarding efficiency. In Section 4, we elaborate on altruism and free-rider issues. In Section 5, we study a particular game where some players have a given use intensity and their strategies are just their private contributions to the public good. In Section 6, we prove a result on neutrality for our game and present an example to deepen such a result. Finally, in Section 7, we conclude with some remarks.",On the use of public goods,https://www.sciencedirect.com/science/article/pii/S0165489621001049,26 October 2021,2021,Research Article,69.0
"Fu Wentao,Le Riche Antoine","School of Economics, Sichuan University, Chengdu, 610065, China,CAC – IXXI, Complex Systems Institute, Lyon, 69007, France","Received 24 August 2020, Revised 8 October 2021, Accepted 14 October 2021, Available online 28 October 2021, Version of Record 9 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.10.003,Cited by (1),.,"In this paper, we examine how an economy evolves when firms learn the quality of a new technology by adopting it in an optimal way. The new technology’s quality could only be learned from the realized quality-dependent outputs, and this learning process stops once the old known technology is applied. In presence of evolving uncertainty and beliefs, whether and when to adopt a new technology become not only the main driving forces of economic growth, but also could explain the difference of GDP per capita across the world.==== While it is observed that new technology is adopted gradually, there is an absence of a complete knowledge on understanding the dynamic process through which uncertainty and belief influence the technology adoption decision.==== To understand this issue, contrary to the existing literature that focuses on a central planner’s problem to learn the unknown state (see, for example, El-Gamal and Sundaram, 1993; Nyarko and Olson, 1996; Koulovatianos et al., 2009; Mirman et al., 2016), we provide a first step towards addressing belief-driven technology adoption within a two-sector endogenous growth model where decisions are decentralized.====In the study on economic growth with uncertainty, Bayesian learning from experimentation has been emphasized as an important role on the equilibrium outcomes. Freixas (1981) studies a one-sector optimal growth model with uncertain production function. To improve the life-time wellbeing, a single agent treats the level of investment as a tool to learn the true production function. Similar papers (such as El-Gamal and Sundaram, 1993; Nyarko and Olson, 1996; Kelly and Kolstad, 1999; Bertocchi and Spagat, 1998; Koulovatianos et al., 2009 and Mirman et al., 2016.) embed the dynamic role of learning (passive or active) into different types of one-sector growth model. All of these works consider a central planner’s problem, in which the planner uses consumption and investment as experimentation in each period to learn the unknown state optimally and thus maximizes the social welfare. The true state would always be learned eventually on the equilibrium path since experimentation is always excised in each period. In contrast, to the literature that focus on the centralized, we consider the decentralized economy. To best of our knowledge, we are the first to consider this case.====However, in the aforementioned literature, a fact is missing that industries often face a selection problem between riskless old technology and risky new technology according to what they have learned. Johnson (2007) and Pástor and Veronesi (2009) show that, theoretically and empirically, the technology selection from Bayesian learning significantly affects the capital accumulation and hence the volatility of stock prices. Taking this fact into account, we aim to introduce technology selection and Bayesian learning into an endogenous growth model.====We consider a two-sector endogenous growth model based on Bond et al. (1996) and Drugeon (2013), where the engine of growth is due to the accumulation of two capital stocks, physical and human capital.==== In the consumable physical capital good sector, producer needs to select one technology from a riskless old and a risky new technology. The TFP of the new technology is initially unknown. A good new technology achieves a high TFP with a positive probability, while a bad new technology always achieves a low TFP. The market updates beliefs on the quality of the new technology only after it is selected and productions are realized. This is considered as an “armed bandit” problem as in DeGroot (2004).====Due to technology selection, the true quality of the new technology could still remain unlearned on the equilibrium path, which makes a significant difference from the existing literature on growth with learning. This happens when the market continuously observes a low TFP is inferred when using the new technology. In this case, the market becomes very pessimistic and then abandon it, after which the market belief remains unchanged and is not degenerated. We show that there exist two balanced growth paths, one corresponding to the good new technology selection, the other sticking to the old technology selection. Such a result is in contrast with the full information case since only one steady state emerges.====One important feature of our model is the rate of achieving a high TFP in the good new technology. We find that a raise of such probability increases the expected TFP under the new technology and hence leads to a higher profitability of human and physical capital. This channel causes a rise in the growth rate of the economy, the GDP and the physical to human capital ratio. Conversely, its effect is ambiguous concerning the consumption to human capital ratio, which depends upon whether the substitution effect dominates the income effect or not.====Since two balanced growth paths are locally determinate, global indeterminacy exists. There is a literature stressing the importance of global analysis in contexts in which equilibrium selection is not determined only by the initial values of the state variables but also about self-fulfilling prophecies.==== When two equilibria exist, it is important to understand which equilibrium will be attained. In the literature, the choice between equilibria relies on history, i.e. past events set the preconditions that drive the economy to one or the other steady state, or by expectations, i.e. self-fulfilling prophecy. Our results contribute to this literature by showing that, within an optimal growth model with uncertainty, the equilibrium selection is due to Bayesian learning.====When the initial belief is high enough, the learning process generates an evolution of market belief leading to different paths. If the high TFP is inferred, the market belief jumps to the good new technology path, while if it is not inferred the market belief shrinks. Such a process will continue until the market belief is too low and then the economy converges to the old technology path. We conclude that the choice among multiple equilibria is essentially resolved by history: that past events set the preconditions that drive the economy to one or another steady state. Moreover, we show that the learning speed occurs after few periods implying an amplified effect of learning on the equilibrium selection.====Since a good new technology could also be abandoned with a positive probability, the economy could end up with a “poverty trap”, a locally determinate steady state where the producer is using an old technology. This result highlights the importance of initial belief in the selection of the technology. We extend the literature on poverty trap by showing the existence of two steady states, one being a “poverty trap”, due to “two armed bandit” problem and uncertainty related to the quality of the technology.====The rest of the paper is organized as follows. In Section 2 we present the model while in Section 3 we obtain the equilibrium and the steady state. Section 4 provides the analysis of the stability properties. Finally, Section 5 concludes. Proofs are gathered in the Appendix.",Endogenous growth model with Bayesian learning and technology selection,https://www.sciencedirect.com/science/article/pii/S0165489621001050,28 October 2021,2021,Research Article,76.0
"Hu Ruiyang,Yang Yibai,Zheng Zhijie","Department of Economics, University of Macau, Taipa, Macao, China,Center for Innovation and Development Studies, Beijing Normal University at Zhuhai, 519087, China","Received 20 March 2021, Revised 26 September 2021, Accepted 5 October 2021, Available online 23 October 2021, Version of Record 9 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.10.001,Cited by (5),"This study explores the effects of ==== of innovation and stimulate it at the same time by raising the size of quality increment. An additional CIA constraint on manufacturing weakens the growth-retarding effect and enhances the growth-promoting effect, whereas an additional CIA constraint on R&D strengthens only the negative growth effect. Our quantitative analysis finds that the relation between ","In this study, we develop a Schumpeterian growth model to analyze the effects of monetary policy on the size of quality increment, economic growth, and social welfare, respectively. In contrast to the previous studies that assume an exogenous quality step size, this study extends the innovation-driven growth model by incorporating an endogenous quality increment channel through which monetary policy induces noticeable impacts on real variables. Money is introduced to this growth-theoretic framework by using the most generalized liquidity constraint via cash in advance (CIA). Specifically, in addition to the well-established approach of a CIA constraint on consumption as in Lucas (1980) and Dotsey and Sarte (2000), in this study we also consider a CIA constraint on manufacturing as in Fuerst (1992) and Arawatari et al. (2018), and a CIA constraint on R&D investment as in Chu and Cozzi (2014) and Chu et al. (2015).====Our assumption regarding CIA constraints on firms’ manufacturing and R&D investment is strongly motivated by recent empirical findings in firms’ liquidity constraints. For example, Bates et al. (2009) and Lyandres and Palazzo (2016) find that the average cash-to-assets ratios for the US firms have sharply increased and become more than doubled since 1980. Ma et al. (2020) report a positive correlation between the industry-level cash- and R&D-to-assets ratios in the US. These results suggest a severe liquidity constraint on firms’ behavior. Moreover, the empirical findings of Liu et al. (2008) indicate that firms’ manufacturing activities are subject to cash constraint. More recent studies, such as Brown et al. (2012) and Brown and Petersen (2015), also reveal that firms tend to use cash to finance investment in R&D, the activities of which, however, suffer from liquidity constraint.====In this monetary Shumpeterian growth model augmented by different CIA constraints, we derive the following results. In the presence of a CIA constraint exclusively on consumption expenditure, an increase in the nominal interest rate raises the real wage rate through reducing labor supply, which generates two counteracting effects on economic growth. First, given that the price markup is increasing in the size of quality increment, a higher wage rate tends to decrease monopoly profit. To recoup a high profit flow, entrepreneurs are incentivized to pursue more radical innovations. Consequently, the increased size of quality increment causes the economic growth rate to rise. Second, a higher nominal interest rate discourages R&D incentives since entrepreneurs face a higher R&D cost in employing labor to produce inventions. As a result, the arrival rate of innovation decreases, causing the economic growth rate to decline. Since the economic growth rate is jointly determined by the arrival rate of innovation and the size of quality increment, the overall effect of the nominal interest rate on economic growth depends on the balance between the above competing forces. By calibrating the model to the US economy, we find that the relation between the nominal interest rate and economic growth is more likely to be monotonically decreasing. Conditional on the Fisher equation that predicts a positive ==== relation between the nominal interest rate and the inflation rate (see Mishkin (1992) and Booth and Ciner (2001) for supportive empirical evidence), our model also implies a negative correlation between inflation and economic growth.====When CIA constraints on consumption and manufacturing are present, a rise in the nominal interest rate reinforces the aforementioned positive effect through causing a larger decline in the monopoly profit, whereas it weakens the negative effect through producing an additional reallocation effect that shifts labor employment from manufacturing to R&D. In this case, the nexus between inflation and economic growth can be either negative or hump-shaped, depending on the strength of the CIA constraint on manufacturing.==== Our study thus provides a novel mechanism that potentially reconciles the mixed empirical evidence on the relation between inflation and economic growth. For example, Vaona (2012) and Barro (2013) find a monotonically decreasing inflation-growth relation. Nevertheless, a number of empirical studies, such as Khan and Senhadji (2001), Burdekin et al. (2004), and Eggoh and Khan (2014), have documented a non-monotonic relation instead.====Furthermore, when consumption expenditure and R&D investment are constrained by cash, a higher nominal interest rate weakens the positive effect on the quality step size and strengthens the negative effect on the innovation arrival rate. This is because a higher nominal interest rate now leads to a larger increase in the R&D cost and thereby a larger decrease in the innovation arrival rate. In addition, the lowered R&D labor demand in turn suppresses the rise in the wage rate that is caused by a stronger constraint on consumption. This then depresses the positive impact of the nominal interest rate on the size of quality increment, as the decline in the monopoly profit becomes smaller in this circumstance. Therefore, the economic growth rate is monotonically decreasing in the nominal interest rate.====Our numerical analysis shows that, by calibrating the model to the US economy, inflation and growth can exhibit either a monotonically decreasing or an inverted-U relation, depending on which aforementioned CIA constraints are imposed. Interestingly, in all above cases, welfare is always decreasing in the nominal interest rate, implying that Friedman rule (i.e., zero-nominal-interest-rate targeting) is socially optimal. Finally, to test the empirical relevance of the model predictions, in Section 5, we perform an empirical analysis by using the US data from 1980 to 2020 and the result reveals a significantly inverted-U effect of inflation on the growth rate of GDP per capita in the US. Therefore, our model in the presence of (i) CIA constraints on consumption and manufacturing and (ii) CIA constraints on the three channels in total (i.e., consumption, manufacturing, and R&D) is able to adequately characterize this stylized fact.====This study closely relates to the literature on inflation and innovation. A representative along this line of studies is the pioneering work of Marquis and Reffett (1994), which explores the effects of inflation on growth in the framework of Romer (1990).==== A great number of subsequent studies have analyzed the effects of inflation in a Schumpeterian quality-ladder model with an identical step size of quality improvement, such as Chu and Lai, 2013, Chu and Cozzi, 2014, Chu et al., 2015, Chu and Ji, 2016, Huang et al. (2017), Oikawa and Ueda (2018), Huang et al. (2021), Gil and Iglésias (2020), and Zheng et al. (2019). One novel exception is Chu et al. (2017), who consider the heterogeneity of quality step sizes by assuming that the quality increment is drawn from an exogenously given distribution, instead of the endogenous choice by entrepreneurs. Our study complements their interesting study and contributes to the literature by allowing the step size of quality increment to be endogenously chosen by profit-maximizing entrepreneurs. Combined with the conventional frequency-of-innovation channel, the novel feature of endogenous quality step size provides a new mechanism to explain the (potentially) inverted-U relation between inflation and economic growth, which helps to reconcile the discrepancy in the empirical literature.====In addition, the positive relation between inflation and price markups in this model is consistent with the result in Wu and Zhang (2001) within a growth framework,==== but it differs from the widely recognized implication of standard New Keynesian models featuring sticky prices. Due to mixed empirical evidence, however, the positive inflation-markup relation is not necessarily implausible. For example, Bils (1987), Rotemberg and Woodford, 1991, Rotemberg and Woodford, 1999, Martins and Scarpetta (2002), and Gali et al. (2007) provide empirical evidence supportive of countercyclical markups; and Banerjee and Russell (2001) and Banerjee et al. (2001) identify a negative long-run relation between inflation and markup in Australia and most of the G7 countries. In sharp contrast, exploiting the Solow residual to estimate the cyclical movements in markups, Haskel et al. (1995) explore a panel data set of two-digit UK manufacturing industries, and find evidence for strongly procyclical markups. Using both aggregate and detailed manufacturing industry data, Nekarda and Ramey (2013) suggest that markups are procyclical unconditionally, and either mildly procyclical or acyclical conditional on demand shocks. Using detailed micro data on local house prices, retail prices and households shopping intensity, Stroebel and Vavra (2019) show that rising house prices increase consumers’ demand by reducing their sensitivity to price changes, and firms raise markups in response. Their novel evidence suggests a procyclical desired or natural markup, which responds to monetary policy endogenously.==== In fact, recent empirical evidence has motivated macroeconomic theorists to reinvestigate existing general equilibrium models for a better understanding of the mechanism under which a positive relation between inflation and price markups can be shaped.==== This study exploits the Schumpeterian growth model and provides a discussion on an alternative possible channel inducing a positive inflation-markup correlation.====The rest of this study is organized as follows. Section 2 presents the model. Sections 3 Implications of monetary policy, 4 Quantitative analysis analytically and numerically explore the effects of monetary policy on the quality increment, economic growth, and social welfare, respectively. Section 5 conducts an empirical analysis. Section 6 concludes.","Inflation, endogenous quality increment, and economic growth",https://www.sciencedirect.com/science/article/pii/S0165489621001037,23 October 2021,2021,Research Article,77.0
"Mock Andrea,Volić Ismar","Department of Mathematics, Wellesley College, 106 Central Street, Wellesley, MA 02481, United States of America","Received 8 April 2021, Revised 29 July 2021, Accepted 24 September 2021, Available online 4 October 2021, Version of Record 18 October 2021.",https://doi.org/10.1016/j.mathsocsci.2021.09.004,Cited by (3),-compromises and ====-compromises.,"Simplicial complexes are an important and versatile tool in various branches of mathematics. The simple definition – a ==== is a collection of nonempty subsets of a finite set containing all the singletons (vertices) and all subsets of sets already in the collection (simplices) – lends itself to varied analysis and many applications. Since each subset of a simplicial complex can be represented via geometric realization as a topological simplex, combinatorial and algebraic topology are natural tools to bring to bear when studying these objects.====Simplicial complexes can potentially serve as a model for any situation where objects or entities interact in some way. One can think of this as the higher-dimensional analog of graphs which successfully model networks of pairwise interactions (edges) among objects (vertices). However, a graph cannot capture the situation where larger subsets of objects interact, while a simplicial complex is ideally suited for this since each of its simplices represents such a multi-fold interaction.====The use of simplicial complexes in modeling complex interactions has been booming in recent years. Applications in fields as disparate as topological data analysis (Carlsson, 2009, Carlsson, 2020, Munch, 2017, Otter et al., 2017), signal processing (Barbarossa and Sardellitti, 2019, Barbarossa et al., 2018, Giusti et al., 2016, Ji et al., 2020, Moore et al., 2012), and neuroscience (Giusti et al., 2016) abound. Social science applications are also starting to emerge; connections to game theory are well-established (Egan, 2008, Faridi et al., 2019, Martino, 2021), and recent work uses simplicial complexes to model social communication and opinion dynamics (Hansen and Ghrist, 2020, Iacopini et al., 2019, Wang et al., 2020).====For the purposes of our work, the most relevant application of simplicial complexes is the paper (Abdou and Keiding, 2019) by Abdou and Keiding. The authors in this article consider a set of agents in a political system and compatibilities among them. If a subset of agents is compatible, they are able to coexist and carry out the processes in their mandate, such as negotiation or passage of legislation. A subset of compatible agents is called a ====. The system of viable configurations can then readily be modeled by a simplicial complex.====The focus of Abdou and Keiding (2019) is the ==== operation, a process of eliminating vertices that are dominated by others in a suitable sense. Strong collapses are a fairly recent notion in the theory of simplicial complexes, due to Barmak and Minian (2012) (see also Barmak, 2011). These collapses are stricter than the classical collapse construction of Whitehead (1939). Their advantage is that they characterize simplicial maps that are ====, and such maps are, in turn, the correct analog of homotopic maps in the category of simplicial complexes. Strong collapses have been used, for example, in topological data analysis (Boissonnat et al., 2018) and the study of the Lusternik–Schnirelmann category of simplicial complexes (Fernández-Ternero et al., 2015).====Abdou and Keiding use strong collapses to develop the idea of a ==== where one agent gives up their standing in favor of another, more centrally placed agent. This leads to the notion of a ==== and, if friendly delegations occur iteratively, the notion of a ====. The authors use various topological tools to examine the conditions under which these compromises may or may not occur, as well as their implications.====The goal of the present work is to further exploit the topology of simplicial complexes as a model for political structures. We interpret some standard topological constructions (simplicial map, wedge, join, cone, suspension) in this context and study their consequences on political systems. For example, the wedge corresponds to merging political structures via representative agents and the cone models the introduction of a mediator.====We also define the notion of the viability of an agent via the star of a vertex and the stability of a structure via the ====-vector of a complex. Much of what we do is devoted to interpreting the interaction of these concepts with the basic constructions on simplicial complexes. For example, we show that introducing mediators or merging structures increases the stability.====One key feature in this work is that we bring homology into the picture. Homology is one of the most useful algebraic invariants of topological spaces. When restricted to simplicial complexes, it lends itself to a relatively easy combinatorial definition and analysis that uses only linear algebra. Since homology is homotopy invariant, it is not quite powerful enough to detect strong equivalences, namely equivalences that can be realized as sequences of strong collapses. However, the presence of nontrivial homology means that a simplicial complex is not contractible, and hence not strongly contractible, and this turns out to convey useful information about the incompatibilities in the structure.====In particular, the presence of nontrivial chains representing homology classes can be directly interpreted as the existence of subsets of agents that are non-viable in specific ways. Much of these observations can also be stated in terms of the ==== of the simplicial complex, which is the smallest subcomplex in which it is no longer possible to perform strong collapses. Homology is essentially able to tell what the core looks like in terms of the number and size of sets of incompatible agents. One can use this for strategic placement of mediators for the greatest impact; this is analogous to the basic algebraic topology procedure of coning off nontrivial cycles as a way to eliminate homology.====Homology is also useful for detecting when certain compromises within structures are not possible. The notions of compromises were defined in Abdou and Keiding (2019), and applying homology to them is one of the ways we revisit and elaborate on the original work by Abdou and Keiding that motivates this paper. We also examine how our definitions of viability and stability mesh with their notion of friendly delegations.====We provide many examples throughout, as well as commentary on when the parallels between the worlds of simplicial complexes and political structures appear to be successful and when they seem to only go so far (see, for example, Remarks 3.2 and 3.12). In particular, we frequently offer potential directions of improvement to the definitions of viability and stability.====One promising avenue is to consider ==== simplicial complexes in which the weights record compatibilities in a more refined way. With more than one issue present, agents may agree on some of them and not the others, and the weights keep track of this. With this in mind, we define stronger notions of viability and stability and hope to continue to examine their merits in future work.====Since we consider this paper as only one of the initial steps (along with Abdou and Keiding, 2019) in what is potentially a rich area of study, throughout the paper we discuss many directions of investigation that could bring sophisticated topology into the realm of political science in an unprecedented way. The dictionary between simplicial complexes and political structures appears to be of great import, and this paper should be thought of as an invitation for its continued exploration.",Political structures and the topology of simplicial complexes,https://www.sciencedirect.com/science/article/pii/S0165489621001025,4 October 2021,2021,Research Article,78.0
"Majumdar Dipjyoti,Roy Souvik","Concordia University, Montreal, Canada,CIREQ, Canada,Indian Statistical Institute, Kolkata, India","Received 27 January 2021, Revised 7 August 2021, Accepted 11 September 2021, Available online 23 September 2021, Version of Record 5 October 2021.",https://doi.org/10.1016/j.mathsocsci.2021.09.002,Cited by (0),"We study probabilistic voting rules in a two-voter model. The notion of incentive compatibility we consider is ==== as introduced in d’Aspremont and Peleg (1988). We show that there exist anonymous and ex-post efficient probabilistic voting rules that are not ==== and at the same time are OBIC with respect to an independently distributed generic prior. This contrasts with the results obtained for deterministic voting mechanisms obtained in Majumdar and Sen (2004) and in Mishra (2016). In case of neutral and efficient rules, there are two kinds of results. First we show that imposing OBIC with respect to some generic prior leads to random dictatorship when there are three alternatives. Second, we show that the result is no longer true when there are four or more alternatives and consequently we provide sufficient conditions on the priors for the result to be true.","In many collective decision making models, randomization is often thought to be a natural way to resolve conflicts of interest. In the random voting model, voters report their ordinal preferences over alternatives to the social planner who then proposes a ==== over alternatives. A random voting rule is thus represented by a ==== (RSCF) that associates with every profile of preferences a lottery over alternatives. The voters have von Neumann-Morgenstern (vNM) preferences over the lotteries. The preferences of the voters are private information, and a critical issue in such problems is to design mechanisms or voting rules that are ====, or in other words induce the voters to report their preferences truthfully in equilibrium. The equilibrium notion that is prevalent in the literature is ==== or truth-telling in dominant strategies. Following the seminal paper by Gibbard (1977), a random voting rule or a RSCF is strategy-proof if the probability distribution under truth-telling first order stochastically dominates the probability distribution under any misreporting of preference by any voter, for every conceivable preferences of the other voters. In the same paper Gibbard shows that any unanimous RSCF is strategy-proof if and only if it is a ====-where each voter is assigned a fixed probability of being the dictator. The probability is fixed in the sense that it is the same across preference profiles. The random dictatorship result is thus a counterpart, in the probabilistic setting, to the classic Gibbard-Satterthwaite dictatorship result for deterministic voting rules (Gibbard, 1973, Satterthwaite, 1975).====Gibbard’s celebrated result demonstrates that strategy-proofness is a demanding requirement and randomization does not significantly expand the set of incentive compatible voting rules relative to the deterministic case. The negative implication of strategy-proofness has motivated the study of weaker solution concepts, primarily for deterministic voting rules. In this paper we explore the implications of weakening the solution concept from strategy-proofness to ====(OBIC) for the case of random voting rules. OBIC is a natural weakening of strategy-proofness in ordinal models. The notion was introduced in d’Aspremont and Peleg (1988) in the context of a different problem, that of representation of committees. A RSCF is OBIC if for every voter the interim expected outcome probability distribution from truth-telling first order stochastically dominates any interim outcome probability distribution induced by misreporting. The interim expected outcome probability is computed with reference to the voter’s ==== about the (possible) preferences of the other voters and is based on the assumption that the other voters follow the truth-telling strategy. For deterministic voting rules or ====(SCF)s that satisfy the additional requirement of unanimity, Majumdar and Sen (2004) show that over unrestricted domain OBIC with independent and ==== priors implies dictatorship. Mishra (2016) extends the result to more general domains. The result was further generalized in Hong and Kim (2020). In the present note we consider a model with two voters. Our objective in this paper is to investigate the structure of RSCFs that are OBIC with respect to some independently distributed generic prior.====We construct an example of an ==== and ==== RSCF that is OBIC with respect to an independently distributed generic prior. This contrasts with the results in Majumdar and Sen (2004) and Mishra (2016) for deterministic SCFs. We then consider ==== RSCFs in the two voter case. Neutrality implies that the RSCF is permutation invariant with respect to the alternatives, see for example, Moulin (1983). We begin with the case of 2 voters and 3 alternatives. We show, that if the RSCFs satisfy the additional property of ==== then OBIC with respect to some independently distributed generic prior implies random dictatorship. This result however does not extend beyond the case of three alternatives. We provide an example of a two voter RSCF with four alternatives that is ====, ====, ====, and OBIC with respect to an independently distributed generic prior.====In the two voter case, for neutral RSCFs, we identify a sufficient condition on priors, ==== ====. While the condition in Majumdar and Sen (2004) is in terms of sums of prior probabilities, Condition ==== is in terms of an appropriately defined matrix of prior probabilities. The class of priors satisfying Condition ====, though generic in a topological sense, is a sub-class of the priors satisfying the condition in Majumdar and Sen (2004). In this two voter model, our core result for neutral RSCFs is the following: for any finite number of alternatives, any ex-post efficient, neutral RSCF is OBIC with respect to a common prior satisfying Condition ==== if and only if it is a random dictatorship.====One obvious limitation of our results is the restriction to two voter models. Unfortunately, so far we have been unable to find a suitable generalization of Condition ==== that would be appropriate for a model with more than two voters. Karmokar and Roy (2020) identify a class of priors having Lebesgue measure one such that OBIC with respect to any such prior together with unanimity implies DSIC. Our result does not follow from this result as it does not hold for generic priors. In fact, neutrality plays a crucial role in deriving our results. In a recent paper, Dasgupta and Mishra (2020) consider a strengthening of the condition of generic priors. In the random assignment model, they consider random assignment mechanisms that are OBIC with respect to all independent priors in some neighbourhood of a given independent prior. This last condition is a local robustness condition. They show that any random assignment mechanism that is locally robust OBIC and satisfies an additional condition of elementary monotonicity is strategy-proof. For a general probabilistic voting model with more than two voters, an appropriate on condition of priors that would precipitate the equivalence between strategy-proofness and OBIC remains an open question.====The rest of the paper is organized as follows. Section 2 introduces the model. Section 3 contains the results for the case of two voters and three alternatives. Section 3.1 provides the example of an anonymous RSCF that is OBIC with respect to a generic prior, while Section 3.2 contains the result for neutral RSCFs. Section 4 deals with neutral RSCFs in the two voter case with more than three alternatives. Section 4.1 provides the example of a neutral, anonymous, ex-post efficient RSCF that is OBIC with respect to a generic prior. Section 4.2 introduces Condition ==== and contains the our main result for neutral RSCFs in the two voter case. Section 5 concludes. The appendix contains the proofs of all the major results in our paper.",Ordinally Bayesian incentive compatible probabilistic voting rules,https://www.sciencedirect.com/science/article/pii/S0165489621001001,23 September 2021,2021,Research Article,79.0
Bolletta Ugo,"Université Paris-Saclay, RITM, Sceaux, France","Received 9 September 2020, Revised 30 July 2021, Accepted 13 September 2021, Available online 22 September 2021, Version of Record 28 September 2021.",https://doi.org/10.1016/j.mathsocsci.2021.09.003,Cited by (4),Designing interventions aimed at fostering ,"Recently, some importance has been attributed to policy interventions that aim at improving outcomes for some groups of targeted individuals by manipulating the composition of their neighbors and exploiting the channel of peer effects. Schools and neighborhoods, where social influence is known as being one of the main drivers of individual outcomes, have been studied through field experimentation. A deep understanding of this kind of interventions is relevant because they represent a costless measure to foster beneficial relationships among individuals, minimizing detrimental encounters, which ultimately improves economically relevant performances. The main obstacle is that once we manipulate the pool of individuals in an environment, we have no control on the endogenous responses of individuals to the said manipulation, because social relationships are endogenous and chosen by individuals.====This paper proposes a theory that considers endogenous choices of peers and peer effects. We model the society through a network, where individuals are nodes and relationships are represented through links. Thus, the specific network configuration impacts how individuals influence each other via peer effects. Agents strategically choose links whose final structure determines behaviors, interpreted as effort provision in this context. The model provides the tools to understand individuals’ reactions to interventions. Our theory is intricately linked to the empirical model known as the linear-in-means model, introduced by Manski (1993). We show that the model developed here can be seen as the data-generating process, once corroborated by structural empirical analysis.====The paper by Carrell et al. (2013), among others, provides an interesting case of compositional interventions. The authors of that paper worked with a school and had the opportunity to manipulate class composition. During an initial period of observation where the authors measured peer effects under random assignment of students to classes, they found heterogeneous effects for ==== (from now on, LAS). At the margin, these students responded positively in terms of schooling outcome to an incremental exposure to ==== (HAS). This evidence prompted an intervention where LAS and HAS were intentionally mixed in classes, intending to improve outcomes for LAS. The observed outcomes of students in the treated classes were significantly worse than the outcome of students with similar abilities in randomized classes. The authors state in their concluding section:====“====”.====We show that the results found in Carrell et al. (2013) are consistent with high levels of segregation, which jeopardizes the potentially beneficial transmission of peer effects. This was also documented in that paper through post-experimental surveys.====The main theoretical results are on the characterization of stable networks. The equilibrium concept is Pairwise Nash stability, which requires that all agents best respond to others’ strategies, but links must be mutually accepted. The model is solved through a sequential algorithm following Watts (2001). There is an iterative process that selects a pair of agents and lets them evaluate whether the link is mutually beneficial. If it is not, either of the two agents can sever any number of already existing links and re-evaluate the selected link. The intuition behind the main result is that agents make distance-based decisions on their friendships, and that the better students choose to avoid interactions with lower ability students, although the latter would benefit from the exchange.====In the main version of the model where schooling outcomes only matter for link formation decisions, we show that the set of stable networks is characterized by local completeness. This means that, when the network is connected and sufficiently dense of links,==== agents form overlapping groups and within each group all agents are friends. Our characterization reduces the set of observable networks and demonstrates characteristics typical of real networks, such as clustering (a friend of my friend is also my friend) and homophily (or assortative matching).==== One of the main implications of the model is that individuals counteract peer effects by sorting themselves into groups of agents with similar traits. This is relevant because in situations where interactions are constrained (e.g. study groups formed by the teacher), connections are in general less assortative. Hence, we should observe a higher impact on outcomes attributable to peer effects, because of the greater heterogeneity of interactions with respect to the unconstrained framework we consider in this model.====To consider further dimensions of heterogeneity, we propose an enriched version of the model where other characteristics not correlated with outcome impact the link formation process. This enables us to explain any observed network structure, using the theory to predict the links in an observed sample and unveil the mechanisms and individual preferences that underlie network formation processes.====From the perspective of policy recommendations, we conclude that segregation in classes jeopardizes peer effects. If the objective of the social planner is to improve the outcome of low achievement students, segregation should therefore be avoided. However, a scheme of incentives needs to be designed to favor interactions in mixed classes that will significantly improve the outcomes of a targeted group of students, without harming other groups of students. The key to this is having a clear picture of the environment and specific context, and therefore it is essential to collect data that reveal the underlying mechanisms behind friendships.====The rest of the paper is organized as follows. Section 2 outlines the related literature. The model is presented in Section 3, where we introduce the setting and notation, and the main results are collected in Section 4. Section 5 discusses a more general version of the model, and its possible applications to experiments and empirical works. A detailed application of the model to Carrell et al. (2013) is carried through Section 6.====Finally, Section 7 concludes. Proofs are in Appendix.",A model of peer effects in school,https://www.sciencedirect.com/science/article/pii/S0165489621001013,22 September 2021,2021,Research Article,80.0
"Gonzalez Stéphane,Lardon Aymeric","Univ Lyon, UJM Saint-Etienne, CNRS, GATE L-SE UMR 5824, F-42023 Saint-Etienne, France","Received 11 January 2021, Revised 18 July 2021, Accepted 5 September 2021, Available online 20 September 2021, Version of Record 8 October 2021.",https://doi.org/10.1016/j.mathsocsci.2021.09.001,Cited by (0),", pairwise stability, and stable matchings, among others. Our characterization of the core invokes the axioms of weak nonemptiness, coalitional unanimity, and Maskin monotonicity together with a principle of independence of irrelevant states, and uses in its proof a holdover property echoing the conventional ancestor property. Taking special cases of this general characterization of the core, we derive new characterizations of the previously mentioned stability concepts.","Many theorists in economics and political science have been occupied in studying a wide variety of stability concepts in social choice and game theory for a century or more. Generally speaking, these stability concepts are mainly founded on the idea that given some prevailing state, individuals possess some blocking power to oppose that state and exercise it when they have an interest to do so. A stable state is understood to be a state for which no individual or group of individuals has the power to change the status quo by choosing a more desirable situation. This arises, for example, in a general equilibrium of markets where economic agents on both the demand and supply sides do not have any incentive to alter their consumption or production decisions at the given market price. In the same vein, elections in political systems rely on voting rules (quorum, majority, etc.) that allow some coalitions of voters to impose their chosen candidate on the entire society. In like manner, equilibrium concepts for non-cooperative games (Nash equilibrium, subgame perfect equilibrium, etc.) recommend a state robust to deviations in strategy in both static and dynamic settings. Likewise, many solution concepts for coalitional games (core, stable set, etc.) stress cooperative agreements on utility allocation that no coalition would contest.====In this article, we consider the general framework of games in effectiveness form (henceforth e-form games), first introduced by Rosenthal (1972), which encompasses a vast range of contexts, including voting problems, normal form games, network problems, and matching models, among others. The canonical e-form game has the following features. A set ==== of players is equipped with preferences over a set ==== of states. Players are mutually aware of each other’s preferences, can form coalitions, and sign binding agreements to oppose a given state. In addition, the blocking power distribution among coalitions is described by an “effectiveness function”; given a prevailing state ==== in ====, coalition ==== is effective for ==== at ==== if it can force all players to move from state ==== to some state in ====. Such a function specifies for every coalition ==== of players and subset ==== of states of ==== whether or not ==== is effective for ====. Without going into details, this way of defining the effectivity of coalitions is similar to the “inducement correspondence” introduced by Greenberg (1990) and is more general than the notions of “effectivity function”, “effectiveness relation” and “effectivity correspondence” respectively proposed by Moulin and Peleg, 1982, Chwe, 1994 and Demuynck et al. (2019). The effectiveness function also corresponds to a special case of the “local effectivity function” suggested by Abdou and Keiding (2003). The literature on effectivity functions generally separates the social states to which the coalitions have a right from the means authorized to achieve them. The first is similar to a notion of rights system (or constitution), that describes to which social states or sets of social states groups of individuals are entitled, while the second describes the rules that delimit the actions permissible to individuals (see for instance Peleg and Peters, 2010). In the standard approach, the effectiveness function describes the rights system allocated to coalitions, while the actions available to individuals are modeled through a game form. The possible outcomes of the game form are then required to be consistent with the effectivity function. More precisely, the game form must provide each group in society with possibilities that are consistent with those provided by the rights system. The standard approach further assumes that the agents are all consequentialists: Once the game form has been defined, the preferences relate only to the outcome of individual acts, not to the nature of the act. In this article, we maintain that e-form games describe, for each coalition, the states to which they can claim, but we allow these states to be identified with authorized behaviors or actions. One of the advantages, especially when we model games in strategic form (see Section 3.2), is that agents are allowed to be non consequentialist, as no assumptions are made on the object of individual preferences (acts or consequences of acts).====Since players can behave cooperatively to oppose a given state, the solution concept we consider here is a version of the core of e-games (see Rosenthal, 1972). A state ==== is core-stable if there are no coalition ==== of players and a subset ==== of states for which ==== is effective for ==== at ==== and in which every player in ==== strictly prefers every state in ==== to ====. The most remarkable feature of the core is the fact that a wide variety of prominent stability concepts in social choice and game theory, such as the Condorcet winner, the Nash equilibrium, pairwise stability, and stable matchings, among others, coincide with the core applied to some classes of e-form games by means of an appropriate effectiveness function (Propositions 3.1, 3.2, 3.3, and 6.8). More precisely, by fixing what constitutes the blocking power of coalitions, we can express these stability concepts in terms of the core for a suitable class of e-form games. The core of e-form games is the counterpart of the core for social environments analyzed by Demuynck et al. (2019). It is also equivalent to the ====-equilibrium for rights structures introduced by Koray and Yildiz (2018). Recently, Korpela et al. (2020) provide a full characterization of the implementation in ====-equilibrium via rights structures .====Despite the diversity of existing stability concepts such as those just mentioned, very little is known about the properties that unify them. To address this issue, we propose to axiomatically characterize the core on a vast range of classes of e-form games. Broadly speaking, the axiomatic method is adopted in the design of solutions for decision problems. It begins with the formulation of the properties of solutions, called axioms, and addresses the question of existence of solutions satisfying some combinations of the axioms. A characterization theorem identifies a unique solution, or a class of solutions, satisfying the axioms which are viewed as building blocks in their construction. The study of the axiomatic foundations of the core across different classes of e-form games appears to be of primary importance since it permits to establish that a wide variety of solution concepts that admit a core representation on a specific class of e-form games are constructed from the same set of axioms. Formally, the core is a correspondence that associates a (possibly empty) subset of core-stable states with each e-form game. Perhaps unexpectedly, the core is characterized on a wide range of classes of e-form games by a set of four axioms which are reasonably weak and intuitive (Corollary 4.6). “Weak nonemptiness” requires that when the core is nonempty, a solution to contain at least one state. “Coalitional unanimity” establishes that if a state ==== is selected for an e-form game, then ==== must belong to any top set of states ==== for players in some coalition ==== effective for ==== at ====. If a state ==== is selected for an e-form game, then “Maskin monotonicity” asserts that it is also selected in an e-form game where ==== has (weakly) improved in the preference rankings of all players. “Independence of irrelevant states” specifies that if a state is selected for an e-form game, then it is still selected when non-selected states are removed from the game. This latter axiom is in line with other principles of independence widely used in characterizations of game-theoretic solutions (see Nash, 1950a, Arrow, 1950, Chernoff, 1954, Sen, 1969, Sen, 1993). For some classes of e-form games such as, for example, those derived from network problems, states cannot be removed without withdrawing players associated with them. Our principle of independence permits withdrawing such players when necessary.====We first show that if a solution is coalitionally unanimous and Maskin monotonic, then it is a subsolution of the core (Theorem 4.2). Then, we prove that if a subsolution of the core is nonempty and satisfies independence of irrelevant states, then it is the core (Theorem 4.5), provided the class of e-form games satisfies a new property, called the holdover property, which plays a key role in the proof of this statement. This property echoes the conventional “ancestor property” and specifies that, given a state ==== in the core of an e-form game, it is always possible to introduce additional states (and their associated players when necessary) in such a way that the core of the new augmented e-form game only contains state ====. This methodology constitutes an alternative to the use of the so-called bracing lemma, which is a typical consistency result for many game-theoretic models (see Thomson, 2011). The complementarity of these two approaches is highlighted in Section 6.2.====Using the building blocks leading up to our axiomatic characterization of the core (Corollary 4.6), we provide new axiomatic characterizations of the Condorcet winner correspondence (Corollary 5.2), the Nash equilibrium correspondence (Proposition 5.5), and the pairwise stability correspondence (Proposition 5.7). This mainly consists in reformulating our general axioms for specific classes of e-form games underlying these stability concepts and showing that these classes satisfy the holdover property. The Condorcet winner has recently been characterized by Horan et al. (2019) with axioms different from ours. As far as we know, the pairwise stability correspondence has never been characterized axiomatically before. Our characterization of the Nash equilibrium correspondence is compared with the existing ones proposed by Peleg and Tijs (1996) and Ray (2000), allowing two other axiomatic characterizations to be established (Proposition 6.4). By invoking a consistency principle instead of an independence principle and applying a bracing lemma in the framework of e-form games, we provide a second axiomatic characterization of the core (Theorem 6.7) and apply it in the context of the stable matchings (Proposition 6.10).====The rest of the article is organized as follows. Section 2 introduces the framework of e-form games and the related concept of the core. Specific classes of e-form games for which the core coincides with existing stability concepts are constructed in Section 3. Section 4 presents the main axiomatic characterization of the core. Section 5 contains the specific characterizations of the Condorcet winner correspondence, the Nash equilibrium correspondence and the pairwise stability correspondence. The consistency principle for e-form games is discussed in Section 6 and used to characterize the Nash equilibrium correspondence and the stable matchings correspondence. All proofs appear in Appendix.",Axiomatic foundations of the core for games in effectiveness form,https://www.sciencedirect.com/science/article/pii/S0165489621000998,20 September 2021,2021,Research Article,81.0
Honda Edward,"Washington University in St. Louis, One Brookings Drive Campus Box 1208, St. Louis, MO 63130, United States","Received 1 April 2020, Revised 17 August 2021, Accepted 27 August 2021, Available online 5 September 2021, Version of Record 22 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.08.006,Cited by (0),"We provide an additive utility representation for any complete and transitive preference over menus of alternatives. The representation makes it seem as if a sophisticated individual takes into account the state dependent payoffs of two individuals that have to consume the same alternative from the menu but may have inconsistent preferences in some of the subjective states. We also consider special cases including when only one state is needed. Furthermore, we find upper bounds on the minimal number of subjective states needed for the representation.","Representations with very different behavioral implications have been developed for preferences over menus of alternatives. The representation of Kreps (1979) imposes Monotonicity, which says that expanding menus by adding alternatives is always desirable. In contrast, the representation of Gul and Pesendorfer (2001) imposes Set Betweenness, which says that the union of two menus must lie in between the two menus in terms of preferences. The interpretation is that shrinking menus by deleting alternatives that will not be chosen is always desirable (because this will lead to less temptations).====While the two models seem to capture behaviors that are very different, it is fair to say that both types of behaviors are observed in reality. That is, sometimes we may want to restrict the size of menus to avoid temptations, but sometimes we may want larger menus to allow for flexibility of choice. Thus, a more general representation that allows for both kinds of behaviors may be desirable.====Gorno (2016) proposes such a representation in a setting where menus are drawn from a finite set of alternatives. He shows that the representation is simply characterized by the completeness and transitivity of preferences over menus. What we provide is a different representation for the same set of preferences. In our representation, two state-dependent payoff functions are summed in each state and one is injective while the other takes the maximizer of this injective function as the input argument. In particular, for a menu ====, the utility derived from the menu is ====where ==== is the set of states, and ==== is the probability of the state ==== and ==== is an injective function for each ====. For illustrative purposes, suppose that the alternatives are meals and that each menu ==== is a menu of meals. Then the use of the same input argument makes it seem as if two separate individuals have to consume the same meal from the menu. Since the two payoff functions may represent different preferences over meals, the interpretation is that a sophisticated individual ranks menus by taking into account the preferences of two individuals that share a meal and have potentially inconsistent preferences over meals. This is in contrast to the representation of Gorno (2016) ====where maximized values are subtracted in some states. This gives the two representations very different interpretations. One advantage of our representation is that it allows for a very simple proof.====The generality of the class of preferences we deal with and the simple proof, however, comes at a cost. The subjective state space may not be unique. Nor is it clear how to characterize the minimal number of states needed for a representation. As a partial solution, we provide upper bounds on the minimal number of states needed for the representation of a preference relation using the result of Gorno (2016).====The rest of the paper proceeds as follows. We set up the model and present the representation theorem in Section 2. In Section 3, we analyze some special cases and briefly discuss the upper bound on the minimal number of states needed. We conclude with some more related literature in Section 4. The proofs are relegated to Appendix.",Sophistication and preference inconsistency in a menu utility representation,https://www.sciencedirect.com/science/article/pii/S0165489621000962,5 September 2021,2021,Research Article,82.0
"Minagawa Junichi,Upmann Thorsten","Faculty of Economics, Chuo University, 742-1 Higashinakano, Hachioji, Tokyo 192-0393, Japan,Helmholtz-Institute for Functional Marine Biodiversity at the University of Oldenburg (HIFMB), Ammerländer Heerstraße 231, 23129 Oldenburg, Germany,Carl von Ossietzky Universität Oldenburg, Germany,Bielefeld University, Germany","Received 14 January 2021, Revised 22 June 2021, Accepted 18 August 2021, Available online 27 August 2021, Version of Record 22 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.08.005,Cited by (1),"We consider an optimal commodity taxation problem under a consumption target with many consumer types and prove the existence of an optimal solution for this problem. The optimal solution obeys taxation rules that are contrary to standard ones such as the inverse-elasticity rule. For a single consumer type, we show that the ==== for the optimal solution obey the uniform pricing rule.","While commodity taxes are usually aimed at raising public revenue, another frequently proposed motive is to interfere with consumption of demerit goods, such as fuel, alcohol, and cigarettes. Recently, the WHO has recommended taxation of sugary drinks as a major action to reduce over-consumption of sugars, which has been identified as a major contributor to obesity, diabetes, and tooth decay (see, e.g., World Health Organisation 2017). In 2018, a tax on sugar added to drinks became effective in UK and in parallel in Ireland; prior to this, Philadelphia, USA (in 2017), St. Helena (in 2014), Mexico (in 2014), Hungary (in 2011) also introduced taxes on beverages with added sugar; and in 2012 France introduced a similar tax on both beverages with added sugar and beverages with artificial sweeteners. While limitation of consumption is a widespread rationale for the taxation of commodities, the role of quantity constraints (or target levels) on consumption has not been studied thoroughly in the commodity taxation literature, pioneered by Ramsey (1927) and Diamond and Mirrlees (1971).====In order to fill this gap, we here shed light on the role of a quantity constraint (or a target level) on consumption for the rules of optimal taxation. To this end, we ignore possible issues of tax revenue, such as a revenue target, the redistribution of public funds, etc., but focus on the case where the primary purpose of the government is to interfere with the total consumption for a group of commodities by means of commodity taxation. While we are predominantly interested in a normative issue, ==== in the efficiency of commodity taxation when a government gives high priority to comply with a specified consumption target, our analysis is also descriptive: If we define the term “tax” (or “subsidy”) in a broad sense covering various types of costs imposed on consumption by the government, we find quite a wide range of real world situations fitting our model with a consumption target. For example, governments introduce charges for cars entering restricted areas (such as central London) to reduce congestion; or they introduce subsidies for the purchase of electric or hybrid cars in order to reduce energy consumption and to limit CO==== emissions. Moreover, in times of the coronavirus, governments introduce fines for non-wanted behaviour, e.g., fines for not wearing a face mask in public, fines for tourists from abroad who refuse to being tested, etc.; and they introduce subsidies for coronavirus vaccination. In all of those cases, the consumption level is the main concern of the government, while the collection of public funds does not play any, or only a subordinate role.==== In this paper, we therefore consider a government facing a given consumption target for a group of commodities (such as different types of sugary drinks), and explore how it should set taxes and subsidies on the commodities to accomplish that target level.====Minagawa and Upmann (2018) formulated an optimal commodity taxation model under such a consumption target where non-compliance with the target is allowed: a government chooses the consumer prices for a given group of commodities to maximise consumer welfare minus the deviation cost of missing the target for the total consumption of the commodities. In that paper, the authors obtained an unconventional taxation rule: the generalised ====-inverse elasticity result, saying that, within this group of commodities, ==== prices should be charged for commodities with high price elasticities of total demand. An intuition for this result is that in order to attain the consumption target, a more price elastic commodity requires a smaller price change than does a less price elastic commodity. In this way, the target level is attained by relatively small price distortions and hence in a more efficient way.====However, there are two limitations to this result. First, the taxation rule is derived from the first-order conditions, implicitly assuming that the second-order conditions (or sufficient conditions) for optimality hold. Hence, the question of whether or not the taxation rule determined by the first-order conditions is indeed optimal remains open. This question of the optimality of the first-order taxation rules frequently arises in optimal taxation models where the objective function is not concave in the choice variables, as discussed by Mirrlees (1986, Sec. 2), and also cautioned by Myles (1995, pp. 113–14). This issue, though, is particularly significant in this paper, as the first-order taxation rule under consideration is contrary to standard taxation rules, which might raise suspicion on the optimality of that first-order taxation rule. Second, Minagawa and Upmann (2018) derive the taxation rule for a single consumer type. It is thus unclear whether or not this is the case for many types of consumers.====Here, we address these open issues: i.e., the validity of the first-order taxation rule and its generalisation to the case of many types of consumers. To this end, we deploy a model with many consumer types and consider the following optimal commodity taxation problem under a consumption target: a government chooses the consumer prices for a group of commodities to maximise social welfare subject to the constraint that the total consumption of the commodities must meet a given target. Since under standard assumptions on preferences indirect utility functions are quasi-convex in prices, sufficient conditions for optimality are hard to verify in that case, as noted by Dixit (1990, p. 84). To deal with this difficulty, we choose another route: We first demonstrate that there exists a solution to our problem; it then follows that, under a constraint qualification, the solution must satisfy the first-order conditions, and hence it obeys the resulting first-order taxation rule. Moreover, this rule implies the generalised anti-inverse elasticity result for the case of many types of consumers: for any taxed commodity the consumer price is proportional to each consumer’s elasticity of total demand with respect to the price of that commodity.====We next consider the case of a single consumer type in more detail and show that the first-order taxation rule derived here is essentially the same as the generalised anti-inverse elasticity rule in Minagawa and Upmann (2018), and it is indeed optimal. We also obtain a result, known as the uniform pricing result, that the optimal consumer prices are all equal if, and only if, the elasticities of Hicksian demand of the taxed commodity with respect to an untaxed commodity are all equal and non-negative (i.e., weakly substitutable); moreover, we reveal that for homothetic preferences, this elasticity condition is equivalent to the condition that the elasticities of Marshallian demand of the taxed commodity with respect to the untaxed commodity are all equal. Finally, we provide an example that yields a unique optimal solution with uniform pricing.",The generalised anti-inverse elasticity rule: An existence result,https://www.sciencedirect.com/science/article/pii/S0165489621000780,27 August 2021,2021,Research Article,83.0
"Ozdogan Ayca,Saglam Ismail","Department of Economics, TOBB University of Economics and Technology, Sogutozu Cad. No:43, Sogutozu, 06560, Ankara, Turkey","Received 25 May 2020, Revised 4 July 2021, Accepted 16 August 2021, Available online 27 August 2021, Version of Record 22 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.08.003,Cited by (0),"We extend Aumann’s (1974) model of ==== by requiring each player to bear an exogenously given cost if s/he disobeys the recommendation of the mediator. Calling the modified solution ==== (costly-CE), we show that in any finite normal-form game that has an unpure ====, the set of costly-CE strictly expands even with an arbitrarily small increase in the disobedience cost provided that the game is non-trivial and there is room for expansion. We also study the effects of the disobedience cost on the total welfare of players and the value of mediation.","One important goal of non-cooperative game theory is to explore how one can attain efficient and cooperative outcomes in a self-enforcing way. A remarkable idea to achieve this goal is mediation through a correlation device (public roulette), as introduced by Aumann (1974). This idea suggests that in certain games some strategy profiles that cannot be observed in any Nash equilibrium (NE) can be played under appropriately chosen correlated recommendations of a credible mediator, forming a correlated equilibrium (CE).==== The enlargement of the set of equilibria through correlated recommendations may increase the maximal total utility the players can expect to attain since some of the additional equilibria may Pareto dominate the Nash equilibria. But this enlargement may also result in a new (aggravated) ‘coordination problem’ if the additional equilibria, (CE====NE), contain a multiplicity of efficient alternatives and the players have unaligned preferences over them. Although the correlating device, or the mediator, in Aumann’s model (1974) helps to coordinate the actions of the players, it cannot completely prevent or eliminate this additional coordination problem, nor its original form that may arise in any game with multiple Nash equilibria. This is inevitably so, because the mediator’s recommendations are only self-enforcing for the players; the mediator imposes no sanctions or punishments if a player chooses not to follow the recommendations after s/he has received them and thereby obtained (conditional) information about how other players may act in the game. However, in practice, individuals may bear tangible or intangible costs whenever they disobey the recommendations of mediatory agencies. A well-known example is the traffic management system, i.e. the traffic lights, which can be viewed as a mediator, sending private but correlated recommendations (based on the outcome of a commonly known lottery) to the drivers at an intersection. Everyone follows or is expected to follow the recommendations of a traffic light believing that every other is going to do so. However, in reality, there are also costs of not obeying recommendations, such as paying a fine for passing at a red light and facing casualties or fatalities. The presence of such costs helps to alleviate the issues related to drivers’ trust in each other in following the recommendations.==== Benefits of instituting such costs may also be substantial for mediatory institutions like government agencies or independent international bodies (such as the European Convention and Court of Human Rights) giving recommendations to all relevant parties that participate in issues such as environmental agreements, legal negotiations, etc.====Despite their practical relevance and potential benefits, disobedience costs seem to have been ignored so far by the theoretical literature studying the CE concept and its extensions. Our paper, to the best of our knowledge, is the first that accommodates these costs and studies their implications on the set of CE outcomes. In more detail, our paper extends Aumann’s (1974) CE concept to allow for a non-negative cost of disobedience for each player, calling the new solution as ==== (costly-CE).==== We prove that in any non-trivial finite normal-form game that has an unpure Nash equilibrium, the set of costly-CE strictly expands even with an arbitrarily small increase in the disobedience cost provided that there is room for expansion. We also study the effects of the disobedience cost on the total welfare of players and the value of mediation. We show that Pareto superior outcomes can be obtained and the value of mediation cannot decrease with an increase in the disobedience cost.====The CE concept has been very appealing because, in some games where the set of CE is larger than the set of NE, some CE outcomes may strictly improve upon NE outcomes. However, even in such games, the efficient outcome maximizing the total welfare may not be attainable and this insufficiency has led game theorists to search for extensions of the CE. The first extension was introduced by Moulin and Vial (1978), which was later termed as “coarse correlated equilibrium” (CCE) by Young (2004) and “weak correlated equilibrium” (WCE) by Forgó (2010). Like the CE solution, the CCE solution picks the outcome of the game according to a commonly known probability distribution. The difference in CCE is that each player must first decide to commit or not to follow the strategy recommended by the mediator before the mediator starts the randomization process. A player who does not commit could choose any of her/his available strategies but s/he cannot receive any information about the outcome of the lottery. Again as in CE, it is ex-ante optimal to commit to the expected outcome of the lottery if a player believes that every other player is doing the same. Moulin and Vial (1978) show that it is possible to improve upon a completely mixed NE by CCE in strategically zero-sum games where CE cannot improve upon NE.====In a more recent study, Forgó (2010) proposes for finite games another generalization of CE, called soft correlated equilibrium (SCE). He shows that neither SCE nor CCE is a special case of the other, and in some normal-form games SCE can generate outcomes that are Pareto superior to those generated by CCE. The only difference of the two solutions is that if players choose in SCE not to commit to the recommendations of the mediator, then they can choose any action other than the one suggested by the mediator. Forgó (2010) shows that while CCE and CE cannot improve upon the unique NE in the Prisoners’ Dilemma game, SCE could do so.====Unlike the earlier extensions in the literature, we do not change the strategy spaces or the information structure in a given game.==== We preserve the whole game structure considered by Aumann (1974) and integrate the payoff table with non-negative costs of disobedience. We should notice that these costs are not already internalized by the payoff tables in Aumann’s (1974) model because we assume that they are borne by players only if they deviate from the strategies recommended by the mediator. So, any player’s payoffs are asymmetrically affected by the cost of disobedience. The payoffs that may be obtained from the recommended strategy stay as they are in the original payoff table free of any cost, while all other payoffs corresponding to deviant strategies decrease by the cost of disobedience. We should also notice that under our costly-CE concept, any strategy profile can be sustained as an equilibrium when the disobedience cost is ====. What we show in this paper is that under certain conditions this can be true even when the disobedience cost is ====.====The rest of the paper is organized as follows: Section 2 introduces the model. Section 3 presents our results, Section 4 concludes and discusses about our future research agenda. We relegate all proofs to Appendix.",Correlated equilibrium under costly disobedience,https://www.sciencedirect.com/science/article/pii/S0165489621000767,27 August 2021,2021,Research Article,84.0
"Kruger Justin,Sanver M. Remzi","Université Paris-Dauphine, Université PSL, CNRS, LAMSADE, 75016 Paris, France","Received 11 February 2021, Revised 8 August 2021, Accepted 8 August 2021, Available online 26 August 2021, Version of Record 22 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.08.002,Cited by (0),"We consider Arrow’s and Wilson’s theorems as conditions on domains, as well as a third condition due to Malawski and Zhou (1994). We describe when domains satisfying each combination of these conditions exist.","Wilson’s (1972) theorem is sometimes considered a strengthening of Arrow (1950) theorem. Indeed, on the full domain, Arrow’s theorem is a ready corollary of Wilson’s result. Although the reverse implication is less intuitively obvious, Malawski and Zhou (1994) give an intermediate result which can be used to fashion a proof of Wilson’s theorem using Arrow’s theorem. They also give domain conditions under which their intermediate result applies. Thus, for domains satisfying these conditions, Wilson’s theorem can be obtained through Arrow’s theorem. Campbell and Kelly (2003) further explore the two theorems under domain restrictions by giving example domains where Arrow’s theorem fails and Wilson’s holds, and vice-versa.====Following Campbell and Kelly (2003), we consider these theorems as conditions on domains. To see how we do this, first consider Arrow’s and Wilson’s theorems in the traditional manner, that is, as implications between properties of social welfare functions. One such version of Arrow’s theorem is the following: if a social welfare function====(1) is defined on a domain containing free triples,====(2) satisfies binary independence, and====(3) is Pareto optimal,====then====(4) this social welfare function is dictatorial.====We remove the property concerning domains – here, (1) the free triple property – from the antecedent of the implication, and designate those domains over which the remaining antecedents (2) and (3) imply the original consequent (4) as Arrow domains. Similarly, we define Wilson domains as those where the implication of Wilson’s theorem – minus any domain properties – holds; and similarly Malawski–Zhou domains for the result of Malawski and Zhou (1994).====Obviously, each of these three conditions – namely Arrow, Wilson and Malawski–Zhou – will be satisfied by some domains and failed by others. An interesting question concerns what logical combinations of the three conditions are possible. This paper gathers the findings that are dispersed in the literature; adds new ones, and provides an almost complete map with respect to the logical (in)compatibilities between these three conditions. To this end, the remainder of the paper is split into three further sections. Immediately after this introductory section, we provide formal definitions in Section 2. Section 3 contains the results that are appraised and catalogued in a Venn diagram in Section 4.",The relationship between Arrow’s and Wilson’s theorems on restricted domains,https://www.sciencedirect.com/science/article/pii/S0165489621000755,26 August 2021,2021,Research Article,85.0
"Li Guanhao,Puppe Clemens,Slinko Arkadii","Department of Mathematics, University of Auckland, New Zealand,Department of Economics and Management, Karlsruhe Institute of Technology, Germany,Higher School of Economics, Russian Federation","Received 27 September 2020, Revised 25 May 2021, Accepted 27 July 2021, Available online 4 August 2021, Version of Record 14 August 2021.",https://doi.org/10.1016/j.mathsocsci.2021.07.005,Cited by (3),"In this paper, we classify all maximal peak-pit ==== domains of maximal width for ==== alternatives. To achieve this, we bring together ideas from several branches of ====. The main tool used in the classification is the ==== of a domain. In contrast to the size of maximal peak-pit ==== domains of maximal width themselves, the size of their associated ideal is constant.","Condorcet domains are sets of linear orders such that the pairwise majority relation is acyclic whenever all individuals have preferences taken from the given set. Condorcet domains are therefore sometimes also referred to as ==== (Fishburn, 1996). Condorcet domains play an important role in applications because they allow both for Arrovian aggregation (Aleskerov, 1999) and for strategy-proof social choice (Puppe and Slinko, 2019). A major research question in the literature has been the problem of describing ‘large’ Condorcet domains. It is well-known that on a set of ==== alternatives there always exist Condorcet domains of cardinality ====, examples are the domains of preferences that are single-peaked with respect to a given linear ordering of the underlying set of alternatives; however, it is also known that, except for the case ====, this is not the largest cardinality of a Condorcet domain, see Monjardet (2009). In the search for large Condorcet domains, the class of so-called ‘peak-pit’ Condorcet domains of maximal width have been a main object of investigation, and in fact members of this class have been shown to constitute the Condorcet domains of maximal cardinality for ====.==== Peak-pit domains are also distinguished by a number of deep connections to other combinatorial concepts such as reduced decompositions of permutations, rhombus tilings and arrangements of pseudolines (Galambos and Reiner, 2008, Danilov et al., 2012).====The concept of the ==== of a domain of linear orders was introduced in Danilov et al. (2012) as a generalization of the spectrum of a rhombus tiling and the family of chamber sets of an arrangement of pseudolines. It was used as a key technical tool in proving that the class of maximal peak-pit domains of maximal width coincides with the class of rhombus tiling domains. Specifically, Danilov et al. (2012) established that the ideal of a peak-pit domain is a separated system of subsets;==== this allowed them to use (i) the result of Leclerc and Zelevinsky (1998) who proved that maximal separated systems of sets are exactly the set of labels of chambers of arrangements of pseudolines, and (ii) the correspondence between rhombus tilings and arrangements of pseudolines established in Elnitsky (1997).====In this paper, we show how the method of ideals can be principally applied to the classification of all maximal peak-pit domains of maximal width, and we carry the steps out for alternative sets with ==== alternatives. Up to isomorphism and flip-isomorphism, there are ==== maximal Condorcet domains on a set of four alternatives, and ==== maximal Condorcet domains on a set of five alternatives (Dittrich, 2018). For ====, only three of the ==== maximal Condorcet domains are peak-pit domains of maximal width: a single-crossing domain, the single-peaked domain, and the domain obtained by Fishburn’s alternating scheme; these domains have cardinalities ====, ==== and ====, respectively. For ====, we show that among the ==== maximal Condorcet domains there are exactly ==== peak-pit domain of maximal width and we list them explicitly; the cardinalities of these domains are ====, ====, ====, ====, ====, ====, ==== and ====.====A number of important properties of peak-pit domains of maximal width follow directly or indirectly from their connections to the aforementioned combinatorial concepts. One of the most important is that every maximal peak-pit domain of maximal width is ====, that is, the two completely reversed orders can be connected by a shortest path in the permutahedron, see Danilov et al. (2012) and Puppe and Slinko (2019). We strengthen this result by showing that any two linear orders of a maximal peak-pit Condorcet domain of maximal width are in fact connected by a ==== path in the permutahedron. We also show that the cardinality of the ideal of every maximal peak-pit domain of maximal width on ==== alternatives is exactly ====. This is all the more remarkable since the cardinality of maximal peak-pit domains itself can vary significantly.",Towards a classification of maximal peak-pit Condorcet domains,https://www.sciencedirect.com/science/article/pii/S0165489621000731,4 August 2021,2021,Research Article,86.0
"Briec Walter,Dumas Audrey,Mekki Ayman","LAMPS, University of Perpignan, 52 Avenue Paul Alduy, F-66860, Perpignan Cedex, France,CDED, University of Perpignan, 52 Avenue Paul Alduy, F-66860, Perpignan Cedex, France","Received 24 July 2019, Revised 26 February 2021, Accepted 19 July 2021, Available online 27 July 2021, Version of Record 11 August 2021.",https://doi.org/10.1016/j.mathsocsci.2021.07.004,Cited by (3),The paper introduces in ==== theory a ====.,"This paper introduces a ==== that looks for possible increases in consumer satisfaction and quantifies the level of inefficiency of a given allocation with respect to the utility possibilities frontier. The construction of a directional distance function in a social choice context is inspired from a sequence of contributions by Luenberger, 1992, Luenberger, 1994, Luenberger, 1996 who proposed a general approach to measure and characterize Pareto efficient allocations. Introducing a function he termed the social benefit function, he showed how to transform the question of Pareto efficiency into an optimality principle. The tools introduced by Luenberger have been applied to production theory by Chambers and Chung, 1996, Chambers et al., 1998 who have extended this concept to measure technical efficiency and productivity. Along this line, Chambers (2003) has shown that consumers’ surplus can be viewed as an exact and superlative cardinal welfare measure. More recently, Chavas (2008) has used the benefit function to explore the connection between benefit functions and fairness. Blackorby and Donaldson (1980) used similar tools in the treatment of indices of absolute inequality. Notice also that X and Silber (2005) used efficiency analysis to the study of the dimensions of human development.====Our approach takes into account possible externalities and, like the benefit function due to Luenberger (1992), it provides a kind of calculus for Pareto efficiency. However, while Luenberger’s framework involves a direct transformation of the bundles consumed by the agents, we focus on the collection of utility functions arranged into a vector function.====The directional distance function translates the evaluation of an allocation at the level of individuals into a ranking of these allocations at the level of the society they compose. The paper shows that the directional distance function satisfies some important properties. However, it fails the axioms of unrestricted domain and indifference of other alternatives proposed by Sen, 1977, Sen, 1979 in his definition of welfarism. The approach proposed in the paper involves a complete transitive preorder satisfying some suitable invariance properties. An elementary transformation of the function is proposed to satisfy the criteria of Pareto efficiency and strong monotonicity.====Directional distance functions and social welfare functions are intimately linked. It is established that it is dually linked to the weighted Rawls welfare function and the indirect Rawls welfare function. The weights simply reflect the relative value of marginal consumption which society places upon each individual. Given a weighting scheme of the individual preferences, the indirect Rawls welfare function gives the maximal attainable Rawls welfare function faced with an utility possibilities set.====A specific relation to the weighted utilitarian (Bentham) welfare function also exists when convexity holds. The directional distance function appears to be dually linked to the weighted utilitarian welfare function and all the duality results obtained in an utilitarian context can be obtained by replacing the summation of individual utility values for the minimum utility value. Under a convexity assumption, the utilitarian social welfare function, arising in an utilitarian context, is dual to the directional distance function. One can then establish the full duality of these concepts and provide an additional characterization of the Negishi theorem.====Lastly, a more general class of distance functions is introduced that includes as a limit case the directional distance function. The construction that is proposed focuses on the criterion of Pareto efficiency. Basically, the traditional way to define inequality measures is a related notion of efficiency. The basic idea is that an inequality measure evaluates some kind of distance between an allocation and an egalitarian allocation. Along this line, a measure is proposed that quantifies the potential improvement of the value of a large class of functions allowing to retrieve as a special case the Nash approach of the bargaining problem (Nash, 1950) and the Kolm–Pollak form (Kolm, 1976).====The paper unfolds as follows. First, we lay down the groundwork and define the notations. Section 3 is devoted to introduce the directional distance function. We show it enjoys some interesting axiomatic properties and, as such, is a tool to characterize Pareto efficiency and compare sub-efficient allocations. The axiomatic of the quasi-order induced by the directional welfare functional are investigated regarding to welfarist theories. Section 4 analyzes the dual properties of the directional distance function and establishes that it can be connected to the weighted utilitarian social welfare functionals. Non-convexities are also analyzed, a duality result derived from the Rawls welfare function is established. Section 5 proposes a generalized notion of directional distance function that establishes some connection to many functional forms used inequality theory.",Directional distance functions and social welfare: Some axiomatic and dual properties,https://www.sciencedirect.com/science/article/pii/S016548962100072X,27 July 2021,2021,Research Article,87.0
"Borkotokey Surajit,Chakrabarti Subhadip,Gilles Robert P.,Gogoi Loyimee,Kumar Rajnish","Department of Mathematics, Dibrugarh University, Dibrugarh 786004, Assam, India,Management School, Queen’s University Belfast, 185 Stranmillis Road, Belfast, BT9 5EE, United Kingdom,Department of Applied Mathematics, Northwestern Polytechnical University, Xi’an, PR China","Received 28 October 2019, Revised 31 May 2021, Accepted 17 July 2021, Available online 26 July 2021, Version of Record 5 August 2021.",https://doi.org/10.1016/j.mathsocsci.2021.07.003,Cited by (0),"We consider a class of cooperative network games with transferable utilities in which players interact through a probabilistic network rather than a regular, deterministic network. In this class of wealth-generating situations we consider probabilistic extensions of the Myerson value and the position value. For the subclass of probabilistic network games in multilinear form, we establish characterizations of these values using an appropriate formulation of component balancedness. We show axiomatizations based on extensions of the well-accepted properties of equal bargaining power, balanced contributions, and balanced link contributions.",None,Probabilistic network values,https://www.sciencedirect.com/science/article/pii/S0165489621000718,26 July 2021,2021,Research Article,88.0
"Busetto Francesca,Codognato Giulio,Tonin Simone","Dipartimento di Scienze Economiche e Statistiche, Università degli Studi di Udine, 33100 Udine, Italy,Economix, UPL, Univ Paris Nanterre, CNRS, F92000 Nanterre, France","Received 8 January 2021, Revised 26 April 2021, Accepted 8 July 2021, Available online 18 July 2021, Version of Record 22 July 2021.",https://doi.org/10.1016/j.mathsocsci.2021.07.001,Cited by (4),"In this paper, we use the ","Vohra (2011) based his monograph on mechanism design on integer programming. He claimed that this approach has basically three advantages: simplicity, unity, and reach, meaning, respectively, that it may simplify arguments, unify disparate results, and solve problems which are beyond the reach of other approaches.====In this paper, we use the evoked advantages of the integer programming approach to analyze the Simple Majority Rule (SMR). Sethuraman et al. (2003) developed Integer Programs (IPs) in which variables assume values only in the set ====. These IPs were inspired by the characterization of decomposable domains introduced by Kalai and Muller (1977); this type of IPs allowed (Sethuraman et al., 2003) to establish a one-to-one correspondence, on domains of antisymmetric preference orderings, between the set of feasible solutions of a binary IP and the set of ASWFs without ties.====Busetto et al. (2015) generalized the approach proposed by Sethuraman et al. (2003), specifying IPs called ternary IPs in that they are allowed to assume values in the set ====. These authors established a one-to-one correspondence between the set of feasible solutions of a ternary IP, which they called IP1, and the set of ASWFs with and without ties.====Here, we consider a reformulation of the Simple Majority Rule (SMR) in the framework of integer programming. We first restate, in terms of IP1, the integer programming version, provided by Sethuraman et al. (2003), of Theorem 1 in Sen (1966), which shows that, when the number of agents is odd, a necessary and sufficient condition for the SMR to be an ASWF is that it is defined on a domain which does not contain a Condorcet triple. This theorem characterizes the SMR as a nondictatorial ASWF without ties and this, in turn, permits us to show, as a straightforward consequence, that the domains which do not contain a Condercet triple are decomposable. Then, we use IP1 to state and prove our main result, which can be seen as an integer programming simplified version of Theorems 2, 3, and 4 in Inada (1969): when the number of agents is even, a necessary and sufficient condition for the SMR to be an ASWF is that it is defined on a domain which is echoic with antagonistic preferences. This theorem characterizes the SMR as a nondictatorial ASWF with ties. Therefore, we straightforwardly show that the domains which are echoic with antagonistic preferences are strictly decomposable. Finally, we show that the set of domains admitting an ASWF with ties based on the SMR is a strict subset of the set of domains admitting an ASWF without ties based on the SMR.====The paper is organized as follows. In Section 2, we introduce the notation and the basic definitions. In Section 3, we use IP1 to prove a new theorem which characterizes the SMR as a nondictatorial ASWF with ties when the number of agents is even. In Section 4, we draw some conclusions.",Simple majority rule and integer programming,https://www.sciencedirect.com/science/article/pii/S016548962100069X,18 July 2021,2021,Research Article,89.0
Klunover Doron,"Department of Economics, Ariel University, 40700 Ariel, Israel","Received 10 April 2021, Revised 8 July 2021, Accepted 9 July 2021, Available online 16 July 2021, Version of Record 24 July 2021.",https://doi.org/10.1016/j.mathsocsci.2021.07.002,Cited by (2),"What are the chances of an ethical individual rising through the ranks of a political party or a corporation in the presence of unethical peers? To answer this question, I consider a four-player two-stage elimination tournament, in which players are partitioned into those who are willing to sabotage other players’ efforts and those who are not. I show that, under certain conditions, the latter are more likely to win the tournament.","It has been acknowledged that office politics is present in almost every organization and that it often dramatically decreases the efficiency of promotion tournaments among peers (Carpenter et al., 2010). Nevertheless, it is evident that some individuals avoid certain types of unethical behavior, such as revealing confidential information about co-workers or sabotaging their work. For instance, in an experiment conducted by Charness et al. (2014), only about 50% of the participants would exploit the opportunity to reduce the performance of their co-workers, even though it was the equilibrium strategy for a status-seeking individual.==== This may be the result of the individual’s moral values, the emotional cost of engaging in such activity due to feelings of guilt and shame, etc.==== Since sabotage is unproductive and even destructive, it is socially desirable that an individual who refrains from sabotage be able to get ahead in an organization. But is that generally the situation? For example, what are the chances of a politician who is unwilling to sabotage her opponents’ campaign becoming the leader of a party? And what are the chances of an employee who is unwilling to sabotage her co-workers moving up the corporate ladder? In what follows, I will present a theoretical model to show that their chances of success are not necessarily less than those of their unethical colleagues. Note that theoretical models which predict sabotage behavior and its consequences appear to be of particular importance, since sabotage is usually unobservable and therefore difficult to detect empirically (as noted by Harbring and Irlenbusch, 2011).====I consider a two-stage elimination tournament with four risk-neutral contestants, in which two hawks (unethical individuals) and two doves (ethical individuals) compete in pairwise imperfectly discriminative contests. In the first stage of the tournament, each hawk competes against a dove. In the second, the two previous winners compete against each other for a single prize. In a given match, a hawk chooses a level of productive effort and a level of sabotage effort, which is deducted from the productive effort of her rival, while a dove chooses only a level of productive effort but does not engage in sabotage.====I show that if there is a risk of punishment for sabotage,==== then the probability that a dove wins the prize is greater than one-half in an interior subgame perfect equilibrium (SPE), or alternatively if there is only one dove and three hawks or one hawk and three doves, then ex-ante an individual dove has a higher probability of winning the tournament than an individual hawk. I focus on characterizing an interior SPE in pure strategies; however, such an equilibrium may not exist and there may or may not be other equilibria, an issue that is discussed later on.====The paper proceeds as follows. In the remainder of this section, we briefly review the most relevant literature. Section 2 presents the model while Section 3 presents the main result. Section 4 concludes the paper.====Lazear (1989) considers a single-stage tournament, in which both players can choose to engage in sabotage but a hawk faces a lower cost to engage in sabotage than a dove. In contrast, we consider a two-stage tournament and assume that the dove’s choice not to engage in sabotage is non-strategic and is due to her ethical or moral values. Specifically, the dove’s choice not to engage in sabotage is assumed to be exogenously, rather than endogenously, determined.====The closest-related paper is Amegashie and Runkel (2007) who consider a four-player two-stage elimination tournament with sabotage. In contrast to our model, they use the “all-pay auction”, which is a perfectly discriminative contest, and only prior to the tournament do they allow (all) contestants to sabotage potential rivals who participate in the parallel contest. They show that if in the first stage of the tournament the least-able player is matched against the second-most-able player, then the former has a probability of greater than one-half of winning in this match since the latter is sabotaged by the most-able player who participates in the parallel contest and prefers to compete with the weaker opponent in the finals. In our model, the dove is more aggressive than the hawk in the first stage, since she can expect a larger net payoff if she reaches the final stage. In both models, therefore, the results are driven by the effect of the players’ expected net payoff in the second stage on their efforts in the first stage. However, Amegashie and Runkel (2007) show that a player increases her expected net payoff in the final stage by sabotaging her potential rival while in our model this cannot be done. Furthermore, Amegashie and Runkel (2007)’s result links the winning probabilities of the players to their abilities while our model focuses on behavioral constraints. Amegashie and Runkel (2007) further show that under a different partition of players there exists an equilibrium in which players do not sabotage. In contrast, in our model the dove would have chosen to sabotage in the absence of any moral reasons not to.====Other dynamic tournaments with sabotage include Gürtler and Münster (2010) who consider a tournament in which three players with different initial positions compete against each other in two rounds. In each round, a player has two binary choices: whether or not to sabotage a rival and whether or not to invest productive effort. The player who is ahead of her rivals after round two is declared to be the winner. They found that the leading player is sabotaged in the second round, thus discouraging him from investing productive effort in the first round, which is consistent with results in the literature.====
 Gürtler et al. (2013) show that this problem can be solved if the choice of whether to invest in productive effort is made prior to the choice of whether to invest in sabotage effort. In contrast to these two models, we assume a continuous choice set of effort and a hierarchical tournament structure that includes two semi-finals and one final match.====There is a large literature on dynamic Tullock contests without sabotage. Baik and Lee (2000) consider a two-stage contest in which effort exerted in the first stage is carried over to the finals. Stein and Rapoport (2005) consider two-stage contests with budget constraints. Stracke (2013) compares between one-stage and two-stage contests when players are heterogeneous. Cohen et al. (2018) consider two-stage elimination contests with head starts.====The players in the current model who do not sabotage are motivated by reasons not necessarily related to monetary incentives and therefore the model can also be viewed as a contribution to the literature on the behavioral aspects of contests, such as the utility of winning, relative payoff maximization, bounded rationality, and judgmental biases. For a review of this literature, see Sheremeta (2015). For a comprehensive review of the contests literature, see Corchón and Serena (2018) and Fu and Wu (2019).",When sabotage fails,https://www.sciencedirect.com/science/article/pii/S0165489621000706,16 July 2021,2021,Research Article,90.0
"Wang Xingtang,Wang Leonard F.S.","Center for Cantonese Merchants Research, Guangdong University of Foreign Studies, Guangzhou, China,Wenlan School of Business, Zhongnan University of Economics and Law, 182# Nanhu Avenue, East Lake High-tech Development Zone, Wuhan 430073, Wuhan, China","Received 18 May 2020, Revised 28 May 2021, Accepted 23 June 2021, Available online 29 June 2021, Version of Record 14 July 2021.",https://doi.org/10.1016/j.mathsocsci.2021.06.001,Cited by (6), delegation while possessing higher profits under price competition ==== delegation.,"In modern enterprises, managerial delegation is often observed as a very common instrument in firms that face an oligopolistic market structure. The strategic delegation literature beginning with Vickers (1985), Fershtman and Judd (1987) and Sklivas (1987) is built on the observation that, under strategic interdependence, delegation of decision making and accompanying actions can serve as commitments that influence competitive interactions with rivals and lead to beneficial outcomes. The literature has been enriched by many studies, which examine the implications of strategic delegation on various issues.==== These mainly argue that, in the case of quantity competition, owners realize strategic advantage by inducing managers to be more aggressive in the product market. In equilibrium, each firm offers a sales-oriented incentive scheme and earns lower profit compared with that under standard quantity competition. In other words, owners face a prisoners’ dilemma type of situation while deciding on incentive schemes for managers, ending up with Pareto inferior outcomes. Fanti et al. (2017) showed that the degree of bargaining interacts with the extent of product differentiation in determining whether the subgame perfect Nash equilibrium is sales delegation or profit maximization, while in their 2017b study, they challenged the results of the ‘classical’ managerial delegation literature and showed that none of the previous results may hold when the owner negotiates managerial compensation with his manager.==== The fundamental assumption underlying this outcome is that owners cannot coordinate with each other before designing managerial delegation contracts.====Different firms produce products of different quality and serve consumers with different tastes; for example, the high-end suits of Italian Versace are sold to high-income people, while Japanese Uniqlo clothes are sold to low-to-middle income everyday consumers. What is the difference between Versace and Uniqlo when it comes to principal–agent strategies? In a context of vertical product differentiation, Barros and Grilo (2002) first analyzed the effect of delegation on quality levels assuming the realization of the random cost associated with the quality level is known, at no cost, by the firm or the agent that undertakes these activities. A firm faces an asymmetry of information since the owner cannot observe the realization of the random variable, which is the agent’s private information. When one firm delegates and the other does not, they find two equilibria that mimic the full information situation, and two equilibria which display quality levels for the delegating firm lower than the full information ones. When the delegation decision is endogenous, there are equilibrium configurations with zero, one and two delegating firms. Wang and Wang (2009) reexamined the equivalence of competition mode in a vertically differentiated product market with the relative performance delegation, and demonstrated the equivalence of product quality and social welfare in the delegation game, irrespective of modes of product competition. In addition, in a three-stage game of quality-delegation-quantity (or price), this showed that the delegation coefficient is different between high-quality and low-quality firms in an asymmetric vertical differentiated model, and a high-quality firm makes better use of the delegation than a low-quality firm. Benassi et al. (2016), in the framework of a vertically differentiated mixed-duopoly with uncovered market and zero costs, study the existence of a price equilibrium when a welfare-maximizing public firm producing low quality goods competes against a profit-maximizing private firm producing high quality goods. They show that a price equilibrium exists if the quality spectrum is wide enough vis-à-vis a measure of the convexity of the distribution of the consumers’ willingness to pay, and that such equilibrium is unique if this sufficient condition is tightened.====All the above papers do not consider the influence of managerial delegation in a vertical structure of production. When the firms produce products of differing quality, they may need to buy inputs from different suppliers. Versace and Uniqlo, for example, make clothes from different fabric suppliers. In this case, what impact will this have if the firms adopt the delegation strategy? As there is no response to this question in the existing literature, in this paper, we analyze the influence of the delegation of downstream firms that produce a varying quality of goods on the profits, consumer surplus and social welfare in a vertically related market. We find that whether it is the price competition or quantity competition, the delegation can enable the downstream firms to grab the profits of their input suppliers. A downstream firm that produces high-quality products earns more from suppliers than that producing low-quality products; however, the delegation of downstream firms brings about reduction of consumer surplus and social welfare.====Under different competitive modes, the comparison of profits is also an important issue in economics. Singh and Vives (1984) show that firms earn higher profits under quantity competition while social welfare and consumer surplus are higher under price than quantity competition with a linear demand structure. Investigation of the effects of both quantity and price competition on profits and social welfare in a theoretical vertical market has always been a concern of scholars (López and Naylor, 2004, López, 2007, Symeonidis, 2008, Mukherjee et al., 2012, Yoshida, 2018). In particular, Pal (2015) found that in the presence of positive network externalities in the product market, under relative performance-based delegation, Bertrand competition yields lower prices and profits than does Cournot competition. On the contrary, these rankings are completely reversed in the presence of negative network externalities. Differing from Pal (2015), who considered only a ==== market, we find that delegation adopted by downstream firms can reverse their profit comparison under different competitive modes in the ==== market; that is, the downstream firms have high profits under quantity competition ==== delegation, while the downstream firms have high profits under price competition ==== delegation.====This paper is organized as follows. Section 2 provides the case where the downstream firms compete on price; Section 3 provides the case where the downstream firms compete on quantity; and Section 4 concludes the paper.","Vertical product differentiation, managerial delegation and social welfare in a vertically-related market",https://www.sciencedirect.com/science/article/pii/S0165489621000676,29 June 2021,2021,Research Article,91.0
"Karni Edi,Zhou Nan","Department of Economics, Johns Hopkins University, United States of America,Johns Hopkins University, Advanced Academic Programs, United States of America","Received 20 November 2020, Revised 9 May 2021, Accepted 31 May 2021, Available online 10 June 2021, Version of Record 18 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.05.005,Cited by (1),"This paper axiomatizes the representations of weighted utility theory with incomplete preferences. These include the general multiple weighted utility representation as well as special cases of multiple utilities or multiple weights only. Some behavioral implications are explored in the context of a portfolio selection problem, which illustrates the potential applicability of such models to a range of problems.",None,Weighted utility theory with incomplete preferences,https://www.sciencedirect.com/science/article/pii/S0165489621000603,10 June 2021,2021,Research Article,92.0
"Camacho Franklin,Pino Pérez Ramón","School of Mathematical and Computational Sciences, Yachay Tech, Urcuquí, Ecuador,Departamento de Matemáticas, Facultad de Ciencias, Universidad de Los Andes, Mérida, Venezuela","Received 18 March 2020, Revised 17 January 2021, Accepted 20 May 2021, Available online 2 June 2021, Version of Record 12 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.05.003,Cited by (2),"We go deeper into the study of the Dominance Plausible Rule (DPR) model of preferences over acts due to Dubois et al. This model corresponds, in the qualitative framework, to Savage’s model. The DPR model relies on two purely ordinal primitives: (1) a preference over consequences and (2) a ","Given a set of states ==== (the possible states of the world) and a set of consequences ====, a policy (act) is a function ====. We denote by ==== the set of policies. In the Savagian model of Expected Utility, one has a probability ==== over the events (sets of states) and a utility function ==== and for classifying the policies, one proceeds via the expected value of the functions ==== where ==== is a policy. The best policies are those maximizing the expected value. Thus, given a utility function ==== over ==== and a probability function ==== over ====, we can define the ==== of ==== as====
 ==== (that is, the expected value of ====). Then, we classify the policies in the following way ====Savage (1954) proved that if the relation between the policies ==== satisfies some axioms, then there is a probability function ==== over the events of ==== and also a utility function ==== over ==== such that the equivalence (1) holds, ==== the decision relation ==== over the policies can be defined via the expected utility when that relation obeys certain rationality criteria.====As a matter of fact, this Savagian model can be used in a natural way for defining preferences between policies when the initial data are the probability measure ==== over events and the utility function ==== over consequences. However, when these numerical functions are not available, but instead we have only preferences between events and preferences between consequences (mere ordinal relations which mimic plausibility relations between events and an ordinal scale of utility) it is necessary to conceive other models for classifying policies in a systematic way.====In that case, there is a model which seems very natural: ==== (see Definition 1). This model was first proposed by Dubois et al. (1997) and deeply studied in Dubois et al. (2003a) (see also Fargier and Perny, 1999, Dubois et al., 2002, Dubois et al., 2003b).====One of the main contributions in the work of Dubois et al. is a characterization – in the Savage style – for the decision relation defined through the Dominance Plausible Rule (DPR) with the help of a preference relation over the consequences and a possibilistic relation over the events (more generally, a relation over the events obtained as intersection of a family of possibilistic relations). More precisely, the Dominance Plausible Rule (DPR) proposed by Dubois et al. (1997) (they call it ====), says that a policy ==== is better than a policy ==== if, and only if, the set of states where ==== dominates ==== is ==== than the set of states where ==== dominates ====. Let us see the formal definition:====The relation defined by (2) tries to capture, in the finite case and in a qualitative way, the definition of a relation given by (1). The relation ==== plays the role of a utility function and the relation ==== plays the role of a probability function. Actually, it establishes the plausibility between events.====A natural question is how to characterize decision relations ==== defined via ==== with plausibility relations which are more discriminant than the possibilistic relations. In a work in this direction (Camacho and Pino Pérez, 2011b), a first attempt to characterize these decision relations, when the plausibility relation is the leximax relation, was given. In this work, we complete this characterization.====Moreover, in the current work, we go further beyond and we give compact characterizations of the decision relations defined via ==== not only with leximax relations, but also with qualitative probability relations and even with probability relations.====It is worth noting that Savage’s Representation Theorem of the decision relations by expected utility can be seen as an answer to the question first evoked by Ramsey (1931) of what is a subjective probability. In an analogous manner, our characterization theorems can be interpreted as an answer to the question of which are the qualitative versions of a subjective qualitative probability, a subjective leximax relation and a subjective finite probability.====Another important point in defense of ==== is that, in virtue of our representation theorems, one can observe that the decision relations obtained using ==== have a good behavior, that is, they enjoy good rationality properties.====It is worth mentioning that our representation theorems are in the same style of Savage’s Theorem, but in the qualitative setting of ====. In this sense, our results are different from Lehmann’s work about generalized qualitative probabilities (Lehmann, 1996). We give postulates over the decision relation ==== in order to be able to characterize this relation by equation (2), when the plausibility relation ==== is a qualitative probability, or a leximax relation or even a finite probability measure.====One important tool in our work is the use of compact characterizations for the plausibility relations previously mentioned. In particular, we give new characterizations of qualitative probability relations and the leximax relations. For the probability relations we use the well known characterization of Kraft et al. (1959) and Scott (1964).====One of the difficulties with the decision relations defined via ==== is that they are not always transitive relations==== as the following example given by Dubois et al. (2002) shows. Note that this example has the structure of the Condorcet paradox.====One might wonder if taking the transitive closure could fix this problem. Unfortunately, as the previous example shows, the relation obtained with the transitive closure is trivial: all the acts are indistinguishable. Thus, this is not a good choice. Fortunately, for natural plausibility relations, we obtain that transitivity holds for the policies having two outputs. This fact is strongly exploited in our characterizations.====The qualitative nature of the ==== model forces the decisions obeying to this rule to be robust. Moreover the decision relations that we characterize with this rule have unique representations (see Theorem 5, Theorem 6, Theorem 7).====This work is organized in the following way: Section 2 is devoted to giving the main results of this work. In it, after explaining the general and abstract form of representation theorems herein considered, we find four subsections. Section 2.1 is devoted to presenting the plausibility relations considered in this work and their characterization. In particular, we give new compact characterizations for qualitative probability relations and leximax relations. In Section 2.2 we define the postulates for decision relations and give three new characterization theorems for the decision relations defined via ==== with three plausibility relations, namely: qualitative probability relations, probability relations and leximax relations. Section 2.3 is devoted to a discussion which contains an interpretation of postulates, a brief comparison with other models in which the set of states is finite and an analysis of the scope of the representation theorems. In Section 2.4 we give an example illustrating the behavior of the decision relations using ====. This work finishes with Section 3, in which we make some observations, raise some questions and point out some working perspectives. In order to facilitate the reading, we have written the proofs of our results in an appendix.",Decision-making through Dominance Plausible Rule: New characterizations,https://www.sciencedirect.com/science/article/pii/S0165489621000585,2 June 2021,2021,Research Article,93.0
"Gersbach Hans,Wickramage Kamali","Center of Economic Research at ETH Zurich, 8092 Zurich, Switzerland,CEPR, 8092 Zurich, Switzerland","Received 1 July 2019, Revised 15 March 2021, Accepted 10 May 2021, Available online 19 May 2021, Version of Record 21 August 2021.",https://doi.org/10.1016/j.mathsocsci.2021.05.002,Cited by (1),"We introduce ‘Balanced Voting’, a voting scheme tailored to fundamental societal decisions. It works as follows: Citizens may abstain from voting on a fundamental direction in a first stage. This guarantees the voting right in a second voting stage on the variants of the fundamental direction chosen in the first. All ‘losers’ from the first stage also obtain voting rights in the second stage, while ‘winners’ do not. We develop a model with two fundamental directions and variants of these directions. Information about the preferences is private. We identify circumstances under which Balanced Voting performs well with regard to utilitarian welfare and Pareto dominance. We discuss the robustness of the results, procedural rules to implement the voting scheme, and extensions. Moreover, we provide several examples for which the scheme might be applied.","Polities are repeatedly confronted with the need to take decisions that are fundamental for a large group of citizens. Decisions such as exiting nuclear power, reversing the course in public indebtedness, enacting comprehensive labor market reforms or joining the European Union have a large and long-lasting impact on the direction a society takes. When a democratic society takes such decisions, some subgroups strongly favor or oppose a fundamental new direction, while others may care much less.====Typically, democratic societies use simple majority voting to take such fundamental decisions. Yet, it is well-known that this rule does not allow agents to express the ==== of their preferences. This may be particularly problematic for fundamental decisions. The majority overrules the minority, even if this minority is much more concerned about the fundamental direction than the majority. Allowing the minority to veto the resulting decision might mitigate the problem, but is also problematic: The protection of the minority would then turn into tyranny of the minority.====In this paper, we introduce a scheme, that we call ‘Balanced Voting’. It is particularly suited for taking fundamental decisions and aims at striking a balance between the intensity of preferences and the protection of minorities. We will compare Balanced Voting with several existing voting schemes in a collective-decision problem involving two related decisions. We will identify circumstances under which Balanced Voting performs better than other voting schemes and examples for which the scheme could be applied.====The idea underlying Balanced Voting is conveniently explained in a two-stage set-up. Suppose a society or a committee of individuals votes on two related binary decisions. The first decision determines the fundamental direction, which is assumed to be irreversible. An example is the decision whether to use nuclear power for energy production or discontinuing its use. The second decision establishes the variant or the way in which the choice of the fundamental direction should be realized. For example, either investing in new nuclear power plants or improving the safety and efficiency of existing ones are possible variants, if the fundamental decision was to keep using nuclear power. Typical non-nuclear variants are the investment in more solar power, hydro power or the building of new coal or gas power stations.====Under Balanced Voting, agents have the option of either voting for the fundamental direction or of abstaining in this first stage. Those who abstain ‘save’ their voting rights for the second stage. The agents who are ‘losers’ in the first voting also obtain voting rights for the second stage, while the ‘winners’ in the first stage are not allowed to vote in the second stage.====Balanced Voting thereby allows individuals who do not feel strongly about the fundamental decision to trade off their voting rights in the first stage for a guaranteed voting right in the second stage. Thus, individuals who are only weakly-inclined towards a particular fundamental direction have the opportunity to exert their influence on this second-stage decision. This allows, for instance, strong advocates or opponents of nuclear power to have more influence on the first-stage decision, i.e. to have a stronger say on their ‘favorite’ decision. Those agents who voted for a fundamental direction, but belong to the ‘losers’, are compensated by the right to vote in the second stage. Hence, if nuclear power is chosen in the first stage, the opponents of nuclear power will be in a better position to limit the number of nuclear power plants to be built in the future and/or to impose higher safety and efficiency standards on existing plants. Similarly, if the decision to discontinue nuclear power is taken in the first stage, strong proponents of nuclear power will have a better chance of selecting the one non-nuclear alternative they prefer.====Procedurally, Balanced Voting can be applied to any sequential collective voting problem. In this paper, we focus on its application to two consecutive collective decisions in which the first decision is much more important for a subgroup of the society than the second one. In addition to the example of nuclear power already mentioned above we will provide five examples for which Balanced Voting could be applied. In one case, the ==== of Balanced Voting is applied implicitly.====It may be useful to illustrate the working of Balanced Voting in an example of everyday life. Consider a group of five individuals who spend the weekend together. They first vote on whether to go to France or Italy and then on a bicycle tour versus hiking in the chosen country.====A voting process under Balanced Voting with five individuals A, B, C, D, E could look as follows:====Balanced Voting allows voters A and B to choose to have their say on the country choice, since this seems more important for them than the second choice. The loser C from the first round obtains his preferred choice in the second ballot, at least. C is the only person who can vote twice. D and E, who care about the tour, abstain in the first ballot and one of them, D, obtains his preferred tour. E is the one who really loses.====Of course many other examples of voting result patterns for such set-ups can be constructed under Balanced Voting. The second example has two parliamentary decisions on the size of the budget deficit and the composition of expenditures. Often a parliament decides first on a debt ceiling – as in the US for instance – and only then on the mix of expenditures. Suppose that in the first stage the decision is whether public debt should be increased or not. If the parliament decides to increase public debt, the second decision could involve determining the mix of expenditures for this additional funding. If the parliament decides not to increase the debt level, the second decision could involve the choice about the mix of tax increases or expenditure cuts to meet the debt ceiling.====The third example is a recent vote in Switzerland on the limitation of second homes in the Alps. The issue at hand was whether there should be a limitation on the share of second homes. The popular vote on the corresponding constitutional initiative in 2012==== has been accepted, but the precise implementation was left to a subsequent decision as to what extent the laws should specify, whether the ==== second homes had to be stopped or whether they could be completed. The alpine cantons cared a lot about such a limitation as they feared an economic-downturn through a collapse of their construction industry. In non-alpine cantons one subgroup cared a lot about imposing the limitation since they favored untouched landscapes. After the voting of the citizenry of the entire country decided to limit second homes, the parliament invited the alpine cantons to help craft the corresponding legislation and to draft a variant that was less painful for them. This closely follows the spirit of Balanced Voting as in the second decision, strongly affected-losers and weakly-inclined individuals were instrumental in specifying the precise implementation of the first decision.====The fourth example concerns the application of Balanced Voting to elections and suggests a balanced election procedure that might even be used for US presidential elections. Suppose that in a majority rule political system with two parties, an election for a public office takes place. Then, in a first election, citizens could decide to which party the office-holder should belong. In a second vote, an office-holder would be selected from a set of candidates named by the party who won the first vote.==== In such circumstances, under Balanced Voting, citizens would need to choose whether to vote or abstain in the first election, when the winning party is selected.====Fifth, a particularly striking example for which the scheme could have been applied is Brexit. Leaving the EU is a fundamental change of direction. Several agreements following such a change were conceivable, ranging from no-deal (and thus, WTO rules apply), free trade arrangements, and customs union, up to a soft Brexit, with retained membership in the European Single Market. Outlining these variants when the Brexit decision was taken and using Balanced Voting would have been a possible application of our scheme. It is, of course, difficult to assess how this would have changed the outcome and whether a softer version of Brexit would be implemented.====Finally, one might also apply Balanced Voting in court decisions taken by a group of judges, with the first vote indicating the basic ruling and the second vote the specific application.====A note of caution is in order. A voting scheme such as Balanced Voting implies that some voters will end up with more voting rights than others. Who will vote more than others will be determined through the voting behaviour and the outcome of the first round. Hence, while ex-ante the voting scheme satisfies anonimity and treats everybody the same way, one may object to such voting schemes on principles grounds because not all voters will end up with the same voting rights. This is an important objection and only future research will tell whether Balanced Voting can indeed be proposed for real world applications.====We will examine Balanced Voting for a particular scenario in which individuals have either high stakes or low stakes in the first decision. The first group cares most about the first collective decision: for instance, people who are sufficiently afraid of nuclear power may wish to exit from this source of electrical power. For individuals in the second group, the preferences on the use of nuclear energy might be conditional on the type of energy that is used instead of nuclear, say solar, wind or coal, for instance. Similarly, a fraction of the electorate may care most about whether a Democrat or a Republican is in the White House, while for another part of the electorate, the preference mostly depends on the person elected. We show that in equilibrium, individuals with strong preferences participate in the first voting stage, while those who are weakly-inclined abstain, provided that the stakes of strongly-inclined individuals are sufficiently high and those of weakly-inclined individuals are sufficiently low. We identify the circumstances under which Balanced Voting performs well with regard to utilitarian welfare and Pareto dominance. These circumstances involve large utility differences between strongly-inclined and weakly-inclined individuals towards the fundamental direction, a sizeable group of individuals that cares strongly about the fundamental direction, and a sufficiently large voting body.====Several innovative voting schemes that lead to higher social welfare have been developed in recent years. In the Storable Votes scheme, developed by Casella (2005), agents can either use their vote in the current decision or store it for future use. This allows agents to concentrate their votes on decisions dealing with issues over which they have intense preferences. Hortala-Vallve (2012) introduced Qualitative Voting, in which each individual is granted a stock of votes that can be freely distributed over a series of binary choices, whereby all votes are cast simultaneously. The Minority Voting scheme introduced by Fahrenberger and Gersbach (2010) focuses on protecting individuals from the accumulation of utility losses over time if they wind up in the minority repeatedly. In a two-stage voting procedure, Minority Voting gives the losers in the first stage exclusive voting rights in the second stage. All these voting schemes link ==== voting decisions. Jackson and Sonnenschein (2007) show that in general, linking repeated voting decisions leads to welfare gains, and that it is possible to achieve full efficiency as the number of decisions grows large.====Other mechanisms that attempt to capture agents’ preference intensity are Cumulative Voting and Vote-trading. In Cumulative Voting, individuals are endowed with a stock of votes they can freely distribute over the candidates standing for election, whereby allocating more than one to a candidate is allowed (see Sawyer and MacRae (1962), Gerber et al. (1998) and Cox (1990)). Under Vote-trading, an agent is permitted to hand over his voting right to another, in return for monetary compensation (see, for example, Coleman (1966) and Philipson and Snyder (1996)).====Balanced Voting uses the ideas from Storable Votes and Minority Voting. It is tailored to fundamental societal decisions and examined under incomplete information. Consequently, Balanced Voting not only allows agents to use their vote on issues about which they feel strongly, but it also provides better protection to minorities with strong preferences, in keeping with the argument of Guinier (1994) that such minorities indeed ==== some special protection.====Our paper is related to the proportionality principle which requires that in collective decision-making processes, the power should be proportional to individual stakes (Brighouse and Fleurbaey, 2010). Under Balanced Voting, people having high stakes on one issue are selected endogenously into the group of people that takes the decision, which fosters proportionality.====The rest of the paper is organized as follows: Section 2 describes the model. Section 3 characterizes the equilibria under Balanced Voting, simple majority voting, Storable Votes and Minority Voting. In Section 5, we compare social welfare under Balanced Voting with social welfare under simple majority voting, Storable Votes and Minority Voting, and we present our main result. In Section 6, we conclude the paper. Appendix A outlines detailed calculations of key expressions. In Appendix B, we provide the proofs. Appendix C contains detailed welfare comparisons for the main theorem.",Balanced voting,https://www.sciencedirect.com/science/article/pii/S0165489621000573,19 May 2021,2021,Research Article,94.0
"El-Jahel Lina,MacCulloch Robert","Department of Accounting and Finance, Faculty of Business and Economics, The University of Auckland, 12 Grafton Road, Auckland, 1010, New Zealand,Department of Economics, Faculty of Business and Economics, The University of Auckland, 12 Grafton Road, Auckland, 1010, New Zealand","Received 30 April 2020, Revised 3 May 2021, Accepted 5 May 2021, Available online 13 May 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.05.001,Cited by (0),"High predictability of returns on housing suggests pervasive irrationality on the part of market participants. In this paper, we show that with small transactions costs, returns may remain highly predictable even if the market contains substantial numbers of risk neutral, rational speculators. The reason is that ","House prices are generally thought to exhibit substantial informational inefficiencies. In a well-known study, Case and Shiller (1989) conclude that “There is substantial persistence through time in the rates of change in indexes of real house prices” due to annual changes in log house prices being followed the next year by changes in the same direction between 1/4 and 1/2 as large.==== Cutler et al. (1991) find that at a lower frequency the divergence between market and fundamental value reverts and house price returns become negatively serially correlated over longer periods. The reason is that “erroneous” market moves appear to be eventually corrected. The topic of predictability and persistence in house price movements remains an empirically relevant issue as shown, for example, by Serrano and Hoesli (2010) and Schindler (2011).====In order to explain the positive auto-correlation in house prices, Caplin and Leahy (2011) provide a search model in which uncertainty is generated by random shocks to supply and demand.  Guren (2013) also develops a search model but with concave demand. This helps amplify price sensitivities to changes in fundamental value, hence generating prolonged periods of autocorrelated house prices. Corradin et al. (2014) solve an optimal portfolio choice problem with transaction costs and predictability in house prices. In their work, house price dynamics follow a geometric Brownian motion where the mean growth rate could be in three possible regimes (high, medium and low) determined by a first order Markov chain. Their solution reduces to solving a number of ordinary differential equations determined by the number of states they consider. They extend both the theoretical and empirical literature on investment decision problems and build on the results of Grossman and Laroque (1990). The focus of their paper is the portfolio choice between risky and housing assets when transaction costs make housing consumption lumpy. In their model, agents’ decisions depend on the wealth to housing ratio and the time varying expected growth rate of house prices.====Our paper presents a simple alternative approach based on the literature of investment under uncertainty and the results of Leahy (1993). We consider two types of investors in the housing market: arbitrageurs (or rational speculators) and liquidity traders.==== Speculators value houses at their fundamental value whereas liquidity traders value houses at their market price. The focus of our approach is on the dynamic modelling of house price movements and the impact of transaction costs on the decisions of speculators entering or exiting the housing market. We show that small transactions costs dramatically reduce the influence of speculators in eliminating informational inefficiencies. The reason is that transactions costs create option values that delay optimal arbitrage transactions until the sale or purchase price of a house diverges sufficiently from the total expected, discounted service flow that represents its fundamental value.====Our contributions to the literature are along the following lines. First, our framework uses continuous time processes to model the dynamics of the housing market. The market price, ====, and the fundamental value of a house, ====, are continuous-time correlated diffusion processes and the market house price mean reverts towards its fundamental value. The growth rate of the house price therefore depends on its fundamental value. In this setup the solution of the Hamilton–Bellman–Jacobi equation on the inaction region, subject to value matching and smooth pasting conditions holding at the bounds, reduces to solving one ordinary differential equation. This allows us to generate rich scenarios whilst maintaining mathematical tractability.====Second, we use results from financial option theory and view the optimal decision of a speculator as similar to holding an option either to sell or buy a house depending on whether one owns a house or not, respectively. Valuing these options relates our work to the investment under uncertainty literature on entry and exit into an industry. In our model the optimal arbitrage strategy results in the valuation of those options for a risk neutral speculator and the optimal trigger points at which transactions take place, thereby allowing us to track the effect of transaction costs on house prices in a simple manner.====Third, we show how the solution may be used to construct a full, market equilibrium in which speculators and liquidity traders buy and sell houses. Speculators transact when the ratio of the fundamental value to the market price deviates sufficiently, buying when the ratio is high and selling when it is low. The methods we use to derive a market equilibrium resemble those employed by Leahy (1993) and Fries et al. (1997) in their studies of firms entering and exiting an industry. Our equilibrium is characterized by a regulated process, ====, at two optimal trigger points, leading to considerable predictability in house prices even in the presence of small transaction costs. Speculators time their individual transactions optimally and are indifferent to the aggregate outcome of their decisions.==== Depending on the housing stock and demand by speculators, this equilibrium could lead to situations where regulation may fail at one of the triggers, leading to periods of boom or bust in which the market price could deviate substantially from the fundamental value. The surprising result in our model is not that some participants in the housing market act irrationally, but that fully rational agents may not dominate in the determination of house prices.====Finally, we conduct Monte Carlo simulations using reasonable parameter values and show that our model can replicate the time series properties of house prices identified in past empirical work. We compare our results, in particular, with those of Case and Shiller (1990). The magnitude of the correlations we obtain is very similar to that found in the above paper which uses house price data from a group of American cities. We can also generate the particular pattern of correlation found by these authors, namely one of short-term positive auto-correlation and long-term negative auto-correlation. Cutler et al. (1991) have argued that this pattern is typical of returns in a number of different speculative markets.====The structure of the paper is as follows. In Section 2 we present the model, solve for the optimal arbitrage decision of a speculator in the housing market and derive the market equilibrium. In Section 3 we perform Monte Carlo simulations of our model to show its ability to replicate the statistical properties of observed data. Section 4 concludes the paper while an appendix provides detailed proofs of the propositions.",Trading in the housing market: A model with transaction costs,https://www.sciencedirect.com/science/article/pii/S0165489621000561,13 May 2021,2021,Research Article,95.0
"Han Weibin,van Deemen Adrian","School of Economics and Management, South China Normal University, Guangzhou Higher Education Mega Center, No. 378, Waihuan Xi Road, Guangzhou 510006, Guangdong, People’s Republic of China","Received 16 September 2020, Revised 23 April 2021, Accepted 27 April 2021, Available online 7 May 2021, Version of Record 21 May 2021.",https://doi.org/10.1016/j.mathsocsci.2021.04.009,Cited by (0),"In this paper, we review some results of the solution of generalized stable sets introduced by Van Deemen (1991) as a variant of stable sets for abstract decision problems. This solution will be investigated and reviewed for the more general case of irreflexive but not necessarily asymmetric or complete dominance relations. It is proven that the fundamental properties of this solution are preserved also for this kind of dominance relations. Two main shortcomings of the solution of generalized stable sets are firstly that it may contain Pareto-suboptimal alternatives when the dominance relation is derived from pairwise majority comparison, and secondly that it may fail to discriminate among the alternatives under consideration when the dominance relation is a Hamilton cycle, i.e. a cycle that includes all alternatives. A refinement of generalized stable sets is proposed in order to address these two shortcomings. This refinement is called the solution of undisturbed generalized stable sets. It will be shown that undisturbed generalized stable sets are ==== and have discriminating power in the case of Hamilton cycles. In addition, and perhaps more important, we prove that this refinement is always a subset of the solutions of the uncovered set and of the unsurpassed set. This implies that undisturbed generalized stable set also may be seen as a refinement of both the uncovered set and the unsurpassed set.","In this paper, an abstract decision problem is presented by an ordered pair ==== where ==== is a set of conceivable alternatives from which a selection has to be made by an individual or a collectivity, and ==== is an irreflexive dominance relation over ====. We read ==== as ==== dominates ==== for any ==== in ====. We do not imposeasymmetry or completeness on ====. In general, a dominance relation may be interpreted in several ways. For instance, it may be seen as a social preference of some collectivity or as a relation over a set of imputations in a cooperative game. We will leave aside the question how a dominance relation is formed, and simply assume that it is an exogenous variable. Many decision problems can be modelled as such a pair. Examples are tournaments==== and majority voting problems in Mello et al. (2005) and Laslier (1997), coalition formation problems in Van Deemen (2013) and Dutta and Jackson (2013), page rank problems in Brandt and Fischer (2007), multi-criteria decision problems in Arrow and Raynaud (1986) and Figueira et al. (2005), exchange market problems with indivisible goods in Subiza and Peris (2014) and finite coalitional games with non-transferable utility==== in Brandt and Harrenstein (2010).====A solution for an abstract decision problem is a function which assigns a single alternative or a subset of alternatives to an abstract decision problem. Since a dominance relation is associated with the evaluation of alternatives, it is logical to use as much information contained in this relation as possible to solve abstract decision problems. Here, we consider exclusively such so-called dominance-based solutions. If there exists an alternative that dominates any other alternative and if it is not dominated by any other alternative, referred to as a ====, then such an alternative should be chosen certainly. The key point is that a dominance relation might be intransitive or cyclic, or even incomplete. In this case, a best alternative may not exist. The central question therefore is which alternatives should be selected on the basis of a dominance relation when a best alternative does not exist. The main objective of this paper is to answer this question by revising some dominance-based solutions in the literature and by proposing a refinement of the generalized stable sets on the basis of the information contained in a dominance relation.====Two prestigious notions were proposed to solve abstract decision problems. One is the core concept introduced by Gillies (1959). The core contains un-dominated alternatives. The rationality behind this notion is that once an alternative ==== in the core set is proposed, this alternative is the maximal element in the dominance relation and no other alternative can block the acceptance of ====. The other notion is the solution of stable sets proposed by Von Neumann and Morgenstern (1944). A stable set specifies a subset of alternatives ==== that satisfies the properties: no alternative in ==== dominates another alternative in ====; any alternative ==== outside ==== can be dominated by an alternative ==== in ====. It can be verified that the core always exists, but might be empty, and that a stable set on the other hand may not exist at all. As for the existence conditions for the core and for stable sets, see Peris and Subiza (1994) and Lucas (1992), respectively, for an excellent review of the state of knowledge at that time. For this reason, a series of more general solutions considered as variations either of the core or of stable set were formulated to deal with every possible abstract decision problem. See Laslier (1997) and Brandt et al. (2016) for variations of the core in details. See Nicolas (2009), Peris and Subiza (2013) and Han et al., 2016, Han and Van Deemen, 2016 for reformulations of stable set solution. So far, existing research after these variations restrictively assume that the dominance relation is either complete or asymmetric. Whenever the dominance relation is relaxed to incomplete or not necessarily asymmetric situation, the existing variations usually cannot maintain its original nature and characteristics.====In this paper, we would like to review and extend our knowledge about generalized stable sets which was proposed byVan Deemen (1991) as a variation of stable set for abstract decision problems with nonempty asymmetric dominance relations. Here, we first extend this notion to irreflexive dominance relations which may be neither asymmetric nor complete. We compare our extension with the one proposed in Van Deemen (1991), and then show that our extension will maintain the original characterizations of generalized stable sets while the proposed extension in Van Deemen (1991) does not. One shortcoming of the collection of generalized stable sets is that it may contain every single alternative as a generalized stable set. Besides, in contrast to for example the solution of the uncovered set, generalized stable sets may contain Pareto suboptimal alternatives when the dominance relation is derived from pairwise majority comparison. For these reasons, we propose a refinement of the solution of generalized stable sets that addresses both problems. We first define a relation over the collection of generalized stable sets by using additional information contained in the dominance relation, and then maximize this relation. We do this by preserving the basic notion and properties of the solution of generalized stable sets. The defined relation over the collection of generalized stable sets will be called a disturbing relation, and the maximal set of this relation will be called undisturbed generalized stable sets. It will be proven that undisturbed generalized stable sets are always nonempty and Pareto optimal whenever a dominance relation is derived from pairwise majority comparison. In addition, it will be shown that the collection of undisturbed generalized stable sets always rules out some generalized stable sets as long as the dominance relation is not regular. Besides, we will show that the solution of undisturbed generalized stable sets refines both the solutions of the uncovered set in Miller (1980) and of the unsurpassed set in Han and Van Deemen (2019).====The rest of this paper is organized as follows. In Section 2, we present necessary definitions and notations concerning alternatives and dominance relations to formulate a general analysis framework for abstract decision problems. We also present and discuss the notions of the core, of stable sets, of the admissible set, of the uncovered set and of the unsurpassed set to show our research motivations in details. In Section 3, we mainly focus on the solution of generalized stable sets. We apply this solution to irreflexive dominance relations, and investigate whether its original characterizations and properties can be preserved. Section 4 is devoted to refining the collection of generalized stable sets. First, we define a disturbing relation over generalized stable sets, propose the notion of undisturbed generalized stable sets, and then reveal the rationality of this proposal by comparing it with other dominance-based solutions introduced in the previous section, in particular the uncovered set and the unsurpassed set. Finally, in Section 5 we end this paper with a discussion of the results.",The solution of generalized stable sets and its refinement,https://www.sciencedirect.com/science/article/pii/S0165489621000470,7 May 2021,2021,Research Article,96.0
"Grandmont Jean-Michel,Seegmuller Thomas,Venditti Alain","CREST-CNRS, France,American Academy of Arts and Sciences, United States of America,Aix-Marseille Univ., CNRS, AMSE, France,EDHEC Business School, France","Available online 4 May 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.003,Cited by (1),This paper is a tribute for Carine Nourry for this special issue of Mathematical Social Sciences.,None,Remembering Carine Nourry,https://www.sciencedirect.com/science/article/pii/S0165489621000238,4 May 2021,2021,Research Article,97.0
"Boitier Vincent,Auvray Emmanuel","Le Mans University, France","Received 28 May 2020, Revised 31 October 2020, Accepted 12 April 2021, Available online 3 May 2021, Version of Record 15 May 2021.",https://doi.org/10.1016/j.mathsocsci.2021.04.004,Cited by (0),"In this theoretical article, we develop a unified framework that encapsulates: i) — a system of heterogeneous cities, ii) — heterogeneous workers, iii) — the presence of ","How does the assignment of heterogeneous agents across heterogeneous cities operate? Can social mixing constitute a unique and stable spatial equilibrium? Can individuals’ preferences be overturned at an aggregate scale? If so, how to rationalize these paradoxical results à la Schelling? What economic factors drive within- and across-city inequalities? Are large towns more unequal than small ones? How to explain the incomplete sorting between attractiveness of sites, firms, and workers? Is a spatial equilibrium optimal?==== If not, how to restore efficiency?====We attempt to address these questions in a comprehensive and unified framework. Toward that goal, we build a system of cities with the following key ingredients. The number of cities is finite, and cities are heterogeneous according to a level of amenities. In each city there is a monocentric city with a continuum of locations. Absentee landlords own houses that are supplied by a competitive market. Firms are exogenously located in the city center, do not consume any space, and reward employees for their work. The system is also populated by an infinite number of workers distributed across two populations (rich vs. poor, black vs. white, etc.). Workers are heterogeneous in terms of income and transportation costs. They also have heterogeneous preferences regarding city composition, as in Miyao, 1978, Miyao, 1979. This means that workers are engaged with (possibly complex) intra- and inter-group externalities.====In such a setting land is not allocated by the bid rent theory, and in the labor market wages are exogenously determined. A spatial equilibrium is a vector of spatial distributions that verifies a variational inequality. This characterization allows us to derive reasonable conditions for uniqueness and (static) stability of a spatial equilibrium (see Boitier (2020)). The within- and across-city income that causes a social dispersion is summarized by a single statistics corresponding to a dissimilarity index.====From this new setting we derive valuable results. First, we derive closed-form solutions for the endogenous variables. The spatial distributions of workers and the dissimilarity index are analytical in the decentralized and centralized economies. Having closed-form solutions is crucial and advantageous. This enables conducting a detailed and robust comparative statistical analysis. This also allows a transparent identification of the driving forces of the workers’ location. Due to its tractability, the model can also be a guide for empirical studies by helping to derive testable predictions that link the obtained dissimilarity index to measurable variables.====Second, we demonstrate that, contrary to conventional wisdom, social mixing (i.e. incomplete sorting) can be a unique and stable spatial equilibrium. Moreover, we advocate that this configuration prevails under a large set of plausible parameter restrictions. This means that the situation frequently emerges as an outcome that reconciles the spatial theory with empirical evidence.====Third, we unveil the determinants of within- and across-city inequalities. We also underline that social mixing is a consequence of three distinguishable components. The first component is a demographic component. It posits that social mixing is improved when a population of workers grows faster than the other. The second factor captures the degree of differences between cities in terms of attractiveness. This component predicts that a system of cities composed of similar (different) amenable cities exhibits low (high) social mixing. The third factor is the combination of income inequalities, transportation costs, and city composition preferences. It clearly shows that there is a relationship between segregation by income and segregation by preferences. In particular, depending on the intra- and inter-group externalities, city composition preferences can mitigate (magnify) income segregation.====Fourth, we find that macroeconomic patterns can deviate from microeconomic preferences. Notably, a society in which there is a desire for a mixed environment generates less social mixing than a society in which there is a preference for a segregated environment. In so doing, we obtain results in line with Schelling, 1971, Schelling, 1978. We then offer a rationale for these counterintuitive findings. We identify city composition preference acts as agglomeration economies and congestion effects in the urban model. In summary, the paradoxical results à la Schelling are intuitively explained by the standard forces present in urban economics.====Fifth, we show that the decentralized economy never achieves an optimal allocation. This is because workers disregard some local externalities compared to what the social planner internalizes. In addition, we investigate two different aspects of inefficiency. On the one hand, we ask whether cities are underpopulated or overpopulated. We highlight that this depends on the nature of the city composition preferences. When these preferences are symmetric, large towns are oversized and small cities are undersized. If the suitable degree of asymmetry is introduced, the converse situation prevails. Large towns are too small and small cities are too big. This counterintuitive result concurs with Albouy et al. (2019). On the other hand, we determine whether social mixing is too high or too low in the system of cities. Again, the answer relies on the feature of city composition preferences.====Last, we stress that economists have a remedy to restore optimality. To eliminate inefficiency, local subsidies are sufficient. These are standard instruments in public economics. We show that local actors can introduce local subsidies. If local subsidies are implemented by competitive local land developers, inefficiency vanishes. This concurs with the results of Henderson and Becker (2000) and Albouy et al. (2019). Local subsidies can also be managed by a central government.====This article contributes to urban economics from different perspectives. It improves the design of spatial models. This is the first attempt to take into account heterogeneous sites, heterogeneous workers, and city composition preferences. Behrens and Robert-Nicoud (2015b) and Albouy et al. (2019) consider a system of cities with heterogeneous sites and a continuum of heterogeneous workers. But there are no neighborhood composition preferences. In addition, to derive results they need to assume that land rent is independent of the workers’ composition of the city. These drawbacks do not exist in our framework. Land rent depends on the composition of the city. In Schelling models of segregation (see Zhang, 2004a, Zhang, 2004b, Pancs and Vriend, 2007, O’Sullivan, 2009, Zhang, 2011, Grauwin et al., 2012 and Boustan (2013)), social preferences are operative but economic factors are absent. Labor market is neutralized. Transportation costs are not integrated. Land is not allocated according to the bid rent theory.====The present article states that a mixed configuration can be unique and stable. This finding is at odds with the literature. In the Schelling model of segregation it has been well advocated that segregation is the only stable equilibrium. In the case of systems of cities (see Abdel-Rahman and Anas, 2004, Seegert, 2011, Behrens and Robert-Nicoud, 2015a and Albouy et al. (2019)), the literature predicts a strict sorting of workers, and there is no social mixing. However, this finding also accords with empirical evidence. It is very uncommon to observe a fully segregated system of cities in the data.====The article also offers paradoxical results à la Schelling. Notably, attractive (adverse) social preferences can generate less (more) social mixing. In fact, the framework provides a reason for these findings. It highlights standard agglomeration and congestion forces as credible rationales. This contrasts with the literature. In the standard Schelling model, social relationships are too bulky, precluding the possibility of obtaining closed-form results. The model is like a black box, and no robust comparative statistical analysis may be available. Consequently, it is difficult to establish what drives paradoxical findings in these standard models. More globally, the fact that our setting can be analytically solved is important. This permits a transparent identification of the driving forces of the workers’ location. This allows disentangling the respective roles of demographic, spatial (natural amenities/disamenities), and economic factors. This also enables guiding empirical studies.====The article joins the debate of policy regulation. Particularly, at which level must an optimal policy be implemented? In standard systems of cities, efficiency is restored at the city-scale with the concurrence of local land developers. Such a policy is possible in our framework. Optimality can also be achieved by local subsidies that are managed by a non-local government.====To conclude, the article complements some results found in the literature. As in Boitier and Auvray (2020), there is a relationship between segregation by income and segregation by preference, and social preferences can mitigate income segregation. In line with Albouy et al. (2019), large cities can be undersized, whereas small cities can be overpopulated.====The article is organized as follows. Section 2 presents the new framework. Section 3 provides the conclusions.",Schelling paradox in a system of cities,https://www.sciencedirect.com/science/article/pii/S0165489621000421,3 May 2021,2021,Research Article,98.0
"Boucekkine Raouf,Desbordes Rodolphe,Melindi-Ghidi Paolo","Aix-Marseille University, CNRS, EHESS, Centrale Marseille, and AMSE, France,IRES, UCLouvain, Belgium,SKEMA Business School - Université Côte d’Azur, France,EconomiX, Paris-Nanterre University, CNRS, France","Received 17 September 2020, Revised 14 February 2021, Accepted 24 March 2021, Available online 29 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.007,Cited by (2),"Elite-biased democracies are those democracies in which former political incumbents and their allies coordinate to impose part of the autocratic institutional rules in the new political regime. We document that this type of democratic transition is much more prevalent than the emergence of pure (popular) democracies in which the majority decides the new institutional rules. We then develop a theoretical model explaining how an elite-biased democracy may arise in an initially autocratic country. To this end, we extend the benchmark political transition model of ==== along two essential directions. First, population is split into majority ==== minority groups under the initial autocratic regime. Second, the minority is an insider as it benefits from a more favourable redistribution by the autocrat. We derive conditions under which elite-biased democracies emerge and characterise them, in particular with respect to pure democracies.","The transition from autocracy to democracy can take different shapes. Democracies can emerge as the result of popular uprisings, both violent and non-violent, but also from a change of the institutional rules by the elites in power. The main difference between these two types of transitions towards democracy is that the former generates new institutional rules, while the latter tends to inherit rules from the previous regime. While the political economy literature, notably the theoretical strand, is rather silent on the emergence of the second type of transition, political scientists have paid more attention to it (see, for instance, Munck, 2011, Biekart, 2015). Albertus and Menaldo (2018) point out a puzzling stylised fact: a significant fraction of democracies are substantially departing from what is usually associated with the very concept of democracy, e.g. inclusiveness, income redistribution from the rich to the poor, strong welfare states.====According to Albertus and Menaldo, this anomaly can be traced back to the specific role of minorities in the democratic transition. Minorities may be partners of the autocratic regime (====) and, under revolutionary threat, both political actors can form a coalition resulting in an ‘elite-biased’ democracy. Minorities may also be ==== under the initial authoritarian regime and form a coalition with a rebelling majority to take down the autocratic elite, resulting in a more egalitarian form of ‘pure’ democracy. In this paper, we seek to define the theoretical foundations for the emergence of elite-biased democracies as politico-economic equilibria.====A paradigmatic example of elite-biased democracies is the democratic transition in Chile in the late eighties. The 1989 constitutional reforms devised by the Pinochet regime, while providing some concessions to the opposition, ensured that the political and economic would retain their influence. In addition to the existing political and economic beneficial arrangements provided by the 1980 autocratic constitution, the legal and financial autonomy of the military was strengthened, the power of the executive branch was weakened, the electoral system was tweaked to give disproportional legislative influence to the regime’s outgoing incumbents and economic allies. So while dictatorship ended in 1990, the rules of the new formal Chile democracy favoured the former insider minorities, conducting to large and persistent inequalities, certainly much above than what a pure democracy triggered by a popular uprising would have delivered.====Our stylised facts in Section 2 highlight that the vast majority of democratic transitions have been biased towards insider elites.====Following this evidence and tracking some of the most salient facts reported, we propose a theory explaining how and under which conditions an elite-biased democracy can emerge. To this end, we extend the benchmark political transition model built up by Acemoglu and Robinson (2005) along two essential directions.==== First, we assume that the population is split into majority and minority groups under the initial autocratic regime. Second, the minority group is favoured by the autocratic elite. This is the insider characteristic of the minority highlighted by Albertus and Menaldo. The model is agnostic as to the origin of this insider minority. It could be racial as in the South-African case or socio-political as in Chile.====Minorities have been the focus of several political economy and political science studies in the last decades. One of the earliest is due to Hirshleifer (1991) and his celebrated “paradox of power”. Smaller groups, which are typically (much) poorer and less powerful initially, can be markedly more motivated than the bigger and more powerful groups to invest in conflictual activity and fight, eventually ending in a dominant position. Two other streams in public economics and political economy have put forward the role of minorities: the literature on redistribution and provision of public goods (see for example, Shayo, 2009) and the literature on the causes of social and political conflicts (see, Esteban and Ray, 1999, Esteban and Ray, 2011). The latter is more connected to our research as it highlights the conditions under which minorities play a role in major conflicts and subsequent political regime changes.==== This said, while having a similar theoretical focus, we shall not use the same modelling tools: instead of contest games with given alternative resource sharing rules, we extend the simple two-regimes game ==== Acemoglu and Robinson.====An essential outcome of our modelling is that, in comparison to Acemoglu and Robinson’s benchmark, we consider a second source of inequality and discrimination: in addition to the population split, the insider minority enjoys a preferential treatment. Society has a three-class structure, the members of the autocrat’s clan being the wealthiest. In this context grievance is double for the majority members, resulting in a strong demand for redistribution. In other words, the preferred tax rate of the majority member is likely to be significantly higher than in a situation, as in the Acemoglu and Robinson’s benchmark, where there is only one source of income discrimination. Elite-biased democracies may then arise if one source of discrimination, economic favouritism, is ==== removed, and replaced by a more good-looking coalition, even though the involved redistribution to the majority members is lower than the counterpart under pure democracy. We show in this paper that this is indeed a possible equilibrium outcome under certain conditions.====To the best of our knowledge, this is the first model showing how an elite-biased democracy might arise in equilibrium. A three-class structure is also considered in Acemoglu et al. (2015) to study the redistributive role of the middle class in democracy and the relationship between democracy, redistribution, and inequality. Indeed, the middle class is typically the pivotal class for institutional change in several contributions either in political economy (see for example Chen and Suen, 2017) or in development (see the well-known middle-class-based modernisation view put forward by Easterly, 2001). The role of minorities in institutional dynamics has been more specifically stressed in the context of fractionalised societies. Besides the work of Esteban and Ray already outlined, other authors have analysed in depth the role of ethnic minorities in different geopolitical contexts. Among others, Padró i Miquel (2007) has documented how African autocrats use pervasive patronage spending to keep office. In an elegant and simple enough theoretical set-up, he further shows how such autocrats can achieve this goal relying on the support of own ethnic groups in the absence of institutionalised succession processes. More recently, Bramoullé and Morault (2021) have analysed violence against minorities in a three-class model with political elite, rich ethnic minority and poor majority. They show that the local elite can maintain its hold on power by sacrificing the rich minority to popular discontent. None of the papers studying the role of dominant minorities has so far considered their role in the emergence of elite-biased democracies.====The paper is organised as follows. Section 2 provides stylised facts on the emergence and presence of elite-biased democracies in the world. Section 3 presents the general specifications of our extension of Acemoglu and Robinson’s model. Section 4 gives the main characteristics of the transitions from autocracy to elite biased-democracy in our model. Section 5 concludes.",A theory of elite-biased democracies,https://www.sciencedirect.com/science/article/pii/S0165489621000275,29 April 2021,2021,Research Article,99.0
"Barinci Jean-Paul,Cho Hye-Jin,Drugeon Jean-Pierre","Université Paris-Saclay, Univ Evry, EPEE, France,Sciences Po Reims, France,Paris School of Economics and Centre National de la Recherche Scientifique, France","Received 16 September 2020, Revised 8 March 2021, Accepted 24 March 2021, Available online 24 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.012,Cited by (1),"This contribution==== introduces a ==== approach of equilibrium dynamics in the context of a simple model of overlapping generations with heterogeneous goods. The class of preferences that is here considered hinges upon an endogenous leisure motive and an elementary savings behaviour, that comes as a simpler alternative to the Diamond tradition in the benchmark contributions about the properties of overlapping generations economies with two industries. The presence of some institution making possible intergenerational transfers is shown to influence both the equilibrium aggregate factors shares and elasticity of substitution along a stationary equilibrium. Both Wealth-to-Capital and Golden Rule steady state equilibria being considered, the economies are categorised, either as Samuelsonian or classical, according to the sign of the transfers between generations at the Golden Rule steady state. The local stability properties of the various types of equilibria are successively investigated, the elasticities of substitution between the two inputs being emphasised to play a key-role for that purpose. Interestingly, the smoothing properties of factors substitution and their respective contribution to the obtention of the local uniqueness property may differ between the Samuelsonian and classical economies.","Dating from the pioneering contributions of Samuelson (1958) in an exchange economy and Diamond (1965) in a production economy, the model of overlapping generations has admittedly become the most convincing ==== to the mainstream optimal accumulation environment. Such a framework is based upon heterogeneous cohorts and emphasises the simplest source of heterogeneity among agents, ====, their dates of birth. The ensued intergenerational trade will hence capture the differences in agents needs over their life cycle. This, ====, clearly appears within the benchmark setup with identical agents living for two periods while being solely endowed with positive resources in their first period of life: the young would save for their ==== while the old would dissave in order to consume. Now, and for a benchmark production economy, the availability of an intergenerational transfers contrivance on top of physical capital radically changes the equilibrium of the model. Entailed economies can indeed be classified in two types depending on the ==== of the transfers between successive generations at the Golden Rule steady state, ====, depending on the relationship between private wealth and the stock of capital. In the Wealth-to-Capital steady state private wealth equals the stock of capital; in the Golden Rule steady state private wealth may be greater or less than the stock of capital. Along the terminology coined by Gale (1973) and emphasised by Weil (2008), the former are called Samuelsonian economies, the latter classical economies. This article will aim at providing an articulation between the signs of the intergenerational transfers, the ensued range of admissible long-run equilibria and the role of the involved ====, everything being allowed by the consideration of a simple ==== environment.====Indeed, and even though it provided an entirely new framework for analysing inter-temporal economies with heterogeneous goods==== and significantly enriched the earlier Leontief-type attempts of, ====, Reichlin (1992), the introduction by Galor (1992) of a two-sector overlapping generations economy with positive factors substitutability only moderately succeeded in the academic area. Notwithstanding the increasing interest into numerical techniques and the ensued disinterest into new and alternative setups that may have hindered its diffusion, key to this misunderstanding was presumably the complexity of his framework and of the local properties of the involved equilibria. As an illustration and in opposition, ====, to the benchmark homogeneous good argument of Diamond (1965) whose dynamical equilibrium is canonically associated with a one-dimensional map, the ensued map was two-dimensional, that complexified any line of comparisons between a homogeneous good and a heterogeneous goods structure. It was not before the contributions by Cremers (2006), Drugeon et al. (2010), Kalray (1996), Li and Lin (2008), Nourry and Venditti (2011a), Nourry and Venditti (2011b) and Venditti (2005) that its structure, and noticeably its efficiency==== properties, got clarified. Yet, while a recent contribution due to Leriche and Magris (2017) has convincingly considered a two-sectors version of the model of overlapping generations with liquidity constraints, this remains a highly demanding environment where uniqueness is commonly difficult to obtain and no clear image is still available for distinguishing the sign of the intergenerational transfers that separate Samuelsonian economies from classical economies, not to mention the qualitative behaviour of their equilibria.====This contribution is then aimed at circumventing these remaining difficulties by letting the analysis rest upon a ==== based approach of equilibrium dynamics==== in the context of a simple model of overlapping generations with heterogeneous goods. The class of preferences that is considered, based upon Reichlin (1986) and Benhabib and Laroque (1988), hinges upon an endogenous leisure motive and an elementary savings behaviour. It will be argued that this comes as a much simpler alternative to the two periods positive consumptions Diamond (1965) tradition retained by Galor (1992) in his benchmark contribution about the properties of overlapping generations economies with two industries. Enriching earlier approaches of a heterogeneous goods world, this article will also introduce an ==== of the measure of factors substitutability for the whole production sector. Considering the associated inter-temporal equilibria, it will then be shown how the contrivance supporting intergenerational transfers—the source of the potential dissociation of savings from the value of the capital good—influences both the equilibrium aggregate factors shares and the equilibrium elasticity of substitution along both the Wealth-to-Capital and the intermediated—Golden Rule—stationary equilibria. To clarify further the issue at stake, the economies will then be categorised according to the sign of the transfers between generations at the Golden Rule steady state. It is, ====, shown that, for Samuelsonian economies, the equilibrium elasticity at the Golden Rule steady state is, ====, larger than the one at the over-accumulating Wealth-to-Capital steady state when the sectoral elasticity of substitution is larger in the investment good industry than in the consumption good industry.====An eventual part aims at characterising the stability properties of the Wealth-to-Capital and Golden Rule steady states and to sketch the scope for non-stationary equilibria in the neighbourhood of these steady states. As a simple illustration, the ==== of each sectorial elasticity of substitution to the Wealth-to-Capital steady state saddle-point stability condition noticeably differs. A strong discrepancy also surprisingly emerges for the Golden Rule steady state where it is shown that while positive transfers, and thus Samuelsonian economies, are univocally associated with saddle-point stability and local determinacy, a larger range of properties is allowed for negative transfers and thus classical economies. Depending indeed upon the respective strengths of contradicting properties on the two sectorial elasticities of substitution, the associated economies may, ====, allow for complex non-stationary equilibria in the neighbourhood of the Golden Rule steady state.====The technological set, an elementary class of preferences and the definition of competitive equilibrium being introduced in Section 2, stationary and non-stationary equilibria are analysed in Section 3 while most of the formal developments are left to the appendix.",On equilibrium elasticities of substitution in simple overlapping generations economies with heterogeneous goods,https://www.sciencedirect.com/science/article/pii/S0165489621000329,24 April 2021,2021,Research Article,100.0
"Abe Takaaki,Funaki Yukihiko","School of Political Science and Economics, Waseda University, 1-6-1, Nishi-waseda, Shinjuku-ku, Tokyo 169-8050, Japan","Received 18 May 2020, Revised 21 February 2021, Accepted 16 April 2021, Available online 24 April 2021, Version of Record 4 May 2021.",https://doi.org/10.1016/j.mathsocsci.2021.04.007,Cited by (0),"In this paper, we introduce a new core concept called the unbinding core by extending the definition of a deviation. In the traditional definition, players deviate if a profitable allocation exists after their deviation, while our new definition requires that all possible allocations are profitable. Therefore, the unbinding core becomes a superset of the traditional core. We examine some properties of the unbinding core. A sufficient condition for the unbinding core to be nonempty is also provided.","The traditional theory of the core for cooperative games is based on the assumption that if the members of a deviating coalition choose a particular allocation, then they can implement the allocation with certainty. The definition of a deviation exhibits this spirit. Let ==== be a coalition of players, ==== be the set of ====-dimensional real vectors, where ==== is the number of the members of ====, and ==== be the set of feasible allocations for ====. A coalition ==== deviates from an allocation ==== if there is ==== such that ==== for every ====: if there is at least one profitable outcome ====, they can deviate and obtain ==== with certainty. The traditional core is defined by the set of allocations from which no coalition deviates. However, there are many actual economic scenes in which the members of a coalition cannot determine or unanimously choose an allocation to deviate.====For example, we consider the following stag hunt game. Each of three hunters ==== chooses his pure strategy ==== or ====. If three hunters choose stag, then they get a large stag and obtain a total payoff of ====. If two hunters choose stag, they obtain a small stag and a total payoff of ====. If one hunter chooses stag, he cannot hunt any stag and obtains zero. If he chooses hare alone, he gets one hare and payoff ====, independent of the others’ choices. The payoffs derived form a stag or a hare are divided equally by the hunters who obtained the target. These three hunters go hunting by forming a team in which each of the members individually chooses stag or hare. If the three-hunter team is formed, then the three-hunter stag hunt game summarized as Table 1 is played. If a two-hunter team is formed, Table 2 is played, and the third hunter hunts one hare and obtains payoff 2. If a one-hunter team is formed, then the hunter similarly gets one hare and payoff ====. Clearly, for each of the three-hunter and two-hunter teams, there are two Nash equilibria: (stag, stag, stag), (hare, hare, hare); and (stag, stag), (hare, hare). As Harsanyi and Selten (1988) discuss, the stag-equilibrium payoff-dominates the hare-equilibrium, and the latter risk-dominates the former. Now, which team of hunters is formed and goes hunting? Moreover, which outcome (namely, a pair of a payoff profile and a strategy profile) is achieved by the hunters? If the members of a two-hunter team agree on the stag-equilibrium and can bind each other to the equilibrium, then the allocation ==== of the stag-equilibrium should be blocked by the coalition of the two hunters through the allocation ====. However, as the theory of equilibrium selection questions, it is not obvious that each of the two hunters commits to accomplish the stag-equilibrium. This example motivates us to propose a new core concept.====In this paper, we introduce a new core concept: the ====. In Section 2, we define the unbinding core and demonstrate it in some examples. In Section 3, we present general differences and similarities between the core and the unbinding core. In Section 4, we propose a sufficient condition for the unbinding core to be nonempty. In Section 5, we provide some concluding remarks. This paper is devoted to introducing our new concept, and all proofs are omitted except the sketch of the proof of Proposition 4.2. The detailed proofs are available from the authors upon request.",The unbinding core for coalitional form games,https://www.sciencedirect.com/science/article/pii/S0165489621000457,24 April 2021,2021,Research Article,101.0
"Augeraud Veron E.,Picard P.M.","GREThA, UMR 5113, University of Bordeaux, France,University Carlos III, Madrid, UC3M, Spain,University of Luxembourg, CREA, Luxembourg","Received 17 September 2020, Revised 17 February 2021, Accepted 24 March 2021, Available online 23 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.006,Cited by (2),"In this paper we investigate the effect of local interaction in a simple ==== model. Agents interact with others if and only if their interaction benefit outweighs their travel cost and therefore meet others only within finite geographic windows. We show that two or more cites may co-exist at the equilibrium provided that they are sufficiently distant. For any interaction surplus function, there exists a unique spatial equilibrium on not too large city supports. The population density within a city is determined by a second order advance-delay differential equation, whose solutions are fully characterized for linear interaction surplus functions. Numerical analyses show that more localized interactions yield flatter population density and land rents over larger extents of the city support. They do not give support to the idea that multiple subcenters can be caused by small and finite geographic windows of interaction.","The urban economic literature explains the emergence of a city through the existence of agglomeration forces. Such forces may stem from face-to-face interaction between managers (Fujita and Ogawa, 1982), technological spillover between firms (Berliant et al., 2002; (Lucas and Rossi-Hansberg, 2002)), social interactions between residents (Mossay and Picard, 2011, Mossay and Picard, 2019), or demand and forward linkages by firms (Mossay et al., 2020). A common simplification in this theoretical literature lies in the assumption of global interaction within the city. That is, it is assumed that, in an urban area, all managers find it attractive to meet every other manager, all firms benefit from copying a bit of every other firm’s technology, residents are willing to interact with every other and, all firms find it profitable to sell and buy intermediate goods to and from every other firm.====The assumption of global interaction is however unwarranted in large urban areas. Intuitively, one does not cross the full city extent of a large metro area for every types of service, purchase, job or social interaction. Indeed, empirical studies show that distance strongly reduces the level of socio-economic interactions. Büchel and von Ehrlich (2016) show that 50 percent of ties formed by Swiss mobile telephone users lie within a 5 km perimeter, which covers on average less than 1 percent of the population. Furthermore, the advent of the Internet has not much change habits: Mok et al. (2010) show that, face-to-face contact remained strongly related to short distances and with same frequency among friends and relatives between the 1970s and 2000s. Levy and Goldenberg (2014) show that, the probability of a social link in a sample of Facebook and email users is proportional to the inverse of the square of the distance between them.====To account for the negative effect of distance on interaction, most urban economic models combine the assumption of global interaction with decaying interaction effectiveness, typically through negative exponential/iceberg travel cost of interaction. When the benefit of interaction decreases at a constant rate with distance, it gets very small at very large distance but never nullifies, which is a rather unrealistic feature. The purpose of this paper is to investigate the alternative case of ‘local interaction’ where such benefit becomes zero at some ‘finite’ distance.====Towards this aim, we revisit Beckmann’s (1976) linear city model with social interaction and homogeneous population.Agents have elastic demand for residential land while they benefit from interaction with each other and incur a travel cost for every interaction. Agents bid for residential land, compete with each other and with other land users (typically, farmers). In this city, the social interaction (net) benefit acts as the agglomeration force and the land market as the dispersion force. In contrast with the literature, agents do not interact with too far individuals because interaction benefits are there lower than their costs. For analytical tractability, we focus on the case of linear travel costs although many results do not qualitatively hinge on such an assumption. We firstly explain the difference between local and global interactions. In particular, the spatial equilibrium includes one city under global interaction while many equilibria exist with many configurations of sufficiently distant cities under local interaction.====In this paper, we focus on local interaction and their impact on the spatial distribution of residents in a city. To the best of our knowledge, our approach contrasts to most urban studies of city structure with global interaction. Indeed, we characterize the spatial equilibrium as a fixed point of the population density function. We show that local interaction imply that the equilibrium population density function is the outcome of a second order advanced-delay differential equation. That is, the changes in population density at any urban location depend on the density at some specific distance to the left and the right hand of this location. This specific distance corresponds to the ‘interaction window’ of agents, that is, the geographical support in which each agent has an incentive to make an interaction. The presence of local interaction makes the problem complex due to the solving of advanced-delay differential equations. We use the Contraction Mapping theorem to show the existence and uniqueness of a spatial equilibrium for a subset of economic parameters for any type of interaction surplus. In the case of linear interaction surplus, the solution for population density is also found to be a piecewise trigonometric function of the location of each agent’s residence.====The characterization of spatial equilibria is further complicated by the fact that land can be used for other purposes (e.g. farming) than residence. To offer a land bid above that of those other land users, residents must be content with small enough land plots. As population density is inversely related to residents’ land use, this means that the equilibrium population density should be high enough within a city. The equilibrium population density therefore jumps from a positive value to zero at the city border. Checking this condition with the above piecewise trigonometric function for population density proves analytically infeasible. Our strategy is to hold that condition and analytically prove that, for generic parameter values of preferences for space, interaction benefit and travel cost, there exists a spatial equilibrium candidate. The latter are proved to be spatial equilibria only for not too large city supports. Numerical simulations then allow us to verify the existence of a unique spatial equilibrium for larger city supports.====Our study confirms that usual properties of urban equilibria apply under local interactions. Bigger population lie on larger geographical extent. However, the latter tends to become inelastic for large enough cities. Higher opportunity cost of land increases population density and attracts a larger population on a same geographical extent. Stronger preferences for residential space make cities wider. A new result specific to local interaction is that more localized interactions tend to yield flatter population density and land rents over a wider city support.====The study also sheds light on the debate about mono- and poly-centric cities. In the urban economics literature, multiple subcenters coexist under the assumptions of global interaction and heterogeneous groups of land users (managers and residents in Fujita and Ogawa, 1982; firms and residents in Berliant et al., 2002 and Rossi-Hansberg and Lucas, 2003; residents with different communities in Mossay and Picard, 2015; Blanchet et al., 2016 etc.). This paper suggests that the first assumption is not the key one. We indeed focus on the location of a homogeneous group of residents and find that sufficiently distant cities emerge as spatial equilibria under local interaction. But we do not find examples for the emergence of several subcenters within the same city. As soon as two cities become close enough, equilibrium forces merge them into a single city with single-peaked population density profile.====Our urban model with local interaction is described by an advanced-delay differential equation, also called Mixed-typeFunctional Differential Equation (MFDE in the following). This is because interaction takes place within a finite window of the urban space. Delay differential equations are used in dynamic applications where delays are present. They have been used in many areas of physics and engineering (system control), life science (epidemiology, immunology, physiology, population dynamics and neural networks). They have very early been recognized as an important aspect of economics for the oscillatory and unstability properties that are created by the reaction delays of investors, governments, etc. (Frisch and Holme, 1935, James and Belz, 1938). Because of their analytical complexity, they have led to a scant amount of economic applications. In growth theory, MFDEs have been used by Boucekkine et al. (2005) who study vintage capital models in which investment is delayed. Solving the optimal control problem using Pontryagin’s method yields to a state variable with delays and an adjoint variable with advances. Continuous time overlapping generations models also lead to MFDE when agents have finite lifetime (d’Albis and Augeraud-Véron, 2007). Long run properties of the dynamics are mainly studied (d’Albis et al., 2014). Even if the dynamics we encounter in this paper is given by a MFDE, its study completely differs from the one used in the previously cited papers. The main difference with the previous literature is that our dynamics is defined on a finite interval (bounded city support). Basic mathematical toolkits are provided in Diekmann et al. (1995) and Mallet-Paret and Verduyn-Lunel (2001) who study MFDE on both infinite and finite intervals. However, the tools developed in Mallet-Paret and Verduyn-Lunel (2001) do not fit our model because our solution is discontinuous on the whole geographical space, but moreover, because it must be solved with additional constraints (integral and positiveness constraints). Therefore, our approach is innovative because it combines tools for MFDE defined on finite interval and also fixed point theorem.====We present our urban economics model with local interactions in Section 2 and its general properties in Section 3. We then focus on the linear travel cost case in Section 4 and explain the difference between spatial equilibria local and global interactions. In that section, we construct the spatial equilibria and discuss simulation exercises. Section 5 shortly extends the model to the impact of SARS-CoV-2 epidemics. Section 6 concludes.",Local social interaction and urban equilibria,https://www.sciencedirect.com/science/article/pii/S0165489621000263,23 April 2021,2021,Research Article,102.0
"Boucekkine Raouf,Seegmuller Thomas,Venditti Alain","Aix-Marseille University, CNRS, AMSE, France,EDHEC Business School, France","Available online 22 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.004,Cited by (0),This paper is an introduction to the special issue of ==== on ==== in memory of Carine Nourry.,None,Advances in growth and macroeconomic dynamics: ,https://www.sciencedirect.com/science/article/pii/S016548962100024X,22 April 2021,2021,Research Article,103.0
"Doğan Serhat,Karagözoğlu Emin,Keskin Kerim,Sağlam Çağrı","Bilkent University, Turkey,CESifo Munich, Germany,ADA University, Azerbaijan","Received 10 November 2020, Revised 18 April 2021, Accepted 18 April 2021, Available online 21 April 2021, Version of Record 30 April 2021.",https://doi.org/10.1016/j.mathsocsci.2021.04.008,Cited by (0),"We study bribing in a sequential team contest with multiple pairwise battles. Allowing for asymmetries in winning prizes and marginal costs of effort, we present the conditions under which (i) a player in a team is offered a bribe by the owner of the other team and (ii) she accepts that bribe. We show that these conditions depend on the ratios of players’ winning prizes and marginal costs of effort: the team owner chooses to bribe the player with the most favorable winning prize to marginal cost of effort ratio, and offers a bribe that leaves her indifferent between accepting (and exerting zero effort) and rejecting (and exerting her optimal effort). In some cases, the competition between players and the negative consequences of one player receiving a bribe on the team performance can drag down the equilibrium bribe to zero. We also study the impact of changes in winning prizes and marginal costs of effort on the equilibrium bribing behavior.","Bribing can be defined as giving someone money or something else of value in order to make her act in one’s favor. Practically, all definitions incorporate phrases such as ‘often illegally’ and/or ‘something dishonest’. Despite the illegality or immorality attached to the act, bribing in real life is ubiquitous. According to an OECD report (see OECD, 2014), bribes amount to 10% of the total value created by all commercial transactions and 34.5% of firm profits are spent on bribing. The World Bank estimates the amount of bribe exchanging hands in a year to be around 1 trillion dollars (see Kaufmann, 2005). Giving or accepting money to rig professional sport competitions is also very widespread. In its ====, Transparency International describes match-fixing as a fully acknowledged, real threat to the integrity of sport and as such it is one of the six major topics in that report (see Sweeney, 2016). Some well-known examples are Olympique de Marseille match-fixing scandal in 1993, Italian Football League match-fixing scandal in 2011–2012, and Spanish Football League match-fixing scandal in 2019.====In this paper, we theoretically study bribing in a sequential team contest with multiple pairwise battles (see Fu et al., 2015). In our model, there are two teams with an equal number of players. To keep the model tractable yet sufficiently rich to derive interesting results, we fix the number of players in each team to three. Hence, the team contest is made up of three pairwise battles. In each battle, the paired players simultaneously exert costly efforts, and a Tullock contest success function (see Tullock, 1980) determines who wins the battle. The team that wins at least two battles wins the contest, and its members collect positive winning prizes, while the members of the losing team receive zero. We allow winning prizes and marginal costs of effort to be heterogeneous across players. The pairs of players in each battle as well as the sequence of those battles are exogenously given at the beginning of the game.==== This model has several applications in sport competitions, such as Ryder Cup in golf and Davis Cup in tennis; but more generally, it applies to team contests where each team member has an individual task on behalf of his team such that her success in that task makes a significant impact on her team’s winning chance, but her efforts cannot directly and completely compensate for the failure of her teammates. Accordingly, one can provide more examples such as bi-party elections and cross-functional R&D alliances.====As we introduce bribing, we make three modeling assumptions: (i) only one of the team owners can offer a bribe, (ii) she can bribe only one player in the other team, and (iii) the bribed player does not exert any effort.==== The team owner that can offer a bribe chooses whom to bribe and by which amount, with the objective of maximizing her own expected payoff. The other team owner does not have a decision to make.====We analyze subgame perfect Nash equilibrium (SPNE) in this sequential game. Our main result shows which player is offered a bribe in equilibrium, by which amount, and whether she accepts it or not. In particular, we show that the answers to all of these questions are closely connected to the ratios of individual winning prizes and marginal costs of effort. More precisely, the team owner chooses to offer a bribe to the player with the most favorable winning prize to marginal cost of effort ratio (after taking into account the same ratios for the players paired with them), and the offered bribe amount leaves that player indifferent between accepting (and exerting zero effort) and rejecting (and exerting her optimal effort). It is further discussed that even a zero bribe can be accepted by a player under some circumstances. Moreover, we conduct numerical comparative static analyses on winning prize and marginal cost parameters in some illustrative examples to observe the impact of a change in these parameters on the equilibrium bribing behavior.====To the best of our knowledge, this is the first paper in the contest theory literature to incorporate bribing into team contests. In fact, bribing (or, cash transfers or side payments) is a very understudied topic even in individual contest games. Preston and Szymanski (2003) formulate a simple model of match-fixing, which takes into account the probability of getting caught and the uncertainty regarding the type of players (i.e., a moral type who does not accept bribes or an immoral type who accepts bribes). The authors describe the conditions under which bribe occurs in equilibrium. Schoonbeek (2009) studies a two-stage Tullock rent-seeking contest. In the first stage, the existing players in the game can choose to bribe a potential entrant to persuade her not to enter. In the second stage, the actual contest takes place. The author characterizes the conditions under which the potential entrant is bribed and hence stays out. Kimbrough and Sheremata (2013) analyze a two-stage game of (potential) conflict and show – both theoretically and experimentally – that in the presence of binding commitments, side payments can lead to large efficiency gains through avoided conflicts. Finally, Esö and Schummer (2004) and Rachmilevitch, 2013, Rachmilevitch, 2015 study bribing in the context of auctions, which can be considered as contest-like interactions. As we mention above, neither these papers nor the others in the literature consider bribing in a team contest.====The organization of the paper is as follows. In Section 2, we introduce the baseline model of team contest with multiple pairwise battles without bribing. In Section 3, we incorporate bribing into the baseline model, present an equilibrium analysis, and report the corresponding results. In Section 4, we conclude by providing a discussion on our modeling assumptions, some caveats, and possible future research questions.",Bribing in team contests,https://www.sciencedirect.com/science/article/pii/S0165489621000469,21 April 2021,2021,Research Article,104.0
Yeşilırmak Muharrem,"ADA University, Azerbaijan,CERGE-EI, Czechia","Received 3 May 2020, Revised 4 September 2020, Accepted 12 April 2021, Available online 21 April 2021, Version of Record 3 May 2021.",https://doi.org/10.1016/j.mathsocsci.2021.04.005,Cited by (0),"Bargaining power of teacher unions over teacher wages has been either reduced or eliminated by several states in U.S. since 2011. This caused public school districts to move away from single salary schedules (fixed compensation regime) and adapt a flexible compensation regime at which teacher wage rises with quality (value added). In this paper, using a fully tractable ==== model of local teacher labor markets, we theoretically analyze the effect of different compensation regimes on teacher efforts and average teacher quality in a district. In our model, teachers, heterogeneous in exogenously set quality, endogenously sort across two districts and also choose teaching efforts. Districts differ by endogenous teacher wages and exogenous revenues. The marginal disutility of effort for a teacher is different across the districts. Teacher labor markets clear in each district and teacher wages are determined. We solve for the unique equilibrium under each compensation regime and theoretically show that low(high) quality teachers exert the highest effort under fixed(flexible) compensation regime and exert the lowest effort under flexible(fixed) compensation regime. Also, we show that average teacher quality is highest in each district under flexible compensation regime and lowest in each district under fixed compensation regime. Our findings are consistent with several empirical studies.","After the Great Recession, U.S. governmental agencies sought ways of using their funds more economically. For that sake, since 2011, several states including Idaho, Indiana, Iowa, Michigan, Tennessee, and Wisconsin passed legislation that either reduced or eliminated the collective bargaining power of teacher unions over teacher wages. This gave public school districts the opportunity to move away from single salary schedules and adapt a flexible compensation regime at which compensation rises with teacher’s quality.==== In this paper, we theoretically compare different teacher compensation regimes with regard to their effects on teacher efforts and average teacher quality in a district. Changes in teacher efforts and average teacher quality translates into change in average student achievement since they are essential inputs in producing achievement as found by empirical studies such as Rockoff (2004) and Rivkin et al. (2005).====Our model economy consists of two public school districts and a continuum of heterogeneous teachers. Teachers differ by exogenously set quality which is uniformly distributed. Teachers derive utility from consumption of the numeraire good, average student achievement in the classroom, and they derive disutility from teaching effort. School districts differ by exogenous revenues and endogenous teacher wages. School districts also differ by working conditions which is captured by a parameter that governs marginal disutility of teaching effort. Achievement depends on teacher’s quality and effort and an exogenously given vector of inputs such as student’s ability, peer effect, private educational spending, parent’s education level, etc. Each teacher chooses effort for each student type in the classroom. Teachers also make a choice among the two school districts which in turn yields the supply of teacher quality for each district. School districts determine their demand for teacher quality so as to equate their revenues with teachers’ wage expenditures. Teacher labor market clears in each district and teachers’ wages are determined. In our model, there exists a unique equilibrium under each compensation regime and we can find closed-form expressions for each endogenous variable.====Using our model, we analyze three teacher compensation regimes: (i) flexible, (ii) fixed, and (iii) mixed. Under flexible compensation regime, a teacher’s total compensation rises with quality and wage rate per unit of quality differs across districts. Under fixed compensation regime, a teacher’s total compensation is fixed in a district and is independent of quality. Fixed wage differs across districts as in data. In the traditional single salary schedule observed in reality, the teacher wage is unrelated to quality as noted in Hanushek (2007) and Podgursky (2007). Therefore, we think of fixed compensation regime in our model as corresponding to the observed single salary schedule. Under mixed compensation regime, first district implements fixed compensation regime and the second district implements flexible compensation regime. We find that, in equilibrium, low quality teachers exert highest effort under fixed compensation regime and exert lowest effort under flexible compensation regime. On the other hand, high quality teachers exert highest effort under flexible compensation regime and exert lowest effort under fixed compensation regime. We also find that average teacher quality is highest in both districts under flexible compensation regime. And it is lowest in both districts under fixed compensation regime. The average qualities in the districts under mixed compensation regime lie between the first two regimes. Moreover, calibrated version of our model implies that economy-wide mean achievement and variance of achievement are highest under fixed compensation regime and lowest under flexible compensation regime.====Our predictions stated above are consistent with the findings of empirical studies. For instance, using a rich structural econometric model for Wisconsin’s districts after Act 10, Biasi (2018) finds that average teacher quality and teacher efforts rise in those districts that switched to flexible pay scheme relative to the districts that kept the single salary schedule. Lovenheim and Willen (2019) analyze the state duty-to-bargain law in U.S., which increased the bargaining power of teacher unions, and finds that education quality, teacher quality and effort decreased as a result leading to decreases in annual earnings of males. Roth (2019) and Anderson et al. (2019) also find positive effects on education quality and achievement following the state laws that decreased collective-bargaining power of teacher unions.====In a dynamic general equilibrium framework, Tamura (2001) shows that per capita incomes converge across states in U.S. if teacher quality is more important than class size in producing human capital. Gilpin and Kaganovich (2012) using a dynamic general equilibrium model compares different teacher pay regimes in terms of teacher quality distribution over time, income inequality, and economic growth. Different from these papers, our static model is silent about economic growth and inequality. On the positive side, in our model, teachers choose effort for each student and choose among the districts different from these papers. Another paper by Yeşilırmak (2019) quantitatively analyzes teacher bonus pay policy for U.S. using an equilibrium political economy model of education at which allocation of both households and teachers across districts are endogenously determined. Different from Yeşilırmak (2019), we do not model households’ choices explicitly although we provide an extension in Section 6. Moreover, in Yeşilırmak (2019), teachers do not have a choice over districts and teacher sorting is determined through the districts’ demands. In our paper, teachers have a choice over districts and teacher sorting is determined by the joint interaction of districts’ demands and teachers’ supply of quality. Furthermore, in Yeşilırmak (2019), it is not possible to theoretically compare teacher effort level (which is same for each student) under different teacher compensation regimes. Another theoretical model of teacher labor markets is studied in Biasi (2018) at which teachers’ salaries are exogenously given and teachers cannot choose effort.====In our model, economy-wide teacher quality distribution is exogenous and does not change with teacher compensation regime. More specifically, teacher quality distribution may be expected to improve as the economy moves from a fixed compensation regime to a flexible one. However, according to OECD (2014), the earning of an average teacher is 68% of the earning of an average college graduate in U.S. in 2012. This implies that the average teacher’s earning should rise by approximately 50% in order to attract an average college graduate into the teaching profession. We believe that such an increase in average teacher’s earning is unlikely in practice when the economy moves to a flexible payment regime. Moreover, as noted in Corcoran et al. (2004) and Hoxby and Leigh (2004), the fraction of highly qualified teachers is decreasing in U.S. over time. Based on these, we believe that our model is not an oversimplification of reality.====This paper is organized as follows. Section 2 explains the model. Equilibrium is defined in Section 3 and its theoretical properties are analyzed in Section 4. Section 5 presents a quantitative analysis of the model. Section 6 discusses possible changes in the results once households’ choices are introduced into the model. Section 7 studies an alternative mixed compensation regime. Section 8 concludes.",A theoretical general equilibrium analysis of local teacher labor markets under different compensation regimes,https://www.sciencedirect.com/science/article/pii/S0165489621000433,21 April 2021,2021,Research Article,105.0
"Alekseev Aleksandr,Sokolov Mikhail V.","Department of Economics, University of Regensburg, Universitätsstr. 31, 93040, Regensburg, Germany,European University at St. Petersburg, 6/1A Gagarinskaya st., St. Petersburg, 191187, Russia,St. Petersburg State University, 7/9 Universitetskaya nab., St. Petersburg, 199034, Russia,Institute for Regional Economic Studies RAS, 38 Serpukhovskaya st., St. Petersburg, 190013, Russia","Received 14 February 2020, Revised 3 December 2020, Accepted 12 April 2021, Available online 21 April 2021, Version of Record 4 May 2021.",https://doi.org/10.1016/j.mathsocsci.2021.04.002,Cited by (0),"This paper contributes to the theory of average rate of change (ARC) measurement. We use an ==== to generalize the conventional ARC measures (such as the difference quotient and the continuously compounded growth rate) in several directions: to outcome variables with arbitrary connected domains, to not necessarily time-shift invariant dependence on time, to more general (than an interval) time sets, to a path-dependent setting, and to a benchmark-based evaluation. We also revisit and generalize the relationship between the ARC measurement and intertemporal choice models.","Values represented as rates of change are ubiquitous in human lives. Employees are concerned with the rates of changes of their wages and retirement accounts, managers and firm owners are concerned with the rates of changes of production and revenues, investors are concerned with the rates of changes of their portfolio values, and policy-makers are concerned with the rates of changes of macroeconomic indicators, such as GDP, prices, and employment. In this paper, we revisit the axiomatic foundations of the formulas used to measure the average rate of change (ARC). We explore the limits of applicability of these well-known formulas by generalizing the existing axioms and asking how much these axioms can be relaxed to accommodate some non-standard settings.====When asked to measure the ARC of a variable over a time interval, an individual will likely use the formula ====where ==== (resp. ====) is the value of an outcome variable at time ==== (resp. ====). For example, if an individual’s retirement account was valued at $10,000 in year 1950 and $100,000 in year 2000, the formula above implies that over the period 1950–2000 the account grew by $1800 every year, on average. This amount represents the average absolute change (measured in dollars) in the retirement account per year. If the individual was interested in measuring the average relative change (in percent), he/she would use the formula ====In our example, this formula implies that during the period of observation the individual’s retirement account was growing at 4.7% annually, on average.====While the application of these formulas is straightforward in many standard situations, the question of how to apply them, with appropriate modifications, in more general situations is far from trivial. These more general situations arise, for example, when the outcome variable takes a form other than a scalar. The following six questions determine the directions along which we attempt to generalize the ARC formulas (1), (2).====To illustrate, imagine that the individual is forecasting the ARC of his/her retirement account over some future period. The value of the retirement account at a future date is assumed to be random, and thus ==== and ==== are function-valued rather than scalar-valued. How can ARC be measured in these settings? There is a variety of non-equivalent ways. Imagine that the individual comes up with two solutions to this new problem. Solution A involves resolving risk ==== by first computing the ARC (2) for each possible realization of ==== and ==== and then taking the expectation of the ARCs. Solution B involves resolving risk ==== by replacing ==== and ==== with their expected values and then computing the ARC. Employing an axiomatic approach, we show that solution B (as well as calculating the expected log-return) is legitimate, while solution A is not. Note that solution A, the expected rate of return, is an example of how the ARC of a stock’s price is sometimes evaluated. In general, we show that the relevant modification to the ARC formulas in this case involves replacing ==== and ==== with ==== and ====, where ==== is a real-valued function.====Other examples with non-scalar outcome variables are inflation measurement (the outcome variable – price vector – is vector-valued), ARC of income inequality (the outcome variable – income distribution – is function-valued), fire spread rate (the outcome variable – shape of burned area – is set-valued). The function ==== in these examples may represent a price index, an index of income inequality, and the burned area in km====, respectively. In general, ==== may be interpreted as a utility function representing preferences over the domain of the outcome variable. Thereby, information solely on the values of the outcome variable is not sufficient to measure ARC, the functional form of an ARC may vary with a target consumer group (as is illustrated, e.g., by the existence of a variety of inflation measures).====The ARC measures (1), (2) are stationary in the sense that they are invariant with respect to a time shift. While dependence of an ARC on time via the difference ==== seems intuitive (since an ARC is most commonly understood as a measure of change per time unit), the stationarity assumption that this difference induces can be too restrictive in some applications. Moreover, values ==== and ==== may have an interpretation other than time (see the next question, for a discussion); in this case, the stationarity assumption, even if it is well defined, may have no economic content. To illustrate, suppose that the individual is interested in measuring the ARC of the real (inflation-adjusted) value of the retirement account. Since inflation fluctuated greatly during the period of 1950–2000, the ARC will not be invariant to a time shift, e.g., from 1951 to 2001, and hence will not be stationary. We show that a proper modification of the ARC formula (2) is defined implicitly as a solution ==== of the equation ====, where ==== can be interpreted as a discount function that corresponds to the discount rate ====. That is, the functional form of an ARC measure depends on the structure of a target consumer group’s intertemporal preferences.====The ARC formulas (1), (2) assume that observations of the outcome variable are ordered by time. Depending on the application, time can be modeled either as continuous or discrete; this results in formally distinct models. Moreover, in some applications, observations can be ordered by a variable other than time. Examples include: distance, if one measures the ARC of a car value per mile driven; a physical property of an object, if one measures, e.g., the ARC of power consumption of a kettle per thickness of a limescale layer; a number of trials, if one measures the ARC of a reaction time per number of practice trials taken, as is common in learning curve estimations. The domain of such an ordering variable may not be a subset of the reals with the usual order. Suppose that an individual is interested in measuring the ARC of a car value and uses the lexicographically ordered pair ==== to order observations. In all of these scenarios the domain of an ordering variable forms a linear order and it would be desirable to generalize the ARC formulas along this direction. We show that in fact no such generalization exists unless the domain is order isomorphic to a subset of the reals with the usual order.====Benchmarking is a universal practice in various fields, such as portfolio management or strategic planning. It involves measuring the outcome variable relative to some reference object. How can we incorporate benchmarking into ARC measurement? Suppose the individual is interested in measuring the ARC of the retirement account relative to the value of his/her partner’s retirement account. The individual considers evaluating ARC of the difference (resp. quotient) of his/her and the partner’s account values. We show that this intuitive approach is actually the only relevant benchmark-based counterpart of the ARC formula (1) (resp. (2)).====Though ARC is typically measured over a single period of time, a generalization is of interest. In particular, one may want to know how ARCs over disjoint intervals can be aggregated. To illustrate, suppose that the individual in our example holds most of the retirement portfolio in stocks. The individual is aware of the “January effect” in finance (Thaler, 1987), an abnormal calendar pattern in January stock returns relative to other months, and wishes to evaluate the strength of this effect on the portfolio. Hence, he/she needs to calculate ARC of the retirement account over the January months only (as well as over the remaining months). The individual considers using formula (1) by computing the ARC over each January month and taking the arithmetic mean. We show that this is indeed a proper way of measuring the ARC in this case, and that in general one would use the time-weighted arithmetic mean (equivalently, the resulting ARC must be the quotient of a measure of change of the outcome variable to elapsed time).====The ARC measures (1), (2) are path-independent in the sense that they depend on the outcome variable only through its values at the endpoints of the interval rather than on the entire path. Path-dependent ARC formulas are of interest in various fields, including measurement of aggregate price/quantity change (Balk, 2008 Chapter 6) productivity (Richter, 1966, Jorgenson and Griliches, 1967) and welfare (Bruce, 1977, Cysne, 2003) measurement. For instance, most if not all statistical agencies calculate their consumer price index as a chained index, which in general produces a path-dependent measure of inflation. Though the conventional line integral approach, initiated by Divisia and Montgomery, suggests some important classes of path-dependent ARC measures, the general structure of path-dependent ARC remains unclear. We show that a proper path-dependent modification of the ARC formula (1) involves replacing the numerator with the integral over ==== whose integrand value at ==== is completely determined by the germ of the path of the outcome variable at time ====. The obtained integral representation comprises, as special cases, most known path-dependent indices, including the Divisia index.====Answering these six questions allows us to substantially revise and complement our earlier work (Alexeev and Sokolov 2014) (henceforth, AS), where some results in the directions of the first two questions were derived. In our analysis, we rely on the difference measurement foundation (specifically, Theorem 5.3 in Wakker, 1988), additive representations (Krantz et al., 1971, Chapter 6; Wakker, 1988, Section 4), and conditional utility formalism (Krantz et al., 1971, Chapter 8). The duality between ARC measurement and time preferences, established in AS, is central to our discussion. Section 2 revisits this duality in the light of a recent study by Doyle (2013) who advocates analyzing time preference models by isolating the rate parameter governing the degree of discounting in a discounting equation. We show that his approach is in fact a universal recipe for constructing an ARC measure. In particular, an ARC measure can be identified with a discount rate that makes an individual indifferent between the initial and final values of the outcome variable. This duality implies a one-to-one correspondence between ARC measures and one-parameter families of time preferences indexed by a discount rate. We show that the ARC formulas (1), (2) correspond to exponential discounting. Since a large body of empirical literature fails to provide evidence to support exponential discounting, this implies that, at least for some applications, the usual ARC formulas are not suitable, even though it is a nearly universal practice to use formulas (1), (2) to calculate ARC. In Section 3, we derive the ARC measures that correspond to some common time preference models, including the discounted utility and the relative discounting model of Ok and Masatlioglu (2007). In Section 4, we apply an axiomatic approach to answer questions 1–3. We show that the characterized ARC measures correspond to the common time preference models discussed in Section 3. Due to the duality, the obtained characterizations of the ARC measures can be equivalently viewed as axiomatic foundations for the corresponding families of time preferences. Section 5 shows how benchmarking can be incorporated into ARC measurement and provides an answer to question 4. In Section 6, the baseline ARC model is generalized to cover the remaining two questions.====Our research contributes to measurement theory. From an axiomatic viewpoint, several particular issues related to the highlighted questions are also analyzed in utility theory, index number theory, and investment appraisal. Question 1 under the special case when the time interval is fixed is related to the literature on difference (change, improvement, preference intensity) measurement (Krantz et al., 1971, Chapter 4; Shapley, 1975, Wakker, 1988, Section 5; Köbberling, 2006; Gerasimou, 2019). Frenzen (1994, Section 2) and Aczél et al. (1996) address the problem of change measurement using a more demanding approach. Vind (2003, Chapters 8, 11, 12) generalizes some of these results in the directions of questions 5 and 6. Particular measures of change are studied in various fields, including measurement of aggregate price/quantity change, productivity measurement, welfare change measurement, and investment appraisal. In particular, in the special case when the outcome variable is a price or quantity vector, questions 1, 4, and 6 are studied in index number theory (Diewert and Nakamura, 1993, Balk, 2008). If the outcome variable is an income distribution, questions 1, 4, 5, and 6 are considered in the literature on social welfare change measurement and income mobility (e.g., see Fields and Ok, 1999, Palmisano and Van de gaer, 2016, Bossert and Dutta, 2019, to mention just a few). If the outcome variable is money, questions 2, 5, and 6 are related to the literature on investment appraisal that characterizes the internal rate of return (Promislow and Spring, 1996, Vilensky and Smolyak, 1999). In particular, Promislow and Spring (1996) propose a general measure-theoretic construct that can be applied to generalize formula (2) in the directions of questions 2, 5, and 6.",How to measure the average rate of change?,https://www.sciencedirect.com/science/article/pii/S0165489621000408,21 April 2021,2021,Research Article,106.0
"Bhattacharya Mihir,Gravel Nicolas","Department of Economics, Ashoka University, Rajiv Gandhi Education City, Rai, Sonipat, NCR, Haryana, 13 1029, India,Centre de Sciences Humaines & Aix-Marseille Univ., CNRS, AMSE, 2, Dr APJ Abdul Kalam Road, 11 0011 Delhi, India","Received 9 August 2020, Revised 6 April 2021, Accepted 7 April 2021, Available online 20 April 2021, Version of Record 22 November 2021.",https://doi.org/10.1016/j.mathsocsci.2021.04.001,Cited by (0),"We show that a majoritarian relation is, among all conceivable ","The “preference of the majority” is indisputably one of the most widely used and discussed social preference. However, the normative justifications of the “majoritarian” way of aggregating individual preferences are surprisingly thin. One important justification has been provided by May (1952), who proves that when there are only two alternatives to be compared, the majority rule is the only mapping of individual preferences into a social ranking that is ====, ====, ==== and ====. A well-known (at least since Condorcet in the late XVIIIth century) limitation of the majority rule is its failure to satisfy transitivity. This limitation is obviously not addressed by May (1952) who restricts his analysis to the two-alternatives case. In the discussion of his impossibility theorem, Arrow (1963) (p. 101), recognizes that the generalization of May’s result to more than two alternatives is not easy. Papers who have proposed such a generalization include Dasgupta and Maskin (2008) and Horan et al. (2019). However, they have done so in the case where the individual preferences are so restricted that the majority rule is transitive or, at least, admits a maximal element (called a Condorcet winner).====An alternative way of justifying the preference of the majority would be of course to combine results in May (1952) and Arrow (1950) through the well-known axiom of ====, which requires the social ranking of any two alternatives to depend only upon the individuals’ ranking of these two alternatives. One could then justify the preference of the majority as being the only mapping of individual preferences into a social ranking that is ====, ====, ====, ====, and ====. However, as recognized by May (1952) himself, the appeal of the binary independence of irrelevant alternatives and neutrality axioms is not clear.====In this paper, we therefore examine an alternative justification for the majority rule. Specifically, we show that the preference of the majority qualifies as being ==== of the collection of preferences from which it emanates. The notion of representativeness on which our argument rides is that underlying the choice of several measures of “central tendency” in classical statistics. A common justification indeed for the ==== of a set of numbers as a “representative statics” for these numbers is that the mean minimizes the sum of the squares of the differences between itself and the represented numbers. Similarly, the ==== of a set of numbers – another widely used measure of “central tendency” – is commonly justified by the fact that it minimizes the sum of the absolute values of those same differences, while the ==== minimizes the degenerate distance between numbers that is 1 if the numbers differ and 0 if they do not. In a similar spirit, it is common in ==== to fit a cloud of points indicating the values taken by a “dependent” variable and a collection of “independent” ones by a specific function whose parameters are “estimated” by minimizing the sum of the (squares of) the discrepancies between the predicted and observed values of the dependent variables. The parametric curve estimated in this fashion is commonly portrayed as “representative” of the cloud of points.====We show herein that the “preference of the majority” is representative in an analogous fashion of the collection of preferences from which it emanates. We specifically show that the preference of the majority minimizes the sum of distances between itself and the preferences for a significant class of distance functions over these preferences which we characterize. We indeed identify the property that any distance on preferences – described as binary relations – must satisfy in order to be minimized by a majoritarian preference. This property happens to be that of ==== of the distance with respect to any three binary relations such that one of them is a Paretian aggregation of the two others. It is common to refer to a preference which is a Paretian aggregation of two others preferences as being “in-between” those two. A distance is additive in this sense if for any two preferences, the sum of the distances between each of the two preferences and a Paretian aggregation of them is always equal to the distance between these two preferences.====Our analysis can thus be seen as a generalization of a literature that discusses the representativeness of the majoritarian preference – in the sense of distance minimization – with respect to the specific ==== (or Kendall) distance initially characterized by Kemeny (1959) and Kemeny and Snell (1962) for linear preferences, and significantly generalized to weak and even non-transitive preferences by Bogart, 1973, Bogart, 1975. It has been known indeed since at least (Barbut, 1980) that the preference of the majority minimizes the sum of pairwise disagreements between itself and all preferences from which it emanates (see Monjardet (2005) for a good synthesis). While this Kemeny distance minimization property of the preference of the majority is very often appealed to in contexts where the preference of the majority is transitive (see for example Demange (2012)), Barbut (1980) has indicated that nothing in the argument was depending upon transitivity. The literature has also established that the majoritarian preference can be seen as the ‘median’ preference in a metric space over preferences in which the metric is the Kemeny distance. For example, Young and Levenglick (1978) have characterized in this fashion all Condorcet consistent rules.====The current paper extends the results on the representativeness of the majority by showing that it holds for the significantly larger class of all distances that are additive – in the sense above – between any three preferences such that one is between the other two. This property of additivity has been used as a primitive axiom, along with others, in all the characterizations of the Kemeny distance that we are aware of (in particular those of Kemeny, 1959, Kemeny and Snell, 1962, Bogart, 1973, Bogart, 1975 and, more recently, Bossert et al. (2016)). We show in this paper that this property of additivity characterizes in fact the much larger class of distances between any two preferences that can be written as a sum of more elementary distances between the pairs of alternatives by which the two preferences differ. While the Kemeny distance is one such distance – which assumes that any two distinct alternatives have a distance of 1 – there are many others that allow different pairs of alternatives to have different positive distances. All such additive distances, and only them, happen to be minimized by the preference of the majority.====The plan for the remainder of the paper is as follows. The next section introduces the notation and the model and provides the main results while Section 3 concludes.",Is the preference of the majority representative ?,https://www.sciencedirect.com/science/article/pii/S0165489621000391,20 April 2021,2021,Research Article,107.0
Techer Kevin,"Univ Lyon, UJM Saint-Etienne, GATE Lyon Saint-Etienne UMR 5824, F-42023 Saint-Etienne, France","Received 19 November 2020, Revised 9 April 2021, Accepted 12 April 2021, Available online 19 April 2021, Version of Record 28 April 2021.",https://doi.org/10.1016/j.mathsocsci.2021.04.003,Cited by (0),"We consider a class of social cost problems in which one polluter interacts with an arbitrary number of potential victims. Agents are supposed to cooperate and negotiate an optimal pollution level together with monetary transfers. We examine multi-choice cooperative games associated with a social cost problem and an assignment (or mapping) of rights. We introduce a class of mappings of rights that takes into account the pollution intensity and we consider three properties on mappings of rights: core compatibility, Kaldor–Hicks core compatibility and no veto power for a victim. We show that there exist only two families of mappings of rights that satisfy core compatibility, while no mapping of rights satisfies Kaldor–Hicks core compatibility and no veto power for a victim.","A large literature has been developed to describe and solve situations known as social cost problems, in which the activity of some agents has harmful effects on others. This paper aims at analyzing precisely the conditions for resolving social cost problems involving one polluter and some potential victims. Two traditions stand out to solve a social cost problem: the Pigouvian and the Coasean traditions. The Pigouvian tradition (Pigou, 1920) advocates a central intervention by means of taxation on the externality. This results in the introduction of the polluter-pays principle whereby the polluter should bear the cost of pollution control and prevention measures. The Coasean tradition (Coase, 1960) challenges this polluter-pays principle. This tradition argues that agents can solve a social cost problem through a bargaining process provided that property rights arewell assigned. The Coase theorem, first formulated by Stigler (1966), summarizes this approach in two properties: first, the ==== states that in the absence of transaction costs, and if property rights are well defined, agents will always reach an optimal agreement by bargaining; second, the ==== or ==== states that the outcome of the bargaining process is independent of the assignment of rights. Throughout this article, we follow the Coase perspective, and we use cooperative game theory to solve social cost problems.====The Coase theorem has been analyzed through the scope of cooperative games by Aivazian and Callen (1981) and more recently by Gonzalez et al. (2019). These articles discuss social cost problems involving more than two agents and consider the Coase theorem in term of non-emptiness of the core. In the present paper, we extend the framework introduced by Gonzalez et al. (2019) which investigates situations in which one polluter interacts with a set of at least two victims. The authors introduce mappings of rights that describe the legal structure of negotiations among agents. Specifically, a mapping of rights assigns to each coalition either the value ====, meaning the coalition is not allowed to negotiate, or ==== if the coalition is allowed to form and negotiate an agreement. This set of mappings of rights can be seen as the set of winning coalition of a proper voting game.==== They propose three properties for mappings of rights: ==== which requires that the core of the cooperative game associated with any social cost problem is non-empty; ==== which requires that a payoff vector in the core ensures a non-negative payoff to each agent; ==== requires that no victim can individually veto an agreement reached by the rest of the society. The authors provide two main results. On the one hand, a mapping of rights satisfies core compatibility if and only if it assigns the rights either to the polluter or to the whole set of victims. On the other hand, no mapping of rights satisfies those three properties at the same time.====Nonetheless, the set of mappings of rights introduced by Gonzalez et al. (2019) is independent of the polluter’s activity level. Either a coalition containing the polluter can negotiate and choose any activity level, or it cannot negotiate. As a result, the model does not take into account the more realistic issue of quotas, which may arise when one wishes to reduce the pollution level. For instance, the ==== defines a pollution quota which is viewed as a limitation on the polluter’s activity. This means that the rights may impose a restriction on the polluter’s activity level as a right of use. In this regard, we introduce the possibility that the rights depend on the polluter’s activity level. This has three main implications for the model: the conditions defining a mapping of rights must be adjusted; the cooperative game must take into account the different activity levels of the polluter; and the solution concept that captures the Coase theorem must be adapted in line with the new setting.====To address these points, we modify the conditions defining mappings of rights. To start with, we impose antitonicity on mappings of rights with respect to the activity level of the polluter. Precisely, if a coalition containing the polluter is allowed to negotiate an agreement for a certain activity level, then the coalition retains the rights whenever the polluter decreases its activity level. Second, we impose monotonicity of a mapping of rights with respect to the participation of victims. If a coalition is allowed to form and negotiate an agreement, it retains the rights when the number of cooperating victims increases. Notice that we can no longer see the set of mapping of rights as generating winning coalitions since a coalition may lose its rights if the activity level increases. Thus we obtain a new class of mappings of rights different from those introduced by Gonzalez et al. (2019). In this class, we pinpoint the mappings of rights that assign the rights to the polluter up to a fixed and regulated activity level. Those mappings of rights reflect the fact that the polluter has to produce up to a quota. When the pollution problem is severe, the only way for the polluter to override this quota is to reach an agreement with the set of victims. For the sake of simplicity, we suppose the polluter has a finite number of activity levels. Each victim can choose whether to participate or not in the negotiation. If the coalition has the right to negotiate, then the polluter proposes a certain activity level and negotiates a binding agreement with victims which agree to participate. These situations where agents have more than one way of acting within a coalition can be modeled using multi-choice games as introduced by Hsiao and Raghavan, 1992, Hsiao and Raghavan, 1993. While for standard cooperative games each agent has two choices: either to join a coalition and fully participate or not participate at all, in multi-choice games each agent can choose its participation level from a finite set of activity levels. Thus, from each social cost problem endowed with a mapping of rights, we can define a multi-choice game. In the same vein as Aivazian and Callen (1981) and Gonzalez et al. (2019), we consider the Coase theorem in terms of non-emptiness of the core of this game. Different extensions of the core have been provided for multi-choice games. Here we choose to retain the one introduced and characterized by Grabisch and Xie (2007), which is the set of payoff vector that are efficient regarding each activity level and such that no coalition has incentive to deviate from an agreement reached by the grand coalition.==== Finally, we naturally extend the properties of core compatibility and Kaldor–Hicks core compatibility from TU-games to multi-choice games. On the other hand, we consider no veto power for a victim regarding the highest activity level of the polluter to fit our new framework.====Our main result extends the subset of mappings of rights which ensure the non-emptiness of the core. We first show that these new mappings of rights that assign a quota on the activity of the polluter are core compatible. Each victim perceives the activity of the polluter as a threat which reduces the incentive for victims to free-ride and ensures the stability of the agreement. Furthermore, any mapping of rights that exclusively assigns the rights either to a subset of victims or to the grand coalition (i.e. the set of victims and the polluter) is core compatible. Since not all mapping of rights are core compatible, this result invalidates the neutrality property of the Coase theorem. We show that the unique way to satisfy core compatibility and no veto power for a victim is to assign the rights to the polluter at its highest activity level. On the contrary, the only mappings of rights that satisfy Kaldor–Hicks core compatibility are those that assign the rights either to a subset of victims or to the grand coalition. Finally, we confirm the impossibility result regarding Kaldor–Hicks core compatibility and no veto power for a victim at the same time.",Stable agreements through liability rules: A multi-choice game approach to the social cost problem,https://www.sciencedirect.com/science/article/pii/S016548962100041X,19 April 2021,2021,Research Article,108.0
"Domingues Dos Santos Manon,Taugourdeau Emmanuelle","Université Gustave Eiffel, ERUDITE, 5 bd. Descartes, Champs/Marne 75454 Marne-La-Vallée, France,CNRS, CREST, ENS Paris-Saclay, 5 avenue Henry Le Chatelier, 91120 Palaiseau, France","Received 17 September 2020, Revised 10 February 2021, Accepted 24 March 2021, Available online 15 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.014,Cited by (1),This paper shows that ,"In France, Portuguese immigrant workers are a singular community. They are less likely to be unemployed and are more concentrated in the construction industry than their peers in other communities, immigrant or native, and their children are also more likely to work in the construction industry. Why are Portuguese workers so distinctive? Are these two distinguishing features of Portuguese workers linked? If they are, how? And are these links efficient? The present contribution provides an original and simple matching model that demonstrates that these stylized facts are likely explained by the density of the social network in the Portuguese community. The density of the Portuguese social network facilitates the propagation of information on job offers, favoring employment. However, this dense social network also leads to the concentration of Portuguese immigrants in the relatively poorly productive industries where they were initially employed.====Currently, the unemployment rate of Portuguese men in France is 5%,==== compared with 9% in the native population and 18% in other immigrant communities. Levels of qualification, which are usually the main explanation for unemployment rate differentials, are not correlated in this case, the proportions of unqualified workers being 60% among Portuguese men, 35% among other immigrants and 17% among natives. Among unskilled male workers, the unemployment gap is 12 percentage points between Portuguese and native workers and 18 percentage points between Portuguese and other immigrant workers. Portuguese workers are obviously also distinguished in terms of additional observed or unobserved characteristics thought to favor productivity, thus making them more employable. This may be supported by the work of Bauer et al. (2002). Using a dataset provided by the German Labor Office, these authors argue that Portuguese immigrants are positively self-selected in terms of unobserved skills compared with the German population. It may also be that they are driven to work harder by a relatively greater desire to return to their home country. Indeed, more than 30% of Portuguese workers declare that they intend to return to their home country, compared with just under 24% among immigrants in general (Domingues dos Santos and Wolff, 2010). Hence, according to Dustmann (2000), since leisure time is relatively more expensive for temporary than for permanent immigrants, the more temporary nature of Portuguese immigration is likely to reduce the reservation wages of these workers and induce them to work harder while abroad. Nevertheless, these arguments do not explain why 57% of unskilled Portuguese males work in construction compared with just 24% of other immigrant men and 13% of natives. Indeed, Portuguese immigrants are not significantly more likely to have worked in the construction industry before migrating than other immigrants (Domingues Dos Santos, 2005), which excludes a migration selection bias in terms of sector-specific human capital. Moreover, the children of Portuguese immigrants are more likely to choose short and vocational studies and to train in construction than are other children with comparable socio-economic backgrounds (Brinbaum and Kieffer, 2009, Lainé and Okba, 2005).====The aim of this paper is to provide an original and simple matching model that highlights that the employment and industry concentration of the Portuguese community can also be explained by the (in)efficiency of their social network. We argue that the Portuguese community in France forms a particularly dense social network, which substantially facilitates the employment of its members, especially in the construction sector where the community was originally employed. This idea is notably based on the following observation: 55% of Portuguese employees found their job through friends and relatives, compared with just 39% of other immigrants and 32% of natives who did likewise. It seems therefore that Portuguese workers are more likely to learn about job opportunities through their network of personal contacts than through more formal methods such as employment agencies or direct applications. This social network efficiency can force a trade-off between job opportunity and job productivity and eventually lead to endogenous concentration of the community in relatively poorly productive activities.====The central role of social networks in the labor market and in migration patterns is well documented, even if these two strands of the literature are somewhat disconnected. First, regarding the role of social networks in the labor market, most of the related literature emphasizes the importance of friends and relatives as sources of employment information. In particular, some studies have analyzed the endogenous formation of these informational networks: following the seminal work of Granovetter (1974) and Boorman (1975), these studies aim to understand the size and structure of such networks (Jackson and Wolinsky, 1996, Calvó-Armengol and Jackson, 2002, Calvó-Armengol, 2004). Other studies have focused on the choice between alternative job search methods, such as direct application, advertisements, job centers or through friends and relatives, and test their efficiency in terms of outflow from unemployment. Most of these empirical studies conclude that the friends and relatives route is the most effective (Holzer, 1988, Blau and Robins, 1990, Addison and Portugal, 2002). From a theoretical perspective finally, the efficiency of job-worker matching has been analyzed when information about job vacancies can be gathered through social contacts in order to determine the critical size of social networks (Calvó-Armengol and Zenou, 2005). Following these contributions, social networks are generally thought to increase unemployment outflow rates and wages. However, these theoretical papers do not provide evidence of the empirical relevance of their results and say nothing about the distribution of the labor force between industries in such a context. Second, regarding the role of social networks in migration patterns, most of the related literature addresses the question of migrant clustering. Prominent examples of migrant clustering are the concentrations of Turks in Germany, Italians in Argentina, and Greeks in Australia. The prevailing explanation for ethnic concentration is the existence of beneficial network externalities. The settling of previous migrants encourages the arrival of new ones in three ways: they provide information about the regional labor and rent markets, they are likely to help new arrivals settle, and they increase the amount of ethnic goods available. This literature focuses on two issues: the process that drives the spatial concentration of immigrants (Bartel, 1989, Carrington et al., 1996) and the consequences of this spatial clustering (Chiswick and Miller, 1995, Chiswick et al., 2002). Nevertheless, the connections between immigrants’ informational networks, job search activities, migratory history, and concentration in particular industries have not been explicitly formalized.====Our paper departs form the existing literature in two ways. First, contrary to the migrant clustering literature, we do not focus on spatial clustering but on sectoral concentration in a given host country. Second, we extend the literature on the effects of social networks on the labor market by analyzing the endogenous concentration of migrants in a specific sector of activity. We also extend the standard modeling approach of Pissarides (2000) by giving the network a very simple explicit structure, and extend Calvó-Armengol and Zenou (2005) by allowing for the presence of two sectors with their own social networks.====Based on the seminal model of Pissarides (2000), this paper demonstrates that the transmission of job information within a community improves the matching efficiency, which lowers unemployment and increases wages (Section 2). When workers choose their allocations between sectors, this information asset is likely to induce sector concentration even when sectors are perfectly symmetric. In this case, we derive the condition allowing for lower average unemployment and higher average welfare in the equilibria with sector concentration (Section 3). Whereas asymmetry with respect to sector productivity favors concentration in the most productive sector, entry cost asymmetry favors concentration in the sector where fixed entry costs are the lowest. Ultimately, we derive the conditions under which workers are trapped in the least productive sector (Section 4). This model can then be used to partially interpret the migration history of the Portuguese community and their relative situation on the labor market in France (Conclusion).","Social network, unemployment and sector trap: A theoretical model explaining the case study of Portuguese immigrants in the French labor market",https://www.sciencedirect.com/science/article/pii/S0165489621000342,15 April 2021,2021,Research Article,109.0
"Benhabib Jess,Hager Mildred","New York University, Department of Economics, 19 West 4th Street, 6th Floor, New York, NY, USA","Received 11 September 2020, Revised 18 January 2021, Accepted 24 March 2021, Available online 15 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.017,Cited by (3),We study an equilibrium model of “revenue diversion” by management and its effects on talent allocation and the earnings distribution. In our ,"Edgeworth (1917), responding to Pareto’s influential work, suggested that an appropriate choice of the distribution of “talent”, coupled with a mapping from talent levels to income levels could, by a change of variables, match to the skewed and power-tailed empirical distribution of income.==== Here, talent may be interpreted broadly as any characteristic that can augment income, including for example education, intelligence, experience, social connections, shrewdness, or business savvy, or a composite of such factors.====
 Fréchet (1939, 1945) showed that a Laplace distribution of talents, coupled with a linear map of log earnings increasing in talents, will yield a hump shaped distribution of earnings, increasing until the median and Pareto above the median,==== close to what is observed empirically. However, talent itself is not directly observable, leaving room for other factors such as revenue diversion to influence income inequality. We generalize Edgeworth’s and Fréchet’s framework to introduce occupational choice as well as the potential for unproductive “revenue diversion” for a richer characterization of the distribution of earnings. Income inequality, or an increase thereof, can now still result from a change in the talent distribution, but equally well result from a better ability to divert income. In our framework, rising managerial pay can in this sense be related to productivity increases, but also to changes in corporate governance or rising monopoly power, as has been argued in the recent literature (see below).====We study an equilibrium model of “revenue diversion” by management, and its effects on the allocation of talent and the distribution of earnings. Agents distribute themselves across two occupations, which we will broadly denote as “workers” and “managers”. In contrast to workers, the managers’ entrepreneurial talent is directed towards both productive as well as unproductive and rent-seeking activities, and any occupation where this is feasible might be thought of as ‘managerial’ (for example, Murphy et al. (1991) simplified the productive and unproductive occupations as “engineers” and “lawyers”, see also literature overview below). The equilibrium allocation of talent depends on earnings levels across occupations. Earnings on the other hand depend on the extent of revenue diversion by managers, as well as the productivity differences across occupations that increases with talent levels. Without revenue diversion, factors earn their marginal products that depend on their talent levels as well as their choice of occupation, and the allocation is efficient. With revenue diversion the earnings of managers and workers can diverge from their marginal products, and the allocation is not efficient anymore. Even though the share of revenue “diverted” may only be a small fraction of overall revenue, it accrues to a small fraction of the population, and therefore can still have noticeable effects on earnings inequality and the right tail of the earnings distribution.====In Section 2 we present our model and show that in both the “revenue diversion” and “no revenue diversion” cases the overall distribution of income will be hump shaped, and will be given by a Pareto distribution above the median.====In Section 3 we will loosely calibrate some cases to match the Pareto tail of income distribution, the 90–50 ratio of the income distribution, and the fraction of managers in the population resulting from optimal occupational choice in equilibrium. Given a talent distribution and productivity differences across occupations conditional on talents, the Pareto tail will be thicker under revenue diversion. Nevertheless, as shown in the calibration Section 3, identifying the precise mix of revenue diversion from the minimal targeting of the moments of the earnings distribution and the ratio of managers in the population is not possible unless we have direct information on productivity differences across occupations conditional on talent levels, or of the distribution of talent. Otherwise various mixtures of revenue diversion and occupational productivity differences conditional on talent levels, together with some distribution of talents can equally well match the features of the earnings distribution that we target. For obvious reasons, direct measures of revenue diversion are not observable and are not reported by firms. But under the assumption that the distribution of talent had been unchanged between 1987 and 2005, the full increase in income inequality would be explained by a variation of only 2% in our diversion parameter (based on data for the tail index provided by Atkinson et al. (2011)).====In a brief extension in Section 4, in addition to workers and management, we also introduce capital under a neoclassical production function, and allow management to divert revenue from both workers and capital. We also consider the case where worker and manager outputs are complementary, and are aggregated into a final aggregate output under a CES technology.====As discussed in various papers cited below in the review of the literature, how much management or CEO earnings are accounted for by performance, as opposed to by management setting the compensation of labor and shareholder capital below their marginal products, can only be indirectly estimated, for example as in the papers of Bertrand and Mullainathan (2001), Bivens and Mishel (2013), or Morck et al. (2005).====One indication of the possibility of increased revenue diversion is the recent rise in CEO and management compensation not accompanied by commensurate productivity growth and the resulting increase in income inequality. These recent changes may be due to changes in corporate governance or rising monopoly power. As also noted in some recent papers, increases in market concentration under monopolistic competition or monopsony can drive a wedge between marginal products of factors and factor prices. This may explain some of the growing inequality of incomes, and the fall in the labor share, documented for example by Autor et al. (2017), Loecker et al. (2020), Boar and Midrigan (2019), Azar et al. (2019), and Benmelech et al. (2018). In a recent empirical paper Greenwald et al. (2020) conclude that 43% of the rise in the real equity wealth created by the U.S. corporate sector came at the expense of labor compensation.====Indeed, our way to model revenue diversion can be added on top of other mechanisms generating income inequality, such as globalization, or skilled biased technical change that increases the remuneration of talent. For example, Gabaix and Landier (2008) postulate that talented executives are matched with larger firms  with executive log earnings increasing with log firm size, generating a Pareto distribution of income. Revenue diversion could increase the thickness of the right tail of the distribution of income on top of these.","Revenue diversion, the allocation of talent, and income distribution",https://www.sciencedirect.com/science/article/pii/S0165489621000378,15 April 2021,2021,Research Article,110.0
"Modesto Leonor,Seegmuller Thomas,Venditti Alain","UCP, Catolica Lisbon School of Business and Economics, Portugal,IZA, Germany,Aix-Marseille Univ., CNRS, AMSE, France,EDHEC Business School, France","Received 15 September 2020, Revised 12 February 2021, Accepted 24 March 2021, Available online 15 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.011,Cited by (1),", international borrowing is constrained and households have taste for domestic public debt. Therefore, capital, public debt and the international asset are not perfect substitutes and the economy is characterized by an investment multiplier. Whatever the level of the debt-output ratio, the existing BGP features expectation-driven fluctuations. If the debt-output ratio is low enough, there is also a second BGP with a lower growth rate. Hence, a lower debt does not stabilize the economy with credit market imperfections. However, a high enough taste for domestic public debt may rule out the BGP with lower growth. This means that if the share of public debt held by domestic households is high enough, global indeterminacy does not occur.","Following the last financial crisis, debt levels have increased dramatically in many advanced countries raising the problem of debt sustainability. The control of the growth rate of public spending has then become a major concern for economists and policy-makers, while the average debt-to-GDP ratio has increased in one decade since the 2008 crisis by more than ==== percentage points. Yet, the situation is quite heterogeneous across countries. While Greece has faced huge sustainability problems after its debt-to-GDP ratio exceeded 150% in 2010, Japan does not seem to face similar difficulties with a debt-to-GDP ratio that has recently reached 250%. These various experiences illustrate the idea that debt sustainability is a complex question.====This is in particular highlighted in the paper by Collard et al. (2015), which provides a model where the amount investors are willing to lend to a country’s government depends on its expected primary surplus, the country level and volatility of growth, and the level of debt they expect the government will be able to raise in the future with the purpose of servicing the debt it seeks to raise today. Based on that, they propose a measure of maximum sustainable government debt for advanced economies that strongly varies across countries.====In this paper we address this complexity of public debt financing through a different channel, re-examining two main questions. On the one hand we study the relationship between the level of debt and growth, and on the other hand we highlight the link between macroeconomic stability and debt through expectations. In comparison to the recent literature, we introduce three additional dimensions. First, while most existing papers are concerned with closed economies (Chéron et al., 2019, Futagami et al., 2008, Maebayashi et al., 2017, Minea and Villieu, 2013), in this paper we consider instead a small open economy. Like us, Morimoto et al. (2017) also addressed the case of a small open economy. However, these authors assume that the different assets: productive capital and domestic and external debt, are perfect substitutes. Therefore, in their work there is no portfolio choice for the households and only the level of total asset holdings plays a role.====Second, following empirical evidence (Chung and Turnovsky, 2010, Manova, 2008), we assume that borrowing on the international market is limited. The loans provided by the rest of the world are constrained and collateralized by the capital invested in the home country.==== As in Fahri and Tirole (2012), Kiyotaki and Moore (1997) or Hirano and Yanagawa (2017), such a credit constraint generates an investment multiplier, which will play a key role inducing expectation-driven fluctuations.====Finally, empirical evidence also highlights that in the Euro area and in the US around 50% of public debt is hold by non-residents, while in Japan and in the UK only 4% and 31% respectively of the national public debt is hold by non-residents. This means that almost the totality of Japanese debt and the majority of the UK debt are held by domestic institutions. Note that, such a characteristic could provide an explanation for the strong sustainability of the Japanese debt. In order to take into account this variety of configurations, we introduce domestic public bonds in the utility function in order to be able to measure the degree of the ====, and quantify its effects.====The aim of this paper is to introduce these three features into a dynamic model with infinitely-lived households where, following Barro (1990), endogenous growth is obtained through public expenditures that improve production and are financed both by taxes on income and public debt. Considering a small open economy, loans can be contracted from the rest of the world. However, because of a credit constraint, where capital plays the role of collateral, and of taste for domestic public debt, there is non-substitutability between the different assets: productive capital, public debt and an international asset. Finally, in accordance with Minea and Villieu (2013) or Morimoto et al. (2017), we assume that public debt follows a stability constraint.====We discuss the effect of public debt on growth and whether a lower debt-output ratio may be stabilizing, by ruling out equilibria multiplicity. Our results are quite different from those provided by the previous literature. When the debt-output ratio is high enough, we show that there is a unique BGP which is locally indeterminate. This striking conclusion is explained by the possibility to borrow on the international market and the existence of an investment multiplier. On the contrary, when the debt-output ratio is low enough, two BGPs may coexist: there is also a BGP with a lower growth rate which is saddle-point stable, implying global indeterminacy and coordination problems. These BGPs can be both characterized by a primary deficit, or the lower one can be characterized by a primary surplus. In this last configuration, where the interest rate is above the growth rate, household taste for domestic public debt plays a key role. When it is high enough, the proportion of domestic public debt is too big to satisfy the household budget constraint when growth is low, which rules out the existence of the BGP with the lowest growth rate. However, taste for domestic public debt does not modify the stability properties of the BGPs. This means that, while a high share of public debt held by domestic households does not affect the local indeterminacy of the BGP with the highest growth rate, it may eliminate the multiplicity of BGPs, i.e. it rules out global indeterminacy.====Our results have also clear policy implications. A lower debt-output ratio does not stabilize endogenous fluctuations. Indeed, the BGP with the highest growth rate is locally indeterminate whatever the level of the debt-output ratio, while a sufficiently low debt-output ratio may promote the multiplicity of BGPs, i.e. may be a source of global indeterminacy. This conclusion is completely different from the one obtained in a small open economy with perfectly substitutable assets in which a sufficiently low public debt stabilizes expectation-driven fluctuations (Morimoto et al., 2017). In a closed economy with perfectly substitutable assets, and a log-linear utility in consumption as in our paper, multiplicity of equilibria is also ruled out for any level of the debt-output ratio (Minea and Villieu, 2013).====The source of multiplicity of equilibria in our model is related to two key features of our framework: ==== the inflow of international assets and ==== the existence of non substitutable assets due to a credit constraint with collateral. Indeed, in a closed economy with perfectly substitutable assets, expectations of higher public expenditures cannot be self-fulfilling. To finance higher public expenditures, a larger debt emission is indeed required, which crowds out private investment having a negative impact on future income. It then prevents the existence of expectation-driven fluctuations. In contrast, in our framework, as a small open economy can import international funds, such a crowding-out effect is no longer relevant. Therefore, a higher public spending is now compatible with an increase of productive investment. The resulting effect on growth is magnified by the collateral role of capital which generates an investment multiplier. Higher investment and growth sustain an expected increase of public spending, which is not possible when assets are perfectly substitutable (Morimoto et al., 2017).====Given our results on local and global indeterminacies, we finally provide a simple numerical illustration in order to check whether the preference of households for domestic bonds can explain the heterogeneity of experiences across some OECD countries using specific calibrations based on empirical evidence. We especially illustrate that while both Greece and Japan are characterized by high debt-output ratios, they do not feature the same dynamics. Indeed, Greece appears to be characterized by a preference for domestic bonds which allows the multiplicity of BGPs, whereas it is not the case for Japan. This may explain why Japanese debt is more stable than the Greek one.====This paper is organized as follows. In Section 2, we present the model. In Section 3, we analyze the existence and the multiplicity of BGPs. We also discuss the features of these BGPs and analyze comparative statics. In Section 4, we analyze the stability of BGPs. We provide a numerical illustration of our results in Section 5. We conclude in Section 6, while most technical details are relegated to an Appendix.",Growth and instability in a small open economy with debt,https://www.sciencedirect.com/science/article/pii/S0165489621000317,15 April 2021,2021,Research Article,111.0
"de la Croix David,Doepke Matthias","IRES/LIDAM, UCLouvain, Belgium,Northwestern University, United States of America","Received 18 September 2020, Revised 2 March 2021, Accepted 24 March 2021, Available online 15 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.005,Cited by (5),A long-standing challenge for ,"Following the seminal papers by Becker (1960) and Becker and Lewis (1973), a lot of research has been devoted to economic models with endogenous fertility, where population growth is determined as a function of the optimizing choices of households. While these models allow us to better understand how fertility is determined and interacts with other economic variables, they do not make prescriptions for optimal population policy. For such a purpose we need to define a social welfare function. Unfortunately, the standard economic approach to welfare does not carry over to situations where the number of people is a choice variable. When comparing two allocations with different population sizes, one has to take a stand on how to value the utility of people who are alive in one allocation but not in the other. Implicitly, this amounts to making assumptions on the utility of not being born.====To illustrate the difficulties involved in this step, consider two well-known existing welfare criteria: average utilitarianism and total utilitarianism. In average utilitarianism, the social welfare function is given by the average utility of everyone who is alive. Implicit in this criterion is the assumption that the utility of not being born is equal to the average utility of those who are alive. To see why this is so, compare two allocations, one in which two people are born with a utility of two each, and one where in addition to the two people with utility two a third person exists with utility equal to one. According to average utilitarianism, the first allocation is to be preferred, since average utility is higher. But no one who is alive in the first allocation is worse off in the second, we merely added a third person with utility equal to one. If we prefer the first allocation, implicitly we are stating that the world would be better off if the third person had not been born. The assumption that a new person has value only if her utility is at least equal to the average utility of the existing people seems arbitrary, and leads to some unattractive conclusions. For example, an allocation with a single person in existence with utility equal to one would be preferred to an arbitrarily large population with average utility ====, no matter how small ==== or how large the population is.====As an alternative, consider total utilitarianism (Dasgupta, 1969), where the objective is to maximize total utility in the economy. Under total utilitarianism, in our first example the second allocation is to be preferred; it always improves welfare to add people with positive utility. Implicitly, therefore, we are now assuming that the utility of not being born is equal to zero. But assigning a utility of zero to unborn people is just as arbitrary as any other real number.==== With total utilitarianism, setting the utility of the unborn is therefore a key step in defining the welfare criterion, but it is not clear how that should be done despite Sophocles’ claim that “Not to be born is, beyond all estimation, best”. (Jebb, 1906)====The central difficulty is that the utility of not being born cannot be determined by introspection. When we formulate social welfare functions for a fixed population size, we only need to compare the welfare of people who all are alive. At an intuitive level, we carry out such comparisons by putting ourselves in someone else’s shoes, i.e., imaging what a given person’s life would be like and how we would evaluate that person’s welfare compared to another person’s, whose welfare we can equally evaluate though such introspection. In fact, we constantly carry out such comparisons regarding our own lives. When we face life decisions, we imagine what life would be like down either path from the fork in the road. After giving it thought, we decide which path we prefer, and what we would be willing to give up to get the one rather than the other. Thinking about other people’s lives in the same way is a small step to make.====When we aim to formulate a welfare function that allows for comparisons across different population sizes, the approach of putting ourselves in other people’s shoes no longer works. Evaluating welfare now involves evaluating how we feel about someone who is born in one allocation, but not in another. No one currently alive has had the experience of never being born. Is it, then, impossible to find an intuitive criterion, based on common human experience, on which a plausible social welfare function can be based?====We argue that, in fact, there exists a view of the world under which the utility of the unborn can be assessed even by people who are currently alive. The key is to find an interpretation where being alive and being unborn are not mutually exclusive, but merely different states which are experienced by one and the same being over time. Such a view of the world does exist and is in fact held by a sizeable fraction of the world’s population. What is required is a world with a fixed supply of souls, who get reincarnated from time to time in different human bodies. For such a soul, determining the optimal population level does not amount to a drastic “to be or not to be” question; instead, a smaller world population merely amounts to a longer wait to the next incarnation.====The major advantage of using the souls approach is that all the relevant tradeoffs can be assessed by introspection. For example, a soul, or equivalently a human believer in reincarnation contemplating her or his own future incarnations, should have no difficulty in forming a preference over the options of being born, say, every 10,000 years as a king, or every 100 years as a farmer. Making such comparisons also helps resolve another challenge in evaluating social welfare, namely to decide on the social discount rate with which to discount the utility of future generations. The social discount rate arises implicitly from the souls’ preferences over different combinations of rates of reincarnation and the associated life experiences.====Of course, the applicability of the souls approach would be limited if it only worked for those people who actually do believe in reincarnation (even though there are many such people====). We will argue, however, that our resulting criteria should also be plausible for others who do not expect their soul to live forever. We view our approach as analogous the “veil of ignorance” metaphor used in traditional social choice theory for fixed populations.==== Even in traditional welfare economics, we face the problem of comparing things without an direct basis for the comparison. Every human lives just a single life, so nobody has actual experience of what it is like to be another person. Nevertheless, we approach welfare evaluations by imagining, counterfactually, that we could be someone else. We imagine a situation behind a veil of ignorance where we do not know whose life we will get to live, and in fact have an equal probability of ending up as any given person. Of course, this could never happen in actual fact. Nevertheless, this metaphor is widely agreed upon as a basis for making welfare evaluations, on the basis that humans are fundamentally similar to each other and can therefore imagine quite well what it would be like to be in someone else’s shoes. Based on this imagination, standard social choice theory develops views of what would be a good world to live in. We take the same approach of imagining a situation which, even though it may not be a technically accurate description of the world, nevertheless provides a basis for evaluating social welfare across different population sizes on the basis of human introspection.====In the following section, we introduce the economic environment and define our welfare criterion. In Section 3, we point out the crucial role played by the discounting of future incarnations in soul-based welfare, and consider alternative settings in which discounting may depend on the probability of incarnation or on the utility derived in a given incarnation. In Section 4, we demonstrate the implications of soul-based welfare criteria for the income–population tradeoff in an economy with fixed resources, and argue that our preferred criteria have attractive properties compared to leading alternatives. In Section 5, we relate our results to the challenges arising from environmental degradation faced by humanity in the near future. Section 5 concludes the paper.",A soul’s view of the optimal population problem,https://www.sciencedirect.com/science/article/pii/S0165489621000251,15 April 2021,2021,Research Article,112.0
"Bosi Stefano,Lloyd-Braga Teresa,Nishimura Kazuo","EPEE, Université Paris-Saclay, France,Universidade Católica Portuguesa, Catolica Lisbon School of Business and Economics, Portugal,RIEB, Kobe University, Japan,RIETI, Japan","Received 17 September 2020, Revised 11 March 2021, Accepted 24 March 2021, Available online 13 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.013,Cited by (7),"Investments in human capital are individual and collective choices carrying significant external effects. Educated parents and friends accelerate our own human capital accumulation. Skilled colleagues at work increase our own productivity. Sharing experiences with cultured people is gratifying by itself.====We introduce human capital externalities in a stylized model à la Uzawa (1965) and we find that growth can be no longer balanced and the equilibrium can be globally indeterminate.====Labor supply goes to one and capital reaches an upper ceiling in a finite lapse of time above the initial critical value of labor supply ensuring the balanced growth path.====Under a constant tax rate, the government should set a positive rate to speed up human capital accumulation during transition to the capital ceiling.","The notion of human capital was introduced by Smith (1776) and later reappraised by Pigou (1928), while the modern theory of human capital in terms of education and health dates back to Schultz (1961) and Becker (1964). In particular, Ben-Porath (1967) paved the way to the research on educational attainment, on-the-job training and wage growth over the life cycle, while Uzawa (1965) pioneered the branch of growth theory based on human capital accumulation. The potential of human capital as engine of perpetual growth was fully recognized in Rosen (1976) empirical contribution and Lucas (1988) influential model. After Lucas model, the literature developed fast with a vast amount of contributions both empirical and theoretical.====Broadly speaking, investments in human capital affect not only individuals, but society at large. Growing in a family with educated parents, or studying in a classroom with motivated classmates, increases the speed at which human capital accumulates. Working where human capital is denser increases our own productivity through the examples and guidance from other people. Finally, living with educated and fit people is gratifying in itself.====Jacobs (1970) and Acemoglu (2009) focus on the role of human capital externalities suggesting that the concentration of economic activity in cities is both an effect of these externalities and an engine of economic growth through the exchange of ideas between workers and entrepreneurs. Other authors consider the effects of human capital on human capital accumulation. For instance, De la Croix and Doepke (2003), Tamura (2001), and Cervellati and Sunde (2005) concentrate on education and human capital formation; while De la Croix and Licandro (1999), and Kalemli-Ozcan et al. (2000) focus on health and human capital accumulation. Very few papers focus on the impact of human capital as a positive externality on human capital accumulation. Among them, Mookherjee et al. (2010) study an overlapping generations economy where parents decide whether or not to educate their children, and educational decisions are affected by location. Local complementarity in investment incentives stems from aspirations formation, learning spillovers and local public good. Cavalcanti and Giannitsarou (2017) focus on positive network externalities (local peer) on human capital accumulation. They reconsider the trade-off between growth and inequality incorporating networks into an endogenous growth model with overlapping generations.====Human capital can also affect technology and preferences.====Following Uzawa (1965), Lucas (1988) considers the role of human capital on labor productivity and, then, as an engine of perpetual growth. In his influential contribution, Lucas also introduces human capital as a potential productive externality in the production function.====With regard to the impact of human capital on preferences, few sociologists find some evidence on the role of education on life enjoyment, while economists seem to neglect the role of human capital on welfare. Among the sociologists, Ross and Wu (1995) find that better educated agents have a more satisfying job; they are in healthier conditions and are more in control of their lives. Among the economists, Finkelstein et al. (2013) observe a complementarity between health and consumption demand; while empirical evidence suggests that human capital does increase the marginal utility of consumption. From a theoretical point of view, Bosi et al. (2021) study a market economy with human capital in the utility function and look at the dynamic implications in the short and long run. They provide also some evidence on the positive impact of education on utility. However, they do not consider human capital as a positive externality, but only as an individual choice. In contrast, we also take also into account the positive external effects of human capital.====All these papers focus on specific effects of human capital on growth through preferences or technology. But none considers the multiple facets of human capital together and the cross effects of externalities on preferences and technology.====The added value of our contribution is threefold. Not only we introduce the human capital in preferences, technology and the speed of capital accumulation in a very standard growth model, and we study the joint effects, but also we explore all the external effects existing in this context. Despite the complexity of interactions, we are able to compute the explicit trajectories and, thus, to provide a global stability analysis. To the best of our knowledge, our contribution is the first to address the complex issue of human capital externalities as a whole and to characterize explicitly the global dynamics in terms bounded and unbounded growth. The choice of basic fundamentals (Cobb–Douglas technology, logarithmic preferences and isoelastic external effects) allows us to solve the system of differential equations driving the human capital accumulation and, therefore, to have a complete picture of the economy in the short and the long run. The explicit solution we obtain is a methodological progress with respect to the existing literature.====Specifically, we introduce different types of human capital externalities within a stylized model à la Lucas (1988), and study how these interact with the choice of time to work versus time to accumulate human capital, as well as their effects on the stability properties of equilibria and growth. To highlight the role of human capital, we simplify our analysis and disregard physical capital==== as well as the possibility of leisure or unemployment: individuals spend their time working or studying (that is accumulating human capital). In the word history, a secular stagnation can take place when individuals do not work much and do not study much, slowing both the accumulation of physical and human capital (see Eichengree, 2015 among others). We do not consider such slow-down in capital accumulation, even if, in our model, growth can be bounded.====Our model is articulated in two parts.====First, we study the effects of human capital externalities on productivity, human capital accumulation and preferences. Second, we introduce an educational fiscal policy to increase the speed of human capital accumulation.====To begin with, we consider a simplified framework where human capital externalities do not affect human capital accumulation.====In this case, the Balanced Growth Path (BGP) exists only for a critical value of the initial labor supply. Below this value, trajectories satisfying the dynamic system violate a necessary transversality condition; while, above such a critical value, the equilibrium labor supply converges to one (its maximal amount), implying that human capital converges to a stationary ceiling. After solving the system of differential equations, we provide the trajectories of human capital and labor supply as explicit functions of time. In the particular case of the BGP, the growth rate increases with agents’ patience, learning easiness, productivity and propensity to accumulate human capital.====Conversely, positive capital externalities on capital accumulation prevent the BGP. In this case, we prove that labor supply reaches the value one in a finite lapse of time and, from then on, the households spend all the time at their disposal working and, therefore, human capital stops growing.====In both cases, with or without effects of capital externalities on capital accumulation, the equilibrium can be globally indeterminate because labor supply is a non-predetermined variable. This implies that two economies with identical preferences, technologies (of production and human capital accumulation) and initial endowment of capital accumulation, may experience different growth paths.====Subsequently, we show that taxation matters during transition. The positive effects of educational public spending externalities and, indirectly, of human capital on capital accumulation, prevents the BGP, as seen above.====When the initial labor supply is below a critical value, we cannot exclude that the decrease in labor supply to zero and the unbounded increase in human capital compensate the loss in working time to ensure an optimal consumption level. When the initial labor supply exceed a critical value, labor supply increases to one in a finite lapse of time, agents stop to invest in education and the human capital attains a stationary ceiling. As above the equilibrium can be globally indeterminate.====These results raise a question on the optimality of this fiscal policy. A government constrained to announce, with commitment, a constant tax rate forever, will fix a positive tax rate even in the case of bounded growth (ceiling). Public spending (and, so, taxation) matters during transition from the initial stock of capital to the ceiling, speeding up the accumulation of human capital but it has no effect once attained the ceiling. At best, this fiscal policy implements a second best because of market imperfections (positive human capital externalities) and the announcement of a fix tax rate with commitment.",Externalities of human capital,https://www.sciencedirect.com/science/article/pii/S0165489621000330,13 April 2021,2021,Research Article,113.0
"León-Ledesma Miguel,Orrillo Jaime","School of Economics and MaGHiC, University of Kent, and CEPR, Kennedy Building, Canterbury CT27FS, UK,Universidade Católica de Brasília, Dept. de Mestrado de Doutorado Economia, QS 07, Lote 01 – EPCT, Office K-244, Brasilia–DF, ZIP: 71966-700, Brazil","Received 14 September 2020, Revised 31 January 2021, Accepted 24 March 2021, Available online 10 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.010,Cited by (1),We propose an extension of the incomplete markets ,"One of the key features of production activities is that they take time. In order to produce, firms have to carry out investments that are funded by either equity or debt. If there are financial frictions in the economy, this then leads to a close link between finance and the level of production of firms and, therefore, real economic activity. The impact of financial variables on the real economy has been the subject of a large literature in macroeconomics. This is especially so since the advent of the Global Financial Crisis. Although that particular crisis originated mainly in sub-prime lending for housing market investment, business default or bankruptcy can also lead to large disruptions in economic activity and to the amplification of shocks. This is even more relevant in the face of the unprecedented Covid-19 crisis that has put business activities in certain sectors under enormous stress. Fig. 1 shows the evolution of business bankruptcy and delinquency rates in the US, which displays a clear counter-cyclical pattern. Both measures increase substantially during recession periods.==== Thus, it is important to develop theoretical frameworks that put business default and bankruptcy at center stage in an economy with incomplete financial markets.====Our paper has two main objectives. First, to introduce firm default in a general equilibrium model of incomplete markets (GEI), collateral constraints, and production, and answer the question whether this setting is consistent with equilibrium. Note that the production aspect of our model has not been addressed in earlier studies with default and collateral. This is important, as the model can generate the counter-cyclical default observed in the data. Secondly, and our main objective here, to analyze the financial polices of the firm in the presence of collateral constraints and bankruptcy. If financial policies like the composition of liabilities between debt and equity matter, they can affect real variables such as investment leading to a transmission mechanism between finance and real economic activity.====We present two main results. First, we show that default and bankruptcy are consistent with the orderly functioning of markets.==== That is to say, there is an equilibrium where firm default exists, although bankruptcy is a disequilibrium signal. Second, we show that, unless we impose very restrictive assumptions on the trading of portfolios and on the way borrowers repay their debts, the financial policies of the firm matter for real outcomes. Specifically, the Fisher Separation Theorem and the Modigliani–Miller Theorem are not satisfied in this context. Although our analysis of financial polices of the firm is carried out in the context of a one-good economy, we deal with the problems of both existence and efficiency in a more general multi-good multi-agent economy with two periods. This is because efficiency in a one-good economy turns out to be trivial. Even though equilibrium efficiency does not hold in an incomplete markets multi-good multi-agent economy, we do obtain a constrained efficiency result. Thus, for the sake of completeness, we retain the multi-good framework when dealing with equilibrium existence.====The model makes several key assumptions regarding ownership structure and repayment incentives:====In models without production, the existence of collateral provides insurance for lenders. However, with production and the dependence of deliveries on second period output, problems of asymmetric information reappear because deliveries depend on the level of production. More precisely, in case of a devaluation of the collateral value below payoff, we would have a moral hazard problem as entrepreneurs have an incentive to reduce production and let firms go bankrupt. In order to incentivize high production and prevent bankruptcy, we introduce a “default” insurance market similar to the one in Araujo et al. (2000). In Araujo et al. (2000) this device is introduced to give an incentive for borrowers to offer high levels of collateral given that, in their model, collateral is endogenous.====On the other hand, in models with production, lenders could be over-pessimistic about repayments as the collateral could suffer a severe loss of value in the future. This would then lead to a trivial equilibrium with zero production. When there is trading in financial markets, we can resolve this problem by making asset receipts anonymous assuming a type of ====. The production (and collateral) of a consumer/firm backs all the assets they issue. Because of anonymity, lenders have to anticipate a default rate on their whole portfolio that will depend on the average level of production of firms in the economy. This will depend on the state of nature and, hence, aggregate economic activity (i.e. default is counter-cyclical). In order to prevent a zero production equilibrium our default insurance market will once again play an important role as our model does not consider any intermediaries or insurance companies to protect lenders from eventual default.====To achieve our main goal of analyzing the financial policies of the firm, we consider the finance economy derived from the corresponding multi-good economy. Specifically, we study under which conditions the Fisher Separation and the Modigliani–Miller Theorems hold in this sole proprietorship environment. Although we should not expect these theorems to hold in our setting because of the nonlinearity or nonadditivity of deliveries in relation to short positions,==== it is still important to study the conditions leading to their failure. This is because the validity of these theorems is not only important from the perspective of corporate finance but also from a macroeconomic point of view (see Bisin et al., 2009 and Gersbach et al., 2015). For instance, if bankruptcy prevents the Modigliani–Miller theorem from holding, then the debt to equity ratios of firms can affect investment, leading to amplification effects of macroeconomic shocks.====Two kinds of equilibria, as in Magill and Quinzii (1996), are defined to this end. The first one is the reduced-form equilibrium in which consumption and production activities are carried out by one individual whose roles as consumer and entrepreneur are indistinguishable. In this kind of setting a situation in which individuals do not honor their financial commitments is called “default” because the financial decisions as consumers and firms are not separated. The second kind of equilibrium is the extensive-form equilibrium where the financial accounts of firms are separated from those of consumers, who are the owners of the firms. In this case, we call “bankruptcy” a situation in which firms do not honor their debts. For the latter to be well defined, we slightly modify the classical definition of the reduced-form equilibrium which requires the decomposition of the debt portfolio. Once this is done, we define the extensive-form equilibrium requiring that both consumers and entrepreneurs (firms) decide separately their consumption and production activities as well as their financial decisions in order to maximize their objective functions: consumers maximize their utility functions subject to their budget constraints and firms maximize the present value of their dividends subject to technology. Under a hypothesis called separation of deliveries, we analyze the equivalence between the reduced-form equilibrium and the extensive-form equilibrium. From this equivalence we analyze the Modigliani–Miller Theorem and Fisher’s Separation Theorem. The latter theorem is studied when the firm maximizes profits instead the present value of its dividends.====Lastly, we analyze the efficiency properties of the equilibrium with endogenous bankruptcy and collateral. Because of the incomplete markets nature of the environment, we focus on analyzing constrained efficiency. We show that the equilibrium is constrained efficient. This is because, when we allow for lump-sum transfers among agents, the constrained efficient transfer is given by the equilibrium insurance premium from the insurance contract offered by lenders.","Production, bankruptcy, and financial policies under collateral constraints",https://www.sciencedirect.com/science/article/pii/S0165489621000305,10 April 2021,2021,Research Article,114.0
"Becker Robert A.,Rincón-Zapatero Juan Pablo","Department of Economics, Indiana University, Bloomington, IN 47405, USA,Departamento de Economia, Universidad Carlos III de Madrid, 28903 Getafe (Madrid), Spain","Received 17 September 2020, Revised 14 February 2021, Accepted 24 March 2021, Available online 9 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.015,Cited by (1),"We reconsider the theory of Thompson aggregators proposed by Marinacci and Montrucchio (Marinacci and Montrucchio, 2010). We prove the existence of a Least Fixed Point (LFP) solution to the Koopmans equation. It is a recursive utility function. Our proof turns on demonstrating the Koopmans operator is a Scott continuous function when its domain is an order ==== of a space of ==== defined on the commodity space. Kleene’s Fixed ==== yields the construction of the LFP by an iterative procedure. We argue the LFP solution is the Koopmans equation’s principal solution. It is constructed by an iterative procedure requiring less information (according to an information ordering) than approximations for any other fixed point. Additional distinctions between the LFP and GFP (Greatest Fixed Point) are presented. A general selection criterion for multiple solutions for functional equations and recursive methods is proposed.","Recursive utility theory aims to describe classes of stationary intertemporal utility functions that are tractable in an array of capital theoretic and macrodynamic applications. Optimal growth models (e.g. Beals and Koopmans, 1969) have been the main application of recursive utility objective functions. The underlying frameworks for deterministic recursive utility theories are cast within discrete time infinite horizon models. The goal is to describe utility functions with many of the attractive properties of stationary additive utility functions with exponential discounting. The set of available consumption sequences is a subset of the set of all real-valued sequences. This subset is the recursive utility function’s domain.====Contemporary recursive utility function research focuses on proving a recursive utility function solves a particular functional equation, the ====. A solution is found as a fixed point of the equation’s corresponding nonlinear operator, the ====. The decision maker may be the planner of optimal growth theory or an infinitely-lived household in general equilibrium models. This agent has an underlying intertemporal preference ordering over a sequence of alternative consumption sequences with generic element ==== where ==== is the time-dated consumption at time ====. Koopmans, 1960, Koopmans, 1972 proposed an axiomatic structure for a consumer’s stationary preference ordering and deduced that a utility representation had a recursive property: the utility function, evaluated at ====, is ==== for some function ====, where ==== is the shift operator ==== and ==== ==== current consumption, ====, and future utility, ====, into a current utility value, ====. The function ==== is the aggregator. Here ==== is a recursive utility function and the Koopmans equation is ==== for each ====.====Lucas and Stokey (1984) turned Koopmans axiomatic theory around by taking the aggregator as the building block or primitive concept defined on the real variables ==== and ====, a set of possible utility functions (or, simply the ====) on the available consumption sequences, and defined the Koopmans equation as before, but now treating ==== as given and ==== as the unknown. Put differently, given an aggregator and a utility space, the problem is to find a ==== satisfying the condition ==== for each ====. If the Koopmans equation has a solution in the utility space, then that function is a recursive utility function that represents some preference ordering over the available consumption sequences. Of course, additional restrictions on the aggregator are essential to show the Koopmans equation has a solution. Lucas and Stokey (1984) in fact proposed sufficient conditions for the solution’s existence and uniqueness within the class of functions in their model’s utility space. Their proof verified that the Koopmans operator is a contraction map on that utility space, which is the Banach space of bounded real-valued functions defined on the set of all bounded nonnegative real-valued sequences endowed with the sup-norm. The Koopmans operator (belonging to ====), denoted by ==== is a self-map on the utility space defined for a given ==== in the utility space, and for each available ====, by the formula: ====If this operator admits a fixed point: ==== for each ====, then that utility function is a recursive utility function. Their definition of the Koopmans operator depends on the joint properties of the aggregator and the underlying utility space specification. The Thompson aggregator case dictates a different utility space underlies the Koopmans operator’s definition.====They prove the Koopmans operator satisfies Blackwell’s sufficient condition for an operator to be a contraction mapping.==== This includes showing the Koopmans operator is a monotone operator, that is ==== (pointwise) implies ==== (pointwise). The Contraction Mapping Theorem then yields ==== sup-norm converges to ====, and consequently, for each ====, ====Here ==== is the ====th iterate of ==== according to the formula: ==== for ====,with ==== for each ====, the zero function, and ====.====The proof that the Koopmans operator is a contraction mapping depends on their assumption that the aggregator satisfies a global Lipschitz condition in its second argument and that Lipschitz constant is smaller than one. The aggregators that satisfy this type of Lipschitz condition are classified now as Blackwell aggregators. Several papers extend their approach to other aggregator specifications. Boyd (1990) and Becker and Boyd (1997) discuss many extensions in the Blackwell family. A number of papers published after Becker and Boyd’s monograph extended the Blackwell model in novel ways where the aggregator’s global Lipschitz condition fails and the Koopmans operator is not a contraction map (see Rincón-Zapatero and Rodriguez-Palmero, 2003, Rincón-Zapatero and Rodriguez-Palmero, 2007, Le Van and Vailakis, 2005, and Felipe Martins-da Rocha and Vailakis, 2010, Felipe Martins-da Rocha and Vailakis, 2013).====Marinacci and Montrucchio (2010) proposed aggregators that did not fit into the previous literature. They named these examples as members of the Thompson aggregator class. For example, the KDW aggregator presented in Section 2 may fail to be a Blackwell aggregator for some interesting economic parameterizations. It is a member of their Thompson class in those situations. They proposed new methods for solving the corresponding Koopmans equation for a given Thompson aggregator. They built on the observation that the Koopmans operator is, in many cases, easily shown to be a monotone operator. Moreover, in their setup, this operator acts on a complete lattice of utility functions. The Tarski Fixed Point Theorem’s (Tarski FPT) conditions hold and extremal fixed points exist (and, may be distinct). One extremal fixed point is the smallest, or Least Fixed Point (LFP) and other is the largest, or Greatest Fixed Point (GFP). The Tarski FP and the existence of extremal solutions to the Koopmans equation are a non-constructive result. We show in our Constructive Recovery Theorem (CRT) (Section 3) these extremal fixed points exist using successive approximations as the Tarski–Kantorovich Fixed Point Theorem’s (TK FPT) conditions hold (see our Mathematical Appendix for the exact form of this result as used here). Our proof turns on verifying an order continuity property holds. It is a purely order theoretic condition and connects to fundamental results obtained by Kantorovich (1939). Details are developed in Section 3. The LFP is found by the iteration yielding the sequence ====, just as in Lucas and Stokey’s work (Lucas and Stokey, 1984). This iteration indexed on the natural numbers can fail to yield the operator’s LFP without order continuity. Marinacci and Montrucchio (2010) did not verify this order continuity property in proposing qualitative properties of the LFP and GFP solutions to the Koopmans equation. Our CRT resolves fills this gap and yields qualitative features of the extremal solutions, such as semi-continuity and concavity properties.====Kantorovich (1939) and Marinacci and Montrucchio (2010, p. 1785) indicate the LFP is the equation’s principal solution. Our CRT successive approximation argument of the LFP states that the sequence of iterates ==== is nondecreasing (pointwise) with each iteration and converges pointwise to the supremum of that sequence, denoted by ====, which is the LFP. This monotonicity property of successive approximations is consistent with an interpretation of a theoretical computational procedure where more information about the LFP recursive utility function is added in each successive iterative step. The idea is that the successive approximation procedure starts at the zero function where there is NO information about any function in the utility space means it is an “approximation” for any possible utility function. The first iteration yields ====. This means we have an approximate utility value for the infinite horizon if positive consumption is limited to a single period only. In this manner, we know more about a prospective LFP utility function than with the uninformative zero function input. In the second iteration, ==== follows. There is more information about the LFP in the sense that two consumption periods have been inputted instead of just one period, as in the first step. At this point we see (by the monotonicity property of the aggregator): ====and this is a better approximation to the LFP solution than obtained in the first iteration. This is the information order interpretation of successive approximation derived from the theoretical computer science literature; it is developed in detail in Section 3 and used to motivate our interest in the Scott topology. This consistency of successive approximations initiated by inputting the zero function with the information ordering is an important property of the LFP construction. This is the basis for our first argument supporting the LFP as the Koopmans equation’s principal solution.====We combine the order theoretic ideas in our working paper (Becker and Rincón-Zapatero, 2017) derived from Kantorovich (1939) with topological order continuity ideas due to Scott (1972) in order to amplify our defense of the LFP as the principal solution. This is undertaken in Section 4. First, a constructive existence argument facilitated by verifying the operator’s Scott topological order continuity property is sufficient to prove the LFP exists. Our LFP construction applies Kleene’s Fixed Point Theorem (Kleene FPT) (see the Appendix). It hypothesizes a Scott continuous operator on a complete lattice of functions and constructs the LFP by a sequence of successive approximations. Our second theme is the LFP differs from the GFP based on an information ordering or theoretical computation perspective. We argue the LFP’s construction and approximation requires less information according to the information ordering than any other solution. It also differs in a qualitative way when viewed through the Scott topology’s properties. The Kleene FPT argument shows the sequence of approximations to the LFP is eventually in each Scott neighborhood of the LFP. This cannot be said within the CRT’s order convergence framework as it lacks any notion of a neighborhood of the LFP. That is, order convergence of the successive approximations alone cannot inform us the sequence of iterations is eventually “close” to the LFP.====This paper’s main result is the Least Fixed Point Existence and Construction Theorem where the Koopmans operator is a Scott continuous self-map on an order bounded subset of the space of possible utility functions. This topological continuity notion is closely related to order continuity, but has subtle differences owing to its topological setup. The Scott continuity property implies a  ==== obtains for an ==== (or, ====) ==== . In particular, this property obtains for the successive approximation sequence based on iteration of the Koopmans operator with initial seed the zero function, which Scott converges to its supremum.====Scott (1972) introduced his eponymous topology within the context of recursive function theory and theories of computation within computer science. The use of Scott’s topological ideas (as distinct from non-topological order convergence notions) is uncommon in the economics literature. Vassilakis (1992) is exceptional on that score. He builds on Scott’s ideas and subsequent developments in logic and computer science to untangle a major conceptual issue arising in game theory whenever infinite regress arguments arise when postulating an equilibrium solution concept. He links computability ideas to approximation ideas. There is a similarity between his approximation ideas (and those in the computer science literature) analogous to the links made here upon iterating the Koopmans operator. However, we do not make the deeper connections to computability theory addressed in Vassilakis’ economic applications.====Section 2 reviews Thompson aggregators, sets up the underlying commodity space and the vector space of possible utility functions from which the functional equation’s solutions are sought. Section 3 reviews order convergence and continuity. Our CRT result appears there as well. We highlight the Koopmans operator’s order continuity structure abstracted in the Scott continuity environment next. Section 4 presents basic Scott topology convergence and continuity concepts followed by an argument supporting the existence and construction of the LFP as the equation’s principal solution. We also argue why the Scott continuity property further distinguishes the LFP from the GFP and why this distinction is a selection criterion identifying the LFP as the Koopmans equation’s principal solution. Section 5 concludes the paper. A Mathematical Appendix reviews basic material on partially ordered sets and lattices as well as states the TK FPT and Kleene FPT as used in this paper. Basic Riesz space concepts are also in that appendix. We follow the Riesz space conventions and definitions in Aliprantis and Border (2006) unless otherwise noted.","Thompson aggregators, Scott continuous Koopmans operators, and Least Fixed Point theory",https://www.sciencedirect.com/science/article/pii/S0165489621000354,9 April 2021,2021,Research Article,115.0
"Jeon Junkee,Koo Hyeng Keun,Park Kyunghyun","Department of Applied Mathematics & Institute of Natural Science, Kyung Hee University, Republic of Korea,Department of Financial Engineering, Ajou University, Republic of Korea,Department of Statistics, The Chinese University of Hong Kong, Hong Kong","Received 31 January 2020, Revised 22 February 2021, Accepted 30 March 2021, Available online 9 April 2021, Version of Record 21 April 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.016,Cited by (0), arising from the optimal stopping problem and provide an integral representation of ====.,"We study the consumption and portfolio selection problem of a finitely lived agent who derives utility from the stock of durable goods. Grossman and Laroque (1990) and Hindy and Huang (1993) investigated the problem for an infinitely lived agent. We study the life cycle (horizon) effect on optimal strategies.====We adopt the utility function proposed by Hindy et al. (1992) and Hindy and Huang (1992). Namely, we consider the following utility function of the agent for time ====: ====where ==== is the agent’s subjective discount rate and ==== is a strictly increasing real-valued function, called the ==== function of ====, and ====where ==== is an exogenously given initial level at time ==== and ==== is a constant. Here, ==== is the stock of durable goods, which depreciates at a rate equal to ====. Thus, the agent derives utility from the services provided by the stock. Alternatively, the stock can be interpreted as the summary of the effects of past consumption on the agent’s satisfaction. In this interpretation, the utility function captures the notion of local substitutability of consumption, which means that a small shift in the timing and the amount of consumption causes a slight change in the utility value, as emphasized by Hindy et al. (1992). The process ==== is the cumulative purchase of durable goods until time ====. We assume that there is no resale market for durable goods, and thus, the process ==== is a non-decreasing process.====A unique feature of the finite-horizon model is the fact the agent’s risk aversion generally decreases over time and becomes almost risk-neutral near the end of the planning horizon. As the stock of durable goods plays the role of insurance, it enables the agent to absorb large risk, and hence, guarantees the optimality of the almost risk-neutral behavior. Hindy and Huang (1993) also explain that the effects of past consumption on current and future utility make an infinitely lived agent more tolerable to absorb large risk. Meanwhile, the agent’s effective risk aversion is constant in the model of Hindy and Huang (1993). In contrast, it is optimal for the finitely lived agent to become nearly risk-neutral around the end of the planning horizon. This unique feature is also related to the empirical findings in Harrison et al. (2007) and Noussair et al. (2014).====The agent never adds durable goods at a finite rate over a non-trivial interval of time, that is, optimal consumption purchase is only sporadic. An infinitely lived agent also exhibits such sporadic consumption behavior (Hindy and Huang, 1993). However, the result is in contrast to that of the certainty case, where there are no risky investments. Bank and Riedel (2000) show that there exists a non-empty time interval over which the agent consumes at a positive rate for the certainty case. We also show that there is no deterministic time when the agent stops purchasing the durable goods. Bank and Riedel (2000) show that in the certainty case, i.e., when there is no risky asset, there exists a deterministic time earlier than the planning time ==== when the agent optimally stops consuming. In our model with risky investments, the agent keeps adding consumption as long as wealth stays positive. However, the agent’s wealth tends to be exhausted before the planning horizon, because of high risk-taking. This result can be explained by the agent’s almost risk-neutral attitude near the end of the planning horizon.====Schroder and Skiadas (2002) show that the model of durable goods in Hindy and Huang (1993) is isomorphic to that proposed by Dybvig (1995), which we will call the consumption ratcheting model following Dybvig’s terminology, where an agent has an ordinary time-separable utility function, but never tolerates a decline in consumption. When one uses the appropriately scaled stock of durable goods in the former, as the rate of consumption in the latter, the two models become mutually equivalent. We also use the isomorphism to derive optimal strategies; however, unlike the infinite horizon model Hindy and Huang (1993), the isomorphism does not lead the same consumption ratcheting model in a finite horizon setup. That is, if we transform our solution with the isomorphism, the consumption rate never declines similarly to that in the consumption ratcheting model; however, optimal wealth at final time ==== remains at a certain positive level, sufficiently large to maintain the final level of consumption for ever after ====. In contrast, optimal wealth at the final time is equal to zero in the finite horizon consumption ratcheting model Jeon et al. (2018). Consequently, this induces a difference in consumption behavior. In our model, the optimal stock of durable goods is bounded near the final time, whereas, consumption keeps increasing unboundedly near the final time in the consumption ratcheting model.====We approach the problem through successive transformations. First, we transform the problem into that of consumption ratcheting using the isomorphism of Schroder and Skiadas (2002). We then transform the problem into a dual problem using the martingale method proposed by Karatzas et al. (1987) and Cox and Huang (1989). Finally, we transform the choice of an increasing process into an optimal stopping problem by noting the equivalence between a monotone right continuous with left limits (RCLL) process and a series of right continuous stopping times when the process first hits pre-specified values.====There is a substantial related literature in addition to that mentioned above. Bank and Riedel (2001) formulate the general utility optimization problem and demonstrate a solution for both finite and infinite horizon problems. They also obtain concrete solutions for specific infinite horizon problems. We obtain an analytic solution for a finite horizon problem with a constant relative risk aversion felicity function. Watson and Scott (2014) study the consumption ratcheting model in a finite horizon with general processes for risky asset prices.====Extensive literature establishes the connection between singular control problems and optimal stopping problems (e.g., Bather and Chernoff, 1966, Karatzas and Shreve, 1984, Karatzas and Shreve, 1985, Baldursson and Karatzas, 1997, Benth and Reikvam, 2004).==== We follow Baldursson and Karatzas (1997), and transform the choice of a monotone process into a series of optimal stopping problems in which one chooses the first time for the process to reach a specific value. Similarly to Benth and Reikvam (2004), the series of problems can be reduced to a single optimal stopping problem. The optimal stopping problem is similar to finding the exercise time of an American option. We obtain the analytic representation of its value and free boundary for optimal stopping using the analytic representation of American options (Kim, 1990, Jacksa, 1991, Carr et al., 1992).====The rest of this paper is organized as follows: In Section 2, we describe our model and state the optimization problem. In Section 3, we formally provide the successive transformations and study the variational inequality associated with the transformed problem. In Section 4, we provide the analytic representation and features of the optimal strategies. In Section 5, we present simulation results and provide an implementation of our model. Section 6 draws conclusions.",Finite horizon portfolio selection with durable goods,https://www.sciencedirect.com/science/article/pii/S0165489621000366,9 April 2021,2021,Research Article,116.0
"Freer Mikhail,Martinelli César","Department of Economics, University of Essex, United Kingdom,Department of Economics, George Mason University, United States of America","Received 17 October 2018, Revised 4 February 2021, Accepted 28 March 2021, Available online 9 April 2021, Version of Record 23 April 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.018,Cited by (3),"We provide a utility ==== for the revealed preference of an agent choosing in an arbitrary space endowed with a separable partial order. The result can be applied to construct new revealed preference tests for choices over infinite consumption streams and probability distribution spaces, among other cases of interest in economics. As an illustration, we construct revealed preference tests for best-responding behavior in strategic games and infinite horizon consumption problems.","We provide a necessary and sufficient revealed preference condition for an observed set of choices to be generated by maximization of a utility function. The condition only requires the space to be endowed with a separable partial order. Our result extends the scope of applications of revealed preference tests to spaces which are not covered by existing results, including infinite consumption streams and probability distributions (including mixed strategies in games). We illustrate our approach by constructing a revealed preference test for best-responding behavior in strategic games and infinite horizon consumption problems.====Starting with Samuelson (1938), Richter (1966), and Afriat (1967), revealed preference literature seeks to test whether an observed set of choices can be consistent with maximization of a utility function. The central premise is that we can only observe choices and not the entire preference relation. Revealed preference theory allows data to speak for itself and therefore avoids the problem of parametric misspecification of preferences. Chambers and Echenique (2016) offer a general review of the revealed preference approach and its use for testing theories of individual behavior.====There is a growing interest in developing a comprehensive approach to revealed preference that can be applied in a wide variety of contexts of interest. There are two related strands of research. The first of them deals with preference extensions in an abstract setting. Suzumura (1976), Duggan (1999), and Demuynck (2009) provide preference extension theorems and their links with consistency conditions in terms of revealed preferences. A drawback of using a completely abstract framework is obtaining a preference relation which may not be representable by a utility function.====The second strand of literature generalizes the classical result of Afriat (1967). In this line, Forges and Minelli (2009) generalizes the applicability of the Afriat test to nonlinear budget sets. Nishimura et al. (2017) extend Afriat results to general topological spaces instead of the standard assumption of a real hyperplane. A fundamental assumption behind results in this line of research is local compactness of the topological space. This assumption does not necessarily hold for important settings like the spaces containing infinite consumption streams and probability measures.====This paper attempts to close the gap between the two strands of literature. In particular, we construct an extension of the revealed preference that can be represented by a utility function under the minimal assumption of separability of the partial order associated to the space of alternatives. This assumption is similar to making the separability assumption over the natural topology of this order.====The remainder of this paper is organized as follows. Section 2 contains the basic definitions. Section 3 shows the main result and its applications. Section 4 provides concluding remarks. All proofs omitted in the text are collected in an Appendix.",A utility representation theorem for general revealed preference,https://www.sciencedirect.com/science/article/pii/S016548962100038X,9 April 2021,2021,Research Article,117.0
Shin Euncheol,"KAIST College of Business, 85 Hoegiro, Dongdaemun-gu, Seoul 02455, Republic of Korea","Received 20 November 2019, Revised 19 January 2021, Accepted 26 January 2021, Available online 9 April 2021, Version of Record 19 April 2021.",https://doi.org/10.1016/j.mathsocsci.2021.01.006,Cited by (1),"This paper presents a bilateral network formation model that explains why some empirical degree distributions exhibit the ==== (IHRP). In my model, a sequentially arriving node forms a link with an existing node through a bilateral agreement. This newborn node prefers a highly linked node; however, the more links an existing node have, the more the marginal return from an additional link diminishes. I prove that the IHRP emerges if and only if the latter effect prevails over the former. I provide two implications of the IHRP. First, I show that the IHRP is related to the existence of equilibria in network games with strategic complementarities. Second, I fit the model to empirical degree distributions and demonstrate that the IHRP is more likely to be observed in social and economic networks. For those networks, my model also exhibits a higher data-fitting performance than other distributions.",None,Social network formation and strategic interaction in large networks,https://www.sciencedirect.com/science/article/pii/S0165489621000202,9 April 2021,2021,Research Article,118.0
"Destrée Nicolas,Gente Karine","EconomiX, UPL, Univ. Paris Nanterre, CNRS, F92000 Nanterre, France,Aix Marseille Univ., CNRS, AMSE, Marseille, France","Received 18 September 2020, Revised 11 February 2021, Accepted 24 March 2021, Available online 8 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.008,Cited by (4),"This paper studies the impact of migration and workers’ ==== on human capital and economic growth when young individuals face debt constraints to finance education. We consider an overlapping generations model ====, but may enhance growth at the equilibrium. The model replicates both negative and positive impacts of migration and remittances on economic growth underlined by the empirical literature. We calibrate the model for 30 economies.","With globalization, both migration and flows of workers’ remittances are increasing. In 2019, there were 272 million of migrants in the world and global remittances accounted for 653 billion of dollars.==== In 2020, the COVID crisis has generated a cut of these remittances for around 20% at the world level. These dramatic changes in such capital inflows really point out the need to understand the impact of such inflows on economic growth.====According to the literature, the effect of migration on economic growth is not clear-cut. Brain drain exerts a negative effect on human capital stock (among others Grubel and Scott (1966), Bhagwati and Hamada (1974) or Wong and Yip (1999)), whereas foreign opportunities for workers constitute an incentive to educate (see Mountford (1997), Vidal (1998) and Beine et al., 2001, Beine et al., 2008 for instance). Studies focusing on remittances also provide mixed evidence. Their impact may be negative (see Chami et al., 2005, Chami et al., 2008, Karagöz (2009), Le (2009) or Nwosa and Akinbobola (2016)) or positive (see León-Ledesma and Piracha (2004), Faini (2007), Vargas-Silva (2007), Pradhan et al. (2008), Nsiah and Fayissa (2013) and Imai et al. (2014) for instance). In addition, some studies underline that the impact of remittances differs across countries or regions (Jawaid and Raza (2012), Siddique et al. (2012) or Cazachevici et al. (2020)). An increase in wealth due to remittances may reduce labor supply (see Acosta (2006), Amuedo-Dorantes and Pozo, 2006, Amuedo-Dorantes and Pozo, 2012) and both the incentive to save and to invest (Athukorala and Sen (2004), Hossain (2014) or Yiheyis and Woldemariam (2016)). As underlined by Chami et al. (2008), the low volatility of remittances, which decreases the uncertainty, can explain the decrease in the incentive to save. According to Amuedo-Dorantes and Pozo (2004), remittances appreciate the real exchange rate (“Dutch disease”) which generates a decrease in international competitiveness and may decrease economic growth. However, Edwards and Ureta (2003), Calero et al. (2009) and Zhunio et al. (2012) show that, empirically, remittances have a positive effect on education – with an increase in children school attendance or in the length of education – which tends to stimulate economic growth. Remittances may also improve the countries’ creditworthiness, which will attract foreign investors (see Ratha, 2005a, Ratha, 2005b, Ratha, 2007). In addition, Catrinescu et al. (2009) explain how the development of institutions may make remittances growth improving, especially bringing remittances towards productive spending. For example, this is the case of the ==== program in Mexico which provides funds for public works and infrastructure (see Khan and Merritt (2020)). In addition to these facts, the impact of remittances can differ according to the level of financial development. When financial development is low, agent have difficulties to borrow for investment and they use remittances to invest. When financial development is high, the increase in wealth does not necessarily imply an increase in investment since borrowing is easier. This is shown by Giuliano and Ruiz-Arranz (2009) or Sobiech (2019).====The objective of this paper is to develop a theoretical setting able to explain this mixed evidence. We consider an overlapping generations model with endogenous growth based on human capital accumulation. In this setting, agents face endogenous debt constraints to finance their education in the home country. However, they receive remittances from their emigrated children. This model is an extension of de la Croix and Michel (2007) in which we introduce growth of births, migration and remittances. The specificity of this setting lies in the no-commitment framework.====Kehoe and Levine, 1993, Kehoe and Levine, 2001 develop such a no-commitment framework in which agents may choose to refund or not their loan but support a penalty in case of default, losing the access to the asset market.==== This assumption makes the borrowing limit endogenous: the maximal amount agents can borrow is such that the utility of refund is not lower than the utility of default. Andolfatto and Gervais (2006) study the role of endogenous constraints when they affect education funding. Assuming that wages and interest rates are exogenous, they consider that young agents face debt constraints to finance human capital investment. They find a negative impact of the conventional policies – defined by education subsidies, income tax and pensions – on welfare. Actually higher pensions make savings less useful, give less incentive to participate in the asset market and therefore raise the utility of default. Azariadis and Lambertini (2003) consider a three-period overlapping-generations endowment economy model in which the presence of endogenous debt constraints introduces complex equilibrium dynamics with multiple steady states and indeterminacy.====Using a similar setting in which they focus on growth based on human capital accumulation, de la Croix and Michel (2007) consider that young agents borrow in order to educate but cannot commit to refund their loan.==== They show that, on one side, a too low interest rate is detrimental for growth since it discourages agents to save and give them incentive to default. On the other side, a too high interest rate is also detrimental for growth since credit becomes too expensive. In addition, they underline that when the incentive to save relaxes the constraint and allows more education, endogenous credit constraints imply global indeterminacy of balanced growth paths. An equilibrium such that agents are unconstrained on the amount they borrow for education may coexist with an equilibrium where agents are credit-constrained to finance education.====We extend this model assuming that, during the childhood, agents may migrate to another country and send remittances (while young workers) to their family stayed at home. Since migrants neither work nor educate in the home country, we consider the decision of migration exogenous like in Cassin (2020) in order to focus on the role of endogenous borrowing constraints and their interaction with remittances. In the home country, non-migrating agents borrow to finance their education but cannot commit to refund their loans.==== Nevertheless, if they want to participate in the asset markets, they need to refund their loans. Thus, the non-migrating agents decide not only the amount they want to borrow, the amount they save but also if they refund or not their loans. The decision of refund is based on their incentive to save which depends on their time preference, the time profile of their income, the education productivity and what they expect for the interest rate. Since saving is a non monotonic function of the interest rate whereas optimal education is decreasing with the interest rate, the relationship between effective borrowing and interest rate is hump-shaped. As in de la Croix and Michel (2007), we show that these mechanisms may generate multiple equilibria. Finally, the nature (constrained/unconstrained) and the number of equilibria depends on the productivity of education. When productivity of education is high enough, agents have incentive to save ====, and may be less constrained: both constrained and unconstrained equilibria co-exist. What agents will finally save depends on their interest rate expectations. When they expect a high interest rate, the amount they can borrow is high but the borrowing cost is high as well, they expect high remittances and net income (of cost of refund) available for saving is low: savings are finally low and the interest rate is high, which corresponds to the unconstrained case. Conversely, when they expect a low interest rate, the amount they can borrow is low but the borrowing cost is low as well, they expect low remittances and net income available for saving is high: savings are finally high and the interest rate is low, which corresponds to the constrained case. When the productivity of education is lower, the utility of refund is ==== low and there may exist two constrained equilibria where one is locally stable.====As in Chami et al. (2008), in this setting, remittances may exert a negative effect on savings, reinforcing the constraint since the penalty of being excluded from the asset market is less damaging. This is especially true at a partial equilibrium level. However, the global effect of remittances on economic growth depends on the reaction of the interest rate too. It happens that this latter effect – which influences strongly the borrowing constraint – may offset the negative effect on the borrowing constraint and thus increase both the borrowing limit and education. As in Cassin (2020), the overall effect of migration and remittances on human capital is not clear-cut. We calibrate this model for 30 recipient countries==== and show that the global effect of migration and remittances on education and growth is positive for the majority of countries in our sample. This result implies that remittances may have a positive impact on borrowing as underlined by Aggarwal et al. (2011).====In this model, remittances exert potentially a destabilizing effect in the sense that, as any type of third period of life’s income, these transfers offer the opportunity to agents to default on their loan. This generates a friction on the financial market, increases the utility of default, reinforcing endogenously the borrowing constraints. This is especially true in developing countries where, due to the lack of health insurance and retiring system, there may be almost no third period of life’s income. In this case, without remittances, the agents’ utility of refunding their loans would always be greater than the utility of default and there would be a unique long-run equilibrium. Thus, remittances are especially destabilizing in countries where “old” agents receive no income of any type. In order to limit frictions and the destabilizing effect of remittances, the policy maker may give incentive for agents to participate in the asset markets, for example improving the education productivity. A noticeable feature of this model is that paradoxically when migrants face more stringent financial constraints in the host country, they are able to send less remittances and this generates less frictions in the home economy. In economies where there is a third period labor income, there may already exist multiple equilibria even without remittances. The calibration shows that the gap between economic growth at each equilibrium is lower with remittances. If we consider this gap as a proxy for volatility, we could argue that remittances may reduce volatility of GDP in recipient countries. This result is in accordance with Chami et al. (2008). Ahamada and Coulibaly (2011) show that a high level of financial development allows remittances to stabilize the economy. In our setting, migration and remittances improve economic growth in constrained economies in 70% of the considered countries but increase financial frictions.==== Finally, the model shows that the equilibrium growth rate is firstly increasing and then decreasing with the interest rate. It is then possible for the policy maker to improve growth of human capital guiding expectations towards a higher (resp. lower) interest rate when it is too low (resp. too high).====The paper is organized as follows. Section 2 deals with the theoretical model and the long-run equilibrium. Section 3 focuses on equilibrium dynamics and presents how migration and remittances could affect growth through their impact on liquidity constraints. In Section 4, we calibrate the model and show the predicted impact of migration and remittances on growth for 30 recipient countries. Section 5 contains concluding remarks.","Migration, remittances and accumulation of human capital with endogenous debt constraints",https://www.sciencedirect.com/science/article/pii/S0165489621000287,8 April 2021,2021,Research Article,119.0
Strulik Holger,"University of Göttingen, Department of Economics, Platz der Göttinger Sieben 3, 37073 Göttingen, Germany","Received 8 June 2020, Revised 30 January 2021, Accepted 21 March 2021, Available online 8 April 2021, Version of Record 16 April 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.001,Cited by (2),"In this paper I discuss a standard model of life cycle consumption behavior when the discount rate depends on the state of health and health deteriorates with increasing age. I show that this feature allows the introduction of time-consistent discounting at a non-constant rate and to model, in a convenient way, the notion that individuals discount future payoffs at higher rates when the risk of death increases. I show that the model generates an empirically plausible age-consumption pattern even when perfect annuity markets exist.","Economists agree that the standard assumption of intertemporal choice theory that future gains and losses are discounted at a constant rate exists mainly for convenience and several proposals have been discussed to model more realistic discounting behavior (Frederick et al., 2002). In this paper, I focus on one aspect in this domain of research, namely the notion that individuals discount the future at higher rates when they grow older and, in particular, when death is near. I capture this phenomenon by introducing health-dependent discounting and physiological aging into a standard life cycle model. Conceptualizing the discount rate as a function of the state of health, time-consistent solutions of intertemporal choice are easily obtained. As the state of health deteriorates, death becomes more likely, and the pure rate of time preference increases. In order to evaluate their survival probability, individuals consider their physiological age (their state of health) instead of their chronological age. The feature that the state of health is time-variant but pre-determined at any age enables the unconventional result that decisions are time-consistent although the discount rate is not constant.====As a measure of health, I use the health deficit index developed by Mitnitski et al. (2001) “as an individual state variable, reflecting severity of illness and proximity to death.” (ibid., p. 323). This measure, also known as frailty index, is an established methodology used by countless studies in gerontology. It has been introduced by Dalgaard and Strulik (2014) into economics (see also Hosseini et al., 2019). The health deficits index computes the number of health deficits present in a person relative to the number of potential health deficits. Health deficits are accumulated in an exponential way as individuals get older (Mitnitski et al., 2002a, Mitnitski et al., 2002b, Abeliansky and Strulik, 2018) and they are a precise predictor of mortality. The prediction of mortality can be so accurate that chronological age adds insignificant explanatory power when added to the regression (Rockwood and Mitnitski, 2007).====A limited number of studies have investigated how aging affects discounting. Huffman et al. (2019) find that, among the elderly, discount rates increase with age. Read and Read (2004) consider individuals from a larger range of ages between 19 and 89 and find the lowest discount rate for individuals of middle age, and thus, a u-shaped age-pattern of discounting. Sozou and Seymour (2003) show that such a u-shaped pattern can be motivated by an evolutionary theory of discounting. Chao et al. (2009) find evidence for a u-shaped association of the discount rate with health deficits and that age loses its predictive power for the discount rate when the state of health is taken into account. Falk et al. (2019) confirm for a large cross-country data set, comprising 80,000 individuals in 76 countries, that increasing life expectancy as well as better individual perception of health status is associated with higher discount factors (i.e. lower discount rates). A recent study by Gassen et al. (2019) argues in favor of an evolutionary channel from the physical condition of the body to time preference and finds a negative association between inflammatory activity (as a measure of health deficits and cellular distress) and the ability to delay gratification.====I apply the new discounting method to motivate a hump-shaped age-consumption profile in a life cycle model.==== The literature has developed several explanations for such a non-monotonous consumption profile (see e.g. Gourinchas and Parker, 2002) and a particularly related proposal is built on age-dependent mortality (Bütler, 2001, Feigenbaum, 2008). This channel, however, breaks down when individuals are allowed to finance old age consumption with annuities. In order to establish health-dependent discounting as an independent pathway, I assume a perfect annuity market to shut off the imperfect-annuities channel. Strulik (2017) shows that the consideration of health in the utility function could also motivate a consumption hump. In contrast to the earlier studies, which were designed to motivate a consumption hump, health-dependent discounting is a more encompassing refinement of preferences that could potentially inspire a host of other applications for which proximity to death influences human behavior.",Intertemporal choice with health-dependent discounting,https://www.sciencedirect.com/science/article/pii/S0165489621000214,8 April 2021,2021,Research Article,120.0
"Constant Karine,Davin Marion","Université Paris Est, Erudite, UPEC, Créteil, France,CEE-M, Univ. Montpellier, CNRS, INRAE, SupAgro, Montpellier, France","Received 18 September 2020, Revised 10 February 2021, Accepted 24 March 2021, Available online 8 April 2021, Version of Record 11 June 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.009,Cited by (4),This article examines how pollution and its health effects during childhood can affect the dynamics of ,"Pollution is one of the most important threat for health. According to the World Health Organization, approximately one-quarter of the global disease burden is due to modifiable environmental factors, representing 13.7 million deaths a year (WHO, 2016). There is considerable evidence that pollution, and in particular air pollution, has a positive and significant effect on morbidity - i.e. the rate of disease in the population – and mortality – i.e. the rate of death.==== While pollution affects the entire population, children are identified as particularly vulnerable to its damaging health effects (see, e.g., Sacks et al., 2011, Beatty and Shimshack, 2014 or WHO, 2018). Empirical studies identify that these larger effects are due to both a larger vulnerability of children, mainly because their lungs, brains and immune system are not completely developed, and a larger exposure, as they spend more time engaging in physical activity outside — where air pollution levels are usually larger (see, e.g., Bateson and Schwartz, 2007).====Such detrimental effects on children’s health are not only a short-term issue but persist later in life (see Currie et al., 2014 for a literature review). Childhood exposure to pollution is found to be associated with poor adult health. Moreover, by increasing school absenteeism (see, e.g., Park et al., 2002) and affecting negatively cognitive and learning abilities of children (see, e.g., Factor-Litvak et al., 2014), environmental degradation deteriorates also human capital formation. Through all these channels, the exposure of children to pollution implies long-term negative consequences on human capital and income when adult, representing a persistent threat to the well-being and abilities of individuals.====In addition, the health effects of pollution are characterized by their unequal distribution within a given generation. Children from households of lower socioeconomic status – in particular in terms of education – are found to be more vulnerable to pollution than those from more privileged households, even if they are exposed to similar levels (see, e.g., Neidell, 2004 or Currie, 2009). Those differences stem from the fact that wealthier and more educated parents are more likely to provide a cleaner environment to their children, but also to invest more in their children’s health (see, e.g., Currie et al., 2014).====All these facts lead us to wonder about the potential role of childhood exposure to pollution in the intergenerational transmission of inequality among agents. In this paper, we aim at examining how this mechanism could occur, what would be the consequences, and therefore, whether environmental policies could be a part of the solution to overcome the inequality issue.====We focus on the dynamics of inequality across generations because inequality represents a major challenge for our society. Since 1980, the gap between rich and poor is at its highest level in most developed countries and follows an upward trend (see, e.g., OECD, 2015 or UN, 2020). Such disparities are multidimensional and concern economic, social and health dimensions. Large health inequalities exist in the population according to the socioeconomic status of individuals. According to the OECD (2019), ====, and ====. These disparities may entail huge costs for society — in terms of well-being, health and social costs, productivity loss, discouraged investments, wasted potential ====. Moreover, a growing number of empirical and theoretical studies emphasize the net detrimental effect of inequality on long-term economic growth through its negative effect on human capital accumulation (see, e.g., Galor, 2011, OECD, 2015 or Constant, 2019). For all these reasons, reducing these disparities has became an explicit goal for many governments and, for that, it seems crucial to explore the different channels through which they occur.====To study the potential role of the health effect of pollution during childhood in the transmission of inequality, we formalize an overlapping generations model, with children and parents, in which agents are heterogeneous in terms of human capital. In accordance with the results emphasized by the literature discussed earlier, we consider the effect of air pollution on children’s health, the possibility for parents to invest in health care to lower this adverse effect, and the role of children’s health in the acquisition of human capital.====Through a theoretical analysis and a numerical illustration, we find that the economy may exhibit different long-term behaviors according to the pollution intensity of production and the initial level of disparities between agents. When they are both sufficiently low, the economy converges toward a long-term state without inequality. However, if production is highly polluting, inequality will always persist across time – whatever the initial level of inequality – and the economy may even be caught in an inequality trap with steadily rising disparities. The underlying mechanism is the following. Parents choose the level of expenditure aiming at reducing the health effects of pollution. Their human capital being heterogeneous, so are their financial abilities and their investments, which entails a heterogeneous vulnerability to pollution among children. Pollution affects more children from poorer households, who will therefore be less able to accumulate human capital and have a lower return on the education investment. Thus, the gap among households increases at each generation due to pollution. Note that we obtain this result despite the fact that we consider no difference in terms of abilities and diminishing marginal returns of human capital accumulation that usually ensure the absence of inequality in the long run. Here, such equality would always be found without pollution. But in the presence of pollution, its detrimental effect on children’s health may dominate and hence prevent human capital convergence in the long run. Thus, the exposure of children to pollution represents an important channel of intergenerational transmission of inequality.====We then explore if specific public interventions focusing on this mechanism are effective to tackle these human capital inequalities. First, pollution being the source of increasing divergence between agents, we examine the consequences of an environmental policy that consists in public maintenance funded by a tax on polluting production. Then, we also study the effects of a combination of an environmental and a health policy, through private health subsidy funded by a production tax. We obtain that an environmental policy is a good option to reduce the inequality issue in the economy but only when pollution intensity and the level of disparities among agents are not too high. Otherwise, it is not sufficient and may even reinforce inequality due to the negative income effect of the tax. In this case, we reveal that adding a health policy to the policy package could be an interesting solution. Typically, when health expenditure is sufficiently determining for health with respect to pollution, such a policy mix can prevent the economy to exhibit rising inequalities for a larger set of emission intensity.====The rest of the paper is organized as follows. Section 2 reviews the most relevant contributions related to our work. The model is presented in Section 3. Equilibria and dynamics of the economy are examined theoretically and illustrated numericaDDlly in Section 4. Section 5 is devoted to the policy implications and Section 6 concludes.","Pollution, children’s health and the evolution of human capital inequality",https://www.sciencedirect.com/science/article/pii/S0165489621000299,8 April 2021,2021,Research Article,121.0
"Dubey Ram Sewak,Laguzzi Giorgio,Ruscitti Francesco","Department of Economics, Feliciano School of Business, Montclair State University, Montclair, NJ 07043, United States of America,University of Freiburg in the Mathematical Logic Group at Eckerstr. 1, 79104 Freiburg im Breisgau, Germany,Department of Economics and Social Sciences, John Cabot University, Via della Lungara 233, 00165 Rome, Italy","Received 27 July 2020, Revised 20 March 2021, Accepted 22 March 2021, Available online 31 March 2021, Version of Record 16 April 2021.",https://doi.org/10.1016/j.mathsocsci.2021.03.002,Cited by (1),: a social welfare order satisfying anonymity and asymptotic density-one Pareto is representable and admits explicit description if and only if ==== contains finitely many elements.,"This paper provides a contribution to the literature on ====, which has drawn much attention from economists, philosophers and social planners. One of the principal issues examined in these studies is how to take the future generations’ well-being into account relative to the well-being of the current generations. A formal discussion of the concept of intergenerational equity has a long history in the economics literature. Ramsey (1928a) observed that discounting one generation’s utility relative to another’s is “ethically indefensible”, and something that “arises merely from the weakness of the imagination”. In the sixties, Ramsey’s intuitive idea found support among scholars in the field of inter-temporal social choice theory. Diamond (1965) formalized it as the concept of “equal treatment” of all generations (present and future) and termed it the ==== axiom on social preferences; this axiom requires the social planner to be indifferent between any pair of utility streams if any one of them is obtained from the other by interchanging the levels of well-being of any two generations. Yet, as is well known, a vast body of the dynamic programming literature is based on the assumption that the objective function is a discounted sum of one-period return functions. However, given any positive discount rate, the method of discounting down-weighs policy consequences that occur in the future, thus undermining the extent to which we care for the welfare of future generations.====Increasingly, economists are therefore being asked to evaluate and formulate policies with a planning horizon that extends over the infinite future. There are plenty of prominent examples: global climate change, non-renewable natural resources, radioactive waste disposal, loss of biodiversity, groundwater pollution, minerals depletion, and many others. This task entails comparing consistently infinite utility streams by means of social preferences that respect certain desirable axioms. Social preferences come in the form of a binary relation (i.e., a reflexive and transitive pair-wise ranking rule) called social welfare relation. The latter is referred to as a social welfare order when the binary relation at hand is also complete (i.e., capable of ranking any pair of infinite utility streams). Furthermore, a numerical representation of a social welfare order is known as social welfare function. The subject of consistent evaluation has received significant attention from the research community.====The standard framework in the literature treats infinite utility streams as elements of the set ====, where ==== is a non-empty subset of real numbers and ==== is the set of natural numbers. The set ==== describes all possible levels of utility that any generation can attain. Observe that aggregating social preferences that satisfy the anonymity axiom presents no challenges. Indeed, a social welfare function which assigns the same value to all utility sequences is trivially anonymous. However, such a social welfare function is of no interest to the social planner as it does not convey any meaningful information. Clearly, we want to look at social preferences that are able to detect changes in utility profiles. To this end, what is needed is some degree of sensitivity (to the levels of individuals’ well-being across infinitely-many generations), which is captured by the ==== axiom. The latter has achieved general consensus among economists and policy makers. This axiom requires a stream of utilities to be ranked above another stream if at least one generation receives a higher utility and no generation receives lower utility compared to the other stream. It is a basic efficiency concept which is generally applied in models of economic growth. We will refer to social preferences satisfying the anonymity (equity) and Pareto (efficiency) axioms as ==== or ==== preferences.====The results of this paper fit well in the body of research studying various versions of the Pareto principles combined with anonymity. In short, we introduce a new efficiency criterion, we refer to as asymptotic density-one Pareto, and we examine social welfare relations on ==== which are complete and satisfy the anonymity and asymptotic density-one Pareto conditions. We show that the restrictions on ==== that lead to the social welfare order at hand having a real-valued representation are the same as the restrictions on ==== that result in the social welfare order admitting explicit description. More specifically, each of the two properties holds if and only if ==== is finite. Our results extend those in Petri (2019) and Dubey et al. (2020). Next we provide a brief overview of the recent literature on social choice and welfare and discuss more in detail the scope and position of our contribution within the established literature.",On social welfare orders satisfying anonymity and asymptotic density-one Pareto,https://www.sciencedirect.com/science/article/pii/S0165489621000226,31 March 2021,2021,Research Article,122.0
"Diss Mostapha,Tsvelikhovskiy Boris","CRESE EA3190, Univ. Bourgogne Franche-Comté, F-25000 Besançon, France,University Mohamed VI Polytechnic, Africa Institute for Research in Economics and Social Sciences (AIRESS), Rabat, Morocco,Department of Mathematics, Northeastern University, Boston, MA, 02115, USA","Received 24 September 2020, Revised 23 November 2020, Accepted 21 February 2021, Available online 29 March 2021, Version of Record 13 April 2021.",https://doi.org/10.1016/j.mathsocsci.2021.02.002,Cited by (0),"Coalitional manipulation in voting is considered to be any scenario in which a group of voters decide to misrepresent their votes in order to secure an outcome they all prefer to the outcome of the election when they vote honestly. The present paper is devoted to studying coalitional manipulability within the class of scoring voting rules. For any such rule and any number of alternatives, we introduce a new approach allowing us to characterize all the outcomes that are manipulable by a coalition of voters. This then opens the possibility of determining the ==== of manipulable outcomes for some well-studied scoring voting rules in the case of small number of alternatives and large electorates, under a well-known assumption on individual preference profiles.","Since the seminal papers of Gibbard (1973) and Satterthwaite (1975), who proved that every non-dictatorial social choice rule can be manipulated in the presence of at least three alternatives, the problem of coalitional manipulation has gone on to receive a lot of attention within the field of social choice theory. Broadly speaking, a given social choice rule is called ==== if there exists a given list of voting preferences and a coalition of voters,==== such that the preferences of all the voters outside the coalition remain the same, while the preferences of voters within the coalition can be altered in such a way that the winner changes and each of the voters from the coalition is ‘happy about the change’.====Scoring voting rules, also called ====, have attracted a considerable amount of attention in the literature dealing with manipulation. This class of voting rules can be defined as follows: each voter’s preference is expressed as a vector that gives the number of points that the voter assigns to each alternative according to his or her position in the voter’s preference. The points assigned by all voters are summed, and the winning alternative is the one with the highest number of points. A number of studies have been conducted on the evaluation of the degree of manipulability of various social choice rules, i.e., the extent to which social choice rules are manipulable by a coalition of voters or by an individual voter. The reader may refer, for instance, to Aleskerov and Kurbanov, 1999, Chamberlin, 1985, Diss, 2015, El Ouafdi et al., 2020a, El Ouafdi et al., 2020b, Favardin and Lepelley, 2006, Favardin et al., 2002, Gehrlein et al., 2013, Kamwa and Moyouwou, 2021, Kelly, 1993, Kim and Roush, 1996, Lepelley and Mbih, 1994, Lepelley et al., 2008, Moyouwou and Tchantcho, 2017, Nitzan, 1985, Peleg, 1979, Pritchard and Wilson, 2007a, Pritchard and Wilson, 2007b, Saari, 1990, and Schürmann (2013). The methodology used in this literature consists first in characterizing the specific conditions that must be met for a given voting rule to be manipulable by a coalition of voters. The final step consists in the evaluation of the (theoretical) probability of this situation under various assumptions on voters’ preferences. For more details on those probabilistic assumptions and their use in social choice theory, the reader can refer to Diss and Merlin (2021) and Gehrlein and Lepelley, 2017, Gehrlein and Lepelley, 2011.====However, as pointed out in a recent paper by El Ouafdi et al. (2020a), with some notable exceptions the results appearing in the literature only deal with three-alternative elections, not because this is the most interesting case, but due to the difficulties arising when considering more than three alternatives. The main goal of this paper is to provide a significant improvement in this direction. This is achieved via presenting systems of linear inequalities which determine manipulable profiles, that is the lists defining the votes of all voters taking part in the decision process, for the whole class of scoring voting rules independently of the number of alternatives. More precisely, we focus on a detailed exposition of the new approach for obtaining the list of linear inequalities that a profile satisfies if and only if it is manipulable in the case of ==== alternatives for all scoring rules.====The paper is organized as follows. Section 2 describes the basic framework. The core of the paper is Section 3, where the main results are presented. We start off by providing a simplified illustration of how our methodology works in the case of three-alternative elections. Then, the precise list of inequalities for the whole class of scoring rules is presented. Attention will be focused on the limiting case where the number of voters tends to infinity. This will enable us to augment the existing literature dealing with the probability of manipulable outcomes by providing, in Section 4, the corresponding values for the three most studied scoring rules in this literature, which are the Plurality, Antiplurality and Borda rules, in the presence of ==== and ==== alternatives under the well-known Impartial and Anonymous Culture assumption (defined later). The results are approximate since they are obtained by Monte Carlo simulations, yet they retain a high degree of precision, since we used profiles of cardinality ==== for ==== and ==== for ==== alternatives. To the best of our knowledge none of those results has appeared in the literature, with the exception of the Plurality rule and ==== (see El Ouafdi et al. (2020a)). The new results clearly indicate that among the three positional rules under consideration Antiplurality has a much lower degree of manipulability (see Tables 5, 6, and 7). The last section presents our conclusions.",Manipulable outcomes within the class of scoring voting rules,https://www.sciencedirect.com/science/article/pii/S0165489621000196,29 March 2021,2021,Research Article,123.0
"de Almeida Prado Fernando Pigeard,Blavatskyy Pavlo","Department of Computing and Mathematics, FFCLRP, University of São Paulo, Av. Bandeirantes, 3900, CEP 14040-901, Brazil,Montpellier Business School, 2300 Avenue des Moulins, 34185, Montpellier Cedex 4, France","Received 26 February 2020, Revised 22 December 2020, Accepted 22 December 2020, Available online 24 February 2021, Version of Record 16 March 2021.",https://doi.org/10.1016/j.mathsocsci.2020.12.004,Cited by (1),"This paper provides a complete characterization of Pure Strategy ==== of demand are firm specific. In particular, we show that the extended game has at least one PSNE when the firms’ demands are sufficiently elastic and when the unsatisfied demand is positive.","In the classic Bertrand (1883) model of oligopolistic price competition, ==== firms face a discontinuous demand because consumers always buy firms’ products at the lowest available price. A discontinuous demand function effectively reduces the result of oligopolistic price competition to the result of the perfect competition when there are two or more firms on the market: all firms make zero profits and the market price does not exceed the constant marginal cost of production.==== Yet, this theoretical result is rarely supported by the data, which is known as the Bertrand paradox (e.g., Pratt et al., 1979). The Bertrand paradox prompted the development of numerous generalizations of Bertrand (1883) model such as limited production capacities (e.g., Edgeworth, 1897, Kreps and Scheinkman, 1983, Thépot, 1995), product differentiation (e.g., Singh and Vives, 1984, Bulow et al., 1985), demand uncertainty (e.g. Lepore, 2012), and the possibility of rebates (e.g. Szech and Weinschenk, 2013).====To resolve the Bertrand paradox, several papers consideroligopolistic price competition with a continuous demand function. The latter is used in the models of oligopolistic price competition with heterogeneous products (e.g., De Palma et al., 1985, Anderson and De Palma, 1988, Anderson et al., 1992) or when a representative consumer is characterized by the variety-seeking behavior (Anderson et al., 1988) or bounded rationality such as habit formation/satisficing (e.g. Schroeder et al., 2018). Blavatskyy (2018) recently derived two forms of a continuous demand function (logit demand and power demand) from a set of behavioral assumptions (axioms) about the effect of price changes on firms’ relative market shares.====The existence and uniqueness of the symmetric Pure Strategy Nash equilibrium (PSNE) for the model of oligopolistic price competition with the logit demand function have been proven in Anderson et al. (1992), Section 7.4 “The Logit with an Outside Alternative”, p. 229. Yet, to the best of our knowledge, the corresponding result for the power demand function has not been established in the literature. Our paper fills this gap and provides additional results.====Let ====, where ==== denotes the price set by firm ====. We denote by ==== the fraction of consumers who buy a product of the ====th firm, ====. The following Power demand is due to Blavatskyy (2018). ====where ==== and ==== are positive constants and ==== is the fraction of unsatisfied demand.====In this paper, we study an oligopoly with ==== firms (players) facing power demand (1). We show that this game has a unique and symmetric Pure Strategic Nash equilibrium (hereafter, ====, ====, or simply, ====) if firms have a strictly positive constant marginal cost and they face a sufficiently elastic demand (that is, if the power coefficient of the demand function ==== is sufficiently large). If the demand is inelastic (====), then the game has no PSNE at all, cf. Blavatskyy (2018, p. 128).====This paper also shows that when firms have zero marginal costs, the model has a set of classic Bertrand equilibria as well as one symmetric non-Bertrand equilibrium. In each of Bertrand equilibria, at least two firms (and at most all firms) charge zero price. In the non-Bertrand Nash equilibrium, all firms charge a strictly positive price. This equilibrium exists only if firms face a moderately elastic demand, i.e., the power coefficient of the demand function is greater than one but less than ====.====Furthermore, we generalize the power demand function to allow for the possibility of zero unsatisfied demand, ==== (i.e., when consumers do not have an outside option and they must buy the product from one of the firms) and show that the above results carry on to this generalization.====We also provide a novel interpretation of the unsatisfied demand ==== as the demand of a social planner who partially supplies the market at price ==== aiming at regulating the equilibrium price set by the players ====. Comparative statics results show that, if the marginal cost is zero, then the social planner can reduce the equilibrium price to zero without absorbing any additional demand.====Finally, we also extend and analyze the game in the case when firms have different marginal costs and the elasticities of demand are firm specific. We show that the extended game has at least one PSNE under mild conditions. We also provide an equilibrium uniqueness result for a duopoly with different marginal costs.",Existence and uniqueness of price equilibrium in oligopoly model with power demand,https://www.sciencedirect.com/science/article/pii/S0165489620301116,24 February 2021,2021,Research Article,124.0
Mhlanga A.,"Department of Mathematics, University of Zimbabwe, Box MP 167 Mount Pleasant, Harare, Zimbabwe","Received 7 August 2019, Revised 4 October 2020, Accepted 3 February 2021, Available online 12 February 2021, Version of Record 23 February 2021.",https://doi.org/10.1016/j.mathsocsci.2021.02.001,Cited by (1),"Xenophobia is a social evil which leaves a trail of destruction wherever it passes through. A ==== is formulated and analyzed to gain an understanding of xenophobia in order to come up with strategies on how we can best control it. The xenophobia-free equilibrium is shown to be globally asymptotically stable when the corresponding threshold parameter is less than unity. Furthermore, the xenophobic equilibrium point exists only when the corresponding threshold parameter is greater than unity and is locally asymptotically stable when the corresponding threshold parameter is greater than unity. The results from the model analysis suggest that negative peer influence promotes the development of xenophobia while counseling and incarceration inhibit its growth. The results from the numerical simulations suggest that although counseling and incarceration can singly reduce xenophobia, the use of counseling of the exposed and the xenophobic coupled with incarceration may be the ideal strategy to stop this menace.","Xenophobia is an unreasonable fear or dislike or hatred of foreigners often accompanied by violence leading to loss of life and looting. According to international media reports, discrimination against fellow African migrants has been on the rise from Kenya to the Maghreb and across Southern Africa. Citizens of most host countries display hostilities and hatred against migrants based on the assumption that they may increase competition for resources (Claassen, 2017). The hostility may often degenerate into violent attacks against foreigners (Oni and Okunade, 2018). Upon attaining independence most African countries called on foreigners to leave, burned their shops and wrecked their source of livelihood (Fanon, 1990). In these states as noted in Fanon (1990), new post independence elite grabbed capital and jobs from fleeing Europeans, while the masses only followed in attacking African migrants (Neocosmos, 2008). It is clear from Neocosmos (Neocosmos, 2008) that the politics of grabbing are contributing factors in xenophobic attacks.====South Africa seems to experience higher rates of occurrence of xenophobic attacks, despite having one of the widely praised constitution in the world with regards to the rights and freedoms of everyone living in the boundaries of nation-state (Simeon, 1998). Black African migrants living in South Africa are subjected to inhuman behavior. In fact, South Africa became increasingly antagonistic towards black migrants a year after it attained its independence (Human Rights Watch, 1998). Zimbabweans and Mozambicans bear the brunt of state’s arrest-and-depart campaign (Human Rights Watch, 1998).====Majority of South Africans hold negative perceptions about black African migrants and are not prepared to extend to them the rights actually guaranteed by their constitution (Crush, 2000). A large percentage of South Africans viewed black migrants as a threat to their economic and social well-being (Claassen, 2017). In general, African immigrants are exposed to xenophobic and poor treatment at the hands of employers and citizens with little or no action coming from the government and civil society (Crush, 2000). In 1998, the Human Rights Watch was criticized by both the Minister of Home Affairs and his deputy for the 1998 report documenting widespread abuse of foreign migrant workers at their places of work. Xenophobic statements by national leaders have helped fuel the problem (Charney, 1995, Neocosmos, 2008). For instance in the first half of 2015, King Goodwill Zwelithini, traditional leader of the Zulu ethnic group, said African migrants should take their things and go, as they supposedly take the jobs and public resources meant for locals leading to a wave of xenophobic attacks from Durban to Johannesburg leaving scores of foreigners dead (Campell, 2015). One of the most original studies of the 2008 attacks identified township community policing meetings as the fora where community links drew participants into the attacks (Misago, 2012). The results of such peer influence in communities had been necessitated by increasing poverty, competition for jobs and housing with fellow African migrants (Claassen, 2017). South Africa also lacks accountability for xenophobic crimes, virtually no one has been convicted for past outbreaks of xenophobic violence, including the Durban violence of April 2015 that displaced thousands of foreign nationals, and the 2008 attacks on foreigners, which resulted in the deaths of more than 60 people across the country (Mavhinga, 2019).====Having gained the status of being a global phenomenon over the years, it is worth noting that xenophobia is not a one-continent affair as it has been practically experienced in one form or the other across different continents of the world. Literature is replete on the concept of xenophobia (Oni and Okunade, 2018, Peil, 1974, Harper, 2010, Marsella and Ring, 2003, Aremu, 2013) however, there are no adequate works on its dynamism. In this manuscript, a model is proposed to understand xenophobia from the mathematical point of view. It is worth mentioning that this is not the first study to use epidemiological contact models to analyze social and behavioral processes (see: Gonzalez et al., 2003, Mubayi et al., 2010, Sanchez et al., 2007, Benedict, 2007, Bhunu, 2014a, Bhunu, 2014b, Bhunu and Mushayabasa, 2012a, Bhunu and Mushayabasa, 2012b). However, we are possibly the first to mathematically explore the growing problem of xenophobia.",A mathematical approach to Xenophobia: The case of South Africa,https://www.sciencedirect.com/science/article/pii/S0165489621000123,12 February 2021,2021,Research Article,125.0
de Boisdeffre Lionel,"University of Paris 1-Panthéon-Sorbonne, 106-112 Bd. de l’Hôpital, 75013 Paris, France","Received 23 January 2020, Revised 7 November 2020, Accepted 27 January 2021, Available online 10 February 2021, Version of Record 24 February 2021.",https://doi.org/10.1016/j.mathsocsci.2021.01.005,Cited by (2),’s (====) to ,"This paper demonstrates the generic existence of equilibrium in incomplete financial markets with differential information, in the context of a two-period pure exchange economy where uncertainty prevails at the first period over the state of nature to be revealed at the second period. Asymmetric information is represented by private finite subsets of states of nature, which each agent is correctly informed to contain the true state of the second period. The scope of this specification is discussed in Section 3, jointly with the information that financial markets may reveal. Consumers exchange consumption goods on spot markets, and, unrestrictively, assets of any kind on typically incomplete financial markets. They are endowed with a bundle of goods in every state, with ordered smooth preferences over consumptions and a perfect foresight of future prices, along Radner (1972).====The paper’s generic existence result generalizes a classical theorem of symmetric information with real assets due to Duffie and Shafer (1985). The current proof builds on a fixed arbitrary set of state prices. This device permits to extend to the current model other results of symmetric information, such as Cass’ (1984), stating that any collection of state prices supports an equilibrium on purely financial markets.====When assets pay off in goods, equilibrium needs not exist, as shown by Hart (1975) in the symmetric information case. His example is based on the collapse of the span of assets’ payoffs, that occurs exceptionally at clearing prices. Duffie and Shafer (1985) shows that equilibrium with real assets exists, except for a closed set of measure zero of economies, parametrized by assets’ payoffs and agents’ endowments.====The current model extends Duffie and Shafer’s (1985) in three ways. First, it allows for asymmetric information amongst consumers. Second, its financial structure may cover any mix of nominal and real assets. Third, but not least, it normalizes to arbitrary values equilibrium prices on every spot market. Normalization to relevant values serves to study subsequently the existence of sequential equilibria when agents loose the perfect foresight of future prices. With no price map, agents typically face an endogenous uncertainty a la Kurz (1994). To be self-fulfilling, their anticipations need therefore focus on sets of relevant values, as argued in Section 7.====The current paper drops Radner’s (1979) rational expectations assumption. It prefers a learning process, presented in Section 3, where agents may infer information from markets with no price model. It is a step towards also replacing Radner’s (1972) perfect foresight assumption by a milder condition on anticipations, which remains consistent with the definition of sequential equilibrium (see Section 7).====The current proof uses standard differential topology arguments, introduced by Debreu, 1970, Debreu, 1972 for the study of general equilibrium. It defines an auxiliary concept of “====” with asymmetric information, shows its full existence from modulo 2 degree theory and derives the generic existence of equilibrium with asymmetric information from Sard’s theorem and Grassmannians’ properties.====The paper is organized as follows: Section 2 presents the model and the concepts of equilibrium and pseudo-equilibrium. Section 3 describes the information that markets reveal. Section 4 presents Grassmannians and their main properties. Section 5 derives from the latter properties the full existence of pseudo-equilibria. Section 6 proves the existence theorems. Section 7 concludes. An Appendix proves Lemmas.",Equilibrium in incomplete markets with differential information: A basic model of generic existence,https://www.sciencedirect.com/science/article/pii/S0165489621000111,10 February 2021,2021,Research Article,126.0
Noguchi Mitsunori,"Meijo University, Japan","Received 12 February 2020, Revised 14 January 2021, Accepted 16 January 2021, Available online 5 February 2021, Version of Record 18 February 2021.",https://doi.org/10.1016/j.mathsocsci.2021.01.003,Cited by (2),We introduce a variant of Milgrom and Weber’s (1985) model of ====-person games with incomplete information (games for short) and define a correspondence that maps each game to its ====-core (the ,"Suppose a game theorist mathematically represents an ====-person game ==== by a vector ==== of payoff functions alone. Any theory with a solution concept would associate some solution set ==== to each such ====. Let ==== be an actual ====-person game with an actual outcome ====, selected by Nature. If ==== is the correct mathematical representation of ====, that is “true” ====, and if the theory applied is correct, one would expect ====. In reality, we merely estimate the actual ==== by some ==== with an unavoidable error and use ==== as a ranged prediction of ====.====Thus, to ensure reliable game-theoretic predictions, ==== must continuously depend on ==== guaranteeing that ==== captures at least a point arbitrarily close to ==== as long as ==== is sufficiently close to ====. If the solution correspondence ==== is single-valued, the problem is simple. The usual continuity for a function fulfills the above requirement. However, it is not immediately clear which notion of continuity must be applied if ==== is multi-valued. We will later demonstrate a concrete example (Example 18) of lower semicontinuity being the correct one.====The seminal work by Fort (1950) investigates a similar problem in a somewhat different context; it considers a family ==== of continuous functions ==== with their domain and range both equal a single compact metric space ==== with the fixed point property. The solution set ==== is the set of fixed points of ====. Fort (1950) calls a fixed point ==== essential if each continuous function ==== sufficiently close to ==== admits a fixed point ==== arbitrarily close to ==== and calls a continuous function ==== essential if all of its fixed points are essential. Unraveling these definitions reveals that the condition that ==== is essential is simply another way of saying that the correspondence ==== is lower semicontinuous at ==== when the sup-norm topology is given to the family ==== of continuous functions ====. Then, using the fact that ==== is a complete metric space, Fort (1950) concludes that generic continuous functions ==== are essential relative to ====, which means that outside some topologically negligible subset of ====, all continuous functions ==== are essential. To be more precise, one version of Fort’s (1950) theorem states as follows: Let ==== be a complete metric space and let ==== be a metric space. If ==== is a nonempty, compact valued, and upper semicontinuous correspondence, then ==== is lower semicontinuous (and hence continuous) at every point in some residual ==== of ====; ==== is dense in ==== since ==== is a complete metric space and hence a Baire space. Recall that a Baire space is a topological space ==== with the property that given any countable collection ==== of open sets in ====, each of which is dense in ====, their intersection is also dense in ====. See, for example, Munkres (1975, p. 293) for further reference.====The same line of argument applies to ====-person games and their solution sets, as Wu and Jiang (1962) initially pursued. Suppose again that the set ==== of profiles ==== of payoff functions fully parametrizes the games. Let ==== denote a joint action space common to all the games in ====. Then, noncooperative game theory, for instance, gives rise to a solution correspondence ==== with which we can associate the notions of essential equilibria and essential games as in the Fort’s argument.====Wu and Jiang (1962) investigated the generic continuity of Nash equilibrium correspondences for ====-person strategic-form games with finite action spaces, Jiang (1963) with the action space ====, Yu (1999) with more general action spaces (compact convex action spaces in a Hausdorff topological vector space), and Correa and Torres-Martínez (2014) for large generalized games. These papers contain a rich list of references for further information.====Recall that we define the ====-core of a strategic-form game as the set of strategy profiles that cannot be ====-blocked by any coalition, where ====-blocking takes place only when a deviating coalition can guarantee all its members strictly more payoffs than the status quo no matter what the reactions of the outsiders may be. Aumann (1961) and Kajii (1992) offer extensive discussions on ====-cores. Yang (2017) studied the essential stability of the ====-cores of strategic-form games with nonordered preferences.====The main objective of the current study is to prove the generic lower semicontinuity of the ====-core correspondence for ====-person strategic-form games with incomplete information modeled after Milgrom and Weber (1985).====We organize the rest of this paper as follows. Section 2 revisits the Milgrom and Weber’s model and describes how our variant of it differs. We state a known set of conditions that ensures the nonemptiness of ====-cores and present a concrete example of a game with a nonempty ====-core. Section 3 discusses how to parameterize the games defined in Section 2 to ensure that the ====-cores of those games are nonempty. Section 4 introduces the ====-core correspondence that assigns to each game its ====-core and discusses how its lower semicontinuity makes sense as a notion for a game to be essential. We also remark that lower semicontinuity is the correct continuity notion for a solution correspondence to furnish reliable game-theoretic predictions for outcomes. Section 5 states our main theorem, which asserts that the ====-core correspondence is generically lower semicontinuous relative to the parameter space, or equivalently, that generic games are essential relative to the parameter space. We give an example to demonstrate that the lower semicontinuity of the ====-core correspondence is only a generic property that may not hold on the entire parameter space. We then prove the main theorem in three steps: (Step 1) The parameter space is a complete metric space. (Step 2) The range of the ====-core correspondence is a compact metric space given an appropriate topology. (Step 3) The graph of the ====-core correspondence is closed and is thus upper semicontinuous, given other conditions. Finally, we appeal to Fort’s (Fort) theorem to complete the proof. Section 6 contains concluding remarks.",Essential stability of the alpha cores of finite games with incomplete information,https://www.sciencedirect.com/science/article/pii/S0165489621000093,5 February 2021,2021,Research Article,127.0
Caputo Michael R.,"Department of Economics, University of Central Florida, P.O. Box 161400, Orlando, FL 32816-1400, United States of America","Received 30 June 2020, Revised 22 December 2020, Accepted 21 January 2021, Available online 2 February 2021, Version of Record 18 February 2021.",https://doi.org/10.1016/j.mathsocsci.2021.01.004,Cited by (1),"The comparative dynamics of the feedback form of the consumption and health-investment demand functions in the health capital model of Grossman (1972) are thoroughly scrutinized for their refutable results. The usual comparative dynamics expressions, to wit, the individual partial derivatives of the demand functions with respect to the parameters and state variables, are not refutable under the basic assumptions of the theory. As a result, sufficient conditions for their ","More than half-a-century ago Grossman (1972) developed the canonical theoretical model in health economics, the so-called health capital model. A novel feature of it is that a rational, forward-looking agent undertakes investment in healthy activities in an effort to increase their stock of health capital, the latter of which, along with the rate of consumption of a numeraire, are arguments of an agent’s felicity function. Extensions of the model exist, most of which are summarized in Grossman (2000), Galama (2015), and Laporte (2015). Several authors, namely Muurinen (1982), Ehrlich and Chuma (1990), Ried (1998), Eisenring (1999), Galama (2015), Laporte (2015), Strulik (2015), and Galama and Van Kippersluis (2019), derived some comparative dynamics for various forms of the health capital model under a host of simplifying assumptions. Despite this activity, understanding of the canonical model’s qualitative properties remains incomplete. That incompleteness is addressed here by undertaking a complete investigation of the model’s comparative dynamics.====Notably, all of the above-mentioned papers differ from the present paper in two fundamental ways, to wit, they (i) studied an open-loop solution of the health capital model, that is, they studied its solution expressed in the form of a time-path of optimal decisions over a planning horizon, and (ii) did not undertake a complete investigation of the model’s comparative dynamics. Regarding (i), the present work instead focuses on a feedback (or closed-loop) solution of the health capital model, i.e., it focuses on decision rules for consumption and health investment, which yield optimal decisions at each point in time of the planning horizon and which only require information on variables and parameters at the time a decision is made. This distinction is important because, as argued in greater detail in Section 2, a feedback solution to an optimal control problem is much more relevant for empirical work, seeing as it yields a decision rule for optimal choices suitable for econometric purposes. As for (ii), the aforesaid authors did not attempt a complete investigation of the comparative dynamics, seeing as they did not derive those that are intrinsic—the definition of intrinsic comparative dynamics is provided below. Because refutable, or equivalently, falsifiable, comparative dynamics are central in forming empirically testable hypotheses in intertemporal economic models and coming to a complete understanding of the underlying economic theory, it is worthwhile to rectify these shortcomings. Indeed, by doing so, new and complementary results are provided about Grossman’s (1972) health capital model.====As for the definition of intrinsic comparative dynamics, they are defined as the refutable comparative dynamics of an intertemporal optimization problem that follow from (i) the assumption that an interior, locally differentiable solution exists, and (ii) the basic assumptions that form the underlying economic theory. In the neoclassical utility maximization model, for example, the negative semidefiniteness of the Slutsky matrix is its intrinsic comparative statics, inasmuch as it follows solely from the assumptions that an interior, locally differentiable solution exists, and the basic assumptions that agents are price takers whose monotonic preferences are independent of prices and income. Moreover, these properties hold irrespective of other assumptions that might be placed on an agent’s preferences. Simply put, a goal of the present work is the derivation of comparative dynamics in the health capital model that are of the same generality and significance as the Slutsky matrix in neoclassical consumer theory.====It is important to understand that intrinsic comparative dynamics are not predicated on assumptions such as homogeneity, separability, or concavity of the integrand or transition functions, or on functional form assumptions. While such assumptions are common and can generate some refutable results, they transcend those required for a differential characterization of the comparative dynamics and thus do not yield intrinsic results. Intrinsic comparative dynamics, on the other hand, are basic (or fundamental) qualitative properties of an intertemporal optimization problem, in that they are the implied by the ubiquitous optimization assertion employed in economics. Accordingly, they must be taken seriously in empirical work. Moreover, they hold irrespective of any additional assumptions that might be placed on a model.====In light of the above, the first contribution is the demonstration that there are no refutable properties possessed by the individual partial derivatives of the consumption and investment demand functions – the traditional comparative dynamics expressions – in the health capital model of Grossman (1972) under basic assumptions, as summarized in Proposition 1. As a result, such expressions do not, in general, generate empirically testable restrictions. This deduction implies that in order to generate refutable propositions for the aforesaid partial derivatives, one must impose further assumptions on an agent’s preferences, restrict the solution to the model in one way or another, or place restrictions on the model’s parameters, all of which go beyond the basic assumptions that define the underlying economic theory of rational health capital accumulation.====One byproduct of the first contribution is that it identifies the partial derivatives that are key to the derivation of refutable results. For example, Proposition 1 shows that the law of demand does not necessarily hold for consumption, and that a sufficient condition for it is that the lifetime marginal utility of wealth is a nondecreasing function of the price of consumption. That is, Proposition 1 shows that if the lifetime marginal utility of wealth does not decrease as the price of consumption increases, then consumption obeys the law of demand. Similarly, consumption might be either normal or inferior, in general. But the proposition identifies a simple necessary and sufficient condition for it to be normal, to wit, the lifetime indirect utility function is concave in wealth.====A second byproduct is that it demonstrates that several assertions made about the model’s comparative dynamics are not generally true. Take, for example, the claim that the model implies healthier individuals invest more in their health. But as shown by Proposition 1, a positive relationship between the stock of health and health investment is not an intrinsic property of the model. Indeed, sufficient conditions for the opposite to hold are that health and wealth are complements in the lifetime indirect utility function and that the said function is strongly concave in health, both of which are plausible. Another claim that is not intrinsic to the model is that the law of demand holds for health investment. This, of course, cannot be too surprising, in that the law of demand is not intrinsic to the much more simple neoclassical utility maximization model. For the final example, note that it is thought that an increase in the rate of health depreciation leads to a decrease in health investment. But it too is not an intrinsic property of the model.====Despite the first contribution, it might come as a surprise to learn that the health capital model does indeed possess refutable comparative dynamics of the intrinsic variety, the derivation of which represents the second contribution. The intrinsic comparative dynamics take the form of a positive semidefinite generalized Slutsky-like matrix, in which linear combinations of the individual partial derivatives of the consumption and investment demand functions comprise the entries of the matrix. Consequently, the sign pattern implied by its positive semidefiniteness means that refutable results are available for linear combinations of the individual partial derivatives of the consumption and investment demand functions, but not necessarily for the individual partial derivatives themselves. As such, the second contribution provides a complementary way to understand the first, to wit, that given basic assumptions of the health capital theory, one cannot derive refutable results for the individual partial derivatives of the consumption and investment demand functions.====While the aforesaid generalized Slutsky-like matrix is as basic to health capital theory as the Slutsky matrix is to neoclassical consumer theory, both being implications of an optimization assertion, it is not directly observable with typical market data. As a result, empirical testing of its properties presents a greater challenge than does that of the neoclassical Slutsky matrix, but it is not necessarily impossible. The primary difficulty in empirically testing the positive semidefiniteness of the generalized Slutsky-like matrix is that partial derivatives of the lifetime indirect utility function appear in it, as shown by Proposition 3. Despite this difficulty, the intertemporal duality theory set forth by Cooper and McLaren (1980) might provide a solution. It offers a way to recover the unobservable partial derivatives of a lifetime indirect utility function using market data, albeit for an intertemporal consumer model that is considerably less general than the health capital model studied herein. For that reason, in order for the intertemporal duality theory to be useful for empirical purposes in the health capital model, the theory must be extended to handle the model’s inherently more complicated structure.====The above contributions are achieved in a version of Grossman’s (1972) model that eschews those aspects related to the allocation of time and household production. Consequently, the derivations that follow are simplified, thereby permitting one to follow them with relative ease. Not surprisingly, the simplification permits a more intuitive and complete understanding of the results, which can then serve as a benchmark case for the derivation of refutable hypotheses in extensions of the model, extensions that might include household production, labor supply, risk, and the like. Finally, note that the present work does not defend nor criticize Grossman’s (1972) model.",New insights in the canonical model of health capital,https://www.sciencedirect.com/science/article/pii/S016548962100010X,2 February 2021,2021,Research Article,128.0
Dizdar Deniz,"Department of Economics, Université de Montréal, C.P. 6128 succ. Centre-Ville, Montréal, H3C 3J7, Canada","Received 17 January 2020, Revised 24 November 2020, Accepted 13 January 2021, Available online 23 January 2021, Version of Record 10 February 2021.",https://doi.org/10.1016/j.mathsocsci.2021.01.002,Cited by (1),"A central result in Fullerton and McAfee’s (1999) analysis of fixed-prize research tournaments shows that if firms’ heterogeneous marginal effort costs are publicly known and the procurer can charge non-discriminatory entry fees, restricting entry to the two most efficient firms is optimal under a (fairly restrictive) sufficient condition on the form of heterogeneity. This note provides a complementary result. I prove a sharp, worst-case bound (across all linear cost structures) for the ratio between the cost of procuring a given total effort from the optimal number of contestants and the corresponding cost for a tournament featuring only the two most efficient firms. The analysis confirms the attractiveness of the smallest possible tournament, with some notable exceptions.","Research tournaments are important mechanisms for procuring innovations. They may mitigate many of the problems that plague traditional procurement contracts in this case, such as non-verifiable quality of the innovation, or difficulties to monitor and verify the efforts and costs of suppliers.==== In an influential paper, Fullerton and McAfee (1999, henceforth FM) studied how to design a fixed-prize research tournament in a setting with ==== potential firms/suppliers that are heterogeneous with respect to their marginal effort costs. FM focused on how such heterogeneity affects the answers to two major design questions for the procurer. How many firms should be admitted to the tournament? How should these ==== be selected when costs are private information prior to the tournament?====FM characterized equilibrium efforts and expected profits in a simultaneous-move, fixed-prize tournament with prize ====, when winning probabilities are determined by a standard Tullock success function and (constant) marginal effort costs are common knowledge among the ==== contestants. Having additional contestants (weakly) increases total effort, which determines the distribution of the quality of the best innovation, but (weakly) decreases firms’ profits and hence their willingness to pay for entering the tournament.====For the case where all firms’ costs are publicly known and the procurer can choose ==== and set a ==== to select a restricted number of contestants, a central result in FM provides a condition which ensures that the total cost of “procuring” a given level of total effort is minimized by the smallest possible tournament, between the two most efficient firms. FM’s sufficient condition, which concerns the differences between firms’ marginal costs, covers many interesting cases but it is violated whenever one of the ==== firms is a (sufficiently) close competitor for another firm.====The purpose of this note is to provide a complementary result that quantifies the worst-case performance of the tournament with ==== contestants: I derive a sharp bound for the ratio between the minimum cost of procuring a given level of total effort (the cost for the optimal value of ====) and the corresponding cost for ====, across all (linear) cost structures and all values of ====. As this bound is quite close to ====, compared to how costly it can be (for some cost structures) to let more than two firms enter, the result generally confirms the attractiveness of the two-player tournament. However, the analysis also provides intuitive insights about when implementing a larger tournament may significantly lower costs for the procurer. Roughly speaking, this is the case if the asymmetry between the two most efficient firms is substantial but not too extreme and if there are other firms that are close competitors for the “second-best” firm.====For cases where marginal costs are i.i.d. and privately known prior to the tournament (but become common knowledge among the contestants before they choose their efforts), FM showed that the ====, an all-pay auction with a small interim prize for entry, can be used to select any desired number of lowest-cost contestants. Comparing the “cost-effectiveness” of tournaments with different numbers of contestants in this incomplete-information model is rather involved, and is beyond the scope of this note.",On the optimality of small research tournaments,https://www.sciencedirect.com/science/article/pii/S0165489621000081,23 January 2021,2021,Research Article,129.0
Kurz Sascha,"Fakultät für Mathematik, Physik und Informatik, Universität Bayreuth, Germany","Received 9 June 2020, Revised 16 October 2020, Accepted 5 January 2021, Available online 14 January 2021, Version of Record 22 January 2021.",https://doi.org/10.1016/j.mathsocsci.2021.01.001,Cited by (1),"The remoteness from a simple game to a weighted game can be measured by the concept of the dimension or the more general Boolean dimension. It is known that both measures can be exponential in the number of voters. For complete simple games it was only recently shown in O’Dwyer and Slinko (2017) that the dimension can also be exponential. Here we show that this is also the case for complete simple games with two types of voters and for the Boolean dimension of general complete simple games, which was posed as an open problem in O’Dwyer and Slinko (2017).","Simple games can be viewed as binary yes/no voting systems in which a proposal is pitted against the status quo, see Taylor et al. (1999) for a general introduction. In the subclass of weighted games each voter has a non-negative weight and a proposal is accepted if the weight sum of its supporters meets or exceeds a preset positive quota. The representation complexity of weighted games is rather low, which makes them interesting candidates for real-world voting systems. More precisely, for a weighted game it is sufficient to list the weights of the ==== voters and the quota. For each subset ==== of the ==== voters we may store the information whether a proposal supported by ==== would be accepted or rejected. This approach would need ==== bits.==== However, each simple game can be written as the intersection of a finite number of weighted games and the smallest possible number is called the dimension of the simple game, see Taylor and Zwicker (1993). Unfortunately, the dimension can also be exponential in the number of voters, see e.g. Korshunov (2003) and Taylor et al. (1999). Complete simple games lie in between the classes of simple and weighted games, see Carreras and Freixas (1996). Here, the game does not admit weights for the voters, while the voters are completely ordered (which will be defined more precisely in the next section). E.g., the voting rules of the Council of the European Union according to the Treaty of Lisbon can be modeled as a non-weighted complete simple game. In O’Dwyer and Slinko (2017) it was shown that the dimension of a complete simple game can also be exponential in the number of voters. However, the stated construction requires that the number of types of different voters also increases without bound. Here we show that the dimension of a complete simple game can be also exponential in the number of voters for just two different types of voters. If all voters are of the same type, then the game is weighted, i.e., has a dimension of 1. The concept of the dimension and the intersection of weighted games were generalized to, more general, Boolean combinations of weighted games, see e.g. Faliszewski et al. (2009) and Korshunov (2003). For simple games the corresponding Boolean dimension can also be exponential in the number of voters, see e.g. Faliszewski et al. (2009) and Korshunov (2003). Whether the Boolean dimension of a complete simple game can also be exponential in the number of voters was posed as an open problem in O’Dwyer and Slinko (2017). Here we answer this question by a construction and show that the Boolean dimension is polynomially bounded in the number of shift minimal winning coalitions (or vectors) and voters. We also answer another open question from O’Dwyer and Slinko (2017) and consider possible restrictions on the weights that still allow a representation of a complete simple game as the intersection of weighted games.====The paper is organized as follows. In Section 2 we introduce the necessary preliminaries. Our results are presented in Section 3.",A note on the growth of the dimension in complete simple games,https://www.sciencedirect.com/science/article/pii/S0165489621000019,14 January 2021,2021,Research Article,130.0
"Neme Pablo,Oviedo Jorge","Instituto de Matemática Aplicada San Luis, Universidad Nacional de San Luis and CONICET, San Luis, Argentina,RedNIE, Argentina","Received 14 June 2020, Revised 25 November 2020, Accepted 22 December 2020, Available online 4 January 2021, Version of Record 15 January 2021.",https://doi.org/10.1016/j.mathsocsci.2020.12.002,Cited by (1),For a many-to-one matching market where firms have strict and ,"A large part of the matching literature studies many-to-one matching markets. The agents in these markets are divided into two disjoint sets: The ====-side of the market, namely resident doctors, students, workers, etc., and the ====-side, namely hospitals, colleges, firms, etc. The main property studied in the matching literature is stability. A matching is called stable if all agents have acceptable partners and there is no unmatched pair (hospital–doctor, college-student, firm–worker, etc.), where both agents would prefer to be matched to each other rather than staying with their current partners under the proposed matching. Each agent has a preference list that determines an order over the agents or sets of agents on the other side of the market, with the possibility of staying unmatched. In this paper, the agents on the many-side have ====-responsive and strict preferences.====Linear programming is a widely used mathematical tool in matching theory. Each matching can be represented by an assignment matrix called the ==== of the matching. Roth et al. (1993), for the marriage market, introduce a linear programme that characterizes all stable matchings as the integer solutions.====Linear programming approaches have been developed for the theory of stable matching markets also by Abeledo and Blum (1996), Abeledo et al. (1996), Abeledo and Rothblum, 1995, Abeledo and Rothblum, 1994, Baïou and Balinski (2000), Fleiner (2001), Fleiner (2003), Sethuraman et al. (2006), Teo and Sethuraman (1998), and many others.====Lotteries over stable matchings – stable fractional matchings – have been studied in many instances in the literature. For the marriage market, Roth et al. (1993) studied stable fractional stable matchings via linear programming.====Each entry of an incidence vector of a stable fractional matching can be interpreted as the time that each agent spends with one agent on the other side of the market. For a stable fractional matching, it can happen that two agents, one of each side of the market, have an incentive to increase the time that they spend together at the expense of those matched agents that they like less than each other at a stable fractional matching. To study a “good” fractional solution, the idea is to avoid this and prevent that agents have incentives to “block” the stable fractional matching in a fractional way. For a marriage market, Roth et al. (1993) define a ==== as a stable fractional matching that fulfils non-linear equalities that represent this non-blocking condition mentioned above. In other words, a stable fractional matching that fulfils the non-linear equalities from Roth et al. (1993), is a strongly stable fractional matching. Neme and Oviedo (2020) give a characterization of the strongly stable fractional matching for the marriage market. Our work extends their result and provides a characterization for the set of many-to-one strongly stable fractional matchings. We extend the strong stability condition from Roth et al. (1993) to a many-to-one matching market. Our first result states that a strongly stable fractional matching is represented by a convex combination among stable matching that are ordered in the eyes of all firms (Theorem 1).====A related paper is Echenique et al. (2013). In their market, they assume that one can observe matchings, not agents’ preferences. They classified agents by their types and, in this way, the market is understood as a many-to-many matching market. The notion of stability that they consider, coincides with the notion of strong stability considered in this paper.====In the school choice set-up, strong stability for lotteries has been introduced by Kesten and Ünver (2015) which they called ex-ante stability for lotteries. In this market, they deal with indifferences in the priority of the schools. Kesten and Ünver (2015) also present a fractional deferred-acceptance algorithm that computes a unique strongly ex-ante stable random matching. Their paper analyses the strategy proofness and efficiency of this mechanism. Our characterization goes in another direction, we study the relationship among the stable matchings that are involved in the lotteries.====Bansal et al. (2007) and Cheng et al. (2008) study the concept of cycles in preferences and cyclic matchings for many-to-many and many-to-one matching markets, respectively. These papers are an extension of Irving and Leather (1986). They prove that finding the full set of stable matchings via cycles in preference and cyclic matchings, requires a polynomial time algorithm. To seek for cycles in preferences, these authors first reduce the preference lists of all agents. We present the reduction procedure for our market in the Appendix. This reduction procedure allows us to find cycles in preferences. Since the cycles of a reduced list are disjoint, we extend the definition of cyclic matching to a set of cycles in the reduced preference profile.====Following the extension of cyclic matching used by Bansal et al. (2007) and Cheng et al. (2008), we define a ==== generated by a stable matching ==== as the set of all cyclic matchings of ==== (including ====). Then, we characterize a strongly stable fractional matching as a lottery over stable matchings that belong to the same connected set (Theorem 2). Moreover, by Theorem 1, we prove that the stable matchings that belong to the same connected set, also have the decreasing order in the eyes of all firms. In this way, we characterize the set of all strongly stable fractional matchings as the union of the convex hulls of these connected sets (Corollary 1).====Roth et al. (1993), (in Corollary 21) proved a necessarily condition that states that in a strongly stable fractional matching, each agent is matched with at most two agents of the other side of the market. Schlegel (2018) generalizes this necessarily condition for the school choice set-up with strict priorities (similar setting as ours). He shows that a strongly stable fractional matching fulfils that each worker has a positive probability to be matched to at most two distinct firms, and for each firm, all but possibly one position are assigned deterministically. For the one position that is assigned by a lottery, two workers have a positive probability of been matched to the firm (here stated as Corollary 2). Further, although he proves that a strongly stable fractional matching is “almost” integral, he does not describes which agents are matched (there are several “almost” integral stable fractional matchings that are not strongly stable, Example 1 presents an “almost” integral stable fractional matching that is not strongly stable). Recall that our characterization gives a necessary and sufficient condition for a stable fractional matching to be strongly stable. As a particular case, our characterization gives an alternative proof for these two results, for the school choice set-up due to Schlegel (2018) is straightforward, and for the marriage market due to Roth et al. (1993), it is only necessary to set all quotas of all firms equal to one. Moreover, our characterization shows explicitly which are the matched agents in a strongly stable fractional matching, through the stable matching involved in the convex combination (==== (9) in proof of Theorem 2).====This paper is organized as follows: In Section 2 we formally introduce the market and preliminary results. In Section 3 we define a strongly stable fractional matching and prove that it can be represented by a convex combination over stable matchings that are ordered for all firms. In Section 4, we discuss cycles and cyclic matching properties used in the characterization result. Further, we present our characterization of a strongly stable fractional matching. The Appendix contains the reduction procedure, lemmas, and proofs of the lemmas needed for our characterization.",On the set of many-to-one strongly stable fractional matchings,https://www.sciencedirect.com/science/article/pii/S0165489620301098,4 January 2021,2021,Research Article,131.0
Shrivastav Sumit,"Indira Gandhi Institute of Development Research (IGIDR), Film City Road, Santosh Nagar, Goregaon (E) - Mumbai, 400065, India","Received 25 February 2020, Revised 26 November 2020, Accepted 22 December 2020, Available online 31 December 2020, Version of Record 5 January 2021.",https://doi.org/10.1016/j.mathsocsci.2020.12.003,Cited by (4),This paper analyzes implications of network compatibility and competition on process innovation in differentiated network goods ,"This paper attempts to analyze firms’ incentives to invest in cost reducing R&D in differentiated network goods duopoly where identical competing firms non-cooperatively choose their respective R&D investment levels before they engage in product market competition. The framework considered in this analysis (a) distinguishes between degree of product differentiation and extent of network compatibility, and (b) allows to examine the implications of imperfect compatibility of any order between the two networks (one network of each firm’s consumers), and, thus, encompasses perfect network compatibility and perfect network incompatibility as two special cases. Network-goods market is replete with examples of imperfectly compatible goods. One prominent example is telecommunication where cellular networks may be partially compatible to each other. If there are two telephone networks, then making within-network calls is cheaper and with better quality than making calls from one network to another. That is, a consumer derives more utility from connecting to a user of the same network than from connecting to a user of different network, implying that the two networks are partially compatible. Katz and Shapiro (1994) comments on how partial compatibility can manifest itself in real world in at least two ways. One is compatibility with attenuated benefits. For example, text from one word processing program can be used in another word processing program but the formatting codes may be different. The example on telecommunication too falls in this category. The second way partial compatibility can be seen in practice is when compatibility extends to some parts but not to the others. An important example is automobile sector. Considering post-purchase network benefits, firms make some of their replacement parts compatible with their rival firms’ products but other parts incompatible. We mention here that observed imperfect compatibility of network goods may be due to technological constraints faced by firms, costs involved in achieving full compatibility, firms’ patenting activities, strategic compatibility choice by firms etc. It seems that the extent to which networks of competing firms are compatible has significant implications on firms’ strategic R&D behavior.====This paper compares and contrasts equilibrium outcomes under Bertrand competition with that under Cournot competition in the product market in order to understand the implications of product market competition on firms’ optimal R&D behavior in alternative scenarios.====It first demonstrates that the strategic nature of competing firms’ R&D investments crucially depends on the relative magnitudes of effective network compatibility and product differentiation measure. Each firm perceives that its own R&D investment and its rival’s R&D investment are strategic substitutes (strategic complements) if effective network compatibility is smaller (larger) than the extent of product substitutability. This is true regardless of whether there is Cournot competition or Bertrand competition in the product market. The reason is, investment in R&D by a firm reduces its own marginal cost of production which enables that firm to behave more aggressively, i.e. to expand output or undercut price depending on the mode of competition in the product market. In the case of smaller (larger) effective network compatibility compared to the extent of product substitutability, the direct negative effect of the rival’s aggressive behavior on a firm’s market demand dominates (is dominated by) the corresponding indirect positive effect due to increase in that firm’s effective network size and, thus, a firm’s marginal gain from investing in R&D decreases (increases) due to increase in its rival’s R&D investment. Clearly, in absence of network externalities, as in the case of standard non-network goods oligopoly, the condition for competing firms’ cost reducing R&D investments to be strategic substitutes is always satisfied, ceteris paribus.==== ==== It is well argued that the strategic nature of competing firms’ choice variables is likely to have significant bearings to the equilibrium outcomes (Bulow et al., 1985). However, to the best of our knowledge, existing studies on process innovation in network goods oligopoly implicitly assume that competing firms’ R&D investments are strategic substitutes (Saaskilahti, 2006, Naskar and Pal, 2020 etc.). This paper offers a more general analysis that allows for R&D investments to be strategic complements as well.====Next, it shows that in the equilibrium, Cournot firms always undertake process innovation, regardless of whether R&D investments are strategic substitutes or strategic complements. In contrast, if R&D investments are strategic substitutes, Bertrand firms do not always undertake process innovation in the equilibrium. It also demonstrates that (i) when R&D investments are strategic complements, Bertrand competition results in higher R&D investment by each firm in the equilibrium compared to that under Cournot competition, and (ii) when R&D investments are strategic substitutes and firms undertake process innovation regardless of the nature of product market competition, each firm invests more (less) in R&D in the equilibrium under Bertrand competition than under Cournot competition provided that the strength of network externalities is (not) sufficiently high, given the extent of network compatibility and degree of product differentiation. These are new results.====A number of studies have attempted to compare equilibrium investments in cost reducing process R&D under Cournot and Bertrand competition. Considering non-network goods oligopoly, Qiu (1997) and Lin and Saggi (2002) demonstrate that a Cournot firm always invests more in process R&D than a Bertrand firm.==== ==== The reason behind this result is as follows. Investment in cost reducing innovation by a non-network good producing firm has two-fold effect on its profit: (i) the direct positive effect, due to reduction in its marginal cost of production, and (ii) the strategic effect, which is the indirect effect of a firm’s R&D investment on its profit via the effect of its R&D investment on the rival firm’s strategic variable, quantity or price, for product market competition. A firm’s marginal cost reduction, due to an increase in its R&D investment, induces contraction in its rival’s output (price) under Cournot (Bertrand) competition and that, in turn, enhances (reduces) its profit. As a result, under Cournot (Bertrand) competition, the strategic effect of R&D investment is positive (negative), which further reinforces (works against) the positive direct effect, in non-network goods duopoly. In network goods duopoly, however, the strategic effect works not only through the rival’s strategic variable for product market competition, but also through strategic interaction between a firm’s own and its rival firm’s strategic variable, and consumers’ expectations regarding network sizes. Note that in the presence of network externalities, firms’ effective market demands are endogenous. If a firm produces more output or sets lower price, consumers’ expectations regarding that firm’s network size increases, which in turn has a positive effect on the equilibrium profit of that firm. When R&D investment levels are strategic complements, this strategic effect via consumers’ expectations is sufficiently large to overcompensate the negative strategic effect via rival’s strategic variable under Bertrand competition. This induces Bertrand firms to invest more in process R&D than Cournot firms. However, in case of strategic substitutes, for a Bertrand firm to invest more in R&D than a Cournot firm, overcompensation by this strategic effect via consumers’ expectations depends on the critical strength of network externalities.====There is a vast and growing literature on innovation by oligopolistic firms in the presence of network externalities. Following Farrell and Saloner, 1985, Farrell and Saloner, 1986 and Katz and Shapiro, 1985, Katz and Shapiro, 1986, several authors have analyzed the relation between network externalities and firms’ incentives for product innovation in oligopolies.==== ==== However, the issue of process innovation in the presence of network externalities has received rather limited attention in the literature so far. To the best of our knowledge, Boivin and Vencatachellum (2002), Saaskilahti (2006) and Naskar and Pal (2020) are the only exceptions in this regard. Considering that networks are perfectly compatible, Boivin and Vencatachellum (2002) examine the impact of network externalities on process R&D in a homogeneous network goods Cournot duopoly. They argue that in the presence of network externalities, firms invest at least as much in process R&D as in the absence of network externalities.==== ==== Saaskilahti (2006) allow for R&D spillover and imperfect network compatibility, but restricts the analysis to the case of price competition in a linear city model. Very recently, Naskar and Pal (2020) offer an analysis of differentiated network goods producing firms’ incentives to invest in cost reducing process R&D under alternative modes of product market competition, quantity and price, in two alternative scenarios: (i) perfect incompatibility of networks and (ii) the extent of network compatibility is same as the extent of product substitutability. This paper is closely related to Naskar and Pal (2020). However, unlike Naskar and Pal (2020), this paper considers a wider parameter space that allows for the extent of network compatibility to vary from zero (i.e. perfect incompatibility) to one (i.e. perfect compatibility). The present analysis encompasses the scenarios considered in Naskar and Pal (2020) as special cases. More importantly, while the scope of Naskar and Pal (2020) is restricted to the case of strategic substitutability of R&D investments by firms, this paper considers both strategic substitutability and strategic complementarity.====The remaining part of the paper is organized as follows. Section 2 presents the framework of the model. Section 3 and Section 4 analyze the equilibrium R&D behavior of firms under Bertrand competition and Cournot competition respectively. Section 5 presents the main result of Cournot–Bertrand R&D comparison. Section 6 deals with further comparison between Bertrand and Cournot competition. Section 7 offers a few extensions of the model that help us to understand (a) the role of R&D spill-over (Section 7.1), (b) the effect of network compatibility on R&D investments in the equilibrium under alternative modes of product market competition (Section 7.2) and (c) implications of alternative expectations formation by consumers on the equilibrium outcomes (Section 7.3). Section 8 concludes.","Network compatibility, intensity of competition and process R&D: A generalization",https://www.sciencedirect.com/science/article/pii/S0165489620301104,31 December 2020,2020,Research Article,132.0
"Lahkar Ratul,Mukherjee Saptarshi","Department of Economics, Ashoka University, Rajiv Gandhi Education City, Sonipat, Haryana 131029, India,Department of Humanities and Social Sciences, Indian Institute of Technology Delhi, Hauz Khas, New Delhi, India","Received 21 April 2020, Revised 13 November 2020, Accepted 30 November 2020, Available online 11 December 2020, Version of Record 21 December 2020.",https://doi.org/10.1016/j.mathsocsci.2020.11.004,Cited by (9),"Due to externalities, the equilibrium behavior in aggregative games is not efficient in the sense of maximizing aggregate payoff. We characterize conditions such that efficiency can be globally implemented in such games under evolutionary dynamics. If payoffs satisfy certain important ","Implementing efficient outcomes has been a topic of abiding interest in economics and is the key objective in the mechanism design literature. The approach in much of this literature is that a planner designs a mechanism upon which agents instantaneously coordinate on the efficient outcome desired by the planner. The most well known class of mechanisms is the Vickrey–Clarke–Groves (VCG) direct mechanism that renders truthful revelation of type the dominant strategy for each agent (Vickrey, 1961, Clarke, 1971, Groves, 1973). Several authors have, however, noted practical problems with this approach to implementation, particularly when the number of agents involved is large; for example, collecting information about types from a large number of agents and computing assignments based upon reported types (Sandholm, 2005), concern about revealing confidential information about types and the possibility of cheating by the bid taker and competing bidders (Rothkopf et al., 1990, Rothkopf, 2007). In view of such problems, an alternative approach to implementation has emerged which views the problem from an evolutionary perspective in which optimal strategies in a mechanism design problem emerges gradually rather than instantaneously (Sandholm, 2002, Sandholm, 2005, Sandholm, 2007, Phelps et al., 2010, Lahkar and Mukherjee, 2019).====In this paper, we adopt this evolutionary approach to implementation in a particular class of games called large population aggregative games. These are games played by a large population (a continuum) of agents in which the payoffs depend upon an agent’s own strategy and the aggregate strategy level in the society.==== ==== Aggregative games are of interest because of their analytical tractability and the fact that important economic applications can be modeled as such games. One such application is the public goods problem considered in our earlier paper (Lahkar and Mukherjee, 2019). In this paper, we generalize our earlier approach to a wider class of aggregative games and analyze two new applications—public bads and the tragedy of the commons. Due to the dependence of payoffs on aggregate strategy, aggregative games are beset by the problem of externalities. This typically causes a distinction between the Nash equilibrium and the efficient state, where the efficient state is the state that maximizes aggregate payoff in the society. For example, in the large population public goods game considered in Lahkar and Mukherjee (2019), externalities are positive because higher contribution by an agent enhances the welfare of others. Hence, the efficient state requires significant positive contribution by all agents whereas the Nash equilibrium entails the minimum possible contribution.====The solution to this problem is to implement a ==== (Sandholm, 2002) to the original game. Under this scheme, a transfer equal to the externality imposed by an agent is made to the agent, with the externality being calculated with respect to the current social state. Sandholm (2002) shows that the resulting externality adjusted game is a potential game (Monderer and Shapley, 1996, Sandholm, 2001) with the aggregate payoff function of the original game as its potential function. This is important because all well-known dynamics in evolutionary game theory converge to a maximizer of the potential function in a potential game (Sandholm, 2001). Hence, if the aggregate payoff function of the original aggregative game has a unique maximizer, and we identify conditions such that it does, then evolutionary dynamics in the externality adjusted game will converge globally to its maximizer, which is the efficient state.====This is the key insight behind evolutionary implementation as developed by Sandholm (2002).==== ====
 Lahkar and Mukherjee’s (2019) analysis of the public goods problem extends this idea in two important ways. First, they allowed for a more general payoff function which need not be decomposable into a common and an idiosyncratic part, as was the case with Sandholm’s models. Second, they allowed the strategy set in their model to be continuous. While this approach raises certain measure theoretic complications, it is useful in an aggregative game like the public goods model in allowing for a more exact and parsimonious characterization of the Nash equilibrium and the efficient state.==== ==== We discuss both these features of aggregative games in further detail in our earlier paper. Using the potential game approach of Sandholm, 2002, Sandholm, 2005, that paper establishes evolutionary implementation of the efficient state in the public goods under standard deterministic evolutionary dynamics. In this paper, we retain the same framework as Lahkar and Mukherjee (2019) by considering an aggregative game with a continuous strategy set and multiple types of agents, each type characterized by a distinct payoff function. As in that paper, the continuous nature of the strategy set simplifies the characterization of efficiency greatly. We then make the following contributions beyond our earlier paper.====First, we generalize evolutionary implementation to a broader class of aggregative games that includes two new applications—public bads and the tragedy of the commons. In economics, these models are used to represent a wide variety of social problems. Our evolutionary analysis provides an important insight into how a large society may be nudged towards the optimal outcome in such situations. Second, we identify the broader class of aggregative games where global evolutionary implementation holds. These are games which satisfy two important concavity conditions-strict concavity of the payoff function of the game with respect to individual strategy (Assumption 3.1) and a weaker concavity condition of the aggregate strategy level with respect to the social state (Assumption 3.3). These conditions ensure that the aggregate payoff function of an aggregative game has a unique maximizer, to which evolutionary dynamics must converge globally in the externality adjusted game.==== ==== Third, we generalize the method of characterizing the unique efficient state of any aggregative game that satisfies the aforementioned concavity conditions (Proposition 3.5). This method is computationally simple but depends crucially upon the aggregative nature of the game. Indeed, applications aside, this analytical tractability is a major reason why we focus on such games. Finally, by generalizing evolutionary implementation to aggregative games with continuous strategy sets, this paper also widens the applicability of the continuous strategy approach to evolutionary game theory that was pioneered by Oechssler and Riedel, 2001, Oechssler and Riedel, 2002.====The public goods model in our earlier paper was an example of positive externalities. The two other applications in the present paper are examples of negative externalities. Together, these three examples provide another general insight about aggregative games. It is that the important concavity conditions that ensure a unique maximizer of the aggregate payoff function are naturally satisfied in such games irrespective of the nature of externalities. This is of significance because we can then apply deterministic evolution to such games to globally implement efficiency. Convergence under deterministic dynamics is fairly rapid. On the other hand, in the type of applications considered by Sandholm, 2002, Sandholm, 2005, Sandholm, 2007, negative externalities do ensure concavity and, hence, a unique maximizer of the aggregate payoff function. But under positive externalities, the relevant payoff functions may be convex which implies that the aggregate payoff function has multiple local maximizers. In that case, global implementation of efficiency would have required the use of stochastic evolutionary methods (Sandholm, 2007). Convergence under stochastic methods is, however, much slower than under deterministic methods.==== ====One question that may arise is why do we need the general analysis of this paper. Could not we have followed the specific approach of our earlier public goods model and analyzed the present applications by relying on the particular features of these models? For the public bads application, we could have. In fact, for this application, we could have derived our conclusions simply as corollaries to our earlier results in Lahkar and Mukherjee (2019) by reversing the order structure on the strategy set. But the general analysis serves the role of highlighting the underlying similarities between these models despite their economic differences. Externalities are different in the two models as well as the origins of those externalities.==== ==== Nevertheless, both models satisfy the important concavity conditions of this paper which drives evolutionary implementation. A straightforward application of the public goods approach to the tragedy of the commons is difficult. The tragedy of the commons is fundamentally different in that it models a common resource instead of a public resource. This is reflected in the payoff functions of this model where even the benefit part depends upon individual strategy whereas in the public goods model, benefits are independent of individual strategy. In particular, this makes characterization of Nash equilibrium more challenging in this model.==== ==== Yet, despite these conceptual and technical differences, our general analysis shows that there are fundamental similarities in the public resource and common resource models which drive evolutionary implementation. The analysis of the tragedy of the commons also shows that our methodology of evolutionary implementation in aggregative games is more widely applicable than just models of public resources.====The idea of a variable externality pricing scheme is closely related to the classical notion of Pigouvian pricing (Pigou, 1920). Both schemes rely on imposing a transfer on agents that force them to internalize the externality they create. The key difference is that the Pigouvian price is calculated with respect to the externality at the efficient state, evolutionary implementation calculates the transfer with respect to the level of externality. Thus, while both implement the efficient solution, evolutionary implementation is informationally less demanding. The planner may not know the efficient state a priori which would make calculating the Pigouvian price difficult. In contrast, there are well established empirical methods to calculate current externality (Lin, 1976). In our view, therefore, evolutionary implementation provides a more feasible way to combat externalities. We discuss this issue in further detail in Section 6.====The rest of the paper is as follows. In Section 2, we discuss the general model of population games with continuous strategy sets and define externalities in such games. We also define potential games and show that the externality adjusted game is a potential game. Section 3 applies these results to aggregative games and characterizes the efficient state in such game. Section 4 discusses evolutionary implementation. In Section 5, we apply our model to the public bads model and the tragedy of the commons. Section 6 discusses the difference between Pigouvian pricing and evolutionary implementation. Section 7 concludes.",Evolutionary implementation in aggregative games,https://www.sciencedirect.com/science/article/pii/S0165489620301074,11 December 2020,2020,Research Article,133.0
Lim Sokchea,"John Carroll University, OH, USA","Received 7 March 2020, Revised 31 July 2020, Accepted 21 September 2020, Available online 8 December 2020, Version of Record 23 December 2020.",https://doi.org/10.1016/j.mathsocsci.2020.09.004,Cited by (1),Could the policy to promote overseas migrant work be to blame for the sluggish development in some South Asian countries? I employ a macro-dynamic model of two ,"In an era of industrialization, Lewis (1954) argues that new industries can be created and old industries expanded without any shortage of labor in countries with abundant cheap labor that moved from agriculture into manufacturing. However, since the 1970s many South Asian countries including Bangladesh, India, Pakistan, Nepal, and Sri Lanka have put in many efforts to promote overseas migrant work. This policy was triggered by significant labor demand in the Middle East and foreign currency earnings as migrant workers send remittances back to their families at home (Lim and Basnet, 2017). According to the World Bank’s Migration and Remittances Data, by 2017 India had over 16 million of its citizens working outside of India, followed by Bangladesh and Pakistan, which exported 7.7 million and 6 million migrants, respectively. Nepal and Sri Lanka each had over 1.7 million of its citizens working abroad. These workers also sent in billions of dollars annually back home. For example, the flow of remittances in Nepal amounted to more than 30% of its GDP in 2016. However, according to the latest 2019–2020 classification by the World Bank, Nepal is still in a low-income group with income less than $1,025; Bangladesh, India and Pakistan are still in lower-middle-income group; and Sri Lanka has just graduated into the upper-middle-income group.====In views of Lewis’s (1954) argument, could the policy to promote overseas migration be to blame for the sluggish development in these poor Asian countries? In this paper, I employ a macro-dynamic model of two small open economies – a rich, labor-importing country and a developing, labor-exporting country – to examine the macroeconomic impacts of this policy. The model employed in this study combines the model used in Lim and Morshed (2017) and that used in Chatterjee and Turnovsky (2018). I extend the model used in Lim and Morshed (2017) by introducing Chatterjee and Turnovsky’s (2018) collateral effect of remittances in the borrowing constraint of the labor-exporting country. I also relax the assumption of inelastic supply of capital in the rich country by allowing capital to be converted from the traded goods with an adjustment cost. Different from Chatterjee and Turnovsky (2018), the model captures endogenous remittances that are linked to the household’s decision to migrate.==== ====Here I briefly describe the analytical framework. The focus should be on the developing country while the rich country is included mainly to capture the endogenous flow of migrant labor and remittances. Thus, there is an abstract in the structure of the rich country. The rich country is assumed to be a small open economy, so that it faces the world interest rate which is exogenously determined outside the model. The assumption of this rich, small open economy is, in fact, consistent with a sample of Middle Eastern countries like Bahrain, Kuwait, Oman, Qatar, Saudi Arabia, and United Arab Emirates, which employ significant amount of migrant labor in their economies. In the rich economy, households consume a traded good and leisure. The production of the traded good uses private capital, native labor, and migrant workers. Private capital can be converted from the traded good with an adjustment cost. The households in the rich country decide only between leisure and working in the domestic production. Thus, no labor migrates from the rich country to work in the developing country. In contrast, households in the developing country decide on the allocation of time between (i) leisure, (ii) domestic manufacturing production, (iii) and migrant work. Migrant workers earn a migrant wage and remit a portion of their earning after consuming in the rich country. This gives rise to an endogenous flow of remittances that is associated with migration, in line with the findings of Lim and Morshed (2015). Labor in both countries including migrant workers is paid with its respective marginal product.====Households in both countries are assumed to have access to the international financial market, but they are constrained by the upward-sloping supply curve of debt. That is, they are charged an interest rate that includes a borrowing premium above the given world interest rate. The borrowing premium – a proxy for the country’s risk – reflects the country repayment capacity and comprises the current level of debt relative to the size of its GDP. For the developing country, the risk premium also captures the country’s remittances as foreign currency earnings in the repayment capacity. Thus, a larger remittance inflow raises its foreign reserves and repayment capacity, thus reducing the risk premium. I assume that migration incurs a cost due to frictions that result from tightened immigration laws or stricter control over hiring migrant workers. This friction parameter also serves as the policy to promote migration of the poor country. For example, this friction can be reduced when the government of the poor country puts forward efforts to establish a formal process to find jobs for migrant labor. This policy is discussed in details in the next section.====The model is then calibrated to yield a long-run equilibrium consistent with the data for samples of Middle Eastern, labor-importing countries and South Asian, labor-exporting countries. In the calibration exercises, I show that remittances are expansionary for the labor-exporting, developing economy through its collateral impact. However, the policy to promote overseas migrant work is contractionary because the negative income effect due to labor migration significantly outweighs the expansionary effect from the remittance inflow. The loss of labor due to the policy hinders domestic capital accumulation, costing its long-run economic development. However, the welfare increases due to rising consumption as a result of increased remittance receipts. Additional results from applying the model to the case of Nepal show that the macroeconomic impacts of the policy to promote overseas migration are more pronounced for a poorer country although it receives a substantial share of remittances to its GDP. The collateral impact of remittances is also larger for a large share of remittances. However, with a large elasticity of wage at a lower level of income, Nepal also suffers a great loss of output due to migration.==== ====The paper contributes to the literature in two important ways. First, the paper provides a theoretical implication for the policy of many poor countries to promote overseas migrant work for its citizens, which has not been studied before. Second, the paper extends the macro-dynamic model in the literature by combining two features – the endogenous migration and the collateral effect of remittances. The extension allows us to evaluate two opposing impacts – the gain from remittances vs the loss of labor due to migration – on the labor-exporting country.====The remainder of the paper is organized as follows: Section 2 provides some background information. Section 3 details the analytical model. Section 4 derives the macroeconomic equilibrium. Section 5 presents and discusses the numerical calibration exercises including the exercise for Nepal, a country that receives a large share of remittances. Section 6 provides some robustness tests. Finally, Section 7 concludes the findings.",Policy to promote overseas migrant work: A macro-dynamic framework,https://www.sciencedirect.com/science/article/pii/S0165489620301062,8 December 2020,2020,Research Article,134.0
"Lou Youcheng,Wang Shouyang","MDIS, Academy of Mathematics and Systems Science, Chinese Academy of Sciences, China,Center for Forecasting Science, Chinese Academy of Sciences, China,School of Economics and Management, University of Chinese Academy of Sciences, China","Received 17 January 2020, Revised 4 November 2020, Accepted 22 November 2020, Available online 30 November 2020, Version of Record 10 December 2020.",https://doi.org/10.1016/j.mathsocsci.2020.11.003,Cited by (1)," of replica finite-agent economies, are equivalent for any network structure.","Consider a rational expectations equilibrium (Hellwig, 1980) economy with an extension to an environment of social networks. Each trader initially holds a private signal. Traders are connected through an exogenously given social network in which traders can observe their neighbors’ signals.====The most popular and most reasonable approach for traders to process observed signals from their neighbors is to directly infer information about the fundamentals of the risky asset from the ==== of observed signals (as well as the price). However, Ozsoylev and Walden (2011) propose a different approach in a finite-agent model==== ====: that traders use a ====, instead of the complete collection, of observed signals to make Bayesian inference. The network structure determines how information diffuses through the network; the information processing approach determines the (new) information available to traders, and traders’ information determines their demand for the risky asset. Therefore, the network structure, signal structure, and information processing approach jointly determine the market equilibrium. The two information processing approaches may lead to different market equilibriums. In this study, we address whether the two resulting economies corresponding to the two information processing approaches are ==== in the sense that they have the ==== market equilibrium (the same equilibrium price and optimal demands).====As the signal structure in our model is general, when we investigate the equivalence between the two economies, we consider a sufficient statistic for observed signals (i.e., the conditional mean of the fundamental) instead of a simple average of observed signals.==== ==== Besides normality and non-degeneracy, we impose no more requirements on the signals.==== Although averaging signals is a reasonable approach,==== ==== it is not clear why traders average observed signals from their neighbors rather than infer information from the complete collection of observed signals.==== ==== Although the average of signals is a sufficient statistic for the complete collection of signals when signals take the classical form of a sum of the fundamental and an independent noise and have the same precision (Ozsoylev and Walden, 2011), it is not the case when signal precisions differ across traders. In addition, even if the average of signals is a sufficient statistic for the complete collection of observed signals, the price and the average of signals together may no longer be a sufficient statistic for the price and the complete collection of observed signals because the price serves as an endogenous public signal determined by market-clearing conditions. In other words, traders may have an incentive to make Bayesian inference based on the complete collection of observed signals instead of the averaged signal because they believe that the complete collection offers more information and thus can provide them with a higher expected utility. Also, as claimed by Lou ⓡ al. (2019): “====” This motivates the current study, wherein we formally justify the above arguments and claim by investigating the equivalence of the two economies.====It is reasonable that traders use a simple average of signals to make Bayesian inference when the two economies are equivalent. However, it is not reasonable when they are not equivalent. Specifically, we show that when the two economies are not equivalent, in the economy using the averaging approach, there exists at least one trader who has an incentive to use the complete collection of observed signals to increase their expected utility, besides the averaged signal (see Proposition 1). This reveals that the economy using an averaging approach is not ==== in some sense and thus validates the claim in Lou ⓡ al. (2019).==== The main result of the paper is a series of characterizations of the equivalence of the two economies. As a basis for the subsequent development, we first show that the two economies are equivalent if and only if the conditional means under the two information processing approaches are almost surely identical (see Proposition 2). We then show that the two economies are equivalent for ==== signal structure when the network graph is complete (see Theorem 1). We also present a ==== condition on the solvability of a system of equations for the equivalence of the two economies (see Theorem 2). It shows that the two finite-agent economies are ==== equivalent in general. We show that, for any signal structure (respectively, non-complete graph), there exists a corresponding network graph (respectively, signal structure) such that the two economies are not equivalent (see Theorem 3, Theorem 4). In addition, for tractability, we also revisit the classical signal structure used in Hellwig (1980) and Ozsoylev and Walden (2011) that takes the form of a sum of fundamental and independent noise. We consider homogeneous preferences, and find that the two finite-agent economies are equivalent for regular graphs, but not for chain and star graphs (see Corollary 2). Our results reveal that the two finite-agent economies generally do not have the same market equilibrium unless the network structure and signal structure coordinate well.====We also consider a ==== replica economy similar to that in Han and Yang (2013) and Walden (2019). The large economy is defined as the limit of a sequence of finite-agent economies where each finite-agent economy consists of several disjoint independent subnetworks. These subnetworks have equal network size and identical network structure. When signals take the classical form, we show that the two large replica economies corresponding to the two information processing approaches are equivalent for any network structure (see Theorem 5). Our analysis justifies the assumption in Ozsoylev and Walden (2011) that traders take an average of their neighbors’ signals to make Bayesian inference in large economies.====The results are of interest for several reasons. First, they identify the inherent coordination condition between the network structure and signal structure which allows traders to make Bayesian inference based on a sufficient statistic (or a simple average) of observed signals to reduce the potential computational burden of Bayesian inference. Second, they reveal the relation and essential distinction between the two information processing approaches and add to the understanding of the impact of social networks on traders’ decision-making and market outcomes under the framework of rational expectations equilibrium (REE). Finally, the results establish that the equivalence of the two economies is determined by the conditional mean without considering the conditional variance which is involved in the expression of traders’ optimal demands. This observation may be of wider significance in other CARA-normality circumstances.==== The rest of the paper is organized as follows: Section 2 introduces the model and formulates the problem. Section 3 presents the main results on the characterizations of the equivalence of the two finite-agent economies. Section 4 considers large economies. Section 5 presents the related literature. Section 6 concludes this paper. All proofs are presented in the Appendix.",The equivalence of two rational expectations equilibrium economies with different approaches to processing neighbors’ information,https://www.sciencedirect.com/science/article/pii/S0165489620301050,30 November 2020,2020,Research Article,135.0
Kuhle Wolfgang,"Zhejiang University, School of Economics, Hangzhou, China,MEA, Max Planck Institute for Social Law and Social Policy, Munich, Germany","Received 24 August 2018, Revised 19 November 2020, Accepted 20 November 2020, Available online 27 November 2020, Version of Record 10 December 2020.",https://doi.org/10.1016/j.mathsocsci.2020.11.002,Cited by (0),"This paper studies aggregate equilibrium models where firms cannot compute future prices with perfect accuracy. Instead, firms use approximations to infer prices. In equilibrium, we find that the precision with which firms can compute prices is endogenous and depends on the level of aggregate supply. At the same time, firms’ individual supplies, and thus aggregate supply, depend on the precision with which future prices are computed. This interplay between supply and firms’ individual ability to infer prices induces multiple equilibria, with inefficiently low output, in economies that otherwise have a unique, efficient, rational expectations equilibrium. Moreover, exogenous parameter changes, which would increase output were there no computational frictions, can diminish the precision of agents’ price forecasts, and reduce output. Our model also accommodates the intuition that large interventions, such as unprecedented quantitative easing, can put agents into “uncharted territory.”","Few practitioners claim that they are able to compute future equilibrium outcomes, such as prices, with any accuracy. Despite this, textbook models implicitly assume that, given all relevant data, agents compute exact numeric values for future equilibrium prices, respectively, the entire distribution of equilibrium prices if the model involves risk. In the present paper we argue that real world agents face considerable computational constraints. Instead of being able to solve the model’s non-linear equilibrium equations, agents can only approximate future equilibrium outcomes. Put differently, we develop a model where agents, just like economic researchers, use polynomials to approximate equilibria.====In our model firms use polynomials to approximate future selling prices for their output. Once firms rely on such approximations, rather than exact solutions, there exist multiple self-fulfilling equilibria. These equilibria differ regarding the precision with which agents compute future prices. There is one group of equilibria in which economic activity falls into intervals where agents’ polynomial approximations are of high quality and the role of the computational friction is small. These equilibria are close to, and can coincide with, the model’s unique rational expectations equilibrium (REE). Computational frictions are important in the remaining equilibria. Aggregate supply in these equilibria is (i) low and (ii) falls into an interval where agents’ approximations, to the equations describing equilibrium, are of poor quality. These equilibria are interpreted as self-fulfilling “glitches” of the overall economy. During such a glitch, agents collectively cut production. This change in aggregate behavior diminishes individual agents’ ability to compute future prices. In turn, agents’ inability to compute prices justifies the initial production cut. The current model thus provides ==== explanation for macroeconomic crises, which does not rely on exogenous parameter uncertainty such as, e.g., depreciation shocks.====Regarding the model’s comparative static predictions, we find that large exogenous parameter changes can complicate agents’ efforts to compute future equilibria. The model thus accommodates the intuition that interventions, such as unprecedented quantitative easing, can put agents into “uncharted territory”. As a consequence we find that large policy interventions tend to increase economic activity less than they would if agents could compute the model’s comparative statics with perfect accuracy.==== To illustrate agents’ bounded computational capacity, we may think of a farmer who must decide in spring how much corn he should plant. This farmer may know the price at which corn tends to sell in years with “normal” supply. Moreover, he might know that small increases in aggregate supply tend to reduce prices, i.e., that demand is locally downward sloping. Finally, he may know that this downward slope tapers off as supply increases. What the farmer surely does not know is all the values that the demand function takes over its entire domain. If he wants to calculate those prices, which obtain once supply differs substantially from those levels that he is familiar with, he must extrapolate demand to approximate the selling price. Likewise, in a macroeconomic interpretation, we can think of a large number of firms that have to choose production today in anticipation of future demand. These firms know the price at which their goods sell in “normal times”. However, if firms collectively cut production today, it is difficult to know exactly whether future selling prices will either increase, due to reduced supply, or fall, since the layoffs, associated with production cuts, reduce demand.==== ==== Indeed, many severe crisis episodes share in common that output ==== prices fall at the same time. The intuition that firms cannot solve for the economy’s overall equilibrium extends naturally to more complex environments where demand concerns vectors of goods, involving substitutes and complements.====Regarding parameter changes, the difficulty that agents face in our model does not originate from stochastically changing parameters. Instead, agents know the magnitude of the parameter change in advance. The difficulty is that they cannot exactly compute how these parameter changes impact future equilibrium prices. Such difficulties are prominently emphasized by practitioners. Soros (1994) presents a collection of case studies highlighting market participants’ inability to anticipate the impact that pre-announced central bank policies have on macroeconomic equilibrium variables. Similarly, Niederhoffer (1997), p. 381, concludes his discussion on excess demand functions: “The difficulty is that nobody knows what the equilibrium level is until at least the morning after the fact”. For recent examples, we refer to comments made on the quantitative easing program: Druckenmiller (2015) commented in retrospect “I didn’t know how it was going to end... I would have said inflation [which] would have been dead wrong”. Taking an ex-ante perspective, Stiglitz (2010) predicted that QE2 would likely bring interest rates down “a little”, but that it was unclear what the risks, ranging from economic growth to“a whole set of other potential risks that - may result from this policy”, were.==== Our agents work with approximate, misspecified, models. Except for special cases, they cannot form rational expectations in the sense of Hutchison (1937), Grunberg and Modigliani (1954), Muth (1961) and Blanchard (1979), which are consistent with the true model. Regarding model misspecification, our approach is thus akin to the literature on learning, Bray (1982), Sargent (1993), Evans and Honkapohja (2001), Woodford (2013) and Esponda and Pouzo (2016), where agents use misspecified algorithms to infer model parameters. These models emphasize statistical/Bayesian learning algorithms in linear environments. In our model, agents use polynomials to make inference on a deterministic, but non-linear, model environment.====Rubinstein and Wolinsky (1994) and Battigalli and Guaitoli (1997) develop Bayesian N-player games of “conjectural equilibrium”. These equilibria formalize Hayek’s 1937 intuition that “[w]e may therefore very well have a position of equilibrium only because some people have no chance of learning about facts, which, if they knew them, would induce them to alter their plans”. Hahn (1978) proves that such equilibria obtain when agents hold exogenously given, incorrect, conjectures over each others’ supply functions. In our model, we may interpret firms’ polynomial approximations as such “conjectures”. Conjectures in our economy, unlike Hahn’s exogenous conjectures, derive endogenously from firms’ approximation efforts. Moreover, firms, knowing that approximation quality varies with aggregate output, appreciate that some conjectures are more reliable than others.====Rothschild (1974), McLennan (1984) and Bonano and Zeeman (1985) study oligopoly settings where firms strategically choose different supplies to learn demand. Due to the cost of experimentation, firms may, even in the long-run, settle with inefficient supply schedules. Firms in our model are small, and thus changing individual supply does not impact prices. The information that firms learn over time is thus determined by overall equilibrium rather than individual experimentation. As a consequence, our model predicts that the economy converges to the rational expectations equilibrium in the long-run.====Our model may be interpreted as a simple ==== setting, as in Keynes (1936) or Samuelson and Nordhaus (2010), which is augmented with a computational friction. Section 3, extends this interpretation to the macroeconomic workhorse models of Diamond (1965), Diamond (1982) and Cooper (1999). In particular we find that computational frictions tend to dampen the short-run impact of fiscal and monetary policies. The current model thus provides one possible explanation of the “forward guidance puzzle” described by Del-Negro et al. (2015), who document that real world economies react less to the announcement of policy changes than realistically calibrated rational expectations models would suggest.====Evans et al. (2019) show that an economy’s unique REE need not obtain even if agents are fully rational. For the standard infinite horizon RBC model, they show that agents, who repeatedly revise their price and output expectations, may fail to coordinate expectations on the economy’s REE. Accordingly, Evans et al. (2019), p. 822, argue that the actual impact of policy changes may differ from what RE theories predict. In Section 4.1 we study whether the non-REE, which we propose here, can satisfy the Evans et al. (2019) criterion of “eductive stability”. To do so, we introduce the current computational friction into the Evans et al. (2019), p. 832, cobweb model. It turns out that the non-REE proposed here satisfy the criterion of eductive stability, even if the coexisting REE itself fails to do so. The current non-REE are, in this sense, more likely to obtain than the REE. Computational constraints are therefore important.====To interpret their findings, Evans et al. (2019), p. 822, distinguish between far-sighted and short-sighted agents. In turn, they show that far-sighted agents, who plan for a long life-cycle, react strongly to changes in aggregate expectations. These strong reactions destabilize equilibria. Our findings are in-line with this interpretation: non-REE, where computational constraints are important, are characterized by agents’ inability to forecast accurately. In this sense, agents are short-sighted, and they react less to changes in aggregate expectations than they would if they could forecast accurately. This short-sightedness thus stabilizes non-REE. Equilibria, where forecasts are inaccurate, are thus more likely expectationally stable than the REE.====Theories of inattention can also explain departures from REE. Gabaix, 2014, Gabaix, 2017, Gabaix, 2020 presents one such theory, which assumes that agents misperceive relevant variables. That is, if a variable’s true value is ====, agents perceive it as ====, where ==== is the “rate of attention” that they devote to variable ====. Agents in Gabaix (2014) solve a two step decision problem. In the first step, where they choose the “rate of attention”, they use a Taylor-series to approximate their own utility function, taking ==== as an exogenous random variable. To comment on the differences between this theory, where agents use approximations to infer their own preferences, and the current approach, where agents use approximations to compute non-linear equilibrium, in greater detail, we sketch the mechanics of the Gabaix (2014) model in Appendix A. In particular, and contrary to what we assume here, when agents choose their actions, they can solve for equilibrium variables. Moreover, Gabaix (2020) emphasizes that agents’ under-reaction to exogenous and endogenous variables, can ensure “determinate equilibria” in economies that feature multiple REE otherwise. In the present paper, on the contrary, agents’ inference problem amplifies the number of equilibria. A final difference is that our agents use polynomials, i.e., textbook techniques, to infer equilibrium variables. The assumption of an attention rate ====, means that agents’ inference is less in line with techniques that a researcher would use.==== ====Our model argues that economies may depart from RE because agents have difficulties to compute general equilibrium. Regarding agents’ computational approach, we assume that they use standard textbook techniques, i.e., Taylor-series expansions. Just like researchers in economics, physics, and engineering, agents rely on first- and second-order polynomial expansions, rather than exact solutions.==== ==== That is, agents approximate a model’s, ====, equilibrium comparative statics using a first-order polynomial expansion ====, which yields ====, and, ====, respectively. Plott and Pogorelskiy (2017) provide experimental evidence supporting the hypothesis that agents indeed use polynomial expansions to forecast prices.====The main text presents the simplest setting. Extensions and proofs are relegated to the appendix. Section 2 introduces the model. Sections 2.1 Polynomial equilibria, 2.2 Parameter changes contain the main results. Section 3 suggests different interpretations. Section 4.1 applies the criterion of eductive stability. Section 4.2 extends our two-period model to a multi-period setting. Section 4.3 introduces asymmetric information and heterogenous expectations. Section 5 concludes.",Equilibrium with computationally constrained agents,https://www.sciencedirect.com/science/article/pii/S0165489620301049,27 November 2020,2020,Research Article,136.0
Shy Oz,"Research Department, Federal Reserve Bank of Atlanta, 1000 Peachtree St. NE, Atlanta, GA 30309, USA","Received 14 April 2020, Revised 13 October 2020, Accepted 2 November 2020, Available online 17 November 2020, Version of Record 15 December 2020.",https://doi.org/10.1016/j.mathsocsci.2020.11.001,Cited by (1),"This article presents a model of two firms with fixed budgets that simultaneously hire lobbyists to obtain exclusive contracts in multiple markets. In a pure-strategy equilibrium, neither firm can increase its payoff by using its reserves to hire more lobbyists in any market. Efficiency criterion is defined in order to facilitate the ranking of lobbyist allocations with the same payoffs, where efficiency is improved with the aggregate number of lobbyists that firms keep on reserve. Efficiency comparisons are used to reduced the number of pure-strategy equilibria.","The economics literature on lobbying consists of theoretical models focusing on a wide variety of applications. Hall and Deardorff (2006) provide an extensive review of the theories of lobbying. The authors distinguish between three approaches to lobbying: theories that view lobbying as an exchange, models that focus on persuasion, and lobbying as a legislative subsidy.====The model developed in this article supplements the existing literature in that it focuses on simultaneous multimarket lobbying. This model has industrial applications where firms hire lobbyists to obtain exclusive contracts or exclusive licenses to operate in multiple markets. The same interpretation also applies to firms making irreversible investments in R&D to win patent races in multiple projects or multiple markets. The model also has a political application where firms or other entities with limited budgets try to influence a specific party’s political campaign in multiple jurisdictions. The model is simple and relies on perfect and complete information.====The lobbying game explored in this article resembles the Colonel Blotto game, first formulated in Borel (1921) and translated to English in Borel (1953). Equilibria of that game were first explored in Borel and Ville (1938). In the Colonel Blotto game, two armies have fixed number of troops to allocate among several battlefields. An army wins a battlefield if it deploys more troops than the rival army. The game has no equilibrium in pure strategies.==== ====Allocation of lobbyists across markets could also be interpreted as bidding. Therefore, in some aspects, the model developed in this article is also related to all-pay first-price auctions with multiple objects analyzed in Benoit and Krishna (2001) and Szentes and Rosenthal (2003). However, the equilibrium concept developed in this article is entirely different because it focuses on moving lobbyists (or budget to hire lobbyists) from reserves to the markets. In the context of multiple markets, Kvasov (2007) characterizes mixed-strategy equilibria for multiple simultaneous contests with limited resources.====In contrast to the above literature, this article applies the game to lobbying by introducing reserves and by modifying the strategy space so that players can move resources from reserves to the markets, but not from one market to another. The advantage of this modification is that it allows for the introduction of an efficiency criterion where allocations that support the same payoffs can be ranked according to the amount of reserves that firms maintain. Higher amounts kept on reserves serve as an indicator of less “waste” of resources spent on lobbying.====This short article is organized as follows. Section 2 describes the multimarket lobbying game. Section 3 defines pure-strategy equilibria and provides some examples. Section 4 characterizes pure-strategy equilibria. Section 5 defines and analyzes reserves-efficiency. Section 6 concludes.",Multimarket lobbying with reserves,https://www.sciencedirect.com/science/article/pii/S0165489620301037,17 November 2020,2020,Research Article,137.0
"Bossert Walter,Cato Susumu","Centre Interuniversitaire de Recherche en Économie Quantitative (CIREQ), University of Montreal, P.O. Box 6128, Station Downtown, Montreal, QC H3C 3J7, Canada,Institute of Social Science, University of Tokyo, 7-3-1, Hongo, Bunkyo-ku, Tokyo 113-0033, Japan","Received 4 March 2020, Revised 13 August 2020, Accepted 26 October 2020, Available online 11 November 2020, Version of Record 21 December 2020.",https://doi.org/10.1016/j.mathsocsci.2020.10.007,Cited by (1),"A new property of collective choice rules that we refer to as superset robustness is introduced, and we employ it in several characterization results. The axiom requires that if all individual preference orderings expand weakly (in the sense of set inclusion), then the corresponding ==== relation must also expand weakly. In other words, if a given profile is changed by adding instances of weak preference to some individual relations, then the social weak preference relation for the expanded profile must contain the social weak preference relation for the original—that is, the social relation cannot contract in response to the addition of pairs to the individual relations. We begin by examining social welfare functions (that is, collective choice rules such that the resulting social preferences are orderings) and then move on to rules that generate transitive (but not necessarily complete) social rankings. The remaining results of the paper focus on Suzumura-consistent collective choice rules. In all of these cases, it turns out that the property of superset robustness is closely related to classes of agreement-based collective choice rules. These are rules such that the social relation is determined by collecting the pairs on whose relative rankings the members of the decisive sets agree.","It is well-known that Arrow’s (1951; 1963; 2012) fundamental impossibility theorem regarding the existence of a dictator does not apply if the population under consideration is infinite. This is established by Fishburn (1970) who, according to Hansson (1976, p. 89), gives credit to Julian Blau for this observation; Blau was aware of this result as early as 1960 without publishing it. A set of individuals is decisive for a collective choice rule if their common strict preference for an alternative against another is always respected in the social ranking. Thus, a dictator corresponds to a decisive singleton, and the weak Pareto principle states that the set of all members of society is decisive. As shown by Kirman and Sondermann (1972) and Hansson (1976), the collection of decisive sets for a social welfare function (a collective choice rule that assigns a social ordering to each profile of individual orderings in its domain) forms an ultrafilter if the function possesses Arrow’s properties of unrestricted domain, weak Pareto, and independence of irrelevant alternatives. Dictatorships correspond to principal ultrafilters (that is, ultrafilters that are generated by a singleton) and, because not all ultrafilters are principal in the infinite case, social welfare functions other than those involving a dictator exist.====This paper reexamines collective choice rules with respect to the structure of their corresponding collections of decisive sets. A novel aspect of our approach is that we employ the property of superset robustness, a condition that ensures a weak set expansion of all individual orderings to result in a weak set expansion of the corresponding social relation. In other words, if a given profile is changed by adding instances of weak preference to some individual relations, then the social weak preference relation for the expanded profile must contain the social weak preference relation for the original—that is, the social relation cannot contract in response to the addition of pairs to the individual relations. A major reason why we consider superset robustness appealing is that it permits us to characterize what we refer to as agreement-based collective choice rules, which are described in more detail below. Although these rules are well-established in the context of social welfare functions, they do not seem to have been characterized and, therefore, our property of superset robustness can be seen as a crucial property that sets them apart from other (ultra)filter-based collective choice rules.====Our first main result refines the observations of Kirman and Sondermann (1972) and Hansson (1976) by adding superset robustness to Arrow’s axioms of unrestricted domain, weak Pareto, and independence of irrelevant alternatives in the context of social welfare functions. Rather than the entire class of social welfare functions whose collections of decisive sets are ultrafilters, we now obtain the agreement-based social welfare functions. According to these rules, the social ranking is obtained in two steps. First, for any set in the ultrafilter, the intersection of the orderings of its members is determined. Second, the union of these intersections over all sets in the ultrafilter is formed, and this union constitutes the social ordering associated with the underlying individual orderings. In the context of ultrafilters, this construction is frequently employed in order to obtain social welfare functions; see Kirman and Sondermann (1972) and Hansson (1976). That these agreement-based social welfare functions are characterized when our superset-robustness property is added to the list of axioms is a new result.====Next, we analyze collective choice rules that generate transitive (but not necessarily complete) social relations. In this setting, we obtain collections of decisive sets that are filters rather than ultrafilters; see Hansson (1976). Again, we add our superset-robustness property to the axioms of unrestricted domain, weak Pareto, and independence of irrelevant alternatives in order to refine the resulting class of transitive collective choice rules. In analogy to our result for social welfare functions, we obtain the class of agreement-based collective choice rules. Now the collection of decisive sets is a filter and not an ultrafilter. An agreement-based collective choice rule with a filter is a straightforward extension of the corresponding approach based on an ultrafilter but it is, to the best of our knowledge, new to the literature.====Using the characterization of agreement-based collectivechoice rules, we provide further results for transitive collective choice rules that apply to finite and infinite populations, respectively. In the finite-population case, we obtain characterizations of the Pareto rule by either adding finite anonymity or replacing weak Pareto by strong Pareto. In the infinite-population case, an important example of a filter is the Fréchet filter. It is given by the collection of all subsets of the population whose complement is finite. We characterize not only the Pareto rule but the agreement-based rule that corresponds to the Fréchet filter by using standard axioms. An important difference between the finite and infinite cases is that instead of an axiomatization of the Pareto rule, the special case that corresponds to the Fréchet filter now emerges as a second possibility and we therefore obtain a joint characterization of these two collective choice methods under finite anonymity.====Our final set of observations is concerned with Suzumura-consistent social relations. Suzumura consistency, introduced by Suzumura (1976), is a natural weakening of transitivity. Itstrengthens acyclicity by ruling out cycles with at least one instance of strict preference. In contrast to acyclicity, however, Suzumura consistency has a well-defined closure operation analogous to the transitive closure of a relation; see Bossert et al. (2005). Note that quasi-transitivity, another well-known weakening of transitivity, suffers from the same shortcoming as acyclicity in this respect: there is no unique way of expanding a relation that is not quasi-transitive to arrive at a (smallest) quasi-transitive relation. In addition, Suzumura (1976) shows that Suzumura consistency is necessary and sufficient for the existence of an ordering extension, thereby strengthening Szpilrajn’s (1930) observation that transitivity is a sufficient (but not necessary) condition for such an extension. Acyclicity is a necessary (but not sufficient) condition because it is implied by Suzumura consistency, and quasi-transitivity is neither necessary nor sufficient for this extension result.====Our first result for Suzumura-consistent collective choice rules is an alternative characterization of the agreement-based collective choice rules. It differs from our earlier axiomatization in two points. First, independence of irrelevant alternatives is strengthened to neutrality; this move permits us to arrive at the same conclusion even though social relations are now merely required to be Suzumura consistent rather than transitive. Second, we employ Sen’s (1970) property of non-negative responsiveness, which is a weakening of May’s (1952) positive responsiveness. As far as we are aware, this is the first characterization that involves a filter structure in which social relations are not required to be transitive (or complete and quasi-transitive). This observation further underlines the subtle role played by our superset-robustness property and suggests that its application may turn out to generate interesting and novel results in other contexts as well. As we do in the transitive case, we provide further results that apply to the finite and infinite case, respectively. An interesting observation is that, in conjunction with the other axioms that we employ, the collective choice rules that are characterized in this section are not only Suzumura consistent but actually transitive—and they already appear in the section that deals with transitive rules so that the observations involving Suzumura consistency provide alternative characterizations.====A notable feature of our rules is that they exclude serial dictatorships, which are characterized by Man and Takayama (2013) and Takayama and Yokotani (2017). In a serial dictatorship, if all individuals in a decisive set are indifferent, then the preferences of individuals outside of the decisive set matter. Our axiom of superset robustness restricts the use of this type of information on individual preferences and, thus, the axiom is not compatible with serial dictatorships. Therefore, superset robustness is a main driving force that leads to the collective choice rules characterized here.====The set of individuals may be finite or infinite in many of our results. Following Fishburn, 1970, Kirman and Sondermann, 1972, and Hansson (1976), collective choice rules with an infinite population have also been examined by  Lauwers and Van Liedekerke, 1995, Mihara, 1997a, Mihara, 1997b, and Cato (2017), for example. However, only few infinite-population characterizations are available so far; exceptions include the axiomatizations of the majority rule for an infinite population by Fey (2004) and Surekha and Rao (2010) in a setting with two alternatives. By applying our observations to the infinite-population case, we obtain characterizations of rules that employ the Fréchet filter. Infinite populations emerge naturally when examining intergenerational settings where trade-offs between present and future generations are of fundamental importance. As a consequence, completeness represents a rather stringent requirement and, therefore, our results may be of assistance in providing guidance under such circumstances.====After introducing the basic definitions (Section 2), our axioms (Section 3), and the notion of agreement-based collective choice rules (Section 4), we examine the traditional Arrow (1951; 1963; 2012) framework that assumes social relations to be orderings (Section 5). Section 6 drops the requirement of completeness while retaining the assumption that social relations are transitive. In Section 7, Suzumura-consistent social relations are linked to filters, and Section 8 concludes.",Superset-robust collective choice rules,https://www.sciencedirect.com/science/article/pii/S0165489620300962,11 November 2020,2020,Research Article,138.0
"Calleja Pedro,Llerena Francesc,Sudhölter Peter","Departament de Matemàtica Econòmica, Financera i Actuarial, Universitat de Barcelona-BEAT, Av. Diagonal, 690, 08034 Barcelona, Spain,Departament de Gestió d’Empreses, Universitat Rovira i Virgili-ECO-SOS, Av. de la Universitat, 1, 43204 Reus, Spain,Department of Business and Economics, University of Southern Denmark, Campusvej, 55, 5230 Odense, Denmark","Received 23 May 2020, Revised 19 September 2020, Accepted 24 October 2020, Available online 2 November 2020, Version of Record 13 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.10.006,Cited by (3),"The constrained equal welfare rule, ====, distributes the surplus according to the uniform gains method and, hence, equalizes the welfare of the agents subsequent to the allocation process, subject to making nobody worse off. We show that ====, is characterized by aggregate monotonicity and bounded pairwise fairness requiring that a player can only gain if his initial payoff does not exceed the initial payoff of any other player by the amount to be divided.","The notion of equity has a significant position in surplus-sharing problems, where a quantity of a divisible resource (e.g., money) is divided among a set of agents who regard resource or welfare egalitarianism as a social value. ==== is reached by distributing the available total resource equally among the agents, whereas ==== prioritizes to equalize the welfare of the agents after the allocation process.==== ==== Nevertheless, if the amount of resource that has to be distributed is small, it may happen that a rich agent has to transfer some of her money to poorer agents in order to reach welfare egalitarianism. The ==== rule, ====, makes the approach to welfare egalitarianism compatible with individual self-interest. Imagine a situation in which a resource has to be divided among a set of agents that are ranked with respect to (w.r.t.) a ====, representing some objective and measurable feature (sometimes called ==== or ====). First, the lowest ranked agents receive equal shares of the resource until they become equal to the second lowest ranked agents, and so forth until the resource is exhausted. Many real-life allocation methods promote constrained welfare egalitarianism. For instance, in the distribution of grants or subsidies by public institutions, families with lower incomes often receive larger scholarships and, subsequent to a natural catastrophe, it is often decided that the more individuals suffer, the more financial support they get. As another interesting application we mention that, if the CO==== emissions rights are distributed according to ====, then regions with higher past accumulated per-capita emissions receive less so that regions with lower past emissions are favored. In a discrete setting, it would be reasonable to allocate refugees to different countries according to ====, where the status quo captures the current numbers of refugees in the countries.====Distributing according to ==== can be seen as a way of obtaining end-state fairness.==== ==== Several surplus-sharing rules have been established and characterized (see, e.g., Moulin, 1987, Young, 1988, Chun, 1989, Herrero et al., 1999, Pfingsten, 1991, Pfingsten, 1998) that do not aim to diminish the inequalities of the ex-post allocations that arise when the rule has been applied. Moulin’s (2002) ====, ====, however, does, but is, formally, defined in a different setting. In the current paper, we adopt Moulin’s (1987) notion of a rule, which assigns to each surplus-sharing problem an allocation of the surplus among the agents so that ==== is the rule which results from ====.==== ====
 Moulin (2002) also studies the connections between ==== and its counterpart for deficit-sharing problems, also known as bankruptcy problems introduced by O’Neill (1962)–see the surveys of Thomson, 2003, Thomson, 2015.====In this paper, we provide two characterizations of ==== for surplus-sharing problems, which yield, when translating the employed simple and intuitive properties to Moulin’s (2002) setting, also characterizations of ==== that, as far as we know, has not yet been characterized. To this end, we introduce properties that prioritize agents with a lower status quo. ==== requires that if the relative welfare difference at the status quo between two agents exceeds the total amount to be divided, then the agent with higher welfare does not receive a positive amount. The weaker ==== and ==== still impose certain egalitarian bounds for allowing positive payoffs for some agents with a significant level of welfare. Similar protective properties for those agents with small “initial starting point” have been used in other models. Examples are ==== (Moreno-Ternero and Roemer, 2012), in a model of resource allocation where agents are capable to transform wealth into non-transferable outcomes, or ==== (Timoner and Izquierdo, 2016), in a context of rationing problems with ex-ante conditions (Hougaard et al., 2012, Hougaard et al., 2013). We first show that ==== is characterized by ==== or budget balance (requiring to exhaust the resource), ====, imposing that no agent is worse off after the application of the rule, ==== (Moulin, 1987), requiring that the assigned payoffs remain unchanged when applying the rule consecutively to an arbitrary partition of the resource, and ====. If we replace weak less first by ==== and add a weak version of ====, a classical invariance property requiring that the share of the surplus of an agent remains unchanged if some other agents take their share and leave, we obtain an additional axiomatization. These characterizations provide significant insights in the differences of the constrained equal welfare rule ==== compared to the ==== and the ====, representing resource and welfare egalitarianism, respectively.====According to Megiddo (1974) a solution on a domain of ==== games is said to be ==== if no player suffers when only the grand coalition becomes richer. If this increment in the worth of the grand coalition is distributed according to ==== applied to the allocation initially proposed by the solution, then the solution ====. We show that an efficient solution supports constrained welfare egalitarianism if and only if it is aggregate monotonic and satisfies ==== requiring that a player can only gain if his initial payoff does not exceed the initial payoff of another player by the amount to be divided. It turns out that Dutta–Ray’s (1989) ==== solution and the ==== solution (Arin et al., 2003) support constrained welfare egalitarianism on the domains of convex games and of games with large cores, respectively.====The remainder of the paper is organized as follows. Section 2 contains some general preliminaries. In Section 3 we introduce ==== and compare it with the ==== and the ====. Section 4 presents the axiomatic analysis of ====. In Section 5 we characterize the set of efficient single-valued solutions that support ====. The Appendix shows that each property in each of the characterization results is logically independent of the remaining properties.",Constrained welfare egalitarianism in surplus-sharing problems,https://www.sciencedirect.com/science/article/pii/S0165489620300950,2 November 2020,2020,Research Article,139.0
"Abraham Anand,Ramachandran Parthasarathy","Department of Management Studies, Indian Institute of Science Bengaluru, India","Received 2 July 2020, Revised 7 October 2020, Accepted 13 October 2020, Available online 24 October 2020, Version of Record 4 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.10.005,Cited by (3),"River water trade is a method for reallocating water among riparian states in an efficient and self-enforcing fashion. In this study, we investigate the functioning of river water markets when the upstream state has the ability to build dams. Using a two-agent model, we describe the implications that dam ownership can have on agent welfare as well as on social welfare. We introduce and discuss the concept of ==== in river sharing problems, which refers to the withholding of water by the upstream state exclusively for the purpose of trade. We investigate the conditions under which an agent would resort to such a behaviour. We show that having such strategic storage creates a welfare loss for the downstream agent and also results in the loss of social welfare. As an empirical illustration, we study the allocation of the Cauvery river water to agricultural users and municipal users located upstream and downstream of the Krishnaraja Sagar (KRS) dam respectively.","Transboundary river water is a contested resource since property rights are not well established (Barret, 1994, Kilgour and Dinar, 2001, Ansink and Weikard, 2009). The naturally established distribution of the water among the riparian states is according to the Absolute Territorial Sovereignty (ATS) doctrine. This allocation is inefficient and disadvantageous to the downstream states. Under such situations, river water trade can allow water to be distributed efficiently among the riparian states. River water trade is a market based solution which can address both quantity and quality decisions (Giannias and Lekakis, 1996, Abraham and Ramachandran, 2019), and can handle two-agent (Lekakis, 1998, Houba, 2008) and multi-agent situations (Wang, 2011). The allocation decision becomes more problematic in the presence of issues such as pollution (Abraham and Ramachandran, 2019), floods (Janmaat and Ruijs, 2007) etc. In this paper, we tackle one such issue, namely the construction of dams. The analytical models in the river sharing literature rarely consider dams. Most of the models in literature are annualized models and they typically ignore seasonal water storage (Ambec and Sprumont, 2002, Ambec and Ehlers, 2008). The consideration of dams is necessary when we take into account the variation in water availability throughout a year. Scarcity in summer (dry) season makes it necessary for a state to store water during monsoon (wet) season. In a transboundary setting, the possibility of dam construction and ownership brings to life some complex and interesting strategic behaviour. The core focus of this paper is “strategic storage” which is one such strategic behaviour in which the upstream agent (dam owner) deliberately withholds water and creates artificial scarcity at the downstream agent.====The literature on the river sharing problem suggests that trade is often regarded as an efficient and appealing alternative. The amenability of such solutions when the agent has an option to build a storage structure like a dam is not adequately explored in literature. Janmaat and Ruijs (2007) study a static two season model of a river shared between two riparian states. They consider a situation where storage of water is possible. In their study, Janmaat and Ruijs (2007) investigate the conditions under which a downstream agent would contribute towards construction of a dam in the upstream agent’s territory in order for flood prevention. This is an example of grey infrastructure used for flood prevention (Álvarez et al., 2019). Dams in the upstream agent’s territory can bring down the flood risk for the downstream. In the analysis carried out by Zeng et al. (2016), they model the benefits derived by different sectors (namely the domestic sector, industrial sector, and irrigation) from water storage along with benefit from hydropower generation. They analyse conflict as well as full and partial cooperative scenarios in an ====-agent interaction on a two season model. As an illustrative example, they look at the case of the Mekong river basin. Zeng et al. (2016) argue that construction of dams causes damage to fisheries and ecological resources along the river and therefore causes uncompensated costs to co-riparians. Their analysis however is limited in the sense that storage capacity of an agent is fixed and the decision variable is whether to build a dam or not. In contrast, our analysis considers a two agent model that treats the capacity also as a decision variable. Storage capacity is considered as a decision variable in Janmaat and Ruijs (2007). In contrast to their model, which maximizes expected utility by considering expected benefits and expected flood costs, we do not consider the aspect of flooding. Instead, our study focusses on the market power derived by the upstream agent due to dam ownership. The ability to withhold water could mean higher market power in water markets (Ansink and Houba, 2012). In our analysis we term this as ====, and study this behaviour using a two-agent model in a static setting.====We theoretically inspect a situation where an agent can store water anticipating a trade with the downstream agent. We demonstrate that even when losses to biodiversity are ignored, sovereignty over water usage rights and dam ownership can cause welfare loss. We expect the upstream agent to exhibit such a behaviour because the competitive market equilibrium is both efficient and individually rational at the same time. If this fact is common knowledge, both agents would resort to individually beneficial strategies in anticipation of the river water trade.====The rest of the paper is organized as follows: First we introduce the problem setting in Section 2. Next we look at the non-cooperative decision making process of upstream agent when it can store water in anticipation of market (Sections 3, 4). Then we analyse the cooperative behaviour in Section 5 and quantify the welfare implications of strategic storage. Through the theoretical analysis, we show that strategic storage can cause a welfare loss to the downstream agent and loss in social welfare as well. Finally, we consider a case study on the Cauvery basin in Karnataka state.",The welfare implications of transboundary storage and dam ownership on river water trade,https://www.sciencedirect.com/science/article/pii/S0165489620300949,24 October 2020,2020,Research Article,140.0
Rogna Marco,"Faculty of Economics and Management, Free University of Bolzano-Bozen, Piazza Universitá 1, 39100, Bolzano-Bozen, Italy","Received 5 March 2019, Revised 16 August 2020, Accepted 14 October 2020, Available online 22 October 2020, Version of Record 27 October 2020.",https://doi.org/10.1016/j.mathsocsci.2020.10.003,Cited by (0),"This paper proposes two new solution concepts for transferable utility coalitional games that are Core selections, the Central Core and the Mid-central Core, with the first being set-valued and the latter point-valued. The basic idea at the root of the Central Core is to allow such Core elements that grant to each player at least the pay-off obtained from the ====Some basic geometrical properties of the Central Core are then analysed, showing that it is a convex ==== that coincides with the Core under particular circumstances, or it is a strict subset of the Core. It is further shown that almost all fundamental axiomatic properties of the Core are preserved by these solutions.====Finally, an axiomatization of the Mid-central Core is provided through the adaptation of the mid-point domination property to a coalitional setting. The Mid-central Core is the only solution satisfying individual and group rationality together with aggregate monotonicity and a version of mid-point domination whose reference set is shaped according to the mentioned axioms.","After the seminal work of Von Neuman and Morgenstern (1947), transferable utility (TU) games in characteristic function form have played a central role in economics, being the main target of that research strand named Cooperative Game Theory. This sub-field of Game Theory is devoted to studying games where players have the possibility to form coalitions and to divide, among the coalition members, the worth that each coalition generates. Therefore, it encompasses a bargaining situation, that is further complicated by the fact that players have to decide which coalition to form. Cooperative games are solved through solution concepts, that can be seen as a tuple (or a set of tuples) (each) containing the set of coalitions that will be formed and the final pay-off, called allocation, that each player in the game will receive. Usually, however, the first element of such tuple is disregarded, or, better, taken as granted, since the coalition of all players is the implicit target. A solution concept, therefore, is just composed by an allocation vector, if point-valued, or a set of allocation vectors, if set-valued.====For TU games in characteristic function form, the list of solution concepts present in the dedicated literature is decisively vast. These can be grouped into classical and not classical solutions.==== ==== The first includes, in chronological order, the stable set (Von Neuman and Morgenstern, 1947), the Shapley value (Shapley, 1953), the Core (Gillies, 1959), the Kernel (Davis and Maschler, 1965) and the Nucleolus (Schmeidler, 1969), whereas, for the latter, we can mention the Egalitarian Solution (Thomson, 1983), the ====-value (Tijs, 1981), the Lorenz-maximal Core (Dutta and Ray, 1989), the Centroid of the Core (González-Díaz and Sánchez-Rodríguez, 2007) and the Alexia value (Tijs et al., 2011). Both lists should by no means be considered exhaustive.====Solution concepts are characterized by axiomatic properties that can be seen or as desiderata that the proponent of the same solution wants to be fulfilled, or as assumptions that seem natural to be respected (Serrano, 2004). According to the axioms satisfied by a solution concept, it can be individuated the principle that has inspired it. For example, the Core is shaped on the idea of stability, since it individuates all that allocation vectors for which no coalition can profitably deviate from full cooperation among all players. Other well known principles are egalitarianism, underpinning the Egalitarian Solution and the Lorenz-maximal Core, and marginalism, at the base of the Shapley value. A direct link between these principles and axiomatic properties can generally be individuated: stability is clearly linked to individual and group rationality, whereas marginalism to monotonicity.====The present paper proposes two new solution concepts, one set-valued and one point-valued, that are Core selections, with the former being preparatory to define the latter. Rather than being based on a single principle, the idea at their base is to try to balance all the three principles mentioned before: stability, egalitarianism and marginalism. Stability is given prevalence and, in fact, they both fully satisfy it being Core selections. However, egalitarianism and marginalism, with these principles being partially antithetical, are further imposed, giving rise to an interesting compromise that has the additional merit to be more sharp than the Core.====After their presentation (Section 2), the proposed solution concepts will be thoroughly described. Firstly, a brief analysis regarding their geometrical properties will be offered (Section 3). Secondly, it will be examined which Core-related axioms they satisfy (Section 4) and, finally, a simple axiomatization of the point-valued solution will be presented (Section 5). In particular, it will be shown that this solution is uniquely characterized by the Core fundamental properties (individual and group rationality) with the addition of aggregate monotonicity and a particular declination of mid-point domination, representing egalitarianism, that incorporates the previous axioms. Section 6 is devoted to conclusions.",The central core and the mid-central core as novel set-valued and point-valued solution concepts for transferable utility coalitional games,https://www.sciencedirect.com/science/article/pii/S0165489620300925,22 October 2020,2020,Research Article,141.0
Poindron Alexis,"Université Paris 1 Panthéon Sorbonne, 106-112, boulevard de l’Hôpital, 75013 Paris, France","Received 17 June 2020, Revised 6 October 2020, Accepted 16 October 2020, Available online 21 October 2020, Version of Record 13 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.10.004,Cited by (7),"We generalise Grabisch and Rusinowska (2013) to non-conformist societies. Agents in a network are iteratively picking a yes/no opinion, where updating stems from mutual influence. We introduce a notion of groups based on the signs of influence. We examine a few canonical societies, namely, conformist, communitarian, with leaders, with anti-conformist agents. We investigate stability issues. Any kind of opinion updating model can be hosted by our formalism, provided that: (i) alternatives are binary; (ii) opinion adoption is reversible and independent among agents; (iii) the process is Markovian and stationary; (iv) the number of agents is finite; (v) time is discrete.","We consider a society ==== of agents invited to express a yes/no opinion. The state of the world ==== is defined as the set of agents saying yes. Agents update their opinions simultaneously (or synchronously) at each period of time, as follows: (i) each agent aggregates the opinions ==== of the society on the basis of the influences that are exerted on him, which influences can be positive, negative or null; (ii) the aggregate ==== is his probability to say yes at the next period. Dynamics of opinions are described by the transition graph ====. The aim of this paper is to relate the properties of ==== with the dynamics obtained.====Prior to votes, where profiles are aggregated to produce a social preference (see, e.g., Arrow (1963) and Suzumura (1983)), agents interact with each other during debates, discussions, advertisements, etc. This process, called opinion formation, is the one studied in the present paper. It is embedded into the literature on opinion formation, diffusion and dynamics in social networks, and into the literature of Ising-like models. This literature is particularly fertile in physics, where agents are particles and where, for this reason, the framework is typically anonymous and the approach statistic. In economics, however, the framework is often non-anonymous, as well as in biology, where Boolean networks are used to model neuronal activity or interactions between genes and proteins. Grabisch and Rusinowska (2013) proposed a non-anonymous model of influence where agents are conformist, meaning that all influences are positive. However, in order to generate rich and various opinion dynamics, and therefore, to better match the dynamics of the real world, it is essential to allow for negative influence. In Grabisch et al. (2019a), a population is composed of a mixture of conformist and anti-conformist agents, in an anonymous framework though. As compared with Förster et al. (2013), an anonymous model of influence with conformist agents only, the presence of anti-conformist agents enormously enriches the landscape of dynamics. We now lack a model which would be both non-anonymous and with negative influence, not only anti-conformism but also as heterogeneous influences as desired: agents would be positively influenced by some neighbours, and negatively by some others.====The present paper fills this gap. Our contribution is mostly to clarify the mechanisms of a vast literature and propose a tractable and unifying formalism to serve as a basis for further investigations. Consistently with this intention, we evidence bridges between some canonical societies that have been studied separately in the literature, namely, conformist, with leaders, communitarian (anti-coordination) and mixed (with anti-conformist agents). In the vein of Grabisch and Rusinowska (2013), Förster et al. (2013) and Grabisch et al. (2019a), we investigate the reachable dynamics and evidence several channels of stability. Not surprisingly, coordination plays in favour of stability, but also anonymity and autonomy (independence from other agents opinions). We evidence similar dynamics for the conformist and the communitarian societies. The mixed society and societies with leaders have the same dynamics too. Besides these clarifying contributions, which pursue the concerns of the above-mentioned papers, our paper examines the utility foundations of the model and the updating scheme. In particular, we briefly discuss its asynchronous version, suggesting that the asynchronous scheme produces more intuitive dynamics.====The paper is structured as follows. Section 2 exposes the model. A summary is proposed in Section 2.9 to ease the reading. Section 3 discusses some particular restrictions on ==== that one might want to impose or not. Section 4 introduces a notion of groups based on the influence graph and examines a few canonical societies. Section 5 investigates stability issues. Related literature is relegated to Section 6. Section 7 concludes. Appendix A provides the proofs of the propositions. Appendix B relaxes the assumption of synchronous updating by investigating asynchronous and correlated updating. Appendix C discusses the utility foundations of the model, in particular, the myopic behaviour of agents. Appendix D provides algebraic considerations of an important tool of our model, along with further connections with Grabisch and Rusinowska (2013). A technical discussion concerning the connections between influences and opinion dynamics can be found in Appendix E.",A general model of binary opinions updating,https://www.sciencedirect.com/science/article/pii/S0165489620300937,21 October 2020,2020,Research Article,142.0
Chen Bo,"Southern Methodist University, United States of America,Hubei University of Economics, China","Received 7 September 2019, Revised 19 March 2020, Accepted 22 May 2020, Available online 21 August 2020, Version of Record 6 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.05.004,Cited by (0),"I formulate a model of a large private economy in which firms and managers form partnerships in an upstream labor (matching) market and then firm–manager pairs compete in a downstream goods market. I establish the existence and the general efficiency, in terms of social welfare, of stable matchings. However, stable matchings are not producer-optimal, in general, even in the cases when the feedback from the goods market does not affect agents’ preferences.","In this paper I formulate a model of a large private economy in which firms and managers form partnerships in an upstream labor (matching) market and then firm–manager pairs compete in a downstream goods market. I show that stable matchings always exist and are, in general, efficient in terms of ====. The competitive feedback from the downstream goods market on the labor market is completely internalized. This extends the classic intuition developed in Becker (1973), Gretsky et al. (1992), and Shapley and Shubik (1971) to a setting of assignment games with ensuing competition.====This study extends the literature on general equilibrium and that on assignment games with externalities. It provides a novel equilibrium characterization of a hybrid economy consisting of two types of competitive markets (one with divisible goods and one with indivisible goods). Specifically, I model the economy as a two-stage game: At the first stage, a large number of firms (literally a continuum) and a large number of managers form one-to-one partnerships in an upstream labor market; at the second stage, firm–manager pairs play a perfect competition in a goods market. The game is solved with Walrasian equilibrium.====Section 1 describes the model in detail. In the labor market, firms and managers decide on whom to work with, given a (foreseen) market clearing price in the downstream goods market. The productivity of a firm–manager pair is determined by this firm’s technology and the human capital of the manager hired by this firm. In the goods market, market demand and firm–manager pairs’ aggregate supply jointly determine the competitive market clearing price of the good. In this setting, every single agent has little impact on the economy, yet each firm–manager pair’s payoff depends on the aggregate outcome of the downstream market, which is summarized by the price of the good. A stable matching is defined as a matching that supports a Walrasian equilibrium of the grand economy.====Section 2 establishes the existence and efficiency of stable matchings. Outcomes of the labor market and the goods market affect each other. In particular, in the labor market, firms’ (managers’) preferences over managers (firms) are not fixed and crucially depend on the foreseen market clearing price of the good.==== ==== I first characterize the relationship between the market clearing price of the goods and (partially) stable matchings in the labor market by applying the results of Gretsky et al. (1992) for assignment games with no externalities, and then I establish the existence of stable matching by means of Berge’s theorem of maximum and the Kakutani–Fan–Glicksberg fixed point theorem for general topological spaces. In spite of the presence of competitive aggregate externality from the downstream, the set of stable matchings coincides with the set of efficient matchings. This result corresponds to the fundamental theorems of welfare economics in the classical general equilibrium literature.==== ==== The intuition is that there is no trading barrier in the grand economy and the aggregate externality is “internalized” by Adam Smith’s “invisible hands”.====Section 3 analyzes the welfare of the producers. Because of the aggregate externality, stable matchings are not in general producer optimal. I provide a non-crossing condition, which ensures that agents’ preferences in the labor market are independent of the goods market. However, this condition does not guarantee producer-optimality of stable matchings. The aggregate externality from the goods market on the welfare of the producers still prevails.==== This study contributes to two streams of literature. Firstly, it contributes a positive general existence result and a novel characterization of stable matchings to the literature that focuses on the existence of stable matchings in the presence of externalities. Beginning with Sasaki and Toda (1996), this literature focuses on matchings in small economies (i.e., with a finite number of agents). In a small economy, each pair of deviators has individual externalities on the other agents’ actions and it is economically reasonable to assume that agents have rational beliefs, in that they can foresee the effects their own actions on others’ actions and the feedback effects of others’ actions (Fisher and Hafalir, 2016). The externalities are multi-dimensional. It is difficult to analyze matchings while carrying rational beliefs, and the existence of stable matchings is not guaranteed under rational beliefs (Sasaki and Toda, 1996, Chen, 2019).====Later on, researchers show the existence of stable matchings by relaxing the rationality requirement on beliefs and restricting preferences and externality types.==== ==== By contrast, this paper considers a large economy (i.e., an infinite number of agents), in which the rationality requirement plays no role as each agent is a price-taker, and analyzes the effect of the feedback from the goods market to the matching market. The feedback affects the agents in the matching market through the market clearing price of good. This substantially reduces the dimension of the problem and affords an application of Kakutani–Fan–Glicksberg fixed point theorem, which was recently introduced into the literature on matchings with no externalities in large economies by Che et al. (2019).====Secondly, this study contributes an efficiency result and a welfare analysis to the literature that examines the efficiency of stable matchings in the presence of externalities. Chade and Eeckhout (2020) also consider stable matchings in large markets with various cases of ensuing externalities from downstream. In fact, my model can be regarded as a case of matching with aggregate spillover in their study. They find that inefficiency arises due to a “missing market”. Chen (2019) also studies stable matchings with ensuing competitive externalities but in a small economy and finds that inefficiency arises due to the existence of market power. This study differs from the above two studies by considering stable matchings with a specific type spillover – competitive externalities from a downstream market – in a framework of general equilibrium and finds an opposite result. Moreover, the proof of my first main result provides a methodology to establish the existence of a stable matching in Chade and Eeckhout’s (2020) general framework of matching with aggregate spillover.",Labor market matching with ensuing competitive externalities in large economies,https://www.sciencedirect.com/science/article/pii/S0165489620300767,21 August 2020,2020,Research Article,143.0
"Brams Steven J.,Gehrlein William V.,Roberts Fred S.","Department of Politics, New York University, New York, NY 10012, USA,Department of Business Administration, University of Delaware, Newark, DE 19716, USA,DIMACS, Rutgers University, 96 Frelinghuysen Rd., Piscataway, NJ 08854, USA","Available online 16 August 2021, Version of Record 3 September 2021.",https://doi.org/10.1016/j.mathsocsci.2021.08.001,Cited by (0),None,None,Peter C. Fishburn (1936–2021),https://www.sciencedirect.com/science/article/pii/S0165489621000743,16 August 2021,2021,Research Article,146.0
"Barañano Ilaski,San Martín Marta","University of the Basque Country UPV/EHU, Spain","Received 11 July 2018, Revised 2 October 2020, Accepted 2 October 2020, Available online 10 October 2020, Version of Record 21 October 2020.",https://doi.org/10.1016/j.mathsocsci.2020.10.001,Cited by (1),This paper studies equilibrium indeterminacy in an extended version of the two-sector ,"Indeterminacy in early real business cycle (RBC) models relieson external effects that induce a sufficiently strong degree of increasing returns.==== ==== Importantly, however, equilibrium indeterminacy may emerge even when increasing returns are absent. Within two-sector endogenous growth models with sector-specific externalities, indeterminacy can arise in a model of exogenous labor when a consumable capital good uses intensively the pure capital good from a private perspective, but it is intensive in itself from the social perspective (Benhabib et al., 2000). In addition to the production side mechanisms for indeterminacy, the role of preferences has also been considered. Equilibrium indeterminacy is known to arise in a model of endogenous labor supply with non-separable preferences in which production activities use only human capital (Mino, 1999).==== ==== This paper pursues this latter line of research in that we study the role that both the specification of leisure and the nature of consumption leisure trade-off may have in obtaining indeterminacy.====As is well known, in static models labor supply depends on the degree of complementarity or substitutability between consumption and leisure. As noted by de Hek (1998, p. 255): “In a dynamic context of economic growth this acquires great significance as the nature of consumption leisure trade-off determines the intertemporal accumulation paths for the economy.” He shows that in an exogenous growth model the complementarity between consumption and leisure has an impact on intertemporal decisions on capital accumulation that can result in a cyclical optimal path. As shown by Mino (1999), the non-separability of the utility function may be a source of indeterminacy in an endogenous growth model. Mino (1999) posits a two-sector endogenous growth model in which production activities use human capital alone, the utility function includes leisure, and there are sector-specific externalities. He studies a particular case of the family of non-separable utility functions proposed by King et al. (1988) that are compatible with a balanced-growth path (BGP, henceforth) and stationary hours worked. He then obtains the necessary and sufficient conditions for equilibrium indeterminacy for his particular specification of the utility function.====In endogenous growth models, the specification of leisure is known to play an important role, since it may affect the existence, uniqueness and stability of the balanced-growth equilibrium. The effects of the leisure specification on long-run growth have already been studied by Ladrón-de Guevara et al. (1997, 1999) and Ortigueira (2000), among others. They show that when pure leisure time is included in the Uzawa–Lucas model (Uzawa, 1965, Lucas, 1988), there may be multiple steady states and global convergence is lost. However, when quality leisure time is considered, there is a unique globally stable steady state equilibrium. The role the specification of leisure plays on the growth effects of different taxation policies has also been considered in the taxation literature. Milesi-Ferretti and Roubini (1998) show that the effects of taxation on growth depend markedly on the specification of leisure. In Gómez (2003) the welfare and growth effects of alternative tax structures depend crucially on the specification of leisure. Mino (2017) shows that in a two-sector endogenous growth model with aggregate externalities, indeterminacies may arise in the presence of qualified leisure.==== ==== Here, we study the extent to which the leisure specification may matter in obtaining indeterminacy when focusing on sector-specific rather than aggregate externalities.====In this paper we generalize the model proposed by Mino (1999) by considering alternative specifications for leisure. First, we consider that utility depends on pure leisure time, but unlike Mino (1999) both the intertemporal elasticity of substitution (IES, henceforth) in consumption and leisure can be greater or lower than one. When multiplicatively separable preferences are considered, the inverse of the IES in consumption and in leisure determine the degree of complementarity or substitutability between consumption and leisure. We show that indeterminacy may only arise if consumption and leisure are Edgeworth complements (in the sense that the cross derivative of utility is positive), which implies that the IES in consumption and in leisure are both greater than one, and individuals are willing to accept deviations from a uniform pattern of consumption and leisure over time. Second, we consider an alternative formulation of preferences assuming that utility depends on total effective units of leisure, i.e., the productivity of leisure depends on the level of human capital. We show that, in this case, regardless of whether consumption and leisure are Edgeworth complements or substitutes, the dynamic system is determinate. Third, we provide the economic interpretation of the indeterminacy conditions. The presence of indeterminacies in endogenous growth models provides a potential explanation that may help us understand why countries with similar endowments and fundamentals grow at different rates on their transition path to a common growth rate. The literature on indeterminacy in endogenous growth models focuses on determining the mechanisms that may give rise to a continuum of equilibria. In this paper, we offer the intuition for the conditions that are required for indeterminacy in our endogenous growth framework.====There are two assets in this economy: consumption and human capital. The dynamic behavior of the economy near the steady state can be explained through an arbitrage condition on these two assets. The intuition behind the conditions required for indeterminacy is the following. Consider an arbitrary equilibrium trajectory of human capital accumulation, and let us study whether another one with a higher human capital accumulation rate could also be justified as an equilibrium. For the second one to be an optimal trajectory, its rate of return on human capital must also be higher. When utility depends on pure leisure time, since the stock of human capital has an asymmetric effect on the time devoted to the different activities, the rate of return on human capital depends on the overall time devoted to production activities (i.e., non-leisure time). Hence, a higher rate of return on human capital can be obtained by devoting less time to leisure. If in addition to this, non-leisure time is reallocated from the final good production sector to the human capital sector, the human capital accumulation rate can be increased. Human capital externalities can encourage this reallocation effect and boost the human capital accumulation rate, thus justifying the existence of multiple equilibria. That is, in the presence of externalities, individuals may be hoping for future consumption to be high enough to justify a sacrifice in both current consumption and leisure, which requires consumption and leisure to be Edgeworth complements, in which case individuals are willing to accept deviations from a uniform pattern of consumption and leisure over time. However, when quality leisure time is considered, the stock of human capital has a symmetric effect on all the activities. In this case, the rate of return on human capital does not depend on leisure, which is constant, and therefore a higher human capital accumulation rate cannot be justified as an equilibrium, and indeterminacy does not arise. The critical requirements for the existence of indeterminacy are the specification of leisure as pure time, the presence of human capital externalities, and the complementarity between consumption and leisure.====The rest of the paper is organized as follows. Section 2 presents our general setup. The equilibrium for the pure leisure specification is characterized in Section 3. We study the stability properties of the long-run equilibrium, characterize the necessary and sufficient conditions for indeterminacy, and provide the economic intuitions behind the results. In Section 4 we study the quality time specification. Finally, Section 5 concludes.",On the indeterminacy of equilibrium in an endogenous growth model with non-separable preferences,https://www.sciencedirect.com/science/article/pii/S0165489620300901,10 October 2020,2020,Research Article,151.0
Vannucci Stefano,"Department of Economics and Statistics, University of Siena, Italy","Received 29 July 2019, Revised 3 June 2020, Accepted 27 September 2020, Available online 1 October 2020, Version of Record 7 October 2020.",https://doi.org/10.1016/j.mathsocsci.2020.09.003,Cited by (0),This paper provides a characterization of the families of uniquely topped total preorders on a finite set which are single peaked with respect to some tree-shaped spectrum.,"A single peaked domain on a given alternative space ==== can be described as a collection ==== of preferences on ==== whose ==== elements define a ==== for any population of agents endowed with preferences that are included in ====.====Such a shared compromise-structure is defined as follows: when comparing a top-alternative ==== of some preference in ==== to any other alternative ==== in ==== ==== with preferences in ==== ==== on the identification of those alternatives in ==== which lie ==== ==== and ==== as a genuine ==== between them, i.e. which are ==== strictly worse than ==== ==== ==== ====.==== ====Thus, the very notion of a single-peaked domain ultimately rests on an underlying ternary ====relation on the relevant alternative space.==== ==== Arguably, a most common and possibly prototypical example of such a ternary relation is the betweenness induced by a ==== on the alternative space as widely employed in standard location models. However, in Black (1948) – the work which first explicitly introduced single peaked preferences – using metric information to define the betweenness relation is regarded as a possible option, but it is ====. Indeed, the betweenness relation considered by Black is the one induced by an ==== so that an alternative ==== lies between alternatives ==== and ==== if and only if it is the ==== of the triplet ====, ====, ====: such a betweenness relation defines single peaked preferences on a line, or ==== (the relevant line is also referred to as their ====). But then, a most obvious generalization of ==== notion of betweenness is ==== namely on a connected graph ==== without cycles. Since any two alternatives are connected in ==== by a unique path (which can also be regarded as a line), just declare ==== to ==== ==== and ==== if and only if it lies on the unique path connecting ==== and ====, and take ==== as the relevant spectrum. Thus, by using the latter betweenness relation, one obtains ==== and the corresponding domains, henceforth also denoted ====.====As a matter of fact, tree-based single peaked preferences were first introduced in Demange (1982) (and also, if only implicitly, in Wendell and McKelvey (1981) for the special ==== ==== case which is so typical of location theory). Remarkably, it turns out that single peaked preference domains on a finite tree share several significant properties with single peaked preference domains on a line. In particular,====(i) the ==== at any profile of tree-based single peaked preferences (see Demange (1982)==== ====);====(ii) thus, in particular, at any tree-based single peaked profile ====, and some appropriate==== ==== version of the median voter theorem consequently holds;====(iii) any sincere single peaked preference profile of a TSP domain yields a a ==== outcome of the game resulting from the combination of such profile with the social choice function which is defined on the aforementioned domain and selects ==== at any preference profile (i.e. equivalently such a social choice function is ==== : see Danilov (1994) and Vannucci (2016)).==== ====Such remarkable properties shared by all TSP domains largely explain the extensive body of literature devoted to them, and to the related problems of their characterization and recognition. Indeed, in view of the properties listed above TSP domains may be regarded as sets of preference configurations that ensure a very robust type of outcome stability under majoritarian decision rules.====In that connection, the characterization and recognition of TSP domains is a significant task in several respects.====First of all, characterizations of TSP domains (possibly, several and mutually independent ones) help ==== of their collection. Specifically, by singling out some of the key properties of TSP domains, characterizations make it easier to locate them (possibly by assisting in the choice of candidate domains to check==== ====), to determine their maximum size, to identify their maximal elements, perhaps even to count them at least for small parameter sizes.====Moreover, if ==== is a TSP domain then it is consistent with the following situation: ==== is endowed with a shared ‘compromise structure’ that makes submission of ==== about private preference rankings a dominant strategy for each agent under majority voting, and the resulting outcome is also immune to the most plausible sort of ==== manipulation. It follows that whenever such a ==== is the range of a submitted ballot-profile under majority voting, then it may be plausibly regarded as ==== information about the ==== preferences of the relevant ‘voters’ (which would ==== be the case for a non-TSP domain).====In particular, recognition and characterization(s) of TSP domains can support the ==== that guarantee a suitably adapted version of coalitional strategy-proofness on the full domain of ‘topped’ total preorders, by combining ==== use of the majority rule on TSP (sub)domains and of random dictatorship (or some other coalitionally strategy-proof rule) on other domains.==== ====However, it turns out that – to the best of the author’sknowledge – characterizations of ==== ==== TSP domains are not yet available in the literature, while a polynomial recognition algorithm has been proposed only for the special case of TSP domains consisting of linear orders (i.e. ==== total preorders).====The aim of the present work is precisely to fill this gap in the literature, providing a characterization of TSP domains of ==== (i.e. of reflexive, connected, transitive binary relations with a ====), with respect to a ==== requiring that an alternative located between the top and another alternative be just ==== (as opposed to ====) than the latter.==== ====Such a problem is addressed here relying upon two building blocks: (i) an adaptation and extension to general topped total preorders of Trick’s Make Tree recognition algorithm for single-peaked linear orders on a tree, which amounts to a specific procedure to build single-peakedness-respecting tree-consistent paths starting from ==== (Trick, 1989), and (ii) an application to the specific issue of TSP domains of a recent characterization of finite tree-betweenness relations due to Chvátal et al. (2011), that in turn relies on the seminal results of Sholander (1952) concerning tree-intervals, with no reference whatsoever to single peakedness.====The present characterization of TSP domains singles out==== ==== properties of the aforementioned characterization of tree-betweenness that the betweenness relations attached to the output of the adapted version of the Make Tree algorithm do satisfy precisely when the input is a TSP domain. As a result, it shows that an ==== of the specific Make Tree procedure (originally designed to find ==== tree – if any such tree exists – that guarantees single peakedness for a domain of linear orders) can also be deployed to find out ==== ==== ==== if any – that guarantee single peakedness for an ",Single peaked domains with tree-shaped spectra,https://www.sciencedirect.com/science/article/pii/S0165489620300895,1 October 2020,2020,Research Article,152.0
Kunimoto Takashi,"School of Economics, Singapore Management University, Singapore","Received 18 September 2017, Revised 3 August 2020, Accepted 2 September 2020, Available online 19 September 2020, Version of Record 6 October 2020.",https://doi.org/10.1016/j.mathsocsci.2020.09.001,Cited by (1),"Artemov, Kunimoto, and Serrano (2013a,b, henceforth, AKS) study a mechanism design problem where arbitrary restrictions are placed on the set of first-order beliefs of agents. Calling these restrictions ====, they adopt ====-rationalizability (Battigalli and Siniscalchi, 2003) and show that ====-incentivecompatibility and ====, which implies that virtual implementation is possible uniformly over all type spaces consistent with ====-restrictions. By appropriately defining ==== in order to restrict attention to ==== environments and thereafter explicitly modelling the assumption of complete information in the language of type spaces, I re-establish the permissive implementation result of Abreu and Matsushima (1992a). However, AKS need to fix a complete information environment throughout their analysis and therefore does not enable us to ask if robust virtual implementation results are “robust” to the relaxation of the complete information environment. The main result of this paper shows that permissive robust virtual implementation results can be extended to nearby incomplete information environments. I also obtain a tight connection between the class of nearby incomplete information environments considered by this paper and that considered by Oury and Tercieux (2012).","The theory of implementation or mechanism design attempts to identify the conditions under which a social choice rule may be decentralized through some institution (or mechanism); that is, when agents, acting on their own self-interest, can arrive at the outcomes prescribed by the social choice rule.==== ==== This paper addresses the question of ====, which requires that the “set” of outcomes prescribed by a given solution concept coincide with the social choice rule. ==== means that the planner contents himself with implementing the social choice rule with arbitrarily high probability. This is an approximate version of ==== implementation, which insists on implementing the social choice rule with probability one.====The basic uncertainty a player faces in a game is the strategic choice of the other players. Then, an agent’s private information about this uncertainty should include everything relevant and can be summarized by the notion of ====. For an agent in a mechanism (or game-form), a type specifies (i) his private information about his own preferences (====), (ii) his belief about the preferences of other agents (====), (iii) his belief about how each payoff type of every other agent chooses their strategy (====), (iv) his belief over all possible second-order beliefs (====), and so on, leading to an infinite hierarchy of beliefs. A standard assumption of the classic approach to mechanism design is that the underlying type space consists of only a pair of the payoff type space and the agents’ first-order beliefs over it and, moreover, this type space is common knowledge among the planner and the agents. In making this assumption, one effectively assumes that each first-order belief corresponds to a unique infinite belief hierarchy.====This common-knowledge assumption is often seen as unrealistic. To make their analysis ==== to the specification of higher-order beliefs, Artemov et al. (AKS, 2013a, AKS, 2013b) (henceforth, AKS)use a type-free solution concept of rationalizability – ====- ==== Battigalli and Siniscalchi (2003) – that guarantees that the predictions are the same for any higher-order beliefs, as long as those predictions are consistent with their ==== restriction on the first-order beliefs. ==== is the requirement that implementation survive any specification of type spaces (i.e., higher-order beliefs) consistent with the common knowledge structure of the environment including ====-restrictions. AKS show that in quasi-transferable environments (to be defined in the next section), a social choice function (henceforth, SCF) is robustly virtually implementable if and only if it satisfies ====-==== and ====-====.====The current paper aims at exploiting the implications of AKS by restricting attention to ==== environments, which have received a lot of special attention in the literature. More specifically, I characterize a complete information environment as ====-restrictions on the set of first-order beliefs. Thus, this paper treats complete information environments in the same way as it does incomplete information environments, while the literature commonly treat these two environments in separate papers.==== ==== This paper offers two main results. My first result (Corollary 1) shows that in complete information environments where there are at least three agents, ==== SCF is robustly virtually implementable. This is the main result of Abreu and Matsushima (AM, henceforth, 1992a). As I already argue in the second paragraph, the implementation results might possibly be sensitive to the way the admissible class of infinite belief hierarchies is encoded into a particular type space. One of the contributions of this paper is to re-establish AM’s (1992a) result by explicitly modelling the assumption of complete information in the formal language of type spaces.====Since the set of complete information environments is “non-generic” relative to the set of all incomplete information environments, it is desirable to check if the implementation results are robust to a small amount of incomplete information. My second result (Corollary 2) shows that when there are at least three agents, any SCF is robustly virtually implementable under a certain class of ==== incomplete information environments. Of course, this result crucially depends on the exact relation between the strength of a robustness test adopted and the class of “nearby” environments over which the robustness test applies.====To illustrate this relation, I start from discussing another concept of robust implementation proposed by Oury and Tercieux (henceforth, OT, 2012). OT require an implementing mechanism to have “some” equilibrium which yields the outcome close to the desired one at any nearby types around the planner’s initial model. OT call this ====. The most relevant result of OT to this paper is the if-part of their Proposition 2. It says that if an SCF is virtually (fully) implementable in rationalizable strategies by a finite mechanism, it is virtually continuously implementable by a finite mechanism. In the light of Proposition 2 of OT, the main difference between this paper and OT boils down to the difference between the class of nearby incomplete information environments considered by this paper and that considered by OT. This paper proposes a notion of closeness between two ====, using the notion of ==== ==== due to Monderer and Samet (1989), whereas OT propose a notion of closeness between two ==== in terms of the product topology of weak convergence of infinite belief hierarchies in the ====, which is a type space containing the set of ==== coherent infinite belief hierarchies (satisfying a certain regularity assumption). Thus, these two notions of closeness are conceptually different. Despite this difference, I establish a formal relationship between them in Section 6.1. Using the same underlying product topology used by OT, I introduce a new topology which characterizes this paper’s class of nearby incomplete information environments. Thus, the class of nearby incomplete information environments used by this paper can be tightly connected to that used by OT.====Corollary 2 of this paper exhibits a stark contrast with the implementation literature using refinements of Nash equilibrium. It is well known that almost any SCF is ==== implementable using refinements of Nash equilibrium. Despite these permissive implementation results, Chung and Ely (2003) show that if a mechanism implements a non-Maskin monotonic SCF in undominated Nash equilibrium, there are a nearby incomplete information environment and an undominated Bayesian Nash equilibrium that is “not” close to any of undominated Nash equilibria.==== ==== ==== is known to be a necessary condition for Nash implementation (see Maskin, 1999) and it is quite demanding in some contexts.==== ====
 Muller and Satterthwaite (1977) state that any onto, ex post efficient SCF defined on the domain of all strict preferences over a finite set of alternatives is dictatorial if it satisfies Maskin monotonicity. Maskin (1999) shows that with only two agents, this result extends to social choice correspondences. Moreover, Aghion et al. (2012 henceforth, AFHKT) show that (1) if an SCF is implementable in subgame perfect equilibrium by a ====, its unique subgame perfect equilibrium cannot be approximated by “any” equilibrium in some nearby incomplete information games==== ==== and (2) if a mechanism implements a non-Maskin monotonic SCF in subgame perfect equilibrium, there are a nearby incomplete information environment and a sequential equilibrium that is “not” close to any of subgame perfect equilibria.==== ====Both Chung and Ely (2003) and AFHKT essentially show that if an SCF is Nash implementable, it is robustly implementable under almost complete information. However, their robustness requirement is much weaker than that adopted in this paper: these authors (i) fix a finite type space all the time; (ii) only perturb the common prior distribution over that fixed type space; and (iii) require that the equilibrium correspondence be nonempty for any nearby environment and have a closed graph in the limit of complete information. On the other hand, this paper adopts a type-free solution concept – ====-rationlizability – under complete information so that all the results of this paper do not depend upon how the admissible class of infinite belief hierarchies is encoded into a type space. Moreover, this paper’s results do not depend upon whether agents possess prior beliefs over the type space or those prior beliefs are common. Thus, virtual implementation can be considered an appropriate alternative if one insists on both permissive implementation results and the robustness to almost complete information simultaneously.====The rest of this paper is organized as follows: in Section 2, I introduce the preliminary notation and definitions. In Section 3, I define a complete information environment as an incomplete information environment with ==== restrictions on the set of first-order beliefs. I exploit the implications of necessary conditions for robust virtual implementation in complete information environments (Proposition 2, Proposition 3). In Section 4, I provide a characterization of robust virtual implementation in quasi-transferable environments with complete information (Theorem 3 and Corollary 1). In Section 5, I show that there is a precise sense in which all the permissive robust virtual implementation results are “robust” to the relaxation of the complete information environments (Theorem 4 and Corollary 2). In Section 6, I conclude the paper with the discussion of some related papers. In particular, Section 6.1 introduces a new topology which is defined by the very topology used by OT. Furthermore, this topology characterizes this paper’s class of nearby incomplete information environments (Proposition 4).",Robust virtual implementation with almost complete information,https://www.sciencedirect.com/science/article/pii/S0165489620300871,19 September 2020,2020,Research Article,153.0
Salamanca Andrés,"Paris School of Economics, France","Received 21 July 2019, Revised 9 September 2020, Accepted 13 September 2020, Available online 19 September 2020, Version of Record 3 October 2020.",https://doi.org/10.1016/j.mathsocsci.2020.09.002,Cited by (1),"In this paper, we study the solution concept of value in transferable utility (TU) games with asymmetric information. In our model contingent contracts are required to be incentive compatible, and thus utility might not be not fully transferable. Our approach differs from the standard methodology of TU games with complete information, which summarizes the cooperative possibilities through the characteristic function. Instead, we consider a model in which monetary transfers are modeled as additional sidepayments in a non-transferable utility (NTU) game. Our main result states that ==== generalization of the Shapley NTU value and ==== extension of the Harsanyi NTU value are interim utility equivalent in our model with sidepayments. As a consequence of this result, we obtain a generalization of the Shapley TU value to games with incomplete information. Its formula, however, cannot be described by a simple closed form expression as in the case of complete information.","Many social and economic outcomes are the result of negotiated agreements between asymmetrically informed individuals. When there is asymmetric information, coordinating a joint decision requires not only to worry about settling fairly the conflict of desires, but also it needs to compel truthful communication, as some individuals may lie in an attempt to manipulate the final agreement. Because coalitional interactions can be very complicated, cooperative game theory has devoted substantial efforts to analyzing negotiated agreements under the simplifying hypothesis of ==== (TU). This hypothesis states that players have the option to give any ==== amount of money to any other player (or even destroy money), and each monetary reapportionment increases or decreases utility by the same amount. A common solution concept in cooperative game theory is the ====, originally introduced by Shapley (1953) for the analysis of TU games. The idea behind this notion is to represent what would be a “fair” outcome of the game. The goal of this paper is to study the extent to which the Shapley value can be extended to TU games with asymmetric information. Our approach differs substantially from the standard methodology of TU games with complete information, which summarizes the cooperative possibilities of a coalition (group of players) by a single number – the “worth” of the coalition – representing the total amount of transferable utility that its members can distribute among themselves. Instead, we consider a model in which monetary transfers are modeled as additional strategic options in an NTU game derived from Harsanyi’s (1963) rational-threats representation of a Bayesian game in strategic form. The reader is referred to Myerson (1991, sec. 9.2) for a detailed explanation of this coalitional representation. The reasons for proceeding in this way are twofold. On the one hand, as already explained, a player possessing private information may not have the incentive to truthfully reveal such information. Therefore, a cooperative agreement may be subject to strategic manipulation. In order to account for this informational aspect, an appropriate Bayesian setting in a game in strategic form is required (see Forges and Serrano, 2013 for more on this issue). On the other hand, as explained by Myerson (1984b), transferring utility at the interim stage (i.e., after each player has received his private information) cannot be modeled without referring to a suitable NTU framework (see also Forges et al., 2002a).====Under complete information, NTU games are modeled by a collection of utility sets describing the feasible outcomes that the players can achieve when they cooperate in different coalitions. This ==== representation rules out ====—situations in which the utilities of the players inside a coalition depend on what the other coalitions are doing. Coalitions are then said to be ====. Value-like solution concepts for NTU games start with the assumption that individuals will agree on some feasible utility allocation that is ====, in the sense that there is no feasible reallocation of the utilities making all individuals better off. Yet, in most game situations there are multiple efficient allocations, each of them being preferred by different individuals. ==== principles are then used in order to identify an efficient allocation to be, in some sense, a fair compromise between the interests of the different individuals. The selected allocation is called an ====. Various NTU values may be defined according to different equity principles. Some of the most notable NTU values are due to Harsanyi (1963), Shapley (1969), and Maschler and Owen, 1989, Maschler and Owen, 1992.====In this paper, we consider game situations with orthogonal coalitions, where coalitional agreements are made at a time when each player may have private information.==== ==== The players private information is represented by a random privately known ====. Under such circumstances, the appropriate object for welfare analysis is the ====—a decision rule that associates a joint action as a function of the players’ types. The enforcement of any such mechanism is subject to ==== guaranteeing that the cooperative agreement does not create incentives for any player to lie about his actual type. We should think of the set of incentive-compatible mechanisms as the set of feasible agreements among which the players can choose. Incentive constraints restrict what is feasible in a way that makes it impossible to describe a cooperative game as a collection of utility sets. Indeed, the fact that a utility allocation is feasible at some type profile does not allow us to determine what utility an individual would obtain by reporting a different type.====An incentive-compatible mechanism is said to be ==== if there does not exist any other incentive-compatible mechanism that gives higher expected utility to some types of the players and lower expected utility to none. Holmström and Myerson (1983) characterized incentive-efficient mechanisms through parametric linear programming problems. The incentive constraints of these optimization problems yield “shadow prices”, which are employed by Myerson, 1984b, Myerson, 1984a to define the ==== of players. Virtual utilities generalize the weighted-utility scales of the Harsanyi–Shapley method of fictitious transfers.==== ==== By considering virtual utilities rather than real utilities (====), Myerson developed a theory for selecting fair solutions among the many incentive-efficient mechanisms. In particular, it was applied in Myerson (1984b) to extend the Shapley NTU value to an environment with incomplete information. The so-defined ==== focuses on the signaling costs associated with incentive compatibility for determining the power of coalitions. More recently, the same virtual utility approach was used by Salamanca (2020) to generalize the Harsanyi NTU value to cooperative games with incomplete information. The resulting solution concept, called the ====, is constructed upon an egalitarian criterion based on the principle of equal gains.====de Clippel (2005) and Salamanca (2020) proposed a series of eloquent examples, in which they argued that the M-value is itself insensitive to some informational externalities. De Clippel’s example starts with a bilateral trade problem with asymmetric information. Then he expands this game by adding a “broker”, whose only contribution is to partly release the buyer and seller from the incentive constraints they face when cooperating together. The broker thus facilitates the exchange by introducing a positive informational externality. One may consider this contribution as being important enough for rewarding the broker in a fair solution. According to the M-value, however, the broker receives a null payoff. The same outcome is obtained when the S-value is computed instead. The coincidence of these two solution concepts in this example is explained by the fact that both values measure the power of coalitions with respect to the signaling costs determined at the level of the grand coalition. Because the presence of the broker in the grand coalition weakens the incentive constraints, both solution concepts proceed as if there were no incentive compatibility issues for the seller–buyer coalition. To remedy this situation, Salamanca (2020) suggested to ==== take into account the incentive constraints faced by the buyer and seller. By doing so, he showed that while the M-value remains unchanged, the S-value prescribes a more appealing outcome that rewards the broker.====On the other hand, Salamanca’s example is a collective choice problem with asymmetric information involving three players who can share the cost of a public work project that could benefit them. In this example, the two uninformed individuals (players 1 and 2) can overcome the potential adverse selection problem they face by ignoring the informed individual (player 3) and agreeing on an outcome that is equitable and efficient for both of them. Therefore, there seems to be no reason for any disagreement between players 1 and 2, and thus no reason for either to bargain with player 3. Then one may argue that a reasonable outcome for this game should leave the informed player with a low expected payoff. However, when we compute the M-value, we observe that the informed player extracts a considerable amount of utility. In contrast, the S-value provides more agreement with what we intuitively expect the outcome to be.====The examples above are analog versions of some (complete information) NTU games introduced by Owen (1971) and Roth (1980), where asymmetric information now takes up the role that was formerly attributed to the lack of transferable utility. In those games, the Shapley NTU value exhibits some “difficulties” of a similar nature to that of the M-value. In contrast, Hart (1985a) showed that the Harsanyi NTU value prescribes more appealing outcomes in Roth’s example. The similarity between the NTU games of Owen and Roth is that both emphasize the restrictions that non-transferable utility might bring for coordinating joint decisions. Indeed, if it were suddenly possible to make unrestricted sidepayments (as in the TU case), then the difficulties with the Shapley NTU value in these examples would simply disappear. Moreover, all the discrepancies between the Shapley and Harsanyi NTU values would vanish, and they would coincide with the Shapley TU value. It is a well-known result that this “value equivalence” is not specific to the previous cited examples, but it remains valid for any NTU game endowed with unrestricted sidepayments.====We have reviewed the literature at length because our approach in this paper is inspired by the development of all previous ideas and results. If we recognize that, in addition to transferring utility, monetary sidepayments can also be used for signaling – that is, for weakening incentive compatibility (see d’Aspremont and Gérard-Varet, 1979, d’Aspremont and Gérard-Varet, 1982) – then it is natural to investigate the possibility for sidepayments to help circumvent the difficulties with the M-value in the examples of de Clippel (2005) and Salamanca (2020). Unfortunately, we already know that sidepayments do not solve the puzzle in de Clippel’s example. This is so because decision options in this game already accommodate (bounded) monetary compensations specifying how much money must be transferred from the buyer to the seller. However, the question remains open for Salamanca’s example. On the other hand, and more importantly, the question is raised as to whether introducing sidepayments would yield an equivalence result between the M- and S-values. A positive answer to this question would provide an indirect way to generalize the Shapley TU value to games with incomplete information.====To address the foregoing questions, we introduce sidepayments by enlarging the contractual possibilities that the players have when committing to a mechanism. Formally, in addition to type-contingent joint decisions, we authorize the players to perform type-contingent monetary sidepayments that we assume enter linearly into their utility functions. It is worth emphasizing that our model may exhibit only ==== monetary transfers. The reason is that since mechanisms are required to be incentive compatible, not all type-contingent plans of sidepayments might be feasible. Indeed, a transfer scheme will typically affect the interim utilities, which makes it impossible to transfer utility across types without affecting the incentive constraints.==== ====Our main result in this paper states that despite the restricted nature of the utility transfers, the M- and S-values are equivalent in our model with sidepayments. Remarkably, this result is not the consequence of the fact that utility transfers may help to satisfy incentive compatibility. Instead, it follows from the fact that coalitional agreements can be made equitable by means of an appropriate transfer scheme. As a direct corollary of our main result we obtain a generalization of the Shapley TU value to games with incomplete information. However, its formula cannot be described by a simple-closed form expression as in the case of complete information. The reason is that incentive constraints make it impossible to unequivocally determine the ==== scales factors involved in interpersonal comparisons of utility. When we compute the value in Salamanca’s example with sidepayments, it turns out that the difficulties observed in the M-value disappear, and the prescribed allocation exhibits properties similar to those observed in the S-value. This illustrates how the equity principles of the M- and S-values get reconciled in the presence of sidepayments.====The rest of the paper is organized as follows: In Section 2 we formally specify the model of a Bayesian cooperative game with sidepayments. Then we introduce the concepts of incentive efficiency and virtual utility. We also briefly describe the virtual utility approach and how it leads to the definitions of the M- and S-values. Section 3 is devoted to illustrating the main conceptual differences between the M- and S-values (in the absence of sidepayments) in some examples proposed by Salamanca, 2019, Salamanca, 2020. Finally, in Section 4 we establish our main result. We also illustrate our methodology and findings using the examples of Section 3.",On the values of Bayesian cooperative games with sidepayments,https://www.sciencedirect.com/science/article/pii/S0165489620300883,19 September 2020,2020,Research Article,154.0
"Badia Bruno D.,Tauman Yair,Tumendemberel Biligbaatar","Department of Economics, Rhodes College, 2000 N Parkway, Memphis, TN 38112, United States of America,Department of Economics, Stony Brook University, 100 Nicolls Road, Stony Brook, NY 11790, United States of America,Adelson School of Entrepreneurship, Interdisciplinary Center Herzliya, Israel,School of Economics, Shanghai University of Finance and Economics, 777 Guoding Road, Shanghai, 200433, China","Received 7 November 2019, Revised 14 August 2020, Accepted 15 August 2020, Available online 1 September 2020, Version of Record 7 September 2020.",https://doi.org/10.1016/j.mathsocsci.2020.08.001,Cited by (3), and lower welfare in the downstream market. We also show that this conclusion does not hold if at least one patentee is an incumbent.,"A patent awards to its owner monopoly rights over the patented innovation. As a monopolist the patentee can act as the sole seller of licenses for adoption of the innovation. Appropriate choice of the licensing mechanism then allows the patentee to obtain a return on innovative effort. This choice as well as the resulting innovation diffusion has been the focus of a large literature following the seminal work of Katz and Shapiro (1986), Kamien and Tauman (1986), and Kamien et al. (1992).====An important assumption made in these studies is the absence of rival innovators; the patented innovation in these models stands alone as an alternative to the technology employed by potential licensees. In this paper we relax that assumption and study the strategic interaction between two innovators who are patentees of competing, perfect substitute innovations that enable the manufacture of a new product. Our main goal is to understand how competition in the market for innovations affects their diffusion.====Our model comprises an infinite number of periods and agents who use a common factor to discount the future. In the first period, innovators simultaneously decide whether to license or delay licensing their innovations. If both delay, the game moves on to the next period where innovators face the same alternatives; and, as long as both innovators postpone licensing, the same structure unfolds in each of the subsequent periods. If an innovator decides to license its innovation, then after learning the decision of its rival it chooses how many licenses to sell to firms in a perfectly competitive pool. Licensees from there on compete in quantities in each of the ensuing periods and the licensor innovator collects the entire profit stream from each of these firms.====Our main result states that in the unique symmetric (stationary subgame-perfect) equilibrium of our game the probability of indefinite licensing delay is positive and, moreover, increasing in the discount factor. It follows from our analysis that competition among outside patentees may hamper innovation (and new product) diffusion and yield an outcome that is less efficient than that induced by a sole patentee, who would certainly license its innovation to at least one firm.====We show that the above result does not hold if at least one innovator is an incumbent, i.e. one that is able to produce the new product. In particular, if at least one innovator also operates as a producer, the presence of a rival innovator increases technology diffusion, and therefore welfare, in comparison to the sole innovator benchmark.====Our results obviously depend on the assumption that the innovations are perfect substitutes. What environments are suitably described by this assumption? Let us next address this question.==== ",On the diffusion of competing innovations,https://www.sciencedirect.com/science/article/pii/S0165489620300779,1 September 2020,2020,Research Article,155.0
"Watt Richard,Gunby Philip","University of Canterbury, New Zealand","Received 18 March 2020, Revised 25 August 2020, Accepted 25 August 2020, Available online 29 August 2020, Version of Record 3 October 2020.",https://doi.org/10.1016/j.mathsocsci.2020.08.003,Cited by (0),"Athletes in many sports can potentially earn significant amounts of prize money from taking less time to complete a given distance than their competitors. Time is literally money in these cases. The objective of an athlete is to choose an optimal race strategy that minimizes race time without becoming prematurely exhausted. If the first part of a race is completed too quickly, the fatigue generated makes the second part of the race slow and painful. If completed too slowly, the second part will be fast but it cannot make up for the time lost earlier on. Either way, the total time taken to complete the entire distance will not be as fast as it could otherwise have been. This optimal decision problem has long been debated by athletes, coaches, and more recently by physicists and applied mathematicians. We show that this optimal choice problem can be solved with solutions that are both simple enough for them to be of practical use but flexible enough for them to be personalized for an athlete. We use the 800 m running race to demonstrate how our technique works since it is a well known ideal example of an event that combines both the elements of speed and endurance.","The aphorism “Time is Money” refers to time as a valuable resource and the reward from doing things as quickly as possible to maximize the return from the time at hand. Nowhere is this more true than in the world of athletics. Running just a little bit faster can result in large monetary gains. An excellent illustrative example is the Diamond League series of meetings run by World Athletics which offers athletes sizeable monetary amounts for winning an event at each meeting (US$10,000) and breaking world records (US$50,000).==== ==== Those who compete in “the Finals” at the end of the League can earn from US$2000 for 8th place through to US$50,000 for 1st place. Often seconds, or fractions of seconds, separates runners from earning these significant amounts. At the 2016 Diamond League event in London, Kendra Harrison earned a world record bonus when she improved the world record in the 100 m hurdles by 0.01 of a second (setting the new record at 12.20 s), effectively earning $5 million dollars per second of improvement over the existing record. Only 0.28 s separated the 800 m winner Donovan Brazier at the 2019 Zurich Finals meeting from 2nd place Nijel Amos, effectively earning Brazier US$107,143 per second in running faster than Amos (a US$50,000 instead of US$20,000 paycheque).====There are many other sources of rewards from running in less time than others than just the Diamond League. Successful athletes can earn considerable incomes from organizations such as USA Track & Field, European Athletics, and British Athletics for setting national and world records, not to mention from promoters and better and more lucrative sponsorship deals going forward in the athletes’ careers. Other sources of rewards, for example being a star track performer at high school, are things such as college scholarships (National Collegiate Athletic Association, 2018, Hoffer et al., 2015). Over the 2017–2018 year in the United States, 33,013 men and 32,026 women received on average US$6683 and US$7263 respectively over all division levels in College track and field scholarships.==== ====But obtaining these earnings, glory and personal satisfaction requires each athlete to win their event, and for track events, this involves choosing an optimal running race strategy. This same problem also exists in other sports like swimming, track-cycling, rowing, and speed-skating.==== ==== For example, consider rowing at the 2012 London Olympics. We see from times provided by World Rowing that Bond and Murray seem to have positive split their 2000 m rowing finals, whereas Sullivan and Cohn negative split their 2000 m sculling finals.==== ==== This is obviously just anecdotal and a very superficial look at a different sport, here rowing, but it suggests that the model could be applicable to sporting events other than athletics. But what strategy is optimal? The optimal pacing problem in running is now well-known in the field of applied mathematics, with an established literature on the mathematics of the optimal speed profile of running races (see, for example, the seminal contribution of Keller, 1974, and the follow-up papers by Mathis, 1989, Woodside, 1991 and Reardon, 2013).==== ==== The problem can be summarized as follows. An athlete would like to run a given distance ==== in the least time possible. How should the athlete distribute his/her effort over this distance to achieve the objective of minimizing the total time taken? The problem is interesting for any sport and distance for which fatigue is a critical issue, where too much initial speed leads to too little ability to generate speed later on. In these situations athletes need to balance accumulation of fatigue through speed against depreciation of fatigue over time in such a way that average speed is maximized over the given distance.====The conclusion of the mathematical literature, which typically uses dynamic optimization techniques to derive an optimal pacing strategy, is that for distances above half a mile (in terms of modern athletics, distances above 800 m) the “optimal” profile involves initial acceleration up to a given maximal speed, which is reached early on in the race, and then constant pace until very near the end of the race, at which point deceleration occurs (“Primarily the theory confirms the accepted view that for distances greater than one half mile, a constant speed is best”, pg 479 Keller, 1974).==== ==== More concretely, for the 800 m event, the literature finds that it is optimal to again initially accelerate up to a given speed, but then to constantly decelerate over the remainder of the distance (Reardon, 2013).==== ==== For distances below 400 m, athletes simply accelerate up to their maximal velocity as quickly as possible, and then attempt to maintain that speed for the rest of the entire distance (although, typically, even in 100 m and 200 m races, deceleration due to fatigue still occurs in the final metres). The interesting cases, then, occur at precisely the 400 m and 800 m distances, where by far the most commonly employed strategy consists of a higher average speed for the first half of the run compared to that of the second half, as is suggested by Reardon (2013).==== ====There are, however, major drawbacks with the dynamic optimization methodology. First, the dynamic optimization technique is very complicated, and largely unusable by coaches and athletes. Coaches and athletes are unlikely to be interested in knowing the optimal profile function, which would dictate the speed to be run for every separate continuous point along the distance. Rather they only want to know the optimal times that should be achieved at given discrete points along the way, for example the half-way point, or perhaps the time for each full lap of the track in longer races. The dynamic optimization methodology is overly complex, and static optimization could be used to enhance the practicality of the exercise.====Second, it is not individualized. The existing models of optimal race strategies rely upon calibration with known world-record performances to populate the model with parameter values. It is doubtful that the same parameter values would apply to lesser athletes, and therefore the optimal race profile that is found is not likely to be optimal for more than only a few elite athletes. Furthermore, no guidance is given as to how each individual athlete should determine the appropriate values of the parameters that are involved in the models, and so calculating the individual optimal profile becomes largely impossible. Imagine that a 17 year-old high-school athlete, who has up to that point been running long-distance events, wants to know how he should best run an 800 m race. Looking to the way elite athletes have run in the past gives very little information, since those elite athletes are specialists in these shorter distance events, and very likely have different physiological characteristics. Specifically, the elite athletes are in the shorter event because they have a natural advantage in speed, whereas our high-school athlete’s advantage is more likely to be in endurance.====Third, the solution that is found in the existing literature for 800 m running, which suggests constant deceleration once an initial top speed has been reached, is unable to explain the evidence of several world class performances over the 800 m distance. In contrast to the conclusions of Reardon’s (2013) dynamic optimization method, many world-record performances involved a period of acceleration around the middle of the race with some even being achieved with overall acceleration (second lap faster than the first).==== ==== Consider, for example, the pacing employed by Sebastian Coe in his world-record 800 m run of 1:42.33 in 1979, where the ordered 200 m sections of the run were completed in, respectively, 24.6 s, 26.0 s, 24.8 s, and 27.0 s.==== ==== At the half-way point in the run, Coe accelerated notably. A similar time profile was used by David Rudisha when he set the (at the time of writing) existing world record in 2012, accelerating from the second 200 m section (run in 25.8 s) to the third (run in 25.0 s) (====, October, 2012, pg. 14). In some world record performances over the 800 m distance, the run has even been completed with a second half faster than the first. In 1972, David Wottle ran a world record equalling time of 1:44.3 s with the second half of the run 1.5 s faster than the first. Likewise, in 1965, Jim Ryan set a world record over 880 yards (804.672 m) running the second half of the distance 1.7 s faster than the first.==== ==== The currently existing (at time of writing) world record for women over 800 m is 1:53.38 s, set by Jarmila Kratochvilova in 1983, again with the second half run faster than the first (although only by 0.36 s) (see ====). Outside of world record performances, and much more recently, in an indoor event (Milrose games) in 2020, Donovan Brazier ran the second half of the 800 m 1.87 s faster than the first half to record the 5th fastest indoor time ever (====, February, 2020, pg. 13). Performances like these cast some doubt over the theory that deceleration after an initially fast start is optimal for all athletes.====What we do to resolve these issues is use a static constrained optimization method to model the optimal pacing strategy over a given distance to run. The objective being to minimize the total time taken for the run subject to constraints imposed by the athlete’s physiological characteristics to determine the optimal distribution of effort to a race distance. Our paper is the first to approach the problem in this way. Our methodology is focused on only one parameter, the value of which summarizes the fundamental physiological characteristics of the athlete in as far as the lactic component of running is concerned, and which can be estimated with relative ease for actual individual athletes. Our model also successfully explains the observed different successful pacing strategies within an optimal solution. We apply standard microeconomics techniques, involving constrained optimization, to consider the optimal distribution of pace of a race. Our primary focus is on finding the optimal split times for the first and second halves of the distance to be run. However, we also nest our base model within itself to divide each of the two separate halves into equal quarter sections. This is useful because for some events, such as the 800 m running race, while the half distance is a natural feature, in this case one 400 m lap, coaches and athletes sometimes also want to know how each optimal lap time should be achieved in terms of the two 200 m sections of each lap.====The present paper then concentrates its attention on the case of optimal pacing for the 800 m distance, but the methodology that we offer could feasibly be applied to any activity that implies physical exhaustion while undertaking a productive activity, including other running events, other sports, and even for example, how hard a farmer should work in his fields in the morning given that fatigue from the morning may condition how hard he can continue to work in the afternoon. The 800 m is particularly interesting, since it is common for athletes with a significant experience in shorter races (e.g. 400 m) and athletes with significant experience in longer races (e.g. 1500 m) to converge on the 800 m. Thus, there are often both speed specialists and endurance specialists present in many 800 m races, and all of them should be interested in their own personal optimal pacing profile. Above all, we are interested in the conclusion of the existing literature that the optimal way for an athlete to run the distance of 800 m is to employ a profile of constant deceleration after an initial top velocity has been reached. In terms of split times, the conclusion is that the optimal profile involves the first half of the race being run faster than the second half. In a similar vein to Mathis (1989), we find that this result is only true for a certain physiological-type, as determined by the value of the principal parameter in our model. It may be that for some athletes the optimal profile is even pacing, or accelerating throughout the race. While our model is not calibrated against top international athletes in order to find parameter values, we do present anecdotal evidence from the 2013 and 2017 World Championships in Athletics and the 2016 Olympic Games to support the claim that the optimal profile depends upon physiological make-up, and that deceleration is not a generically optimal pacing strategy for minimizing the time taken to run. We also provide evidence from two well-known runners, Sebastian Coe and David Rudisha, that supports the critical assumptions made on the physiological constraints.",Time is money: An economic analysis of the optimal pacing problem,https://www.sciencedirect.com/science/article/pii/S0165489620300858,29 August 2020,2020,Research Article,156.0
"Dam My,Ha-Huy Thai,Le Van Cuong,Nguyen Thi Tuyet Mai","Université Paris-Saclay, Univ Evry, EPEE, 91025, Evry-Courcouronnes, France,TIMAS, Thang Long University, Vietnam,IPAG Business school, France,Paris School of Economics, France,Thuongmai University, Hanoi, Vietnam,University of Paris 1, Panthéon-Sorbonne, France","Received 21 September 2019, Revised 19 August 2020, Accepted 19 August 2020, Available online 27 August 2020, Version of Record 22 September 2020.",https://doi.org/10.1016/j.mathsocsci.2020.08.002,Cited by (0),"This article considers a two-sector economy with externalities. In particular, the analysis involves an industrial sector whose polluting production activities have negative effects on the regeneration of a natural resource in the other sector. Without convexity or supermodularity, we prove that the economy evolves to increase the ==== (a similar notion to the ==== in Kamihigashi and Roy (2007)), and establish the conditions ensuring the convergence of the economy in the long run.","Natural resources play an important role in the economy. Intriguingly, natural resources are not always a boon to economic growth. While abundant resources may help a country overcome the fixed cost problem and avoid thepoverty trap (see Le Van et al., 2010), they might induce an economy to consume beyond its means, potentially leading to stagnation in the long run (see Rodriguez and Sachs, 1999, Eliason and Turnowsky, 2004).====The existing literature has explored the impact of natural resources in presence of externalities in a multisector economy. In particular, consider an economy with an industrial production sector and a natural resource exploitation sector (such as forestry or fishery). While the natural resource may enhance the productivity of the production sector or provide an additional source of income to the representative household, the production sector typically engages in polluting industrial activities at the detriment of the renewable resource, as has been studied by Beltratti et al. (1998), and Ayong Le Kama (2001). These authors consider the renewable resource as a consumption good as well as a production input. The regenerating capacity of the resource is impaired by pollution from the final good sector. Under suitable conditions, the existence of a stationary state and its local stability are proved.====This approach is appealing, but as Wirl, 1999, Wirl, 2004 has observed, there is always room for limit cycles. Multiple long-run outcomes exist and are separated by a threshold, even under the convexity of the model. In this paper, we propose a new approach to study a two-sector economy with a renewable resource under discrete time configuration. We specify the conditions that ensure long-run convergence of the economy. Our approach can be applied not only to the work of Beltratti et al. (1998) and Ayong Le Kama (2001), but also for other multisector models.====We consider a two-sector economy with an industrial sector that uses intermediate inputs to produce a final consumption good, and another sector, called the exploitation sector, which engages in exploiting a renewable resource. This resource can be sold directly at an exogenously determined market price. We assume there is an infinitely-lived representative consumer who allocates total incomes between consumption and capital investment to maximize intertemporal utility. She can use the income generated from the sales of the natural resource to invest in physical capital or to purchase consumption good.====This problem is challenging since we cannot follow the standard techniques laid out in the dynamic programming literature to study the long-term behavior of the economy. Usually, as well presented in Stokey & Lucas (with Prescott) (Stokey and Lucas, 1989), the Euler equations provide us with information on the optimal choice of investment and exploitation. In our economy, analyzing the Euler equations might not be appropriate since we are not sure whether the optimal choice belongs to the interior of the domain of definition. Moreover, since supermodularity is violated due to the indirect utility function having negative crossed derivatives, we cannot apply the techniques of Amir (1996).==== ====To overcome this difficulty, we introduce the concept ====, which is the difference between the discounted value of production, and the existing resource stock and capital.==== ==== This concept is similar to the ==== presented by Majumdar and Nermuth (1982), Dechert and Nishimura (1983), Mitra and Ray (1984) or Kamihigashi and Roy (2007). As we shall see, the analysis of the ==== can help illuminate our understanding of economic dynamics. Following Kamihigashi and Roy (2007), we prove that the economy evolves to increase the value of the ==== sometime in the future. This property has an important implication. It ensures that in the long run, the economy gets very close a steady state.==== ==== Furthermore, we specify conditions for the uniqueness of the steady states, and for the convergence of the economy in the long run.====The rest of the article is organized as follows. Section 2 considers the problem of the representative consumer without the negative externality of the production sector on the exploitation sector. Section 3 takes into account the negative impact of the polluting industrial sector on the regenerating capacity of the other sector. This chapter contains the main results of our paper, including the characterization of the conditions for the uniqueness of the steady states, and the long-run convergence of the economy. All proofs are given in the Appendix.",Economic dynamics with renewable resources and pollution,https://www.sciencedirect.com/science/article/pii/S0165489620300780,27 August 2020,2020,Research Article,157.0
"Meyer Patrick,Ponthiere Gregory","IMT Atlantique, Lab-STICC, Université Bretagne Loire, France,University Paris Est (ERUDITE), Paris School of Economics and Institut universitaire de France, France","Received 3 December 2019, Revised 27 April 2020, Accepted 15 July 2020, Available online 21 August 2020, Version of Record 28 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.07.004,Cited by (1)," increases lifetime entropy at age ==== if and only if the quantity of information revealed by the event of a death at age ==== exceeds lifetime entropy at age ==== divided by the probability to survive from age ==== to age ====. There exist, under general conditions, two threshold ages: first, a low threshold age below which a rise in mortality risk decreases lifetime entropy, and above which it raises lifetime entropy; second, a high threshold age above which a rise in mortality risk reduces lifetime entropy. Using French life tables, we show that the gap between those two threshold ages has been increasing over the last two centuries.","Risk about the duration of life is a major dimension of human condition. Whereas all individuals know that they will die one day, no one knows precisely ==== one’s death will take place. This risk is faced by everyone, but is somewhat diffuse and abstract, and thus hard to quantify in an intuitive way.==== ====In a recent work, Meyer and Ponthiere (2020) proposed to quantify risk about the duration of life by means of Shannon’s entropy index defined to the base 2 (see Shannon, 1948). Lifetime entropy is defined as follows==== ====: ====where ==== denotes lifetime entropy at age ==== and ==== is the probability of a life of length ==== for an individual of age ====.====Lifetime entropy ==== is the mathematical expectation of the quantity of information revealed by the event of a life of a particular length ====, or, alternatively, of the quantity of information revealed by the event of a death at an age ====. As such, ==== can be regarded as the informational equivalent of the standard life expectancy. Instead of measuring the mathematical expectation of the duration of life, ==== measures the mathematical expectation of the quantity of information revealed by a particular duration of life.====A specificity of Shannon’s lifetime entropy with respect to other measures of risk about the duration of life, such as Kannisto’s coefficient ====
 (Kannisto, 2000), the standard deviation of the age at death (Lan Karen Cheung and Robine, 2007, Edwards and Tuljapurkar, 2005) and Gini indexes of the length of life (Smits and Monden, 2009), is to measure risk about the duration of life in terms of bits, i.e. the quantity of information revealed by tossing a fair coin. As such, that indicator makes the – quite abstract – risk about the duration of life commensurable with the risk involved in tossing a given number of fair coins, a life experience with which individuals are familiar.==== ==== Thus, thanks to its reliance on the bit metric, Shannon’s lifetime entropy index allows to make humans more familiar with the risk about the duration of life, by making that risk comparable with the risk involved in tossing fair coins.====An interesting application of Shannon’s lifetime entropy index consists of computing that ==== at various ages of life ====. The resulting series ==== shows how the risk about the duration of life is progressively resolved when individuals become older and older.==== ==== In young adulthood, numerous scenarios can occur concerning the duration of life, and lifetime entropy is high. On the contrary, once old ages are reached, the number of possible scenarios for the duration of life becomes smaller, and lifetime entropy is reduced. Thus the computation of Shannon’s lifetime entropy indexes at successive ages provides also a picture of how the quantity of risk about the duration of life varies with the age, and is progressively resolved when individuals become older.====Whereas the effect of a change of an age-specific probability of death on standard life expectancy is unambiguous, the same is not true as far as its effect on its informational equivalent – i.e. human lifetime entropy – is concerned. Hence an interesting question is to know how sensitive lifetime entropy is to a change in age-specific probability of death. Does a rise in mortality risk at a given age tend to increase, or, on the contrary, to decrease risk about the duration of life measured by lifetime entropy?====Studying the effect of age-specific mortality on lifetime entropy matters for several reasons. First, when there is a change in an age-specific probability of death, this change affects the entire probability distribution of life durations in a non-trivial way: whereas some durations of life become more likely than before the change, other scenarios, associated to other durations of life, become less likely. Hence the net effect on lifetime entropy is hard to quantify. Second, individual decisions in terms of insurance or precautionary savings depend not only on the expected duration of life, but, also, on the extent of risk about the duration of life.==== ==== Hence, it is important, from a predictive perspective, to understand how mortality shocks affect not only life expectancy, but, also, lifetime entropy. Third, in real-world economies, mortality shocks due to, for instance, epidemics, heat waves or pollution peaks, can affect various ages of life in distinct ways, and there is ==== no reason why the effect of a mortality shock on lifetime entropy would be of the same sign or magnitude across the different ages of life. In order to better understand the impact of those mortality shocks on lifetime entropy, it is thus important to examine how the relation between a mortality shock and lifetime entropy varies with age.====The goal of this paper is to examine the impact of a change in age-specific mortality risk on risk about the duration of life, as measured by Shannon’s lifetime entropy index. For that purpose, we first propose to decompose that effect into its various components, to identify the necessary and sufficient condition under which a rise in mortality risk at a given age contributes to increase lifetime entropy. Then, we use that condition to study how the sign of the effect of a change in mortality on lifetime entropy varies with age.====Anticipating our results, we first show that a rise in the probability of death at age ==== increases lifetime entropy at age ==== if and only if the quantity of information revealed by the event of a death at age ==== exceeds lifetime entropy at age ==== divided by the probability to survive from age ==== to age ====. Then, we show that there exist, under general conditions, two threshold ages: first, a low threshold age below which a rise in mortality risk decreases lifetime entropy, and above which it raises lifetime entropy; second, a high threshold age above which a rise in mortality risk reduces lifetime entropy. Finally, using French life tables, we identify those two threshold ages, and we show that the gap between these has been increasing over the last two centuries.====Our work is related to Zhang and Vaupel (2009), which examines the effect of averting deaths on life disparity measured by life expectancy lost due to death. Using a framework with age as a continuous variable, Zhang and Vaupel (2009) showed that there exist, under general conditions on Keyfitz’s entropy of the life table (Keyfitz, 1977), a unique threshold age below which averting deaths reduces life disparity, and above which averting deaths increases life disparity. The existence and characterization of that unique threshold age is also studied recently by Aburto et al. (2019). In comparison to Zhang and Vaupel (2009) and Aburto et al. (2019), our approach differs on three main grounds. First, we focus on the effect of a change in mortality on risk about the duration of life not measured by means of lifetime disparity as measured by life expectancy lost due to death, but by means of Shannon’s lifetime entropy index defined to the base 2. Second, at the technical level, our framework is in discrete time rather than in continuous time, which makes the identification of threshold ages more difficult to prove analytically. Third, at the level of results, whereas Zhang and Vaupel (2009) and Aburto et al. (2019) identify a ==== threshold age using conditions on life table entropy, we identify, on the contrary, not one, but ==== threshold ages, by making assumptions on the pattern of age-specific mortality. The high threshold age (below which a rise in mortality raises lifetime entropy, and beyond which it reduces lifetime entropy) is quite similar to the threshold age studied by Zhang and Vaupel (2009) and Aburto et al. (2019). In addition, we show that there exists also another, lower threshold age, below which a rise in mortality reduces lifetime entropy. We show that this low threshold age, which was equal to 6 years in the early 19th century, has turned out to vanish to age 0 in the second half of the 20th century, leaving us with a unique threshold age, below which a rise in mortality raises lifetime entropy, and above which a rise in mortality reduces lifetime entropy.====The rest of the paper is organized as follows. Section 2 studies the relation between Shannon’s lifetime entropy and the probability of death at a particular age. Then, Section 3 examines the existence of threshold ages at which the sign of that relation changes. Our findings are illustrated by means of the case of France (1816–2016) in Section 4. Concluding remarks are left to Section 5.",Threshold ages for the relation between lifetime entropy and mortality risk,https://www.sciencedirect.com/science/article/pii/S0165489620300640,21 August 2020,2020,Research Article,158.0
Mao Liang,"College of Economics, Shenzhen University, Shenzhen, Guangdong 518060, China","Received 4 August 2019, Revised 25 June 2020, Accepted 27 June 2020, Available online 10 August 2020, Version of Record 14 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.06.005,Cited by (2),"This paper extends the traditional two-player noncooperative ==== by adding a recommendation stage before the regular bargaining stage. At the recommendation stage, a coordinator recommends a feasible payoff pair to the players. If both players accept the recommendation, the game ends and the recommendation is enforced. Otherwise, the game enters the bargaining stage, where each player is picked as the proposer of any bargaining round according to an exogenous ==== characterizing their relative bargaining power, and both players receive their respective reservation payoffs with a certain risk of breakdown if they fail to reach an agreement in this round. We characterize the unique subgame perfect equilibrium outcome of this extended game. The equilibrium recommendation, which will be accepted by both players, is optimal for the coordinator among all acceptable recommendations. A player’s payoff in optimal recommendation increases with her bargaining power. As the risk of breakdown vanishes, the optimal recommendation converges to an asymmetric Nash bargaining solution parameterized by the ==== at the bargaining stage.","The bargaining game is an important model that is often used to study the distribution of cooperative surplus among players. One problem of the traditional noncooperative bargaining model is that the bargaining procedure typically involves infinite rounds. Although the subgame perfect equilibrium (SPE) usually specifies a unique no-delay outcome for two-player bargaining under complete information, situations can be more complicated with more than two players or under incomplete information, and may result in multiple equilibria or inefficient equilibrium outcomes.==== ==== One potential solution to these problems is to introduce a recommendation stage before the usual bargaining process takes place. This recommendation stage provides an option for the players to focus on straightforward and efficient bargaining outcomes, and avoid expatiatory and costly bargaining processes.====As an attempt in this direction, this paper focuses on a two-player bargaining game under complete information. We leave more complex model setups of multi-player and incomplete information for future studies.==== ==== Specifically, we will analyze a model of two-player (player ==== and player ====) noncooperative bargaining with a third party called a coordinator. Before the players bargain to reach an agreement on a set ==== of feasible payoff outcomes, the coordinator makes a recommendation from ====. This recommendation will be enforced if both players accept it. If either player rejects it, the players have to follow the usual bargaining procedure to determine the outcome. As in the literature,==== ==== players’ relative bargaining power can be characterized by a probability ==== (or ====) that ==== (or ====) will be picked as the proposer in each bargaining round. There is also a risk of breakdown ==== when both players receive their reservation payoffs respectively if they disagree.====We are interested in the outcome the coordinator will recommend, and the properties of this recommendation. It is proven that the coordinator can find a nonempty set of acceptable recommendations ====. In this set, the coordinator will choose an optimal recommendation ==== according to her preference over ====, and ==== will be accepted by both players. Each player’s payoff in ==== increases with her bargaining power. When the risk of breakdown ==== goes to zero, ==== converges to ====, an asymmetric Nash bargaining solution (ANBS) of the bargaining game parameterized by the relative bargaining power.====There are two fundamental approaches in the bargaining literature. The cooperative approach usually specifies a normatively attractive solution, e.g., Nash (1950)’s solution, or Kalai and Smorodinsky (1975)’s solution. One explanation is that a benevolent social planner chooses the solution. By contrast, the noncooperative approach discusses the equilibrium outcome under certain bargaining procedures. For example, see Rubinstein (1982), Muthoo (1999), and Mao (2017). A research agenda named the Nash program (Nash, 1953) is devoted to building connections between the two approaches; that is, to see how a normatively desirable outcome can be implemented as an equilibrium under a proper bargaining procedure. Among others, Laruelle and Valenciano (2008), Miyakawa (2008), Britz et al. (2010), Anbarci and Sun (2013), and Kawamori (2014) provide some recent noncooperative foundations for ANBS and other solutions. This paper also suggests a noncooperative implementation of an ANBS as the unique equilibrium under some conditions (including ====). Unlike other works, this solution is now realized at the recommendation stage, without the players getting involved in any bargaining process. It also implies that although the coordinator’s preference generally has a significant impact on the equilibrium outcome, when ==== is small we need not assume the coordinator to be a benevolent social planner to implement a desirable bargaining solution.====There is also some literature about recommendation in bargaining. For example, Schelling (1957) and Dickinson and Hunnicutt, 2005, Dickinson and Hunnicutt, 2010 focus on nonbinding recommendations and emphasize their focal point effect on the bargaining outcomes. By contrast, we analyze a binding recommendation that will be enforced once players accept it.====Section 2 describes the setup of the game. Section 3 derives the optimal recommendation as the unique SPE outcome. This optimal recommendation is analyzed in Section 4. Section 5 concludes this study.",Optimal recommendation in two-player bargaining games,https://www.sciencedirect.com/science/article/pii/S0165489620300755,10 August 2020,2020,Research Article,159.0
"Gaertner Wulf,Xu Yongsheng","Department of Economics, University of Osnabrück, D-49069 Osnabrück, Germany,CPNSS, London School of Economics, London, WC2A 2AE, UK,Department of Economics, Georgia State University, Andrew Young School of Policy Studies, P.O. Box 3992, Atlanta, GA 30302-3992, USA","Received 28 March 2020, Revised 21 July 2020, Accepted 22 July 2020, Available online 1 August 2020, Version of Record 8 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.07.006,Cited by (2),The class of rules that we propose and characterize can be viewed as a variant of the standard model in the literature on cost and surplus sharing. It basically has two reference points: an equal share of the loss and a weighted difference between an agent’s endowment or claim and the average endowment of the individuals concerned. Our class of rules comprises some prominent sharing rules such as equal split and the proportionality principle.,"The object of our study are rules that propose solutions in situations in which losses have to be shared. We consider cases where several agents who possess some initial endowment face a situation where they are asked, perhaps forced, to give something away so that they will no longer be able to keep their status quo allocation, and we are looking for rules which may be acceptable to the persons concerned to resolve such situations. Cases such as these have been considered from different angles in economics. Closest to what we have in mind, though still substantially different, are situations of bankruptcy and bequest. In the former, various agents have claims of differing size against a bank or firm and the liquidation value that remains is not sufficient to satisfy all entitlements. In the latter, a father, let us say, made promises to his children but after his death, the heirs find out that the estate he left behind is not large enough to honor all promises. Another case, and an important one, is taxation which in the words of Young (1988, p. 322) perhaps is “the most familiar example of a distribution problem involving loss”.====Various theoretical concepts exist in the literature which prescribe how a deficit or loss should be shared. Most prominent are the proportional solution and the egalitarian rule (Moulin, 1987, Moulin, 2002) but also the constrained equal-awards solution and the constrained equal-losses rule (Herrero and Villar, 2001), not to forget the Talmud rule (Aumann and Maschler, 1985), the latter being a compromise between the constrained equal-awards and the constrained equal-losses rule. Young (1988) provided a characterization of equal sacrifice methods in taxation. One should also mention contributions by Shapley (1953), O’Neil (1982), Pfingsten (1991), and the comprehensive investigations by Thomson, 2003, Thomson, 2013, Thomson, 2015, Thomson, 2019 on the adjudication of conflicting claims and the division problem in the light of resource allocation. The latter aspect has also recently been addressed by Ju and Moreno-Ternero (2018). Ju et al. (2007) characterize non-manipulable division rules which may treat an agent differently from other persons based on this agent’s characteristic vector. Finally, Hougaard et al. (2012) generalize the analysis of Thomson and Yeh (2008) who developed the concept of operators on the space of rules. Hougaard et al. introduce the notion of baselines which represent some reference point in the division problem of adjudicating conflicting claims.====The class of rules that we propose and characterize in this paper can be viewed as another variant of the standard model in the literature on cost and surplus sharing. It basically has two reference points: an equal share of the loss and a weighted difference between an agent’s endowment or claim and the average endowment of the people concerned. The following section presents the model and provides its axiomatic characterization. A few final remarks are offered in Section 3.",Loss sharing: Characterizing a new class of rules,https://www.sciencedirect.com/science/article/pii/S0165489620300664,1 August 2020,2020,Research Article,160.0
"Dubey Ram Sewak,Laguzzi Giorgio,Ruscitti Francesco","Department of Economics, Feliciano School of Business, Montclair State University, Montclair, NJ 07043, United States of America,University of Freiburg, Department of Mathematics, Ernst-Zermelo Strasse 1, 79104 Freiburg im Breisgau, Germany,Department of Economics and Social Sciences, John Cabot University, Via della Lungara 233, 00165 Rome, Italy","Received 17 January 2020, Revised 14 July 2020, Accepted 15 July 2020, Available online 25 July 2020, Version of Record 1 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.07.003,Cited by (6),"This paper examines the representation and explicit description of social welfare orders on infinite utility streams. It is assumed that the social welfare orders under investigation satisfy upper asymptotic Pareto and anonymity axioms. We prove that there exists no real-valued representation of such social welfare orders. In addition, we establish that the existence of a social welfare order satisfying the anonymity and upper asymptotic Pareto axioms implies the existence of a non-Ramsey set, which is a non-constructive object. Thus, we conclude that the social welfare orders under study do not admit explicit description.","This paper deals with efficiency and intergenerational equity in the setting of policies that affect present and future generations. Relevant questions are: how should a social planner weigh the welfare of the present generation against the well-being of future generations? Is there a conflict (and in what sense) between intergenerational equity and efficiency in the evaluation of infinite utility streams? This subject has received wide attention in the economics, philosophy and political science literature in recent years. In this paper we investigate preference relations, on the space of infinite utility streams, that are complete, transitive, invariant to finite permutations, and respect some version of the Pareto ordering: equitable preferences, for short. We stick to the standard framework which concerns the problem of defining a social welfare order on the set ==== of infinite utility streams, where ==== is of the form ====, ==== denotes a non-empty subset of real numbers, and ==== is the set of natural numbers. There is a vast body of literature on the subject matter. In what follows we will briefly overview it in order to highlight and put our own contribution in context.====In a pioneering paper, Ramsey (1928) observed that discounting one generation’s utility relative to another’s is “ethically indefensible”, and something that “arises merely from the weakness of the imagination”. Following in Ramsey’s footsteps, Diamond (1965) introduced the concept of ==== (as an axiom imposed on preferences over infinite utility streams) to formalize the principle of equitable preferences (“equal treatment” of present and future generations). This axiom requires that two infinite utility streams be indifferent if one is obtained from the other by interchanging the utility level of any two generations. There is also broad consensus among scholars on another desirable attribute that preferences should possess, namely the Pareto criteria. In its strongest form, the Pareto principle asserts that one utility stream must be deemed strictly better than another if at least one generation is better off and no generation is worse off. Therefore, a question that naturally arises is whether one can aggregate infinite utility streams with a social welfare function,==== ==== and consistently evaluate them while respecting anonymity and some form of the Pareto axiom. This question was first approached formally by Diamond (1965) who showed that, if the possible range of utilities in each period is the closed interval ====, a social welfare order that displays anonymity and the strong Pareto ordering cannot be continuous in the topology induced by the supremum norm. Hence, there does not exist any continuous (in the topology induced by the sup norm) social welfare function satisfying the anonymity and strong Pareto axioms. Basu and Mitra (2003) refined Diamond’s result by showing that the non-representability result still holds when continuity is dispensed with, and even for subsets ==== of the real numbers containing only two elements. The bottom line is that when ==== contains at least two elements, there exists no representable social welfare order satisfying the anonymity and strong Pareto axioms. Hence, one cannot exploit canonical constrained-maximization techniques to figure out an optimal policy. One potential way out consists in weakening the strong Pareto condition while still demanding that the social welfare order be representable. However, Crespo et al. (2009) established that if ==== contains at least two elements, there is no social welfare function satisfying anonymity and the infinite Pareto axiom.==== ==== On the other hand, Basu and Mitra (2007) provided an example of a social welfare function that satisfies anonymity and the weak Pareto principle.==== ==== Petri (2019) examined a version of the Pareto axiom, namely lower asymptotic Pareto, which is weaker than infinite Pareto and stronger than weak Pareto. In a nutshell, given any pair ==== and ==== of infinite utility streams, if ==== dominates ==== and the lower asymptotic density of the subset of natural numbers such that ==== is positive, the lower asymptotic Pareto ordering requires ==== to be preferred to ====. Petri exhibited an explicit formula for a social welfare function satisfying lower asymptotic Pareto and anonymity under the assumption that Y is finite. This result leaves us wondering whether there exists a social welfare function if one considers a Pareto ordering that is weaker than infinite Pareto but stronger than lower asymptotic Pareto. We address this issue by focusing on a version of the Pareto ordering that we term upper asymptotic Pareto as it hinges on the upper asymptotic density of the subset of natural numbers over which a welfare improvement occurs. It is easy to see that upper asymptotic Pareto is weaker than infinite Pareto. Moreover, as will become clear later, upper asymptotic Pareto is stronger than lower asymptotic Pareto. Therefore, a question arises: is the existence of a social welfare function still guaranteed if ==== is any non-trivial domain and one postulates upper asymptotic Pareto together with anonymity? Proposition 1 provides a negative answer to the preceding question.==== ====Petri (2019) found a social welfare function satisfying lower asymptotic Pareto and anonymity on a domain ==== containing finitely many elements, but we know from Proposition 1 that there is no numerical representation of a social welfare order satisfying weak upper asymptotic Pareto and anonymity. Although a real-valued representation of an underlying social welfare order can be very useful, yet pairwise ranking of utility streams would suffice for the purpose of policy-making as long as the binary relation at hand exists and can be operationalized. Therefore, we wish to know if it is possible to describe explicitly (for the purpose of economic policy) a social welfare order satisfying weak upper asymptotic Pareto and anonymity. To provide some background on this line of enquiry, before we preview our own result, recall that Svensson (1980) established the existence of a social welfare order that satisfies the anonymity and strong Pareto axioms, assuming the set ==== of possible range of utilities to be the closed interval ====. However, his possibility result relies on Szpilrajn’s Lemma whose proof depends on the axiom of choice. Consequently, this social welfare order cannot be used by policy makers for social decision-making. In the wake of Svensson’s result, Fleurbaey and Michel (2003) conjectured that “there exists no explicit description (that is, avoiding the axiom of choice or similar contrivances) of an ordering which satisfies the Anonymity and Weak Pareto axioms”. As shown by Lauwers (2010) and Zame (2007), it turns out that the axiom of choice is unavoidable for the existence of a social welfare order satisfying the anonymity and Pareto axioms. The proof of their result relies on the existence of non-Ramsey sets and non-measurable sets, respectively. Similar to the above findings, in Proposition 2 of the present paper we show that the existence of a social welfare order satisfying anonymity and weak upper asymptotic Pareto (we know such order does exist, in view of Svensson (1980)), on a domain ==== containing at least two elements, entails the existence of a non-Ramsey set.====In order to highlight the scope of our results within the existing literature, it is worth considering the Table 1. It summarizes some results on the representation and constructive nature of anonymous social welfare orders satisfying various forms of the Pareto axiom.==== ==== We defer further discussion on the question mark appearing in Table 1 to the concluding remarks.====The remainder of the paper is organized as follows. In Section 2 we introduce the basic notation which will be used throughout the paper and gather all the definitions. In Section 3 we state and prove our main results (Proposition 1, Proposition 2). Section 4 concludes.",On the representation and construction of equitable social welfare orders,https://www.sciencedirect.com/science/article/pii/S0165489620300639,25 July 2020,2020,Research Article,161.0
Terzopoulou Zoi,"Institute for Logic, Language and Computation, University of Amsterdam, P.O. Box 94242, 1090 GE, Amsterdam, Netherlands","Received 22 November 2019, Revised 14 July 2020, Accepted 15 July 2020, Available online 23 July 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.07.005,Cited by (0),"Suppose that a group of individuals are asked to aggregate their judgments on different—possibly logically interconnected—propositions in order to reach a collective decision. Quota rules are natural aggregation rules requiring that a proposition be collectively accepted if and only if the number of individuals that agree with it exceeds a given threshold. In cases where the individuals may also abstain on some of the issues at stake and report incomplete judgments, there are several ways for determining the relevant threshold, depending on the number of abstentions or the margin between those that agree and those that disagree with a given proposition. In this paper I systematically design quota rules for incomplete inputs, within the framework of judgment aggregation, and explore their formal properties. In particular, I characterise axiomatically three distinct classes of quota rules, extending known results of the literature that so far only applied to complete inputs.","Collective decision making takes place at smaller or larger scale, supporting the democratic foundations of our society. Various issues, often logically interconnected, are at stake in different decision contexts, ranging from important political decisions to light compromises between relatives and friends. Throughout such contexts, some of the individuals whose opinions are to be aggregated into a collective decision might—being given the chance—choose to abstain from the procedure and not report a clear-cut personal judgment. For example, members of parliaments may abstain when they do not feel adequately informed about an issue at hand or when there exists some conflict of interest. More generally, individuals may not care about all the issues with which they are presented, or they may find the process of coming up with a concrete opinion costly, for any personal reason.====Abstentions are an essential part of collective decision making and are widely studied by practical and theoretical political scientists in election contexts (e.g., Pattie and Johnston, 2001, Perea, 2002, Plane and Gershtenson, 2004, Adams et al., 2006, Laruelle and Valenciano, 2011). Still, there is room for further analysis within formal models pertaining particularly to social choice theory. In this paper I delve deeper into judgment aggregation, a formal framework for collective decision making about binary (====) issues linked to each other through logic (List, 2012, Grossi and Pigozzi, 2014, Endriss, 2016). Problems of judgment aggregation are central in various disciplines, like philosophy, law, economics, and artificial intelligence among others. In order to better illustrate the need for directing attention to abstentions (to which I also refer as ====) in judgment aggregation, let us consider an example.====Binary decisions on logically interconnected issues feature in several application domains that differ from the one of Example 1 and where abstentions are frequent, such as political referendums, juridical cases, and companies’ policy making. In general, a threshold associated with a specific proposition (e.g., establishing a new contract) is the minimum number of individuals that need to agree with that proposition in order for it to be collectively accepted. The starting point of the analysis that follows is that individuals who abstain on a given issue do not hold any positive or negative opinion about that issue—the two options (==== and ====) are incomparable to them. For instance, we assume that the individuals who did not express a judgment about farm A in Example 1 merely did not have an opinion on the issue.",Quota rules for incomplete judgments,https://www.sciencedirect.com/science/article/pii/S0165489620300652,23 July 2020,2020,Research Article,162.0
"Escudé Matteo,Sinander Ludvig","Department of Economics, European University Institute, Italy and Department of Economics and Finance, LUISS, Italy,Department of Economics, Northwestern University, United States of America","Received 6 February 2020, Revised 10 July 2020, Accepted 11 July 2020, Available online 17 July 2020, Version of Record 25 July 2020.",https://doi.org/10.1016/j.mathsocsci.2020.07.002,Cited by (1),"A strictly strategy-proof mechanism is one that asks agents to use ==== monotonicity plus the envelope formula, echoing a well-known characterisation of (weak) strategy-proofness. A consequence is that strategy-proofness can be made strict by an arbitrarily small modification, so that strictness is ‘essentially for free’.","Two popular notions of robustness in mechanism design are strategy-proofness and full implementation. The former requires that agents use weakly dominant strategies; the latter that ==== equilibrium leads to the desired outcome.====In applications, it is common to have the one without the other. Consider the canonical auction setting with valuations identically and independently distributed on compact supports. The first-price auction has a unique Bayes–Nash equilibrium implementing the efficient allocation,==== ==== but agents’ strategies are not weakly dominant. The second-price auction has an efficient equilibrium in weakly dominant strategies, but also has many other, inefficient, equilibria.==== ====A sufficient condition for both kinds of robustness is ==== strategy-proofness: agents use ==== dominant strategies. Weak strategy-proofness obviously follows, and full implementation follows because a game can have at most one strictly dominant strategy profile. Strict strategy-proofness has the further appeal of strategic simplicity: since each agent has a unique strictly dominant strategy, she need not reason about her opponents’ behaviour.====In this paper, we study strict strategy-proofness in the canonical one-dimensional mechanism design setting with private values and quasi-linear and strictly single-crossing preferences. It is well-known that weak strategy-proofness is equivalent to monotonicity plus the envelope formula; we show in Section 2 that ==== strategy-proofness is equivalent ==== monotonicity plus the envelope formula.====In Section 3, we derive the implication that any weakly strategy-proof mechanism is ==== strictly strategy-proof, meaning that it can be made strictly strategy-proof by an arbitrarily small modification. This can be viewed as a novel robustness property of weak strategy-proofness. It follows further that a principal designing a weakly strategy-proof mechanism can achieve ==== strategy-proofness at arbitrarily small cost.",Strictly strategy-proof auctions,https://www.sciencedirect.com/science/article/pii/S0165489620300627,17 July 2020,2020,Research Article,163.0
"Elkind Edith,Grandi Umberto,Rossi Francesca,Slinko Arkadii","University of Oxford, United Kingdom,University of Toulouse, France,IBM T.J. Watson Research Lab, NY, USA,The University of Auckland, New Zealand","Received 24 April 2019, Revised 7 July 2020, Accepted 7 July 2020, Available online 11 July 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.07.001,Cited by (2),"By the Gibbard–Satterthwaite theorem, every reasonable voting rule for three or more alternatives is susceptible to manipulation: there exist elections where one or more voters can change the election outcome in their favour by unilaterally modifying their vote. When a given election admits several such voters, strategic voting becomes a game among potential manipulators: a manipulative vote that leads to a better outcome when other voters are truthful may lead to disastrous results when other voters choose to manipulate as well. We consider this situation from the perspective of a boundedly rational voter, using an appropriately adapted cognitive hierarchy framework to model voters’ limitations. We investigate the complexity of algorithmic questions that such a voter faces when deciding on whether to manipulate. We focus on ====-approval voting rules, with ====. We provide polynomial-time algorithms for ==== and hardness results for ","Imagine that you and your friends are choosing a restaurant to go to for dinner. Everybody is asked to name their two most preferred cuisines, and the cuisine named most frequently will be selected (this voting rule is known as 2-approval). Your favourite cuisine is Japanese and your second most preferred cuisine is Indian. Indian is quite popular among your friends, and you know that if you name it among your favourite two cuisines, it will be selected. On the other hand, you also know that only a few of your friends like Chinese food. Will you vote for Japanese and Chinese to give Japanese cuisine a chance?====This example illustrates that group decision-making is a complex process that involves agents reasoning about other agents’ preferences. Individual decision-makers would like to influence the final decision in a way that is beneficial to them, and hence they may be strategic in communicating their individual preferences. Indeed, it is essentially impossible to eliminate strategic behaviour by a clever choice of the voting rule: the groundbreaking result of Gibbard (1973) and Satterthwaite (1975) states that, under any onto and non-dictatorial social choice rule, there exist situations where a voter can achieve a better outcome by casting a strategic vote rather than the sincere one, provided that everyone else votes sincerely; in what follows, we will refer to voters that can benefit from voting strategically when others remain sincere as ==== or simply ====.====The Gibbard–Satterthwaite theorem alerts us that strategic behaviour of voters cannot be ignored, but it does not tell us under which circumstances it actually happens. Of course, if there is just a single GS-manipulator at a given profile, and he==== ==== is fully aware of other voters’ preferences, it is rational for him to manipulate. However, even in this case this voter may prefer to vote truthfully, simply because he may assign a high value to announcing his true preferences; we call such voters ====, or ====. Moreover, if there are two or more GS-manipulators, it is no longer easy for them to make up their mind in favour of manipulation: while the Gibbard–Satterthwaite theorem tells us that each of these voters would benefit from voting strategically assuming that all other voters remain truthful, it does not offer any predictions when several voters may be able to manipulate simultaneously.====The overarching goal of this paper is to investigate the complexity of the decision that a strategic participant of a voting game must make in the presence of other strategic voters. To obtain a realistic model, we assume that a voter is a boundedly rational agent, most notably in its representation of other agents’ strategic abilities, and we suggest a model for such a behaviour. We also assume that voters have bounded computational abilities, and use the toolbox of computational social choice (Brandt et al., 2015) to study the computational complexity of the problems that strategic voters face. Thus, our work merges two views of bounded rationality: the strategic one, which is inspired by the game theory and economics literature, and the algorithmic one, which is more common in the computer science literature and goes back to Bartholdi et al. (1989) and Bartholdi and Orlin (1991). As this is the first study to combine these two views of bounded rationality in the context of voting, it is natural to start by investigating simple voting rules. Therefore, throughout the paper we focus on the family of ====-approval voting rules for ====.",Cognitive hierarchy and voting manipulation in ,https://www.sciencedirect.com/science/article/pii/S0165489620300615,11 July 2020,2020,Research Article,164.0
"Gersbach Hans,Haller Hans","CER-ETH, Center for Economic Research at ETH Zurich and CEPR, Zürichbergstrasse 18, 8092 Zurich, Switzerland,Department of Economics, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061-0316, USA","Received 13 August 2019, Revised 2 June 2020, Accepted 2 June 2020, Available online 2 July 2020, Version of Record 27 July 2020.",https://doi.org/10.1016/j.mathsocsci.2020.06.001,Cited by (0),"We study the self-organization of a population into productive partnerships (or “firms”) when agents are confronted with a hold-up problem upon making relation-specific investments in those firms. The problem may be mitigated if agents can leave a partnership in which they have invested, bearing the costs yet foregoing the benefits of the investment, join another partnership, invest there anew, and appropriate the surplus created by the new investment. To capture the idea we introduce the notion of reinvestment-proof equilibria in which no agent has an incentive to reinvest or to change his investment in the current firm. We show that the presence of a small inefficient firm causes substantial efficiency gains in all larger firms.",None,On efficient firm formation,https://www.sciencedirect.com/science/article/pii/S0165489620300573,2 July 2020,2020,Research Article,165.0
"Ganuza Juan-José,Llobet Gerard","Universitat Pompeu Fabra and Barcelona GSE, Spain,CEMFI, Spain,CEPR, United Kingdom","Received 11 July 2019, Revised 29 November 2019, Accepted 28 January 2020, Available online 25 June 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.011,Cited by (0),"This paper shows that the concession model discourages firms from acquiring information about the future profitability of a project. Uninformed contractors carry out good and bad projects because they are profitable in expected terms even though it would have been optimal to invest in screening them out according to their value. White elephants are identified as ==== negative net present-value projects that are nevertheless undertaken. Institutional arrangements that limit the losses that firms can bear exacerbate this distortion. We characterize the optimal concession contract, which fosters the acquisition of information and achieves the first best by conditioning the duration of the concession to the realization of the demand and includes payments for not carrying out some projects.","On May 28, 1873 the New York Times published an article called “White Elephants” that starts with the following paragraph ====As Bullen (2011) explains, this story is probably a myth as white elephants were considered a symbol of virtue and status in Siam and no king would consider them burdensome. Nevertheless, this story has taken root in the economics literature, popularized in papers such as (Robinson and Torvik, 2005), and it is now commonly used to identify facilities and infrastructures of little practical use or value. More precisely, white elephants are associated with projects with a negative social return.====The perception that these white elephants are common is widespread and each country has its favorite example: The Montréal-Mirabel International Airport was once the largest in the world in terms of surface==== ; the New South China Mall, the largest mall in the world, has sat mostly vacant; the Brisbane’s Clem Jones Tunnel had less than 50% of the projected traffic even after tolls were slashed by half; the “Radial” tool highways around Madrid suffered from an even superior demand overestimation.==== ====In this paper we discuss how these white elephants may come about in the construction of public infrastructure by private firms, which are compensated through concession contracts. We argue that standard procurement procedures do not provide incentives for firms to get informed about relevant characteristics of the demand (or the cost) and, as a result, contractors engage in an insufficient screening of projects and end up building unnecessary infrastructures.====Concession contracts, typically signed between a government agency and a private consortium of firms (the concessionaire), involve the transfer of the construction and/or the operation of an asset from the former to the latter for a period of time. Toll highways are a classical example. Their construction is managed by a concessionaire that engages in a long-term contract with a government, designed to recover the large investment cost through user fees (i.e. toll revenues). This kind of schemes has been extended in recent years to all types of Public–Private Partnerships (PPPs).====  They are now common in the construction of roads, prisons, airports, hospitals, railway infrastructure, etc., where the government pays a fee according to their usage. As opposed to what occurs in public-work contracts where the government assumes all risk and manages the infrastructure directly, concession contracts are used to transfer the risk to the concessionaire.====The risk of such projects is high, since PPP concessions covering infrastructures are long-term contracts — to allow investors to recover the huge upfront investment, and their profitability depends on variables (traffic, costs, etc.) that are difficult to forecast even over a short horizon. In the case of demand estimation, (Bain and Polakovic, 2005) report, using a sample of highway concession projects collected by Standard and Poors (S&P), that first-year traffic volumes average about 76% of their predicted values and an error with a standard deviation of 0.26. A similar error persisted in the years 2 to 5.==== ==== The underperformance of the concessions is striking since one of its main advantages over a standard public-work contract is precisely the fact that a firm can better assess the demand since its own money is at stake.====Our paper provides an explanation for this result that arises from the interaction between the (asymmetric) consequences of the unpredictability of revenue (or costs==== ====) and the incentives for firms to acquire information. Regarding revenue risk, it is often the case that if traffic is lower than expected, concessionaires will force a renegotiation of the contract. Guasch (2004) analyzes concession contracts in Latin America and shows that over 30% of the concession contracts are renegotiated. In the transportation sector this proportion increases to 54.7%. The results are similar in other contracts characterized by large sunk investments, long concession horizons, and demands risk such as water and sanitation, where renegotiation occurs in 74.4%  of the cases. Importantly, Guasch (2004) also reports that most renegotiations favor the concessionaire by raising tariffs (62% of the cases) and/or through a decrease in the required investment. In other cases the duration of the concession is extended. These changes are in contrast with what occurs when revenues are higher than expected. In that case concessionaires typically cash the extra profits.====These asymmetries are controversial and, in many countries, they have had an impact in the public debate. In this paper we highlight another important distortion that these asymmetries might entail. The core idea of our work is that if the potential losses of the concessions are limited by a future renegotiation (while the firm appropriates the potential upside) the incentives to acquire information are reduced, negatively affecting project selection.====We propose a principal–agent model where the public sector (the principal) is the sponsor interested in carrying out a public-work project. This project has an unknown value that might be uncovered with the costly acquisition of information by the contractor (the agent). The principal designs a simple concession contract that assigns a proportion of the value of the project to the contractor, for example, through the collection of toll fees during a limited period of time. The agent decides on the acquisition of information and, contingent on that, whether to invest in the project or not. The goal of the principal is to foster the acquisition of information and to induce investment only when the value of the project is higher than its cost.====We characterize the optimal concession contract and we show that typically both objectives (undertaking only projects that generate positive surplus and inducing the acquisition of information when it is efficient to do so) cannot be attained at the same time. As a result some white elephants inevitably emerge in equilibrium. The intuition is as follows. Suppose first that the cost of acquiring information is small. Then, the first best can be attained by designing a concession contract that allocates a share of the revenue to the firm that, once informed, allows it to break even only when a project has a positive social value. Naturally, the incentives to acquire information decrease as its cost increases. To prevent the firm from carrying out the project without information the concession contract must be distorted. In particular, the value of information rises if the share of the revenues that the firm appropriates is reduced and avoiding bad projects becomes more valuable. In this second-best world, some good projects are not undertaken. Furthermore, for a sufficiently high cost of information, the distortions necessary to foster the acquisition of information become too expensive. In that case the principal prefers not to induce the acquisition of information, leading to some bad projects (white elephants) to be undertaken in equilibrium. Our result is not specific to the case studied here and similar implications arise when we extend the model to allow for competition.====It is important to notice that our definition of white elephants is restricted to those projects for which proper ex-ante information acquisition was optimal but it did not occur. This is in contrast with the way they are typically interpreted, where evaluation of projects like the ones mentioned earlier is carried out ex-post. The presence of uncertainty inevitably implies that for some projects the costs will never be recovered but this does not imply a bad upfront project selection. The question that our paper addresses is how those bad projects that ex-ante could have been efficiently screened out by investing in information end up being built, and how standard contracts could be adapted to provide the proper incentives and reduce their cost for society.====In practice, as mentioned above, an important element of these concession contracts is the limit on the losses that the contractor can incur due to the renegotiation or the rescue of the infrastructure by the public sector. When we introduce this element in the model, we show that the lower the losses that the firm might absorb, the more critical is the problem of providing incentives for the acquisition of information. As a result, distortions increase when fewer losses can be absorbed leading to more white elephants. An important policy recommendation of our analysis is that governments should compensate bankrupt concessionaires according to the value of the concession and not the incurred cost. This value can be uncovered by auctioning the failed concession.==== ====Underlying our analysis there are two important assumptions. First, we assume that the firm can acquire information beyond the knowledge of the principal. This assumption is very natural in the case of many PPPs in which the contractor might have an expertise in a particular infrastructure or a particular market. We claim, however, that this assumption is also appealing in a more general context even when the firm does not necessarily possess an advantage in the acquisition of information simply because the public sector typically discloses all the available information prior to the contract. This is due not only to the legal requirements. The provision of information might also entail efficiency gains by inducing a better match between the firm and the project and by reducing the winner’s course when the project is allocated through an auction. Thus, our private information acquisition process ought to be understood as the additional precision in the assessment of the value of the project that the firm might obtain by carrying out its own study.====Second, in the benchmark model we take the institution of concessions as given and we do not allow the firm to make any payments above and beyond the cost of construction of the infrastructure plus the cost of acquiring information. As it is well-known, if the firm can make upfront payments the incentive problem in the acquisition of information is alleviated. In the limit, if the firm could pay the expected social value of the project, transferring ownership of the infrastructure, the first best could also be attained regardless of the cost of information acquisition. In the extensions section of the paper we show, however, that in order to keep firm rents under control, it is often optimal to make a limited usage of these upfront payments.==== ====Our paper is related to the literature that has studied the optimal concession contract. The prevalence of renegotiations highlights the importance of managing the risk of these long-term contracts. While the firm has control of most cost components, the demand is often exogenous to its actions. This might be due to many reasons such as the fact that the quality of infrastructures like highways can be specified as part of the contract. As a result, demand risk should be absorbed by the government. Transferring risk to the firm has little effect on performance but might induce contract renegotiation.====One way to achieve this goal is to adjust the duration of the concession to the evolution of the demand. Engel et al., 1997, Engel et al., 2001 proposed the Least Present Value of the Revenues (LPVR) mechanism that has become the most influential way to exploit this idea. The mechanism consists of a flexible-term concession that awards the contract to the bidder that demands the lowest discounted total revenue for the project. The winner is then entitled to receive the revenues of the concession up to the point in which their discounted flow equals the present value of the revenue offered in the bid. At that point the concession expires.====The previous literature has usually ignored the optimal selection of projects, which is the main focus of our work. Here we show that although in our model the revenues of the project are exogenous, a standard application of the LPVR mechanism is suboptimal because it does not provide incentives for firms to acquire information. Taking this effect into account requires some demand risk to be transferred to the contractor in order to provide incentives to get informed.==== ====However, our main theoretical result, the characterization of the optimal concession contract, relies on a powerful idea of the LPVR mechanism: the need to enrich the concession contract with the information available regarding the realized demand. We show that a concession contract with this feature, together with the possibility of paying for not undertaking a project, can attain the first best. As in the standard application, when the realization of the demand is sufficiently high (indicating that it is likely that information was acquired), the firm is compensated for the total cost. It also receives a compensation when the project is not carried out. If demand is low, however, the firm is penalized. Although it can be argued that a mechanism involving payments to the firm for not undertaking the project may not be politically feasible, we believe that our approach opens a new way to think about the provision of incentives in concession contracts when project selection matters. We also characterize the second best contract when such a compensation is not feasible and we show that the distortions that arise are similar to those described in the benchmark model.====From a more general point of view our paper is related to the literature that has studied expert delegation in a principal–agent framework. A principal needs the advice of a potentially biased expert for taking a decision and designs a contract for giving the expert incentives to acquire information. This literature started with Lambert (1986) and has continued with contributions like Demski and Sappington (1987) and Feess and Walzl (2004). Zambrano (2019) is probably the closest paper. It studies a setting in which the principal has to choose between a risky project and a safe one, and pays for the expert advice according to the revenues of the project. The optimal contract implies a distortion in project choice in favor of the risky one (since the optimal decision without information is to choose the safe project).====Our paper also fits in the literature on information acquisition, including papers like Persico (2000) and Szalay (2009). In particular, Szalay (2009) analyzes in a principal–agent framework how the contract offered by the principal shapes the incentives for the agent to acquire information. As opposed to what we find in our paper, in that framework high-powered contracts, that make the agent’s informational rents more risky, increase incentives for information acquisition.====The paper proceeds as follows. Sections 2 The model, 3 The optimal contract discuss the basic model. Section 4 analyzes the effects of limits on the losses that firms can absorb and Section 5 characterizes the optimal flexible-term concession contract. Section 6 briefly discusses extensions of the model, which are developed in detail in Ganuza and Llobet (2019). Section 7 concludes. All proofs are relegated to a technical appendix.",The simple economics of white elephants,https://www.sciencedirect.com/science/article/pii/S0165489620300317,25 June 2020,2020,Research Article,166.0
Kelly Jerry S.,"Department of Economics, Syracuse University, Syracuse, NY 13244-1020, USA","Received 13 November 2019, Revised 23 March 2020, Accepted 27 May 2020, Available online 12 June 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.05.003,Cited by (3), alternatives.,"We characterize the social choice correspondence that, at each profile of preferences, selects exactly the set of Pareto optimal alternatives. Although the collection of Pareto optimal states at any situation has long been a central object of concern in welfare economics, there is only one (very recent) paper (Duddy and Piggins, 2020) we know of characterizing that correspondence and it uses very different properties.==== ====The central result is that, for at least four alternatives, the Pareto correspondence is completely characterized by four conditions: The first of these is the Pareto condition which says an alternative ==== is not in the chosen set if there is an alternative ==== such that everyone without exception strictly prefers ==== to ====. Clearly, the Pareto correspondence satisfies the Pareto condition This may seem odd to require at first, but the condition of excluding Pareto dominated alternatives is very weak and is used in characterizing many standard social choice correspondences. This use of the Pareto condition for characterization of the Pareto correspondence will be taken up in the conclusion.====The three additional conditions are tops-in, balancedness, and stability.  Tops-in is the requirement that everyone’s top-ranked alternative is in the chosen set. Balancedness, (introduced in Kelly and Qi, 2019), says that if one individual has ==== ranked immediately above ==== and another individual has ==== ranked immediately above ====, then if we change preferences only by switching ==== and ==== for those two individuals, the choice set is unchanged. Finally, a correspondence satisfies stability (related to a condition in Campbell et al., 2018) if whenever an individual ranks a chosen element ==== just above an element ==== and then the situation changes only by bringing ==== down just below ==== for that individual, the consequences are very restricted: If the chosen set changes, it must either drop ==== or add ==== Note that we are not restricting ourselves in this paper to “desirable” conditions. First, because what is desirable is highly context-dependent. But second, the Pareto correspondence itself is not desirable – if only because choice sets are often too large – so any set of characterizing conditions likely includes some that are undesirable. We characterize the Pareto correspondence not because it is a good social procedure but because it is a foundation piece of social choice theory, much of which is concerned with refinements of this correspondence.====Examples are presented to show these conditions are independent and necessary. Two related results, using weaker conditions, for the cases of two or three alternatives are also included.",Characterization of the Pareto social choice correspondence,https://www.sciencedirect.com/science/article/pii/S0165489620300561,12 June 2020,2020,Research Article,168.0
"Altan Başak,Sunay M. Oğuz","Department of Economics, Ozyegin University, Istanbul, Turkey,Open Networking Foundation, CA, USA","Received 14 February 2017, Revised 31 July 2018, Accepted 8 June 2020, Available online 12 June 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.06.003,Cited by (1),"This study identifies optimal transmission mechanisms for a streaming video service in a peer-to-peer network structure as a function of the number of active peers, a common service value, a common discount factor, and costs accumulated by the server and peers under two different peer participation settings: no peer churn and with peer churn. We calculate a uniform service price for every possible peer-to-peer distribution tree and identify the optimal transmission mechanism under different cost structures.","The Internet has been one of the most impactful innovations.==== ==== According to Cisco Systems Inc. (2017), it is expected that by 2021, the global annual Internet traffic will reach 3.3 zettabytes per year with video being the most significant contributor, accounting to 82% of the overall traffic. Live and on-demand video streaming services have become increasingly popular, providing a significant portion of the video traffic. Cisco Systems Inc. (2017) reports that the Internet video traffic will grow fourfold from 2016 to 2021 and that live Internet video will account for 13% of the Internet video traffic by 2021. Additionally, live video traffic is expected to grow 15-fold from 2016 to 2021. The defining feature of these live applications is that the video is consumed while it is being downloaded. Due to the lack of widespread support of sending streaming media on the public Internet, two transmission mechanisms are possible for such services: client–server (CS) mechanism using content-delivery networks (CDN) and peer-to-peer (P2P) mechanism. In the CS mechanism, a server provides service to all peers. When CDNs are employed, the server is replicated in strategical locations on the Internet to serve nearby peers directly. Conversely, in the P2P mechanism, the server provides service to a subset of the peers, and the content is progressively propagated through the peer population via seeding (sharing the content with other peers). Even though P2P file download has achieved significant popularity, success for P2P video streaming has been more limited.====In this study, we identify the optimal distribution tree among all P2P mechanisms as well as among the CS mechanism with one server under different cost (linear, concave and convex) and peer participation (no peer churn and peer churn) structures with uniform pricing. There has been considerable research on incentives to participate in P2P systems. In this study, we are not interested in the incentives of a peer to share the streaming media. We consider a live streaming service similar to a P2PTV where if a peer is assigned to seed the video to another peer, the peer will simultaneously upload the content while downloading it. The P2PTV system turns users into a re-broadcaster of the video if necessary. These services are quite active in China (Fowler and McBride, 2005). Peers pay for the broadcast service up front while subscribing to the video delivery system. In this context, the users pay a fix price to access certain content and it is insensitive to some aspects of video streaming such as the time they receive the content or prioritization. Pay up front uniform pricing is quite commonly used in video streaming services such as Netflix and Youtube. This pricing strategy is quite useful in tractability as users pay the same price. Even though Netflix and Youtube currently do not have P2P network structure at the moment, they are expected to switch to P2P structure in near future due to its scalability. Netflix explores the possibility to integrate P2P as an alternative channel for video streaming as it faced tremendous challenges dealing with ever increasing traffic in its traditional hub-and-spoke model.====In this paper, we develop a framework for streaming video to identify the optimal transmission mechanism. In the literature, one of the major stated advantages of P2P mechanisms over the CS mechanism has been their ability to lower the server costs as the server streams the media to a subset of peers. However, this cost-cutting scheme also lowers the payoffs of the peers due to higher peer costs as a result of seeding and potentially lower observed quality-of-experience (QoE). This inevitably lowers the price that the server can charge for the service, which in turn lowers the revenues accrued by the server. In an attempt to capture the trade-off between lower costs and higher prices, we consider a streaming video broadcast system with ==== identical peers and one server. The server announces the price of the video service. The peers decide whether to participate. Then, the server starts streaming the video with the chosen distribution mechanism. We assume that a peer is capable of simultaneous transmission, reception and playback. In a P2P mechanism, if a peer is required to seed the content to another peer seeding the content starts one period after downloading it. All agents, the server and the peers, are risk neutral and have the same discount factor. The cost of the server seeding the content to ====-many peers is ==== with ====. We study three cost structures: (i) linear, ====; (ii) concave, ====; (iii) convex, ====. For a peer, the cost of downloading the content is ==== and uploading the content to ====-many peers is ====. We consider a payment scheme where peers pay for the service upfront regardless of the pre-roll delays. However, if a peer is scheduled to receive the content from another peer, the peer has to wait to be able to download it. The structure of the P2P tree and location of this peer on this tree determine how long the peer has to wait. In an ====-staged P2P tree, peers located in the first stage receive the video directly from the server in the first period. However, peers located on stage ==== receive the video from the peers located on stage ====. That is, stage ==== peers have to wait ==== periods to start watching the content even though they pay for the service in the first period. The interaction between the server and the peers is as follows. First, the server announces the price and distribution mechanism of the video service. Then, peers choose whether to participate. Finally, the server starts streaming with the distribution mechanism chosen. We study two peer participation settings. In the first one, no peer churn, a peer stays in the system until she watches the entire video. In the second one, with peer churn, even if a peer pays for the service, she may leave the system before watching the entire video.====There has been considerable amount of work in the literature from the technical perspective of P2P video-streaming. Synopses of different approaches are reviewed in Yong et al. (2008) and Zhang and Hassanein (2012). In general, the related literature has focused on improving QoE. Specifically, systems and associated mechanisms that provide higher video quality, better playback continuity, lower set-up and pre-roll delay and lower delay variance amongst peers have been developed and studied. However, the QoE provided by the network affects how users utilize the network. Hence, the operators must decide on what video distribution technology to use, and how much to charge for the service. In this study, we examine a server offering a video streaming service to a set of users (peers) where the server must simultaneously set both the price and the distribution mechanism which affects the QoE.====Despite the importance of the subject, studies on communication systems that take the economic perspective into account have been very scarce. In one of the early papers, He and Walrand (2006) introduce a generic model for pricing Internet services with multiple service providers and propose a fair revenue-sharing scheme based on the concept of weighted proportional fairness. Krishnan et al. (2007) study the economic characteristics of P2P networks as well as media distribution strategy in P2P networks. Early work on the Internet focuses on pricing mechanisms to allocate the transmission capacity among end users.==== ==== Later work, however, focuses on Internet backbone competition. Laffont et al., 2001, Laffont et al., 2003 study the pricing strategies of Internet backbone providers and effects of access charges on the competition among Internet backbone providers. Two perfectly substitutable backbone providers compete for end users and have interconnection agreements called peering contracts. These two studies examine peering contracts which handle the cost of serving an end user from the other backbone. Crémer et al. (2000) study Internet backbone providers and analyse the strategies that can be used by the dominant network. While peering contracts and backbone competition are relevant to our study, we focus our attention on a monopolist service provider’s optimal price choice and allocation structure of streaming video service among a set of users.====This study establishes that for all peer participation structures, the optimal distribution structure is either CS or a two-staged P2P mechanism where the server seeds the content to a set of peer in the first stage and these peers seed the content to the rest on the second stage. When there is no peer churn, we establish that if the marginal cost of uploading a video chunk is cheaper for the server, then P2P distribution mechanism fails and the server prefers to directly serve all the users. Additionally, we show that, under linear or concave costs, the optimal distribution structure is either CS or the two-staged P2P where the server outsources the entire broadcast service by seeding to only one peer in the first stage. For sufficiently low values of the discount factor, the server prefers sending the video chunks to all the peers directly, which is relevant for a server streaming live events. We also establish that, under concave costs, the set of the discount factor supporting CS distribution mechanism expands as ==== increases. When the server has convex costs, however, as the discount factor increases, the number of the peers who directly receive the video chunks from the server decreases gradually. When there is peer churn, as long as the discount factor is sufficiently low, CS is preferred to all two-staged P2P mechanisms. Otherwise, a particular two-staged P2P structure is optimal. Under linear and convex costs, as the discount factor increases, the number of the peers who directly receives the video from the server in the first stage decreases. However, such a gradual decrease does not exist for concave costs.====The remainder of this paper is organized as follows. Section 2 discusses the model. A mathematical analysis and corresponding results for different peer participation structures are presented in Section 3. Finally, conclusions are drawn in Section 4. All proofs are relegated to the Appendix.",Optimal peer-to-peer network for streaming multimedia broadcast,https://www.sciencedirect.com/science/article/pii/S0165489620300597,12 June 2020,2020,Research Article,169.0
Suzuki Toru,"University of Technology Sydney, PO Box 123, Broadway NSW 2007, Australia","Received 31 May 2019, Revised 25 November 2019, Accepted 4 June 2020, Available online 11 June 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.06.002,Cited by (0),"Since sending explicit messages can be costly, people often utilize “what is not said,” i.e., informative silence, to economize communication. This paper studies the efficient communication rule, which is fully informative while minimizing the use of explicit messages, in cooperative environments. It is shown that when the notion of context is defined as the finest mutually self-evident event that contains the current state, the efficient use of informative silence exhibits the defining property of indexicals in natural languages. While the efficient use of silence could be complex, it is also found that the efficient use of silence can be as “simple” as the use of indexicals in natural languages if and only if the information structure satisfies some centrality and dominance properties.","Uttering and writing words might not be as laborious as piling up bricks, yet they still consume time and cognitive energy. Consequently, people often utilize “informative silence” to economize communication. For example, suppose that a manager wants a worker to perform his routine task. Unless the worker is new to the office, the manager usually does not need to provide any explicit instruction to make the worker perform the routine; a competent worker would interpret the fact that the manager did not give any explicit instruction as the implicit instruction to perform his routine. This type of informative silence can be found in almost every practical communication; in fact, if we need to be explicit about every single detail as silence conveys no information, our verbal messages would sound like commands to a robot, and our written messages would look like a computer program. Understanding tacit communication in cooperative environments is important for economics since many economic activities rely on cooperative communication that utilizes implicit messages.====If people can communicate with silence so appropriately and broadly across various environments, the use and interpretation of informative silence need to follow “conversational logic” shared by a linguistic community, i.e., pragmatics.==== ==== The purpose of this paper is to investigate such a tacit communication rule based on the approach that is familiar to economic theory; the tacit communication rule is analyzed as the efficient communication rule designed by a fictitious linguistic engineer. This paper shows that the way silence conveys information in the efficient rule is analogous to indexicals in natural languages. The finding provides novel insights into how economic agents combine explicit and implicit messages in cooperative communication.====The current paper considers the following simple communication problem to analyze tacit communication in a cooperative environment. There are two agents, a speaker and a listener. There is a finite set of states, and each agent is endowed with a partitional information function where each cell in the speaker (listener)’s partition is interpreted as the speaker (listener)’s “situation” at the state. A communication rule is then defined as the sender’s messaging rule that specifies whether to send an explicit message, which is costly, or remain silent, which is costless, at each state. The premise of this paper is that the tacit communication rule, i.e., the pragmatics of informative silence, is determined by a fictitious linguistic engineer who designs the efficient communication rule in the cooperative environment. The current paper then studies the efficient communication rule that fully conveys the speaker’s private information while minimizing the use of explicit messages.====Due to the nature of silence, an agent cannot use informative silence to indicate two different situations that cannot be distinguished from the perspective of the listener. Thus, when the speaker uses informative silence in one situation, it could restrict the feasibility of informative silence in other situations. Since the set of indistinguishable situations for the listener can be interwoven, solving the trade-off can be complex. Moreover, because of the combinatorial nature of the problem, a small change in the prior probability could dramatically change the efficient use of silence. Consequently, there is no simple regularity in the efficient use of informative silence in general.====The key to understanding the efficient use of silence is the concept of ====. In semiotics and philosophy of language, indexicals are signs or words whose reference is systematically determined by a context. For instance, the indexical “I” itself does not refer to any particular person by itself, but it refers to Mike in the context where Mike utters the word “I”. That is, the word “I” operates as a function that specifies the content of “I” once the input variable “utterer” is given by a context. Similarly, the indexical “now” itself does not refer to a particular time by itself, but the content is determined once a context provides the time of utterance. In other words, an indexical indicates an object without using the name by utilizing contextual information, which is public by nature. For example, the indexical “now” exploits the fact that the date of the utterance is common knowledge between agents, whereas the indexical “I” utilizes the fact that the speaker of “I” is common knowledge between agents. In the current paper, since the subject of communication is the speaker’s situation, a context is defined as the finest mutually self-evident event that contains the current state. Then, it is shown that informative silence in efficient communication rules also has an indexical property; silence itself does not refer to any particular situation by itself, but the content of silence is determined once a context is given. More specifically, given a context, call a situation or a union of situations of the speaker as an implicitly expressible event if there exists a fully informative rule that uses silence for the event. Then, informative silence in any efficient communication rule systematically refers to the most likely implicitly expressible event given the current context. It is also found that the common knowledge property in the definition of context is essential for the indexicality of informative silence; that is, if the notion of context is defined in a way that lacks common knowledge property, the efficient use of silence could fail to have indexical property.====While the indexical property can help us to find the efficient use of silence to some degree, it does not always tell us the exact use immediately. This is because informative silence could refer to a set of situations rather than a specific situation given a context, and finding out the union of situations that is most likely given a context requires combinatorial optimization. The computationally demanding nature is contrary to indexicals in natural languages, which directly refers to an entity rather than a non-trivial combination of entities in a context. This paper then characterizes the information structure in which informative silence in efficient communication rules directly refers to a specific situation rather than some combination of situations as indexicals in natural languages do. To characterize the information structure, a notion of ==== in the information structure is introduced. Intuitively, the speaker’s situation is central in a context if her situation is relevant to every possible situation of the listener in the current context.==== ==== Another important property is local dominance, which is a strong version of the notion of “most common situation”. It is shown that informative silence in the efficient rule directly refers to a specific situation as indexicals in natural languages do if and only if the specific situation is central and locally dominant in the current context. This result has an intriguing implication on the amount of reasoning that the use of indexicals in natural languages demands; the condition of centrality suggests that when the speaker’s informative silence is as direct as indexicals in natural languages, the derivation does not require the consideration of the listener’s situation that is not directly relevant to the speaker’s current situation.====Directly referring to a specific situation still does not make informative silence as user-friendly as indexicals in natural languages; checking local dominance could require tedious numerical evaluations, whereas indexicals in natural languages always refer to an object intuitively given the spatial, temporal, and personal relations given by context without using any cardinal information. Thus, this paper also provides the information structure in which agents can find the efficient use of silence only from ordinal information. More specifically, I provide a condition that guarantees the local dominance condition when agents only know the probability ranking over events.====The rest of this paper is organized as follows. After the literature review, Section 2 introduces the model. In Section 3, the efficient communication rule is analyzed, and the indexical structure of informative silence is identified. Section 4 characterizes information structures in which the efficient use of silence works as intuitively as indexicals in natural languages do. Section 5 provides some discussions followed by concluding remarks in Section 6.==== The current paper contributes to the literature on “economics and language”, which considers natural languages as a fundamental institution for economic activities and explains the properties of natural languages based on the method that is familiar to economics. The approach of the current paper is in line with Rubinstein, 1996, Rubinstein, 2000, which derive some properties of natural languages as if they are optimally designed by a fictitious linguistic engineer.==== ==== In his seminal paper, Rubinstein (1996) demonstrates that some binary relations in natural languages can be obtained from indication-friendliness, informativeness, and describability.==== ==== In the current paper, on the other hand, a concept in ==== is obtained from the perspective of efficiency; informative silence has indexicality when a communication rule is designed to be efficient.====While pragmatics traditionally focuses on communication in cooperative environments, the economics literature that utilizes some concepts in pragmatics has mostly focused on strategic environments, e.g., Glazer and Rubinstein, 2001, Glazer and Rubinstein, 2006, and Suzuki (2017).==== ==== The current paper studies pragmatics in the context of cooperative communication by following the basic tenet of some major theories in pragmatics: people can communicate beyond what is explicitly stated since they share some tacit principles to infer the meaning of an implicit expression beyond what is explicitly stated.==== ==== Since those theories are not based on formal models, it is not easy to evaluate to what extent the current paper shares their principles. However, the current paper is at least in line with their idea that the economy of communication plays an important role in tacit communication.==== ====The current paper is also related to the literature on “communication in teams”. When communication is costly, communication becomes a non-trivial economic problem even under the common interest setting. Marschak and Radner (1972) analyze the optimal information protocol in an organization based on statistical decision theory. Arrow (1974) observes that when communication is costly, people use “organizational code” to economize communication. Cremer et al. (2007) formalize the idea of organizational code and study the nature of the optimal code under different organizational structures. As in the current paper, Cremer et al. (2007) derive the organizational code as an efficient communication system in an environment where communication is costly. However, the idea of informative silence is qualitatively different from that of organizational code in Cremer et al. (2007); the premise of their paper is that agents are boundedly rational and can deal with only a fixed number of messages. Then, since their code system is coarser than their state space, the efficient code is essentially an optimal partition of the state space that balances the trade-off between interpretation costs and informativeness. On the contrary, in the current paper, the agent ==== describe her current situation as her message space is large enough. However, since sending an explicit message is costly, the main question is how she can fully convey her private information while minimizing her use of explicit messages given an information structure.====The model of this paper is built on the framework of Aumann (1976). Traditionally, the study of communication in this setting focuses on the process of two-sided communication that achieves common knowledge of posterior beliefs, e.g., Geanakoplos and Polemarchakis (1982) and Parikh et al. (1990). Since the literature of “consensus and communication” analyzes cooperative communication, their models do not adopt a game theoretical formulation as in the current paper. However, unlike in the current paper, they focus on the setting in which agents ==== report their private information; their main question is whether agents can reach consensus through communication ==== such as posterior beliefs or actions. The question of the current paper, on the other hand, is entirely different; this paper is interested in ==== an agent can efficiently convey her private information to another agent when she ==== report her private information but sending an explicit report is costly. Thus, unlike in the literature on consensus and communication, the current paper focuses on one-sided communication even though the result of the current paper can be extended to two-sided communication.====Finally, the current paper shows that when indexical silence works as intuitively as indexicals in natural languages do, the underlying information structure has to satisfy a centrality condition, which suggests that the derivation of the efficient use of silence does not require the consideration of the listener’s situation that is not directly relevant to the speaker’s current situation. If the property of natural languages reflects the cognitive capacity of the human mind, the centrality condition can be interpreted as “revealed simplicity”. Thus, the current paper also contributes to the literature on bounded rationality that formalizes notions of simplicity and complexity relative to underlying economic problems. For example, Neyman (1985) and Rubinstein (1986) utilize finite state automata to define the complexity of strategies in repeated games. Recently, Li (2017) defines simplicity of dominant strategies, i.e., obviously-dominant strategy, based on whether the agent can identify dominance without contingent reasoning, which is similar to the finding of the current paper.",Efficient communication and indexicality,https://www.sciencedirect.com/science/article/pii/S0165489620300585,11 June 2020,2020,Research Article,170.0
"Baharad Eyal,Neeman Zvika,Rubinchik Anna","Department of Economics; Bar Ilan University, Israel,Department of Economics; Tel Aviv University, Israel,Department of Economics; University of Haifa, Israel","Received 11 January 2017, Revised 7 January 2019, Accepted 12 September 2019, Available online 28 May 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2019.09.007,Cited by (2),"We demonstrate that the inconsistency associated with judgment aggregation, known as the “doctrinal paradox”, is not a rare exception. There are ==== individuals who have opinions about ==== propositions. Each opinion expresses the degree of belief or conviction and thus belongs to the unit interval ====. We work with an arbitrary proposition aggregator that maps opinions about ==== propositions into an overall opinion in ==== and an arbitrary individual opinions aggregator mapping opinions of ","The aggregation of binary judgments is well known to be problematic. We focus on a subclass of problems related to the so-called ==== (cf. Kornhauser and Sager, 1993). We demonstrate the prevalence of this paradox in a general class of judgement aggregators.====Before formulating our result more precisely, we start with a brief overview of the related literature.====The classical description of the paradox proceeds as follows. Suppose that a three-judge court has to make a decision on whether a defendant is liable for breach of contract. According to the existing legal regime, the defendant is liable if and only if the contract is valid (proposition 1), and the contract was breached (proposition 2). Assume that the first judge is convinced that both propositions are true, the second judge believes that the contract was valid but it was not breached, and the third one believes the opposite, i.e., that the first proposition is false while the second is true. Thus, the matrix of opinions is ====
 where the rows list the opinions of the three judges, the first two columns correspond to the propositions and the third one — to the conclusion of each judge. “Proposition-wise” majority voting would result in the assertion that both propositions should be true (2 against 1), thus in this case the defendant would be considered liable. However, only the first judge should conclude that the defendant is liable since he is the only one who believes that both propositions hold, and hence majority voting over the final decision yields the opposite verdict.====Note that under the first method for aggregating the judgements the initial step was to use majority voting to aggregate across individuals to form common ==== which then were to be aggregated into a conclusion. This is a ==== or ====based procedure. The second method of aggregation describes the ==== or ====based procedure.====Several contributions assess the relative merits of these two aggregation procedures for given aggregation rules (see, e.g., Bovens and Rabinowicz, 2004, Bovens and Rabinowicz, 2006, Pettit, 2001). De Clippel and Eliaz (2015) consider an environment where every voter receives a signal regarding truth values of the premises and all voters agree on how to reach the final decision based on the actual premises that describe the state of nature. A super-majority rule aggregates the individual reports about either premises or individual decisions. The authors find the premise-based approach to be superior to the outcome-based in the following sense. The set of symmetric Bayesian Nash equilibria (SBNE) of the premise-based voters’ game is a proper superset of the SBNE of the outcome-based game. Moreover in the presence of a separating set of signals distinguishing the two states, the SBNE in the premise-based game yields the efficient information aggregation: the probability of a discrepancy between the outcome of the game and the decision based on the actual state of nature is converging to zero as the number of voters approaches infinity.====The next natural question is whether there is a way to avoid the paradox altogether, i.e., if we do not insist on majority aggregators, is there a way to reach a common conclusion that is independent of the procedure (premise- or conclusion-based)?====Dietrich and Mongin (2010) examine the implication of two axioms: independence on premises and unanimity preservation on premises and non-premises. They show when aggregation functions that satisfy these axioms are a dictatorship or an oligarchy.====By and large, if individual opinions are restricted to be binary, the answer to this question is negative (Dietrich, 2006, List and Pettit, 2002, List, 2004, List and Pettit, 2004, Dietrich, 2007, Nehring and Puppe, 2008), and the analysis has intricate connections to the well-known impossibility results of K.==== ====Arrow and A.==== ====Sen, see Dietrich and List, 2007, Dietrich and List, 2008 and Dokow and Holzman (2010).====There is, however, an important caveat. The problem is typically formulated for a given set of rules that aggregate basic premises into conclusions or into ==== propositions. Thus aggregation rules across propositions have to abide by the laws of the underlying logic, or be ==== (Dietrich and List, 2010). With such a collection of consistency requirements it is often impossible to find a non-dictatorial rule that aggregates individual judgements, both basic and derived. This is true even if the individual judgements are restricted to be truth-functionally coherent themselves. The impossibility results extend even to multi-valued opinions (Guilbaud, 1952, Pauly and Van Hees, 2006).====Furthermore, it is possible to extend the domain to contain opinions that can describe the full range of judges’ degrees of confidence, or ====, i.e., allow the entries in the matrix of opinions to be from the unit interval ====, see, for example, Nehring (2007) and List (2005) and a survey by Genest and Zidek (1986). We also follow this convention.====Dietrich and List (2010) offer a unified approach to the aggregation problem. With the opinions being represented by real numbers, basic premises have to be combined in a consistent way given either the language syntax or the standard probability calculus. The basic lesson (most closely related to the current study) is that under a variety of natural restrictions imposed by language or underlying uncertainty, the linear aggregation rule is the only one that works in a consistent way (Dietrich and List, 2017a, Dietrich and List, 2017b). Herzberg (2013) proposes using multi-valued algebra as a framework for propositional-attitude aggregation: algebra homomorphism appears to be the only ==== aggregator of individual ==== that, naturally, satisfies the consistency requirements imposed by the language expressed as an algebraic structure. Thus, classical desiderata in this context are closely connected to linearity (see, also McConway, 1981).====We address the main question in an indirect way. First, we are agnostic about both the events and the context that give rise to the opinions of the judges. We start with the basic premises, much like the first two columns in the matrix of opinions above. We then work with two aggregators. The first one describes the relation between the basic premises and the conclusion, both on the individual and on the aggregate level (it generates the third column in the matrix of the example). The second one is the aggregator across individuals, which is most familiar to us from the social choice literature mentioned above.====In other words, our “language” imposes only a single validity restriction and this restriction is endogenously determined as the choice of one of the aggregators a-priori is not constrained. Given the first one, the output of the second aggregator is required to be consistent so as to avoid the doctrinal paradox. In the view of the results mentioned above it is not surprising that a pair of linear aggregators satisfy this restriction.====As we demonstrate in the next section, the impossibility results mentioned above cannot be extended to our setting even if the opinions are binary. One might conclude that in this case the paradox can be easily avoided. However, as we show, this conclusion is incorrect.====Our main result is that consistent aggregation is fragile on an unrestricted domain of opinions in the following sense. Choose a function that aggregates basic propositions into conclusions, or a deliberation rule. Then, for a given matrix of opinions, the set of all functions that can be used to aggregate across individuals while avoiding the paradox is very small, i.e., nowhere dense in the set of uniformly bounded functions.",The rarity of consistent aggregators,https://www.sciencedirect.com/science/article/pii/S0165489617300082,28 May 2020,2020,Research Article,171.0
"Boratyn Daria,Kirsch Werner,Słomczyński Wojciech,Stolicki Dariusz,Życzkowski Karol","Jagiellonian Center for Quantitative Research in Political Science, Jagiellonian University, Poland,Institute of Mathematics, Jagiellonian University, Poland,Fakultät für Mathematik und Informatik, FernUniversität Hagen, Germany,Dimitris-Tsatos-Institut für Europäische Verfassungswissenschaften, Germany,Institute of Political Science and International Relations, Jagiellonian University, Poland,Institute of Physics, Jagiellonian University, Poland","Received 20 December 2019, Accepted 9 April 2020, Available online 14 May 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.04.002,Cited by (2),"-th largest player under the uniform distribution. We analyze the average voting power of the ====-th largest player and its dependence on the quota, obtaining analytical and numerical results for small values of ","An ====–player ==== ==== is described by a ==== ====, where ==== is the standard ====–dimensional probability simplex, and a ==== ====. In such a game, the set of winning coalitions ====, where ==== is the set of players, is defined as follows: ====We denote the set of all ====-player weighted voting games by ====.====By ==== we mean a weighted voting game in which the number of players ==== and the quota ==== are fixed, and the weight vector ==== is drawn from the standard probability simplex ==== with some probability measure. Such games seem to be interesting for a number of reasons. First, the analysis of random weighted voting games enhances our understanding of weighted voting games in general. One of the major challenges in the field lies in the fact that generic results are usually rather difficult to obtain, while the behavior of weighted voting games in specific cases depends heavily on the characteristics of the specific weight vector and is often subject to number-theoretic peculiarities. For instance, some of the fundamental questions touch the relationship between the quota ==== and the influence of individual players or efficiency of the system as a whole. Yet, for fixed weight vectors those dependencies are not only discontinuous, but highly erratic. Randomizing the weights, and thus averaging them over the simplex, smooths out the peculiarities of specific weight vectors, revealing hitherto unobserved regularities.====Second, randomizing the weights is likely to be of interest from the standpoint of voting rule design. Rule design tends to take place before players’ weights are fixed, and thus any predictions regarding the effects of the rules must, to the extent such effects depend on voting weights, necessarily be probabilistic. Also, just like players’ preferences are treated as random to abstract away from particular issues and focus the attention on the voting rules themselves (Roth, 1988), treating voting weights as random further abstracts away the particular configuration of players and brings other parameters (such as the number of players or the quota) into the forefront.====Obviously, the characteristics of a random weighted voting game depend on the choice of the probability measure. In the present article, we focus on the uniform (Lebesgue) measure (which is equivalent to the familiar Impartial Anonymous Culture Model used in computational social choice, see Kuga and Nagatani, 1974, Gehrlein and Fishburn, 1976. For that measure we obtain exact closed-form formulae for the expectation and density of the distribution of voting weight of the ====-th largest player, an analytical formula for the expected values of product–moments of voting weights, a general theorem about the functional form of the relation between the expected values of the absolute and normalized Penrose–Banzhaf indices of the ====-th largest player and the quota, the characteristic function of the distribution of coalition weights, and an approximation of the Coleman efficiency index (the power of a collectivity to act). All of those results constitute an original contribution of the paper. We further outline several applications of those results in the field of mathematical voting theory and in some other areas.",Average weights and power in weighted voting games,https://www.sciencedirect.com/science/article/pii/S0165489620300408,14 May 2020,2020,Research Article,172.0
"Dietzenbacher Bas,Yanovskaya Elena","International Laboratory of Game Theory and Decision Making, National Research University Higher School of Economics, St. Petersburg, Russian Federation","Received 5 March 2020, Revised 29 April 2020, Accepted 30 April 2020, Available online 11 May 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.04.006,Cited by (2),"This note shows that the egalitarian Dutta and Ray (1989) solution for transferable utility games is self-antidual on the class of exact partition games. By applying a careful antiduality analysis, we derive several new axiomatic characterizations. Moreover, we point out an error in earlier work on antiduality and repair and strengthen several related characterizations on the class of convex games.","This note focuses on the structure and implications of a specific set of axioms related to the egalitarian Dutta and Ray (1989) solution for transferable utility games. For convex games, the Dutta and Ray (1989) solution prescribes the Lorenz dominating core element. The Dutta and Ray (1989) solution is mainly studied on the class of convex games and several axiomatic characterizations are provided. Already since the work of Dutta (1990), these characterizations are generally based on consistency properties. Whereas Dutta (1990) combined a fixed solution for two-player games with the consistency properties of Davis and Maschler (1965) and Hart and Mas-Colell (1989), these results were turned into full axiomatic characterizations by Klijn et al. (2000). Moreover, Klijn et al. (2000) provided a third characterization on the class of convex games based on an alternative consistency property.====Recently, Llerena and Mauri (2017) introduced the larger class of exact partition games and showed that the Dutta and Ray (1989) solution for these games behaves as for convex games, i.e. it assigns to each such game the Lorenz dominating core element. Dietzenbacher and Yanovskaya (2020) showed that some axiomatic characterizations of Klijn et al. (2000) for convex games can be extended to the class of exact partition games. Moreover, they provided another characterization based on the consistency property of Moulin (1985).====Oishi and Nakayama (2009) introduced an antiduality notion within the context of transferable utility games to structure games and solutions. Two games are antidual if the worth of each coalition in one game equals the worth of its complement minus the worth of the grand coalition in the other game. Two solutions are antidual if one solution assigns to each game the negated set of payoff allocations assigned by the other solution to the antidual game. A solution is self-antidual if it coincides with its antidual. Oishi and Nakayama (2009) showed that some well-known classes of games are antidual to each other and several solutions are self-antidual. Oishi et al. (2016) further developed the antiduality notion and showed that the classes of all games, balanced games, and convex games are each closed under antiduality. Moreover, they introduced antiduality for axioms and characterizations to uncover new structures. Two axioms are antidual if for each two antidual solutions, one property is satisfied by one solution if and only if the other property is satisfied by the other solution. An axiom is self-antidual if for each two antidual solutions, the property is satisfied by one solution if and only if it is satisfied by the other solution. In particular, Oishi et al. (2016) showed that the Dutta and Ray (1989) solution is self-antidual on the class of convex games and revealed new axiomatic characterizations antidual to the results of Klijn et al. (2000). However, these antidual characterizations are invalid due to an incorrect claim about self-antiduality of the axiom equal division stability.====The purpose of this note is twofold. On the one hand, we show that the class of exact partition games is closed under antiduality and that the Dutta and Ray (1989) solution is self-antidual on this class. By applying a careful antiduality analysis, we derive several new axiomatic characterizations of the Dutta and Ray (1989) solution on the class of exact partition games from the results of Dietzenbacher and Yanovskaya (2020). On the other hand, we repair and strengthen the results of Oishi et al. (2016) related to the Dutta and Ray (1989) solution on the class of convex games by providing the correct antidual of equal division stability and weakening the corresponding consistency properties.====This note is organized in the following way. Section 2 provides the preliminary notions and notations for transferable utility games and the Dutta and Ray (1989) solution. Section 3 performs an antiduality analysis on the class of exact partition games. Section 4 presents concluding remarks on antiduality in larger classes of games.",Antiduality in exact partition games,https://www.sciencedirect.com/science/article/pii/S0165489620300457,11 May 2020,2020,Research Article,173.0
"Jones Michael A.,McCune David,Wilson Jennifer M.","American Mathematical Society, Mathematical Reviews, 416 Fourth Street, Ann Arbor, MI 48103, USA,William Jewell College, Liberty, MO 64068, USA,Eugene Lang College, The New School, New York, NY 10011, USA","Received 16 August 2019, Revised 2 May 2020, Accepted 2 May 2020, Available online 6 May 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.05.001,Cited by (2),"We survey the apportionment methods used by the Republican Party in their 2012 and 2016 state presidential primaries, with a focus on the seven methods that are proportional. All of the proportional methods are quota-based, and all but one are new (or at least previously unstudied). After comparing the apportionment methods for three candidates using simplicial geometry, we evaluate how they differ in bias toward the top and bottom vote-getting candidates. We also compare the methods by how they distinguish among candidates in close elections. We use the bias comparisons to suggest which methods should be used at different junctures in the primary season. We discuss how these new methods were implemented in practice and summarize how successful these methods were in making the Republican delegate process more proportional.","The delegate selection process for the Republican Party Presidential Primary has changed greatly since the 2008 primary. In that primary season, delegate allocation in most of the state Republican contests were based on a “winner-take-all” model, allowing John McCain (the Republican nominee) to rapidly attain a sizable majority over his closest competitor, Mitt Romney. McCain secured a majority of the delegates by early March, resulting in a relatively short primary. In contrast, the 2008 Democratic Party Presidential Primary was notable for the protracted contest between the two top Democratic Party presidential candidates, Barack Obama and Hillary Clinton. In the Democratic primary, all states were required to use a proportional (rather than winner-take-all) method to allocate delegates and states holding primaries later in the season were rewarded with the “carrot” of bonus delegates (Putnam, 2012), resulting in a longer primary season. The short Republican primary in 2008 resulted in a lower level of popular and media interest in the Republican primary than in the longer Democratic primary.====To rectify this imbalance and prolong the 2012 contest, the Republican Party formed the Temporary Delegate Selection Committee to make recommendations that would ensure a larger number of voters an opportunity to weigh in on the future Republican Party presidential candidate (Jewitt, 2013, Putnam, 2012). These changes included adjustments to the primary calendar as well as stipulations about how delegates could be allocated to the candidates. The new changes required all states (with the exception of the first four states, Iowa, New Hampshire, South Carolina and Nevada) to hold contests in March or later, and further required those states holding elections before April 1 to use a method of proportional allocation for awarding their statewide delegates. States holding primaries after April 1 could continue to use whatever method they desired. Any state not abiding by these rules was penalized by losing 50% of their delegates.====The hope was that by adopting proportional allocation methods for the early primaries, a wider variety of candidates would receive delegates and thus remain viable longer. The requirements essentially left the decision for how to allocate delegates, traditionally a Republican Party prerogative, in local hands, with the threat of reduced delegates to encourage states to abide by the new guidelines.====These rule changes were only partially successful in changing the behavior of the state Republican Party contests (Jewitt, 2013, Putnam, 2012). Arizona, Michigan and Florida chose to move their state contests to late February and early March, forcing the four early states to move their elections even earlier, preferring to lose delegates than to lose influence and the opportunity to have state voters weigh in meaningfully on the presidential nominee (Jewitt, 2013). Michigan even continued to use a winner-take-all method, although the national Republican Party penalized them only once for this double infraction. The 2012 contest was drawn out with Mitt Romney clinching the nomination in early April, although the timing of this event may have been due to specifics of demography (Putnam, 2012).====One other result was, however, that an increasing number of state parties chose to adopt proportional methods for their statewide delegates. Unlike the Democratic Party which mandated a uniform system for proportional apportioning of delegates (the method known as Hamilton’s method in the U.S.), the Republican Temporary Delegate Selection Committee did not specify a particular method for attaining proportional allocation. Suggestions for “rounding rules” were provided, but states were allowed to devise their own rules. It is these rules – these apportionment methods – that we analyze in this paper.====The evolution of delegate selection rules in both parties has been the subject of extensive research with respect to its effect on elections (Ansolabehere and King, 1990, Putnam, 2012, Jewitt, 2013). Meinke et al. (2006) examined an empirical model to explain why parties might be motivated to adopt more open processes. However, there has been virtually no analysis of the mathematical structure of these rules. Geist et al. (2010) examined the effect of using Hamilton’s method in the Democratic Party Delegate Selection Rules, and Jones et al. (2019) looks at the effect of cutoffs on this selection process.====In countries with proportional representation, apportionment methods are often used to determine the number of seats assigned to each political party based on the number of votes each party receives. In the U.S., they determine the number of seats a state receives in the U.S. House of Representatives based on each state’s population. Because apportionment methods are used to determine each state’s representation, there has been an emphasis on using apportionment methods that advantage neither large nor small states; favoring states based on their population (large or small) has been referred to as ==== in the apportionment literature. The quantification of bias has been discussed in Balinski and Young (2001) and Pukelsheim (2017).====Apportionment methods used in delegate selection arguably have a different objective than methods used to determine representation in governmental bodies. For primaries, a favorable bias toward the vote leader, corresponding to a state with a large population, and inclusion of cutoffs help consolidate support for more popular candidates. A bias toward weaker candidates has the effect of widening participation by allowing more candidates to receive delegates. Thus, there is a tension between arriving at a nominee quickly so that the nominee can focus on the general election and allowing the party to test out platform positions on their voters.====In this paper we survey the apportionment methods used by the Republican Party state primaries in the 2012 and 2016 presidential primaries. To the best of our knowledge most of these methods are new (or, at least, previously unstudied) and so this article is in part a testament to the creativity of state Republican Party officials. The paper is structured as follows. In Section 2 we introduce the simplicial geometry used in Lucas, 1983, Bradberry, 1992 and Balinski and Young (2001) to visually compare apportionment methods. In Section 3 we formally define each of the seven proportional methods used in Republican primaries. In Section 4 we identify in which state(s) and in what year(s) each method was used. In Section 5 we discuss the bias of each method in favor of the leading candidate and against the lowest-ranked candidate. We argue that states holding early primaries should select methods that give more delegates to lower-ranked candidates in an effort to build consensus of the party platform, while states holding late primaries should select methods that consolidate support for the leading candidate(s). In Section 6 we discuss how these new methods are implemented in practice. We conclude with a brief summary and discussion of the success of these methods and their implementation within the state primaries in making the Republican delegate selection process more proportional and inclusive.",New quota-based apportionment methods: The allocation of delegates in the Republican Presidential Primary,https://www.sciencedirect.com/science/article/pii/S0165489620300469,6 May 2020,2020,Research Article,174.0
Gutiérrez-López Esther,"Departamento de Economí,a Aplicada IV, Universidad del País Vasco U.P.V./E.H.U., Spain","Received 25 October 2019, Revised 31 March 2020, Accepted 30 April 2020, Available online 6 May 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.04.005,Cited by (4),"We define and characterize the class of all ====, ==== and ====and a new axiom called ====-====; or replacing the ==== and ==== ====-====axioms by a new axiom called ====-====. Finally, we give two characterizations of the whole family of egalitarian solidarity values in such a way that the parameter ==== is obtained endogenously by the set of axioms.","One of the main issues in economic allocation problems is the trade-off between marginalism and egalitarianism. In this paper we consider this issue in the context of cooperative games with transferable utility. For these games, the ==== (Shapley, 1953) and the ==== are two well-known values. The basic principle behind the Shapley value is paying players according to their productivity. It can be expressed formally by the ==== axiom (Young, 1985), that is, if the marginal contributions of a player in two games are the same, then her values should be the same. Alternatively, it can be expressed by the ==== axiom, that is, if all the marginal contributions of a player in a game are zero, then the player should obtain zero. Thus, the Shapley value is the most marginalistic solution and it does not allow for any kind of solidarity among players. In contrast, the equal division solution distributes the worth of the grand coalition equally among all players, and it can be seen as too egalitarian. One way to provide solution concepts that are in between these two extremes is considering convex combinations of the Shapley value and the equal division solution. This class is suggested by Joosten (1996) and they are called ====. van den Brink et al. (2013) provide two different axiomatizations of this family using ==== and ==== properties. Casajus and Huettner (2013) also characterize this family using ====, ====, ==== and the ====. The latter axiom points out situations in which solidarity is possible by requiring that null players obtain non-negative payoff if the grand coalition has a non-negative worth.====Several attempts have been made to design solutions with a greater degree of solidarity among players. We focus here on the ====, introduced by Sprumont (1990). The Shapley value is based on the individual marginal contributions of a player to the coalitions she belongs to. In the solidarity value the individual marginal contribution is replaced by the average of the marginal contributions of all players that are in the coalition. Nowak and Radzik (1994) characterize this value axiomatically by means of the same axioms as the Shapley value but replacing the ==== axiom by the ==== axiom (A-null stands for average null). In this case, a player receives zero if the average of the marginal contributions is zero for all the coalitions she belongs to.====Kamijo and Kongo (2012) show that the Shapley value, the equal division solution and the solidarity value differ in just one axiom that specifies the type of player that can be removed from the game without affecting the remaining players’ payoffs. Casajus and Huettner (2014a) introduce and characterize a class of solidarity values in which the equal division solution and the Shapley value are extreme points and the solidarity value is the center. These values are distinguished by the type of player whose removal from a game does not affect the remaining players’ payoffs. They are called ====. Béal et al. (2017) and Yokote et al. (2018) define and characterize two new classes of solidarity values which contain all the previously mentioned values.====In this paper, we introduce a new class of values which combines solidaristic and egalitarian principles. We define and characterize the family of ====, which are convex combinations of the solidarity value and the equal division solution. Our first characterization (Theorem 3) employs the classical axioms of ====, ==== and ==== and a new axiom called ====-====. In our second axiomatization (Theorem 4) the ==== and the ====-==== are replaced by a new axiom called ====-====. In these two results the parameter ==== is given exogenously. We also provide a characterization of the whole family of egalitarian solidarity values (Theorem 8) in such a way that the parameter ==== is obtained endogenously by the set of axioms. These axioms are: ==== and ==== (or the ==== ). ==== is stronger than ==== and ==== is weaker than ====- ==== Moreover, ==== is similar to the ==== introduced in Casajus and Huettner (2013) to characterize the family of egalitarian Shapley values, and ==== is weaker than the ==== axiom used in Casajus and Huettner (2013). Finally, we give a second characterization of the family of egalitarian solidarity values (Theorem 9), replacing ==== and ==== in Theorem 8 by an axiom called ==== Thus, the axiom that distinguishes egalitarian solidarity values from egalitarian Shapley values is basically the ====: if the whole society is productive, the A-null players (instead of the null players) need not receive a negative payoff.====The paper is organized as follows. Section 2 is devoted to some preliminaries. Section 3 defines the family of egalitarian solidarity values and shows a recursive formula to calculate them. Finally, in Section 4, we provide the axiomatic characterizations and we relate our class to the previously mentioned classes of values.",Axiomatic characterizations of the egalitarian solidarity values,https://www.sciencedirect.com/science/article/pii/S0165489620300445,6 May 2020,2020,Research Article,175.0
"La Torre Davide,Malik Tufail,Marsiglio Simone","SKEMA Business School and Université Côte d’Azur, Sophia Antipolis Campus, 60 rue Dostoievski, CS30085, 06902 Sophia Antipolis Cedex, France,Merck & Co., Inc., 770 Sumneytown Pike, West Point, PA 19486, USA,University of Pisa, Department of Economics and Management, via Cosimo Ridolfi 10, 56124 Pisa, Italy","Received 2 July 2019, Revised 30 January 2020, Accepted 13 March 2020, Available online 25 April 2020, Version of Record 9 November 2020.",https://doi.org/10.1016/j.mathsocsci.2020.03.005,Cited by (16),We analyze the ,"Despite the recent improvements in hygiene, sanitation, vaccination, and access to treatment, infectious diseases are still one of the most prevalent causes of morbidity and mortality in both developed and developing countries (Lopez et al., 2006). While the severity and the transmission of infections are magnified by the problems associated with malnutrition and poverty in developing economies (Gavazzi et al., 2004), in industrialized countries they are amplified by the challenges related to population aging (Gavazzi and Krause, 2002). This explains why, other than the urgent need to support and encourage improvements in the biomedical sector, policymakers in both the developed and the developing world are currently faced by the pressing need to understand how to use public policy to reduce the social costs of epidemic diseases. In particular, intervention programs may be broadly classified as either treatment or prevention, thus assessing how such different programs may work and which may be most effective in specific situations is a critical priority in the policymaking process. Following the seminal papers by Sanders (1971), Sethi (1974) and Sethi and Staats (1978), several works in the mathematical epidemiology literature have analyzed these issues from different points of view (good surveys include Diekmann and Heesterbeek, 2000, Hethcote, 2000, Hethcote, 2008). The relevance of the topic from an economic point of view has recently given birth to a growing economic epidemiology literature. Several works have tried to integrate the mathematical approaches in an economic framework, by analyzing either the social planner’s welfare maximization or cost minimization problem; however, in all such attempts the economic setup proves to be overly simplified to preserve tractability, often neglecting the macroeconomic consequences of infectious diseases (for recent surveys see Gersovitz and Hammer, 2003, Klein et al., 2007, Philipson, 2000). Our goal in this paper is exactly to fill this gap by developing a stylized but fully fledged macroeconomic–epidemiological model allowing to quantify and evaluate the effectiveness of prevention and treatment strategies.====We analyze a susceptible–infected–susceptible (or SIS) model (Kermack and McKendrick, 1927), in which individuals can be either infected or susceptible, but never become immune. Since in this framework individuals become susceptible again upon recovery, the disease tends to persist, suggesting thus that the model is more suitable to describe an endemic rather than an epidemic disease (Hethcote, 2008). It can thus characterize the diffusion of some bacterial agent diseases, like meningitis, plague and sexually transmitted diseases, and some protozoan agent diseases, like malaria and sleeping sickness (Hethcote, 2008), which are likely to affect human populations in both developed and developing countries. While the SIS framework is well known in the mathematical epidemiology literature, in the economics literature only a few attempts have been made to develop a framework able to integrate the SIS setup in an economic setting. Most of the papers rely on a microeconomic perspective assuming that the budget or wealth available to policymakers in order to contrast the spread of the disease is completely exogenous (Anderson et al., 2010, Gersovitz and Hammer, 2004, Goldman and Lightwood, 2002, Rowthorn and Toxvard, 2012). Even if allowing to clearly evaluate the impact of different policy measures on the disease dynamics, this approach does not permit to take into account how the prevalence of the disease may affect the amount of resource available at macroeconomic level. In our setting the spread of the disease reduces the size of the healthy labor force (Goldman and Bao, 2004, Papageorge, 2016), reducing in turn the income available in the economy to finance policy interventions. The social planner thus needs to optimally determine the income tax rate in order to reduce the spread of the disease, taking into account that the degree of infection endogenously determines the tax revenue which is possible to collect for any given tax rate. Such an endogeneity of the public budget constraint allows us to account for the mutual implications between macroeconomic conditions and health expenditures, consistently with recent empirical evidence showing their positive association (Velenyi and Smitz, 2014). The problem is therefore not trivial at all, but we show that it is possible to characterize, even analytically, its optimal solution. Our results allow to assess from a macroeconomic point of view the effectiveness of alternative policies, namely prevention and treatment, clearly identifying which policy is most desirable in different circumstances.====Only few papers analyze the implications of infectious diseases on macroeconomic outcomes (Goenka and Liu, 2012, Goenka and Liu, 2019: Goenka et al., 2014), but none of them discuss the role of public policy in reducing the prevalence of the disease in a way comparable to ours. Goenka and Liu, 2012, Goenka and Liu, 2019 study the implications of disease prevalence on economic activities in neoclassical and endogenous growth models respectively, showing how such health-economy feedback effects may give rise to nontrivial dynamics, like cycles, chaos, multiple balanced growth paths and poverty traps. Probably the paper most closely related to ours is Goenka et al.’s (2014), who focus on an economic growth model in which an infectious disease decreases output by reducing the availability of the labor force, justifying the allocation of some resources to health expenditure in order to reduce the spread of the disease by improving health capital. In their setting a social planner optimally determines how to allocate the available resources between physical capital investment, consumption and health expenditure, and therefore the budget available to finance health policy is endogenous. Public policy indirectly (through health capital accumulation) affects the disease dynamics by simultaneously increasing the speed of recovery and decreasing the rate of infection. Similar to them, in our macroeconomic framework the disease prevalence lowers economic production by reducing the size of the labor force employed in productive activities, reducing thus the availability of resources to fight the spread of the disease. Different from them, in our setting public policy directly affects the disease dynamics by either increasing the speed of recovery (via treatment) or decreasing the rate of infection (via prevention), making thus possible to understand how different forms of policy interventions may affect economic and health outcomes both in the short and long run. This also allows us to assess the effectiveness of the two main forms of health policy measures (i.e., prevention and treatment) under different circumstances showing that whether prevention or treatment is most desirable crucially depends on the magnitude of the infectivity rate, providing thus neat policy recommendations.====Our approach in this paper consists of extending a basic mathematical epidemiology model to consider the implications of optimal health policy in a stylized but fully fledged macroeconomic framework. To a large extent it departs from some of the assumptions traditionally introduced in economic models, and in particular from those related to rational behavior and the private sector’s behavior. We follow the mathematical epidemiology literature to model the diffusion of the disease, and in particular in assuming that the probability of transmission (conditional on exposure to an infected individual) is constant; the economic epidemiology literature often relaxes this assumption by claiming that rational individuals may modify their actions by implementing different forms of protective behavior (such as the choice of safer sex partners in the case of sexually transmitted diseases), which are likely to reduce the probability of transmission as the disease incidence increases (Philipson, 2000). Such a rational epidemics approach is well suited to characterize specific diseases like HIV, for which individuals may react to the change in prevalence by avoiding potential infectious contacts; however, it is less suited to characterize many bacterial or viral diseases, in which infected individuals become infectious before the symptoms become visible, which largely reduces the avoidance motive (Adda, 2016). We also follow the mathematical epidemiology literature in assuming that the health intervention measures are entirely provided by the public sector, such that there is no need to model the eventual interaction between public and private actions; the economic epidemiology literature discusses the possibility that publicly provided measures become completely ineffective because of responses induced in the provision of private programs (Philipson, 2000). However, our approach departs also to some extent from the traditional mathematical epidemiology literature in the definition of the objective function. While most of the papers in the field consider a basic instantaneous loss function which depends only on the direct (financial) costs of the implemented policy measure, by following the economic epidemiology literature our instantaneous loss function reflects social costs depending on both the direct cost associated with the policy instruments and the indirect cost associated with the spread of the infection (Philipson, 2000). Such an approach, borrowing from both the mathematical epidemiology and economic epidemiology literatures, allows us to analyze in the simplest possible framework to what extent publicly provided prevention and treatment measures differ in determining the short and long run effects of infections diseases, along with their implications on the social costs of epidemics and about the desirability of alternative eradication policies.====The paper proceeds as follows. Section 2 recalls the traditional SIS epidemiology model briefly outlining the role of prevention and treatment in our extended optimal control model. Section 3 introduces our macroeconomic model, where the social planner tries to minimize the social costs of the endemic disease by maintaining a balanced budget at any point in time. We then separately analyze the implications of optimal prevention and optimal treatment comparing their effects on the dynamics and equilibrium outcomes of susceptibles and infecteds. While in Section 4 we focus on prevention which tends to directly reduce the degree of disease incidence, in Section 5 we focus on treatment which instead tends to directly increase the speed of recovery from the disease. We show that both the policies can effectively be used to achieve complete eradication of the disease in the long run equilibrium, however they substantially differ in their associated short run economic costs. In order to account for this, we compare the social costs associated with prevention and treatment in Section 6, where we quantitatively assess their relative effectiveness identifying which policy may be more cost-effective under different circumstances. We show that prevention is most desirable whenever the infectivity rate is low, while treatment becomes the best option when the infectivity rate is high. Finally, Section 7 proposes concluding remarks and directions for future research.",Optimal control of prevention and treatment in a basic macroeconomic–epidemiological model,https://www.sciencedirect.com/science/article/pii/S0165489620300433,25 April 2020,2020,Research Article,176.0
"Crettez Bertrand,Nessah Rabia","Université Panthéon-Assas, Paris II, CRED, EA 7321, 21 Rue Valette, 75005 Paris, France,IESEG School of Management-LEM-CNRS, 3 rue de la Digue, 59000 Lille, France","Received 3 January 2020, Revised 17 April 2020, Accepted 17 April 2020, Available online 23 April 2020, Version of Record 7 May 2020.",https://doi.org/10.1016/j.mathsocsci.2020.04.004,Cited by (2),We provide existence and characterization results for unilateral support equilibrium in ====-player games. A unilateral support equilibrium is a strategy profile such that the teammates of each agent ====-core. We also compare unilateral support equilibrium relative to a coalition structure with the ====-absolute optimal solution.,"Ordinary market transactions do not always rely on self-interest alone, but are often carried out with goodwill on both sides (Sugden, 2018). There is also evidence from experimental games (as in the Market Game or the Trust Game) that cooperative behavior occurs quite frequently (see, ====, Colman et al. (2008)).====To account for these observations one may assume that agents are others-regarding.==== ==== The notion of Berge equilibrium is a major solution concept for analyzing the interactions of such agents (Berge, 1957, pp 88–89, Zhukovskii, 1985, Courtois et al., 2015 and Larbani and Zhukovskii, 2017). A Berge equilibrium is a strategy profile such that the teammates of each agent choose their strategies in order to maximize his utility. This notion of equilibrium has been used to study ==== the forwarding dilemma (Gao and Tembine, 2017), the graph model for conflict resolution (Italino and Chaves, 2019), or climate change games (Courtois et al., 2017).==== ====This notion, however, can be criticized on the grounds that it involves unlikely self-sacrifice from the agents, and unrealistically strong cooperation. That is because, in a Berge equilibrium, an agent may receive less than his maximin payoff. Moreover, for each individual, ==== the other ones ==== to support him. The notion of Berge–Vaisman equilibrium addresses the first criticism since it requires that each agent receives at least his maximin payoff (see Zhukovskii and Chikrii (1994)). The notion of Berge–Nash equilibrium, ====, a strategy profile that is both a Berge and a Nash equilibrium, addresses both criticisms (see Larbani and Nessah (2008)).==== ==== Indeed, in a Nash equilibrium there is no need to cooperate and any player receives a payoff no lower than his maximin.====A third possible criticism of Berge refers to the fact that, whenever agents receive some support, it is not necessarily ==== supplied, as opposed to what Berge equilibrium assumes (one may help some individuals, but one does not help everybody). To address this criticism, it is useful to refer to the notion of coalitional Berge equilibrium (see Courtois et al. (2017)). In such an equilibrium, certain groups of agents cooperate to help other (possibly different) groups of agents. What is disputable, however, is the fact that people ==== coordinate their actions to help others.====The purpose of the present paper is to advance the study of unilateral support equilibrium, an alternative solution concept for other-regarding agents proposed by Schouten et al. (2018), wherein coordination is not assumed. A unilateral support equilibrium is a strategy profile such that the teammates of each agent ==== choose their strategies to maximize his payoff.====While Nessah and Moussa (2014) as well as Courtois et al. (2017) provide general existence and characterization results for Berge equilibrium, to the best of our knowledge there are no comparable findings for unilateral support equilibrium. The present paper establishes existence and characterizes such an equilibrium.==== ==== It also addresses the existence and characterization of unilateral support equilibrium relative to a coalitional structure. That new notion embodies two ideas. Firstly, mutual support actually exists locally. That is, agents tend to gather in different groups, where mutual support mainly benefits the group members, and the larger the group size, the more likely the absence of coordination among the group members. Secondly, not all actors take part in mutual support. Those actors, therefore, tend to behave in a non-cooperative way. To sum up, a unilateral support equilibrium relative to a coalitional structure is a strategy profile such that some agents follow a Nash behavior, whereas the other ones play strategies akin to unilateral support equilibria.====We also advance the comparison between Berge equilibrium, strong Berge equilibrium and unilateral support equilibrium.==== ==== Here we show that a strong Berge equilibrium is a unilateral support equilibrium, and we provide an example of a unilateral support equilibrium that is neither a Berge equilibrium nor a strong Berge equilibrium. In this sense, one can say that a unilateral support equilibrium is a weakening of both Berge and strong Berge equilibria.====Furthermore, we relate the notion of unilateral support equilibrium to strong Nash equilibrium and the ====-core (Aumann, 1959).==== ==== We study a simple example showing that a unilateral support equilibrium may neither be a strong Nash equilibrium nor belong to the ====-core. Finally, we compare the ====-absolute optimal solution considered in Nessah and Tazdait (2012) with unilateral support equilibrium relative to a coalition structure.==== ==== We show that any ====-absolute optimal solution is a unilateral support equilibrium relative to a coalition.====The paper unfolds as follows. Section 2, we first set up an ====-player game, formally state some definitions and compare unilateral support equilibrium to different notions of equilibrium. In Section 3 we establish a sufficient condition under which a unilateral support equilibrium exists, and we give a characterization of that equilibrium. We tackle the existence of unilateral support equilibrium relative to a coalition in Section 4. Section 5 briefly concludes the paper.",On the existence of unilateral support equilibrium,https://www.sciencedirect.com/science/article/pii/S0165489620300421,23 April 2020,2020,Research Article,177.0
"Yang Guangjing,Sun Hao,Hou Dongshuang","Department of Applied Mathematics, Northwestern Polytechnical University, No.127, Youyixilu, Xi’an, China","Received 21 August 2019, Revised 17 February 2020, Accepted 17 April 2020, Available online 23 April 2020, Version of Record 7 May 2020.",https://doi.org/10.1016/j.mathsocsci.2020.04.003,Cited by (1),"In this paper, we present a noncooperative sequential ==== with transferable utility. Instead of assuming exogenous protocol or random-proposer recognition, we provide a mechanism in which the protocol is generated endogenously. Besides, a partial breakdown ==== is considered rather than a discount factor when the proposal is rejected by some player. We show that for each partial breakdown ====, stationary subgame perfect equilibria exist if the characteristic function game is totally balanced. Moreover, the outcomes of stationary subgame perfect equilibria in our model coincide with the core allocations.","The aim of this paper is to explore a noncooperative method to model ====-person coalitional bargaining game with transferable utility and discover the approach to no-delay and efficient agreements as well as fair payoff allocations.====Since the publication of the seminal paper by Rubinstein (1982), coalitional bargaining has been a very active research direction in game theory. Chatterjee et al. (1993) extend the two-person alternating-offer model of Rubinstein (1982) to the multi-person situation, where an exogenous protocol, which is a fixed order over players, plays an essential role in determining the outcomes. Hereafter, from different perspectives, the spirit of the protocol is widely used by a number of studies including Bloch (1996), Ray and Vohra (1999) and Kóczy (2009), but also invites arguments such as being too specific on the selection of the first proposer. Focusing on this shortage, Okada (1996) proposes a stationary bargaining game in which the proposer is randomly recognized with equal probability from all players, and this randomization mechanism is further generalized in Okada (2011). Other contributions that involve the design of random-proposer can be found in Yan, 2003, Gomes, 2005, Hyndman and Ray, 2007 and Okada (2010). In spite of a good attempt for avoiding being partial to some players in the bargaining, randomization mechanism has the same defect as the fixed order protocol: the recognition probabilities of proposers are exogenous, which means that they are artificially fixed before the game getting started. Then, this arises a natural question which has already been mentioned in Okada (1996): “how is the protocol actually determined?” Or in other words, is there some mechanism that determines proposers endogenously?====With such a question in mind, the paper puts forward a method for the selections of proposers being endogenously determined by the strategies of players. In our model, at the first stage of each period, every active player simultaneously proposes a feasible payoff vector and a permutation of players, if all active players choose the same payoff vector, the game goes to the next stage and the combination of permutations of active players becomes the current protocol. At the second stage, the bargaining process continues the same procedure as in Chatterjee et al. (1993): the first player in the active player set under the current protocol becomes the proposer. Then he makes a proposal containing a coalition (which contains the proposer himself) and a feasible payoff vector for the coalition. All other players in the coalition respond sequentially according to the current protocol. If they all accept the proposal, then they leave the game with the allocation which has been consensus within the coalition. The remaining players become active and continue the negotiations at the next period.====Another main distinguishing feature of our game model appears in the situation when the proposal is rejected by some member of the coalition. Instead of appointing the first rejector as the new proposer with certainty, we introduce the concept of partial breakdown probability (PBP) for distinguishing two events that may happen. The first rejector may become the new proposer with a certain probability. Whereas in another case, the coalition could also be swept out of the game with the allocation determined in the first stage and the remaining players continue the negotiations at the next period. It is worth mentioning that our model has no restriction about discounting, which is a general assumption in the literature and prescribes that players discount their future payoffs by a common discount factor. Instead of imposing this strong assumption, which compulsively reduces the payoff of players if there are delays in the negotiation process, in our model, we let the rejector face the risk of getting the payoff determined in the first stage. Our main result shows that for each PBP, stationary subgame perfect equilibrium payoff set coincides with the core if the characteristic function game is totally balanced.====We briefly mention that our paper is not the first one to attempt to endogenize the bargaining protocol. For example, the mechanism proposed by Pérez-Castrillo and Wettstein (2001) for implementing the Shapley value includes a round of biddings to select the proposer. Evans (1997) and Yildirim (2007) allow players to make investments at a cost to become the proposer. Serrano and Vohra (1997) and Chang and Hu (2017) use the same proposer-selecting process as we do in the first stage of our model. Among those literature, Serrano and Vohra (1997) perhaps is the most relevant to our study, since they also analyze the mechanism, which involves the endogenous protocol and implements the core. However, we will compare our model and results with theirs in Section 5 and show that in some sense, their bargaining game can be regarded as a special case of ours when we restrict the PBP to an extreme condition.====The paper is organized as follows. Section 2 is devoted to some basic definitions and notations. In Section 3, we introduce our noncooperative bargaining model. Section 4 presents our main results. In Section 5, some further discussions are involved.",A noncooperative bargaining game with endogenous protocol and partial breakdown,https://www.sciencedirect.com/science/article/pii/S016548962030041X,23 April 2020,2020,Research Article,178.0
"Calleja Pedro,Llerena Francesc","Departament de Matemàtica Econòmica, Financera i Actuarial, Universitat de Barcelona-BEAT, Av. Diagonal, 690, 08034 Barcelona, Spain,Departament de Gestió d’Empreses, Universitat Rovira i Virgili-CREIP, Av. de la Universitat, 1, 43204 Reus, Spain","Received 3 April 2019, Revised 28 November 2019, Accepted 9 April 2020, Available online 18 April 2020, Version of Record 7 May 2020.",https://doi.org/10.1016/j.mathsocsci.2020.04.001,Cited by (7),The ,"Probably the most relevant single-valued solution for cooperative games with transferable utility (games, hereafter) is the ==== (Shapley, 1953). Many characterizations of this solution, including his original axiomatic approach, use the principle that if a player contributes zero to all coalitions, then she must receive a zero payoff: the ====. Various authors have proposed alternative foundations of the Shapley value imposing the ====. Particularly, van den Brink (2001) interprets the Shapley value as the unique solution satisfying, additionally, ====, imposing that all the gains from cooperation are distributed among the players, and ====, a property inspired by Myerson’s (1977) fairness. For single-valued solutions, ==== essentially imposes that if a game suffers an impact consisting in adding another game in which two players are symmetric, then their payoffs should change by the same amount. If we measure the relevance of a player in terms of her marginal contributions to all coalitions, ==== is a quite natural requirement since adding such a game does not change the contributions of symmetric players.====In this paper, we study what solutions emerge when weakening ==== into ==== (van den Brink et al., 2016),==== ==== combined again with ==== and either the ==== or the ====, which states that if a player contributes only her individual worth to all coalitions then she must receive her individual worth. ====, a property very much related to ==== (Arin, 2013), can be viewed as a solidarity axiom in the sense that if only the worth of the grand coalition varies, while the worth of all other coalitions remain unchanged, then players’ payoffs should be affected equally.==== ====Another different principle used from Hart and Mas-Colell (1989) to interpret the Shapley value is ====. Consistency is an outstanding relational property widely used in the axiomatic analysis of solutions imposing that an original agreement should be reconfirmed in the underlying reduced game when some agents leave.==== ====
 Calleja and Llerena (2019) impose ==== and ==== together with ====, a classical invariance property with respect to changes in scale and origin, to characterize the Shapley value. In this work, we impose ====, that is, ==== when only one or two agents stay, to select the Shapley value from the set of solutions satisfying ====, ==== and the ====.==== ==== This characterization has the flavour of Hart and Mas-Colell (1989), van den Brink (2001) and Calleja and Llerena (2019) but it uses substantially weaker versions of ==== and ====.====The remainder of this paper is organized as follows. In Section 2, we introduce some preliminaries. In Section 3, we characterize the family of single-valued solutions satisfying ====, ==== and either the ==== or the ====. Remarkably, these characterizations can be extended to any domain of games. In Section 4, we provide a new axiomatization of the Shapley value by means of ====, ==== and the ==== These properties still characterize the Shapley value on the domain of convex games. Interestingly, we show that incompatibilities emerge when replacing self consistency by a huge class of consistency properties that includes, as particular cases, ==== (Davis and Maschler, 1965), ==== (Moulin, 1985) and ==== (Funaki, 1998).","Consistency, weak fairness, and the Shapley value",https://www.sciencedirect.com/science/article/pii/S0165489620300391,18 April 2020,2020,Research Article,179.0
"Cornand Camille,Dos Santos Ferreira Rodolphe","Univ Lyon, CNRS, GATE L-SE UMR 5824, F-69130 Ecully, France,BETA-Strasbourg University, France,Católica Lisbon School of Business and Economics, Portugal","Received 18 July 2019, Revised 22 January 2020, Accepted 4 February 2020, Available online 8 April 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.02.003,Cited by (3),.,"One of the main features of oligopoly theory, already present in the seminal contribution of Cournot (1838, ch.VII), is the tension between the cooperative and the competitive attitudes potentially adopted by the oligopolistic firms. Full cooperation leads to the collusive outcome, which maximizes joint profits, but collusion is generally not enforceable as a Nash equilibrium, since any firm can expect to obtain a larger profit by engaging alone in competition with the other firms. Of course, as the incentive to compete is shared by those other firms, everyone ends up in a worst position, possibly very bad indeed when competition is harsh. There are different ways of preventing competition from becoming too harsh, leaving some place to cooperation. Here, we will consider ====: firm owners, assumed to exercise sufficient control over their managers’ conduct, commit them to an objective that mitigates competition (Vickers, 1985, Fershtman and Judd, 1987, Sklivas, 1987). More precisely, we will adopt Miller and Pazgal (2001) two-stage differentiated duopoly game where firm managers maximize at the second stage a weighted sum of the profits of the two firms, with weights chosen at the first stage by the owners, the relative weight put on the rival’s profit being seen as the degree of cooperation.==== ==== The aim of Miller and Pazgal paper is to show that the second stage regime of competition, whether in prices, quantities or a mix of the two, is immaterial for the subgame perfect equilibrium outcome. Our aim is different: while simply assuming price competition at the second stage, we want to investigate the consequences of information heterogeneity.====Heterogeneous information adds a coordination problem to the cooperation versus competition trade-off. Private information induces price dispersion, and price dispersion squeezes aggregate profits because of concavity of the payoff functions. This squeeze is the more severe the less differentiated the products, undifferentiation conferring a prominent – eventually all-important – place to coordination, whatever the balance of the cooperative and competitive attitudes. This situation fits nicely into the metaphor proposed by Keynes (1936, ch.12) to characterize the working of well organized financial markets, that of the ==== in which the winner is the contestant who expresses the opinion closest to the average, whatever the average opinion may be. Because of the high liquidity of financial assets, investors are not exclusively interested in the expected yield of those assets over their whole life, determining their fundamental value, but also in the expected capital gains (or losses) obtained in case of early liquidation, which depend upon the variations of their conventional value to be set by the market. When the latter kind of expectations predominates, investors turn into speculators and end up disregarding fundamentals and striving to anticipate “what average opinion expects the average opinion to be” (Keynes, 1936 p.156).====So, agents make investment choices by referring to their expectations of the fundamental and the conventional values. In doing so, they respond to ==== and ====, respectively. If information is perfect, these motives are compatible, all the agents simply coordinating on the fundamental value. A conflict between the two motives will however emerge as soon as information is imperfect (blurring the fundamental) and, more importantly, dispersed (obstructing coordination). Matching simultaneously the fundamental and the conventional values can then only be the result of a trade-off. In their seminal abstract beauty contest game, Morris and Shin (2002) have built this trade-off in the representative agent’s loss function, a convex combination of two distances of the agent’s action, from the fundamental and from the average. In this game, public information may be overvalued with respect to more precise private information because of its coordinating role, and of course the more so the higher the relative weight put on the coordination motive.====A further point that should be kept in mind is that Morris and Shin consider a contest, in which agents are willing to meet the market, but wish to beat it at the same time: “an investor gains from predicting the average opinion ====” (Cornand and Heinemann, 2008 p.733). Hence, a third motive – the ==== – is also present, although discreetly, opening the way to an easy transfer of the beauty contest game from its original use in the analysis of financial markets to industrial organization applications (Angeletos and Pavan, 2007, Myatt and Wallace, 2012). In such applications, with a continuum of agents, the competition motive is however kept latent: since agents’ actions have only an insignificant influence on the market, the market advantage occurs as an externality rather than as the consequence of strategic decisions. Myatt and Wallace, 2015, Myatt and Wallace, 2018 address oligopolistic competition proper, with a “small group” of producers, but merge in some sense the fundamental and the competition motives by taking as fundamental targets for the producers the Cournot quantity and the Bertrand price, respectively, which already integrate the intensity of competition. By referring instead to the corresponding collusive strategies as the fundamental targets, Cornand and Dos Santos Ferreira (2019) disentangle the two motives and make apparent the full interplay of the fundamental, coordination and competition motives.==== ====We shall use the same device in this paper, but we want principally to introduce an important innovation. In beauty contest games, the relative weights on the different motives have up to now been taken as part of the model structure, no space being left for the agents to manage the conflicts between them.==== ==== This is still true in the present case but, by adopting the strategic delegation approach, we shall admit that firm owners, acting as principals at a preliminary stage, do have some latitude in dealing with those conflicts through the manipulation of the weight put on the competition motive.====To be explicit, price setting by firm managers at the second stage is performed on the basis of noisy information on the fundamental random value – market size – the realization of which is unknown. This information is made up of public ex ante information and private ex post information, the quality of the latter being virtually asymmetric (which allows to distinguish the effects on a firm’s behavior of changes in the precisions of each one of the two private signals). Given demand linearity and Gaussian distributions, we obtain a quadratic-payoff coordination game, which can be viewed as a standard beauty contest, with loss functions having weights partly determined by the degrees of cooperation chosen by firm owners at the first stage. Higher degrees of cooperation induce equilibrium prices closer to their collusive value – the fundamental – but also higher concerns for coordination, giving more weight to public relative to private information. The subgame-perfect equilibrium degree of cooperation of each firm increases with the relative quality of its own private information and with the structural intensity of competition (as the two products become less differentiated). In the limit case of the homogeneous duopoly, the degree of cooperation reaches for each firm its maximum, collusive, value, but the concern for coordination reaches its maximum value too, making public information all important and possibly disconnecting equilibrium prices from ex post information on fundamentals.====In this paper, we endogenize the weight put on the competition motive, but leave information entirely exogenous. Two natural extensions of the present work are consequently the analysis of deliberate costly ==== and that of strategic private ====. The former is the object of Myatt and Wallace, 2015, Myatt and Wallace, 2018 in the context of Cournot and Bertrand competition, respectively. The extent and kind of information acquisition depends upon the structural intensity of competition (resulting from the degree of product differentiation and the industry concentration) but also upon conduct, underlying the opposition between Cournot and Bertrand. Making conduct result from a continuous trade-off between cooperation and competition, settled under strategic delegation, breaks with the arbitrariness of simply selecting ==== Cournot or Bertrand, and may be quite enlightening, in particular in a context where there is reversal of comparative statics results when switching regimes of competition (see Myatt and Wallace, 2018 s.3). The same remark applies to the literature on private information sharing, where the opposition between Cournot and Bertrand competition appears again crucial (Novshek and Sonnenschein, 1982, Clarke, 1983, Gal-Or, 1985, Li, 1985, Vives, 1988) with Cournot homogeneous oligopoly, (Vives, 1984) and Sakai 1986 with both Cournot and Bertrand differentiated oligopolies, (Raith, 1996 with a general model).==== ==== More recently, however, the question has been reexamined in a Cournot homogeneous duopoly with delegation and private information on random costs (Theilen, 2007) and also under supply function competition, which is another way to go beyond the opposition between Cournot and Bertrand (Vives, 2011, Vives, 2017).====Two further possible extensions of another kind concern welfare issues==== ==== and abandoning the normal specification for signals,==== ==== both having been left for conciseness out of the scope of the present paper.====The remaining of the paper is structured as follows. Section 2 presents the two-stage duopoly delegation game under certainty and perfect information. The full information benchmark allows us to identify the three motives of the beauty contest and to put light on the benefit from cooperation (maximized at the full information optimal degree of cooperation). Section 3 derives the subgame perfect equilibrium under imperfect and dispersed information. We analyze how changes in the quality of public and private information, as well as changes in the structural intensity of competition, affect the competitors’ payoffs and ultimately the extent of their cooperation. Section 4 concludes.",Cooperation in a differentiated duopoly when information is dispersed: A beauty contest game with endogenous concern for coordination,https://www.sciencedirect.com/science/article/pii/S0165489620300305,8 April 2020,2020,Research Article,180.0
"Diss Mostapha,Mahajne Muhammad","CRESE EA3190, Univ. Bourgogne Franche-Comté, F-25000 Besançon, France,Univ Lyon, UJM Saint-Etienne, CNRS, GATE L-SE UMR 5824, F-42023 Saint-Etienne, France","Received 17 June 2019, Revised 17 January 2020, Accepted 31 March 2020, Available online 7 April 2020, Version of Record 15 April 2020.",https://doi.org/10.1016/j.mathsocsci.2020.03.004,Cited by (2),"We define and examine the concept of social acceptability of committees in multi-winner elections context. We say that a committee is socially acceptable if each member in this committee is socially acceptable, i.e., the number of voters who rank her in their top half of the candidates is at least as large as the number of voters who rank her in the least preferred half, otherwise she is unacceptable. We focus on the social acceptability of (","In multi-winner elections, the goal is to select a subset of candidates (i.e., a committee) of a pre-given size, such as electing parliaments, shortlisting job candidates or choosing public locations for a set of facilities, such as hospitals or fire stations (Faliszewski et al., 2017). In this paper we generalize and examine the concept of social acceptability of candidates, which has been introduced by Mahajne and Volij (2018a) for single-winner elections context,==== ==== to multi-winner election setting.====Consider a set of candidates. We say that a voter places a given candidate ==== if she prefers her to at least half of the candidates, and she places her ==== if at least half of the candidates are preferred to her.==== ==== We say that a candidate is ==== with respect to a given preference profile, if she is placed above the line by at least as many voters as those who place her below the line. One might say that social acceptability evaluates the candidates taking part in the election by considering that every candidate has a simple scale of 3 “palms”: left, right, and middle. Each voter places herself on one of the three palms: the leftist if she ranks the considered candidate below the line, the rightist if above the line and the middle if on the line. According to the social acceptability concept, the situation in which the left palm is heavier than the right one is not socially acceptable. Failing to be socially acceptable may be a significant weakness for a candidate, because this means that more voters place this candidate in their least favorite “half” of the candidates than in their most favorite “half” and thus a majority of voters may be uncomfortable with such a candidate.====Social acceptability of committees is defined in this paper as follows. Given a preference profile, we say that a committee is ==== if all of its members are socially acceptable, and it is ==== if all of its members are socially unacceptable and it is ==== ==== if some of its members are unacceptable. The selection of completely unacceptable committee or even a partly unacceptable committee with significant number of socially unacceptable members can be regarded as a severe​ drawback since a majority of voters may be uncomfortable with respect to every committee member or to a significant number of committee members and thus with respect to the whole committee. In other words, social unacceptability may cause dissatisfaction in some sense and a majority of voters may feel disappointed or skeptical regarding some or all of its elected members.====The concept of social acceptability of committees can be relevant in many situations. For instance, when choosing a team of athletes based on skills by some experts who may be asked to rank a list of good athletes, the team manager would like each team member to pass a certain threshold and be good enough in a relative sense, and would delay or disqualify a socially unacceptable candidate, where a majority of experts ranked her in the bottom half of the candidates. In such a situation, the requirement is that every team member would be relatively good in the sense of social acceptability. Another example, in which social acceptability can be relevant, is the selection of new faculty members for a university department. In a departmental appointments committee, each member may rank all the faculty candidates based on their resume, and the head may eliminate candidates who have not passed a relative threshold: the number of the appointments committee members who rank them below the line must not be more than the number of members who rank them above the line, i.e., the selected candidates at this step have to be socially acceptable. In a second stage, the appointments committee may make the final decision by personal interviews with the remaining candidates. In doing so, it is worth mentioning that the department head aims to reduce the importance of accurate rankings. In other words, social acceptability weakens the importance of the ranking, and thus the majoritarian, so a candidate may be socially acceptable although she is not ranked in the top places by a majority of voters. Using this argument, one might say that social acceptability works in some similar spirit as proportional representation,==== ==== in which a candidate can be selected although she is not ranked in top places by a majority of voters. However, we should argue that the two concepts work in a different way and may result in significantly different outcomes in terms of the committee memberships.====This paper examines the social acceptability of Condorcet committees. There are several definitions of ==== as possible generalizations of the well-known concept of Condorcet winner. One of them, which we consider in this paper, is due to Gehrlein (1985). A Condorcet committee ==== Gehrlein is a committee such that every one of its members beats every non-member by a majority. As noticed by Aziz et al. (2017), we may want the selected committee to be a collection of high-quality individuals who do not necessarily need to cooperate with each other. It may be the case, for instance, when we are shortlisting a group of people for an academic position or for some prize. In this case, it is understandable that each member of the selected committee should be preferred by a majority of voters to all the non-members. In other words, the objective should be to select a Condorcet committee ==== Gehrlein. Condorcet committees have clear advantages and stability is the most important concern among them. Indeed, a committee that is not a Condorcet committee is unstable in the sense that a majority of voters would want to replace one or more of its members. Notice finally that the concept of Condorcet committees has received great attention in the recent literature of social choice theory. The reader may refer for instance to the works of Aziz et al. (2017), Coelho (2004), Elkind et al. (2015), Kamwa (2017), Barberà and Coelho (2008), Diss and Doghmi (2016), Diss et al. (2020), Fishburn, 1981a, Fishburn, 1981b, Gehrlein (1985), Kaymak and Sanver (2003) and Ratliff (2003), among others. Many of these papers proposed and analyzed a number of committee selection rules that satisfy the Gehrlein extension of Condorcet principle, i.e., elect the Condorcet committee ==== Gehrlein, whenever it exists.====We show that the Condorcet committee and social acceptability are two notions which may be difficult to conjugate. For instance, a Condorcet committee may be partly and even completely unacceptable under certain conditions. The incompatibility between the Condorcet principle and the social acceptability is maintained even if we require that every member of the Condorcet committee is preferred to each non-member with a given ====
 (==== 1) fraction of the number of voters (====-Condorcet committee). In the second part of the paper, we study the compatibility of the two notions under some restrictions on the individual preferences. We show, on the one hand, that if the preferences of the voters are single-peaked or single-caved and the committee size does not exceed a certain number, then a Condorcet committee must be socially acceptable, and if the preferences are single-crossing or group-separable, then it may be socially acceptable but may not. On the other hand, single-crossingness, which can guarantee Condorcet winner to be socially acceptable (Mahajne and Volij, 2018b), cannot guarantee Condorcet committee (of size 2 or more) to be socially acceptable.====We also evaluate the probability of a Condorcet committee to be socially (un)acceptable under the assumption of Impartial Anonymous Culture (IAC). This assumption, introduced by Gehrlein and Fishburn (1976), is a commonly used hypothesis in the literature of social choice theory when computing the theoretical likelihood of electoral events, and stipulates that all voting situations (defined later) are equally likely to be observed. Our results show that in general, under IAC, Condorcet committees are exposed to social unacceptability to a significant extent. For instance, when there are six candidates, then about 80 percent of the voting situations that lead to a Condorcet committee of size four are expected to be partly or completely unacceptable.====The paper is organized as follows. Section 2 lays out the basic definitions. Section 3 states and proves theoretical results regarding social acceptability of Condorcet committees. Section 4 presents the results regarding the probability evaluations of having a socially (un)acceptable Condorcet committee under IAC assumption, and Section 5 concludes. The proofs of our results are presented in Appendix. However, due to lack of space, some proofs are omitted here and can be found in the working paper version of this article (Diss and Mahajne, 2019).",Social acceptability of Condorcet committees,https://www.sciencedirect.com/science/article/pii/S016548962030038X,7 April 2020,2020,Research Article,181.0
"Lemus Ana B.,Moreno Diego","Departamento de Economía, Universidad Carlos III de Madrid, Spain","Received 31 July 2019, Revised 18 December 2019, Accepted 12 January 2020, Available online 31 March 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.010,Cited by (0),The OECD’s recommendation that transfer prices between ,"Transfer pricing policies of multinational enterprises have important implications since exports and imports from related parties are a dominant share of the trade flows — see Bernard et al. (2009). Transfer prices serve the purpose of both allocating costs to subsidiaries and determining the tax liabilities of parents and subsidiaries. Policy makers are aware of the possible use of transfer prices as a device for shifting profits into low tax jurisdictions, and tend to follow the OECD Transfer Pricing Guidelines for Multinational Enterprises and Tax Administrations, which recommend that, for tax purposes, internal pricing policies be consistent with the ==== (ALP); i.e., that transfer prices be established, for tax purposes, on a market value basis, thus comparable to transactions between independent (unrelated) parties — see (OECD, 2017). Multinational enterprises must therefore choose whether to use a single transfer price for both internal transactions and tax purposes (i.e., keep ====), or use internal transfer prices different from those used for tax purposes (i.e., keep ====).====In this paper, assuming that transfer prices are consistent with the ALP for tax purposes, we study firms’ choices of accounting policies under vertical separation, imperfect competition and quantity setting, and identify the properties of the ensuing market equilibria. While under vertical integration the choice between keeping one or two sets of books is irrelevant, under delegation this choice affects managerial incentives, and hence firms’ consolidated profits, even when tax rates are equal across jurisdictions. Also, quantity setting provides a reduced form model for the analysis of more complex forms of imperfect competition; e.g., capacity choice followed by some kind of price competition, see, e.g., Kreps and Scheinkman (1983) and Moreno and Ubeda (2006), or competition via supply functions, see Delgado and Moreno (2004).====We abstract from many distinctive features of the activities of multinational enterprises — see, e.g., Markusen (2002) – and take as given the market structure, including firms’ decisions to become multinational enterprises (rather than, e.g., licensing their products), the location of their headquarters, etc. While incorporating these features into the model, and explaining firms’ choices, would undoubtedly be of interest, it would make the analysis overly complex.====In our framework, there are two multinational enterprises producing a homogeneous good that is sold in two markets, which we refer to as the Latin (home) market and the Greek (external) market. Parents engage in Cournot competition in the Latin market, while subsidiaries, in turn, engage in Cournot competition in the Greek market. Parents simultaneously choose their accounting policies and, upon observing accounting policy choices, simultaneously make output and, when relevant, internal transfer pricing decisions. Competition in the Latin market provides a reference price on comparable market transactions, and hence determines firms’ tax liabilities. Subsidiaries, upon observing parents’ accounting policies, outputs, and internal transfer pricing decisions, simultaneously make output decisions.====Since changing a firm’s accounting policy typically involves high administrative and consulting costs, in our setting the choice of accounting policy serves as a commitment device — see, e.g., Göx (2000) and Arya and Mittendorf (2008). The literature has studied other commitment instruments such as distorting managerial compensation, e.g., Fershtman and Judd (1987), Sklivas (1987), sinking capacity investments, e.g., Dixit (1980), Spence (1977), building inventories, e.g., Ware (1985), limiting information acquisition, e.g., Einy et al. (2002), Gal-Or (1988), or using cost allocation rules, e.g., Gal-Or (1993), Hughes and Kao (1998). Moreover, since accounting policies tend to be public, we assume that they are observed by competing firms prior to making output and transfer pricing decisions. (Accounting policies are disclosed in management discussions and annual reports, and are reported to securities and exchange commissions, tax authorities, etc.) Also, following the literature, we assume that parents maximize consolidated profits, while subsidiaries maximize their own profits — see Gal-Or (1993).====Thus, firms face a three stage non-cooperative game of complete information. A subgame perfect equilibrium (SPE henceforth) of this game identifies the firms’ accounting policies as well as their outputs in the Latin and Greek market. Using backward induction, we identify the SPE of this game. Given parents accounting policies, outputs, and transfer pricing decisions, which determine the subsidiaries’ costs, subsidiaries compete a la Cournot. It is therefore straightforward to identify the ensuing equilibrium in the Greek market. We proceed to study the equilibria that arise in subgames identified by parents accounting policy decisions.====In a subgame in which both parents keep one set of books, i.e., the good is transferred to subsidiaries at the price it is sold in the Latin market, a parent’s output decision must internalize its impact on the transfer price of its subsidiary, as well as its subsidiary’s rival. Thus, parents’ efforts to alleviate double marginalization intensify competition in the Latin market. As a result, in equilibrium the output served to the Latin (Greek) is larger (smaller), and hence the surplus realized is larger (smaller), than under vertical integration. However, we show that firms’ consolidated profits are greater, although the total surplus is smaller, than under vertical integration. Thus, keeping one set of books serves parents as an instrument to soften competition in the Greek market, and ultimately lead to larger consolidated profits. Hence the adoption of the ALP, when it leads firms to keep one set of books, provides a rationale for vertical separation. Consequently, adopting the ALP as a guideline for regulating transfer prices, when it leads to both firms keep one set of books, has negative welfare implications for the external country, i.e., reduces the surplus. (Interestingly, adopting the ALP does not achieve the objective of protecting the external country’s tax base either — see Lemus and Moreno (2019).)====In a subgame in which both parents keep two sets of books, i.e., transfer prices differ from those used for tax purposes, internal transfer prices give parents an instrument to try and gain an advantage in the external market. As a result, competition intensifies in the Greek market. Interestingly, the ALP creates a subtle link between the two markets, which intensifies competition in the Latin market as well: since the transfer price for tax purposes is the price in the Latin market, each parent can improve the competitive advantage of its subsidiary by increasing the tax liability of its subsidiary’s rival, which can be achieved by increasing output in the Latin market, thus reducing the price. As a result, the output in both markets is greater, and hence the surplus is larger, than under vertical integration. Therefore, adopting the ALP improves market efficiency when firms keep two sets of books.====In a subgame in which parents accounting policies are asymmetric, i.e., one parent keeps one set of books and the other keeps two sets of books, the subsidiary of the firm keeping two sets of books becomes dominant in the external market, while the parent using one set of books becomes dominant in the home market. In these equilibria, the firm choosing to keep one set of books is a weak position given the intensity of competition in both the home and the external market. We show that in both markets the output and the surplus are larger, while the sum of the firms’ profits is smaller, than under vertical integration. Thus, when at least one firm chooses to keep two sets of books, the adoption of the ALP leads to a surplus increase in both markets.====With these results in hand, we identify firms’ accounting policies in the SPE of the dynamic game. We restrict attention to pure strategy equilibria. Assuming that following parents’ accounting policy choices play forms a SPE, we show that parents’ interaction at the stage of choosing their accounting policies is described by a simple symmetric two-action static game ====. A Nash equilibrium of ==== corresponds to a SPE of the dynamic game.====Depending on the parameters of the model, which in our framework are the tax rate common to both markets and the size of the Latin market relative to the Greek market, ==== is either a prisoners’ dilemma game, a game of chicken, a coordination game, or a cooperation game. When ==== is a prisoners’ dilemma game, the unique SPE involves both firms keeping two sets of books. When ==== is a game of chicken, there are two pure strategy SPE, both of which involve firms using asymmetric accounting policies. When ==== is a coordination game, there are two SPE: one in which both firms keep one set of books, and another one in which both firms keep two sets of books. Finally, When ==== is a cooperation game, the unique SPE involves both firms keeping one set of books.====When ==== is a cooperation game the welfare consequences of adopting the ALP are negative. When ==== is a coordination game the equilibrium in which both firms keep one set of books Pareto dominates that in which both firms keep two sets of books, i.e., the firms’ profits are larger in the former than in the latter. Thus, even when ==== is a coordination game, any equilibrium concept that accounts for firms’ communication opportunities, e.g., Ferreira’s (1996) communication equilibrium, will select this equilibrium as the more likely. Hence, also in the case the welfare consequences of adopting the ALP are negative. When ==== is either a prisoners’ dilemma game or a game of chicken, the welfare consequences of adopting the ALP are positive. It is therefore useful to explore the parameter constellations that give rise to the different types of games.====We show that all four types of games may arise. For intermediate values of the relative size of the Latin market and the tax rate, a prisoners’ dilemma arises, whereas for larger values of these parameters a game of chicken arises. A coordination or a cooperation game arises when the relative size of the Latin market is small, or when both the tax rate is large, and the Latin and Greeks markets are of similar size. For these parameter constellations a firm has no incentives to switch to keeping two sets of books when the rival keeps one set of books: A deviating firm stands to gain at most the profit of the Stackelberg leader in this market (which is equal to half of the monopoly profits), but incurs the cost of losing entirely its position in the Latin market; such deviation is not profitable since the Greek market’s equilibrium outcome is close to monopoly, in which profits are shared equally by firms. See Fig. 1 in Section 4 for a precise description of the parameter constellations leading to each type of game.====While adopting the ALP is inconsequential under perfect competition, see Hirshleifer (1956), or when firms are vertically integrated, under imperfect competition and vertical separation its neutrality is lost: the adoption of the ALP significantly affects firms’ behavior and market outcomes.====The literature studying the consequences of the ALP under these conditions has considered alternative market structures. In a monopoly setting, Baldenius et al. (2004) show that keeping two sets of books is optimal as it allows the firm to deal with conflicting managerial objectives. A number of papers study strategic transfer pricing and tax distortions in a differentiated good symmetric duopoly of price competition, assuming that firms keep one set of books: Narayanan and Smith (2000) consider the firms’ organizational structure. Göx (2000) shows that strategic transfer pricing softens competition. This literature tends to conclude that under quantity competition, because quantities are strategic substitutes, strategic transfer pricing intensifies competition. In contrast, our results show that the conclusion that under the ALP quantity competition intensifies competition is not warranted.====Closer to our work, Dürr and Göx (2011) establishes conditions under which keeping one set of books is a dominant strategy in a differentiated goods duopoly of price competition. In contrast, we study a homogeneous good symmetric duopoly of quantity competition. (As argued above, quantity competition serves as a reduced form of more complex models of competition.) Further, we focus on the analysis of the game ==== that multinational enterprises face when choosing their accounting policies, and show that depending on the parameters ==== is one of four well-known two-player two-action games: a cooperation, coordination, chicken or prisoners’ dilemma game. We provide an equilibrium analysis of ==== and identify firms’ equilibrium accounting policies, i.e., the conditions under which firms keep one set of books, or two sets of books, or asymmetric accounting policies. We find that firms keep one set of books nor just when ==== is a cooperation game, i.e., when keeping one set of books is a dominant strategy as in Dürr and Göx (2011), but also when ==== is a coordination game. Moreover, the space of parameters in which ==== is a coordination game is considerably larger than that in which it is a cooperation game. Thus, in a significant part of the parameter space firms keep one set of books even though this is not a dominant strategy. Also, while in Dürr and Göx (2011) the set of possible transfer prices is exogenously given, in our framework transfer prices for tax purposes are endogenously determined. In addition, assuming a linear demand allows us to identify the parameter regions in which the different equilibria arise. (And because these results involve evaluating the sign of certain polynomials, continuity warrants that similar results arise for demand functions in the neighborhood of the set of linear demand functions.) Dürr and Göx (2011) instead consider twice differentiable demand functions and impose conditions on their derivatives. Their main result (Proposition 4), however, involves assumptions regarding equilibrium objects, and hence is not informative of the conditions on primitives these assumptions involve.====A previous literature has established that, in the absence of the ALP, vertical separation intensifies or alleviates competition depending on whether firms compete in quantities or prices — see Vickers (1985), Fershtman and Judd (1987), Sklivas (1987), Alles and Datar (1998). Interestingly, our results showing that under certain conditions the ALP softens competition in the external market recovers a rationale for vertical separation under quantity competition. (Obviously, the result that vertical separation intensifies competition with quantity setting does not hold for other market structures; e.g., Moresi and Schwartz (2017), shows that a vertically integrated input monopolist supplying to a differentiated downstream rival may prefer the rival to expand even under Cournot competition. Also, the organization literature has studied alternative schemes for managers’ remuneration that overcome the shortcoming of delegation under quantity competition, see, e.g., Jansen et al., 2007, Jansen et al., 2009.)====The rest of the paper is organized as follows. We introduce the basic setup in Section 2. In Section 3 we study the equilibria of the subgames given the firms’ choice of accounting policies. In Section 4 we study the choice of accounting policies. We conclude in Section 5. The proofs of our results are relegated to Appendix.",Strategic incentives for keeping one set of books under the Arm’s Length Principle,https://www.sciencedirect.com/science/article/pii/S0165489620300299,31 March 2020,2020,Research Article,182.0
"Dam Kaniṣka,Robinson-Cortés Alejandro","Center for Research and Teaching in Economics (CIDE), 3655 Carretera Mexico-Toluca, Lomas de Santa Fe, 01210 Mexico City, Mexico,Division of the Humanities and Social Sciences, California Institute of Technology, 1200 East California Blvd. Pasadena, CA 91125, United States of America","Received 26 July 2019, Revised 6 March 2020, Accepted 6 March 2020, Available online 31 March 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.03.001,Cited by (1),"Motivated by empirical evidence, we develop an incentive contracting model under oligopolistic competition to study how incumbent firms adjust managerial incentives following deregulation policies that enhance competition. We show that firms elicit higher managerial effort by offering stronger incentives as an optimal response to entry, as long as incumbent firms act as production leaders. Our model draws a link between an industry-specific feature, the time needed to build production capacity, and the effect that product market competition has on executive compensation. We offer novel testable implications regarding how this industry-specific feature shapes the incentive structure of executive pay.","There is a plethora of empirical evidence that supports the Hicksian view (Hicks, 1935) that executive compensation tends to be more performance-sensitive in more competitive environments (e.g. Nickell, 1996, Van Reenen, 2011). A series of empirical studies have used industry-specific regulatory reforms to analyze the effect of competition on executive pay (Crawford et al., 1995, Hubbard and Palia, 1995, Kole and Lehn, 1999, Palia, 2000, Cuñat and Guadalupe, 2009a, Dasgupta et al., 2017). These studies focus on how deregulation policies that increase competition in the product market affect the structure of managerial incentive contracts. The main takeaway from this literature is that, following a deregulation policy that intensifies product market competition, firms reduce managerial slack by increasing executive compensation and strengthening its pay-performance sensitivity.====Our objective in this paper is to explain the nature of the aforementioned empirical regularity, and to offer new insights into how executive pay is shaped by industry-specific features. First, we provide a simple model of oligopolistic competition with firm entry that shows why incumbent firms find it optimal to reduce managerial slack when competition rises because of deregulation. Then, we use our model to derive novel empirical implications regarding the ==== in an industry. Our model shows that this industry-specific feature is a crucial factor when analyzing the effect that firm entry has on executive compensation. According to our model, the relationship observed in the empirical studies obtains in industries in which the time to build capacity is such that incumbents act as production leaders and entrants as followers. This result goes in line with the empirical literature given that existing studies focus on industries in which it takes time to build production capacity, such as banking, manufacturing, and the airline industry.====The question of how product market competition shapes managerial incentives is far from being new in the literature.==== ==== Notwithstanding, our approach is novel in that we analyze it explicitly in a framework of firm entry. Because incumbent firms anticipate (and accommodate) future entry with relaxing regulation, we use a standard model of sequential quantity-setting oligopoly, in which entrant firms choose their managerial contracts and quantities after observing those of the incumbents. Our focus is on the strategic response of incumbents regarding managerial incentive pay as they foresee the entry of new firms. In line with the empirical literature, our main finding is that it is optimal for incumbents to strengthen incentive pay and reduce managerial slack when they foresee the entry of new firms into the product market. Moreover, we show that the strength of the managerial incentives offered by incumbents is increasing in the number of entrants—higher competitive pressure leads to steeper incentives and lower managerial slack.====Our model incorporates managerial incentive contracts into the Stackelberg quantity competition framework proposed by Daughety (1990). There is a fixed number of incumbents and a set of potential entrants with more entrants meaning greater competitive pressure on the incumbent firms. Both incumbents (in the pre-entry stage) and entrants (in the post-entry stage) play Cournot games among themselves; entrants take the aggregate output of incumbents as given. All firms are initially inefficient, and each hires a risk neutral manager whose principal task is to exert non-verifiable R&D effort to bring down the constant marginal cost of production, what is often termed “process innovation”. We assume that the final realizations of marginal costs are private information among firms, and that incentive contracts are publicly observable. Hence, even though the marginal costs of rival firms are unknown, each firm observes a signal of how likely every other firm is to reduce its marginal cost.====The crux of our model is that managerial effort is beneficial to incumbents in two ways. First, steeper incentives that induce each manager to exert higher effort directly increase the likelihood of cost reduction (value-of-cost-reduction effect). Second, they also alter the beliefs of the rival firms about the true cost realization of a given firm (marginal-profitability-of-effort effect). Even if a manager fails to achieve the cost target, her effort is profitable in as much as it makes the rivals believe that a cost reduction has actually been attained. More intensified product market competition affects each of these two effects through the market size and the effective size of cost reduction. As the entrants’ optimal contracting and production decisions are negatively affected by the aggregate incumbent output, the entry of new firms implies an increase in both market size and the effective size of cost reduction for incumbents. In turn, this implies both a higher expected value of cost reduction and expected marginal profitability of effort, which makes it optimal for the incumbents to elicit higher managerial effort by strengthening incentives. It is worth noting that, even in the absence of the marginal-profitability-of-effort effect, a growing number of entrants strengthens the value-of-cost-reduction effect. Such case arises, for example, when marginal costs are public information and managerial effort is unprofitable beyond cost reduction.====The key to our main result is that incumbent firms are able to strategically pre-commit to managerial contracts, which in turn determine technological efficiency endogenously. The general intuition goes in line with the seminal works of Fudenberg and Tirole (1984) and Bulow, Geanakoplos, and Klemperer (1985). In a standard entry model, when an incumbent and an entrant compete in quantities (strategic substitutes), lowering the marginal cost of the incumbent decreases the entrant’s total profits (since the incumbent’s optimal output increases). Hence, when costs are endogenously determined, incumbents find it optimal to behave more aggressively in cost-reduction activities. In our framework, this corresponds to incumbents offering stronger managerial incentives which are observed by the entrant firms. Thus, by making a commitment to be more aggressive, the incumbents push the entrants into a more passive posture. This is an example of the ‘top-dog’ strategy, according to the terminology proposed by Fudenberg and Tirole (1984). This sort of aggressive or accommodating behavior on behalf of the incumbent firms does not emerge under simultaneous competition because the incumbents fail to reap such benefits due to the lack of pre-commitment to any investment strategy. By contrast, under strategic complementarity, e.g. price competition, the aforementioned result is reversed because the incumbent firms would commit to a strategy of ‘underinvestment’ (weakened managerial incentives) after which the entrants would optimally respond by lowering their prices. Fudenberg and Tirole (1984) call such underinvestment strategy to avoid stoking competition ‘puppy-dog ploy’.====The paper is organized as follows. In Section 2, we review the related literature. In Section 3, we outline the model. In Section 4, we solve for the equilibrium and present our main results. In Section 5, we present testable implications of our model. In Section 6, we analyze two extensions, hierarchical entry and price competition. We conclude in Section 7. All proofs are relegated to Appendix B, most of which follow from Result 1 in Appendix A.",Executive compensation and competitive pressure in the product market: How does firm entry shape managerial incentives?,https://www.sciencedirect.com/science/article/pii/S0165489620300275,31 March 2020,2020,Research Article,183.0
"Bos Iwan,Marini Marco A.,Saulle Riccardo D.","Department of Organization, Strategy and Entrepreneurship, School of Business and Economics, Maastricht University, Netherlands,Department of Social and Economic Sciences, Sapienza University of Rome and CREI, Italy,DSEA, University of Padova, Italy","Received 31 July 2019, Revised 22 December 2019, Accepted 28 January 2020, Available online 23 March 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.013,Cited by (7),"Research on collusion in vertically differentiated markets is conducted under one or two potentially restrictive assumptions. Either there is a single industry-wide cartel or costs are assumed to be independent of quality or quantity. We explore the extent to which these assumptions are indeed restrictive by relaxing both. For a wide range of coalition structures, profit-maximizing cartels of any size price most of their lower quality products out of the market as long as production costs do not increase too much with quality. If these costs rise sufficiently, however, then market share is maintained for all product variants. All cartel sizes may emerge in equilibrium when exclusively considering individual deviations, but the industry-wide cartel is the only one immune to deviations by coalitions of members. Overall, our findings suggest that firms have a strong incentive to coordinate prices when the products involved are vertically differentiated.","Two equally-priced products, A and B, are said to be vertically (or quality) differentiated when all buyers prefer A to B or ==== . Quality differences, whether perceived or actual, typically allow those producing superior quality to charge more for their product or service. This has several strategic implications in terms of pricing and quality variations offered. For example, Mussa and Rosen (1978) establish that a monopolist prefers to sell lower quality goods at higher prices when compared to a competitive market. Similar analyses have been conducted for monopolistically competitive and duopolistic markets by Shaked and Sutton (1982) and Champsaur and Rochet (1989), respectively.====A more contemporary body of work explores the presence of quality heterogeneity in relation to price collusion.==== ====
 Häckner (1994), for instance, considers an infinitely repeated duopoly version of the vertical differentiation model in Gabszewicz and Thisse (1979) and Shaked and Sutton (1982). Among other things, he finds that the high quality supplier has the strongest incentive to chisel on the agreement. Within a representative consumer framework with horizontal and vertical product differentiation, Symeonidis (1999) draws the opposite conclusion; that is, it is the low quality supplier who is most eager to defect. Recently, Bos and Marini (2019) have shown that the contrasting conclusions of Häckner (1994) and Symeonidis (1999) critically depend on the price-cost margin of cartel members. Specifically, they establish a negative relationship between the collusive price-cost margin and the incentive to deviate from a price-fixing agreement.====All this and related analyses are performed under one or two potentially restrictive assumptions, however. Either production costs are taken to be identical (or even absent) or the cartel is assumed all-inclusive, ====, each industry member takes part in the anticompetitive coalition. As to the first, producing higher quality products often requires more costly inputs. In many cases, therefore, it would be more natural to assume that production costs are increasing with both quality ==== quantity.==== ==== Regarding the second, many discovered cartels have been less than all-inclusive in the sense that they faced competition from at least one firm not participating in the agreement. In the French yogurt cartel, for example, eleven firms fixed prices of supermarket own-brand yogurt from 2006 to 2012. Yet, the premium producer Danone did not take part in the conspiracy.==== ==== For the year 2013, there is evidence of price collusion between two premium ice cream brands: Ben & Jerry’s and Häagen-Dazs.==== ==== As yet another example, the global ==== and ==== cartels from the 1990s excluded Chinese competitors.==== ====In this paper, our goal is to explore the extent to which these two assumptions are indeed restrictive. Towards that end, we study a modified version of the model in Mussa and Rosen (1978). Specifically, we adapt that setting to allow for oligopolistic price competition. Each firm is assumed to produce a unique quality variant at constant unit costs, which are increasing with quality. Within this framework, we examine properties of optimal price-fixing contracts and, in particular, how these depend on costs and the inclusiveness of the cartel. We furthermore analyse what coalitions are likely to form by endogenizing the cartel formation process.====Let us summarize some of our main findings. In line with the existing literature, a profit-maximizing all-inclusive cartel prefers to exclusively sell the highest quality product when unit costs increase weakly less than proportionally with quality. If costs increase more than proportionally, however, the optimal cartel contract stipulates positive sales for all product variants. A similar result holds for less than all-inclusive cartels. That is, if unit costs are not rising too much with quality, then colluders prefer to boost sales of their top quality product by pricing lower quality variants out of the market. Yet, when a cartel faces competition from an outsider offering inferior quality, it additionally chooses to produce the lowest quality available within the coalition (====, a so-called fighting brand). When unit costs increase more than proportionally with quality, the outcome is comparable to that under full collusion; all variants remain on sale.====Regarding the composition of cartels, we consider two coalition formation procedures which we coin the ==== and the ==== rule. These rules differ in terms of whether the coalition breaks down in the event of deviations. Under the aggregative rule, the remaining coalition structure stays unaffected by a defecting firm. By contrast, a deviating seller triggers a collapse into singletons under the unanimity rule. Studying these two polar cases is useful in settings like ours where coalition externalities are positive and monotonic, because it allows to predict the outcome for ==== non-deviating firm partition.==== ==== We consider both individual and coalitional stability, ====, whether coalition structures are immune to deviations by individual or subgroups of members.====Assuming constant differences between adjacent qualities and associated costs, we find that any coalition structure can be individually stable under both the aggregative and the unanimity rule. Yet, with the possible exception of the grand coalition, none of them is coalitionally stable. Additional numerical analyses suggest that the all-inclusive cartel can also be stable when costs are increasing more or less than proportionally with quality. In terms of policy implications, these findings suggest that firms have a fairly strong incentive to collude when the products involved are vertically differentiated. In particular, an industry-wide price increase in conjunction with a decrease of the number of quality variants offered should be considered a tell-tale sign of collusion.==== ====Since each seller has a unique market position within our setting, this paper is related to literature about cartel formation with heterogeneous firms. Donsimoni (1985), for instance, considers a collusive price leadership model with different unit costs. She establishes the existence of a stable partial cartel comprising the most efficient industry members. In a related fashion, Bos and Harrington (2010) show that there is a positive relation between firm size and the incentive to join a cartel.==== ==== Recently, Merker (2019) suggests that the size of partial cartels may be inversely related to the degree of horizontal product differentiation. The only other work we are aware of that combines coalition formation with quality differentiation is Gabszewicz et al. (2019). In a three-firm market, and under the assumption that costs depend exclusively on quality, they establish that the grand coalition does not emerge in equilibrium.====Section 2 introduces the model. Section 3 discusses the noncollusive solution. Section 4 presents properties of an optimal price-fixing contract under full and partial collusion, respectively. Endogenous cartel formation is considered in Section 5. Section 6 concludes. All proofs are relegated to Appendix A.",Cartel formation with quality differentiation,https://www.sciencedirect.com/science/article/pii/S0165489620300366,23 March 2020,2020,Research Article,184.0
"Corchón Luis C.,Torregrosa Ramón J.","Universidad Carlos III de Madrid, Spain,Universidad de Salamanca and IME, Spain","Received 31 July 2019, Revised 31 January 2020, Accepted 31 January 2020, Available online 23 March 2020, Version of Record 25 March 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.012,Cited by (3)," for a class of demand functions when firms are identical and produce under constant returns to scale. We focus on inelastic and convex demand yielding non-concave profit functions. We show that equilibrium is unique, stable and yields well-behaved comparative statics. The model can yield huge welfare losses, non-downward sloping ==== and non-identical firms.","“Cournot equilibrium” has more than 50.000 Google citations. Why does such a well-known and studied equilibrium concept merit one more round of analysis? In short, we believe that the properties of Cournot equilibrium when profits are non-concave in its own output have been scantily studied.==== ==== In this paper we set out to remedy this situation by showing that when profits are non concave, equilibrium may not exist; and when it does exist, welfare losses can be huge, the Laffer curve does not exist and no tax reform is Pareto improving. The latter three properties can occur even though the Cournot equilibrium is unique, stable and yields comparative static properties that agree with our intuition.==== ====In this paper we consider a special form of the demand function, namely ====, where ==== is the market price, ==== is aggregate output and ==== and ==== are parameters.==== ==== This function allows the computation of the Cournot equilibrium and generalizes well-known functional forms such as linear (when ====) and isoelastic (when ====, ==== and ====). We focus on the case ==== because the results on existence, uniqueness and welfare losses for ==== are well-known (see Anderson and Renault, 2003, Corchón, 2008). One of the reasons for which the case of ==== is important is that it provides a simple way to deal with markets with inelastic demands and imperfect competition: demand is inelastic by setting ==== and ====. Examples of these markets include health care, electricity and gasoline.==== ==== In order to focus on the issues arising from a non-concave demand, we assume that marginal costs are constant and identical.====First, we show that ==== (where ==== is the number of firms) is a necessary and sufficient condition for the existence of equilibrium. This finding contrasts with the results shown by McManus, 1962, McManus, 1964 and Roberts and Sonnenschein (1976). Nonetheless, these results require an upper-hemi continuous inverse demand which does not hold for the demand used in this paper.==== ==== In the Appendix A.1 we show that the methods developed in our paper can be used to tackle the case of firms with different constant marginal costs and a more general inverse demand function. All that is needed is:====– that the industry marginal revenue is larger (resp. smaller) than the sum of marginal costs for small (resp. large) aggregate output. This guarantees a solution in aggregate output to the system of equations composed by First Order Conditions (FOC) of profit maximization (Conditions A and B in the Appendix A.1);====– a condition that guarantees that the output of each firm that solves FOC is positive (condition C in Appendix A.1);====– and a condition that implies that each Second Order Condition of profit maximization holds globally, when the other firms are at the output calculated from FOC (condition D in Appendix A.1).====Other sufficient conditions for the existence of equilibrium, when firms are possibly different, are given by the following authors: Hahn (1962) assumes ==== where ==== is the output of firm ==== (see al-Nowaihi and Levine, 1985. p 309).==== ==== In our set up, this equates to ====. This condition can be required in equilibrium (and is therefore not valid for showing the existence of equilibrium) and for every output. In our model the latter implies that setting all outputs except ==== equal to zero, ====. Novshek (1985) assumes ==== (which in our set up is equivalent to ====). Caplin and Nalebuff (1991) assume that ==== is concave (which in our set up is equivalent to ====), Amir (1996) assumes a log-concave ==== (which in our set up is equivalent to ====) and Ewerhart (2014) assumes a biconcave demand (which in our set up is equivalent to ====). Thus, under identical firms and constant marginal cost, ==== is weaker than any condition so far considered in the literature.====In addition, we show that equilibrium is stable according to the gradient dynamics, if ====, and that comparative statics agree with our intuition. This implies that the properties of equilibrium in this paper are not due to the equilibrium being unstable or the consequence of odd comparative static results.====Once we have secured the foundations of our model, we focus our attention on three important topics.====In sum, when ==== Cournot equilibrium may exist, but it displays characteristics very different from those in the case where ====.====The remaining part of the paper is structured as follows: the next section outlines the model; Section 3 presents the results obtained on the existence of equilibrium and its properties; Section 4 studies social welfare; Section 5 analyzes the Laffer curve; and Section 6 studies tax reform. Finally, Section 7 discusses the conclusions drawn.",Cournot equilibrium revisited,https://www.sciencedirect.com/science/article/pii/S0165489620300354,23 March 2020,2020,Research Article,185.0
"González-Maestre Miguel,Granero Lluís M.","Universidad de Murcia, Spain,Universitat de València, Spain","Received 30 July 2019, Revised 21 February 2020, Accepted 22 February 2020, Available online 20 March 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.02.005,Cited by (2),"Under spatial product differentiation and product design, we identify conditions for either excessive or insufficient firm entry. We extend previous settings, based on the Salop circular model, to analyze the combined role of positive demand ","This paper examines competition in product design and its implications on pricing and welfare in a model of spatial product differentiation with elastic demand. Our analysis reveals that product design and demand elasticity have a relevant impact on strategic trade-offs that affect pricing and welfare.====In our analysis, we consider the Salop (1979) circle model of product location, with two additional items. First, product design is introduced along the lines of González-Maestre and Granero (2018), who explore a spatial representation of product design in the market for a differentiated product. Second, the conventional assumption of a completely inelastic demand in models of spatial product differentiation is relaxed, as in Gu and Wenzel (2009), where the excess entry theorem in those models only holds when the demand elasticity is sufficiently small.====The model in this paper adopts the conceptual framework by Johnson and Myatt (2006), where decisions on product design lead to rotations in demand. Specifically, we follow González-Maestre and Granero (2018) in considering design as being mediated through product location in a version of the Salop model in which product design has two components: a horizontal decision of which type of consumer to target, and the degree to which the product is tailored to this type of consumer. A less generic product implies then a higher valuation by targeted consumers, but it also implies a lower valuation for other consumers with different tastes. This gives rise to rotations in demand that lead to a trade-off in designing the product. In our version of the Salop model of product location, where consumers are located on the circumference of a circle, those demand rotations rely on location-based design decisions. In particular, product location along an arc captures a conventional horizontal differentiation, and product location into the interior of the circle along a ray captures the extent to which the product is tailored to the specific type of consumer located at the extreme point of that ray on the circumference of the circle. In that context, extreme designs are represented by points either on the circumference (fully tailored product) or at the center of the circle (fully generic product), whereas intermediate designs are represented by interior points of the circle.==== ====Our spatial representation of product design is related to the Launhardt duopoly model of horizontal and vertical product differentiation (Launhardt, 1885). In that model, firms locate at different points along a “street” (as in Hotelling, 1929) but have different transportation technologies for product delivery. Then, if firms are located together, the firm with the lower transportation cost would supply all consumers if firms were to charge the same mill price. Hence, the product which is easier to transport may be viewed as a product of higher quality (see Dos Santos Ferreira and Thisse, 1996). This leads to conventional vertical product differentiation in which a higher quality (associated with a lower transportation cost) increases the willingness to pay for all consumers. In contrast, our setting considers that product design can be chosen such that the willingness to pay for some consumers is increased when the product is tailored to them (they face lower transportation costs in the Launhardt terminology) but this also reduces the willingness to pay for other consumers with different tastes (who end up facing higher transportation costs).==== ====Our results reveal that an increase in the number of rival firms above some critical level leads to a higher degree of product targeting. The economic explanation is that, to relax price competition, each firm has an incentive to tailor its product design to targeted consumers when competition intensifies. In those circumstances, firm entry impacts price through two effects: first, there is a direct effect, such that price falls with firm entry; and second, there is an indirect product targeting effect, such that a higher degree of targeting (associated with entry) makes nearer target consumers better off and more distant non-target consumers worse off. The former direct effect is conventional, whereas the latter indirect effect rests on demand rotations from endogenous design and it can lead to price-increasing competition, as in González-Maestre and Granero (2018).==== ==== Here, in contrast with González-Maestre and Granero (2018), and with most of the conventional models of spatial product differentiation, we consider that demand is not completely inelastic. This feature has relevant consequences, particularly on social welfare. Our results suggest that both the equilibrium and the socially optimal levels of design targeting are increasing in the number of rival firms. With an exogenous number of firms, we find that the socially optimal level of design targeting is greater than is the equilibrium level of targeting when the elasticity of demand is below a certain threshold, but the opposite situation is the case when that elasticity is above the threshold. Notably, this result shows that the socially insufficient degree of product targeting obtained by González-Maestre and Granero (2018) is due to the particular assumption of zero-demand elasticity.====Previous contributions that introduce price-dependent demand into spatial models for exogenous product design include Boeckem (1994), where consumers are heterogeneous with respect to reservation prices; Anderson and de Palma (2000), where local and global competition interact with a constant demand elasticity; Rath and Zhao (2001), where each consumer’s demand is linear and the quantity purchased depends on price in the Hotelling setting; Peitz (2002), where the Hotelling and the Salop settings are considered for unit-elastic demand to study the existence of Nash equilibrium in prices; and Gu and Wenzel (2009), where the focus is on whether demand elasticity gives way to either excess or insufficient entry. In our analysis, we combine the presence of an endogenous degree of targeted product design, as in González-Maestre and Granero (2018), with the assumption of a positive and constant demand elasticity, as in Gu and Wenzel (2009). In line with González-Maestre and Granero (2018), we find that the degree of targeted product design increases with the relative size of the market. In that context, we identify a number of effects, such that product design affects social welfare mediated through free entry and demand elasticity. From those effects, we show that excessive entry is more likely for intermediate market sizes. In contrast, insufficient entry is more plausible in either large or small economies with high demand elasticity. Therefore, our analysis shows that (i) when the demand elasticity is positive, the excessive entry result obtained in González-Maestre and Granero (2018) can be reversed, provided that the demand elasticity is large enough; and (ii) the interaction between endogenous product design and positive elasticity implies that, in contrast with Gu and Wenzel (2009), the excessive versus insufficient entry result depends not only on demand elasticity but also on market size.====The rest of the paper is organized as follows. Section 2 introduces the framework and examines the basic model for an exogenous number of firms. Subsequently, Section 3 considers the model with endogenous firm entry. Finally, Section 4 gathers the main conclusions.",Excessive vs. insufficient entry in spatial models: When product design and market size matter,https://www.sciencedirect.com/science/article/pii/S0165489620300342,20 March 2020,2020,Research Article,186.0
"Alipranti Maria,Petrakis Emmanuel","Department of Economics, University of Macedonia, Greece,Department of Economics, University of Crete, Greece,Departamento de Economía, Universidad Carlos III de Madrid, Spain","Received 1 July 2019, Revised 22 February 2020, Accepted 22 February 2020, Available online 19 March 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.02.004,Cited by (2),"We show that ==== arises in equilibrium in the downstream market of a vertically related ==== with bottleneck. The upstream monopolist offers fixed fee discounts to the downstream firms in order to motivate them to set prices , instead of quantities, in the final good market. This is in sharp contrast with the bulk of the literature in which ","Nowadays contracts signed among firms in different stages of the production chain – input suppliers and final good manufacturers, wholesalers and retailers – have become all the more complex, including often besides a per-unit of input price, a variety of other terms, such as fixed fees, trade discounts, slotting and listing fees, and other lump-sum payments. These contracts have received lately a lot of attention by researchers and policy makers due to their significant implications on firms’ behavior, market outcomes and social welfare.==== ====In this paper, we investigate the mode of competition that arises in equilibrium in a vertically related market in which an upstream monopolist supplier trades with two horizontally differentiated downstream firms using three-part tariff contracts. The latter includes a wholesale price and a fixed fee to be paid by the downstream firm to the upstream supplier ==== a compensation fee to be transferred downstream in the end of the game. This compensation fee can be envisaged as a discount over the fixed fee. We consider a stylized three-stage game with observable actions. In the first stage, the upstream monopolist makes, simultaneously and non-cooperatively, take-it-or-leave-it offers to the downstream firms stipulating the type of contract (==== or ====) to be offered by the downstream firm to final consumers==== ==== along with a compensation fee that the supplier will pay back to the downstream firm if the latter abides by the agreement. Each downstream firm then, simultaneously and non-cooperatively, accepts or rejects the offer. In the second stage, the upstream monopolist bargains, simultaneously and non-cooperatively, with each downstream firm over their two-part tariff (i.e., a wholesale price and a fixed fee) trading terms. Finally, downstream firms compete in the market.====It is well known, since the seminal paper of Singh and Vives (1984), that if firms are free to choose their strategic variable to compete in the market, it is a dominant strategy for each firm to choose quantity when they produce substitute goods. As a result, Cournot competition arises in the market equilibrium. Over the last thirty years, the Singh and Vives result has been generalized and extended in a variety of alternative economic settings. Tanaka, 2001a, Tanaka, 2001b shows that the Singh and Vives result holds too in a market with ==== oligopolistic firms and a market with vertical product differentiation, respectively. Reisinger and Ressner (2009) point out the sensitivity of this result in markets with demand uncertainty. Matsumura and Ogawa (2012) and Scrimitore (2013) explore its robustness in mixed markets consisting of welfare maximizing and profit maximizing firms. In particular, Matsumura and Ogawa (2012) show that in a mixed duopoly, price instead of quantity competition arises in equilibrium, independently of whether products are substitutes or complements. Yet, Scrimitore (2013) indicates that the latter result can be reversed in the presence of sufficiently high firm subsidization.====We show that, contrary to Singh and Vives (1984) and the bulk of the literature thereof, Bertrand competition arises in equilibrium in a vertically related industry in which an upstream monopolist supplier and two downstream firms trade via three-part tariff contracts. Intuitively, each supplier–buyer pair (that is, a pair consisting of a representative of the upstream monopolist and one downstream firm) achieves higher joint profits under a price rather than under a quantity contract offered by the downstream firm to final consumers, independently of the rival downstream firm’s contract offer. The main reason is that the upstream monopolist has stronger incentives to increase the aggressiveness of the downstream firms when the latter offer quantity contracts, and hence faces a more severe commitment problem than under price contracts. These suggest that each supplier–buyer pair has incentives to opt for a price instead of a quantity contract. Yet, the downstream firm obtains higher profits under a quantity contract and will offer such a contract to the final consumers, unless it receives a compensation fee from the upstream supplier. We show that it is profitable for the upstream supplier to offer appropriate compensation fees to motivate downstream firms to offer price instead of quantity contracts. As a consequence, the upstream supplier and the downstream firms have mutual incentives to agree on offering price contracts to final consumers, and thus Bertrand competition arises in the downstream market equilibrium.====Extending our analysis in a few dimensions, we show that our main findings are robust under downstream firms’ marginal cost asymmetries as well as under an alternative two-stage timing specification of our game. On the other hand, when final goods are complements, downstream firms prefer to offer price contracts to final consumers, while the upstream supplier prefers quantity contracts but only its bargaining power is not too high. Yet, in the latter case the upstream supplier has no incentive to offer compensation fees to downstream firms to motivate them to offer quantity contracts to final consumers, and thus Bertrand competition arises once more in the downstream market equilibrium.====Our paper contributes to the recent literature that examines the endogenous selection of strategic variables in vertically related markets. Correa-López (2007) and Manasakis and Vlassis (2014) reconfirm the Singh and Vives result in an unionized market with an industry-wide union and in a vertically related market in which vertical chains trade via wholesale price contracts, respectively. In addition, Correa-López (2007) identifies stringent conditions under which a reversal of this result occurs: Under decentralized unions and particular constellations of the unions’ bargaining power and relative preference over wages. Chirco and Scrimitore (2013) show that in a market with network externalities and managerial delegation, a reversal of Singh and Vives result occurs, provided that the network externalities are sufficiently strong. Basak and Wang (2016) show that in a vertically related industry with centralized bargaining over a two-part tariff contract, it is a dominant strategy for each downstream firm to offer a price contract to final consumers. Finally, Rozanova (2017) shows that bargaining over contingent two-part tariff contracts between an upstream supplier and two downstream firms leads to downstream firms offering price contracts to final consumers in equilibrium. Our analysis extends this literature, by showing that Bertrand competition arises in equilibrium in a vertically related market in which the trading between an upstream supplier and two downstream firms is conducted via more complex three-part tariffs contracts.====Our paper contributes also to the recently growing literature on vertical contracting via three-part tariff contracts that analyzes a number of alternative forms of payments made by the upstream to the downstream firms, such as slotting allowances, upfront payments, and discount fees. Marx and Shaffer (2007) investigate the role of upfront payments on downstream exclusion when two competing retailers make take-it-or-leave-it offers to a common manufacturer. They show that exclusion arises in equilibrium when a retailer has all the bargaining power vis à vis a manufacturer. Therefore, the dominant retailer can foreclose its rival by requesting an ex-ante upfront payment by the manufacturer in exchange to a higher fixed fee paid by the retailer if trade takes place. In contrast, Miklós-Thal et al. (2011) show that foreclosure can be avoided when the retailers’ offers are contingent on the relationship being exclusive or not. In a similar vein, Rey and Whinston (2013) examine whether the retailers’ bargaining power and upfront slotting allowances could prevent less powerful manufactures from getting carried by all retailers. They show that when retailers can offer instead of a single three-part tariff, a menu of three part-tariffs, that is, a three-part tariff designed for common agency and a respective one designed for exclusive dealing, there is always an equilibrium in which exclusion does not occur. This literature has focused on examining the role of three-part tariffs and, more specifically, the role of ex-ante upfront payments in foreclosing retailers market. In contrast, we consider three-part tariffs in which the compensation fees constitute ex-ante agreements between the upstream supplier and the downstream firms, which are, however, paid ex-post by the upstream supplier only if a downstream firm abides by the agreement. In addition, our focus is on the role of three-part tariffs in forming the mode of downstream competition. We thus highlight that three-part tariff contracts, apart from inducing exclusion in the downstream market, may crucially affect the mode of downstream competition.====The remainder of the paper is organized as follows. Section 2 describes our model. Section 3 presents the equilibrium analysis and states the main results. Section 4 discusses a few extensions of our base model. Finally, Section 5 concludes. All proofs are relegated to Appendix.",Fixed fee discounts and Bertrand competition in vertically related markets,https://www.sciencedirect.com/science/article/pii/S0165489620300329,19 March 2020,2020,Research Article,187.0
"d’Aspremont Claude,Dos Santos Ferreira Rodolphe","CORE, Université Catholique de Louvain, Belgium,BETA-Strasbourg University and Católica Lisbon School of Business and Economics, France","Received 31 July 2019, Revised 22 January 2020, Accepted 31 January 2020, Available online 19 March 2020, Version of Record 4 August 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.009,Cited by (2),"The paper uses the most general version of a Dixit–Stiglitz economy and the concept of oligopolistic equilibrium, defined in previous work, with firms maximizing profits in prices and quantities under a market share and a market size constraint. The purpose here is to take even more advantage of separability so as to partition the oligopolistic sector into groups. Weak separability simplifies quantity conjectures and homothetic separability simplifies price conjectures. Oligopolistic equilibria can in addition be approximated by introducing group expenditure conjectures. Finally, the way different groups interact within the same ==== is illustrated within the same framework.","The main obstacles in developing a theory of General Oligopolistic Equilibrium==== ==== have been well understood. They are of two types. First, from the modeler’s point of view, combining the difficulties inherent to oligopoly theory, already present in a partial equilibrium context, with those of general equilibrium theory leads easily to intractability and even to non-existence of equilibrium. Second, from the players’ point of view, it may be unrealistic to suppose that they are able or willing to take into account all the conceivable interactions, however weak, that might concern them. Fortunately, Dixit and Stiglitz (1977) contribution afforded a popular and successful way to bypass these obstacles, by reducing the economy to two sectors, one competitive, the other oligopolistic with identical firms supplying final products to a representative consumer, and then by considering the limiting monopolistic competition case of insignificant firms so as to eliminate strategic interactions (each firm behaving as a monopoly in its own niche). Because of its tractability, this type of modeling has become dominant in trade and macroeconomic applications, first on the basis of the simple CES sub-utility case and, more recently, beyond the CES, by relaxing either homotheticity or additivity.==== ====We want to argue that going beyond the CES case is not the only way of gaining in model flexibility and applicability. Even allowing for strategic interactions between large heterogeneous firms in the oligopolistic sector, alternative simplifications may keep the model tractable and give a reasonable account of firms’ conjectures and calculations. In previous work (d’ Aspremont and Dos Santos Ferreira, 2016, d’ Aspremont and Dos Santos Ferreira, 2017) we have defined a concept of oligopolistic equilibrium by referring to a Dixit–Stiglitz economy, with two sectors corresponding to the two arguments of the representative consumer’s separable utility function==== ====
 ====, where ==== is a function aggregating ==== differentiated goods into a single composite good and ==== is the quantity of a numeraire good, the composition of which is left implicit.==== ==== Here, we want to go a step further, by assuming separability of ==== itself:==== ====
 ====, meaning that the set of the ==== differentiated goods can be partitioned into ==== groups, aggregated each into a composite good through the corresponding aggregator function ==== ( ====).====We could of course continue to use the same equilibrium concept, ignoring the fact that the consumer’s preferences have more structure now. However, if we suppose that the goods in each group are traded in the same ====, we can exploit the additional separability by redefining the oligopolistic equilibrium concept in a way that reduces the range of firms’ conjectures and simplifies their calculations. The partition of the oligopolistic sector into relevant markets is indeed the basis for the formation of conjectures by the actors about their competitors’ actions and their general environment. These conjectures are detailed when the competitors are close rivals or partners, but tend to become coarser and coarser as they cover more and more distant activities. Hence, the actors’ conjectures are naturally structured by this partition, which determines the level of aggregation of the required information. In the terminology we shall use, the partition of the oligopolistic sector into relevant markets is identified with the separability of sub-utility ==== into ==== of goods.==== ==== We keep Dixit–Stiglitz two-==== distinction when referring to the separability between the numeraire good ==== and the differentiated goods ====.====Our basic concept of oligopolistic equilibrium supposes that firms behave strategically in price-quantity pairs, maximizing profits under two constraints, on market share and on market size. Taking advantage of the partition of the oligopolistic sector into relevant markets, the market share constraint of each firm will concern the conjectured actions of the sole group of competitors acting in the corresponding relevant market. Also, the market size constraint will involve less informational requirements: conjectures about quantities in other groups are not part of them and in addition, under homothetic separability, only conjectures about group-specific price indices are required.====Different degrees and types of separability will be exploited, with two different orientations. One is to make more apparent the general equilibrium structure of the model by considering groups of goods that are linked by close relations of substitutability or complementarity, with simplified interactions between groups. The other is to focus on the partial equilibrium dimension, looking more closely, within an industry, at the interaction of groups of firms characterized by similar degrees of competitive toughness.====The main output of our approach to oligopolistic behavior is to derive an equilibrium markup formula to be used to measure market conduct. The formula derived in the previous (above-mentioned) work has already been used in an empirical application. Sakamoto and Stiegert (2018) study sales data of ground coffee in the US in order to evaluate, using this formula, the market conduct of the main brands, clustered into two groups, identified to the dominant group and the competitive fringe but distinguished on the basis of preferences separability. They find that “the methodology is not burdensome to implement empirically because its primary requirements are estimates of elasticities of substitution”. The objective of the present paper is to make the methodology even less burdensome, by restricting the relevant market of the competitors. As compared to the New Empirical Industrial Organization (NEIO) approach when extended to differentiated products (==== Nevo, 1998), our approach is more parsimonious in the parameter space: the number of conduct parameters increases linearly with the number of goods and not with the square of the number of goods. But it keeps the flexibility of the NEIO approach, since the conduct parameters to be estimated are continuous, in contrast to the so-called “menu approach” where a menu of models to be tested (say Bertrand vs. collusion) is fixed in advance. As Schmalensee (2012) points out, “the best way forward may be to attempt to develop and employ parsimonious parameterizations in the spirit of the ‘conjectural variations’ approach that can provide reliable reduced-form estimates of the location of conduct along ‘the in-between range of incomplete collusion”’ (p.172).====The consequences of weak separability in simplifying quantity conjectures are analyzed in Section 2. Those of stronger – homothetic – separability in further simplifying price conjectures are examined in Section 3. Section 4 introduces the idea of approximating oligopolistic equilibria by supposing that each firm forms a conjecture about the income to be spent in its group as if this group were independent. Section 5 considers the way different groups interact within the same industry. We formulate some concluding remarks in Section 6.",Exploiting separability in a multisectoral model of oligopolistic competition,https://www.sciencedirect.com/science/article/pii/S0165489620300287,19 March 2020,2020,Research Article,188.0
Navarro Florian,"LEM UMR 9221, France","Received 17 January 2019, Revised 7 February 2020, Accepted 7 February 2020, Available online 22 February 2020, Version of Record 6 March 2020.",https://doi.org/10.1016/j.mathsocsci.2020.02.001,Cited by (5),"A cooperative game with transferable utility is a situation in which players can generate worth by cooperating and obtain a certain payoff via a sharing rule. In this paper we assume that the cooperation between players is restricted by an acyclic graph. We introduce new sharing rule for this type of game. We also offer an axiomatization of this sharing rule, based mainly on linearity and a consistency axiom. For unanimity games, this sharing rule identifies the center of the graph.","Cooperation restricted games structured by a graph have first been introduced by Myerson (1977). In his article, he extended the Shapley Value (1953) to situations in which communication was incomplete. In this framework, direct communication between two players is feasible only if an edge is linking these two players. Coalitions can only form between connected players. This work set the path for numerous different extensions of the Shapley value on communication structures. We can cite, among others, the Position value (Borm et al., 1992), its generalization (Ghintran, 2010) and two sharing rules introduced by Hamiache, 1999, Hamiache, 2004. Another popular sharing rule defined on the same framework is the Average Tree solution proposed by Herings et al. (2008). The restriction of communication was also studied using other frameworks. Coalition structures (Aumann and Dreze, 1974) partition players into pre-determined coalitions that can cooperate. Conference structures (Myerson, 1980) allow for a more flexible framework in which players can belong to several conferences, and can be modeled as an hypergraph. Permission structures (van den Brink and Gilles, 1996) restrict cooperation by the positions of the players in a hierarchical framework. Games on antimatroids were also developed (Algaba et al., 2003). In the present work we will focus solely on communication structures as introduced in Myerson (1977). Similarly to the work of Herings et al. (2008), we will focus on acyclic graphs.====As surveyed by Thomson (2006), the concept of consistency has played a fundamental role in the axiomatic approach in game theory, economics and political science. This concept is built upon the idea that the sharing rule considered must give the same payoff when applied to a game and some modification of that game. In one of his works (Hamiache, 2001), Hamiache builds in this fashion a game associated to the original game and shows that the associated game converges to an inessential game, which is then solved by the use of the axioms of inessentiality and continuity. This way, he characterized the Shapley value by relying mainly on consistency and inessential games. Hamiache and Navarro (2019) recently extended the result of 2001 to games with restricted communication and offered a new sharing rules characterized using the same axioms used in 2001 to characterize the Shapley value.====In a previous work, Hamiache (1999) already relied on consistency, used in a very specific manner. He introduced an associated game that depends on the sharing rule itself. In addition to the use of other axioms, finding the fixed point in the resulting equation gave the formula for a new sharing rule without relying on convergence and continuity.==== ==== The present work builds upon the work of 1999. We use the axioms of linearity, independence of irrelevant players==== ==== and associated consistency already used by Hamiache. In addition, we use an axiom of initial conditions (similar in spirit to the Standardness axiom used by Hart and Mas-Colell (1989)) and efficiency. Moreover, we define a different associated game which allows us to characterize a (widely) different sharing rule than the ones existing in the literature.====Since its introduction by Hamiache in 1999 and 2001, the axiom of associated consistency has been proven a powerful tool for characterizing different sharing rules of the literature. Hwang (2006) used it to characterize the equal allocation of non-separable costs. Driessn (2010) proposed a characterization of the family of symmetric, efficient and linear sharing rules and Xu et al. (2013) characterized the Equal Surplus Division value. The ability to tailor the associated game in order to offer characterization of different sharing rules gives to this principle its flexibility and usefulness.====The problematic of sharing rules on games with communication structures is closely related to the concept of centrality in graphs. In their article, Gomez et al. (2003) investigate centrality with a game theoretical approach. Their approach is similar to ours, however, whereas the present work uses game theoretical axioms to characterize a known centrality concept (the center), their article explores properties of a centrality measure build upon the Shapley value that are linked to centrality issues.====In an unanimity game over the grand coalition, the only asymmetry present is induced by the graph, since all players are equal in the game. Hence, sharing the generated worth of ==== between the players is then similar to measuring the position of the players in the graph, ==== measuring their centrality. This issue of centrality measures is all the more obvious if we consider (as we do in this article) of linear sharing rules since finding a solution on unanimity games leads to a solution for all games (Hamiache, 1999). The simplest method used to measure centrality is to identify the center of the graph. In this work we introduce a sharing rule for games on acyclic graph which, when applied to unanimity games, gives the same payoff to all players in the center of the graph and gives nothing to players outside the center. Hence we are able to offer a definition of the center of a graph in a cooperative game theoretic approach relying mainly on linearity and consistency. We also prove that the axioms used in the characterization are independent. The results we obtain are widely different from those provided by other sharing rules proposed in the literature such as the Average tree value (Herings et al., 2008) and the Myerson (1977) in that they give more importance to the information from the graph over information from the game.",The center value: A sharing rule for cooperative games on acyclic graphs,https://www.sciencedirect.com/science/article/pii/S0165489620300172,22 February 2020,2020,Research Article,189.0
Rouskas Evangelos,"Section Microeconomics, Department of Regional and Economic Development, Agricultural University of Athens, Greece","Received 24 October 2018, Revised 15 November 2019, Accepted 7 February 2020, Available online 13 February 2020, Version of Record 25 February 2020.",https://doi.org/10.1016/j.mathsocsci.2020.02.002,Cited by (1),"In this paper, I propose a model that can support Varian’s (Varian, 1980) equilibrium search behavior with an arbitrarily large number of sellers, even when the first price observation is costly for the consumers with positive search costs and the search is endogenous. In my model, the consumers with zero search costs have the same low valuation for all sellers’ products, whereas the consumers with positive search costs (i) learn the price and their valuation for the product of each seller after engaging in costly search for the corresponding seller; and (ii) in the pre-search phase, anticipate that the valuation for the product of each seller is high with strictly positive and lower than unity exogenous ","The equilibrium in Varian’s model of sales (Varian, 1980) is characterized by the search behavior whereby consumers with zero search costs observe all prices in the market, whereas consumers with positive search costs observe one price with probability one by visiting a seller at random. Janssen and Moraga-González (2004, Proposition 3(ii)) showed that this search behavior cannot be sustained in equilibrium with an arbitrarily large number of sellers if the first price observation is costly for the consumers with positive search costs, and the search is endogenous. The rationale for this nonexistence result is that the expected price is increasing monotonically with the number of sellers, and, consequently, even when the search cost is very low, there exists a critical number of sellers above which the expected price is prohibitively high for the consumers with positive search costs.====In this paper, I present a version of Varian’s model with endogenous search that does not give rise to the aforementioned nonexistence result. My key assumption is that, in the pre-search phase, each consumer with positive search costs faces uncertainty with regard to the valuation for the product of each seller; the valuation is high with strictly positive and lower than unity exogenous probability and low with the remaining probability. Whereas the valuation of the consumers with zero search costs is always low.==== ==== In my version, for some parameters, the expected price does not exceed the low valuation, irrespective of the number of sellers. This can render searching one price optimal for the consumers with positive search costs, even though the number of sellers may grow arbitrarily large.====For instance, consider a real-world online platform where consumers enter to buy an item. The consumers may differ in that some of them may rate highly, for example, either the quick delivery time or the very good packaging of the item. Those consumers with either time-delivery or packaging concerns will enjoy a high gross benefit if they buy the item from a seller that satisfies their requirements, and will receive a low gross benefit otherwise. To find out which seller will meet their preferences in the best way, the consumers can consult online reviews for each seller. Hann and Terwiesch (2003) showed that there exist frictional costs in online environments. This means that the process of searching online for the price of each seller and consulting online reviews for each seller may be costly. My theoretical setting analyzes such situations.==== The present paper belongs to the literature on consumer search. The reader is referred to Baye et al. (2006) and Anderson and Renault (2018) for surveys of this literature. Ding and Zhang (2018) adopt a similar—at least in certain aspects—framework. These authors assume that some consumers know their valuation for the product of each seller, whereas some other consumers have to engage in costly search to learn their valuation. Furthermore, with some probability, the valuation for the product of each seller is high, and with the remaining probability the valuation is low.==== ====The paper proceeds as follows. Section 2 presents the model, Section 3 provides the analysis, and Section 4 concludes. The proofs are presented in the Appendix.",A model of sales with a large number of sellers,https://www.sciencedirect.com/science/article/pii/S0165489620300184,13 February 2020,2020,Research Article,190.0
"Mizukami Hideki,Wakayama Takuma","College of Economics, Aoyama Gakuin University, 4-4-25 Shibuya, Shibuya-ku, Tokyo 150-8366, Japan,Faculty of Economics, Ryukoku University, 67 Tsukamoto-cho, Fukakusa, Fushimi-ku, Kyoto 612-8577, Japan","Received 30 January 2019, Revised 27 January 2020, Accepted 27 January 2020, Available online 1 February 2020, Version of Record 20 February 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.008,Cited by (3),We consider the problem where agents bargain over their shares of a perfectly divisible commodity. The aim of this paper is to identify the class of bargaining solutions induced by ,"We consider problems where agents bargain over their shares of a perfectly divisible commodity. Examples of such problems include bargaining over the share of profit between an employer and a labor union or bargaining over the distribution of property. In such situations, only when all agents agree on a feasible outcome, can they arrive at it; otherwise, they arrive at the predetermined outcome or nothing. To avoid disagreements, they may decide to follow a recommendation made by an impartial arbitrator (e.g., a central arbitration committee like that in the UK presiding over labor disputes, a judge presiding over civil trials). Axiomatic bargaining theory, initiated by Nash (1950), deals with these situations.==== ==== This theory provides numerous “bargaining solutions” that associate a profile of utility levels with each utility possibility set. As pointed out by Raiffa (1953), a bargaining solution can be interpreted as the recommendation of the arbitrator.====The shape of the utility possibility set is based on agents’ preferences. Since preferences are usually unknown to the arbitrator, selfish agents may have an incentive to gain by manipulating the bargaining solution through misrepresentation of their preferences. Thus, the arbitrator faces the problem of constructing a mechanism whose equilibrium outcomes are always outcomes the arbitrator wishes to recommend. If the arbitrator can construct such a mechanism, then the bargaining solution is said to be “implementable”.====Unfortunately, however, we cannot directly apply the standard notion of implementability to bargaining solutions. This is because the notion of implementability is usually formalized by means of “allocation rules” that are defined on the set of underlying physical outcomes, whereas bargaining solutions are defined on the utility space. Thus, we explicitly consider the underlying physical outcomes and allocation rules.==== ==== This paper focuses on the class of allocation rules satisfying ==== (each agent is indifferent among all the selected allocations) and ==== (if an allocation is chosen, then any other allocation that all agents find indifferent to it should also be chosen).====The aim of this paper is to identify bargaining solutions induced by implementable allocation rules.==== ==== To verify this, we consider ==== (Roemer, 1986, Roemer, 1988); it requires that if two preference profiles correspond to the same utility possibility set, then the allocation rule should assign to each of the preference profiles outcomes that are indistinguishable in terms of utility across the preference profiles. This condition is necessary and sufficient for any allocation rule to induce a bargaining solution.====There have been many studies on Nash or subgame-perfect implementation in the context of the bargaining problem.==== ==== Unlike previous studies, we examine dominant strategy implementation. One motivation for dominant strategy implementation is that the use of dominant strategies does not require an agent to believe that other agents will behave rationally. That is, under dominant strategy implementation, we can construct a mechanism with weak rationality assumptions.==== ====We first characterize the class of ==== allocation rules in our setting (Proposition 1). This result tells us that any ==== allocation rule must be ====. When we turn our attention to ==== allocation rules, the well-known revelation principle (Proposition 2) states that ==== implies ==== (truth-telling is a dominant strategy for everyone). Moreover, given that an allocation rule is ====, we establish that dictatorial rules are the only allocation rules satisfying ====, ====, and the two auxiliary conditions called ==== and ==== (Proposition 3). By invoking these results, it turns out that only dictatorial solutions are induced by ==== allocation rules satisfying ====, ====, and ==== (Theorem 1). Our dictatorial results are similar to the well-known Gibbard–Satterthwaite theorem (Gibbard, 1973, Satterthwaite, 1975). Since the Gibbard–Satterthwaite theorem cannot be applied to restricted preference environments such as ours, our dictatorial results are not a by-product of this theorem. In fact, in our setting, there is a non-dictatorial rule satisfying ====, ====, and ====. However, such non-dictatorial rules cannot induce any bargaining solution; that is, they all violate ====. This suggests that ==== is the source of our dictatorial results.====The remainder of this paper is organized as follows. Section 2 provides basic notation and definitions. Section 3 offers a characterization of ==== allocation rules and identifies the class of bargaining solutions induced by ==== allocation rules. Section 4 concludes with some remarks. Appendix A Proof of, Appendix B Proof of, Appendix C Alternative proof of contain proofs that are omitted from the main text.",Dominant strategy implementation of bargaining solutions,https://www.sciencedirect.com/science/article/pii/S0165489620300160,1 February 2020,2020,Research Article,191.0
"González-Maestre Miguel,Peñarrubia Diego","Departamento de Fundamentos del Análisis Económico, Universidad de Murcia, 30100 Murcia, Spain","Received 1 March 2019, Revised 11 October 2019, Accepted 20 January 2020, Available online 28 January 2020, Version of Record 14 February 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.007,Cited by (0),"In a two-sector economy, we consider the endogenous determination of the level of competition in the imperfectly competitive sector, under democratic, non-manipulated voting. In the context of the Salop’s (1979) circular model, we identify conditions such that in addition to a competitive equilibrium (with most voters choosing the efficient level of competition intensity), an inefficient equilibrium is observed in which a majority of fully rational and informed citizens vote for an inefficiently low level of competition intensity. In particular, we show that under reasonable conditions, a coalition of middle-class entrepreneurs, who vote for a lower level of competition intensity, defeats a “coalition of extremes” formed by high- and low-productivity entrepreneurs and pure consumers, who vote for a higher level of competition intensity. In addition, we show that the higher the degree of similarity among entrepreneurs, the more inefficient the anticompetitive equilibrium.","Why, in some countries, is it difficult to implement structural reforms? Many studies show that institutional differences are a basic determinant of per capita income and successful economic performance. However, reformist proposals in countries with problems of corruption, lack of competition or transparency, and in general, institutional inefficiency, tend to fail as a result of inefficient legal and/or administrative process. The usual explanation is that institutional quality is not an exogenous variable but the result of a decision process which is subject to cultural, historic, and sociological conditions. In particular, there are various forms of manipulation of democratic voting, such as lobbyist activities and the control of mass media by interest groups.==== ====The concept of multiple equilibria is usually argued in the literature dealing with this issue. In particular, coordination problems might explain the so-called “development traps”, associated with the interrelation between institutions, technological choice, competition, growth, and productivity.==== ====This paper contributes to the general issue of “coordination traps” focusing on the endogenous determination of the institutional framework of competition policy. To this end, we consider a two-sector economy with a perfectly competitive sector, and an imperfectly competitive sector based on the Salop’s (1979) circular model.==== ==== A key aspect of the model is that the level of competition in the imperfectly competitive sector is determined by democratic and non-manipulated voting. Specifically, we assume that voters choose the level of a parameter reflecting the intensity of competition, given the number of competitors.====We consider two different scenarios. In the first scenario, analyzed in Section 2, all voters have the same marginal productivity in the imperfectly competitive sector but have different entry cost. In the second scenario (Section 3) the entry cost is homogeneous but marginal productivities are different across agents.====The main insights from our analysis are the following:====(i) In the two considered models, we show that, in addition to a highly competitive equilibrium (in which most voters choose the most efficient level of competition intensity), an anticompetitive equilibrium appears in which a majority of fully rational and fully informed citizens vote for an inefficient increase of this competition intensity. This is what we call an anticompetitive trap. A key aspect of this result is that, at the inefficient, anticompetitive equilibrium, the proportion of active entrepreneurs is higher than at the efficient, competitive equilibrium. (Proposition 3)====(ii) In the model with heterogeneous productivities, we show that the emergence of an anticompetitive trap is associated with a coalition of middle-class entrepreneurs, voting for lower competition, which defeats a “coalition of extremes” formed by high and low productive entrepreneurs plus pure consumers, who vote for higher competition. (Proposition 3, combined with Proposition 2b).====(iii) In addition, we show that the anticompetitive effect associated with the inefficient level of competition intensity is greater the lower the degree of entrepreneurs’ productivity dispersion. (Proposition 4).====There are three main ideas behind the intuition of these insights:====First, the anticompetitive trap associated with insight (i) relies on the fact that, because of the timing of the game, the expectations of an anticompetitive choice at the voting stage are self-fulfilling: once a sufficiently large number of potential entrepreneurs have entered the market, majority voting allowsthem to sustain the anticompetitive equilibrium. In consequence, two identical economies, with identical ex-ante parameters, might end up in two completely different equilibria, in terms of efficiency.====Second, regarding insight (ii), heterogeneous productivities mean that middle-class entrepreneurs’ interests might be in contradiction not only with consumers’ interests==== ==== but also with those of highly productive entrepreneurs. The idea is that very productive entrepreneurs prefer more competition to increase their market shares, at the expense of low and middle productivity entrepreneurs.==== ====Third, insight (iii) means that a low level of dispersion in entrepreneurs’ productivity enhances the political power of a majority of middle-class entrepreneurs with similar interests, which is associated with a lower level of competition.====Notably, in general, the appearance of inefficient results under majority voting can be considered as a natural outcome when voters’ utility functions are heterogeneous. However, our previously summarized insights indicate that the connection between the voting and entry incentives, in the context of democratic competition policy, yields non-obvious results like multiple equilibria, “coalition of extremes”, and comparative statics linking entrepreneurs’ heterogeneity with the degree of equilibrium inefficiency.====A crucial assumption behind our results is that the decision on entry, by potential entrepreneurs, occurs before the voting stage, which is previous to the price competition among active entrepreneurs. The plausibility of this timing, against an alternative timing in which voting occurs before entry, is controversial. However, we posit that our assumption is reasonable. Usually, the decisions on entry are long-run decisions such as financial investments, or the acquisition of human capital and professional skills, which, under reasonable conditions, cannot be changed as quickly as many aspects of the decisions on competition policy. Nevertheless, Appendix A extends the analysis of the model of Section 2 in an alternative setting in which the voting stage is previous to the entry stage. Under this condition, we demonstrate the existence of a unique efficient equilibrium.==== ",Anticompetitive traps and voting,https://www.sciencedirect.com/science/article/pii/S0165489620300159,28 January 2020,2020,Research Article,192.0
"Bahar Gal,Smorodinsky Rann,Tennenholtz Moshe",Technion—Israel Institute of Technology,"Received 19 June 2019, Revised 18 December 2019, Accepted 8 January 2020, Available online 18 January 2020, Version of Record 8 February 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.006,Cited by (4),"We consider social learning where agents can only observe part of the population (modeled as neighbors on an undirected graph), face many decision problems, and arrival order of the agents is unknown. The central question we pose is whether there is a natural observability graph that prevents the information cascade phenomenon. We introduce the ‘celebrities graph’ and prove that indeed it allows for proper information aggregation in large populations even when the order at which agents decide is random and even when different issues are decided in different orders.","When making decisions on various topics, we often turn to see how our colleagues, friends and family members decide in similar circumstances. By doing so, we harness the collective knowledge with the hope of making better informed decisions. Such decisions pertain to multiple issues, such as the choice of a restaurant, a mortgage plan, a service provider and so on. Anytime we face a new decision problem, we turn to look at those among our friends who have already faced a similar dilemma and have made a decision. As the order by which decisions are made differ from one issue to another, it may well be the case that for an individual agent, the relevant circle of influence changes from one issue to another.====Learning from what others do introduces an inherent trap, known as an ‘information cascade’. Under an information cascade, an initial set of agents is, due to sheer misfortune, ill-informed. As a result, these agents take an inferior action. Subsequent agents are then convinced that the aforementioned action is optimal (“how can so many agents be wrong?”) and so dismiss their own private information and follow the herd by taking the inferior action as well.====The herding literature typically (and implicitly) assumes all agents are familiar with each other in the sense that an agent always observes the choice of action made by ==== its predecessors in this sequential decision-making process. In reality, however, this is hardly the case and we mostly view only a subset of predecessors. This turns out to be key in circumventing the information cascade problem.====Whereas partial observability seems to diminish the risk of an information cascade, it introduces a new difficulty to social learning. If people observe less peers, then there may be no hope for the collective knowledge to be aggregated and for learning to occur. Consider an extreme case, when agents never observe each other. Obviously no information cascades will occur yet no one can enjoy the collective knowledge. As a result, many agents are bound to take an inferior action. In this paper, we identify a natural observability graph that is sufficiently sparse to avoid information cascades but is also sufficiently dense to allow aggregation of information.====In fact, the observation graph we identify, allows for simultaneous learning over multiple issues. Each issue is associated with its optimal action and with a (random) order according to which people make their decision. The latter suggests some inherent difficulties: First, agents only observe part of their neighbors, those who preceded them in the realized order. They do not observe neighbors who decide later nor do they observe anything about the neighbors of their neighbors. For each of the neighbors they do observe, they only see her action and nothing else. Second, the same graph must be relevant to all issues and all realized orders. In particular, it cannot be designed in an online manner to fit the realized order at which agents choose (for more on this, see Section 1.2).==== ",Multi-issue social learning,https://www.sciencedirect.com/science/article/pii/S0165489620300147,18 January 2020,2020,Research Article,193.0
"Béal Sylvain,Moyouwou Issofa,Rémila Eric,Solal Philippe","Univ. Bourgogne-Franche-Comté, CRESE EA3190, 30 Avenue de l’Observatoire, 25009 Besançon, France,École Normale Supérieure, Université de Yaoundé I, BP 47 Yaoundé, Cameroun,Université de Saint-Etienne, CNRS, GATE L-SE UMR 5824, F-42023 Saint-Etienne, France","Received 9 May 2019, Revised 6 November 2019, Accepted 8 January 2020, Available online 16 January 2020, Version of Record 31 January 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.005,Cited by (5),"A situation in which a finite set of agents can obtain certain payoffs by cooperation can be described by a cooperative game with transferable utility, or simply a TU-game. In the literature, various models of games with restricted cooperation can be found, in which only certain subsets of the agent set are allowed to form. In this article, we consider such sets of feasible coalitions that are closed under intersection, i.e., for any two feasible coalitions, their intersection is also feasible. Such set systems, called intersection closed systems, are a generalization of the convex geometries. We use the concept of ","In its classical interpretation, a cooperative TU-game describes a situation in which the members of each coalition can cooperate to form a feasible coalition and earn its worth. Cooperative games on set systems are cooperative games in which the agents have restricted cooperation possibilities, which are defined by a set system. The first model in which the restrictions are defined by the connected subgraphs of a graph is due to Myerson (1977). Since then, many other set systems have been used to model restricted cooperation: Algaba et al., 2000, Algaba et al., 2001 study cooperative games on union stable systems, Algaba et al. (2004) consider TU-games on antimatroids, Bilbao (1998) and Bilbao and Edelman (2000) introduce TU-games on convex geometries, Bilbao (2003) studies TU-games on augmenting set systems, and Lange and Grabisch (2009) consider TU-games on regular set systems. More recently, van den Brink et al. (2011) consider TU-games on union closed systems, Algaba et al. (2018) introduce TU-games on accessible union stable systems and Algaba et al. (2019) study TU-games on voting structures. All these models are relevant to apprehend many applications as underlined by Moretti and Patrone (2008) and van den Brink (2017), among others. Bilbao (2000) collects important results on several such combinatorial structures.====The purpose of this article is to study TU-games on intersection closed systems. A set system is intersection closed if the intersection of two feasible coalitions is still feasible. Every convex geometry is intersection closed, but an intersection closed system is not necessarily a convex geometry. Intersection closed systems appear naturally in various contexts: in a matching problem, the set of coalitions for which each member also has her partner in the coalition is intersection closed, the set of connected coalitions of nodes in a cycle-free graph is intersection closed, and the set of losing coalitions in a voting game is intersection closed. To the best of our knowledge, Bilbao and Edelman (2000) is the unique work involving intersection closed systems. They focus on set-valued solution concepts while we investigate (single-valued) allocation rules.====Section 2 is a preliminary section in which we consider a new basis for the set of TU-games. In most cases, the basis of upper TU-games, also called unanimity TU-games, is used to analyze cooperative games on set systems. Here, the lower TU-games prove much more appealing. Section 3 presents TU-games on intersection closed systems. Section 4 introduces the restricted TU-games on intersection closed systems through a closure operator. We determine the image and the kernel of its operator. In particular, we provide a basis for the image of this operator in terms of lower games and a basis for the kernel of this operator in terms of Dirac TU-games. We then provide a formula to compute the coordinates of the restricted TU-games in the basis of lower TU-games. We also derive an expression of the closure operator in the basis of lower TU-games. Section 5 provides an axiomatic study of allocation rules for TU-games on intersection closed systems. We focus on allocation rules that are obtained by applying to the restricted TU-games an allocation rule for classical TU-games. An instance of such allocation rules is the allocation rule defined as the Shapley value (1953) on the restricted TU-game. We call it the Intersection rule. We combine three types of axioms. Some axioms describe the effect of the intersection closed system on the allocation rule. Some axioms describe the impact of the coalition function on the allocation rule. Finally, we invoke an axiom of consistency, which is specific to the targeted allocation rule. The first axiomatic result is a characterization of the Intersection rule that can be considered as close as possible to the classical characterization of the Shapley value. The second axiomatic result characterizes any allocation rule obtained by applying to the restricted TU-games an efficient and additive allocation rule for classical TU-games. As a particular case, we thus obtain a second characterization of the Intersection rule. These results exploit the properties found in Section 4.",Cooperative games on intersection closed systems and the Shapley value,https://www.sciencedirect.com/science/article/pii/S0165489620300135,16 January 2020,2020,Research Article,194.0
"Alcalde-Unzu Jorge,Echavarri Rebeca,Husillos Javier","Department of Economics, Public University of Navarre, Spain,Institute for Advanced Research in Business and Economics (INARBE), Public University of Navarre, Spain","Received 7 May 2018, Revised 26 December 2019, Accepted 8 January 2020, Available online 16 January 2020, Version of Record 24 January 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.002,Cited by (3),"Discrimination against born and unborn females is a well-documented phenomenon in countries such as India, China, Taiwan or Korea. Empirical studies support both additive and substitutive relationships between prenatal and postnatal discriminatory practices against females. We introduce a theoretical evolutionary model that endogenizes the preference for sons in a society, and consequently, can explain why one type of relationship or the other emerges in a society.","Sex-based discrimination against female children has led to an alarming decline in the number of young females in many countries, but mainly in the Asia-Pacific region. Since Rosenzweig and Schultz (1982) found that female children receive less educational and health endowments in some communities than their male counterparts, a wide range of studies have confirmed the practice of postnatal discrimination against young females (Bhalotra, 2010, Echavarri and Husillos, 2016, Oster, 2009, Qian, 2008). Females also experience discrimination even before birth through the practice of sex-selective abortion of female fetuses (Echavarri and Ezcurra, 2010, Kim, 2005, Lin et al., 2014). The impact of prenatal and postnatal discrimination has modified the overall population’s statistics. According to data from the United Nations Population Division (2018) – compiled by the World Bank – the 2016 worldwide ratio of male births to female births (====) was 1.5 percent higher than in ====. Further, this 2016 ratio is 1.07, which is greater than the biologically expected figure of 1.05. This sharp increase in the sex ratio at birth has been mainly observed in Asia. For instance, this ratio has increased by 4.5 percent in the eastern Asia-Pacific region and 3.5 percent in South Asia. The increase in the sex ratio at birth has been of particular concern in China, where it stands at 7.6 percent. However, this phenomenon is not confined to developing countries, as the sex ratio at birth in upper-middle-income countries has increased by 3.3 percent between 1962 and 2016.====The prenatal and postnatal dimensions of discrimination are intricately related. However, controversial empirical evidence exists regarding the nature of this relationship. On the one hand, Goodkind’s (1996) pioneering study shows how, in China during the late 1970s and the 1980s, there was a decrease in postnatal discrimination as well as a substantial increase in prenatal discrimination derived from access to sex-selective abortion technologies (this is called a ==== relationship between prenatal and postnatal discrimination). On the other hand, Goodkind (1996) finds an increase in both postnatal and prenatal discrimination in India during almost the same period (this is called an ==== relationship between prenatal and postnatal discrimination). Recent literature has documented both types of relationships (see, for instance, Lin et al., 2014, and Echavarri and Husillos, 2016, for substitutive relationships, and Nandi, 2015, for a non-substitutive relationship).====Lin et al. (2014) provide the only formal model to our knowledge that captures the relationship between prenatal and postnatal discrimination.==== ==== This model can only explain the substitutive relationship between both discriminatory practices with the following transmission channel: the diffusion of prenatal sex-detection technologies leads couples who would have otherwise neglected a girl after birth to terminate a female pregnancy.====The model developed by Lin et al. (2014) is based on the assumption of exogenous – and therefore, static – preferences. However, previous literature shows that dropping this assumption uncovers the coexistence of multiple equilibria in most models and the conjunction of additional transmission mechanisms. Two different evolutionary approaches have been proposed to examine the changes in gender-related preferences. One modeling avenue considers preference changes as the result of a Bayesian learning process about ==== preferences. Following this approach, Fernandez (2013) explains the geographical diversity in gender inequality in labor markets. The existence of true preferences is a controversial assumption in some scenarios, as it is the case of preferences for sons over daughters. The second modeling avenue departs from the true-preferences assumption and extends evolutionary biological models of natural selection to account for cultural transmission (see the pioneering model by biologists Cavalli-Sforza and Feldman (1981)). Following this route, Hiller and Baudin (2016) develop a model ==== Bisin and Verdier (2001) to explain the diversity in gender roles in different societies, and Fogarty and Feldman (2011) model the evolutionary dynamics of the perceived value of daughters in the society. Specifically, in Fogarty and Feldman’s (2011) model, sex-biased preferences in the society might spread or erode depending on cultural traits, while these preferences would simultaneously affect cultural traits. This paper follows Fogarty and Feldman’s (2011) explanation for the evolution of societal sex-biased preferences: we consider a variable called ==== that reflects the perceived value of sons over daughters in the society, and we use the levels of prenatal and postnatal discrimination as the cultural traits.====Boyd and Richerson (1985), and thereafter Bowles (1998), model cultural transmission processes using the replicator dynamics, which provides a more intuitive and explicit mechanism to capture the influence of past values of one variable on its present value as the result of the natural selection process. We follow this approach and propose a system of replicator dynamics equations that include both the impact of societal son preference on the evolution of the behavioral discriminatory practices and the impact of these practices on the evolution of societal son preference.====Therefore, the key aspect of our model is the endogenous determination of societal son preference, which depends on the diffusion of discrimination against born and unborn females. Consequently, the diffusion of prenatal sex-detection technologies has two effects on postnatal discrimination against females. On the one hand, there is the same transmission channel of Lin et al. (2014) that favors the emergence of a substitutive relationship. On the other hand, a new force favors an additive relationship: the increase in prenatal sex discrimination could strengthen the societal son preference, which would increase the predisposition to discriminate against daughters. The strength of each force will determine the relationship that emerges in each society. By doing so, our paper contributes to the literature by presenting a simple theoretical evolutionary model that characterizes the context for the emergence of each type of relationship between prenatal and postnatal discrimination.====Understanding the dynamics of discrimination has substantial policy implications. For instance, consider a society that has experienced a decrease in postnatal discrimination. Our model would help to determine: (====) if this decrease is a result of an erosion in the societal son preference, or (====) if societal son preference remains invariant, and the observed decrease in postnatal discrimination is caused by the substitution of postnatal by prenatal discrimination.====Additionally, our findings show that there are scenarios in which the rise of sex-selective abortions might legitimize discriminatory practices against females, thus increasing the societal son preference, which results in further discrimination in the postnatal period as well. These results become relevant with the diffusion of inexpensive technologies to detect the sex of the fetus (====, amniocentesis or chorionic villus sampling ultrasound) that make prenatal discrimination a feasible option in many communities.====The remainder of this paper is organized as follows. Section 2 introduces the evolutionary model and the solution concept that we use: institutionalized social states. Section 3 studies the impact of the access to prenatal sex-detection technologies on the institutionalized social states. Section 4 presents some concluding remarks, while the appendices include complementary results and proofs.",An evolutionary model of prenatal and postnatal discrimination against females,https://www.sciencedirect.com/science/article/pii/S016548962030010X,16 January 2020,2020,Research Article,195.0
"Kajanovičová Viktória,Novotný Branislav,Pospíšil Michal","Basta digital s.r.o., Námestie SNP 30, 811 01 Bratislava, Slovak Republic,Mathematical Institute, Slovak Academy of Sciences, Štefánikova 49, 814 73 Bratislava, Slovak Republic,Department of Mathematical Analysis and Numerical Mathematics, Faculty of Mathematics, Physics and Informatics, Comenius University in Bratislava, Mlynská dolina, 842 48 Bratislava, Slovak Republic","Received 7 August 2019, Revised 23 December 2019, Accepted 8 January 2020, Available online 16 January 2020, Version of Record 8 February 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.004,Cited by (1),Ramsey model is a neoclassical model of economic growth. It describes the time evolution of capital and consumption in a closed economy with the ,"Ramsey model is a problem in optimal control theory, in which one maximizes the utility of consumption that is tied to a capital via a dynamic constraint. Classically, the equation is as follows (see e.g. part 1.2 Aghion and Howitt, 1998): ====where ==== is capital per worker, ==== is consumption per worker, ==== is the depreciation rate of capital, ==== is ==== population growth rate and ==== is the production function representing the output of the economy per worker for a given ====. The simplest choice of the function ==== is ==== with a constant ====. This leads to a very simple behavior of the economy, by which we mean the time evolution of the pair ====. In Guerrini (2010d) Guerrini showed that the behavior of the economics is more interesting, when one introduces logistic population growth. He provided closed form solution for this model and investigated asymptotic behavior of its solution. In Guerrini (2010a) he provided solution for Ramsey model with von Bertalanffy population growth. In works Guerrini, 2010b, Guerrini, 2010c he generalizes the model to a setting with more general population growth. He assumes that the growth rate is bounded in the sense that ==== and also there exists a limit ====. On the other hand in works Ferrara, 2011a, Ferrara, 2011b Ferrara solves simpler model, namely Solow model with similarly general population growth. He also assumes that the growth rate is bounded and convergent, but he allows population to be decreasing, i.e. ====.====In the mentioned works about the Ramsey model, author jumps rather quickly from the formulation of the problem in optimal control theory to solving the associated dynamical system arising from the Pontryagin’s principle. In our work we are linking these two aspects in more detail. This is especially needed since in our solution of Ramsey model, we are not assuming that the population growth is bounded nor convergent. The population can be increasing or decreasing as long as it stays positive. This leads to a possibly diverging utility which has to be addressed.====The paper is organized as follows. In the next section, we recall a simple form of the Pontryagin’s maximum principle used later to solve the Ramsey model, we define basic notions and recall known results on optimal paths. In Section 3, we establish Ramsey model for a general population growth and derive its solution. Section 4 is devoted to study of asymptotic properties of solutions of the Ramsey model with particular types of populations; especially a generalized logistic growth, which generalizes Guerrini (2010d), and a periodic population evolution, which was not yet covered.",Ramsey model with non-constant population growth,https://www.sciencedirect.com/science/article/pii/S0165489620300123,16 January 2020,2020,Research Article,196.0
"Cao Jiyun,Sinha Uday Bhanu","The School of Economics, Nankai University, Tianjin, China,Collaborative Innovation Center for China Economy, Tianjin, China,Department of Economics, Delhi School of Economics, New Delhi, India","Received 20 June 2018, Revised 25 November 2019, Accepted 8 January 2020, Available online 14 January 2020, Version of Record 12 March 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.001,Cited by (2),The existing literature has considered licensing of a patented cost-reducing innovation either in a homogeneous good ,"The literature on licensing of patented innovation in oligopoly market structure is quite vast. The main focus of this literature is the optimal licensing contract in terms of auction, fixed fee, royalty and two-part tariff in an oligopoly market with one firm being the owner of the patented innovation (either an insider when it competes; or an outside innovator when it does not compete in the product market) and a number of other firms competing in the market under Cournot or Bertrand.====The literature on patent licensing under oligopolistic competition is broadly divided into two classes: homogeneous goods and differentiated goods industry. Thus, we have two classes of models in the literature where one class assumes the characteristics of the goods produced by all firms are homogeneous and another class assumes that goods produced by all firms are differentiated from each other. The main results on optimal licensing contracts are shown to be different by different authors. In the context of homogeneous good oligopoly, the most notable papers are Rockett (1990), Wang (1998) and Kamien and Tauman (2002) and the royalty licensing is shown to be optimal. Sen and Tauman (2007) showed the auction plus royalty as the optimal licensing contract in a homogeneous good Cournot oligopoly model with both insider and outsider patentee. Poddar and Sinha (2010) introduced the initial cost asymmetry in the pre-innovation stage and showed that fixed fee, royalty and two-part tariff can each be chosen as an optimal contract depending on the parameters in a homogeneous good Cournot duopoly model. On the other hand, in the case of differentiated goods oligopoly some of the main contributions are Muto (1993), Wang and Yang (1999), Mukherjee and Balasubramanian (2001), Faulí-Oller and Sandonís (2002), Sandonís and Faulí-Oller (2008), Faulí-Oller et al. (2013) and Bagchi and Mukherjee (2014). In particular the two-part tariff licensing contract was shown to be optimal by Faulí-Oller and Sandonís (2002) in the context of differentiated goods. The similar results are shown by others with some variation in the context of Cournot or Bertrand competition with differentiated products or with insider or outsider patentee.====As far as we know there is no paper which brings the market competition between homogeneous and differentiated goods producers together and analyzes the problem of licensing in such market. However, in reality we always find some goods are very close substitutes between them whereas some goods are more differentiated in the same market. For example, the goods of mineral water are very close substitutes between them whereas they are more differentiated with Coca Cola in the soft drink market; similarly, some red wines are also close substitutes between them whereas they are more differentiated with strong liquors in the hard drink market. We can also find that Chinese patent medicines for a disease are close substitutes between them whereas they are more differentiated with chemical medicines for the same disease. In fact one can look around and see that this kind of market structure where some goods are very close substitutes and some others are more differentiated is a feature of almost all markets. Unfortunately, there is very little analysis of this sort in economic theory and certainly none in technology transfer context.==== ==== It is also true that technology transfer would be more likely between very close substitute goods producers (because of close production technologies) than between highly differentiated goods producers. Thus, we deal with the technology transfer problem between producers of similar goods (due to the close production technologies) in the presence of other differentiated goods produced by other firms in an oligopolistic industry. This is the novel feature of our model as compared to all other models in the literature.====Our model is simplified to capture these phenomena by assuming that the goods with small degree of product differentiation are homogeneous (say good or brand X) and there exists a differentiated good (say Y). In such context we consider the technology transfer between the producers of brand X where they compete with other producers of another differentiated brand Y. Specifically, in order to capture this idea in the simplest possible way, we consider four firms competing under Cournot conjecture in a market where two firms produce good X and the other two firms produce a differentiated good Y. Then we focus on the problem of technology transfer if one of the producers of brand X is also an innovator having a process innovation which is cost-reducing type and can be licensed to the other producer of brand X only. This may be due to the fact that producers of good Y are located in another country with weak patent protection, or due to a high international licensing cost, or due to the possibility that the cost-reducing innovation is only compatible for adoption for the production of good X but not for the production of good Y. Thus, we consider the environment where the producers of brand X use a similar production technology and the producers of brand Y are producing a differentiated good by using a different production technology or platform of production where this new technology is not compatible for adoption. This is the novel feature of our model where the licensing is possible only between the producers of homogeneous good X while it is not possible to any producer of differentiated good Y. This is a significant departure from the conventional models of licensing in differentiated goods industry where the licensing is possible across all brands of the product and the focus of the existing literature is to analyze the optimal licensing contract when the degree of product differentiation varies in such environment. Thus, we add a new perspective to this patent licensing literature by considering both homogeneous and differentiated goods in the same model.====To develop the idea, we consider an oligopoly market with four firms and two varieties where each variety is produced by two firms. In this theoretical framework, we analyze optimal licensing contract between two producers producing the homogeneous good X in the presence of a differentiated good Y produced by another two firms. We consider both possibilities: when the innovator is also a competitor in the market and the other case when the innovator is an R&D lab and does not take part in production and competition in the market. Then we also compare the incentive for innovation for the innovator as an insider to that of being an outsider.====Another way to motivate the paper is that it provides a rationale for the existence of variety of licensing contract in reality with the help of a single model.==== ====
 Wang (1998) established the optimality of royalty contract in a Cournot duopoly with homogeneous goods. In another contribution, Wang (2002) analyzed the optimal licensing in a Cournot duopoly with differentiated goods with one of the firms having a cost-reducing innovation. In Wang (2002) only fixed fee and royalty licensing contracts are considered and it is established that depending on parameter configuration either contract can be superior for the patent-holding firm and even drastic technology would be licensed. There exists a large literature on patent licensing in oligopoly market but they have either considered all producers produce homogeneous goods or all producers produce differentiated goods. In our model licensing of cost-reducing innovation is possible only between producers of a single variety. We focus on optimal patent licensing contract and analyze how the degree of product differentiation interacts with the licensing contract that is agreed upon between the two firms producing homogeneous goods. Within the framework of a single model we show that fixed fee, royalty and two-part tariff can each be chosen as an optimal contract depending on the parameters. More specifically, in the presence of a differentiated brand, if the innovation is non-drastic, the optimal intra-brand patent licensing contract involves only royalty when the degree of inter-brand differentiation is high, a two-part tariff when it is intermediate, and a fixed fee alone when it is low. If the innovation is drastic, the optimal intra-brand patent licensing contract is only a fixed fee when the degree of inter-brand differentiation is low and a two-part tariff otherwise. Thus, our model also generates the empirically observed variation of licensing contracts in the form of fixed fee, royalty and two-part tariff.==== ====The presence of a differentiated brand is the key in our model and the degree of product differentiation impacts on the form of licensing contract. When the innovator is also a producer in the market there is an interesting trade-off for the innovator i.e., the benefit of obtaining licensing revenue and the cost of enhanced competition from the rival licensee. This feature is common to all other related models in the existing literature and patent licensing decisions are generally investigated as the degree of product differentiation changes. However, the existing framework of differentiated goods only allows for changes in the degree of product differentiation for all firms including the licensees that receive the cost-reducing innovation. Thus, while studying the increase in product differentiation the adverse effect of competition enhancing effect on the innovator becomes lower thus making it attractive for the innovator to license its innovation. In contrast, in our model the trade-off between licensing revenue and competition enhancing effect are interlaced with the existence of other two firms producing a differentiated brand but they do not take part in licensing and the licensing takes place within a single brand. Thus, the effect of product differentiation is more subtle. In our model as the degree of product differentiation increases the competition becomes more direct between licensee and licensor as the competing brand is moving away from competition. Note that the business stealing effect from the competing brand is reduced as the degree of product differentiation increases and therefore the tradeoff is more similar to licensing in homogeneous goods case as the degree of product differentiation is larger. This is a key difference with the conventional licensing story with differentiated products. Hence, our characterization of optimal licensing contract is different from the existing literature on licensing in differentiated Cournot models such as Wang (2002) and Faulí-Oller et al. (2013) among others.====The remainder of the paper is organized as follows. Sections 2 The inside innovator, 3 The outside innovator consider respectively the situations of the innovator as an insider and as an outsider and analyze the optimal licensing scheme in those scenarios. Section 4 compares the incentive for innovation between an inside innovator and an outside innovator. Section 5 concludes.",Intra-brand patent licensing with inter-brand competition,https://www.sciencedirect.com/science/article/pii/S0165489620300093,14 January 2020,2020,Research Article,197.0
Stewart Rush T.,"Munich Center for Mathematical Philosophy, LMU Munich, Munich, Germany","Received 8 July 2019, Revised 16 December 2019, Accepted 8 January 2020, Available online 13 January 2020, Version of Record 28 January 2020.",https://doi.org/10.1016/j.mathsocsci.2020.01.003,Cited by (3),"This paper generalizes rationalizability of a choice function by a single acyclic ==== to rationalizability by a ==== of such relations. Rather than selecting those options in a menu that are maximal with respect to a single ====, a weakly pseudo-rationalizable choice function selects those options that are maximal with respect to at least one binary relation in a given set. I characterize the class of weakly pseudo-rationalizable choice functions in terms of simple functional properties. This result also generalizes Aizerman and Malishevski’s characterization of pseudo-rationalizable choice functions, that is, choice functions rationalizable by a set of total orders.","In many of the most important and influential theories of rational and social choice, preferences are assumed to be complete and transitive (e.g., Von Neumann and Morgenstern, 1944; Arrow, 1951; Savage, 1954; Anscombe and Aumann, 1963). The view that preferences are or should be so can be questioned on numerous grounds. As Sen has stressed, such a preferential structure is not required for reasoned choice. Neither incompleteness nor (certain forms of) intransitivity prevents choosing maximal alternatives, i.e., alternatives to which no other option is strictly preferred. As Sen sees it, “there is no analytical reason, nor any practical necessity, why the ranking of alternative conclusions must take this highly restrictive form” (2017, p. 455). In decision problems with finitely many options at least, acyclicity suffices for maximization. Moreover, relaxing transitivity of social preference opens up certain possibilities in the setting of social choice (e.g., Sen, 1969). There is also a sizable empirical literature on behavioral departures from transitive preferences (e.g., Tversky, 1969, Loomes et al., 1991).====Still, choosing maximal options according to an acyclic relation retains the assumption that an agent’s or group’s preferences are or should be given by a binary relation. It is another level of heterodoxy to relax binariness. One motivation for doing so comes from unresolved conflicts in values or goals at the time of choice. According to Sen, such unresolved conflict is the “primary source” of incompleteness (Sen, 2018 p. 11). In such cases, various values induce different preference relations on the options. There are many different decision rules one might employ in such cases. But Isaac Levi, who Sen often cites in this context (e.g., Sen, 2004, Sen, 2017, p. 460), argues that each such preference relation amounts to an evaluation of the options that the agent regards as permissible to use in choice (1986).==== ==== So the admissible options in any menu are those that are maximal with respect to ==== way of evaluating the options that the agent regards as permissible. On such a view, the set of admissible options according to the set of preference relations may resist representation in terms of a single, categorical or all-things-considered binary preference relation.====In this paper, I characterize those preference structures that allow for incompleteness due to unresolved conflicts in values, while also allowing that the conflicting values themselves may fail to induce complete or transitive relations. Formally, I characterize those choice functions that, for any menu of alternatives, select those options that are maximal according to some acyclic binary relation in a ==== of such relations. The binary relations are not required to be weak or total orders. Such choice functions will be called ====.====There are several potential interpretations, both descriptive and normative. For example, a decision maker faced with a set of options may rank them along multiple dimensions. If there is no analytic reason or practical necessity for single preference relations to take the form of weak orderings, as Sen argues, it is difficult to see why any given dimension must take such a highly restrictive form. One rule such a decision maker could follow is to restrict attention to options that are best along at least one dimension. The set of such options might be interpreted as a type of consideration set that filters out the other available options from the decision maker’s attention. Lleras et al. refer to a consideration set mapping that satisfies Sen’s property ==== as a ==== (2017). When interpreted as a consideration set map, a weakly pseudo-rationalizable correspondence is a special case of a competition filter.==== ====To take another possible interpretation, the set of rankings that weakly pseudo-rationalize a given choice function may already include potential ways of trading off yet more fundamental values or dimensions of comparison, but the decision maker has not settled on a uniquely acceptable form of compromise. Rather, under such unresolved conflict among candidate rankings of the options, there are various candidate rankings, corresponding to ways of trading off the fundamental values, that the decision maker has not ruled out and ==== form the relevant set of relations for weak pseudo-rationalizability. The choice-worthy options are the options that are maximal with respect to ==== permissible way of evaluating the options, some way the decision maker has not ruled out. This interpretation extends Levi’s philosophical motivation for ====-admissibility beyond the expected utility setting. Levi has long advocated a theory that allows both probabilities and utilities to be “indeterminate” (1974). Given a set of probabilities ==== over states and a set of utilities ==== over consequences of options, an option ==== is said to be ==== in a menu of alternatives ==== if there is some probability–utility pair ==== with ==== and ==== such that ==== maximizes ====-expected ====-utility in ====. In such a framework, the probability–utility pairs induce various weak order rankings of the options and Levi’s ====-admissibility rule selects those options in a menu that are maximal according to some ranking. These ideas have been extended in various ways by Seidenfeld, Schervish, and Kadane (e.g.,Seidenfeld et al., 1995, Seidenfeld et al., 2010). Choice functions generated by Levi’s ====-admissibility criterion generally violate binariness. But they do nonetheless enjoy a certain discernible structure. Rather than being rationalizable by a single complete and transitive relation, they are ====-rationalizable by a ==== of such relations. Independently of Levi’s work, Aizerman and Malishevski characterize pseudo-rationalizable choice functions in terms of simple properties of abstract choice functions (Aizerman and Malishevski, 1981). This work (and more) is reviewed in Aizerman (1985) and extended in Aleskerov et al. (2007), but there does not appear to be a characterization of the more general case in which the set of relations used to rationalize a choice function consists of merely acyclic relations. Given that standard presentations of choice theory typically distinguish rationalizability of a choice function (by an acyclic binary relation) from stronger concepts like rationalizability by particular types of orderings, Theorem 4 fills an obvious gap in the literature extending rationalizability concepts to rationalizability by sets of relations.====Other studies explore using multiple relations or “rationales” to explain choice behavior. Kalai et al. (2002) work in the framework of element-valued choice functions and introduce the notion of ====. An element-valued choice function is said to be rationalized by a set of total orderings if the chosen option from a menu is maximal with respect to some ordering in the set. The central notion of this paper, weak pseudo-rationalizability, is one way to generalize rationalization by multiple rationales to set-valued choice functions while relaxing the requirement that the rationales be total orders. So, we make less restrictive assumptions about the nature of rationales. Manzini and Mariotti’s ==== also appeals to a fixed set of rationales to rationalize an element-valued choice function. However, they do not require that the rationales be total orders and, importantly, the rationales are applied sequentially to eliminate inferior options (2007). Manzini and Mariotti interpret sequentially rationalizable choice as a form of bounded rationality, one that allows for some but not all departures from “the standard model of rationality”. A similar interpretation is possible for weakly pseudo-rationalizable choice functions. If, unlike Sen or Levi, one thinks of rational (categorical) preferences as inducing a weak ordering of the options (or at least inducing a binary preference relation rationalizing choice), weak pseudo-rationalizability could be conceived of as choice behavior that violates some but not all rationality constraints. Our characterization of weakly pseudo-rationalizable choice functions facilitates comparisons to characterizations of rationalizability (Theorem 1) and rationalizability by a weak ordering (Theorem 2) so that possible departures from the standard model of rationality are apparent. Yet another interpretation of weakly pseudo-rationalizable choice, then, is as a certain form of boundedly rational choice.====After rehearsing the basic notions relating to binary relations, choice functions, and rationalizability that will be employed in Section 2, I briefly review subrationalizability and pseudo-rationalizability in Section 3. In Section 4, I introduce an axiom—a weakening of standard expansion consistency—that, along with Sen’s ====, allows us to state a characterization of weak pseudo-rationalizability (Theorem 4).",Weak pseudo-rationalizability,https://www.sciencedirect.com/science/article/pii/S0165489620300111,13 January 2020,2020,Research Article,198.0
Abe Takaaki,"School of Political Science and Economics, Waseda University, 1-6-1, Nishi-waseda, Shinjuku-ku, Tokyo 169-8050, Japan","Received 21 July 2019, Revised 10 November 2019, Accepted 3 December 2019, Available online 16 December 2019, Version of Record 23 December 2019.",https://doi.org/10.1016/j.mathsocsci.2019.12.001,Cited by (1),"We axiomatize Hart and Kurz’s (1983) two coalition aggregation functions known as the ====-function and the ====-function. A coalition aggregation function is a mapping that assigns a partition to each profile of coalitions selected by players. Through our axiomatization results, we observe that neither the ====-function nor the ","The main purpose of this paper is to answer the following question: what coalition structure should be assigned to a profile of coalitions selected by players? To clarify this question, we begin with a simple example. Suppose that there are three players, ====. Every player chooses a coalition that she/he wants to form: for example, each student submits a list of students with whom he wants to share a room. We suppose that players 1 and 2 want to form the three-person coalition ====, while player 3 wants to form the two-person coalition with player 2, namely, ====. Their choices are summarized as ====For simplicity, we omit the parentheses of a coalition if it is not confusing: we write ==== instead of ====. We call such a tuple ==== a ====. Now, what coalition structure is “optimal” for this coalition profile? The first attempts to address this question were made by Hart and Kurz, 1983, Hart and Kurz, 1984. They defined two aggregation rules known as the ====-function ==== and the ====-function ====. These two rules exhibit two different views of coalition formation. On one hand, one might consider a coalition to be a result from a unanimous agreement among all its members. The ====-function formulates this view. Let ==== be a coalition of players. If ==== the players in ==== choose the same coalition ====, then ==== is formed. However, if someone in ==== chooses a different coalition, the proposers of ==== are partitioned into singletons. In the three-person example above, neither ==== nor ==== obtains a unanimous agreement. Therefore, the ====-function assigns the three one-person coalitions to the coalition profile. On the other hand, one might consider that a coalition is the largest set of players who share the same choice. In this view, the choice of a coalition means the set of all players with whom he is willing to share the same coalition. The ====-function exhibits this view. In the same example, players 1 and 2 choose the same coalition ====. In other words, they accept all players. However, player 3 does not admit player 1. Although we will provide its formal definition in the next section, we can define the ====-function by simply grouping the players who choose the same coalition. Therefore, the ====-function assigns the two-person coalition to players 1 and 2. Player 3 is the only player who chooses coalition ====. Hence, the ====-function assigns his one-person coalition to player 3.====In general, we call such a function that assigns a coalition structure to each coalition profile a ====. Our motivation for studying a coalition aggregation function is to associate players’ choices with coalition formation theory. Coalition formation theory has been mainly studied in the frameworks in which players do not select any action and/or choice, ====, cooperative games and hedonic games. However, when we form a team or a group, our choices and decisions must implicitly/explicitly influence its result. Therefore, we attempt to analyze coalition formation in terms of players’ choices. However, besides the ====- and ====-functions, there are many coalition aggregation functions. How should we choose one coalition aggregation function? As Hart and Kurz (1983) state, “there is no universally correct answer to this problem”. Nevertheless, once one chooses a certain function, she must be supposed to answer why she selects it. Our axiomatization results must be helpful to answer this question.====As we will discuss in the following sections, the ====- and ====-functions do not satisfy a property called monotonicity. Monotonicity is a guarantee for players who have an acceptive attitude. To see this, we fix a player and the choices of the other players. Even if the player changes his choice to accept more players by choosing a superset, the ====- and ====-functions may provide him with a smaller coalition. Monotonicity guarantees that his acceptive change does not end up such a contrary result: even if his invitation is declined, it does not break his original affiliation.====After we axiomatize the ====-function and the ====-function in the next section, we propose a monotonic coalition aggregation function in Section 3. Its axiomatization is also provided. In Section 4, we summarize our results and offer some concluding remarks. Table 1 in Section 4 shows the axiomatic systems of the coalition aggregation functions we study in this paper. All the proofs and the independence of the axioms are provided in the appendix.",Axiomatizations of coalition aggregation functions,https://www.sciencedirect.com/science/article/pii/S0165489619300939,16 December 2019,2019,Research Article,199.0
"Naskar Mili,Pal Rupayan","ANZ Operations and Technology, India,Indira Gandhi Institute of Development Research (IGIDR), India","Received 17 March 2019, Revised 30 September 2019, Accepted 28 November 2019, Available online 6 December 2019, Version of Record 16 December 2019.",https://doi.org/10.1016/j.mathsocsci.2019.11.006,Cited by (15),This paper examines the implication of the nature of competition in a market with network externalities on strategic investment in process R&D by firms. It shows that network externalities have a larger positive effect on process R&D under ,"This paper revisits the Cournot–Bertrand comparison of firms’ incentives to invest in cost reducing (process) R&D. Developing a model of differentiated network goods duopoly, where firms non-cooperatively choose levels of investments in process R&D before engaging in product market competition, this paper demonstrates the following. First, while network externalities have a positive effect on R&D regardless of the nature of product market competition, the effect is larger in the case of Bertrand competition compared to that in the case of Cournot competition. Second, and more importantly, Bertrand R&D is higher than Cournot R&D, if network externalities are sufficiently strong, regardless of the degree of product differentiation. Otherwise, if network externalities are not sufficiently strong, Bertrand R&D can be higher, lower or equal to Cournot R&D depending on the degree of product differentiation and the strength of network externalities. These results are robust to whether networks are perfectly incompatible or imperfectly compatible to each other, except that the critical strength of network externalities in the former case is less than that in the later case. These are new results.====Analyses of the equilibrium investment levels in process R&D in the case of non-network goods oligopoly suggest that under Cournot competition firms invest more in process R&D than that under Bertrand competition (see, for example, Qiu, 1997, Lin and Saggi, 2002).==== ==== This is because an increase in a firm’s investment in process R&D has, not only a direct positive effect, but also a strategic effect on its profit, which works through influencing rival firm’s strategic variable — quantity or price. While under Cournot competition the strategic effect is positive, it is negative under Bertrand competition. The reason for this ranking to be reversed in network goods oligopoly under certain conditions is as follows. When firms produce network goods, an increase in output (or a decrease in price) of a firm enhances consumers’ expectations regarding network size, which shifts its demand curve outward and that in turn has a positive effect on its profit. Thus, in the presence of network externalities, an increase in a firm’s investment in process R&D has an additional positive effect (henceforth, network effect) on its profit regardless of the nature of competition. Further, Bertrand firms play more aggressively in the product market than Cournot firms even in network goods oligopoly and more aggressive play has an indirect positive effect on profits, via consumers’ expectations, which is larger in the case of stronger network externalities (Pal, 2014b). As a result, the positive network effect is larger under Bertrand competition than under Cournot competition and it over compensates for the negative strategic effect under Bertrand competition, unless network externalities are sufficiently weak. Since the possibility of free riding on rival firm’s network does not exist in the case of incompatible networks, aggressive play by a firm creates captive demand for its own product and, thus, the critical strength of network externalities necessary to guarantee higher R&D under Bertrand competition than that under Cournot competition is less in the case of incompatible networks compared to that in the case of (imperfect) network compatibility.====Starting with the seminal works of Farrell and Saloner, 1985, Farrell and Saloner, 1986 and Katz and Shapiro, 1985, Katz and Shapiro, 1986, a number of studies have examined the implication of network externalities on product development and introduction of new products by oligopolistic firms in different scenarios.==== ==== However, the literature on process R&D in the context of network goods oligopoly is rather sparse.  Boivin and Vencatachellum (2002) and Saaskilahti (2006) are the only two exceptions in this regard. Boivin and Vencatachellum (2002) consider homogeneous network goods Cournot duopoly to examine the implications of network externalities on firm’s R&D behavior. They also argue that firms in Cournot duopoly tend to invest more in process R&D in the presence of network externalities compared to that in absence of network externalities.==== ==== On the other hand, considering a linear city model of price competition, Saaskilahti (2006) focuses on possible implications of product compatibility and product quality on investment in process R&D. This paper contributes to this literature by exploring the effects of the nature of product market competition – Cournot versus Bertrand – and the strength of network externalities on firms’ choice of investment in process R&D in differentiated network goods duopoly, both in the case of imperfect network compatibility and in the case of incompatible networks.====The rest of the paper is organized as follows. Section 2 describes the model setup. Section 3 presents the analysis and results in the case of imperfect network compatibility. Section 4 examines robustness of the main result by considering perfectly incompatible networks. Section 5 concludes.",Network externalities and process R&D: A Cournot–Bertrand comparison,https://www.sciencedirect.com/science/article/pii/S0165489619300927,6 December 2019,2019,Research Article,200.0
"Le Van Cuong,Pham Ngoc-Sang","IPAG Business School, France,Paris School of Economics, CNRS, France,TIMAS, Vietnam,Kobe University, Japan,Academy of Policy and Development (APD), Vietnam,Montpellier Business School, France","Received 12 March 2019, Revised 28 October 2019, Accepted 18 November 2019, Available online 28 November 2019, Version of Record 7 December 2019.",https://doi.org/10.1016/j.mathsocsci.2019.11.005,Cited by (0), analysis is also provided.,"Inferior and Giffen goods have been mentioned in most microeconomics textbooks (see Mas-Colell et al. (1995), Jehle and Reny (2011), Varian (2014) for instance).==== ==== However, they are usually illustrated by pictures. In this paper, we present a class of differentiable, strictly increasing, concave utility functions exhibiting an explicit demand of a good which may have Giffen behavior. In our example, the consumption set is ====, and the demand function generated by our simple utility function has a closed-form. Thanks to this tractability, we provide a necessary and sufficient condition (based on prices and consumers’ preferences and income) under which this good is normal, inferior or Giffen good. This helps us to analytically study income and prices effects. In particular, we show that the Giffen behavior arises when the price is not so high and the consumer’s income is at the middle level. This is supported by empirical evidences in Jensen and Miller (2008): when the price of a staple good increases, the poor people responds by decreasing their demand of this good while the group in the middle increases demand.====The second part of our paper focuses on the general equilibrium effects. Our utility function leads to an interesting point in general equilibrium context: the price of a good may be an increasing function of the aggregate supply of this good. Moreover, we show that the Giffen behavior may arise in equilibrium when preferences or/and endowments of agents change.====In the existing literature, several examples of Giffen good have been proposed. However, in most of the cases, utility functions are piecewise-defined or demand functions are not explicit or the consumption set is restricted. [4] provide a collection of papers studying Giffen goods, including the paper of Doi et al. (2009).====Here, we just mention two recent papers (Haagsma, 2012, Biederman, 2015). Haagsma (2012) presents a separable utility function generating Giffen behavior.==== ==== In this example, the consumption set is restricted (precisely, it is ==== with ====) and the utility function is quasi-concave but not concave. Moreover, in Haagsma (2012), the good 1 demand ==== is always decreasing in the income, denoted by ====, whatever the prices and the consumer’s income. However, in our model, the sign of ==== depends on prices and the consumer’s income.  Biederman (2015) introduces a concave utility function==== ==== and gives some numerical examples where Giffen behavior arises. However, the demand function is not explicit. In our paper, we can explicitly derive the demand function.",Demand and equilibrium with inferior and Giffen behaviors,https://www.sciencedirect.com/science/article/pii/S0165489619300915,28 November 2019,2019,Research Article,201.0
"Gilboa-Freedman Gail,Smorodinsky Rann","School of Entrepreneurship, The Interdisciplinary Center Herzliya, Israel,Faculty of Industrial Engineering and management, Technion, Israel","Received 12 July 2019, Revised 17 November 2019, Accepted 18 November 2019, Available online 25 November 2019, Version of Record 18 December 2019.",https://doi.org/10.1016/j.mathsocsci.2019.11.004,Cited by (3),"Privacy, in the sense of control over access to one’s personal information, is a central concern in the context of online decision making, both in general and in relation to online platforms in particular. For at least some agents, a belief that one online platform jeopardizes users’ privacy more than another may tip the scales in favor of the latter. Thus, understanding how privacy considerations come into play is central for any economic or social analysis. To this end, we study how agents rank online platforms (or mechanisms, as we call them) from a privacy perspective. We propose a very simple model of privacy-jeopardizing mechanisms, along with a normative methodology for understanding how these mechanisms are ranked. Similarly to classic work in decision theory, we postulate several axioms that we believe a privacy order should satisfy, and then characterize the set of orders that comply with these axioms. These orders turn out to be related to the notion of ","The question of how deeply people value their privacy is central to the study of many economic systems. As our digital footprints become more and more revealing of who we are, privacy considerations are becoming increasingly acute in determining our choices over the web. If two competing platforms offer similar services but one provides better privacy, this could be an important factor in directing more traffic to that platform. For instance, consider the realm of instant communication. While the instant messaging application WhatsApp has become the de facto standard in many countries, its leadership in some locations is now contended by Telegram, a competing messaging app that gained over 100 million users during its first two and a half years and is growing at a rapid pace (Anon, 0000a). The unexpected success of Telegram is attributed by many to its privacy-preserving nature. On the company’s website (Anon, 0000b), the app is pitched as providing the highest level of security, and the first factor listed under “Why switch to Telegram?” is the fact that “Telegram messages are heavily encrypted and can self-destruct”.====While we can observe the relative success of firms like WhatsApp and Telegram and link their performance to their privacy policies, the question of how individual users make privacy-motivated decisions remains opaque. Consider, for example, a user who is deliberating over the merits of two leading search engines. Imagine that the user is wealthy, and that he does not wish to reveal in his online presence that he lives in an affluent area. Both platforms will extract data about the user’s zip code and use it to match the user to advertisers, though not necessarily in the same way. A third party might observe these ads and draw conclusions about the user’s location (for example, guessing the user’s zip code from the highest matched ad). In this context the top-matched ad serves as the signal, and the area type (affluent vs. lower-income) is the secret. We are interested in studying how the user would rank the two platforms in terms of privacy protection.====Our main contribution is to propose a normative methodology for studying how privacy jeopardizing platforms compare in terms of protecting personal information. We use the term“mechanism”, instead of platforms as it is prevalent in the literature on privacy. We identify a set of “natural” properties that any preference (for one platform over another) which captures privacy concerns must comply with. Then, we identify a family of functions that serves as a natural measure of privacy loss: any function of this form complies with all of the aforementioned natural properties; and any preference order (over mechanisms) that complies with all these properties must be induced by a function from that family. Interestingly, this family is closely related to the familiar information theoretic notion of ==== (Feldman, 1989, Liese and Vajda, 2006).====We now provide a more formal example:====We now describe a model that formalizes a representation for mechanisms which jeopardize privacy, and an order over such mechanisms that complies with various axioms that are natural in the context of privacy. Taking our cue from the diet application example, we define a mechanism as comprising a set of signals (outputs) and a function that maps each secret value (we use the term ‘type’ interchangeably) into a random signal (formally, a distribution over the set of signals). The private information (for simplicity, hereinafter referred to as “the type” or “the secret”) and the signal can each take one of finitely many values. We identify a mechanism with a row stochastic matrix where the entry ==== denotes the conditional probability of the signal ==== given the type ====. An outsider may therefore learn something about the type by observing the realized signal. But different mechanisms disclose different kinds of information, and hence exhibit different privacy levels.====Our consideration of stochastic matrix from types to signals is similar to the definition of a “mechanism” in the privacy literature (Roth and Roughgarden, 2010). Therefore, from now on we will use the term “mechanism” instead of platform, to reflect the fact that it is specific kind of platform, represented by the row stochastic matrix described above.====A “privacy preference” is a binary preference over the set of all mechanisms that share the same type set (in fact only the cardinality of that set matters), and that comply with a set of axioms which we discuss in detail in our model. The preference order is type-dependent, meaning that an agent wishing to keep two pieces of information private (i.e., having two distinct secrets) may have two separate preference orders over the set of mechanisms. Note that in Example 1.1., in a hypothetical situation where Alex is not diabetic, the privacy order need not be the same as in the case where she is.====The model we propose, as Example 1.1 demonstrates, is a prior-free model. That is, the probability distribution over the set of secrets is not specified and hence does not affect the privacy preference we study. The reason we pursue a prior-free notion of preference is that an adversary often has no meaningful prior distribution over the set of secret values. Moreover, sometimes identifying a meaningful prior is the raison d’etre of the mechanism, and the mechanism’s output is actually collected for the purpose of identifying the prior distribution. To offer a concrete example, consider an experimental study that aims to identify the proportion of people carrying the Zika virus in a specific population. The data includes a distribution for the blood types of those infected by the virus and those not infected. Thus, publishing someone’s blood type also provides information about the probability that the individual carries the virus. Is there any meaningful prior on the distribution of people who are Zika-infected or Zika-not-infected before the experimental study is conducted?====Our main result characterizes the preference orders that comply with the axioms in our model by a family of functions: any function of this form induces a privacy preference. By privacy preference, we refer to an order that complies with the aforementioned axioms. On the other hand, any privacy preference must be induced by one of these functions. As noted earlier, the family we identify is closely related to the familiar information theoretic notion of ====. The ====-divergence between two probability vectors is the expected value of the function ====, taken at the likelihood ratio of the signals (a formal definition of ====-divergence is given in the sequel). Thus, our model of privacy loss fits itself to a preference order that is represented by the distance between distributions, as a natural representation of how easy it is to distinguish between secrets that are associated with these distributions. This is an intuitive result: the more distant from each other the distributions are, the more the agent’s privacy is jeopardized.====To demonstrate the applicability of our characterization theorem, we study privacy loss using data provided to us by the Recommendation Team at Microsoft Research. Recommendation systems are an excellent example of a mechanism in which the empirical trade-off between system performance and privacy preservation is very clear, as supplying customers with accurate and useful recommendations requires them to share personal information on their preferences, tastes, or habits. In this context, we show how the proposed model is useful for prioritizing data subsets in the context of privacy loss, or for identifying what pieces of information are more exposed.====The rest of the paper is organized as follows: in Section 2 we briefly survey the relevant literature. In Section 3 we characterize privacy loss order over a set of mechanisms with a binary secret, and in Section 4 we extend the result to mechanisms with an arbitrary number of secret values. In Section 5, we demonstrate the usefulness of our theoretical result in the context of recommendation systems. We summarize our findings and conclude in Section 6.",On the properties that characterize privacy,https://www.sciencedirect.com/science/article/pii/S0165489619300903,25 November 2019,2019,Research Article,202.0
Ortega Josué,"Queen’s Management School, Queen’s University Belfast, UK,Center for European Economic Research (ZEW), Mannheim, Germany","Received 11 January 2019, Revised 8 July 2019, Accepted 5 November 2019, Available online 14 November 2019, Version of Record 25 November 2019.",https://doi.org/10.1016/j.mathsocsci.2019.11.003,Cited by (10),"I study the problem of allocating objects among agents without using money. Agents can receive several objects and have dichotomous preferences, meaning that they either consider objects to be acceptable or not. In this set-up, the egalitarian solution is more appealing than the competitive equilibrium with equal incomes because it is Lorenz dominant, unique in utilities, and group strategy-proof. Both solutions are disjoint.","An assignment problem is an allocation problem where scarce objects are to be allocated among several agents without using monetary transfers. Assignment problems include the allocation of senators to committees, courses to students, or job interviews to applicants. In this paper, I study assignment problems in which each agent can receive more than one object, but at most one unit of each, and several identical units are available of each object. These are called multi-unit assignment problems. They include the three examples previously discussed. A U.S. senator on average participates in four committees,==== ==== a student can take many courses during a semester, and a job candidate can schedule many interviews. However, senators cannot have more than one seat on each committee, students cannot take a course twice for credit, and applicants cannot be interviewed more than once for the same position.====For such multi-unit assignment problems, we would like to have a systematic (probabilistic) procedure to decide fairly which agents should get which objects, which, at the same time, does not offer incentives to coalitions of agents to lie about their true preferences. In this paper, I show that the egalitarian solution achieves this purpose for multi-unit assignment problems in the dichotomous preference domain, in which objects are either considered acceptable or not, and in which agents are indifferent between all objects that they find acceptable.====The egalitarian solution is based on the well-known leximin principle. In the domain of dichotomous preferences, it is Lorenz dominant, unique in utilities, and impossible to manipulate by groups. In contrast, the celebrated competitive equilibrium with equal incomes (Hylland and Zeckhauser, 1979, Budish, 2011, Reny, 2017) fails to satisfy these three properties. Both solutions are disjoint, meaning that in general we cannot obtain the ==== solution as a competitive equilibrium when agents are endowed with ====. This is a stark difference with the single-unit case, in which both solutions coincide (Bogomolnaia and Moulin, 2004).====Lorenz dominance is “====” (Foster and Ok, 1999) and is “====” (Dutta and Ray, 1989). In our set-up, the fact that a utility profile is Lorenz dominant implies that it uniquely maximizes any strictly concave utility function representing agents’ preferences and is, therefore, a strong fairness property.====Uniqueness of the solution (in the utility profile obtained) is also a desirable property because it provides a clear recommendation of how the resources should be split. On the contrary, a multi-valued solution leaves the schedule designer with the complicated task of selecting a particular division among those suggested by the solution, thus raising the possibility of justified complaints by some agents who may argue that other allocations were also recommended by the solution that were more beneficial to them.====It is equally interesting that the egalitarian solution is group strategy-proof, implying that coalitions of agents can never profit from misrepresenting their preferences. On the contrary, the competitive solution is manipulable by groups in this set-up, as in many others. Yet, it is remarkable that even in our small dichotomous preference domain, where the possibilities to misreport are very limited, the pseudo-market solution can still be manipulated by coalitions of agents.====The fact that the egalitarian solution satisfies these three desirable properties is a strong argument for recommending its use whenever agents have dichotomous preferences.====The dichotomous preference domain is admittedly simple and not suitable for modelling problems in which objects are either complements or substitutes. However, this set-up is helpful to represent scheduling problems (such as the tennis allocation problem studied by [28]; see Table 1), in which agents are either compatible or incompatible with each object and want to maximize the number of objects they obtain, or for the aforementioned problems of assigning job interviews to candidates or seats for performances to the public, among others.==== ====Focusing on this particular domain of preferences will be helpful to show the properties of the egalitarian solution, while, at the same time, it will make the problem complicated enough to identify why the competitive equilibrium with equal incomes fails to be unique and group strategy-proof. The reason behind it is that the number of identical copies available of some objects (their supply) equals their total demand. I call these objects perfect. Because their demand is always equal to their supply, they can have a zero competitive price. However, they could also have a positive price, thus generating a large set of competitive equilibria (Table 2). Therefore, a coalition of agents can collectively misrepresent their preferences in order to make a set of objects perfect to reduce their price. Manipulating agents can benefit unambiguously from collectively reducing their demand for perfect objects (Table 4).",Multi-unit assignment under dichotomous preferences,https://www.sciencedirect.com/science/article/pii/S0165489619300897,14 November 2019,2019,Research Article,203.0
Takemura Ryo,"Nihon University, Japan","Received 10 September 2018, Revised 16 July 2019, Accepted 4 November 2019, Available online 11 November 2019, Version of Record 27 November 2019.",https://doi.org/10.1016/j.mathsocsci.2019.11.001,Cited by (1),We investigate an interaction between ,"When one opens economics textbooks, one finds a number of graphs representing or illustrating various mathematical functions such as supply and demand functions (for example, Krugman and Wells (2012), Varian (1992), Mas-Colell et al. (1995) and Hoy et al. (2011)). In fact, it is difficult to explain economics concepts such as law of demand and supply without any graphs or diagrams. Thus, in actual economic reasoning or problem solving using thus obtained economics concepts, it is natural to employ graphs and diagrams. However, models of human reasoning that are not restricted to economic reasoning that have been proposed in various fields have been restricted to models of reasoning with linguistic representations such as sentences and symbols. For example, models of economic reasoning proposed in qualitative reasoning studies are models with linguistic or symbolic representation using differential equations, algebraic calculations, causal relations, and so on. Cf. Samuelson (1947), Quirk (1997), Farley and Lin (1990) and Kalagnanam et al. (1991).====Considering actual human reasoning, we usually use not only linguistic or symbolic representations, but also, graphical or diagrammatic representations. Based on this observation, diagrammatic reasoning and heterogeneous reasoning employing combinations of linguistic and diagrammatic representations have been recently investigated from various viewpoints, such as mathematical logic, computer science, and cognitive science. See, for example, Larkin and Simon (1987), Allwein and Barwise (1996) and Shimojima (2015). For economic reasoning, Tabachneck-Schijf et al. (1997) proposed a cognitive model of heterogeneous reasoning called CaMeRa (computational model of multiple representations) based on cognitive experiments used to compare linguistic/sentential, mathematical symbolic, and graphical representations.====Among various graphs employed in economics, one of the most basic and frequently appearing graphs is the demand and supply graph, which also forms the basis of the CaMeRa model. Although laws of demand and supply generally hold for any demand and supply functions, it is not possible to draw a graph of ==== function. Thus, to illustrate laws of demand and supply by graphs, there are many different representatives of arbitrary demand and supply graphs (including linear graphs) in economics text books.====In this article, we focus on linear graphs, whose slopes are ==== in particular, and discuss the use of such linear graphs in economic reasoning. As it is often pointed out, a drawn graph is more or less specific, and it shows something not logically derived in general. Shimojima (2015) called such property of diagrammatic representations ====, and analyzed it from a viewpoint of mathematical logic and cognitive science. Thus, the use of graphs and diagrams in economics have been restricted to be apparatus of explanation. However, linear graphs have the following advantages: (1) linear graphs are easily constructed and reasoning using such graphs is effectively manageable; (2) they are qualitatively determinate, that is, which conclusion is qualitatively deduced from the given premises is uniquely determined with respect to our problems. We investigate the use of linear graphs in certain types of economic reasoning by revealing their range of applications, and we propose a model of economic reasoning with linear graphs.====One of the most effective techniques to construct models of reasoning is that developed through mathematical logic. Recently, interactions between mathematical logic and economics have been discussed. For example, Kaneko, 2002, Kaneko, 2013 has investigated these from a rather wide viewpoint. On the one hand, economics can provide concrete and actual frameworks of reasoning problems, and on the other hand, mathematical logic can provide notions and techniques developed in traditional reasoning studies. One of the most remarkable examples of such interaction is the application of epistemic logic to game theory. Kaneko (2002) provides an introduction to economists on game theoretical applications of epistemic logic. Recent research has shown that epistemic logic can be applied to the analysis of common knowledge and an economic agent’s rationality in game theory. Other than epistemic logic, in research on qualitative reasoning, for example in Lin and Farley (1991) and Berndsen and Daniels (1991), the logic programming PROLOG has been traditionally applied as an implementation of their reasoning models. Furthermore, there are more recent examples: Hinkkanen et al. (2003) applies notions and techniques of set-theoretical model theory to integrate qualitative and quantitative frameworks; Kerber et al. (2016) surveys applications of theorem provers based on proof theory in mathematical logic to economics problems; Blume et al. (2015) discusses market design from the viewpoint of computational complexity theory. In this article, by applying proof-theoretical techniques, we propose a model of economic reasoning with graphs. Such a model may be useful to investigate the notion of rationality in economics, actual problem solving, decision making, and for teaching economics.====The economic reasoning discussed in this article has been studied within the framework of qualitative reasoning, e.g., Farley and Lin (1990) and Kalagnanam et al. (1991). Qualitative reasoning studies investigate reasoning based on qualitative information instead of precise quantitative information (Iwasaki, 1997). In qualitative reasoning studies, with the aim of implementation, economic reasoning “without graphs” is investigated. In some aspects, the economic reasoning we investigate here is an extension of previous research of qualitative reasoning, where either the demand or supply curve is allowed to shift just once. Such an analysis has been extended to a more complicated, multivariable setting in Farley and Lin (1990). However, we concentrate on analyzing the basic demand and supply market, but allow simultaneous shifts of the demand and supply curves as in the following Example 2.1.====In Section 2, by using techniques of differentiation, we specify the range over which linear demand and supply functions can be effectively applied. Based on the analysis by differentiation, in Section 3, we discuss the use of linear demand and supply graphs, and propose a model of economic reasoning with graphs within the framework of proof theory of mathematical logic.",Economic reasoning with demand and supply graphs,https://www.sciencedirect.com/science/article/pii/S0165489619300873,11 November 2019,2019,Research Article,204.0
Park Seyoung,"School of Business and Economics, Loughborough University, Epinal Way, Loughborough, Leicestershire, LE11 3TU, UK,Nottingham University Business School, University of Nottingham, Jubilee Campus, Nottingham, NG8 1BB, UK","Received 11 December 2017, Revised 5 February 2019, Accepted 4 November 2019, Available online 8 November 2019, Version of Record 4 December 2019.",https://doi.org/10.1016/j.mathsocsci.2019.11.002,Cited by (3),One can find it challenging to deal with verification theorems for ,"Optimal life-cycle consumption and investment decisions with annuitization have received full attention in political economy, economics and finance spheres. Indeed, annuitization is arguably one of the most important life-cycle decisions just as consumption and investment in order to achieve successful retirement.====Proving verification theorems for models with annuitization can be a considerable challenge because the question of existence of optimal strategies with discretionary stopping seems to be tricky to be answered. In addition, an expectation associated with annuitization is, in general, very difficult to compute. Compared to duality theory of Karatzas and Wang (2000), I pose a more tractable framework to prove verification theorems for problems with discretionary stopping, without resorting to the state price density (of the stochastic discount factor). Using the principle of dynamic programming, the proposed approach can be readily applied to proving verification theorems for problems with annuitization in a complete or an incomplete market.====I revisit an annuitization model of Park (2015) and prove the verification theorems for the model. While Yaari (1965) and Richard (1975) suggest full and immediate annuitization in the absence of bequest motives, i.e., all savings should be annuitized at all dates,==== ==== this paper follows Park (2015) and thus, considers the case where individuals need to annuitize all of their wealth at one point in time. This resembles the current situation in the UK, where individuals determine when to start their retirement pension but must do so at one point in time.==== ==== Actually, most variable annuity contracts in the US also provide individuals with an option to annuitize that can only be exercised once. My analysis would cover social security as well. In social security, benefits are provided in the form of a lifetime annuity based on a retirement age of 65. If individuals can opt to retire earlier (as of age 62) or later (up to age 70), they can withdraw a smaller or larger annuity, respectively.==== ==== In this framework, I endogenously determine the optimal timing for full annuitization and this is effectively an optimal retirement problem.==== ====As a matter of fact, the date of annuitization does not need to be always the same with the date of retirement. In line with the recent trend of optimal claiming of life annuity income even after retirement in the US, on July 2014, the US Treasury department has allowed individuals to purchase a deferred income annuity (DIA) inside tax-sheltered retirement plans. Similar to life annuity considered in this paper, to purchase the DIA, individuals have to pay a lump sum at one distinct point in exchange for a defined (or a fixed) lifetime income. However, to purchase the DIA, individuals do not have to pay such a lump sum at only retirement. Rather, individuals can purchase it at a relatively young age and delay its income generating date until a relatively old age (usually after the normal retirement age).==== ==== Although I do not take the DIA into this paper explicitly, in Section 5, I partially address the optimal claiming of life annuity income in the US by considering an exponentially distributed retirement age. In this case, individuals can withdraw their annuity income before or after the normal retirement age. Ultimately, I can investigate the effects of variation in retirement time on the annuitization decision.====One unique feature of the suggested framework is the application of the variational inequality approach of Bensoussan and Lions (1982) to the annuitization problem solved by the proposed value function. I verify the uniqueness and existence for the value function. Further, I obtain analytic comparative statics for the optimal strategies. In terms of tractable applications, I hope that this paper will lend itself to the verification for many other interesting utility maximization problems with discretionary stopping.====The paper is organized as follows. In Section 2, I specify the basic model in which the annuitization model of Park (2015) is revisited. In Section 3, I prove the uniqueness and existence theorem for the model. In Section 4, I provide analytic comparative statics for the derived optimal strategies. In Section 5, I consider more general case for annuitization than Section 2, where individuals are allowed to invest in both bonds and stocks after annuitization. In Section 6, I conclude the paper.",Verification theorems for models of optimal consumption and investment with annuitization,https://www.sciencedirect.com/science/article/pii/S0165489619300885,8 November 2019,2019,Research Article,205.0
"Trudeau Christian,Vidal-Puga Juan","Department of Economics, University of Windsor, 401 Sunset Avenue, Windsor, Ontario, Canada,Economics, Society and Territory (ECOSOT), Universidade de Vigo, 36200 Vigo (Pontevedra), Spain,Departamento de Estatística e IO, Universidade de Vigo, 36200 Vigo (Pontevedra), Spain","Received 13 November 2018, Revised 8 October 2019, Accepted 21 October 2019, Available online 5 November 2019, Version of Record 5 November 2019.",https://doi.org/10.1016/j.mathsocsci.2019.10.002,Cited by (5),We introduce a new family of cooperative games for which there is coincidence between the nucleolus and the ====. These so-called ==== of cliques. We examine multiple games defined on graphs that provide a fertile ground for applications of our results.,"The Shapley value (Shapley, 1953) and the (pre)nucleolus (Schmeidler, 1969) are two well known values for cooperative games. The Shapley value is an average of the contributions of an agent, while the prenucleolus is the value that minimizes the dissatisfaction of the worst-off coalitions. The nucleolus differs from the prenucleolus by only taking into account individually rational imputations.====Coincidence between the Shapley value and the (pre)nucleolus is uncommon and, in general, difficult to check without computing both values. Recently, Yokote et al. (2017) provide a sufficient and necessary condition for this coincidence to hold, but it requires the computation of both the Shapley value and of a parametric family of sets, for which the computation mimics that of the (pre)nucleolus.==== ==== This characterization can be applied in order to identify the coincidence in some particular classes of games, such as airport games (Littlechild and Owen, 1973), bidder collusion games (Graham et al., 1990) and polluted river games (Ni and Wang, 2007). Csóka and Herings (2017) also find coincidence in some three-agent games based on bankruptcy problems. As discussed by Kar et al. (2009), for general coalitional form games we have coincidence if the game only has two agents or if all agents are symmetric within the normalized game. Some other games have also been proposed (Deng and Papadimitriou, 1994, van den Nouweland et al., 1996), all having in common that the value of a coalition is equal to the sum of the values created by the pairs composing that coalition. These games are called 2-additive games. The coincidence persists in games that satisfy the so-called ==== property (Kar et al., 2009). These games are such that the contributions of agent ==== to any coalition and its complement sum up to an agent-specific constant. A particular instance of such games is studied by Chun et al. (2016).====González-Díaz and Sánchez-Rodríguez (2014) also study the coincidence from a geometric point of view. Instead of providing classes of games where both values coincide, they study the properties that lead to this result in some already existing classes, as for example ====-games. A similar, yet different problem, is the invariance of the payoff assigned by an allocation rule to a specific player in two related games. See Béal et al. (2015) for the case of the Shapley value.====In this paper, we present another family of games, called ====, in which the Shapley value and the nucleolus coincide. The family can be described as follows: the set of agents is divided into cliques that cover it. A coalition creates value when it contains many agents belonging to the same clique, with the value increasing linearly with the number of agents in the same clique. Agents may belong to more than one clique, but the intersection of two cliques contains at most one agent. Finally, if two agents are not in the same clique, there exists at most one way to “connect” them through a chain of connected cliques.====The family of clique games has a non-empty intersection with ====-games, and that intersection consists of 2-additive games. Some clique games are not ====-games, and some ====-games are not clique games. A clique game is convex, and hence its Shapley value is the average of the extreme points in its core. We thus obtain a link between three crucial concepts of cooperative game theory: the nucleolus, the core, and the Shapley value.====Naturally, graph-induced games provide a fertile ground to apply our result. We first consider the graph-restricted cooperative games introduced by Myerson (1977). In these games, a coalitional value function is accompanied by a graph that summarizes the cooperation possibilities: a coalition ==== cannot fully cooperate if some of its members have no path between them that uses only the vertices of agents in ====. When we consider a symmetric coalitional value function, assigning shares of the value created among agents is akin to defining centrality measures (Gomez et al., 2003). We show that when the coalitional value function increases linearly with the number of agents in a coalition, we obtain coincidence of the Shapley value (known as the Myerson value in this context) and the nucleolus for a family of graphs.====Another graph-induced game that we study is the minimum coloring game (Deng et al., 1999), in which the graph represents conflicts between pairs of agents. We wish to assign agents to facilities, but cannot assign agents that are in conflict to the same facility. As facilities all have a cost of one, we wish to minimize the number of facilities used. Okamoto (2008) noticed a coincidence between the Shapley value and the nucleolus for a particular family of graphs. We explain this coincidence by the fact that the graphs induce clique games.====Our third example is the minimum cost spanning tree (====) problem (Bird, 1976). This well-studied problem has agents connecting to a source through a network, with the cost of an edge being a fixed amount that is paid if the edge is used, regardless of the number of users of the edge. Any such problem has a non-empty core even though it may not be convex. Moreover, its Shapley value is not always in its core (Dutta and Kar, 2004). When we consider elementary ==== problems (in which all edges have a cost of 0 or 1), the subset of cycle-complete problems (Trudeau, 2012) generates clique games. Cycle-complete problems are such that if there exist multiple distinct free paths between a pair of agents the edge connecting them directly must also be free. Our result on clique games then applies, yielding that the nucleolus coincides with the Shapley value and the permutation-weighted average of extreme core allocations.====The paper is divided as follows: preliminary definitions are in Section 2. Section 3 describes and illustrates clique games. Section 4 contains the coincidence results. Applications to graph-induced games are discussed in Section 5.",Clique games: A family of games with coincidence between the nucleolus and the Shapley value,https://www.sciencedirect.com/science/article/pii/S0165489619300782,5 November 2019,2019,Research Article,206.0
"Courtin Sébastien,Laruelle Annick","Normandie Univ, Unicaen, CREM, UMR CNRS 6211, France,Departamento de Fundamentos del Análisis Económico I, Universidad del País Vasco, Avenida Lehendakari Aguirre, 83, E-48015 Bilbao, Spain,IKERBASQUE, Basque Foundation of Science, 48011, Bilbao, Spain","Received 11 September 2018, Revised 18 October 2019, Accepted 20 October 2019, Available online 5 November 2019, Version of Record 5 November 2019.",https://doi.org/10.1016/j.mathsocsci.2019.10.001,Cited by (2),This paper deals with rules that specify the collective acceptance or rejection of a proposal with several dimensions. We introduce the notions of separability and weightedness in this context. We provide a partial characterization of separable rules and show the independence between separability and weightedness.,"Many collective decision processes at a local, national or international level use a simple voting scheme: the question is unidimensional, meaning that individuals answer “yes” or “no” to a single question. We refer to Andjiga et al. (2003) and Laruelle and Valenciano (2008) for a detailed description of these processes. Some extensions include the consideration of non-binary choices: Felsenthal and Machover (1997) or Laruelle and Valenciano (2012) introduce decision systems in which “abstention” or “non-participation” are permitted; Freixas (2005) and Freixas et al. (2014) study ==== games where multiple dimensions concern both the input and the output.====Here, we are interested in another type of extension. Consider a situation in which a group of experts have to decide whether a candidate qualifies for a promotion. Several criteria can be taken into account: performance, ability to work in a team, and leadership. Each expert gives a positive or negative evaluation of the candidate for each criterion. The question is to choose the decision-making rule that will aggregate all evaluations into a promotion or non-promotion decision. A similar problem occurs when the success of a student is based on tests taken at different periods or on different subjects. This paper models these situations through multi-dimensional decision-making rules, that is, rules with several voters casting a binary choice (yes/no) on several dimensions resulting in a dichotomous (yes/no) decision. More precisely, we model decision processes where: (i) There are several voters. (ii) There are several dimensions. (iii) Each voter expresses a binary choice (“yes” or “no”) on each dimension. (iv) A decision process maps each choice to a final binary decision (“yes” or “no”). We also study some properties of these rules, such as separability and weightedness, and show that these notions are not related. It is worth noting that multi-dimensional rules can be seen as voting rules (simple games) whose set of voters has a product structure.====A multi-dimensional decision process is separable if it can be decomposed voter by voter or dimension by dimension. In other words, we can represent some multi-dimensional systems by combining single-dimensional systems. We show that not all rules are separable, by means of examples. We then provide some necessary and sufficient conditions which guarantee separability (Proposition 1, Proposition 2, Proposition 3, Proposition 4). Remarks 1 to 6 give the specific structure of some separable multi-dimensional rules. The properties of separability are related to well-known paradoxes in the theory of voting: the referendum paradox (see Feix et al. (2004)) and the Ostrogorski paradox (Ostrogorski, 1970).====A multi-dimensional decision process is weighted if it can be represented by a mean of weights and a threshold for the system. When the system has only one dimension, Taylor and Zwicker (1992) produce sufficient and necessary conditions to ensure weightedness. This result is generalized to systems with multiple dimensions (Theorem 1, Theorem 2) and the independence between separability and weightedness is provided.====This paper is structured as follows: Section 2 introduces the general framework of multi-dimensional rules. Section 3 presents separability conditions, while Section 4 defines and characterizes weighted multi-dimensional rules. Finally, Section 5 shows that separability and weightedness are different notions.",Multi-dimensional rules,https://www.sciencedirect.com/science/article/pii/S0165489619300770,5 November 2019,2019,Research Article,207.0
Balbuzanov Ivan,"Department of Economics, University of Melbourne, L4 111 Barry St, The University of Melbourne VIC 3010, Australia","Received 21 May 2018, Revised 21 August 2019, Accepted 22 August 2019, Available online 26 August 2019, Version of Record 12 March 2020.",https://doi.org/10.1016/j.mathsocsci.2019.08.005,Cited by (1),"I study kidney exchange with strict ordinal preferences and with constraints on the lengths of the exchange cycles. Efficient deterministic mechanisms have poor fairness properties in this environment. Instead, I propose an individually rational, ordinally efficient and anonymous random mechanism for two-way kidney exchange based on Bogomolnaia and Moulin’s (2001) Probabilistic Serial mechanism. Individual rationality incentivizes patient–donor pairs who are compatible with each other to participate in the exchange, thus increasing the overall transplantation rate. Finally, individual rationality, ex-post efficiency and weak strategyproofness are incompatible for any mechanism.","The creation and design of living-donor kidney-exchange programs, which allow patient–donor pairs to exchange kidneys among themselves without the use of monetary transfers, have received significant attention recently. See Sönmez and Ünver (2013) for a recent survey.====The guidelines and legal frameworks governing kidney-exchange programs around the world enshrine the balance of the dual goals of efficiency and equity as the main desideratum. The National Organ Transplantation Act stipulates the equitable allocation of organs in the US, while the Council of Europe’s Convention on Human Rights and Biomedicine requires “equitable access to transplantation services” (Council of Europe, 2002). The United Network for Organ Sharing in the US has as its main goal the balancing of utility and justice (Wallis et al., 2011). Exchanges in Canada, Australia, and New Zealand place similar emphasis on equity (Malik and Cole, 2014; AKX, 2015; NRTS, 2017).====For legal and logistical reasons described in Section 2, the maximum number of pairs participating in each exchange cycle is usually limited in practice. Without randomization, such constraints prevent the equitable treatment of patients. Section 3 presents the model of constrained object exchange without monetary transfers with strict ordinal preferences. Within that context, Section 4 shows that deterministic mechanisms indeed fail to meet the desiderata in the presence of cycle-length constraints. Namely, anonymity and constrained Pareto optimality are incompatible with one another. Anonymity is the basic equity criterion considered by this paper and requires that mechanism participants’ identities are irrelevant for determining their expected allocations.==== ====Turning to the main results of the paper, Section 5 proposes a random mechanism in which each trade involves no more than two pairs, called the ==== (2CPS) mechanism. It is based on the Probabilistic Serial (PS) mechanism (Bogomolnaia and Moulin, 2001).==== ==== The 2CPS mechanism is individually rational, incentivizing the participation of compatible patient–donor pairs. It is ordinally efficient,==== ==== i.e. Pareto optimal with respect to first-order stochastic dominance, and, balancing the needs for efficiency and equity, also anonymous. The ==== (2CSE) mechanisms, a more general class of mechanisms, retains the first two properties and can be defined so that agents are categorized in priority classes and all within a given priority class receive equitable treatment.==== ==== In the other main result of the paper, Section 6 shows that no mechanism satisfies a given cycle-length constraint while being individually rational, efficient, and weakly strategyproof.==== ====Following Nicoló and Rodríguez-Álvarez (2012) and subsequent work, I depart from the seminal kidney-exchange papers’ assumption of dichotomous preferences, which views all compatible kidneys as perfect substitutes. As discussed in Section 2, a variety of factors beyond blood- and tissue-type compatibility affect the survival rates of kidney grafts. Thus, reducing the problem to dichotomous compatibility-based preferences ignores welfare-relevant information. I assume strict preferences as in Nicoló and Rodríguez-Álvarez (2012).====More importantly, an individually rational mechanism taking non-dichotomous preferences into account incentivizes the participation of any patient, who is compatible with her related donor, by guaranteeing her a kidney with expected graft-survival rate at least as high as her donor’s. In contrast, if collapsing to dichotomous compatibility-based preferences, such a patient may receive a kidney with worse expected outcome than her donor’s. The participation of such compatible pairs would increase the transplantation rates for incompatible pairs.==== ==== The literature on the participation of compatible pairs is discussed in Section 1.1.====The 2CSE and 2CPS mechanisms are based on a simultaneous-eating algorithm. The algorithm treats all kidneys ==== they are infinitely divisible objects and all agents ==== they are claiming increasing object shares in continuous time starting with their most preferred object. The algorithm ends when all objects have been completely claimed or, equivalently, when all patients have one unit of probability shares. The share that patient ==== has claimed from object ==== is treated as the probability with which patient ==== receives kidney ====. This induces a symmetric probability-share matrix ====, where ==== denotes the probability that patient–donor pairs ==== and ==== trade with each other or, if ====, the probability that pair ==== does not participate in an exchange with another pair. Special care needs to be taken to guarantee that the resulting matrix represents a lottery over deterministic matchings satisfying the cycle-length constraint. The usual approach, based on the Birkhoff–von Neumann theorem, is not viable here. Instead, I use Edmonds’ characterization of the matching polytope (Edmonds, 1965), which implies sufficient and necessary conditions for a bistochastic matrix to be the representation of a lottery over two-way exchanges.",Short trading cycles: Paired kidney exchange with strict ordinal preferences,https://www.sciencedirect.com/science/article/pii/S0165489619300654,26 August 2019,2019,Research Article,208.0
Horan Sean,"Département de sciences économiques, Université de Montréal and CIREQ, C-6018 Pavillon Lionel-Groulx, 3150 rue Jean-Brillant, Montréal QC, Canada H3T 1N8","Received 27 November 2018, Revised 20 September 2019, Accepted 23 September 2019, Available online 3 October 2019, Version of Record 29 October 2019.",https://doi.org/10.1016/j.mathsocsci.2019.09.006,Cited by (14)," model as a point of departure, I investigate what can be learned about models of choice with default when the no-choice behavior of the decision-maker is unobservable.","When modeling discrete choice behavior, it is natural to include a “default” option to capture situations where the decision maker selects ==== of the feasible alternatives.==== ==== The idea dates back to Corbin and Marley (1974).==== ==== However, the practice only really took off with the recent work of Manzini and Mariotti (2014) and others (Aguiar, 2015, Aguiar, 2017, Berbeglia, 2018, Brady and Rehbeck, 2016, Demirkan and Kimya, 2018, Echenique et al., 2018, Zhang, 2016). This is a welcome development in the discrete choice literature. Indeed, as the experimental evidence suggests, choice avoidance and deferral are rampant phenomena (Dhar, 1997). At the same time, the practice of including a default option presents an empirical challenge since, as Brady and Rehbeck (p. 1204) point out, “it may be difficult to observe a consumer “choosing” nothing” outside of an experimental setting.====In the current paper, my objective is to investigate what can be learned about ==== when non-choice probabilities are unobservable. My point of departure is Manzini and Mariotti’s model of ====, where the decision maker independently notices each feasible alternative with some probability before selecting the most preferred of the alternatives noticed. Since there is some chance that she notices nothing, this is a model of choice with default. Since choice data seldom contains information about non-choice however, the model is difficult to test directly. For this reason, I consider a ==== version of Manzini and Mariotti’s model that “nets out” the probability of non-choice before renormalizing the choice probabilities of the feasible alternatives. This gives an indirect way to test the model against observable choice frequencies. Manzini and Mariotti discuss this version of their model but leave its axiomatization as an open question.====I establish three results for the standardized independent random consideration model. Theorem 1 axiomatizes the model in terms of key differences from the ==== (Luce, 1959, McFadden, 1974), the workhorse of the empirical literature. In turn, Theorem 2 shows that the model parameters can be identified from choice on small menus. Finally, Theorem 3 shows that the model is consistent not only with Block and Marschak’s (1960) ==== (RUM) but also Luce and Suppes’ (1965) ====, where the utilities of the alternatives are independently distributed.====Together, these results show that the no-choice option does not play a significant role in the independent random consideration model. For one, the axioms that characterize the standardized model also serve, with minor modification, to characterize the original model (Proposition 1). What is more, missing no-choice data does not present a problem for identification: the consideration function and preference, even the no-choice probabilities can be determined from observed choice frequencies (Proposition 2). Finally, the original model is also independent RUM (Proposition 3).====I conclude the paper by asking whether the lessons learned from independent random consideration have broader implications. For ==== models of choice with default, I show that the standardized model inherits some features of the original model. Perhaps the most striking connection is that the standardized model is RUM whenever the original model with default is RUM (Theorem 4).====At the same time, I emphasize that independent random consideration is somewhat unusual in terms of its close connection to the standardized model. For other models, the no-choice option plays a more integral role. As a result, the properties of the standardized model (in terms of axiomatics and identification) may be quite different from the original model with default. When this is the case, it is critical to ask whether the targeted applications are ones where non-choice probabilities are observable.==== The current paper is part of a recent literature which extends discrete choice models to incorporate non-choice.==== ==== While this literature generally acknowledges the challenges that arise when non-choice is unobservable, few papers address these challenges directly. One notable exception is Aguiar (2015), who gave axiomatic foundations for a generalization of Manzini and Mariotti’s model of choice with default ==== the standardized version of the model. Echenique et al. (2018) also considered a version of their model without default but take a different approach. Instead of renormalizing the choice probabilities, they directly remove the default option from the representation.====Also worth noting is the connection between Theorem 3, Theorem 4 of the current paper and the rich literature on random utility. Theorem 3, in particular, shows that correlation among the utilities of alternatives is not required for context-dependent patterns of substitution. This distinguishes independent random consideration from some widely-used models of random utility, including McFadden’s (1978) generalized extreme values and Revelt and Train’s (1998) mixed logit.====Finally, the current paper is related to the work of Marley (1991) and Ravid (2015). To define a general class of models where discrete choice behavior is based on binary advantage, Marley proposed a weak version of the L-Independence condition that I use to characterize independent random consideration (in Theorem 1 and Proposition 1). Independent from my work, Ravid recently proposed a model of discrete choice that is entirely characterized by L-Independence.",Random consideration and choice: A case study of “default” options,https://www.sciencedirect.com/science/article/pii/S0165489619300769,3 October 2019,2019,Research Article,216.0
Chèze Guillaume,"Institut de Mathématiques de Toulouse, UMR 5219 Université de Toulouse, CNRS UPS IMT, F-31062 Toulouse Cedex 9, France","Received 13 December 2018, Revised 17 September 2019, Accepted 23 September 2019, Available online 28 September 2019, Version of Record 1 October 2019.",https://doi.org/10.1016/j.mathsocsci.2019.09.005,Cited by (3),In this article we suggest a ====We show that in this model there exist explicit couples of measures for which no algorithm outputs an equitable fair division with connected parts.====We also show that there exist explicit sets of measures for which no algorithm in this model outputs a fair division which maximizes the utilitarian ====.====The main tool of our approach is ====.,"In 1837, Pierre Wantzel showed that there exists no general construction using only compass and straightedge which divides an angle into three equal angles. The proof relies on algebra and field theory. The angle trisection problem can be seen as a fair division problem: we have a portion of pizza and we want to divide it in a fair way between three friends (by using only compass and straightedge constructions…). Wantzel’s theorem says that this problem has no solution.====In this article, we are going to study a similar fair division problem and we are going to use similar tools.====In the following, we consider a heterogeneous good, for example: a cake, land, time or computer memory, represented by the interval ==== and ==== players with different points of view. We associate to each player a non-atomic probability measure ==== on the interval ====. These measures represent the utility functions of the player. This means that if ==== is a part of the cake then ==== is the value associated by the ====th player to this part of the cake. As ==== are probability measures, we have ==== for all ====.====A division of ==== is a partition ==== where ==== is the part given to the ====th player. A division is ==== when each ==== is an interval.====Several notions of fair division exist.====We say that a division is ==== when for ====, ====.====We say that a division is ==== when for ====, we have ====.====We say that a division is ==== when for all ====, we have ====.====We say that the division ==== ==== when ==== for all partitions ====.====The problem of fair division (theoretical existence of fair division and construction of algorithms) has been studied in several papers (Steinhaus, 1948, Dubins and Spanier, 1961, Even and Paz, 1984, Edmonds and Pruhs, 2011, Brams and Taylor, 1995, Robertson and Webb, 1997, Pikhurko, 2000, Thomson, 2006, Procaccia, 2013, Brams et al., 2013, Aziz and Mackenzie, 2016), and books about this topic, see e.g. Robertson and Webb (1998), Brams and Taylor (1996), Procaccia (2016) and Barbanel (2005). These results appear in the mathematics, economics, political science, artificial intelligence and computer science literature. Recently, the cake cutting problem has been studied intensively by computer scientists for solving resource allocation problems in multi-agent systems, see e.g. Chevaleyre et al. (2006), Chen et al. (2013), Kash et al. (2013) and Brânzei and Miltersen (2015).====A practical problem is the computation of fair divisions. In order to describe algorithms we thus need a model of computation. There exist two main classes of cake cutting algorithms: discrete and continuous protocols (also called moving knife methods). Here, we study only discrete algorithms. These kinds of algorithms can be described thanks to the classical model introduced by Robertson and Webb and formalized by Woeginger and Sgall in Woeginger and Sgall (2007). In this model we suppose that a mediator interacts with the agents. The mediator asks two types of queries: either cutting a piece with a given value, or evaluating a given piece. More precisely, the two types of queries allowed are:====In the Robertson–Webb model the mediator can adapt the queries from the previous answers given by the players. In this model, the complexity counts the finite number of queries necessary to get a fair division. For a rigorous description of this model one can consult: Woeginger and Sgall (2007) and Brânzei and Nisan (2017).====The result of a query is a real number and thus the mediator has to manipulate real numbers. There exist two possible models of computation which allow this task.====First, we can consider computable real numbers. Roughly speaking a real number is said to be computable if there exists a Turing machine which writes digit by digit the (infinite) decimal expansion of this number. Unfortunately, this model of computation is not natural in our setting because we cannot decide in this model if a computable number is equal to 0. This means that we cannot decide if two computable numbers are equal. Thus, with this model, the mediator cannot check if a fair division is equitable.====Second, we can consider the BSS model of computation. This model has been developed by Blum, Shub and Smale (BSS). It allows one to study algorithms over a ring. Roughly speaking a BSS machine has registers which can hold arbitrary elements of the studied ring (here ====), and perform exact arithmetic (====) and can branch on conditions based on exact comparisons (====). Furthermore, with this theory when the ring is ==== then we recover the classical theory of Turing machine. For a detailed description of this model see Blum et al., 1989, Blum et al., 1998.====In this article we are going to suppose that the mediator uses a BSS machine. We call this new model of computation the BSSRW model (Blum–Shub–Smale–Robertson–Webb model) and we are going to prove impossibility results.====In the fair division literature some impossibility results have been already given.====Stromquist in Stromquist (2008) has proved that there exists no algorithm giving a simple and envy-free fair division for ==== players. When ====, the classical “Cut and Choose” algorithm gives a simple and envy-free fair division.====Cechlárová and Pillárová have shown, in Cechlárová and Pillárová (2012), that there exists no algorithm computing a simple and equitable fair division for ==== players in the Robertson–Webb model.====The strategy used in these articles is the following: they suppose that an algorithm computing the desired division exists and then by an iteration process they construct from this algorithm a set of measures giving a contradiction. Thus they obtain a result of this kind: for all algorithms in the Robertson–Webb model there exists a set of measures for which the desired fair division cannot be given.====It must be noticed that this approach gives for each algorithm a set of measures leading to a contradiction. Thus the set of measures is related to the algorithm. Moreover, the measures are not explicitly given. Therefore, we can imagine that these sets of measure correspond to very complicated situations not appearing in practice and that for “reasonable” sets of measures the contradiction does not occur.====Procaccia and Wang have also given an impossibility result for equitable fair division in Procaccia and Wang (2017). As a corollary of a theorem about a lower bound for equitable division they deduce that there exists no algorithm giving an equitable fair division. However, with this approach we still cannot give an explicit example of measures such that no algorithm in the Robertson–Webb model returns an equitable division with this input.====In the first part of this article, we are going to study simple equitable fair divisions. This topic has been less studied than proportional and envy-free divisions. However, there exist some results showing the existence of such fair divisions (Cechlárová et al., 2013, Segal-Halevi and Sziklai, 2018, Chèze, 2017). Furthermore, if we consider a continuous protocol, it is possible to get an equitable fair division (not necessarily simple) thanks to Austin’s moving knife procedure, see Austin (1982). Recently, Brams and Landweber have given a method which computes a maximally equitable division when ====, see Brams and Landweber (2018). However, their approach does not use one of the standard models of computation. Indeed, in this method the players give their measures to the mediator.====Here, we are going to give explicit examples where two players cannot get an equitable fair division with connected parts if we use our suggested model of computation.====The strategy used to prove this theorem is the following: We are going to show that if there exists an equitable and simple division ==== then the final cutpoint ==== must satisfy a polynomial equation. Then, with elementary field theory, we can show that ==== cannot be computed with the BSSRW model.====Now, if we use Abel’s impossibility theorem and Galois’ theory showing that some polynomials are not solved by radicals, then we obtain other examples as stated in the next theorem:",Cake cutting: Explicit examples for impossibility results,https://www.sciencedirect.com/science/article/pii/S0165489619300757,28 September 2019,2019,Research Article,217.0
"Hernandez Gonzalo,Muñoz Roberto","Facultad de Ingeniería y Ciencias, Universidad Adolfo Ibañez, Chile,Centro Científico Tecnológico de Valparaíso, Chile,Departamento de Ingeniería Comercial, Universidad Técnica Federico Santa María, Chile","Received 13 January 2017, Revised 27 August 2019, Accepted 5 September 2019, Available online 26 September 2019, Version of Record 30 September 2019.",https://doi.org/10.1016/j.mathsocsci.2019.09.002,Cited by (4),", where we study the impact of alternative regulatory interventions. We provide numerical evidence suggesting that policies designed to reduce horizontal differentiation might be more effective than those designed to limit access charges; this result seems robust to the presence of different forms of price discrimination. We should interpret these findings cautiously due to the existence of potential implementation costs for each policy.","In recent decades, a growing stream of the literature has focused on social interactions modeled through the use of network structures or graphs, in which agents are represented by nodes, and their relationships are represented by arcs between those nodes. These network structures play an important role in many economic situations and have been widely studied. Jackson (2010) provides an outstanding summary of theory and applications. However, as far as we know, no other authors have focused on modeling customer preferences and participation decisions in an oligopolistic and differentiated industry, in which customers interact through a social network, and firms may use nonlinear pricing schemes.====By treating customers in a social network environment, we allow for the number of interaction events (calls, messages, visits, games, social meetings, money transfers, etc.) between any pair of individuals to depend not only on operator prices and the level of differentiation in services but also on how “close” they are in the social network. This context has been shown to be relevant not only in determining more realistic market outcomes but also in studying how regulation should be implemented when it is needed. Over the last several years, some influential articles in the telecommunications literature have focused on the study of equilibrium interconnection strategies, in a framework with recognized consumer heterogeneity (see, for example, Dessein, 2004, Hahn, 2004). These approaches represent a significant improvement in the effort to obtain more realistic models. However, they lack a consumer social network structure—heterogeneity is instead modeled by varying the consumer propensity to interact.====Several papers are closely related to this article. The seminal ones are in the telecommunication literature: Laffont et al., 1998a, Laffont et al., 1998b and Armstrong, 1998, Armstrong, 2002. The equilibrium behavior of competing firms in the presence of heterogeneous consumers has been analyzed by Dessein, 2004, Hahn, 2004, Hurkens and Jeon, 2009, Hoernig et al., 2011, and Jullien et al. (2013), among others. Cambini and Valletti (2008) model information exchange where the closer the parties are in social terms, the higher the intensity of information exchange, but a social network is not explicitly modeled. The use of social networks to model the connections among consumers was introduced by Harrison et al. (2006) but in a context of linear pricing schemes and in the absence of agents’ participation decisions.====In this article, we model a game where consumers are connected through a social network, and they have to make optimal participation and preference decisions in a market characterized by the presence of two competing firms (see Hellmann and Staudigl (2014), for a survey of general coevolutionary models of network formation and play). As our main goal is to develop a tool to study the impact of a social network on the demand side of the market, we adapt the model developed in Laffont et al., 1998a, Laffont et al., 1998b, accounting for how consumers are connected through the network. In particular, while firms offer horizontally differentiated products to maximize profits, consumers optimally decide to sign a service contract with only one of the firms based on the connection each firm offers in the network. For example, when two firms offer horizontally differentiated services, consumers will optimally decide to which firm to subscribe (if any). Even more, in this decision, consumers will take into account not only each firm’s pricing scheme, but also their own position in the network. Our modeling strategy seeks to be flexible enough to study a variety of economic problems while simplifying exposure by using profit functions similar to those in the telecommunications industry. It is clear, however, that profit functions can be easily changed to represent other industries characterized by the presence of social networks on the demand side. Similar approaches have been used in other regulated industries in the presence of strategic interaction (Safari et al., 2014).==== ====In particular we are interested in analyzing the role of firms’ price discrimination because different forms of price discrimination are prevalent in network markets. We look at two prevalent nonlinear pricing schemes: the first is price discrimination that depends on whether the counterpart in the transaction is a client. This is the case for on-net–off-net prices that exist in a variety of industries including telecoms, social clubs (with access to restaurants, cinemas, etc.) or sport clubs (golf, tennis, etc.).==== ==== The second is when two-part tariffs are feasible, which is in fact even more common than the previous scheme.==== ==== At first glance, adding price discrimination to the problem may seem unnecessary; however, we will show that this pricing scheme has important effects on the way agents interact in network markets.====Finally, we illustrate the applicability of the model by studying the effectiveness of two alternative regulatory interventions under different pricing schemes in telecommunication markets. First, we provide numerical evidence that suggests the traditional regulatory intervention (reduced access charges) produces a positive impact on competition and welfare by reducing the equilibrium price faced by consumers. Second, we consider an alternative policy intervention in which horizontal differentiation is reduced. Our numerical findings also suggest that policies designed to reduce horizontal differentiation might be more effective than those designed to limit access charges. Welfare also increases when the social network is more dense, but we do not consider this network characteristic to be subject to policy manipulation.====The rest of the paper is organized as follows: in Section 2, we develop the basic economic model, including pricing schemes, agents’ optimal decisions, horizontal differentiation and the general firm problem. In Section 3, we develop an example application in the regulation of the telecom industry. Finally, the conclusions are stated in Section 4.",A discrete model of market interaction in the presence of social networks and price discrimination,https://www.sciencedirect.com/science/article/pii/S0165489617300112,26 September 2019,2019,Research Article,218.0
"Ho Wai-Hong,Zhu Lin","Department of Economics, Faculty of Social Sciences, University of Macau, Macao,Department of Economics, Trade and Statistics, Business School, Jiangsu University of Technology, China","Received 16 October 2017, Revised 4 July 2019, Accepted 10 September 2019, Available online 19 September 2019, Version of Record 26 September 2019.",https://doi.org/10.1016/j.mathsocsci.2019.09.003,Cited by (0),"This paper explores the role of bubbles in an overlapping-generations economy where some agents are more productive than the others. Due to missing credit markets, productive and unproductive agents cannot trade their financial assets to achieve mutual benefit, making the economy to perform below its potential. Introducing bubbles on the one hand crowds out some productive resources as it does in the current literature. It on the other hand may divert resources away from unproductive agents to productive ones. We find that bubbles are growth enhancing when the unproductive agents’ productivity level lies within an interval. Furthermore, in sharp contrast to the existing literature, the equilibrium bubble size is stable. This finding enables us to construct a two-state stationary sunspot equilibrium in which one state is associated with small bubbles and low growth whereas the other one has large bubbles and high growth.","What are bubbles? Why do they exist? How do they affect capital accumulation and growth? These questions have been perplexing the public, policy makers and economists for decades. One well-received thinking is that, just like what Tirole (1985) has argued, bubbles can only be considered as a remedy to the problem of dynamic inefficiency. Imagine an economy where capital is a productive asset and serves as a mean to store value over time. When this economy is over-accumulating capital in the sense that the amount of investment required to sustain this capital accumulating process is even higher than the return that this process can generate, bubbles can be used to release those inefficient investment and doing it can yield return which is bigger than that of the investment process it replaces. However, if this economy is not over-accumulating capital, bubbles will have no useful role to play and hence should not exist. Later on, Grossman and Yanagawa (1993) examine the role of bubbles in a setting similar to the one in Tirole (1985) except that growth is perpetual. They find that bubbles, whenever they exist, are growth ====. Recently, Martin and Ventura (2012) show that in an economy with two different types of agents, one is productive and the other one is unproductive, bubbles can transfer resources from the latter type of agents to the former type, thereby mitigating the effects of financial frictions. These transfers of resources through bubbles can expend the economy’s capital stock and output. Their finding is very interesting and has been widely regarded as a conceptual breakthrough in this line of research. But one limitation of their theory is that it is based on a neoclassical growth environment. In this paper, we extend the framework of Martin and Ventura (2012) to an environment where growth is driven by capital production a la Romer (1986). Such simple specification can help us to bring out the similarities and differences with the Grossman and Yanagawa’s (1993) analysis at the sharpest manner.====We consider a very stylized two-period overlapping generations model borrowed heavily from Martin and Ventura (2012). In each generation, agents are divided into two types in accordance with their productivity levels. For the type of agents who are productive, they are able to convert one unit of output into ==== units of capital. Their unproductive counter-parts can convert one unit of output into ==== units of capital only where ==== is smaller than one. Just like Martin and Ventura (2012), these two types of agents are not allowed to borrow from or lend to each other due to frictions in credit market. If bubbles do not exist, the economy will certainly perform below its full potential level because agents must invest their own resources. When bubbles are introduced, then, the possibility of trade across agents is open. Bubbles start in the economy with zero cost and they do not produce any output. The reason why anyone wants to buy bubbles is to sell them at a later date. There are two kinds of owners/suppliers of bubbles: those old agents who purchased bubbles at their youth age and those young agents who create new bubbles. Naturally, only young agents may choose to purchase bubbles since the old do not save. We find that, when the level of unproductive agents’ productivity lies within an interval, it is the unproductive agents who create bubbles and sell them to productive ones. In such situation, bubbles promote growth. Furthermore, the equilibrium size of bubbles is unique and stable. That means, whenever bubbles deviate from their equilibrium size, they tend to move back to its original size. To the contrary, in Grossman and Yanagawa (1993), Hashimoto et al. (2017) and Kunieda and Akihisa (2016), if bubbles happen to deviate from their equilibrium size, they never return to it. In other words, the equilibrium size of bubbles in their models is unstable. With the aid of this feature in the bubbly steady state and self-fulfilling belief, we are able to create a rational expectations equilibrium with two-state support variables, in which one state has large bubbles and high growth whereas the other state has small bubbles and low growth. Arguably, this finding can be used to explain why some countries have big bubbles and some have small bubbles even though their economic fundamentals have no big differences.====This is of course not the first paper which addresses the role of bubbles in an endogenous growth framework plagued by credit market frictions. Recently, Hirano and Yanagawa (2017) show that bubbles are more likely to promote growth when the degree of pledgeability is relatively low. Our paper and theirs have several differences. Firstly, they use an infinite horizon setup while ours is a standard two-period OLG model. Secondly, their source of financial frictions is limited pledgeibility on using future income for loan repayment. In our model, credit markets are just simply assumed away. Thirdly, we show that the degree of unproductive agents’ productivity is critical to the usefulness of bubbles even when the quality of the financial system is poor (i.e. the degree of pledgeability is very low). One notable contribution in this line of research is Kunieda (2014) which shows in a perpetual youth model with financial frictions how bubbles can be used to promote capital accumulation in a neoclassical growth model. The rest of this paper is structured as follows. Section 2 lays out the basic environment. Section 3 studies the steady state situation of the model. Section 4 derives a self-fulfilling recurrent bubbly episode with two cycles as a rational expectation equilibrium. Section 5 concludes and discusses some possible extensions.","Bubbles, growth and sunspots with credit market frictions",https://www.sciencedirect.com/science/article/pii/S0165489619300733,19 September 2019,2019,Research Article,219.0
Bolle Friedel,"Europa-Universität Viadrina, Frankfurt (Oder), D - 15207, Frankfurt (Oder), Germany","Received 23 January 2019, Revised 31 August 2019, Accepted 1 September 2019, Available online 18 September 2019, Version of Record 25 September 2019.",https://doi.org/10.1016/j.mathsocsci.2019.09.001,Cited by (6),"The ==== of some members of parliament sometimes imply voting against the party line, but these members fear sanctions. In the mostly unique pure strategy equilibrium of such a ====, voters always follow the party line, even if their true preferences are strong and (opportunity) costs from sanctions are arbitrarily small. I qualify this disturbing result by applying the equilibrium selection theory of Harsanyi and Selten (1988). Pure strategy equilibria are selected only for large deviation costs. Otherwise, mixed strategy equilibria are selected which support majorities in true preferences (====), for negligible costs almost with certainty.","Voting is the central institution of collective decision making in democracies. In addition to parliaments, it is used in associations, parties, and international organizations. A further large field of application is common ownership in the form of shareholder firms or condominiums. Parliamentary voting mostly follows party lines, but the larger the impact of a decision and the more it touches fundamental ethical values, the more influential are individual (====) preferences concerning the outcome of the vote. ==== try to keep deviations from the party line at a low level by making them costly for deviators. Lobbyists generate (opportunity) costs for certain decisions, thus counteracting or reinforcing the influence of party whips. In organizations and shareholder firms, there are often long-term coalitions or personal relations of members or owners, providing and requiring solidarity. In this paper, I want to discuss situations where voters experience conflicts because following their true preferences is connected with costs of voting against the party line,==== ==== against their coalition, against friends, against the public opinion, or against bribes. This conflict is the core of the ====.====Groseclose and Milyo (2010) find that most voting games have a unique pure strategy equilibrium. Does this result settle the question of behavior in voting games? A unique pure strategy equilibrium is certainly salient; but, in the case of voting games, it is dubious because, in these equilibria, voters ==== follow their true preferences. There are indeed parliaments with high party discipline. The 35th Canadian Parliament (1993–97) saw dissent in any party only in 160 of 735 divisions (Malloy, 2003); i.e., in about four of five votes all party whips succeeded in preventing any deviation. In other parliaments, deviations are more frequent. It is plausible that, ceteris paribus, the magnitude of punishment is decisive, which depends on legal and institutional issues but also on the number of members with deviating true preferences. If there are few possible deviators then party whips’ threat of severe punishment (e.g. expulsion from the party) may be believable; if there are many deviators, it becomes questionable. The votes in the British parliament concerning ==== where Party Whips have completely lost control are good examples. Both parliaments use the ==== where party whips have official positions (with chief whips and deputies). In the face of the different success of party whips, I want to investigate the plausible conjecture that party whips’ success depends on high enough and believable threats of punishment.==== ====The voting game belongs to a class of coordination games called ==== (BTPG) games. The basic game of this class has ==== players who can produce a public good (pass a proposal) with benefits ==== for player ==== if and only if at least ==== of them contribute a service (vote Yes) with costs ====. There are many examples of such games beyond voting and, although we will describe our games in a voting terminology, the results are applicable to general BTPG games. The special assumption in ==== is that, from the viewpoint of some players a public good is produced while, for others, it is a public bad. Such a situation is characteristic not only for voting but also for the production of many public goods if, for example, aesthetic or ethical questions are involved. In the literature, players in BTPG games usually ==== enjoy the production of the public good. Therefore, we stick to this label and call such games ====. Alternatively, ==== players suffer from the existence of the public good. Again following the literature on public goods, we call such games ====. As far as I know, voting games have been investigated only by Groseclose and Milyo, 2010, Groseclose and Milyo, 2013 and Bolle (2018). Models which feature a decision between “voting” or “not voting” are related but not equivalent (Downs, 1957, Riker and Ordeshook, 1968, Palfrey and Rosenthal, 1985). These models are mainly concerned with the “paradox of voting”, namely that we observe considerable participation rates in general elections although a single voter is decisive with a negligible probability. The literature on simultaneous and sequential voting is vast and differentiated, but the models are rather different from our voting game.====At first glance, BTPG games are rather simple. Take the case of ==== players (voters) and a necessary majority of ==== Yes votes for passing a proposal. Pure strategy equilibria can easily be characterized but mixed strategy equilibria cannot. Numerical computations show that such games can have between zero and eight strictly mixed strategy equilibria. In this paper, it is shown that even symmetric games can have asymmetric strictly mixed strategy equilibria. In the case ====, these are completely characterized by Proposition 5, Proposition 6. If game theory can teach us something about the outcome of real voting games, then we must not disregard this multiplicity of equilibria. Concentrating on ==== (where all cost/benefit ratios ==== are equal), this paper is a first step in this direction. I will characterize these equilibria and ask whether the pure strategy equilibrium or one of the other equilibria is the winner in the procedure of equilibrium selection proposed by Harsanyi and Selten (1988). The result supports plausibility: If costs of deviations (punishment) are high enough, pure strategy equilibria emerge where party whips succeed. With lower costs, mixed strategy equilibria emerge which honor ==== (majorities in true preferences) with a certain probability, for negligible punishment almost with certainty.====Why apply Harsanyi and Selten’s (1988) concept? Let us first assert that the use of games for the discussion of social interactions ==== needs additional assumptions if there is more than one equilibrium. Strategies for tackling the problem of multiple Nash equilibria are ==== or ==== of equilibria. Refinement employs equilibrium concepts with additional requirements (e.g. subgame perfectness) and is most important for dynamic games. For static games, mostly equilibrium selection is applied. Assuming a unique pure strategy equilibrium (if existent) to be played is one possibility. Other concepts rely on “tracing procedures”. They follow a continuous path of best reply combinations starting from a state where players expect all others to act randomly or where their information about the parameters of the game is minimal. Continuously reducing the expectation of random behavior or of “noise” we reach, under certain conditions, one of the Nash equilibria of the game. Harsanyi and Selten (1988) have introduced the Linear Tracing Procedure and the related concept of Risk Dominance. Let us now regard the (in my view) two most prominent competitors of this concept. Carlsson and Van Damme (1993) transform common knowledge games into games of incomplete information with private and correlated signals (Global Games). While incomplete information (noise) vanishes, equilibrium play converges, under certain conditions, to one of the Nash equilibria of the original game. A further prominent method of equilibrium selection was suggested by McKelvey and Palfrey (1995) as a limit of their Quantal Response Equilibria (QRE) which assume all strategies (from a finite set) to be played with probabilities that are ordered according to the utilities these strategies gain against the strategies of the other players. When random deviations from the best response vanish, QRE (generically) converges to a Nash equilibrium.==== ==== The Global Games selection, however, has turned out to be easily applicable only in games where strategies are strategic complements. Frankel et al. (2003) show that, otherwise, the result may depend on the distribution of “noise”. As, in voting games, equilibria coexist where strategies are strategic complements or substitutes (see Proposition 4) the Global Games selection does not seem to be suitable. The QRE selection suffers from the problem that QREs depend not only on cost/benefit ratios but also on the magnitude of costs and benefits. This also applies to limits of QRE. Therefore, our choice of the QRE selection depends on the question whether we assume two players with ==== and ==== as symmetric or not. As, after a linear positive transformation, their utilities are equal, Harsanyi and Selten (1988) assume these players to be symmetric and play equal strategies. (Note that players with ==== and ==== are not symmetric.) Accepting this argument, we should not apply the QRE selection. I admit, however, that also the stance that magnitudes matter has some plausibility.==== ==== In any case, investigating equilibrium selection in the class of almost symmetric BTPG games via QRE seems to be an awkward choice.====Concerning BTPG games, it is mainly the extreme games, namely the Volunteer’s Dilemma (==== in public good games, ==== in common pool games) and, in particular, the ==== (==== in public good games and ==== in common pool games) which are discussed in the literature. The Volunteer’s Dilemma has been introduced and investigated by Diekmann, 1985, Diekmann, 1993. The Stag Hunt game has been introduced by Rousseau (1997, original edition 1762). The symmetric 2 × 2 Stag Hunt game is most popular for theoretical and experimental discussions of equilibrium selection by Pay-Off Dominance vs. Risk Dominance or other concepts of equilibrium selection. For an overview about experimental studies of BTPG games see Spiller and Bolle (2017).====In the following, I will describe the equilibria of BTPG games and will add some new general results concerning the structure of equilibria. Mainly, however, we concentrate on almost symmetric voting games, which are important as benchmarks and principle examples for theoretical considerations. In the next section, the model is presented and pure strategy equilibria are described. In Section 3, some general properties of strictly mixed strategy equilibria are derived. Section 4 investigates almost symmetric BTPG games. In Section 5, Harsanyi and Selten’s (1988) proposal for equilibrium selection is applied and discussed. Section 6 concludes. Proposition 2(iii) is a simple but extremely helpful description of the expected revenues of players. Proposition 3, Proposition 5–9 describe new results.",When will party whips succeed? Evidence from almost symmetric voting games,https://www.sciencedirect.com/science/article/pii/S0165489619300721,18 September 2019,2019,Research Article,220.0
"Kelly Jerry S.,Qi Shaofang","Department of Economics, Syracuse University, Syracuse, NY 13244-1020, USA,School of Business and Economics, Humboldt University Berlin, Spandauer Str. 1, 10178 Berlin, Germany","Received 21 November 2018, Revised 5 May 2019, Accepted 14 August 2019, Available online 16 September 2019, Version of Record 30 September 2019.",https://doi.org/10.1016/j.mathsocsci.2019.08.003,Cited by (2)," if, for each pair of alternatives, ==== and ====, and each pair of individuals, ==== and ==== , whenever a profile has ==== adjacent to but just above ==== for individual ==== while individual ==== has ==== adjacent to but just above ====, then only switching ==== and ====.","We consider a new condition, balancedness. A social choice correspondence satisfies ==== if, for each pair of alternatives, ==== and ====, and each pair of individuals, ==== and ====, whenever a profile has ==== adjacent to but just above ==== for individual ==== while individual ==== has ==== adjacent to but just above ====, then only switching ==== and ==== in the orderings for both of those two individuals leaves the choice set unchanged.====Social choice theory often treats responsiveness conditions, like monotonicity, but balancedness is a ==== property. It is a natural equity condition that simultaneously incorporates some equal treatment for individuals short of anonymity, some equal treatment of alternatives short of neutrality, and some equal treatment of position of alternatives in orderings (for example, raising ==== just above ==== in the bottom two ranks for individual ==== exactly offsets lowering ==== just below ==== in the top two ranks for ====).====With only two alternatives, balancedness is equivalent to anonymity. With more than two alternatives, balancedness is a condition independent of anonymity. Any dictatorial correspondence, however, is unbalanced. For a dictatorial correspondence, one and only one individual is “effective”: changing preferences for every individual other than the dictator leaves the choice set unaffected. In addition to excluding dictatorship, we show that balancedness implies that any non-constant social choice correspondence must have every individual to be effective (Theorem 1).====For the equal treatment of positions incorporated by balancedness, we present results from different perspectives. First, we consider the interaction of balancedness with a specific property, tops-only. By tops-only, any change that affects no one’s top-ranked option leaves the choice set unaffected. Plurality voting is an obvious tops-only rule. Unlike balancedness, tops-only clearly treats the top-rank differently from the remaining ones. The expected conflict between balancedness and tops-only is characterized through a kind of impossibility result (Theorem 2): whenever a social choice correspondence satisfies both balancedness and tops-only, it remains constant on all non-unanimous profiles (i.e., profiles without common option top-ranked by all individuals).====We next examine the implication of balancedness within the class of scoring rules. A scoring rule assigns weights to ranks, lower weights to higher ranks, and the score for each option is the sum of weights corresponding to all ranks that option occupies in individual orderings. The Borda rule adds a constant value to weights from one rank to the next, and picks the alternatives with the lowest score. Balancedness holds for the Borda rule: the changes of scores from raising ==== just above ==== in the bottom two ranks for individual ==== is exactly offset by the changes of scores from lowering ==== just below ==== in the top two ranks for ====, leaving the choice set unchanged. We show that except for a counterexample with small numbers of individuals and alternatives, balancedness actually uniquely characterizes the Borda rule within the class of scoring rules (Theorem 4, Theorem 5, Theorem 6).====To be clear, we are not examining balancedness because we advocate it as a property that ==== hold for social choice correspondences. Rather we study balancedness because it is a common property of several correspondences that have been central to social choice theory. As noted, Borda satisfies balancedness. The Pareto correspondence, which at profile ==== selects those alternatives ==== such that no other alternative ==== that everyone prefers to ====, also satisfies balancedness. The majority preference relation is unaffected by an interchange of any transposition pair. Accordingly, a social choice correspondence constructed from majority voting relations will also satisfy balancedness. This category includes examples like Copeland’s rule (which selects the alternatives that defeat the most other alternatives by simple majority vote) and the top-cycle correspondence (which selects the alternatives that are highest in the transitive closure of the majority voting relation).====In fact, with more than two alternatives, anonymity, neutrality, the Pareto condition and balancedness simultaneously hold for the Pareto correspondence,==== ==== the Borda rule, the Copeland rule, and top-cycle, while all those rules fail tops-only. On the other hand, anonymity, neutrality, the Pareto condition and tops-only simultaneously hold for plurality and union-of-the-tops, while balancedness fails for both of them. Dictatorship fails balancedness but not tops-only, and maximin fails both. Thus besides anonymity, balancedness is a condition also independent of unanimity, neutrality, the Pareto condition, and tops-only.==== ====Several properties related to balancedness appear in some recent work. Derya (2014) considers a variable-population setting that allows for individual indifference and introduces a “cancellation property”. Cancellation bears some similarity to balancedness. First, the definition of cancellation (p. 152–153) hinges critically on allowing for individual indifference (and is for a variable-population setting): it becomes meaningless if one only considers the domain with strict preferences (as we assume). To illustrate, consider a profile (starting with strict preferences) where ==== ranked above ==== by individual #1 and ==== above ==== by #2. Cancellation says that making ==== and ==== indifferent for #1 and ==== and ==== indifferent for #2 will not change the outcome. If we assume further that ==== is (originally) ranked above ==== for #1 and ==== above ==== for #2, then repeatedly applying cancellation to make ==== all indifferent for both #1 and #2 will not change the outcome. In other words, every time cancellation is applied, one actually enlarges an indifference class by bringing together two previously neighboring ones, yet can never separate an indifference class into two different ones. This implies, in particular, cancellation ==== be applied to generate a transposition pair through the process considered by the balancedness property.==== ==== Moreover, although Derya provides a result that uses cancellation to characterize Borda rule within the class of scoring rule, the Borda rule there is a different version on the domain of weak preferences and for a variable-population setting, which is not equal to or stronger than our result in terms of either the result itself or the proof technique.==== ====Mihara (2017) introduces a property called “Positional Cancellation”, which requires that changes in the relative positions of two alternatives that cancel each other (e.g., the changes in the relative position of ==== if changing from ==== to ==== for #1 and changing from ==== to ==== for #2) do not alter the social preference between the two. Mihara works with social welfare functions not social choice correspondences and allows for individual indifference. If adapted to the context of a social choice correspondence, (the modified) Positional Cancellation is a strictly stronger property than balancedness.==== ==== Mihara uses the cancellation property, together with “reversal” (a neutrality property) and “positive responsiveness” (a monotonicity property), to characterize the Borda ranking among ==== ranking rules. Balancedness is also strictly weaker than “Invariance for Average-Position Preserving Reversals” (IAPPR) introduced by Sato (2017), which states that “if some voters reverse their preferences for two common alternatives, then the set of winners should be unchanged as long as this reversal preserves the average position of each alternative”.==== ==== Sato characterizes the Borda correspondence through IAPPR, neutrality and positive responsiveness among all social choice correspondences, while we use balancedness alone to characterize the Borda correspondence but within the class of scoring rules.==== ",Balancedness of social choice correspondences,https://www.sciencedirect.com/science/article/pii/S0165489619300630,16 September 2019,2019,Research Article,221.0
"Jindapon Paan,Van Essen Matt","University of Alabama, United States of America","Received 29 August 2018, Revised 21 August 2019, Accepted 22 August 2019, Available online 26 August 2019, Version of Record 3 September 2019.",https://doi.org/10.1016/j.mathsocsci.2019.08.004,Cited by (0),We provide a ,"This paper seeks to analyze how strategic voting behavior in government institutions can have macroeconomic implications in the form of business cycles. There is significant empirical support for the fact that politicians, via the policies they propose, attempt to manipulate the macroeconomic environment and that their effort varies with the proximity to an election period. Haynes and Stone (1990), for instance, find evidence supporting the political business cycle model at the national level when partisan preferences are taken into account. Additionally, there is evidence for this type of behavior at the local level. Bhattacharyya and Wassmer (1995) find local government spending increases (and revenue falls) in election years.==== ====The foundation for our analysis begins with the now classic paper on political business cycles due to Nordhaus (1975). Nordhaus uses a decision theoretic model where the majority party in the legislature chooses the level of unemployment (and indirectly the level inflation through the Phillips Curve). Now because voters care more about past performance (the so called Retroactive Voter Hypothesis), legislative incentives are such that they initially choose a high level of unemployment and gradually lower this level before each election.==== ==== Hence, they create a cyclical pattern in the level of unemployment or a political business cycle. While Nordhaus’s decision theoretic approach is elegant, he abstracts away from some fundamental details about the voting process. In particular, his results do not depend on policy preferences of each political party, the voting rule used to pass legislation, or other institutional details such as presidential veto power.==== ====In this paper, our goal is to accommodate these features and provide a more microeconomic foundation for political business cycles. Since the number of legislative proposals in any given election term tends to be quite large,==== ==== it is natural to model this process as a differential game between members of two political parties.==== ==== In each time period, the legislature meets to vote on policy proposals which influence the unemployment level (and indirectly the inflation level). We break up each time period into two stages: a proposal stage; and a voting stage. In the proposal stage, a representative of the majority party makes a proposal and then this proposal is voted on in the second stage by all members. If the proposal passes, then it is immediately enacted. Otherwise, there is no change in the macro economy in that period. We find an equilibrium of this game and examine the consequences that strategic behavior has on the expected level of unemployment over an election term and also its impact over multiple election terms.====In equilibrium, we find that proposals and the voting rule followed by each member of the legislature depend on the political party to which that member belongs and the current state of unemployment. Specifically, the proposal does not depend on the voting rule followed by members of the legislature. Hence, some proposals are made with the full expectation that they will not pass. A sort of “deadlock” property.==== ==== Next, voting strategies for a particular party are determined by a party-specific threshold function which varies with time. A party’s threshold determines which proposals members of that party will vote “yes” or “no” on. In particular, if the unemployment level, at a certain time period, crosses above a party’s threshold, then all members of that party will vote yes on any proposal which lowers the unemployment rate. Similarly, all members will vote no to any proposal which increases the unemployment rate. We find that each party has a different threshold where both thresholds are decreasing in time over each election term. As a result, there is an area of conflict between the two parties. We show conditions under which this feature can lead to political business cycles in a single election term or multiple election terms. Subsequently, we extend our model to accommodate a president with veto power and discuss how this addition changes our results.==== ====Our model differs from Nordhaus in several fundamental ways. First, we do not allow the majority party to choose the unemployment level, but rather assume that the unemployment level is influenced by the policies passed in the legislature. Thus, we can derive a continuous time path of the unemployment rate over multiple election terms while Nordhaus predicts a sudden jump in the unemployment level at the beginning of each term. Second, we specifically model the proposal and voting process as a game. These differences produce several new results. For example, it is now possible to have a political business cycle that appears in one election term. This is since both the majority party and the minority party can influence the change in the unemployment level. In general we find the voting rule, policy preferences of both parties, and the change in majority party over two election terms can all influence whether cycles will exist.==== ",Political business cycles in a dynamic bipartisan voting model,https://www.sciencedirect.com/science/article/pii/S0165489619300642,26 August 2019,2019,Research Article,222.0
Blavatskyy Pavlo,"Montpellier Business School, 2300, Avenue des Moulins, 34185 Montpellier Cedex 4, France","Received 22 June 2017, Revised 24 June 2018, Accepted 7 August 2019, Available online 19 August 2019, Version of Record 31 October 2019.",https://doi.org/10.1016/j.mathsocsci.2019.08.001,Cited by (0),. violations of period-wise monotonicity).,"Intertemporal choice involves outcomes that are received in different time periods. An example of an intertemporal choice problem is a tradeoff between a smaller sooner outcome and a larger later outcome. Time preferences that govern decisions over future consumption plans may be inconsistent, imprecise, affected by noise or random errors. One of the first microeconomic models of stochastic time preferences was a random utility or random preference approach. In general, this approach assumes that a decision maker has several preference relations, one of which is drawn at random when a choice decision is made (====.====., Falmagne, 1985, Loomes and Sugden, 1995). More specifically, in application to intertemporal choice, Coller and Williams (1999, p. 115, section 4.2), Warner and Pleeter (2001, p. 38, Section III.A) and Harrison et al. (2002, p. 1611, Section III.A) assumed that a decision maker maximizes the present discounted value of future outcomes with the discount factor being a random variable (rather than a fixed subjective parameter).====Another popular model of stochastic time preferences is a Fechner (1860) model of random errors, also known as strong utility. This model is employed ==== by Chabris et al. (2008, p. 248), Ida and Goto (2009, p. 1174, formula 1), Tanaka et al. (2010, p.567, formula 1) and Toubia et al. (2013, section 2.1, p. 617). In this literature, the modal choice is always the alternative yielding the highest discounted utility==== ====
 (Samuelson, 1937) but actual decisions are perturbed by random errors. Errors are additive on the utility scale—the more distant are two alternatives in terms of discounted utility the less likely is their desirability ranking to be reversed by mistake. When random errors are drawn from the normal or logistic distribution, the Fechner model becomes equivalent to an econometric homoscedastic probit or logit model of discrete choice. Blavatskyy (2017, Section 3, p. 143) provides axiomatic characterization of the Fechner model of time preferences using the axiom of cardinal independence (Blavatskyy, 2013)—a weaker version of Wakker, 1984, Wakker, 1989 tradeoff consistency.==== ==== Several papers model stochastic time preferences with the help of Luce (1959) choice model, which is also known as strict utility, ====., Andersen et al. (2008, p. 599, equation 9) and Meier and Sprenger (2015, p. 276, equation 1). Blavatskyy (2017, Section 5, p. 145) gives axiomatic characterization of Luce choice model of time preferences.====Models of probabilistic choice are often compared in the benchmark case of period-wise dominance when one intertemporal choice alternative yields a more desirable outcome than another alternative in every time period. Arguably, this leaves very little room for error. Indeed, Blavatskyy and Maafi (2018) found that nobody out of 75 subjects violated period-wise dominance (across two repetitions).====The random preference/utility approach rules out violations of period-wise dominance. Yet, nothing within the Fechner model prevents such violations.==== ====
 Luce (1959) choice model shares the same limitation as the Fechner model—its formula ==== does not rule out violations of period-wise monotonicity. Luce (1959) proposed to cure this problem through a two-stage procedure: first, a decision maker detects and deletes dominated alternatives from the choice set; and then the decision maker chooses in a probabilistic manner among the remaining alternatives. Yet, this two-stage algorithm produces a discontinuous choice function and, to the best of my knowledge, it has never been applied to time preferences. This paper presents a modification of the Fechner model (equivalent to a heteroscedastic probit/logit model) that is consistent with the period-wise dominance. Our proposed model is similar in spirit to Wilcox, 2008, Wilcox, 2011 contextual utility model of risk preferences.====A more impatient (delay averse) decision maker has a smaller probability of choosing an intertemporal choice alternative that delivers more delayed outcomes. Random preference/utility approach is monotone in time preference parameters: larger discounting always results in smaller probabilities of choosing more delayed alternatives. Unfortunately, Fechner model andLuce (1959) choice model are not monotone in time preference parameters (====. examples in Section 3). In our proposed modification of the Fechner model, a relatively less patient (====., more delay averse) decision maker is always more likely to choose an option that gives an advancement in consumption (====., Horowitz, 1992).====In some intertemporal choice problems, random preference/utility approach leads to violations of weak stochastic transitivity that are rarely observed in the data (====.====., Rieskamp et al., 2006). This problem can be illustrated with the following example from Blavatskyy (2017, p. 142). Consider three choice alternatives:====(A) to receive $74 today;====(B) to receive $100 tomorrow;====(C) to receive $37 today and $64—the day after tomorrow.====Consider an individual who maximizes the present discounted value with a random daily discount factor that is equally likely to take one of three possible values: ====, ==== and 1. This decision maker chooses B over A with probability ==== (when the discount factor happens to be ==== and 1). This decision maker also chooses C over B with probability ==== (when the discount factor happens to be ==== and 1). Yet, paradoxically, the same decision maker also chooses A over C with probability ==== (when the discount factor happens to be ==== and ====).====In the Fechner model, the modal choice cannot be intransitive so that weak stochastic transitivity is always satisfied. In Luce (1959) choice model the modal choice pattern is also always transitive. A modification of the Fechner model presented in this paper also respects weak stochastic transitivity. In fact, weak stochastic transitivity is one of the model’s axioms.====A random discount factor model is descriptively restrictive in some intertemporal choice problems. A present discounted value maximizer always respects the second-order temporal dominance (====. Bøhren and Hansen, 1980 proposition 2, p. 50). Hence, even when the discount factor is a random variable, for any possible realization of this random variable, the alternative that second-order temporally dominates all other alternatives remains the preferred choice option. For example, the prospect of receiving $10 now and $10—in two months always yields a higher present discounted value than the prospect of receiving $20 in one month for any discount factor between 0 and 1. Yet, Blavatskyy and Maafi (2018) found that 72 out of 75 subjects (96%) violate the second-order temporal dominance and their revealed choice patterns cannot be captured within the random discount factor model.====The Fechner model and Luce (1959) choice model do not necessarily restrict revealed choice patterns to be always consistent with the second-order temporal dominance. The model of stochastic time preferences presented in this paper is also not restricted to satisfy the second-order temporal dominance. The proposed model has a natural econometric interpretation in terms of heteroscedastic random errors.====Dai and Busemeyer (2014) consider a family of diffusionmodels==== ==== similar to the decision field theory of Busemeyer and Townsend, 1992, Busemeyer and Townsend, 1993 for choice under uncertainty. Some of their proposed diffusion models are equivalent to a heteroscedastic probit/logit model, similarly as our proposed model. Yet, all diffusion models of Dai and Busemeyer (2014) have a similar drawback as the Fechner model and (one-stage) Luce (1959) choice model—a decision maker may violate period-wise monotonicity.====The remainder is organized as follows. Section 2 presents our new model of stochastic time preferences, its interpretation as heteroscedastic probit/logit and its axioms. Section 3 illustrates that the new model is monotone in time preference parameters. Section 4 compares the goodness of fit of the new model to experimental data collected by Blavatskyy and Maafi (2018). Section 5 concludes.",Future plans and errors,https://www.sciencedirect.com/science/article/pii/S0165489619300617,19 August 2019,2019,Research Article,223.0
Hagiwara Makoto,"Department of Industrial Engineering and Economics, School of Engineering, Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, Tokyo, 152-8552, Japan","Received 20 July 2018, Revised 16 July 2019, Accepted 7 August 2019, Available online 14 August 2019, Version of Record 20 August 2019.",https://doi.org/10.1016/j.mathsocsci.2019.08.002,Cited by (1),", then ==== and ==== together are sufficient for double implementability. If there are at least two partially honest agents, then ==== is sufficient for double implementability. In addition, we show that if there is at least one partially honest agent and ==== is satisfied, then “LY-condition” is necessary and sufficient for double implementability. From these results, we obtain several positive corollaries.","We consider the implementation problem with at least three agents. We study double implementability of social choice correspondences in Nash equilibria and undominated Nash equilibria, which we hereafter refer to as “double implementability.” There are two reasons to investigate double implementability. First, in laboratory experiments, subjects do not always adopt undominated strategies (see, for example, Cason et al., 2006, Katok et al., 2002). Second, in several pivotal-mechanism experiments in which truth-telling is a dominant strategy for each agent, Nash equilibria have been frequently observed (Cason et al., 2006). A possible explanation is that, even though some subjects could not identify undominated strategies, they were able to determine how to improve upon a strategy.====For Nash implementability, “Maskin-invariance” and “no-veto-power” together are sufficient (Maskin, 1999).==== ==== There are several social choice correspondences that satisfy ==== but violate ====.==== ==== In this case, to examine Nash implementability, we verify whether a social choice correspondence satisfies a necessary and sufficient condition, such as the one proposed by Sjöström (1991). However, it is not easy to check the condition. Subsequently, to verify Nash implementability more easily, other sufficient conditions are proposed (Doghmi and Ziad, 2015): “DZ-invariance,” “weak no-veto-power,” and “unanimity.”==== ====When there are at least three agents, if a social choice correspondence is Nash implementable, then it is doubly implementable (Yamato, 1999). Then, ====, ====, and ==== together are sufficient for double implementability (Doghmi and Ziad, 2015, Yamato, 1999). This result is provided indirectly by means of two game forms. We prove it directly by constructing another game form (Proposition 4). This game form is also applied in the proof of our first theorem.====We consider “partially honest” agents as defined by Dutta and Sen (2012).==== ==== A partially honest agent prefers reporting the true preference profile whenever a lie does not allow him to obtain an outcome that he prefers; otherwise, he prefers announcing a message inducing an outcome that he prefers.====For Nash implementability, if there are at least three agents out of which at least one agent is partially honest, then ==== is sufficient (Dutta and Sen, 2012). We show that if there are at least three agents out of which at least one agent is partially honest, then ==== and ==== together are sufficient for double implementability (Theorem 1). Each of ==== and ==== is weaker than ==== (Remark 1).====For Nash implementability, if there are at least three agents out of which at least two agents are partially honest, then ==== is sufficient (Kimya, 2015). We show that if there are at least three agents out of which at least two agents are partially honest, then ==== is sufficient for double implementability (Theorem 2). Most of social choice correspondences satisfy ====. Then, those are doubly implementable.====Fig. 1 illustrates previous results, and Fig. 2 summarizes our results concerning sufficient conditions.==== ==== When at least one agent is partially honest, if some social choice correspondence does not satisfy ====, we cannot verify using Theorem 1 whether the social choice correspondence is doubly implementable or not.==== ==== Then, it is important to provide a necessary and sufficient condition for double implementability when at least one agent is partially honest.====Since most of social choice correspondences satisfy ====, we focus on the social choice correspondences that satisfy this condition. For Nash implementability, if there are at least three agents out of which at least one agent is partially honest and ==== is satisfied, then ==== is necessary and sufficient (Lombardi and Yoshihara, 2019). We show that if there are at least three agents out of which at least one agent is partially honest and ==== is satisfied, then ==== is necessary and sufficient for double implementability (Theorem 3). Based on the result of Lombardi and Yoshihara (2019) and Theorem 3, double implementability is equivalent to Nash implementability (Corollary 1).====From our results, we obtain several positive corollaries in the allocation problem of an infinitely divisible resource with single-peaked preferences, with single-plateaued preferences, and the many-to-one matching problem. See Section 5. For several positive corollaries in other applications such as a coalitional problem, see an earlier version of this paper (Hagiwara, 2017) and Lombardi and Yoshihara (2019).====This paper is organized as follows: Section 2 presents the model. Section 3 reports the sufficient conditions for double implementability under the assumptions concerning the existence of partially honest agents when there are at least three agents. Section 4 provides a characterization for double implementability when there are at least three agents out of which at least one partially honest agent exists and ==== is satisfied. Section 5 presents the concluding remarks.",Double implementation without no-veto-power,https://www.sciencedirect.com/science/article/pii/S0165489619300629,14 August 2019,2019,Research Article,224.0
"Karos Dominik,Rachmilevitch Shiran","School of Business and Economics, Maastricht University, The Netherlands,Department of Economics, University of Haifa, Israel","Received 29 November 2018, Revised 25 July 2019, Accepted 27 July 2019, Available online 1 August 2019, Version of Record 6 August 2019.",https://doi.org/10.1016/j.mathsocsci.2019.07.006,Cited by (0),"A payoff allocation in a bargaining problem is ==== if each player obtains at least one ====th of her ideal payoff. The egalitarian solution of a bargaining problem may select a payoff configuration which is not midpoint dominant. We propose and characterize the solution which selects for each bargaining problem the feasible allocation that is closest to the egalitarian allocation, subject to being midpoint dominant. Our main axiom, ","In a bargaining problem ==== players have to agree on a feasible utility allocation: if they cannot reach an agreement, they will receive their disagreement payoffs. A bargaining solution is a rule that selects a unique feasible payoff vector for every bargaining problem—as an arbitrator would do (Luce and Raiffa, 1957).  Gerber and Upmann (2006) illustrate how the choice of different bargaining rules can lead to very different policy implications; so, any bargaining rule must be selected with care.====One of the simplest options a (maybe careless) arbitrator faces is to determine a “dictator” by rolling an ====-sided fair die: all non-selected players obtain their respective disagreement payoff, and the selected player will receive her maximum possible payoff given this constraint. The requirement that a bargaining solution (ex ante) Pareto-dominate this simple procedure has been formalized by Sobel (1981) as the ==== axiom. It is satisfied by the Nash solution (Nash, 1950) and the Kalai–Smorodinsky solution (Kalai and Smorodinsky, 1975) (see also Moulin, 1983, Rachmilevitch, 2014 respectively); but it is violated by the ==== solutions of Kalai (1977). Even worse, we show that the only solution that is monotonic and midpoint dominant is the procedure above—which, in general, is not Pareto efficient.====We propose a variant of the egalitarian — that is, monotonic and symmetric — solution that is designed to overcome this problem: the ====, which selects for every problem, among the points that are (coordinate wise) greater than or equal to the midpoint, the one which is closest to the egalitarian point. We characterize this solution by four axioms: weak Pareto efficiency, symmetry, midpoint domination, and midpoint monotonicity. The first three are well-known in the bargaining literature; the fourth is novel, and the requirement it imposes is as follows: when the set of feasible utility vectors expands, the only justification to lower any player’s payoff is that this is necessary for avoiding a violation of midpoint domination. In other words, all players should benefit from the addition of alternatives, unless this would imply a violation of midpoint domination.====Technically, we proceed in two steps. First, we characterize our solution on the class of problems in which the weak and strong Pareto frontiers coincide, which is denoted by ====. We then prove an extension theorem for bargaining solutions: a bargaining solution is ==== if it is order-preserving with respect to some (arbitrary) order on the set of bargaining problems. Our theorem provides a sufficient condition for the uniqueness of an order preserving extension of a bargaining solution from ==== to the set of all bargaining problems.====The rest of the paper is organized as follows. Section 2 introduces some notation and standard axioms. Section 3 starts by characterizing the unique solution that satisfies monotonicity and midpoint domination, which is not Pareto efficient. This negative result motivates our midpoint monotonicity axiom that we propose at the end of that section. We present and analyze our new solution in Section 4. The extension theorem and its application are in Section 5. Section 6 contains a brief discussion.",The midpoint-constrained egalitarian bargaining solution,https://www.sciencedirect.com/science/article/pii/S0165489619300605,1 August 2019,2019,Research Article,225.0
Chiu W. Henry,"Economics, School of Social Sciences, The University of Manchester, Manchester, M13 9PL, UK","Received 30 November 2018, Revised 9 July 2019, Accepted 20 July 2019, Available online 31 July 2019, Version of Record 16 August 2019.",https://doi.org/10.1016/j.mathsocsci.2019.07.004,Cited by (3),"This paper considers the effects of changes in risk on optimal ==== in an analytical framework that is capable of replicating and extending important results obtained in the EU framework and is yet far more descriptive and no less tractable at the same time. We show that a version of the general framework can be used to establish in the class of problems considered by Rothschild and Stiglitz (1971) and Machina (1989) the equivalence between behavioral assumptions and the characteristics of the utility function, which are in turn necessary and sufficient for comparative statics results.","A large part of the economics of risk and uncertainty is justly concerned with how the introduction of risk, or more generally changes in risk, affect individuals’ economic behavior. The lasting popularity of the Expected Utility (EU) model is in no small part attributable to the sharp comparative statics results that can be generated in the framework under conditions on the von Neumann–Morgenstern (VNM) utility function that in many cases have plausible behavioral interpretations. For a large class of problems, Rothschild and Stiglitz (1971) provide a sufficient condition on the VNM utility function for ascertaining the effect of an increase in risk (in the sense defined in Rothschild and Stiglitz, 1970) of some underlying economic variable on the optimal choice of a control variable. The condition in its general form however has no obvious behavioral interpretation.==== ==== In the more specific setting of a two-asset portfolio choice problem, Cheng et al. (1987) and Hadar and Seo (1990) respectively identify the conditions on the VNM utility function under which the effects on the optimal portfolio choice can be ascertained when the return of one of the assets deteriorates in the senses of first-degree and second-degree stochastic dominance. Chiu et al. (2012) generalize these results by establishing the effects of a deterioration in the return of one of the assets in the sense of an ====th-degree risk increase (Ekern, 1980)==== ==== and further offer a behavioral interpretation for the condition on the VNM utility function sufficient for the result.====Violations of the independence axiom that underpins the EU model such as the Allais paradox and the common ratio effect are, however, well-known by the 1960s. In response, a number of “non-expected utility theories” have been proposed, the most successful among which is what becomes known as Rank-Dependent Expected Utility (RDEU) theory first put forward by Quiggin (1982).==== ==== To account for other patterns of experimental evidence they uncover as well as those similar to the Allais paradox, Kahneman and Tversky (1979) propose Prospect Theory that allows for preferences to be reference-dependent. What is termed Cumulative Prospect Theory was later proposed and axiomatized to allow for reference-dependent preferences while retaining both monotonicity and transitivity, which Prospect Theory was shown not to always obey simultaneously.==== ==== While these models allow for non-linearity in probabilities, they all retain a form of partial event-separability to permit an explicit specification of the relationship between the utility of a distribution and the utilities of its possible outcomes.==== ====
 Machina, 1982a, Machina, 1982b, Machina and Neilson (1987), Machina (1989), and Wang (1993), on the other hand, show that, without the independence axiom or a weaker replacement for it, most “EU analysis” can be carried out with the “local utility function” (which is the probability derivative of the preference functional) taking the place of the von Neumann–Morgenstern (VNM) utility function as long as a differentiability condition on the preference-representing functional is assumed.====These impressive theoretical advances notwithstanding, while its dominance is somewhat diminished, the EU model continues to be the most widely used analytical framework for not just empirical but also theoretical research in the economics of uncertainty. What explains its lasting popularity in the presence of patently more descriptive alternatives, as many researchers observe, is the fact that none of the alternative models is as simple and easy to use as the EU model. The greater complexity of these alternative models is furthermore seen as inevitable as they can all be understood as attempts at a generalized version of “EU analysis” – analysis of choice under risk based on the characterizations of preference regularities in terms of an analogue of the VNM utility function – where a weaker replacement for the independence axiom necessarily implies a more complex preference functional.==== ==== What underlies this seemingly inevitable modeling trade-off is the implicit and nearly consensual belief that a model without a degree of cardinalization necessary for EU analysis==== ==== is incapable of replicating a version of the elegant characterizations of preferences and the multitudes of intuitively appealing results on optimal decisions derived in the EU framework, or worse, it would be a model void of empirical contents. While this is understandable in view of the fact that Bernoulli’s original EU hypothesis was a response to the observed inadequacy of the even earlier hypothesis that individuals choose among lotteries on the basis of their expected values, it should seem peculiar in the wider context of consumer choice theory where it was widely recognized by the 1940s that the assumption of cardinal utility is not just unnecessary but undesirable. What is more, even the arguably most descriptive to date of the non-EU models, namely cumulative prospect theory, with its much more unwieldy preference functional, is not without experimental evidence challenging its theoretical underpinning: All rank-dependent models, including cumulative prospect theory, imply “ordinal independence”, systematic violations of which have been observed in experiments (Wu, 1994). Since ordinal independence is a form of partial “event separability” as explained in Machina (2009), available experimental evidence is seen to be inconsistent not just with preference functionals linear in probabilities but also with those assuming weaker forms of event separability, the very property that makes possible the derivation of an analogue of the VNM utility function in non-EU models.====This paper shows that a meaningful ordinal theory of choice under risk capable of replicating and extending important results derived in the EU framework is not just feasible but due to its more general and yet simpler nature, it can be more descriptive and no less tractable at the same time. Furthermore, being analytically more akin to standard consumer theory, the analysis of decision under risk in this framework employs little more than elementary mathematical techniques and, even in fairly general cases is amenable to simple graphical illustration. The key to this more general and yet simpler analytical framework is to dispense with EU analysis altogether and use instead a parameter-based representation of changes in probability distribution resulting from decisions under risk. More specifically, while we assume, as do existing models, that preferences over probability distributions are representable by a utility function, unlike existing models, we make ==== further assumption to ==== the preference-representing utility function in any way. In particular, there is no assumption that implies any form of event-separability and thus there is no explicit specification of how the utility of a distribution relates to the utility of each of the distribution’s possible outcomes obtained with certainty. Neither is there any assumption of differentiability on the preference-representing utility function (which requires a cardinalization) to derive the “local utility”. As a result, there is no analogue of the VNM utility function. For characterizing behavioral assumptions, we use instead an ordinal utility function of decision parameters enabled by the parameter-based representation of distributional changes. Focusing in this paper on the effects of changes in risk on optimal decisions under risk, we show that a version of the general framework can be used to establish in the class of problems considered by Rothschild and Stiglitz (1971) and Machina (1989) the equivalence between behavioral assumptions and the characteristics of the preference-representing utility function, which are in turn necessary and sufficient for comparative statics results. That is, we show that not only can existing comparative statics results be extended to an analytical framework that require no assumption of a differentiability condition or a version of the independence axiom but their necessary and sufficient conditions can also be characterized in terms, not of the VNM or local utility functions whose behavioral interpretations may not be immediately clear, but of behavioral assumptions on the preferences themselves.====As the analytical framework assumes no event-separability of any kind, it is consistent with all observed patterns of behavior, including those reported in Wu (1994) that violate “ordinal independence”. As shown in Chiu (2018a), in this framework a behavioral hypothesis can instead be formulated that accounts for the full range of putatively paradoxical behavioral patterns including simultaneous gambling and insurance, the common ratio effect, Kahneman and Tversky’s (1979) reflection effect, and preference reversal. Chiu (2018b) further shows that the analytical framework can be extended to a setting with subjective uncertainty where preference conditions on attitudes towards ambiguity can be identified that account for not just the Ellsberg paradox but also behavioral patterns suggested by Machina (2009, 2014) to be inconsistent with the predictions of major existing models of ambiguity aversion. In the context of our theoretical framework assuming only utility-representable preferences, far from being void of empirical contents, the behavioral hypotheses in terms of preferences over distributions, not conditions on an analogue of the VNM utility function, are more precise and are ready to be more precisely tested in experiments.====The rest of the paper is organized as follows. Section 2 sets out notions of a risk deterioration and key behavioral assumptions. Section 3 develops the analytical framework for decision-making under risk assuming only utility-representable preferences and discusses the comparative statics results that can be obtained. Section 4 concludes.",Comparative statics in an ordinal theory of choice under risk,https://www.sciencedirect.com/science/article/pii/S0165489619300587,31 July 2019,2019,Research Article,226.0
"Gan Jiarui,Suksompong Warut,Voudouris Alexandros A.","Department of Computer Science, University of Oxford, United Kingdom of Great Britain and Northern Ireland","Received 1 May 2019, Revised 13 July 2019, Accepted 22 July 2019, Available online 30 July 2019, Version of Record 5 August 2019.",https://doi.org/10.1016/j.mathsocsci.2019.07.005,Cited by (23),"We consider the house allocation problem, where ==== houses are to be assigned to ","In the ====, also known as the ====, a set of ==== houses are to be assigned to a set of ==== agents with preferences over the houses, under the constraint that each agent is assigned exactly one house (Hylland and Zeckhauser, 1979, Zhou, 1990, Abdulkadiroglu and Sönmez, 2003). Some economic efficiency condition is often desired, for example that the assignment is ====. This means that no other assignment makes some agent better off and no agent worse off in comparison to the current assignment (Abraham et al., 2004, Manlove, 2013).====In this note, we investigate the issue of fairness in house allocation using the well-established fairness notion of ==== (Foley, 1967, Varian, 1974). An allocation is said to be ==== if every agent likes her house at least as much as any other assigned house. Clearly, an envy-free allocation does not always exist, for example when all agents have the same strict ranking over the houses. If the number of agents is equal to the number of houses, then all houses must be assigned. In this case, it is easy to see that determining whether an envy-free assignment exists, and computing one if so, can be done in polynomial time. Indeed, we can simply construct a bipartite graph with the agents on one side and the houses on the other side, and add an edge between an agent and a house whenever the agent likes the house at least as much as any other house. An envy-free assignment exists if and only if the graph admits a perfect matching; it is well-known that the latter condition can be checked in polynomial time.====The purpose of our note is to study envy-freeness in the general house allocation problem where the number of houses can exceed the number of agents. Formally, there are ==== houses ==== and ==== agents ====, where ====. Each agent has a ranking over the houses, where ties are permitted. Allowing the number of agents and the number of houses to be different makes the problem more complex, and we can no longer determine the existence of envy-free assignments solely by matching agents to their favorite houses. For example, if there are three houses and two agents with the rankings ==== and ==== over the houses, then even though both agents compete for the same top house, there is an envy-free assignment that assigns house 2 to agent 1 and house 3 to agent 2. Nevertheless, we present a polynomial-time algorithm that determines whether an envy-free assignment exists, and computes one if it does. We then show that if the number of houses exceeds the number of agents by a logarithmic factor, an envy-free assignment exists with high probability.====To the best of our knowledge, the only work before ours to have considered envy-freeness in house allocation is that of Beynier et al. (2018). Their work focuses exclusively on the ==== case but contains the extra feature that agents are placed on a network that describes the envy relation, and they showed algorithms and hardness results for different networks. Recently, Segal-Halevi (2019) studied a concept called envy-free matchings on bipartite graphs, and provided conditions under which a non-empty envy-free matching exists along with algorithms to compute such matchings. In contrast to this note, his study is restricted to unweighted bipartite graphs, which correspond to each agent either approving or disapproving each house, and does not require every agent to be assigned to a house.",Envy-freeness in house allocation problems,https://www.sciencedirect.com/science/article/pii/S0165489619300599,30 July 2019,2019,Research Article,227.0
Nakamura Yuta,"Department of Economics, Keio University, Tokyo 108-8345, Japan","Received 18 January 2019, Revised 20 May 2019, Accepted 21 July 2019, Available online 26 July 2019, Version of Record 30 July 2019.",https://doi.org/10.1016/j.mathsocsci.2019.07.002,Cited by (1),"Moulin (1986) characterizes the pivotal mechanisms under the assumption of the full domain of quasi-linear preferences. In this paper, we provide properties of restricted domains that are necessary and sufficient for Moulin’s (1986) strategy-proof characterizations to hold. We also provide simple ==== that imply these properties.","In his seminal work, Moulin (1986) provides two strategy-proof characterizations of Clarke’s (1971) pivotal mechanisms. First, he shows that efficiency, strategy-proofness, and no free ride characterize the pivotal mechanisms. Second, he shows that the same characterization holds if no free ride is replaced by welfare lower boundedness. However, since Moulin (1986) only considers the full domain of quasi-linear preferences, the uniqueness results of the pivotal mechanisms do not necessarily hold in more structured, restricted domains. In other words, it is still unknown under which economic environment the use of the pivotal mechanisms is justified by Moulin’s (1986) characterizations. To address this issue, we provide properties of restricted domains that are necessary and sufficient for Moulin’s (1986) characterizations to hold. We also provide simple economic conditions that imply these properties. Our results thus identify the conditions under which Moulin’s (1986) characterizations apply.",Strategy-proof characterizations of the pivotal mechanisms on restricted domains,https://www.sciencedirect.com/science/article/pii/S0165489619300563,26 July 2019,2019,Research Article,228.0
Kolpin Van,"University of Oregon, Department of Economics, 1285 University of Oregon, Eugene, OR, 97403-1285, United States","Received 26 January 2019, Revised 30 June 2019, Accepted 20 July 2019, Available online 26 July 2019, Version of Record 1 August 2019.",https://doi.org/10.1016/j.mathsocsci.2019.07.003,Cited by (0),A decades old debate regarding the state and local ,"One prominent feature of the 2017 Tax Cuts and Jobs Act is its modification of state and local tax (SALT) deductibility. This change has spurred considerable debate in state and federal congresses as well as in the popular press. Since at least the Reagan era, some authors (e.g., Bartlett, 1985, Kaeding, 2017, Greszler et al., 2017) have argued that state and local tax deductions inhibit economic efficiency and should be eliminated from US tax code, a position that we henceforth refer to as SALT-free doctrine. Others have argued that SALT-free doctrine constitutes a thinly veiled and politically motivated attack on “blue states” (e.g., Meyerson, 2017, Parlapiano and Lai, 2017). The purpose of this paper is to use a context free of partisan political considerations to examine the apolitical equilibrium implications of adopting SALT-free doctrine. We do so by examining a tax policy game between state and federal governments that are solely motivated by coherent welfare objectives. That is, each state’s assessment of its own citizens’ welfare is shared by the federal government’s assessment of the welfare of this same group of citizens. We find that in this definitively apolitical context, SALT deductions serve as a mechanism for internalizing nonresident externalities. SALT-free doctrine, on the other hand, disables this mechanism and strictly ==== equilibrium welfare levels of both state and federal governments. Moreover, we find that in some environments, the adoption of SALT-free doctrine can also induce strictly ==== aggregate tax burdens (the sum of equilibrium state and federal taxes) than would emerge when SALT deductions are permitted.====Well-intentioned state and local governments will of course tailor their tax/expenditures to the benefit of their residents. This may appear to suggest that the impact of state tax policy on nonresidents can be safely ignored when seeking to identify equilibrium tax policy. However, despite self-interested state motivations, in-state expenditures routinely convey substantial benefits to nonresidents. Indeed, better schools, better roads, better bridges, and, more generally, better infrastructure bestow benefits upon tourists, potential future residents (42% of the US citizenry live in two or more states in their lifetime and 77% of college graduates change communities at least once – Cohn and Morin, 2008), children who may become future residents, consumers of goods shipped via out-of-state roads, etc. Moreover, citizens throughout the nation benefit from the presence of a skilled and educated workforce, irrespective of where the skills and education were actually acquired. An optimally behaving and well-intentioned federal government will design its federal tax policy while accounting for the comprehensive impact that these design decisions will have on state-level decisions and, ultimately, on the welfare of all of its citizenry. Consequently, an appropriate model of strategic tax policy formation cannot ignore the external impact of state level policies.====The traditional optimal taxation theory literature features a single social planner (e.g., a central government) who seeks to design tax policy in a manner that maximizes social welfare. This optimization problem can lead to a tension between equity and efficiency as social welfare may rise with increased equity while the taxation necessary to induce this equity enhancement may disincentivize the activities that are required to produce the income that can be subjected to taxation in the first place. Numerous complexities also arise in practice such as (to name just a few) the implementation and evasion costs affiliated with various tax instruments, information asymmetries between citizens and the government that serves them, and the presence of “fairness” objectives that may arise independently from welfare maximization. Extensive surveys of the optimal taxation literature can be found in such sources as Boadway (2012), Mankiw et al. (2009), Piketty and Saez (2013) and Slemrod (1990).====The SALT deduction debate on which this paper is focused can be viewed as falling under the broad umbrella of the optimal taxation literature in the sense that federal and state governments also seek to design their taxation policies so as to maximize the welfare of their respective constituencies. A key departure from the traditional optimal tax context, however, is the presence of multiple social planners with policy setting authority, each with different welfare objectives. This distinction lies at the very heart of the SALT deduction debate. Indeed, this debate has been particularly heated when it comes to the incentives that SALT deduction has provided to state and local policy makers and there has been considerable hand-wringing over the extent to which the deduction limitations dictated by the Tax Cuts and Jobs Act will ultimately hamstring these policy makers. (See, for instance, Davison, 2019.) Our paper presents a stylized model of taxation and public good provision that suppresses peripheral complexities so as to highlight the strategic tensions between federal and state governments that are central to the SALT deduction debate.====The process by which tax law is actually formulated at the federal and state levels in practice is of course steeped in political pressures. Nonetheless, public servants are presumably driven, at least in part, by a desire to “do good” for their constituents. As such, our stylized apolitical model of tax policy formation can be viewed as something of an “idealized first-order approximation” of state and federal government behavior. Our findings indicate that blanket claims regarding the undesirability of SALT deductions in federal tax code (e.g., Millsap, 2017) fail to be valid in the context of apolitical welfare maximizing state and federal governments. Even so, this conclusion should not be misconstrued as a blanket endorsement of SALT deduction. This is especially true when viewed through the lens of “real world” tax policy formation in which government officials may be significantly motivated by political considerations rather than solely by benevolent concern for their constituents.",Apolitical SALT-free Tax Equilibria,https://www.sciencedirect.com/science/article/pii/S0165489619300575,26 July 2019,2019,Research Article,229.0
"Bednay Dezső,Moskalenko Anna,Tasnádi Attila","Department of Mathematics, Faculty of Economics, Corvinus University of Budapest, Fővám tér 8, 1093 Budapest, Hungary,Department of Economics, Universitat Rovira i Virgili and CREIP, Avinguda de la Universitat 1, Reus, Spain,Department of Operations Research, Faculty of Science, Eötvös Loránd University, Pázmány Péter sétány 1/C, 1117 Budapest, Hungary","Received 13 December 2018, Revised 14 July 2019, Accepted 15 July 2019, Available online 18 July 2019, Version of Record 23 July 2019.",https://doi.org/10.1016/j.mathsocsci.2019.07.001,Cited by (0),The Gibbard–Satterthwaite theorem roughly states that we have to accept dictatorship or manipulability in case of at least three alternatives. A large strand of the literature estimates the degree of manipulability of ,"The classic result of Gibbard (1973) and Satterthwaite (1975) states that for at least three alternatives every universal and resolute social choice function is either manipulable or dictatorial. There is a large literature on how to escape from the negative implications of the Gibbard–Satterthwaite theorem by restricting the set of possible preference profiles, most of them related to single-picked preferences and their generalizations (e.g. Black, 1958; Moulin, 1980; Barberá et al., 1993; and Nehring and Puppe, 2007a and 2007b, just to name a few). Since the normative approach does not give us the ultimate answer for choosing between social choice functions, another strand of the literature tries to estimate to which extent different voting rules are susceptible to manipulation and to compare the common voting rules according to their ‘degree of manipulability’.====There is no universally accepted way to measure the degree of manipulability, but one of the most common approaches is to consider the ratio of preference profiles where manipulation is possible to the total number of profiles, which is called the Nitzan–Kelly’s index (NKI, hereinafter) of manipulability, since it was first introduced in Nitzan (1985) and Kelly (1988).==== ==== A voting rule is thought to be less manipulable if it is manipulable at fewer preference profiles, or equivalently, if it has a smaller NKI (clearly, the dictatorial voting rule is the least manipulable one). There are a number of studies investigating voting rules under this approach. Kelly (1988) found the minimal number of manipulable profiles for social choice rules which are unanimous and non-dictatorial. This research direction is continued in Fristrup and Keiding (1998), Favardin et al. (2002) and a series of studies in Maus et al. (2007a, 2007b, 2007c, 2007d).====Kelly (1993) compares the manipulability of the Borda rule with the manipulability of different classes of social choice procedures by developing computational results. Aleskerov and Kurbanov (1999) continue this line of research. The authors study the degree of manipulability of several social choice rules via computational experiments, considering the NKI and in addition introducing some new indices, which are further elaborated in Aleskerov et al. (2009, 2011, 2012). Peters et al. (2012) study both theoretically and using simulations the manipulability of the approval voting rule and a family of ====-approval rules.====In this paper we follow a different route and formulate indices in relation to the dictatorial voting rule, thus picking dictatorship as a reference point instead of manipulability, when looking at the two incompatible properties appearing in the Gibbard–Satterthwaite theorem. In Bednay et al. (2017) we have derived the plurality rule as the most balanced one in the sense that it minimizes the sum of the distances to all dictatorial rules, and we have obtained the reverse-plurality rule by maximizing the distance to the closest dictatorial rule. Based on this approach we introduce the non-dictatorship index (NDI). When employing manipulability indices on the set of commonly used social choice functions, the literature strives for the rules with the lowest manipulability index by assuming that a relatively less manipulable rule is deemed to be more desirable. In an analogous way, we are looking for the social choice function with the highest NDI, i.e., the social choice function with low degree of dictatorship.====Our research is also motivated by the fact that regarding manipulability and non-dictatorship there is a kind of weak agreement on the ‘most extreme’ social choice functions. In particular, the reverse-plurality rule, which is the most extreme social choice function in the sense that it lies the furthest away from the closest dictatorial rule, is also group manipulable at every preference profile. Moreover, the reverse-dictatorial social choice function, which always chooses the worst alternative of a fixed voter, is individually manipulable at each profile. Trivially, any dictatorial voting rule is non-manipulable at any preference profile.====The aim of this paper is to investigate the relationship between NDIs and NKIs and to put them into a common framework. By employing computer simulations, we estimate the NDIs of some well-known social choice functions (some scoring rules and Condorcet consistent rules). We calculate NDIs for 3, 4 and 5 alternatives and up to 100 voters by generating 1000 random preference profiles, where each profile is selected with the same probability, i.e. we assume an impartial culture. We find that among the prominent social choice functions the plurality rule has the smallest NDI, the Borda count, the Black rule and the Copeland method follow with approximately identical NDIs, while ====-approval voting (for ==== or ====) has the highest NDI among the most common social choice functions.==== ==== In measuring manipulability we restrict ourselves to NKI, which measures the strategy-proofness by counting the number of profiles on which a social choice function is manipulable. While for determining the values for NDI we have written our own program, for determining NKI we have downloaded the results available at Aleskerov et al. (2013) where we employ the alphabetical tie-breaking rule.====We find that, when unifying the NDIs and NKIs for our social choice functions under study, both indices move in the opposite directions, which is a plausible sign for our non-dictatorship index. Next we look at both NDI and NKI of the social choice functions. From our findings we would like to highlight that basically the plurality rule performs the worst in terms of both NDI and NKI with the exception of the case of 4 alternatives for which the ====-approval voting rule has an even higher NKI. However, there is no such analogously best performing rule based on the two indices.====The structure of the paper is as follows. Section 2 introduces the basic notations and the indices to measure the degree of dictatorship of social choice functions. Section 3 presents the social choice rules under study. Section 4 explains the computational scheme, presents and discusses the results. Finally, Section 5 concludes.",Dictatorship versus manipulability,https://www.sciencedirect.com/science/article/pii/S0165489619300551,18 July 2019,2019,Research Article,230.0
"Mbodji O.S.,Nguyen-Huu A.,Pirvu T.A.","McMaster University, 1280 Main Street West, Hamilton, ON L8S 4L8, Canada,CEE-M, University of Montpellier, CNRS, INRA, Montpellier SupAgro, Montpellier, France","Received 11 January 2018, Revised 16 March 2019, Accepted 12 June 2019, Available online 9 July 2019, Version of Record 31 July 2019.",https://doi.org/10.1016/j.mathsocsci.2019.06.003,Cited by (0),We study an intra-household decision process in the Merton financial portfolio problem. This writes as an optimal consumption–investment problem in finite horizon for the case of two separate consumption streams and a shared final ,"An important dimension of household savings decisions is the possibility of individual consumption streams out of the common wealth. In the standard household economics literature, e.g., (Browning and Lusardi, 1996, Browning, 2000), the decision is most impacted by arbitrage with individual incomes. It has already been argued in Ortigueira and Siassi (2013) that the household – as a risk-sharing institution – considers personal savings as contribution to a group insurance, and that ignoring intra-household risk sharing introduces a bias in the response of savings to income shocks. This generalizes to any group of economic agents when considered as a pool of investors. In the present paper, we take a drastically different stand from the literature by focusing exclusively on the financial management of savings. The goal of this paper is more precisely to study the impact of heterogeneous preferences within the household on portfolio initial allocation, in a complex and dynamic financial world.====General intra-household decision problems constitute a well-known challenge. It is acknowledged that bargaining concepts (Nash style or Kalay–Smorodinsky, see Manser and Brown (1980) are not the only “collective” decision process alternative to modeling the household as a single decision unit. In his seminal contribution, Chiappori (Chiappori, 1988) minimally defines collective rationality by simply requiring the household to be on the Pareto-efficient frontier. This notably leads the household to derive, in his respective labor-consumption problem, an income sharing rule based on the common initial endowment: the household gathers its wealth and then divides it, for each member and separate activities. He then shows (Chiappori, 1992) in that setting that household decisions are efficient if and only if some sharing rule exists. Obtaining such a rule is our objective, in a very specific context: the Merton portfolio problem.====Specifically, the household (e.g. spouses) involves two separated utility functions ==== and ==== for consumption streams of money ==== and ==== respectively, and two distinct (non necessarily constant nor equal) discount rates ==== and ==== to measure their individual impatience. Additionally, since we consider the problem in a finite horizon ====, the household shares a common utility function ==== to evaluate together the resulting terminal wealth ====, discounted with rate ====. This wealth is obtained as the result of a financial portfolio strategy on the time interval ====, starting from an initial endowment ==== and invested in risky or riskless assets. We pose the problem of maximizing ==== by choosing the optimal consumption rates ==== for ==== and the portfolio allocation ====. Having now sketched the mathematical problem, we may comment on it from different standpoints. We show in the present paper that the above problem can be divided in three subproblems: one for each utility stream. Each of them is resolved using standard techniques (Karatzas and Shreve, 1987), so that the division at ==== of initial wealth toward each household member represents our main contribution – the ==== – and the household’s determinant decision. The parallel with Pareto-optimality is strong: if (1.1) is the objective function of a representative agent, then maximizing this expected utility is equivalent to solving individual agents’ problems and finding the Pareto weights (the vector of initial wealth allocations). As we obtain an optimal sharing of the initial wealth for the three different utility valuations in this setting (see Theorem 4.4 hereafter), we are inclined to conjecture ==== that some collective rationality arises from it (see in particular (Chiappori, 1988 p. 74)).====What influences this sharing rule? In our setting, the financial portfolio is self-financed: it starts with a given initial wealth but does not undergo any additional injection of savings. We thus isolate the fundamental factors influencing the sharing rule, which are risk aversion (embedded in utility functions) and impatience (in individual discount rates). Our contribution mainly attempts to provide the sharing rule regarding those typical financial dimensions, that is, when members of the household have different levels of risk aversion or impatience. In this respect, our article really stands as a contribution to the portfolio management literature.====But in the later research field, problem (1.1) is relatively new. In the classical portfolio management problem, the single agent model is the default representation. It has already been argued (Six, 2010) that consumption and terminal wealth should be evaluated distinctly, as the nature of the reward is different (empirical works (Meyer and Meyer, 2005) highlight a greater risk aversion toward consumption than toward wealth). In Six (2010), Six makes that distinction, yet for a single agent. He shows, as we do, the separability of the problem, and that the ==== (CSP), defined as the allocation of initial wealth dedicated to consumption of the household on ====, drastically depends – but monotonously – on the initial wealth. By introducing two separate consumption streams, we go further than (Six, 2010), since there is an intricate dependence of each consumption stream with the common resulting wealth. We show that our model, unlike the one of Six (2010), can exhibit a hump shaped consumption satisfaction proportion.====We end the paper with a numerical application with closed form solutions based on the consumption–savings problem of Wachter (Wachter, 2002). Especially, we show that the previously mentioned ==== increases with the initial endowment, but the increase is slower for the more risk averse agent. The CSP is an important dimension as it defines how much money is devoted to consumption, and how much to wealth possession. When the initial endowment is large enough, the less risk averse agent allocates relatively more money to future consumption. This mainly shapes the initial sharing rule, as expected. This allows us to study the CSP dependence with respect to the market price of risk and risk aversion. The numerical results revealed that, unlike the effect of market price of risk change which is marginal, a change in risk aversion can significantly impact the CSP. Those intuitions, developed in Section 5, are for the benefit of portfolio managers. As we mentioned at the very beginning, we acknowledge that the study of a household financial portfolio problem is at odds with existing literature focused on labor-consumption-leisure arbitrage, and that little household data would be available to help reveal the pertinence of the implemented setting. Nevertheless, the application seems much more realistic when one thinks of the situation of a mutual fund portfolio manager working for a pool of heterogeneous clients, or in the framework of a hedge fund optimal dividend distribution from the shareholders perspective. If household decision theory is our starting point to introduce the problem, the applications are more numerous in the financial industry. We comment shortly in the paper on the possibility to extend the setting to ==== consumption streams. The reader will easily be able to transpose the present results to a much larger pool of agents.====The remainder of the article is organized as follows. Section 2 introduces the financial market, assumed to be complete, portfolios and their properties. Section 3 presents the household problem and we comment on how to extend the setting to more than two agents. Section 4 develops the central theorem, i.e., the optimal initial sharing rule. Accordingly, the problem can be separated in three subproblems which are solved explicitly via classical duality techniques, see (Karatzas et al., 1987). We compare the single agent problem and the multiple agent problem. Section 5 presents the specific case based on power utility functions and a mean reverting price of risk, first introduced in Wachter (2002).",Optimal sharing rule for a household with a portfolio management problem,https://www.sciencedirect.com/science/article/pii/S0165489619300484,9 July 2019,2019,Research Article,231.0
"Su Francis Edward,Zerbib Shira","Department of Mathematics, Harvey Mudd College, Claremont, CA 91711, United States,Department of Mathematics, Iowa State University, Ames IA 50011, United States","Received 26 October 2017, Revised 6 September 2018, Accepted 26 June 2019, Available online 2 July 2019, Version of Record 18 July 2019.",https://doi.org/10.1016/j.mathsocsci.2019.06.007,Cited by (2),"We survey a host of results from discrete geometry that have bearing on the analysis of geometric models of approval voting. Such models view the political spectrum as a geometric space, with ","The theory of set intersections has a natural connection to the study of approval voting. In approval voting, voters cast votes for as many candidates as they wish, and the candidate with the most votes wins the election (Brams and Fishburn, 1983). Approval voting has been championed as an election system that tends to elect moderate candidates and reduces the incentive to vote strategically (Brams and Fishburn, 1978, Laslier, 2009, Myerson and Weber, 1993), has desirable game-theoretic properties (Laslier and Sanver, 2010), and can be a favorable alternative to plurality voting (e.g., see Bouton and Castanheira, 2012, Bouton et al., 2016, Weber, 1995, Myerson, 2002).====The connection between approval voting and geometry arises from thinking of the political spectrum as a geometric space. By ====, we refer to the set of all possible political positions that voters can hold. Presently, for simplicity, we shall assume that every point of the political spectrum is represented by a candidate, so that ‘candidate’ and ‘position on the spectrum’ are synonymous. The political spectrum is often modeled as a line, with conservative candidates on the right and liberal candidates on the left. In other settings, a spectrum could be multi-dimensional (Mazur et al., 2018), or circular (Hardin, 2010), etc.====Berg et al. (2010) initiated the following model that ties intersections of geometric sets to approval voting. We assume each voter has an ====: the positions on the spectrum that she finds acceptable to vote for. Such a set may have some restrictions that are natural for a given problem. For instance, in ====, an approval set is naturally a convex set if when the voter approves candidates ==== and ====, she would also approve any candidate on a straight line between ==== and ====. But there are many other potential restrictions.====A political spectrum, with a collection of voters and their approval sets, is called a ====. Thus we may think of a society ==== with ==== voters as a pair ==== where ==== is a political spectrum and ==== is a collection of ==== approval sets that are subsets of ====. We define the ==== ==== of a society ==== to be the largest fraction of voters who can agree on a candidate. If ==== is a candidate who lies in the largest number of approval sets and that number is ====, then we call ==== an ==== (there may be many) and we see that ====. Thus studying approval voting is equivalent to understanding the ways that sets in ==== can intersect.====In discrete geometry, there are a number of theorems of the following type: given some ‘local’ intersection property, some ‘global’ intersection property must hold. A ‘local’ property could be information like ‘every two elements of ==== intersect’ or ‘when we pick 10 elements in ==== some 3 of them have a common point’ and the ‘global’ conclusion is of the form ‘there is a point ==== that lies in at least half the sets of ====’ or ‘there is a set of 7 points where each set of ==== contains at least one of these points’. Such theorems about set intersections can be translated to statements about approval voting.====For instance, it follows from a classical theorem of Helly (discussed later) that a collection of pairwise-intersecting intervals must have a point common to all the sets. The implication for a society whose political spectrum is a line and approval sets are intervals (such a society is called a ==== society – see Fig. 1) is that if approval sets are pairwise intersecting, then there is a candidate who receives the approval of every voter, i.e., ====. Here we see a rather strong hypothesis yields a rather strong conclusion, but we may be more interested in situations when we can guarantee a weaker conclusion. For instance, what hypotheses would guarantee the approval winner gets at least half the votes, or some other fraction of the votes? A first result in this direction is the following theorem:====The authors note this bound can be improved slightly based on the relationship between ==== and ====. If ==== for ====, they show ====, where ==== is the number of voters. For any ==== this gives the bound above, and as ====, this bound converges to ====.====Since then there have been a number of other results of this type (e.g. Klawe et al., 2014, Hardin, 2010, Mazur et al., 2018, Davis et al., 2014) that consider other hypotheses on approval sets and other geometric spaces for the political spectrum ====. These results are all in some sense variants of Helly’s Theorem.====This paper grew out of the realization that another concept from discrete geometry, the piercing number of a collection of sets, has a natural interpretation in voting theory, and a host of results about the piercing number readily translate into results about approval voting. A ==== of a collection of sets ==== is a set of points intersecting every set in ====. The ==== of ==== is the minimal size of a piercing set. If ==== is a collection of voter approval sets of society ====, then a piercing set is a ==== of ====: a set of candidates such that each voter is happy with at least one of the candidates. Thus results about piercing numbers have implications in approval voting on the size of possible representative candidate sets, as well as on the agreement proportions of societies.====The goal of our paper is to survey a selection of these results for the social choice community who may not yet be familiar with these results from discrete geometry. We are not attempting to be exhaustive; rather our intent is to give the reader a flavor of these results and their interpretations. We will cite known piercing results as theorems with references, and we state their implications for approval voting as corollaries.====We should mention that there is work on the social choice side that considers approval sets and their intersections (e.g., Laslier et al., 2017, Núñez and Xefteris, 2017) though these generally have not focused, as mathematicians have, on the geometric nature of the sets involved.",Piercing numbers in approval voting,https://www.sciencedirect.com/science/article/pii/S016548961930054X,2 July 2019,2019,Research Article,232.0
Hino Yoshifumi,"Business Administration Program, Vietnam–Japan University, Luu Huu Phuoc Street, My Dinh 1 Ward, Nam Tu Liem District, Hanoi, 12015, Viet Nam","Received 30 October 2017, Revised 20 June 2019, Accepted 22 June 2019, Available online 29 June 2019, Version of Record 3 July 2019.",https://doi.org/10.1016/j.mathsocsci.2019.06.005,Cited by (1),"We consider an infinitely repeated prisoner’s dilemma game with costly observation, where each player chooses a pairing of an action and an observational decision. If the player observes the opponent, the player pays an observation cost and observes the action just played by the opponent. Otherwise, the player cannot obtain any information about the action chosen by the opponent. We then introduce a correlated signal at the beginning of each stage game (nonpublic randomization) and prove an efficiency result without any implicit communication on the condition that the discount factor is sufficiently close to one and the observation cost is sufficiently small. We find that our results hold under both an arbitrary strongly or weakly correlated signal.","In this paper, we consider an infinitely repeated prisoner’s dilemma game with costly observation. In our game, each player chooses whether to observe the opponent in each period, but cannot obtain any information about the action chosen by the opponent without observation. This observation incurs a positive observation cost. Importantly, it is an open question whether efficiency is achievable in an infinitely repeated prisoner’s dilemma game or not. We show, though, that if the players receive private and correlated signals at the beginning of each period, then efficiency can be approximated by a sequential equilibrium.====To address this, we introduce “====” in the form of correlated signals at the beginning of each stage game where each player receives a binary signal. As players receive these signals before the choice of action, the signals do not contain any information about the action chosen by the opponent. We demonstrate the effects of this nonpublic randomization throughout the paper.====Our main contribution is to provide an efficiency result without communication in a repeated prisoner’s dilemma game. This is in contrast to the existing literature, which yields an efficiency result only when communication is available. An additional contribution is the construction of sequential equilibria. Our construction of the strategy, as explained at the end of this section, is quite simple and uses only a three-state automaton, whereas other studies require a substantially more complicated construction (e.g., Miyagawa et al. (2008) employ a six-state automaton).====Moreover, our propositions require only (1) that the distribution of the signals is not independent, and (2) that the distribution of the signals has full support. Therefore, our signal requirement is not restrictive and can therefore approximate two kinds of crucial situations: an independent signal and a perfectly correlated signal (public randomization). If the correlation coefficient is arbitrarily close to zero, the signals are an almost independent signal; if the correlation coefficient is arbitrarily close to either ==== or ====, the signal is close to the perfectly correlated signal.====As discussed, we use nonpublic randomization in this analysis, which can be interpreted as a special type of a mediator. Some previous works show that efficiency is achievable if a dynamic mediator is available. Aoyagi (2005) uses dynamic mediated strategies under ====-perfect monitoring. Costly observation, our monitoring structure, is not ====-perfect, so his result does not hold in our model. In contrast, Rahman and Obara, 2010, Rahman, 2012 consider a contract and a mediator, with (Rahman and Obara, 2010) assuming that the mediator is endogenous, while Rahman (2012) provides some propositions given the mediator. In both these studies, the entire profile of recommendations is revealed publicly at the end of the each stage game, whereas in our model, the recommendation is never made public and so the players’ continuation strategies do not rely on recommendations made in the past. In addition, unlike our propositions, most of the existing literature on mediators requires strongly correlated recommendations to provide their results.====Costly observation is a kind of imperfect monitoring. Some studies on imperfect monitoring confine their attention to public monitoring, with Abreu et al. (1990) characterizing public perfect equilibria, Fudenberg et al. (1994) providing conditions for folk theorem and Fudenberg and Levine (1994) describing the limit set of public perfect equilibrium payoffs. However, when the monitoring structure is private, analysis is much more difficult, with seminal work being Sekiguchi (1997), who shows a nearly efficient sequential equilibrium under private monitoring given that the private signals are almost perfect and players are patient.====We can divide subsequent studies on private monitoring into three kinds of approaches: belief-based equilibrium analysis, belief-free equilibrium analysis, and communication-based equilibrium analysis. For the most part, the present paper and most of the existing literature on costly observation employ belief-based equilibrium analysis, with Miyagawa et al. (2003) analyzing the same monitoring structure as this analysis. They premise a sufficiently small observation cost and demonstrate a Nash folk theorem; that is, any payoff vector that Pareto-dominates a stage-game Nash payoff vector can be achieved by a sequential equilibrium. Their sufficient condition for the folk theorem requires that each player can choose at least three actions so that players can communicate via mixed actions. However, their result does not cover an infinitely repeated prisoner’s dilemma game where each player can choose only two kinds of actions.====The monitoring structure in Flesch and Perea (2009) is close to that in this paper, and assumes that players cannot obtain any information when they do not observe the opponent. In their model, players can purchase precise information about the actions taken in the past by the other player. They show that a folk theorem holds without any randomization device or cheap talk when at least three players (resp. four players) are involved, and each player has at least four actions (resp. three actions). On this basis, Flesch and Perea (2009) conjecture that an efficiency result could not hold in a two-player prisoner’s dilemma game. However, our result (Proposition 2) proves that an efficiency result holds in their monitoring structure when a nonpublic randomization device is available.====In other related work, Lehrer and Solan (2018) and Kandori and Obara (2004) assume that the observational decision is observable. Lehrer and Solan (2018) suppose that the observational decision is common knowledge and analyze a high frequency repeated game, where they characterize the limit set of public perfect equilibrium payoffs as the observation cost tends to zero and prove that the set is a strict subset of the set of feasible and individually rational payoffs. Alternatively, Kandori and Obara (2004) assume that each player can obtain almost perfect information about the observational decision of the opponent when observing the opponent, and provide an efficiency result for any level of observation cost.====Elsewhere, Miyagawa et al. (2008) introduce a free signal about actions taken by players. In their model, players obtain an imperfect signal even when they do not observe the opponent. They also assume that players choose an observational decision after the choice of action, and that public randomization devices are available just before each decision, and by doing so demonstrate a folk theorem for any level of observation cost.====One belief-free equilibrium analysis that includes costly observation is Sugaya (2011), who considers repeated games under general private monitoring. He shows that folk theorem holds when the number of each player’s signal is sufficiently large. However, as the number of signals that players can observe in costly observations is small, his result does not cover costly observation.====Many studies (e.g., Compte (1998), Kandori and Matsushima (1998), Fudenberg and Levine (2007), and Obara (2009)) implement communication-based equilibrium analysis. Communication enables players to share information without cost and sometimes helps to coordinate with each other. For example, Ben-Porath and Kahneman (2003) and Sugaya and Wolitzky (2016) undertake communication-based equilibrium analysis. Ben-Porath and Kahneman (2003) show that a folk theorem holds under costly observation. Lastly, Sugaya and Wolitzky (2016) consider an infinitely repeated game under general private monitoring and show a simple sufficient condition for the existence of a recursive upper bound on the sequential equilibrium payoff set in two-player repeated games. These are unlike our model where there is no communication.====Let us now briefly explain our equilibrium. Consider a simple repeated prisoner’s dilemma game where each player chooses ==== or ==== every period. As mentioned, a private signal ==== for player ==== is realized at the beginning of the stage game. Assume that ==== for each ====. Our strategy has only two states on the equilibrium path, a cooperation state and a defection state, and one state off the path, an error state. In the cooperation state, player ==== chooses ==== if ====, and mixes ==== and ==== if ====. Here, player ==== observes the opponent in the cooperation state if and only if he chooses ====. Player ==== chooses ==== and does not observe the opponent in the defection state, and chooses an optimal action and observational decision in the error state given his/her belief and the state of the opponent.====The state transition of each player ==== is then conditional not only on what the player observes but also on any action by the player. If player ==== chooses ==== or observes ====, the state remains the same, but if player ==== chooses action ==== and observes ====, then the state moves to the defection state in the next period. However, the states remain the same in both the defection and error states. That is, this strategy is a variant of the grim strategy whose trigger is playing ==== and observing ==== in the same period.====The role of nonpublic randomization is to change the belief of the action chosen by the opponent and change the best response action in the cooperation state. Both actions ==== and ==== must be indifferent for player ==== when the player receives ==== in the cooperation state because player ==== randomizes actions ==== and ====. Player ==== believes that ==== is realized with a higher probability when the player receives ==== than when the player receives ==== because ====. This implies that player ==== chooses ==== with a higher probability. Given player ==== wants to remain in the cooperation state, player ==== then has a stronger incentive to choose ==== when the player receives ==== than when ====. Therefore, action ==== is suboptimal for player ==== when ==== is realized and his state is the cooperation state.====The fact that action ==== is suboptimal for player ==== when the player receives ==== in the cooperation state yields an incentive to observe the opponent. Assume that player ==== does not observe the opponent when choosing action ==== in the cooperation state. Player ==== then cannot know to which state he should move. We show that no matter what player ==== chooses in the next period, player ==== must pay an opportunity cost when receiving ====. Accordingly, action ==== is suboptimal for player ==== when ==== and his state is the cooperation state. This means that the player must pay some opportunity cost of choosing action ==== if the action of the opponent was ==== in the previous period. Action ==== is also costly for player ==== when the action of the opponent was ==== in the previous period.====Therefore, player ==== must pay some positive (opportunity) costs, irrespective of the action in the next period. Thus, if the observation cost is sufficiently small when compared with the above costs, player ==== prefers to observe the opponent when choosing action ==== in the cooperation state. Owing to the nonpublic signal, players thus have an incentive to observe the opponent. Now suppose a lack of nonpublic randomization. Players are then always indifferent between ==== and ==== in the cooperation state, and players will prefer ==== in the defection state. This means that one of the optimal continuation strategies with respect to action is choosing ==== every period irrespective of what players observe in that period. Therefore, this strategy does not constitute a sequential equilibrium.====The rest of this paper is organized as follows. Section 2 introduces the model of a repeated prisoner’s dilemma with costly observation. Section 3 discusses our results and Section 4 provides some concluding remarks.",An efficiency result in a repeated prisoner’s dilemma game under costly observation with nonpublic randomization,https://www.sciencedirect.com/science/article/pii/S0165489619300526,29 June 2019,2019,Research Article,233.0
Galambos Adam,"Department of Economics, Lawrence University, Appleton, WI, United States","Received 1 September 2017, Revised 3 June 2018, Accepted 22 June 2019, Available online 27 June 2019, Version of Record 4 July 2019.",https://doi.org/10.1016/j.mathsocsci.2019.06.006,Cited by (0),.,"The extensive literature on revealed preference theory goes back to Paul Samuelson’s seminal work in the 1930s. In Samuelson’s words, “[i]n its narrow version the theory of “revealed preference” confines itself to a finite set of observable price-quantity competitive demand data, and attempts to discover the full empirical implications of the hypothesis that the individual is consistent”. (Samuelson, 1953) This consistency is embodied in some version of the Strong Axiom of Revealed Preference. The quote highlights that for Samuelson, the revealed preference approach was motivated by his goal of deriving ==== theorems; that is, results that can be translated to a set of operations that were empirically meaningful. (Hands, 2001 pp. 60–69).==== ==== In fact, as Hands points out, Samuelson did not use the words “revealed preference” in his early work, presumably because he was intent on putting consumer theory on the more solid foundations of choice data. In principle, neither utility nor preference need appear as part of such a theory. The first goal of what became revealed preference theory was, therefore, to put consumer theory on foundations that are observable and empirically meaningful.====In practical terms, this meant that the theory had to be restated without reference to utility or preference, which were purely theoretical notions. Instead, the empirically meaningful notion of observed choice had to suffice. In the philosophy of science literature, this process of restating theory in terms of observables only is called ====. A revealed preference axiom characterizing a choice theory is equivalent to the ==== for that theory. A particularly simple equivalent of the Ramsey sentence, such as the Strong Axiom of Revealed Preference, tells us what the “core” of the theory is in observational terms, and can give us some insight into and intuition about the meaning of the theory. The first contribution of this paper is to identify the notion of “simplicity” of a revealed preference axiom, or any Ramsey sentence of a theory, with its ====, i.e. the logical complexity of the language necessary to state it. This refinement of Ramsey eliminability opens up the possibility of characterizing the complexity of a revealed preference axiom, and also of comparing the complexity of different revealed preference axioms.====Empirically meaningful foundations make it possible (or easier) to test the theory as well. Do observations corroborate or contradict the theory? The revealed preference approach thus also came to be viewed as a way of making the foundations of microeconomics testable as a scientific theory. Therefore a desirable property for a revealed preference axiom is tractability: it should be possible to check in finite and “reasonable” time whether choice data satisfy the axiom. (Gradwohl and Shmaya, 2015) The second contribution of this paper is to point out that the simplicity and the computational tractability of revealed preference axioms are very tightly connected. This follows from important results in descriptive complexity theory. The implication is that there is a simple, elegant connection between the simplicity of the language of a revealed preference axiom, and the computational tractability of using the axiom to test data. Both of these issues have received attention in the revealed preference literature, but the connection between the two has not been made. The first three theorems in this paper draw conclusions about the computational complexity of revealed preference questions based on the descriptive complexity of revealed preference conditions; the last theorem draws conclusions about the descriptive complexity of any revealed preference condition for Nash equilibrium based on the computational complexity of Nash rationalizability.====The main contributions of this paper are thus in showing how results from descriptive complexity theory can shed light on important issues that the revealed preference literature has grappled with, and in characterizing the descriptive complexity of revealed preference axioms of various choice theories by introducing a suitable refinement of Ramsey eliminability. Finally, we illustrate this approach for the Nash equilibrium solution concept. We first show that the Nash rationalizability problem is ====-complete, i.e., computationally intractable. Then we use this result and a fundamental theorem from descriptive complexity theory to show that the descriptive complexity of any revealed preference axiom for Nash equilibrium must be higher than first order logic extended by transitive closure.====In this paper, only finite structures are considered. Understanding the finite case is generally important in choice theories, and assuming finiteness makes it possible to ask questions about computational tractability as well. For finite structures, some of the approaches to falsifiability of choice theories in the literature are not particularly meaningful (Chambers et al., 2017). Section 4 will address this and other questions regarding related literature. Section 2 introduces the required formal logic framework. Section 3 provides a formalization of the notion of simplicity for axioms, which is a refinement of Ramsey eliminability (to be defined in that section), and shows that this notion of simplicity is equivalent to the tractability of testing whether observed data satisfy the axiom.",Descriptive complexity and revealed preference theory,https://www.sciencedirect.com/science/article/pii/S0165489619300538,27 June 2019,2019,Research Article,234.0
"Krishna R. Vijay,Sadowski Philipp","Florida State University, United States,Duke University, United States","Received 8 February 2017, Revised 15 March 2019, Accepted 17 June 2019, Available online 26 June 2019, Version of Record 28 June 2019.",https://doi.org/10.1016/j.mathsocsci.2019.06.004,Cited by (0),"If price volatility is caused in some part by taste shocks, then it should be positively correlated with the liquidity premium. Our argument is based on Krishna and Sadowski (2014), who provide foundations for a representation of dynamic choice with taste shocks, and show that volatility in tastes corresponds to a desire to maintain flexibility. To formally connect volatile tastes to price volatility and preference for flexibility to the liquidity premium, we analyze a modified simple Lucas tree economy with two otherwise identical assets, where one provides more liquidity because its output can be traded on an auxiliary international market, and where the representative agent is uncertain about his degree of future ","Following Black (1987), a growing literature has argued that taste shocks are important for our understanding of business cycles and asset prices. For example, Smith and Whitelaw (2009) find evidence that the largest component of changes in the equity risk premium is variation in risk aversion, rather than the amount of risk, and Bekaert et al. (2010) show that stochastic risk aversion that is not driven by, or perfectly correlated with, the fundamentals of the economy can simultaneously explain a range of asset pricing phenomena as well as the behavior of bond and stock markets.====At the same time, a powerful critique of the use of taste shocks in empirical work notes that taste shocks are typically not directly observable, so that they become free parameters. As Nason (1997) writes, ‘… for taste shocks to have economically meaningful content, they must be grounded solidly in economic theory and tied to observable phenomena, which is not always easy.’====In this paper we build on the axiomatically founded model of taste shocks in Krishna and Sadowski (2014) (henceforth KS), in which those shocks can be uniquely identified from observable behavior. Based on this identification, KS provide comparative statics that link more volatile tastes to greater preference for flexibility, that is, a desire not to commit to future choice ahead of time. We argue that more volatile tastes should correspond to higher price volatility, and a desire to maintain flexibility should lead to a higher liquidity premium.====There is indeed strong evidence of a positive correlation between the liquidity premium and price volatility, for example in Nagel (2012). Existing explanations of this phenomenon understand the demand for liquidity as a reaction to increased volatility, for instance via the assumption that risk aversion increases when volatility is high.==== ==== We show instead that taste shocks, such as varying risk aversion, can directly drive both, price volatility and the liquidity premium.==== ====To formally make our argument in a very simple model, we consider a representative agent, who is modeled as in KS and receives iid shocks to his aversion to risk in current consumption. We then analyze a small Lucas tree economy with closed asset markets but partially open goods markets, that is, we enrich the most basic Lucas tree economy by adding a second productive asset. Output from the two assets are perfect substitutes for domestic consumption, but only one of them can be traded on the international market, and so provides more liquidity. Our main set of results establishes that, indeed, more severe taste shocks correspond to higher price volatility (which taste shock models are often used to explain), and they also drive a larger liquidity premium (ie, a higher price of the liquid as compared to the illiquid output and corresponding asset). This direct link between price volatility and the liquidity premium could provide discipline for the use of taste shock models in applied work.====The basic intuition for the relationship between volatility in tastes and in prices is as follows. The current realized utility (the taste for current consumption) obviously affects the representative agent’s valuation of his rights to current output. At the same time, an agent with iidtaste shocks values shares of the productive asset independently of the current realized utility. The price for trading shares of the asset against rights to current output should be determined by the difference in these valuations. Hence, more variation in utilities should imply more variation in prices. This intuition is incomplete, because the equilibrium price in the Lucas model depends not on the representative agent’s realized utility, but on the realized ==== utility in the current output of the productive asset (from the first order condition that ensures that at the equilibrium price all output is consumed). We handle this gap between intuition and model with a simple trick: We interpret shares as probabilistic rights to the entire output, so that utility becomes linear in the share for any given level of output, and the marginal utility of additional probability of receiving the current output is precisely the realized utility of current output.==== ====Now suppose that one productive asset provides less liquid output than another productive asset, as suggested above. We expect preference for flexibility to be associated with a tendency to invest less in the asset with illiquid output and to value that output less. In order to capture this intuition in our simple Lucas tree economy, consider the following variant: There is a green tree that produces a perishable green fruit which is only for domestic consumption. There is also a red tree which produces a perishable red fruit. Both trees produce the same uncertain amount of fruit, and the two fruit are perfect substitutes domestically. The only difference is that probabilistic rights to some amount of red fruit can be traded internationally for rights to some other amount of red fruit. Therefore, ownership rights to red output provide the agent with more flexibility or ‘liquidity’ than ownership rights to green fruit, and the propensity to invest in the more liquid asset results in a greater price for the this asset – the liquidity premium – which is increasing in the desire for flexibility.====The remainder of the paper is structured as follows. Section 2 introduces the representation as well as the comparative statics from KS. Section 3 analyzes the standard Lucas tree economy without liquidity concerns and relates volatility in tastes to volatility in prices. Section 4 adds liquidity concerns to this economy and presents our main results.",Preferences with taste shock representations: Price volatility and the liquidity premium,https://www.sciencedirect.com/science/article/pii/S0165489619300514,26 June 2019,2019,Research Article,235.0
"Koster Maurice,Boonen Tim J.","University of Amsterdam, Amsterdam School of Economics/CeNDEF, Roetersstraat 11, 1018WB, Amsterdam, The Netherlands,University of Amsterdam, Amsterdam School of Economics, Roetersstraat 11, 1018WB Amsterdam, The Netherlands","Received 3 September 2018, Revised 7 June 2019, Accepted 7 June 2019, Available online 17 June 2019, Version of Record 25 June 2019.",https://doi.org/10.1016/j.mathsocsci.2019.06.002,Cited by (5),"This paper presents a model of a multi-divisional firm to share the joint yet uncertain and fixed cost of running a central operational unit. A firm aims at allocating this cost ====, subject to constraints imposed by the asymmetric and limited liabilities of the different divisions. We study solutions that are made up of a vector of ==== payments which are allocated in absence of costs, and a remaining solution that is contingent on the cost. Under a mild continuity condition we find different classes of ==== solutions. The class of egalitarian proportional solutions is characterized by dependency on the disutility of the total cost instead of details of the distribution. In this class, there is a unique proportional solution which systematically minimizes the maximal transfer. A fundamentally different egalitarian solution is the ","Consider a multi-divisional firm with a central service unit — to which each of ==== divisions have equal access. Running this shared facility is costly and the divisions are charged for the full and uncertain cost. We will focus on cost allocation, and study for ==== contracts that the firm may use to share the ==== realized cost. The firm puts upper bounds on the liability of a division, just as long as the total of maximal liabilities is enough to cover the costs arising in the worst-case scenario. The maximal liabilities of the divisions may be the result from exogenous risk capital allocations within the firm, and are limiting the divisions’ capacity to bear risk (see, e.g., Myers and Read, 2001).====We assume that it is up to a benevolent manager to allocate the random cost, which is considered a social bad, among the divisions. We propagate an allocation of the total cost that reconciles the possible asymmetric way in which the divisions can be ultimately be held responsible for upon realization of the cost. In this paper we take egalitarianism as the fundamental principle that should govern the allocation, which means that ideally the realizations of the cost are shared equally by the divisions. However, this is not necessarily feasible in case liabilities are different and the realized cost is in this respect high enough. In such scenarios the divisions with high liabilities may ==== contribute more than those with low liabilities. Despite the fact that the divisions cannot be treated equally in those cases, we will cherish the idea of cost allocations that are symmetric functions of the liability profiles.====We pose the question whether symmetric ==== solutions exist according to which all agents face the same ==== disutility level, even if this requires asymmetric solutions to realized instances of the constrained cost allocation problem. In order to make sense of intercomparison of the agents, we will assume that all have the same beliefs regarding the underlying probability distribution, and assume that all try to minimize an expected cost with possible subjective but homogeneous probabilities. We interpret this expected cost as disutility. Using the same disutility function for the collective of agents makes the concept of egalitarianism straightforward.====In case the liabilities are not high enough to be able to bear the share of ==== of the realized cost, solutions may demand higher contributions from those agents with the higher liabilities. When such solutions are egalitarian, these agents will be compensated by those with low liabilities in case of a low realized cost. This compensation scheme via ==== is such that all agents are assured to be exposed, ====, to the same disutility.====One particularly interesting class of solutions is when allocations for these ==== are obtained by solving a ==== by a ====. A rationing problem describes the situation in which we allocate a given amount (often referred to as estate) among a group of agents when the available amount is not enough to satisfy all their claims. A rationing rule calculates shares for agents such that (1) no agent gets more than his/her claim, and (2) all get a non-negative share.==== ==== With the realized cost as estate and the profile of liabilities as claims, each constrained cost allocation problem is the natural counterpart of a rationing problem. In fact, the constrained cost allocation problem generalizes the rationing problem to allow for a stochastic cost. Then, each rationing rule can be taken to define a cost allocation solution. We show that each rationing rule that is continuous in the “claims” component can be used to define an egalitarian solution. In particular, included are many solutions that are symmetric as function of the profile of liabilities, meaning that agents are regarded equal and possible asymmetries between the proposed allocations should be motivated by the differences in liability. We discuss two special subclasses of solutions therein, one that is generated by proportional rules and the other is based on the constrained equal awards rationing rule.====Characteristic of the proportional solutions is that all satisfy an invariance property regarding the underlying distribution of the total cost; as long as the disutility of the total cost is the same, the cost allocation solution is the same. We prove that egalitarian solutions with this ==== (IDPP) property are in fact proportional solutions. The egalitarian solution that we get using the constrained equal awards rationing rule as generator is denoted as the ==== (====) solution. The solution is characterized as the unique egalitarian solution with the ==== (LS) property, according to which marginal increases of the realized cost affect the agent’s marginal contribution in the same way — for those agents whose liabilities are still not met.====Our aim is to allocate equal cost shares whenever this is feasible. In case the liabilities are high enough so that we can allocate the share of ==== of the realized cost, we propose this as the final settlement of the cost allocation problem. We will refer to this property as ==== (SSL). A particular proportional solution is introduced as an egalitarian proportional solution with the SSL property. Additionally, we show that this proportional solution systematically minimizes the maximal transfers within the class of egalitarian proportional solutions. More precisely, the vector of transfers generated by this solution Lorenz-dominates all others used by the egalitarian proportional solutions. Nevertheless, we show that ==== also satisfies SSL and also that it uses a (weakly) lower maximal transfer than does any egalitarian proportional solution. In particular this means that the vector of transfers according to ==== is not dominated by the vector of transfers corresponding to any proportional solution. We discuss the egalitarian solution underpinned by the constrained equal losses rationing rule as an example of a solution that does not even satisfy SSL, despite the fact that the underlying rationing rule is credited egalitarian properties in the rationing framework.====Whereas egalitarian solutions play a central role in this paper, this does not exclude meaningful deviations from the egalitarian allocation. We finalize this paper by showing that the strict interpretation of egalitarianism may be relaxed, and that much of the reasoning throughout the paper can be used to allocate the total cost such that the disutility of agents are desired proportions of the total disutility. Then this more general set-up bridges the gap between the theory of allocating a random cost and a social norm. It is the social norm that governs the fixed proportions of the total disutility ====, subject to ==== feasibility defined by the individual liabilities. And those social norms exogenous to the model may require other proportions in which the disutility of the total cost is shared ====, leading to alternative concepts of ====. Deviations from pure egalitarianism may be motivated by non-symmetric initially received contributions to bare the total cost. Our fairness constraint is akin to the financial fairness condition in risk-sharing (Pazdera et al., 2017). This more general idea of extending a pure egalitarian setup to fairness only requires a slight adaptation of our original model formulation. In addition, the ==== solutions inherit the very same structure of the pure egalitarianism that is elemental to the standard cost allocation solutions like ====; basically, the solutions only make a correction of the transfers at zero cost. The concept of fairness was originally introduced for risk exchanges by Bühlmann and Jewell (1979). Bühlmann and Jewell (1979) and also Pazdera et al. (2017) consider the case of risk-sharing, where the agents have initial stochastic endowments to be shared, and the focus is on the induced incentives for agents to participate or not. In this paper we concentrate on the fundamentally different problem of allocating the stochastic cost of a public good.====Habis and Herings (2013) and Ertemel and Kumar (2018) also study stochastic rationing problems, but where both the estate and the claims are considered stochastic. Like us, the authors concentrate on ==== solutions, but with a focus on notions of stability and enhancing cooperation. Kıbrıs and Kıbrıs (2013), Karagözoğlu (2014) and Boonen (2017) study investment problems with an endogenous and stochastic estate, which are seen as bankruptcy problems in case of default and that are solved as such. Instead of investing with risk of a defaulting counterparty, we concentrate in this paper on a public good of which the cost needs to be allocated under liability constraints. Hougaard and Moulin (2018) study a problem of sharing the cost of a stochastic network, where the focus is on determining cost shares ==== such that these equal the expectation over the random realization of the network of the shares ====. Xue (2018) and Long et al. (2019) study cost-sharing without liability constraints, but with uncertain claims on a divisible commodity. Their objectives focus on maximizing social welfare functions based on notions of waste and deficit.====The rest of this paper is organized as follows. Section 2 specifies the model, and Section 3 provides our construction of egalitarian solutions and transfers. The ordering of transfers is studied in Section 4. Sections 5 Characterization of proportional solutions, 6 Characterization of constrained egalitarian solutions are devoted to defining and characterizing of our solutions based on the proportional and constrained equal awards rules. Section 7 compares these solutions based on a ranking of the transfers. Section 8 provides a generalization of the concept of egalitarianism to the case where we first exogenously allocate the disutility of the total cost in the desired ratio to the agents. Section 9 provides a remark in which we explain how our solution concept can be generalized to situations where egalitarian solutions do not exist, i.e., where some of the agents have too low liabilities to take a fair share of the risk. Finally, Section 10 concludes. All proofs are delegated to the Appendix.",Constrained stochastic cost allocation,https://www.sciencedirect.com/science/article/pii/S0165489619300411,17 June 2019,2019,Research Article,236.0
Akyol Ethem,"Department of Economics, TOBB University of Economics and Technology, So ̈ğütözü Cad. No:43, Ankara, Turkey","Received 22 February 2018, Revised 27 May 2019, Accepted 1 June 2019, Available online 12 June 2019, Version of Record 17 June 2019.",https://doi.org/10.1016/j.mathsocsci.2019.06.001,Cited by (5),"We consider second-price auctions with participation costs and investigate the effects of resale after an auction. There are two potential bidders whose valuations are commonly known. Bidders incur a privately known cost to participate in the auction. We examine how resale affects bidders’ entry behavior, seller’s expected revenue, and social welfare. We show that resale increases (decreases) entry by the lower-(higher-)value bidder, inducing a “more symmetric” equilibrium, and generates higher revenue for the seller. Furthermore, while resale may be detrimental to ex post welfare, it always increases the ex ante welfare.","We study second-price auctions when bidders have to incur participation costs==== ==== in order to enter the auction. Our objective is to investigate the effects of a resale opportunity after the auction takes place. We consider a second-price auction with two bidders who are asymmetric in their valuations of an object, and assume that the actual participant(s) are not revealed at the beginning of the auction.==== ==== We further assume that bidders’ valuations are commonly known and that the participation costs of bidders are private and independently distributed. When resale is allowed, the reallocation of an object is allowed in the following way. The winner of the auction can sell the object by making a take-it-or-leave-it offer to the other bidder after the auction is complete. The resale stage is assumed to be costless.==== ==== Although there is a second-price auction in the first stage, which is efficient in that the higher-value participant always gets the object, our model includes the following motive for resale. Because participating in the auction is costly, it may be that a higher-value bidder does not enter the auction, whereas a lower-value bidder does do so.====First, we analyze bidders’ entry behavior when there is no resale opportunity and also when there is a resale opportunity. Then, we compare how the bidders’ behavior changes. For each case, we show that there is always an “intuitive” equilibrium in which the higher-value bidder participates in the auction more frequently than the lower-value bidder does. Furthermore, we show that we may also have “non-intuitive” equilibria, and provide sufficient conditions for the existence of unique equilibrium.==== ==== To understand the effects of resale, we show that once resale is allowed, there is always an equilibrium in which the lower-value bidder enters more frequently and the higher-value bidder enters less frequently than when resale is not allowed. This yields the effect of resale on the bidders’ entry behavior. The lower-value bidder wants to get the object and sell it at a higher price in the next stage. On the other hand, the higher-value bidder enters less frequently because he faces the risk of paying more frequently when he enters the auction.====Second, we investigate how resale affects the initial seller’s revenue. This analysis is nontrivial, given that we may have multiple equilibria. To answer this question, we provide a more general result. We show that the “more symmetric” an equilibrium (when resale is either allowed or not allowed), the higher the revenue is to the seller. Furthermore, we show that the most “symmetric” of the equilibria in both cases must be an equilibrium in the resale case. Hence, the equilibrium that gives the highest expected revenue to the seller is a resale equilibrium.====Then, we focus on the effects of resale on social welfare. Using a simple example, we show that resale is not necessarily superior to no-resale in terms of ex post welfare. This is quite surprising in that it is commonly agreed that resale increases ex post welfare since it gives a chance to a lower-value bidder to sell the object to a bidder whose valuation for the object is higher, which results in a more efficient outcome as a bidder with a higher valuation gets the object at the end. However, our model shows that this may not always be the case, owing to participation costs. Then, we investigate the effects of resale opportunity on ex ante welfare. Here, we show that, contrary to the findings of the revenue analysis, a more asymmetric equilibrium may improve ex ante welfare more than a symmetric equilibrium does. However, we find that the equilibrium that induces the highest ex ante social welfare must be an equilibrium in the resale case.====This study is related to the literature on auctions with costly entry. Among many others, Green and Laffont (1984), McAfee and McMillan (1987), Levin and Smith (1994), and Tan and Yilankaya (2006) study auctions with costly entry ==== a resale opportunity. Most of these studies focus on environments with ==== bidders and investigate the properties of a ==== only, although there may exist multiple equilibria, even in a symmetric setting. In an exceptional paper, Tan and Yilankaya (2006) study a two-bidder private-value environment and assume that bidders have to incur a commonly known cost in order to participate in the auction. They first study the case in which the bidders are symmetric==== ==== and provide sufficient conditions for a symmetric equilibrium to be the unique equilibrium, as well as those for the existence of asymmetric equilibria. They also consider the asymmetric case, providing sufficient conditions for the existence of multiple equilibria, as well as those for uniqueness. However, while they investigate the existence of equilibria, they do not examine the properties of those equilibria (e.g., welfare properties or revenue effects).====This study is also related to the literature on auctions with resale. Gupta and Lebrun (1999) consider first-price auctions with private values followed by a resale stage, before which values are revealed. Pagnozzi (2007) considers an environment with two bidders and commonly known values. He then shows why a higher-value bidder may prefer to drop out of the ascending auction before the price has reached her valuation, and then acquire the prize in the aftermarket. Garratt and Tröger (2006) consider a model with a speculator bidder who has no use value of the object, but who can benefit from participating in the auction when resale is allowed. Hafalir and Krishna (2008) consider asymmetric auctions with private values. In their model, values are ==== revealed after the auction. Bidders update their beliefs on their opponent’s values based only on the identity of the winner and the winning bid. They show that, in contrast to the case when resale is not allowed, a revenue ranking between first- and second-price auctions can be obtained. Furthermore, the first-price auction yields a higher revenue than the second-price auction does for regular distributions. Hafalir and Krishna (2009) compare first-price auctions with and without resale in terms of revenue and social welfare. Although a general ranking is not available, they show that for three families of distributions for which the equilibria of first-price auctions are available in closed form, resale yields greater revenue to the seller than the no-resale case does. Furthermore, they show using an example that resale may decrease the ex ante welfare.====To the best of our knowledge, there are only two other studies that incorporate costly entry with resale. Xu et al. (2013) consider an environment with ex ante ==== bidders. Bidders have privately known values and participation costs, which may be one of two types. Similar to our model, while  bidders must incur a participation cost for the initial auction, the resale stage is costless. Under these assumptions, they characterize an equilibrium when resale is allowed. However, owing to the existence of incomplete multidimensional information, only some numerical examples can be derived for the effect of resale on revenue and efficiency. Then, Celik and Yilankaya (2017) consider a similar setup, where bidders’ valuations are privately known and drawn from a symmetric distribution, but the participation cost is the same for each bidder and is ====. Furthermore, their resale stage is costless. They show that although bidders are ex ante symmetric, there may exist asymmetric equilibria that induce resale opportunities. As such, they examine the effect of resale on revenue and efficiency. Our model simplifies the bidding behavior by assuming commonly known values. Then, we focus on the differences in bidders’ entry behavior between when resale is allowed and when it is not possible. This allows us to derive analytical results for the effects of resale on bidders’ entry behavior, the seller’s revenue, and social welfare.====The remainder of the paper is organized as follows. Section 2 presents the model, after which Section 3 describes the equilibrium for the resale and no-resale cases. This section also compares the equilibria between the two cases. Sections 4 Seller’s revenue: Symmetrization effect, 5 Social welfare present the results for the seller’s revenue and social welfare, respectively. Lastly, we discuss our results, before concluding the paper. The Appendix contains all proofs omitted from the main body of the paper.",Effects of resale in second-price auctions with private participation costs,https://www.sciencedirect.com/science/article/pii/S016548961930040X,12 June 2019,2019,Research Article,237.0
Lee Jong Jae,"Department of Mathematical Economics and Finance, Economics and Management School, Wuhan University, Wuhan, 430072, China","Received 16 December 2016, Revised 27 August 2018, Accepted 7 May 2019, Available online 11 June 2019, Version of Record 17 June 2019.",https://doi.org/10.1016/j.mathsocsci.2019.05.005,Cited by (0),"Holmström and Myerson (1983) show that we need only check for efficiency on common knowledge events to determine that an incentive compatible decision rule is efficient. In this paper, we show that this result still holds in a more general setup: (i) in a type space where the agents’ type does not need to be derived from a prior; (ii) with a notion of common certainty instead of common knowledge. More importantly, we strengthen the Holmström–Myersonresult by showing that we need only check for efficiency in a ==== subset of common certainty events known as self-evident events and furthermore, that this is the ","The question of whether individuals are able to achieve an efficient allocation has been central in economics. With complete information, the answer is likely to be positive. If a currently given allocation, the so-called status quo allocation, is inefficient, some individual may propose another allocation that would make him better off without making others worse off. As Coase (1960) argues, other individuals would accept the proposal unless the cost of bargaining is substantial. In an economy with incomplete information, the conclusion is less likely to be true. Even when the proposed allocation leads to a Pareto improvement, individuals may reject the proposal. Behind this seemingly puzzling argument lies the possibility that the act of proposing itself reveals the proposer’s private information, and thus may reverse the preferences of individuals. Noticing this possibility, Wilson (1978) proposes two notions of efficiency in economies with incomplete information by requiring both notions to satisfy no revelation of private information. That is, an allocation is efficient unless there exists a common knowledge event on which another allocation Pareto-dominates it.====Holmström and Myerson (1983) (henceforth, HM) elaborate on Wilson’s notions further by identifying three different issues embedded in them: one about defining the notion of efficiency in the presence of privation information, another about whether each individual would follow a decision rule sincerely, so-called incentive compatibility, and the last one about the condition under which an incentive compatible and efficient (in short, incentive efficient) decision rule==== ==== would be implemented without revealing any private information of individuals.====Using this tripartite separation, HM define the notion of efficiency analogously to the case with complete information. A decision rule is efficient if there is no other decision rule that every individual, conditional on her private information, prefers to it. Accordingly, to ask whether a decision rule is incentive efficient is not the same as to ask whether such a decision rule can be implemented without the possibility of information revelation. In other words, there may exist an incentive efficient decision rule that is implementable through some information revelation.====Surprisingly, however, HM show that ====.==== ==== This implies that checking for efficiency on common knowledge events is sufficient to determine an incentive efficient decision rule. This saves one the effort of considering all possible events. Due to this advantage, it has been widely utilized in various contexts. For example, Vohra (1999) states the following: ====In this paper, we investigate HM’s result in a more general setup. Specifically, we consider type spaces where the agents’ type is not necessarily derived from a prior distribution.====  In addition, by replacing the notion of common knowledge with a weaker notion of common certainty or, equivalently, common belief with probability ==== (Brandenburger and Dekel, 1987, Monderer and Samet, 1989), we find that there are more events to check for efficiency to determine that an incentive compatible decision rule is efficient (Example 1, Theorem 2). This may raise concerns that it actually weakens HM’s result by increasing the burden of checking for efficiency. However, we argue that one may disregard them. We need only check for efficiency in a ==== subset of common knowledge events known as self-evident events (Theorem 3). Furthermore, the class of self-evident events is the ==== class of events that one needs to check. The intuition behind the result is the following: A self-evident event is the event within which beliefs of agents are beliefs-closed (Footnote 7), i.e. every agent assigns probability ==== to the possibility that the other agents’ types lie in the set. This implies that outside the self-evident event, there exists type ==== of agent ==== to which another agent ====
 ascribes zero probability. If type ==== of agent ==== proposes a change from the status quo decision rule to an incentive compatible decision rule that makes himself better off without hurting the others, then agent ==== would come to know the proposer’s type immediately.====Our result, adapted to the original setup of HM that has a finite type space with a prior distribution, suggests that one may safely assume a strictly positive prior for the study of an incentive compatible and efficient decision rule. In that setup, every non-null subset of the type space is belief-closed. Particularly, a self-evident event corresponds to a subset in which every type profile is assigned a positive probability. Therefore, an incentive compatible and efficient decision rule is invariant even when one focuses on the subset whose elements occur with positive probabilities. In other words, one can just start from a finite type space equipped with a strictly positive prior. This is a rationale for assuming a strictly positive prior that HM’s result does not address.====This paper is organized as follows. In Section 2, we begin with a description of the economy with incomplete information. We also present the notions of common certainty and self-evident events, and in Section 3, by adapting them to the environment, we compare them with HM’s definition of common knowledge. In Section 4, we present our main result. Finally in Section 5, we conclude.",Common certainty and efficiency with incomplete information,https://www.sciencedirect.com/science/article/pii/S0165489616302025,11 June 2019,2019,Research Article,238.0
"Anchugina Nina,Ryan Matthew,Slinko Arkadii","Centre for Social Data Analytics, Auckland University of Technology, New Zealand,School of Economics, Auckland University of Technology, New Zealand,Department of Mathematics, University of Auckland, New Zealand","Received 12 July 2018, Revised 5 May 2019, Accepted 28 May 2019, Available online 3 June 2019, Version of Record 30 August 2019.",https://doi.org/10.1016/j.mathsocsci.2019.05.004,Cited by (3),It is well-known that for a group of ,"In the context of intertemporal choice, there are various situations which call for the use of ==== (weighted averages) of discount functions to evaluate streams of outcomes. Consider, for example, a utilitarian Social Planner aggregating the preferences of a group of discounted utility maximisers over a common consumption stream. If all members of the group share a common utility function but have heterogeneous discount functions, the utilitarian will maximise discounted utility using a discount function that is a mixture of the individual discount functions. Zuber (2011) demonstrates that a social welfare function is stationary and satisfies Pareto axiom if and only if individual discount functions are exponential with the same time preference rate and the social welfare function involves Paretian aggregation of those individual exponential discount functions. Jackson and Yariv (2015) similarly show that if each individual discount function is exponential, any social welfare function that respects Pareto efficiency ==== be utilitarian.==== ==== Mixtures of discount functions also arise when a Planner is uncertain about the appropriate discount factor to apply to distant future consumption streams, as in the analysis of Gollier and Weitzman (1998), Gollier and Weitzman (2010). Recent evidence from neuropsychology even suggests that individuals make intertemporal choices by aggregating different internal motives, leading to preference representations involving mixtures of discount functions (McClure et al., 2007).====It is therefore of interest to understand the properties of mixed discount functions. It is well known that these properties may differ in significant ways from those of the functions being mixed. Jackson and Yariv demonstrate (Jackson and Yariv, 2014 Proposition 1) that any non-trivial mixture of heterogeneous exponential discount functions will exhibit strictly decreasing impatience — whenever two dated outcomes have the same discounted utility (with respect to the mixed discount function), delaying both dated outcomes by the same amount of time will produce a strict preference for the later of the two.==== ==== Similar phenomena have been observed by several other authors, including, most recently, by Pennesi (2017, Theorem 1).====Related results may also be found in reliability theory, or survival analysis, where mixtures of survival functions are considered.==== ==== A survival function is the decumulative distribution function for a random “failure” time. Such functions therefore have similar mathematical properties to discount functions. Indeed, discounting is sometimes motivated by uncertainty about one’s time of death. When pooling data on the same “component” installed in different “machines”, (more generally, when considering the survival function for a population comprised of heterogeneous sub-populations), the survival function for a randomly selected component – the proportion of components expected to survive to a given time – is described by the average of the individual survival functions. Sozou (1998) observes that a continuous mixture of survival functions with the exponential form ====, in which ==== is uniformly distributed on ====, produces a hyperbolic function.==== ==== More generally, Barlow et al. (1963, Theorem 3.4) prove that any mixture of exponential survival functions will exhibit a decreasing failure rate, which is equivalent to a decreasing rate of time preference in the discounting context.==== ==== Gurland and Sethuraman (1995) even provide examples of survival functions with ==== failure rates whose mixtures exhibit failure rates that are decreasing.====There are also well-known results on the asymptotic behaviour of failure rates for mixtures of survival functions – exponential or otherwise – showing convergence to the lowest asymptotic failure rate amongst the functions being mixed (see Block et al. (2003, Corollary 2.1), which refines an earlier result of Block and Joe (1997)). Weitzman (1998) and others have demonstrated similar results for mixtures of discount functions.====A common theme emerges from these various contributions. Mixing of heterogeneous discount (or survival) functions enhances the tendency for impatience (or failure rates) to diminish over time. The intuition for this general tendency is easy to grasp by considering the case of survival functions. Let ==== be survival functions for the same component installed in ==== different machines, and let ====be the average of these functions. Then ==== is the probability that a randomly selected component survives to time ====. Its associated failure rate is ====It is straightforward to show that ====The square-bracketed term is the probability that a randomly selected component comes from machine ==== ==== ====. For functions with constant failure rates (i.e., exponential survival functions) it is obvious that the conditioning event will shift probability mass towards machines with lower failure rates, and that this tendency will only increase with ====. This type of “Bayesian” explanation for the behaviour of the mixture of exponential survival functions can be found in Barlow (1985).====A framework for articulating the general principle that lies behind these disparate results has been suggested by Prelec’s (2004). Prelec defines a “more decreasingly impatient (DI) than” relation on discount functions,==== ==== derived from an underlying property of intertemporal preferences. He also characterises this comparative property for the case of preferences with a discounted utility representation over dated outcomes (Fishburn and Rubinstein, 1982). He shows that the discount function ==== exhibits more decreasing impatience than ==== if and only if ==== is a convex transformation of ====. The analogous relation on survival functions had previously been proposed and studied by Lee (1981). If the transformation is ==== convex, we will say that ==== is “uniformly” more DI than ====. (This concept was not defined by Prelec but is introduced in the present paper.) Prelec (2004, Corollary 4) establishes that the mixture of two equally DI functions will be more (in fact, uniformly more) DI than either.==== ====The present paper generalises and extends Prelec’s analysis in several directions.====First, we extend Prelec’s result from two to ==== functions: we show that mixing ==== of equally decreasingly impatient functions yields a mixture that is uniformly more decreasingly impatient than each (Theorem 1). The key to this extension is the fact that the sum of log-convex functions is log-convex (Artin, 1964).====Second, we study mixtures of functions that can be weakly ordered by decreasing impatience but need not be ==== DI. The results for this case turn out to be more nuanced than one might expect. From the previous result we know that if all functions are equally DI, their mixture will uniformly dominate (in the sense of DI) each of them. It is therefore natural to suppose that the mixture of DI-ordered functions will uniformly dominate the least DI of the functions being mixed. We show that this intuition is false (Proposition 6) and establish necessary and sufficient conditions for the mixture to uniformly dominate the least DI component (Theorem 2).====Third, under additional smoothness assumptions, we are able to study mixtures of arbitrary discount functions. These functions may or may not exhibit decreasing impatience, and may or may not be weakly ordered by comparative decreasing impatience. Smoothness allows us to use the local index of decreasing impatience introduced by Prelec (2004). This is analogous to the Arrow–Pratt index of absolute risk aversion for expected utility. For smooth discount functions, one function is more decreasingly impatient than another if the index of the former weakly dominates the index of the latter (i.e., is weakly larger at every point in time). We show that the index of the mixed discount function always weakly dominates the ==== of the indices of the component functions, and we provide necessary and sufficient conditions for the index of the mixture to strictly exceed this minimum at any given point (Theorem 3).====In Section 4 we show that several results regarding the aggregation of discount or survival functions from decision theory and reliability theory follow as direct corollaries of our main theorems.====Before getting to our main results, the following section reviews the notions of absolute and comparative decreasing impatience. In Section 2.2 we recall Prelec’s  (2004) definitions of decreasing, and strictly decreasing, impatience for preferences over dated outcomes, and their characterisation for preferences with a discounted utility representation. In Section 2.3 we discuss Prelec’s (2004) “more decreasingly impatient than” relation, which is a natural comparative analogue to the decreasing impatience property, and introduce a “uniformly more decreasingly impatient than” relation, which provides a similar comparative analogue to the strictly decreasing impatience property. Appendix A.1 contains some basic facts about log-convex functions which will be needed for this discussion and the subsequent analysis.",Mixing discount functions: Implications for collective time preferences,https://www.sciencedirect.com/science/article/pii/S0165489619300393,3 June 2019,2019,Research Article,239.0
"Dubois Marc,Mussard Stéphane",", Université de Montpellier, France,, Université de Nîmes, France","Received 29 September 2018, Revised 28 February 2019, Accepted 13 May 2019, Available online 31 May 2019, Version of Record 19 June 2019.",https://doi.org/10.1016/j.mathsocsci.2019.05.001,Cited by (1),", are determined. The result relies on Bell polynomials and states that an income transfer principle of any order does not necessarily satisfy the utility transfer principle of the corresponding order.","At present, there is a consensus that individuals’ well-being should be the central object of interest for public policies. However redistributive policies might be applied to transferable resources – here referred to as income in a broad sense – that do not have a linear correspondence with individuals’ well-being. Therefore, the study of resource redistribution such as income transfers constitutes a non-trivial issue for analyses of well-being. Kaplow (2010) provides a framework for the interplay between the assumptions made on individual utility functions and the assumptions made on the social welfare functions (SWFs) for the purpose of income redistribution. This analysis is consistent with SWFs satisfying the Pigou–Dalton principle of income transfer, which states that a rich-to-poor income transfer improves social welfare. Kaplow shows that the hypotheses related to the utility function, being the same for all individuals in the society, play a direct role in fulfilling the Pigou–Dalton principle of income transfer. Moreover, the assumptions made on the shape of the SWF have a more subtle influence on such a fulfillment.====The Pigou–Dalton principle of utility transfer, as in Adler (2012), is among the axioms that help characterize the shape of an SWF. This principle requires that a utility transfer from a well-off agent to a less well-off one improves social welfare. Adler’s (2012) ethical view can be decomposed into two aspects. On the one hand, some assumptions on the utility function have to be made, and on the other hand, the Pigou–Dalton principle of utility transfer has to be imposed to ensure that the SWF is shaped by a strictly concave mapping of utilities. The first aspect aims at defining individuals’ well-being, which is formally expressed as a utility function based on idealized preferences. The question of whether utility should be determined by – empirically grounded – individuals’ ordinary preferences or by – normative – idealized preferences is related to this aspect.==== ==== The second aspect aims at determining the shape of the SWF. The continuous, additive form of the SWF (additive separability) and the Pigou–Dalton principle of utility transfer are the key characteristics of the shape of ==== SWFs supported by Adler (2012), that outline some priority to be imposed on worse-off agents.==== ==== Alternatively, any (pure) utilitarian SWF is designed by a linear transformation of utility so that the Pigou–Dalton principle of utility transfer is not respected.====This approach with two aspects allows comparing policy recommendations provided by prioritarian SWFs with those provided by utilitarian SWFs. The assumptions made on the utility function are independent of whether the social planner is utilitarian or prioritarian. In particular, as long as the utility function is assumed to be increasing and concave (====, utility is assumed to be derived from income at a positive and decreasing rate), both utilitarian and prioritarian social planners behave in accordance with the Pigou–Dalton principle of income transfer. When income has a concave correspondence with utility, the Pigou–Dalton principle of utility transfer is not equivalent to its corresponding principle of income transfer, because the former is not necessary for the latter to hold. This relation between these two principles is relevant to discussing the implications of second-order stochastic dominance in terms of income. The dominance criterion ensures that a pair of income distributions is ranked if one distribution can be obtained from the other through a sequence of income increments and/or rich-to-poor transfers, otherwise it is inconclusive.====Higher-order dominance criteria are able to rank income distributions in cases where the second-order dominance criterion fails to provide a ranking. Gayant and Le Pape (2017) explore third-, fourth-, and higher-order dominance criteria, and especially their normative content embodied by the generalized principle of income transfer ==== Fishburn and Willig (1984). The generalized principle of income transfer encompasses (in a recursive pattern): the second-order income transfer principle which is slightly weaker than the Pigou–Dalton condition, and the third-order income transfer principle which is similar in spirit to Kolm’s (1976) diminishing transfers principle. The latter states that a given progressive transfer is increasingly valuable insofar as the recipient is poorer. It is equivalent to saying that a rich-to-poor transfer between poorer individuals coupled with a poor-to-rich transfer between richer individuals improve social welfare. One step further, the fourth-order income transfer principle states that two rich-to-poor transfers – one between much poorer individuals and the other one between much richer individuals – coupled with two poor-to-rich transfers between individuals with intermediate incomes, improve social welfare. Higher-order transfer principles of income exhibit more sensitivity to transfers occurring in the lower tail of the income distribution. Gayant and Le Pape (2017) assert that these principles display an aversion to a general degree of income inequality.====In practice, numerous redistribution policies involve complex sequences of income transfers that are relevant to higher-order transfer principles. Let us take a simple example with two generations ==== and ==== whose income distributions are respectively ==== and ====. Every individual of generation ==== gives ==== unit of income to one individual of generation ====. Before redistribution, the aggregate distribution is ==== and after redistribution it becomes ====. The second-order dominance criterion cannot rank this pair of aggregate income distributions because the sequence of transfers needed to convert ==== into ==== involves two poor-to-rich transfers: ==== unit is given by the third individual to the fourth one, and ==== unit is given by the fifth individual to the sixth one. To analyze how a social planner would rank both aggregate income distributions, the fourth-order income transfer principle is necessary. Whenever the social planner respects the principle of transfer, and so refers to the fourth-order dominance criterion, the social planner judges the income redistribution to be welfare-improving.====In this paper, we compare (Fishburn and Willig’s 1984) income transfer principles with principles which follow the same pattern but rely on utility transfers. As with the Pigou–Dalton principle of utility transfer, the latter are axioms that help characterize the shape of an SWF. Alternatively, the income transfer principles result from the interplay between the assumptions on the utility function and the utility transfer principles. This raises two questions: (i) Which types of assumptions should we impose on the utility function so that the utility transfer principle of any given order implies the corresponding order of income transfer principle? (ii) Are the assumptions involved in an income transfer principle’s being of a certain order such that they also imply the utility transfer principle of the corresponding order? Two results are obtained. (i) Assuming that the utility function is increasing concave with higher-order derivatives alternating in sign up to a given order ==== provides a sufficient condition for the utility transfer principle of order ==== to imply the corresponding income transfer principle of order ====. (ii) An alternative characterization of the conditions under which additively separable SWFs satisfy income transfer principles ==== Fishburn and Willig (1984) is proposed. For any given order, a critical shape of SWF that corresponds to a maximum degree of convexity is determined such that the income transfer principle is satisfied, whereas SWFs beyond this shape fail to respect the principle. Such a critical shape is mathematically expressed with the aid of Bell polynomials, which are functions of the successive derivatives of the utility function. This expression epitomizes the complexity of the interplay between higher-order income and utility transfer principles. The assumptions underlying result (i) impose weaker conditions on the shape of the SWF than those ensuring the fulfillment of the utility transfer principle of order ====. Hence, under these assumptions, it is shown that SWFs satisfying a given order of income transfer principle do not necessarily satisfy the corresponding order of utility transfer principle. The results are particularly appropriate for providing ethical assessments in a risky framework, where individual well-being should be function of some attitude to risk. The study of the interplay between utility and income transfer principles of – at least – order ==== is necessary whatever the degree of risk under consideration in an intertemporal assessment. The results shed light on the implications of assumptions such as individuals’ risk aversion, prudence, temperance, and aversion to any given higher-degree risk (Ekern, 1980). These risk aversions are illustrated and discussed through the prism of income/utility transfers relevant to debates on water scarcity.====This paper is organized as follows. Section 2 presents the motivations, notation and definitions. Section 3 presents the main results and a table summarizing them. Section 4 presents an illustration of resource redistribution and water scarcity in a risky universe. Section 5 presents some conclusion.",Utility and income transfer principles: Interplay and incompatibility,https://www.sciencedirect.com/science/article/pii/S0165489619300368,31 May 2019,2019,Research Article,240.0
Grimmett Geoffrey R.,"Statistical Laboratory, Centre for Mathematical Sciences, Cambridge University, Wilberforce Road, Cambridge CB3 0WB, UK","Received 19 December 2018, Revised 7 March 2019, Accepted 14 May 2019, Available online 17 May 2019, Version of Record 3 June 2019.",https://doi.org/10.1016/j.mathsocsci.2019.05.003,Cited by (3),"We examine two aspects of the mathematical basis for two-tier voting systems, such as that of the Council of the European Union. These aspects concern the use of square-root weights and the choice of quota.==== and ====. These are in agreement when the underlying random variables are independent, but not generally. We review their possible implications for two-tier voting systems, especially in the context of the so-called collective bias model. We show that the two square-root laws invoked by Penrose are unified through the use of conditional influence.====In an elaboration of the square-root system, Słomczyński and Życzkowski have proposed an exact value for the quota ==== to be achieved in a successful vote of a two-tier system, and they have presented numerical and theoretical evidence in its support. We indicate some numerical and mathematical issues arising in the use of a Gaussian (or normal) approximation in this context, and we propose that other values of ==== may be as good if not better than ====. We discuss certain aspects of the relationship between theoreticians and politicians in the design of a two-tier voting system, and we reach the conclusion that the choice of quota in the square-root system is an issue for politicians informed by theory.",None,On influence and compromise in two-tier voting systems,https://www.sciencedirect.com/science/article/pii/S0165489619300381,17 May 2019,2019,Research Article,241.0
Sun Lan,"School of Economics and Management, Yunnan Normal University, Kunming, China,Center for Mathematical Economics, Universität Bielefeld, Bielefeld, Germany","Received 7 December 2017, Revised 8 January 2019, Accepted 13 May 2019, Available online 17 May 2019, Version of Record 29 May 2019.",https://doi.org/10.1016/j.mathsocsci.2019.05.002,Cited by (2),"In this paper, we propose a definition of ==== because of dynamic inconsistency. However, in the case in which player 2 only treats a zero-probability message as an unexpected news, an HTE is a refinement of sequential ====. In Milgrom and Roberts (1982) model of limit pricing, there exists a unique HTE for each interesting case, in addition, the HTE survives the Intuitive Criterion but not vice versa.","Ortoleva (2012) models an agent who does not update according to Bayes’ rule but instead “rationally” chooses a new prior among a set of priors when her original prior assigns a small probability to a realized event. He provides axiomatic foundations for his model in the form of a Hypothesis Testing representation theorem for suitably defined preferences. Both the testing threshold and the set of priors are subjective; therefore, an agent who follows this updating rule is aware of and can anticipate her updating behaviour when formulating plans.====Specifically, we consider the preferences of an agent over acts ==== that are functions from state space ==== to a set of consequences ====. If the preference relation is characterized by Dynamic Coherence in conjunction with other standard postulates, then the agent’s behaviour can be represented by a Hypothesis Testing model ====. According to this representation, the agent has a utility function ==== over consequences; a prior over priors ====; and a threshold ====. She then acts as follows: Before any information arrives, she has a set of priors ==== with probability assessment ==== over ====. She chooses ==== as her original prior, which is assigned the highest probability by ==== among all ====. Then, she forms her preference as the standard expected utility maximizer. As new information (an event) ==== is revealed, the agent evaluates the probability of the occurrence of the event as ====. She retains her original prior ==== and proceeds with Bayesian updating on ==== using ==== if the event ==== is anticipated, i.e., ====. However, if ====, she rejects her original prior ==== and searches for a new prior ==== among ==== such that ==== is the most likely one conditional on event ====, that is, ====, where ====Using this ====, she proceeds with Bayesian updating and forms her preference by maximizing expected utility.====Ortoleva (2012) applied his model in the “Beer–Quiche” game and discussed the Hypothesis Testing Equilibrium (HTE) when ==== for this specific game. In this example, there exists a unique HTE that coincides with the selection of the Intuitive Criterion (IC) of Cho and Kreps (1987). In the special case of ====, player 2 is still a Bayesian player since she changes her prior only if zero probability event occurs. Therefore, an HTE with ==== is a refinement of sequential Nash equilibrium. This paper develops the idea of nesting the hypothesis model into general signalling games with non-Bayesian players and proposes a general concept of HTE, where we allow the testing threshold ====. When ====, player 2 is a non-Bayesian player and the dynamic consistency condition is violated. Signalling games with non-Bayesian players play an important role across economic activities and beyond such as the expert persuader’s problem discussed in Galperti (2017). The persuader may send a surprising evidence which is assigned to a small probability ==== by the listener’s prior to change the listener’s worldview (her prior). We show that an HTE with ==== may differ from the sequential Nash equilibrium.====In our model, player 2’s belief in a signalling game updates as follows: Before player 1 moves, player 2 has a prior over a (finite) set of strategies that player 1 may use, and she determines the strategy that she believes player 1 will use most likely, which induces her original belief. After she observes a message sent by player 1, she evaluates the probability of the observed message using her original belief. She retains her original belief and uses it to proceed with Bayesian updating if the probability of the message she observed is greater than her testing threshold ====. However, if the probability is less than or equal to ====, she discards her original belief (she believes that player 1 may use a strategy other than her original conjecture). She then searches for a new belief that can be induced by another “rational” strategy by player 1 such that it is the most likely strategy conditional on the observed message. In a Hypothesis Testing equilibrium, the strategy of player 1 that induces player 2’s original belief coincides with the strategy that player 1 actually uses.====The difficulty concerns how to construct the set of player 2’s beliefs and how to assign a prior over the set of possible strategies. Here, we adapt the idea of Ortoleva (2012) to allow all beliefs that can be “rationalized” by at least one strategy of player 2 under consideration.====This paper is organized as follows. In the next section, I briefly recall the basic concepts and definitions from Ortoleva (2012) on the updating rule of the Hypothesis Testing model and the framework of general signalling games. Section 3 defines the general HTE and discusses its main properties. Section 4 analyses the HTE of Milgrom–Roberts’ limit pricing model in a finite framework, and Section 5 provides the conclusions and some remarks.",Hypothesis testing equilibrium in signalling games,https://www.sciencedirect.com/science/article/pii/S016548961930037X,17 May 2019,2019,Research Article,242.0
Chèze Guillaume,"Institut de Mathématiques de Toulouse, UMR 5219 Université de Toulouse, CNRS UPS IMT, F-31062 Toulouse Cedex 9, France","Received 17 December 2018, Revised 12 April 2019, Accepted 15 April 2019, Available online 15 May 2019, Version of Record 15 May 2019.",https://doi.org/10.1016/j.mathsocsci.2019.04.001,Cited by (3),"In this note we study a problem of fair division in the absence of full information. We give an algorithm that solves the following problem: ==== persons want to cut a cake into ==== shares so that each person will get at least ==== of the cake for his or her own measure, furthermore the preferences of one person are secret. How can we construct such shares? Our algorithm is a slight modification of the Even–Paz algorithm and allows to give a connected part to each agent. Moreover, the number of cuts used during the algorithm is optimal: ====.","Fair division is an old problem. The following situation already appears in the Bible. Two persons (in the Bible Abraham and Lot) want to share a land. Furthermore, this division must be fair. This means that each agent think that he or she has obtained at least ==== of the land for his or her own point of view. The following protocol, called “Cut and Choose”, is then used:====The first player cuts the land into two pieces with equal values for him or her. The second player chooses one of the two pieces.====With this strategy each player gets a connected piece with a value bigger than ==== for his or her point of view.====This protocol can also be used to divide a cake or a heterogeneous good as time or computer memory between two agents. How can we generalize this protocol to ==== agents?====Several answers exist. In order to describe them we need to state some definitions.====We consider a heterogeneous good, for example: a cake, represented by the interval ==== and ==== agents with subjective preferences. We associate to each agent a non-atomic probability measure ==== on the interval ====. These measures represent the utility functions of the agent. This means that if ==== is a part of the cake then ==== is the value associated to this part of the cake by the ====th agent. As ==== are probability measures, we have ==== for all ====.====A division of ==== is a partition ==== where each ==== is given to one agent. Thus there exists a permutation ==== such that ==== is associated to ====. A division is ==== when each ==== is an interval.====Several notions of fair division exist.====We say that a division is ==== when ====.====We say that a division is ==== when for ====, we have ====.====The problem of fair division (theoretical existence of fair division and construction of algorithms) has been studied in several papers (Steinhaus, 1948, Dubins and Spanier, 1961, Even and Paz, 1984, Edmonds and Pruhs, 2011, Brams and Taylor, 1995, Robertson and Webb, 1997, Aziz and Mackenzie, 2016), and books about this topic, see e.g. Robertson and Webb, 1998, Brams and Taylor, 1996, Procaccia, 2016, Barbanel, 2005. These results appear in the mathematics, economics, political science, artificial intelligence and computer science literature. Recently, the cake cutting problem has been studied intensively by computer scientists for solving resource allocation problems in multi agents systems, see e.g. Chevaleyre et al. (2006).====In this note we are going to study proportional fair divisions. This topic has been studied extensively. Several algorithms already exist, see e.g. Robertson and Webb (1998). In order to describe algorithms we thus need a model of computation. There exist two main classes of cake cutting algorithms: discrete and continuous protocols (also called moving knife methods). Here, we study only discrete algorithms. These kinds of algorithms can be described thanks to the classical model introduced by Robertson and Webb and formalized by Woeginger and Sgall in Woeginger and Sgall (2007). In this model we suppose that a mediator interacts with the agents. The mediator asks two type of queries: either cutting a piece with a given value, or evaluating a given piece. More precisely, the two types of queries allowed are:====We remark that in the “Cut and Choose” algorithm only these two kinds of queries are used.====In the Robertson–Webb model the mediator can adapt the queries from the previous answers given by the players. In this model, the complexity counts the finite number of queries necessary to get a fair division. For a rigorous description of this model consult: Woeginger and Sgall (2007).====An optimal algorithm for proportional fair division has been given by Even and Paz in Even and Paz (1984). When there are only two agents this algorithm corresponds to “Cut and Choose“. When there are ==== agents, this algorithm uses a recursive strategy and it is sometimes called “Divide and Conquer”. Some properties of this approach are studied in Brams et al. (2011).====In this note we are going to study one property of the “Cut and Choose” algorithm: The measure ==== is not used for the construction of the partition.====Indeed, a partition ==== is given and the second player chooses ==== or ====. Thus, with two agents, even if one player does not participate to the construction of the partition, we can get a proportional division. In this note, we are going to show that when there are ==== agents we can construct a proportional fair division with connected portions even if the measure of one agent is unknown. We call this agent the ====.====One application suggested by the article (Asada et al., 2018) is the following: During a birthday party with ==== guests and one host, a cake is divided into ==== pieces before it is presented to the birthday boy or girl, but he or she gets any portion. In this article, we give an algorithm which returns a partition assuring ==== of the cake to each persons (the ==== guests and the host) for his or her own measure.====More precisely we are going to prove:==== ==== ",How to share a cake with a secret agent,https://www.sciencedirect.com/science/article/pii/S0165489619300290,15 May 2019,2019,Research Article,243.0
Masiliūnas Aidas,"Global Asia Institute, National University of Singapore, 10 Lower Kent Ridge Road, Singapore 119076, Singapore","Received 28 July 2017, Revised 6 March 2019, Accepted 29 March 2019, Available online 12 April 2019, Version of Record 29 April 2019.",https://doi.org/10.1016/j.mathsocsci.2019.03.005,Cited by (1)," of the strategies chosen by sophisticated players. Our model predicts that no players would switch from the efficient to the inefficient action, but deviations in the other direction are possible. Three types of equilibria may exist: in the first type lock-in is sustained, while in the other two types lock-in is overcome. We determine the existence conditions for each of these equilibria and show that the equilibria in which lock-in is overcome are more likely and the transition is faster when sophisticated players have a longer planning horizon, or when the history of inefficient coordination is shorter.","Research in natural and social sciences identified the presence of alternative stable states in many important settings.==== ==== The main problem in these situations is the emergence of an inefficient state,==== ==== or “inefficient lock-in”, from which no individual has incentives to deviate. Lock-in has been identified as the primary cause of inefficient social customs (Akerlof, 1980), inefficient economic and political institutions (North, 1990) and inefficient technologies (Shapiro and Varian, 1999, Cowan, 1990, David, 1985). For example, employees might tolerate misconduct, overwork or use inefficient procedures and technologies as long as sufficiently many others engage in such behavior. Lock-in might be prevented, but policy makers often deal with situations in which lock-in has already occurred, and it is important to know how it can be overcome. This paper studies whether lock-in can be endogenously overcome in a population of sophisticated and myopic agents and identifies how the likelihood and the speed of such transitions depend on the parameters of the game.====No previous literature has addressed the question of whether the presence of multiple sophisticated players can help overcome inefficient lock-in. Game theory can make predictions about whether lock-in will occur (Harsanyi and Selten, 1988, Kandori et al., 1993, Young, 1993), but not whether it can be overcome, because the predictions of standard solution concepts are invariant to the history of play. The question of how to anticipate and avoid or facilitate transitions between states has been studied in other disciplines,==== ==== often using models with adaptive agents from complexity science (Gao et al., 2016, Battiston et al., 2016). However, these models cannot predict whether lock-in will persist if agents are strategic and farsighted, as they are in human societies.====We are able to model and study lock-in through the interaction of two types of players, “myopic“ and “sophisticated”. Myopic players use adaptive learning: they form beliefs about the actions of other players based on observed history and choose the myopic best-response. Inefficient lock-in is modeled through the beliefs of myopic players, who experienced a history of inefficient coordination. Belief-based learning by myopic players creates incentives for sophisticated players to use “strategic teaching”, that is to deviate from the inefficient state to induce a future deviation by myopic players. Sophisticated players anticipate how their actions will affect subsequent beliefs and actions of myopic players, and our solution concept requires the choice path of sophisticated players to be optimal given the choices of all other players. A combination of game-theoretic reasoning and adaptive learning generates predictions about the occurrence and speed of transitions between stable states, in contrast to models with only strategic or only adaptive players, which cannot make such predictions.==== ==== We show that three types of outcomes are possible on the equilibrium path: sophisticated players might deviate from the inefficient state immediately, after a delay, or they might never deviate. No player ever deviates from the efficient state, therefore inefficient lock-in is overcome in the first two cases, but not in the third. We specify the existence conditions and the speed of transition for each equilibrium type, obtaining testable predictions about the behavioral patterns==== ==== and about the comparative statics.==== ====Models with adaptive and sophisticated players have been used in the literature to address issues other than lock-in. Camerer et al., 2002, Chong et al., 2006 use a sophisticated experience-weighted attraction (EWA) model in which some players are adaptive (as in Camerer and Ho, 1999), while others are sophisticated, anticipate the behavior of adaptive players and use strategic teaching. Sophisticated EWA can be used to explain why sophisticated borrowers repay loans to adaptive lenders in early periods to secure loans in later periods of a repeated trust game. A simplified version of sophisticated EWA is used by Brandts et al. (2016) to explain why players choose to lower the effort cost of other group members in a minimum effort game following coordination failure. The learning model in Brandts et al. (2016) assumes that unsophisticated types best-respond to the distribution of actions observed in the previous round, while sophisticated types expect all others to be unsophisticated, therefore efficient coordination in future rounds can be facilitated by lowering the effort cost of other group members. Hyndman et al. (2009) explain behavior in two-player coordination games using a model that combines adaptive players, who follow weighted fictitious play, with farsighted players, who anticipate the learning process and maximize the discounted sum of expected payoffs.==== ====Our model rests on the concept of strategic teaching, just as the sophisticated EWA and other models discussed above, but we use a different approach for a different purpose and therefore make three important contributions to the literature. First, we use strategic teaching to refine rather than alter the predictions of standard solution concepts. Strategic teaching has been previously used to explain deviations from the Nash equilibrium (e.g. repaying loans in early periods, as in Camerer et al., 2002, Chong et al., 2006; or helping others at a cost to oneself, Brandts et al., 2016). In the setting we study, standard solution concepts generate vacuous predictions, therefore we use strategic teaching to refine the paths of play that can be supported in equilibrium. As a result, we develop the first game-theoretic solution concept that can make predictions about the likelihood and timing of transitions between stable states. Second, we address the problem of strategic uncertainty, devising a method to apply models of strategic teaching to games with multiple strategic agents. Previously, the scope of applications was limited because the existing models bypassed the problem of strategic uncertainty by assuming that sophisticated players expect all opponents to be myopic.==== ==== We show that the issue of strategic uncertainty can be resolved using standard game-theoretic tools, resulting in a model that is both more flexible and more realistic, and therefore could potentially fit data better. Third, the models in the previous literature can explain experimental data only ex-post, while we make testable ex-ante predictions. The characterization of the solution concept allows us to study its properties, such as the speed of convergence, and the conditions under which each equilibrium type exists, generating novel testable predictions. In the previous literature, ex-ante predictions could be obtained only using simulations with specific parameter values (e.g. Chong et al., 2006).====Compared to the previous models of strategic teaching, we make different assumptions about the behavior of the two types of players, because stronger assumptions are needed to obtain a tractable model. First, we characterize the sophisticated player equilibrium only for critical mass games. Our intention is not to devise a solution concept that explains behavioral regularities in different games (although it could be characterized in other games), but rather a model that can explain how and when inefficient lock-in can be overcome. Second, we assume that myopic players update beliefs using weighted fictitious play (just as in Hyndman et al., 2009, Ellison, 1997) instead of EWA (Camerer et al., 2002). We use a belief-based updating rule because under the assumptions made here, sophisticated players cannot directly influence the payoffs of the myopic players; therefore, if myopic players used a payoff-based learning rule (such as reinforcement learning, Erev and Roth, 1998), strategic teaching would have no effect. It has also been found that belief-based models can explain behavior in experiments with coordination games,==== ==== and they typically fit better than other models (Ho and Weigelt, 1996, Battalio et al., 1998). Weighted fictitious play is sufficiently rich to accurately explain actions and beliefs in critical mass game experiments (Masiliūnas, 2017), but it is also sufficiently simple, allowing us to set up a tractable model of strategic teaching. Other updating rules might be more attractive from a descriptive viewpoint (such as EWA, Camerer and Ho, 1999) or a normative viewpoint (standard Bayesian updating using a Dirichlet prior), but they could not be used to characterize equilibria with the methods used in this study.====Other papers study the interaction between farsighted and myopic players using different methods and in different games, and are therefore less related to our study than the strategic teaching literature. Ellison (1997) models a population of adaptive players, who learn according to fictitious play and are repeatedly paired to play a binary choice coordination game. Adding one rational player to the population of adaptive players can change the outcome from coordination on the inefficient equilibrium to coordination on the efficient one, as long as the number of players is fixed and the rational player is sufficiently patient. Acemoglu and Jackson (2014) develop an overlapping generations model that shows how a social norm of low cooperation can be overturned by a single forward-looking player. Schipper (2019) uses an optimal control model with two players and shows how a strategic player can control an adaptive player in repeated games with strategic substitutes or strategic complements. Mengel (2014) studies adaptive players who are also forward-looking and finds that in two-player coordination games the efficient equilibrium may be stochastically stable, in contrast to the case with only adaptive players. Models studied in these papers do not deal with strategic uncertainty, and therefore could not be applied to games with multiple sophisticated players.====The rest of the paper is organized as follows. Section 2 provides a general definition of our solution concept, the “sophisticated player equilibrium”. Section 3 constructs the characterization of this solution concept in a critical mass game, which is defined in Section 3.1. Section 3.2 specifies the behavior of myopic players, conditional on the observed history. Section 3.3 refines the behavior of sophisticated players, under the assumptions that are discussed in Section 3.4. Section 3.5 shows that the efficient state is absorbing, and is implemented when myopic players deviate from the inefficient state. Section 3.6 specifies this switching time of myopic players, as well as the payoffs received by sophisticated players. Section 3.7 characterizes the three types of equilibria. Section 3.8 illustrates the theoretical results using a numerical example. Section 3.9 shows how the speed of transition and the types of equilibria that exist respond to changes in game parameters. Section 4 concludes.",Overcoming inefficient lock-in in coordination games with sophisticated and myopic players,https://www.sciencedirect.com/science/article/pii/S0165489619300289,12 April 2019,2019,Research Article,244.0
"Galaabaatar Tsogbadral,Uyanık Metin","Department of Economics, Ryerson University, Toronto, ON M5B2K3, Canada,Department of Economics, Johns Hopkins University, Baltimore, MD 21218, United States,School of Economics, University of Queensland, Brisbane, QLD 4072, Australia","Received 4 September 2018, Revised 22 March 2019, Accepted 29 March 2019, Available online 8 April 2019, Version of Record 22 April 2019.",https://doi.org/10.1016/j.mathsocsci.2019.03.004,Cited by (11),"This paper contributes to the interplay of the behavioral assumptions on a ==== and the structure of the choice space on which it is defined: the (ES) research program of Eilenberg (1941) and Sonnenschein (1965) to which Schmeidler (1971) is an especially influential contribution. We show that the presence of the ==== and mixture-continuity properties, both empirically non-falsifiable in principle, foreclose the possibility of consistency (transitivity) without decisiveness (completeness), or decisiveness without consistency, or in the presence of a weak consistency condition, both indecisiveness and inconsistency altogether. Second, we delineate how semi-transitivity of a relation is already hidden in linearity assumptions; and third, offer sufficient conditions that yield an ==== that reduces a general setting to the usual order on an interval, and thereby yields the classic theorem of Herstein–Milnor (1953). We remark on extensions to a generalized mixture set of Chipman–Fishburn.","Khan–Uyanık (2017), henceforth KU, highlight what they refer to as the Eilenberg–Sonnenschein (ES) research program, and present a comprehensive treatment of the two-way relationship between the properties of a binary relation and of the set over which the relation is defined. Confining themselves exclusively to the topological register, they develop a general theory of completeness and transitivity of a continuous and ====-non-trivial (==== any natural number) binary relation defined on a ====-connected topological space. As is well-known, the transitivity postulate in the presence of completeness was studied in Eilenberg (1941) and Sonnenschein (1965), and completeness in the presence of transitivity, by Schmeidler (1971), but all only for ====. KU also broaden the theory to include a sufficiency result, based on the work of Sen (1969), that delineates the consequences of ====-connected (connected) choice sets for transitivity alone of a given continuous binary relation. All this notwithstanding, the fact remains that the bulk of modern decision theory, in of itself, and in terms of applications to mathematical economics and mathematical social sciences more generally, does not confine itself solely to the topological register. It is therefore natural to ask for the added consequences that can be obtained by supplementing it with the algebraic one. However, this is not the question we ask here.====Rather than a supplementation, we inquire into the reformulation of the ES program for the setting considered in the pioneering work of von Neumann–Morgenstern (1947) and Marschak (1950), and one given a definitive treatment in Herstein–Milnor (1953). There, as is well-understood, it is not so much a question of supplementing topology by algebra as it is of working with a novel mathematical setting of a mixture space, a setting that is endowed with a most minimal topological requirement. In particular, instead of a topology being assumed on the space of objects, one simply utilizes the standard Euclidean unit interval to combine the objects.==== ==== This consequence of working with a generalization of a linear to a mixture set leads, as a necessary concomitant, to the fact that the results in KU, assuming as they do a topology on the choice set, have no direct application to this alternative set-up. New mathematical argumentation along with its associated techniques is required.==== ==== Even if the choice set is taken to be the simplex, endowed with both topological and algebraic structures, their results cannot be directly used and applied. Furthermore, given that the “topological action” involves only the unit interval, the question of working with any richer concept of connectedness and its natural extensions is excluded by default. And therefore, rather than going ==== from behavioral properties of preference relations to deduce properties of the topology on the choice set, we can only go ==== to deduce consequences for behavior of the continuity properties of the given relation, continuity now being formalized by the Archimedean property and by mixture-continuity, both rooted in the unit interval.==== ====On moving beyond broad methodological remarks, this paper contributes to a particular strain within the ES program. This is the work of Dubra (2011), Karni–Safra (2015) and McCarthy–Mikkola (2018). Its trajectory can be simply laid out. Dubra, in his specialization of the choice set to the space of lotteries on a set of finite prizes, showed how mixture-continuity and the Archimedean property, together with the independence axiom, yield Schmeidler’s conclusion that a non-trivial, reflexive and transitive relation is necessarily complete. Karni–Safra underscore the thrust of Dubra’s contribution by relaxing independence to the “betweenness” property or the “cone-monotonicity” property, and again, like him, by appealing to Schmeidler’s theorem to obtain completeness of the given relation. McCarthy–Mikkola revert to the original Dubra setting with the independence axiom in full operation, and generalize Dubra’s result to a convex set in a linear space that is not limited to be finite-dimensional. We apply Occam’s razor, and show that these particular results obtain without any linearity assumption on preferences! In particular, all of the results of Dubra, Karni–Safra and McCarthy–Mikkola follow as corollaries of the results reported here.==== ====But this is perhaps not the primary contribution of the paper. It is rather to show that once the question is set within the broad outlines of the ==== direction of the ES program, a worthwhile tripartite contribution to mixture sets, as formulated and studied in Herstein and Milnor (1953), naturally follows, and takes it place squarely within the research program. First, we show that under semi-transitivity and transitivity of the symmetric part of a reflexive and non-trivial binary relation, mixture-continuity and the Archimedean property yield ==== completeness and transitivity of the relation. The two properties are bundled conclusions: under the assumed hypotheses, one property cannot be obtained without the other. Any agent cannot be consistent without being decisive, or decisive without being consistent.==== ==== Second, we present a result (Theorem 2) and its two corollaries that concern only the transitivity postulate and its various relaxations as in Sen (1969).==== ==== Furthermore, under the completeness postulate, we show that the transitivity of the asymmetric part of a mixture-continuous binary relation, and a convexity property, are sufficient for transitivity. Third, we present an isomorphism theorem that reduces a general setting to the usual order on an interval, and thereby yields the classic theorem of Herstein–Milnor. In what may be the most surprising and anti-climactic finding, we show (in Theorem 3 ) that if the preference relation is “very nice”, the model essentially collapses to a situation where the standard greater-than-or-equal-to relation on a unit interval is being investigated. The consequence of this for the Herstein–Milnor representation theorem are unmistakable, and we are thereby led directly to an alternative proof of their result.==== ==== Finally, we remark in Section 4 how most of our results can be extended to generalized mixture sets as formulated and studied in Fishburn (1964), and Chipman (1971). In sum, it is in this bundling of the results of Eilenberg, Sonnenschein, Sen and Schmeidler, Herstein–Milnor serving as an important subtext, that each individual contribution is mutually illuminated and allows a maturer theory.====But mature or otherwise, the question remains as to what precisely these theorems offer in terms of the antecedent literature. How can the theory be applied? We have already referred to the work of Dubra, Karni, Safra, McCarthy and Mikkola (henceforth DKSMM), but it is Section 5 that we attempt a more careful reading and systematic discussion of the literature with these theorems in pure theory in hand. We do so under the criterial rubric of ==== and ==== on the one hand, and of ==== and ==== on the other. We have already mentioned the taking of Occam’s razor to the theorems and of removing ==== in them; the criteria of ==== is only a little less straightforward — the Malinvaud–Samuelson exposing of the independence axiom in von Neumann–Morgenstern (1947) being the archetypical example.==== ==== As regards the other two criteria, they are robustness criteria inspired by Gerasimou (2013), and perhaps ought to be seen as further elaboration of the incorporated ==== criterion. In any case, we turn to their formal explication and discussion in Section 5.",Completeness and transitivity of preferences on mixture sets,https://www.sciencedirect.com/science/article/pii/S0165489619300277,May 2019,2019,Research Article,245.0
"Bergantiños G.,Navarro-Ramos A.","Economics, Society and Territory, Facultad de Economía, Campus Lagoas-Marcosende, s/n, Universidade de Vigo, Vigo, Pontevedra, Spain","Received 6 September 2018, Revised 1 February 2019, Accepted 19 March 2019, Available online 27 March 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.mathsocsci.2019.03.003,Cited by (5), rule based on a painting procedure. Agents paint the edges on the paths connecting them to the sources. We prove that the painting rule coincides with the folk rule.,"We study situations where a group of agents need services provided by several sources. Agents need to be connected, directly or indirectly, to all sources. Every connection is costly. Situations of this kind are called minimum cost spanning tree problems with multiple sources and are extensions of the classical minimum cost spanning tree problem (where there is a single source).====The first issue addressed is to find the least costly networks connecting all agents with all sources. Obviously, such a network is a tree. It can also be found in polynomial time using the same algorithms as in the classical problem (e.g., Kruskal (1956) and Prim (1957)).====The second issue addressed is how to allocate the cost of the tree obtained among the agents. Several papers have studied this issue in minimum cost spanning tree problems, but as far as we know only three have considered it in the case of multiple sources. Rosenthal (1987) and Kuipers (1997) study a situation slightly different from this paper, whereas Bergantiños et al. (2017) study the same situation as we present here. Rosenthal (1987) considers situations where all sources provide the same service and agents want to be connected to at least one of them. He considers a cooperative game and studies the core of that game. Kuipers (1997) considers situations where each source offers a different service and each agent needs to be connected to a subset of the sources. He also considers a cooperative game and seeks to determine under what conditions the core is non-empty. Bergantiños et al. (2017) study the same situation as in this paper. They extend different definitions of the folk rule, defined for classical minimum cost spanning tree problems, to the case of multiple sources. They also present some axiomatic characterizations of the folk rule.====In classical minimum cost spanning tree problems the folk rule is one of the most important rules. It has been studied in several papers, including Bergantiños and Kar (2010), Bergantiños et al., 2010, Bergantiños et al., 2011, Bergantiños et al., 2014, Bergantiños and Vidal-Puga, 2007, Bergantiños and Vidal-Puga, 2009, Branzei et al. (2004), and Tijs et al. (2006).====Our paper is closely related to that of Bergantiños et al. (2014). They study a general framework of connection problems involving a single source, which contains classical minimum cost spanning tree problems. They propose a cost allocation rule, called the ==== because it can be interpreted through a painting story. The idea is the following: start with a tree ====; for each agent, identify the unique path in ==== from that agent to the source. Agents start painting the first edge on that path. Following a protocol, an agent continues painting until all edges on her path have been painted. They also give some axiomatic characterizations of the painting rule. They prove that the painting rule coincides with the folk rule in classical minimal cost spanning tree problem. Thus, they obtain a new way of computing the folk rule and a new axiomatic characterization.====The objective of this paper is to extend the definition of the painting rule to the case of minimum cost spanning tree problems with multiple sources. The main problem that arises when doing this is that given a tree and an agent, several paths in the tree could connect the agent to a source. In order to avoid this problem, we define a two-phase procedure: In Phase 1, given a tree ====, we compute a tree ==== with the same cost as ==== such that ==== is also a tree when it is restricted to the set of sources. Notice that for each agent there is a unique path in ==== connecting the agent with the set of all sources. In Phase 2 we apply the ideas of the painting rule to the tree ====. This extension of the painting rule is not straightforward because it could depend on the tree ==== considered initially and the tree ==== computed in Phase 1, which is not determined solely by ====. In Proposition 2 we prove that for each tree ==== and ==== considered, the painting rule always coincides with the folk rule. Thus, the painting rule is independent of the trees ==== and ==== considered.====The paper is organized as follows. Section 2 introduces minimum cost spanning tree problems with multiple sources. Section 3 introduces the painting rule.",The folk rule through a painting procedure for minimum cost spanning tree problems with multiple sources,https://www.sciencedirect.com/science/article/pii/S0165489619300265,May 2019,2019,Research Article,246.0
"Kasper Laura,Peters Hans,Vermeulen Dries","Maastricht University, Netherlands,Saarland University, Germany","Received 17 January 2019, Revised 14 March 2019, Accepted 16 March 2019, Available online 25 March 2019, Version of Record 5 April 2019.",https://doi.org/10.1016/j.mathsocsci.2019.03.002,Cited by (7),"We identify the maximal voting correspondence which is ==== Consistent and satisfies two participation conditions, namely the Top Property and the Bottom Property — thereby extending a result in Pérez (2001). The former participation condition says that if an alternative is in the chosen set at a profile of rankings and a ranking is added with that alternative on top, then it remains to be a member of the chosen set. The latter says that if an alternative is not in the chosen set at a profile of rankings and a ranking is added with that alternative at bottom, then the alternative is again not in the chosen set. In particular, voting functions (single-valued voting correspondences) with these three properties select from this maximal correspondence, and we demonstrate several ways in which this can or cannot be done.","Within a democratic voting system it should be in the own interest of voters not to abstain from voting. Abstaining from voting, however, can be rational even in voting systems that are often regarded as truly democratic. An example is the Dutch referendum for the EU Association Treaty with the Ukraine on April 6, 2016. A majority voted against this treaty, but the total voter turnout was about 32%, which is hardly above the minimally required 30%: if some of the ‘yes’ voters would have stayed at home then the referendum would have been invalid and the ‘no’ to the treaty avoided.====Another natural and desirable property of a voting system is Condorcet Consistency (de Condorcet, 1785): if there is an alternative ==== (candidate, political party, law, etc.) such that for each other alternative ==== there is a majority of the voters ranking ==== above ====, then ==== should be chosen. Also this property is often violated. Again in the Netherlands, van Deemen and Vergunst (1998) provide evidence that D66 (a progressive liberal party) may have been the Condorcet winner in several consecutive Dutch elections for Parliament, but it never won those elections and, consequently, could never take the lead in the formation of a new government.====In this paper we study voting correspondences which combine the two desirable conditions above: they should be Condorcet Consistent and they should not admit situations where voters might be better off by abstaining. The latter will mean, more precisely, that it should never happen that an additional voter, by ranking an alternative in the chosen set first, makes that this alternative is no longer in the chosen set; or, by ranking a non-chosen alternative last, turns this into an alternative in the chosen set. In the two-alternative referendum on the Ukraine Association treaty mentioned above both these situations occurred. In Nurmi (2002) and Pérez (2001) these phenomena are referred to as the positive and negative strong no show paradoxes, respectively. Thus, we are looking for Condorcet Consistent voting correspondences that avoid both these paradoxes, and we call the avoidance of these paradoxes the Top Property and the Bottom Property, respectively.====As a general motivation for considering the Top en Bottom Properties, think of a social choice correspondence as selecting those alternatives that are acceptable given a certain profile of rankings. If an additional voter ranks one of these alternatives first, then it should a fortiori be acceptable. A similar argument can be made for the Bottom Property.====A main result in the paper is the identification of the maximal Condorcet Consistent voting correspondence ==== that satisfies the Top Property, that is: each Condorcet Consistent voting correspondence satisfying the Top Property must be contained in ==== and, ====, ==== is the union of all smaller voting correspondences with the two properties. We also show that this result remains true if we add the Bottom Property, but not if we replace the Top Property by the Bottom Property. The Minimax Rule (cf. Black, 1958, Simpson, 1969, Kramer, 1977) in particular is contained in ====. We exhibit several ways in which voting functions (single-valued voting correspondences) can select from ==== while maintaining the Top and/or Bottom Property.====Our main result is closely related to a result in Pérez (2001), which already identifies ==== as containing each Condorcet Consistent voting correspondence that satisfies the Top Property (called Positive Involvement in Pérez, 2001); or the Bottom Property (called Negative Involvement) and an additional condition. Our terminology is closer to that in Felsenthal and Nurmi (2016), who investigate a number of well-known voting correspondences with respect to the occurrence of the positive and negative strong no show paradoxes. See also Felsenthal and Tideman (2013).====The Bottom Property is sufficient to rule out the no show paradox as formulated in Brams and Fishburn (1983). Moulin (1988) shows that if there are at least four alternatives and at least twenty-five voters, no Condorcet Consistent voting function can satisfy a condition called ‘participation’: this condition requires that no voter can be worse off by voting than by abstention. The participation condition rules out a no show paradox that is weaker (hence, occurs more often) than the combination of the two strong versions that we consider and, consequently, the participation condition in Moulin (1988) is stronger than the combination of the Top Property and the Bottom Property.====When studying voting correspondences, one can extend the rankings of a voter from alternatives to sets. This approach is considered by Jimeno et al. (2009) and Brandt et al. (2017). In particular, their optimistic and pessimistic extensions are somewhat related in spirit to the Top and Bottom Property, respectively, but they consider the strong participation requirement as in Moulin (1988). See, further, Duddy (2014) and Núñez and Sanver (2017). The latter paper considers the relation between monotonicity (if a chosen alternative shifts up in a ranking, the rest remaining the same, then it remains to be chosen) and the no-show paradox. Duddy (2014) allows weak preference rankings and shows that then every Condorcet consistent voting function violates both the Top Property and the Bottom Property if there are at least four alternatives.====The paper is organized as follows. In Section 2 we provide basic concepts and definitions, and in Section 3 we consider voting correspondences. The main results are summarized in Corollary 3.6, in particular establishing correspondence ==== as the maximal correspondence satisfying Condorcet Consistency, the Top Property and the Bottom Property. In Section 4 we consider voting functions which select from ====. We may, in particular, choose from the Minimax Rule according to a fixed tie-breaking ranking, but this result does not extend to ====. Section 5 concludes. Some of the proofs are relegated to the Appendix.",Condorcet Consistency and the strong no show paradoxes,https://www.sciencedirect.com/science/article/pii/S0165489619300253,May 2019,2019,Research Article,247.0
"Grandi Umberto,Hughes Daniel,Rossi Francesca,Slinko Arkadii","IRIT, University of Toulouse, France,Department of Mathematics, The University of Auckland, New Zealand,Department of Mathematics, University of Padova, Italy","Received 6 April 2018, Revised 5 February 2019, Accepted 7 March 2019, Available online 19 March 2019, Version of Record 29 March 2019.",https://doi.org/10.1016/j.mathsocsci.2019.03.001,Cited by (6),"The Gibbard–Satterthwaite theorem states that for any non-dictatorial voting system there will exist an election where a voter, called a manipulator, can change the election outcome in their favour by voting strategically. When a given preference profile admits several manipulators, voting becomes a game played by these voters, who have to reason strategically about each other’s actions. To complicate the game even further, some voters, called countermanipulators, may try to counteract potential actions of manipulators. Previously, voting manipulation games have been studied mostly for the Plurality rule. We extend this to ","Voting is a common method of preference aggregation, which enables the participating agents to identify the best candidate given the individual agents’ rankings of the candidates. However, no “reasonable” voting rule is immune to manipulation: as shown by Gibbard (1973) and Satterthwaite (1975), if there are at least three candidates, then any onto, non-dictatorial voting rule admits a preference profile (a collection of voters’ rankings) where some voter would be better off by submitting a ranking that differs from his truthful one or, in other words, his truthful vote is not the best response to the votes of other voters. We call such voter a Gibbard–Satterthwaite manipulator or GS-manipulator for short. When such a manipulator is unique, he==== ==== then has a disproportional influence on the election outcome. However, in the presence of multiple manipulators, their attempt to manipulate the election simultaneously in an uncoordinated fashion (and we assume that no coordination devices exist) may result in an outcome that differs not just from the outcome under truthful voting, but also from the outcome that any of the GS-manipulators could anticipate. This may be due to possible complex interference among the different manipulative votes, and may deter some of the voters (especially risk-averse ones) from manipulating. When we also include in consideration those voters who cannot manipulate themselves but can prevent others from manipulating – the so-called countermanipulators – the situation becomes even more complex and can be described only in game-theoretic terms. Let us illustrate the type of situations on which we focus by an example.====As shown in the above example, even for such a simple voting rule as Plurality, a single profile can give us a number of different games depending on which voters are strategic and which are not. A non-strategic voter has only his sincere vote in his strategy set, while a strategic voter has more than one strategy. In this paper we are interested in the properties of the normal-form games that arise under ====-Approval voting rules (and Plurality corresponds to 1-Approval). These rules are simple enough to allow for classification of voting manipulations, but complex enough to admit realisations of non-trivial games.====As is usually the case in any initial investigation, it is customary to assume the full information framework, in which everybody’s sincere preferences are publicly known as well as their strategy sets. However, the voting intentions of the voters remain private to those voters.====An important novel feature of the class of games considered in this paper, which distinguishes them from voting games that have been considered in prior literature (see Section 2 for related literature survey), is the introduction of types of players which are characterised by their strategy sets. Since our framework is fully ordinal, these strategy sets do not have randomised strategies. There are several reasons for the introduction of types. Firstly, by reducing players’ strategy sets we can model their bounded rationality. Secondly, the knowledge of the sincere profile does not allow to unambiguously decide who is strategic and who is not. Voters may be able to manipulate but reject this on moral grounds, or they may be unable to calculate their manipulation. On the other hand, a voter may not be able to manipulate but can take preventive measures from a disastrous effect of someone else’s manipulation (such as voter 4 in Example 1). The introduction of “personalised” strategy sets allows us to bring into a spotlight and to study in isolation various aspects of strategic manipulation, e.g., the interaction of Gibbard–Satterthwaite manipulators (e.g., voters 1 and 2 in Example 1) or the interaction of a manipulator and a countermanipulator.====Given a voting rule, every profile of voters’ preferences gives rise to a number of games that can be played; we will call them ====. We treat the strategy sets as a state of nature whose move makes the structure of the game a public knowledge. One of the most interesting cases is when only GS-manipulators are strategic (no countermanipulators exist), and such games will be called ==== or ====. This assumption, among other things, means that voters vote sincerely if they cannot change the result of the election, a similar assumption to the truth-bias of Obraztsova et al. (2013).====The purpose of our work is to investigate the game-theoretic complexity of voting manipulation games, showing that manipulation in practice might be complicated by, e.g., the absence of Nash equilibria, or the existence of multiple equilibria, which in the absence of coordination mechanisms is a source of strategic complexity. The simplest non-trivial example of our framework involves two players each having two strategies: one sincere and one insincere, we call them ====-by-==== games. They can be both GS-manipulators or, alternatively, one can be a GS-manipulator and another a countermanipulator. Observe that this does not imply that the election from which such games arise has two voters only, but simply only two voters at the given profile are strategic. We begin by answering the question: which ====-by-==== games can be represented as voting manipulation games? As existing classifications of games turn out to be too fine-grained for our purposes, we develop a simple coarser classification, and observe that the definition of GS-games imposes certain restrictions on players’ strategies. Combining this observation with symmetry arguments, we arrive at six basic types of ====-by-==== games that may be encountered by two manipulators. We then show that, while all six games can be obtained as GS-games under the ====-Approval voting rule, for the Plurality rule (i.e., 1-Approval) only four of them are realisable. In the same spirit we also obtain a classification of ====-by-==== manipulator and countermanipulator games.====Since in the presence of countermanipulators a pure Nash equilibrium may not exist even in a very simple ====-by-==== game, we bring under the spotlight the situation when all players are GS-manipulators. We study the existence of pure strategy Nash equilibria in such games. We show that every GS-game for Plurality has a Nash equilibrium, and identify necessary and sufficient conditions for the existence of Nash equilibria for ====-Approval games. It appears that a mild rationality condition which we call soundness assumption is necessary and sufficient. We also find sufficient conditions for the existence of Nash equilibria of ====-Approval games, assuming that manipulating voters choose manipulation strategies which are in some sense minimal. However, we show that this minimality assumption fails to ensure the existence of Nash equilibria for ====-Approval games.====The paper is organised as follows. In Section 2 we discuss related work, and in Section 3 we introduce voting manipulation games. The main contributions of the paper are presented in Section 4, in which we classify 2-by-2 voting manipulation games, and in Section 5, where we study the existence of Nash equilibria in arbitrarily GS-games for ====-Approval voting rules. Section 6 provides a discussion of the results presented and Section 7 concludes the paper.",Gibbard–Satterthwaite games for ,https://www.sciencedirect.com/science/article/pii/S0165489619300228,May 2019,2019,Research Article,248.0
"Gersbach Hans,Haller Hans","CER-ETH—Center of Economic Research at ETH Zurich, Zürichbergstrasse 18, 8092 Zurich, Switzerland,Department of Economics, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061–0316, USA","Received 16 January 2017, Revised 23 April 2018, Accepted 26 November 2018, Available online 19 March 2019, Version of Record 24 May 2019.",https://doi.org/10.1016/j.mathsocsci.2018.11.002,Cited by (0),"We develop a model that combines competitive exchange of private commodities across endogenously formed groups with public good provision and global collective decisions. There is a tension between local and global collective decisions that can cause non-existence of competitive equilibria with endogenous household formation and public choice. In particular, we show that group formation and collective decisions on public goods may destabilize each other, even if there exist favorable conditions for matching on the one hand, and for global collective decisions on the other hand. We establish sufficient conditions for the existence of competitive equilibria with endogenous household formation and public choice and illustrate a variety of phenomena when households take local collective decisions and have a voice in global collective decisions.","Economic theory, including theories of optimal taxation, and empirical research have traditionally adhered to the unitaryhousehold model and treated households as if they were single consumers. However, this has changed. Chiappori, 1988, Chiappori, 1992 and Apps and Rees (1988) introduced models of collective rationality of households as an alternative to the representative consumer or unitary model.==== ====
 Inspired by Chiappori’s work, Haller (2000) studied competitive exchange among exogenously given households whose decision-making conforms with collective rationality. Subsequent generalizations resulted in the general equilibrium model of Gersbach and Haller (2011) that allows for endogenous household formation. Gersbach and Haller (2010) highlight the similarities and differences between the household model and club models with multiple private goods.==== ====Most of the work on general equilibrium models with multi-member households is predicated on two interrelated premises. First, the allocation of resources is impacted by the prevailing household structure, that is the partition of the population into households. Second, formation and stability of households and the resulting household structure are influenced by economic conditions and the anticipated allocative implications of household membership.====Previous investigations (Gersbach and Haller, 2010, Gersbach and Haller, 2011) have considered consumers who care about their own private consumption, and possibly the identity and private consumption of other members of their own household. Such “household-specific preferences” allow for consumption externalities within households and for local public goods for the household. They disallow consumption externalities across households and global public goods. Public goods give rise to well known issues: Who provides them? How much should be provided? How should they be funded? Here, we extend the model by introducing government-provided global public goods that may affect the entire population. The externalities caused by private good consumption are still confined to members of the same household.====A very common way of treating public goods in a general equilibrium context is given by Lindahl equilibrium; see Milleron (1972). Lindahl equilibrium resolves all allocative and distributional questions and yields a Pareto optimum. But Lindahl prices or tax rates are difficult to assess if individual preferences or demand functions are only known by the individual. It is well known that individuals have incentives to misrepresent their preferences and public good provision requires coercion in large societies (see e.g. Mailath and Postlewaite (1990)). Moreover, in our context, there is the additional problem whether Lindahl prices should be determined at the individual or the household level in the case of multi-member households.====Thus, we opt for coercive taxation and voting as the alternative mechanism to provide public goods. In our model, we use a wealth tax with a uniform tax rate. First of all, wealth is much easier to observe than preferences. Second, it does not matter who gets taxed, individuals or households. Given the tax scheme, the public policy (amount of public goods and tax rate) can be changed via majority voting. Voting on public policies is a frequent provision in the literature, and in practice.==== ==== At least since Bowen (1943) and Bergstrom (1979), a large literature has explored whether majority voting leads to efficient provision of public goods (see, e.g., Persson and Tabellini (2002) for a review). We focus on public good provision with coercive taxation and majority voting embedded in a market economy — and endogenous household formation. In particular, we are concerned with the existence of equilibria, stability of matching and generalized median voter results when public good provision occurs through voting in such settings.==== ====Integrating the public provision of public goods into general equilibrium theory poses other well known challenges. It ultimately necessitates the existence of governmental authorities and the introduction of constitutional rules. First, public or collective provision of public goods requires an authority with the power to tax people. Second, rules for collective decision-making on public good provision involving all citizens have to be specified. Third, one has to decide whether non-discriminatory constitutional clauses for taxation ought to be introduced.====We incorporate public good provision and a constitutionally founded state into the general equilibrium models with group formation developed in Gersbach and Haller (2011).==== ==== We impose a uniform tax rule to fund public good provision. That is, public goods are financed by a linear tax on the wealth of individuals. Like for local collective decisions at the group level, we adopt a flexible approach to global collective decisions on public good provision. In particular, we require that in equilibrium, public good bundles and a financing scheme cannot be improved upon by a group of households of a certain total membership size. Special cases are simple majority or super-majority requirements.====We require that members of a group vote for a policy change only if none of the members would be harmed by the change. One interesting implication of that rule is illustrated by Example 1. Suppose males do not care about public schools whereas females support funding of public school systems by local governments. Then positive government funding can be sustained in equilibrium even if the majority of the population is male — in case heterosexual couples vote against a proposed reduction of funding. Yet the example also allows for the possibility that not enough heterosexual couples form, resulting in a majority voting for a reduction of funding. Thus, if individual voting behavior takes the impact on fellow group members into account, this may affect voting outcomes. In turn, the choice of public goods and their funding may influence group formation — which is not the case in Example 1, but is the case in Example 2.====In Gersbach and Haller (2011), we obtain several equilibrium notions distinguished by the stability requirements for households. These notions can be extended by adding ====. Thus, we will focus on “competitive equilibria with endogenous household formation and ====”. We are going to impose the most stringent stability requirement for households: that no group of consumers can benefit from forming a new household.==== First, we develop a formal structure to integrate decisions to form, join or exit groups; local collective decisions on consumption in such groups; competitive exchange across endogenously formed groups; and global collective decisions on public good provision across all citizens. The formal framework allows to illustrate a variety of phenomena such as the one in Example 1 described above.====Second, we show that paradoxically, a priori stable matchings and stable collective decisions (Condorcet winners) may destabilize each other when they are integrated. The noteworthy feature of this result is that favorable conditions for matching and global collective decisions in the form of the simple majority rule prevail. In particular, for any given tax rate, a stable matching, i.e., a stable household structure exists. In addition, global collective decisions have Condorcet winners for any conceivable household structure. Despite these features, however, there are circumstances in which no stable matching exists. In other words, stable matchings and stable outcomes of voting according to the simple majority rule cannot be achieved simultaneously.====Third, we consider two polar cases of price expectations. In Section 3, we explore the consequences when a group of consumers contemplating alternative public good levels are unaware or disregard the impact of such a change on the prices of private goods. In Section 5, such myopic price expectations are contrasted with rational price expectations where consumers correctly predict the impact of a policy change on the prices of private goods. We identify conditions under which both types of price expectations yield the same equilibrium outcomes. However, in general neither equilibrium concept (with rational price expectations or myopic price expectations) implies the other. Fourth, we present median-voter type results for economies in which public good provision is embedded in a market system cum household formation.====To relate our paper to the literature, it is useful to start with the tension between matching and global collective decisions. This manifests itself in one of our main results (Proposition 2): It shows that public good provision implemented via collective decisions on taxation can destabilize any household structure — even if a stable matching exists given any conceivable collective decision. This kind of non-existence is absent from the matching literature because there, global collective decisions are typically irrelevant. See the work on hedonic coalitions (e.g., Banerjee et al. (2001), Bogomolnaia and Jackson (2002), and Greenberg (1978)), matching (e.g., Alkan (1988), Gale and Shapley (1962) and Roth and Sotomayor (1990)), assignment games (e.g., Roth and Sotomayor (1990) and Shapley and Shubik (1972)), and multilateral bargaining (e.g., Bennett, 1988, Bennett, 1997, Crawford and Rochford (1986) and Rochford (1984)). Consequently, that literature does not observe the tension between matching and global collective decisions.====In Gersbach and Haller (2011), we explore the potential tension between endogenous household formation and consumption decisions of households on the one hand and competitive markets for private goods on the other hand. In that analysis, group and consumption externalities within households play a crucial role. Now the emphasis shifts to the tension between matching and global collective decisions. For that purpose, externalities play still an important role like in the illustrative Example 1 and for Proposition 2, but receive a less prominent treatment in most of the paper.====Let us emphasize that Gersbach and Haller (2011) do not (and cannot) observe the tension between matching and global collective decisions either. In the striking Example 3 of their paper, stable matching and market clearing cannot be achieved simultaneously. That example exhibits group as well as consumption externalities and active markets for private consumption goods. In the novel example given in the Appendix that underlies our current Proposition 2, the assumptions and conclusions are very different: There is a single private good so that market clearing is not an issue. But in contrast to Gersbach and Haller (2011), there is now a global public good and there exists a tension between stable matching and collective decisions on the provision and financing of the public good — despite the absence of local consumption externalities.==== ====The theory of second-best taxation in the context of local public good provision and linear wealth taxation has been significantly developed by Guesnerie and Oddou, 1979, Guesnerie and Oddou, 1981 and Westhoff (1977). They consider coalition formation in which a coalition cannot benefit from public goods produced by other coalitions. They show that it may not be socially desirable that the grand coalition forms and that stable coalition structures may exist in which each coalition chooses a particular level of public goods and a corresponding tax rate. Our model shares the perspective that local externalities will typically lead to the partition of society into subpopulations. It differs, however, in a variety of crucial aspects. First, apart from local externalities there are also global public goods. No citizen can be excluded from the benefits when a positive level of the public good is provided. Second, in our model, local externalities and global public goods can be present simultaneously.==== ==== Third, groups or coalitions trade through competitive markets. As a consequence, equilibrium notions and their properties differ significantly from the cited second-best literature.====In the tradition of a large literature on public good provision and in particular the work of Guesnerie and Oddou, 1979, Guesnerie and Oddou, 1981 and Buchanan and Yoon (2004), which we will discuss below, we assume the presence of tax rules. While we focus on linear wealth taxes, the essential constraint is that the tax schedule is given when public good provision takes place. We refer to Gersbach et al. (2013) for the justification why such tax rules should be separated from the actual decision on public good provision.====Another strand of literature that works with tax rules is exemplified in Buchanan and Yoon (2004). When all individuals have to pay the same tax rate, majoritarian institutions and the prospect for individual membership in more than one decision authority limit exploitation of the tax base, and thus limit the tragedy of the fiscal commons. In our context, overexploitation of the tax base by majoritarian institutions does not occur as tax rules prevail and tax revenues are solely used for the financing of public goods.====The paper is organized as follows. In the next section we introduce the model. The concept of competitive equilibrium with public choice and myopic price expectations is introduced and investigated in Section 3. In Section 4, we elaborate on endowment redistribution. In Section 5, we define the concept of competitive equilibrium with public choice and rational price expectations. We study the relationship between the two equilibrium concepts. Section 6 concludes. An Appendix contains the proof of Proposition 2.","Households, markets and public choice",https://www.sciencedirect.com/science/article/pii/S0165489617300161,19 March 2019,2019,Research Article,249.0
"De Sinopoli Francesco,Iannantuoni Giovanna,Manzoni Elena,Pimienta Carlos","Department of Economics, University of Verona, Italy,Department of Economics, Management and Statistics, University of Milano-Bicocca, Italy,School of Economics, The University of New South Wales, Australia","Received 28 November 2016, Revised 25 January 2019, Accepted 29 January 2019, Available online 18 March 2019, Version of Record 29 March 2019.",https://doi.org/10.1016/j.mathsocsci.2019.01.004,Cited by (1),"We introduce a model with strategic voting in a parliamentary election with proportional representation and uncertainty about the voter’s preferences. In any equilibrium of the model, most of the voters only vote for those parties whose positions are extreme. In the resulting parliament, a consensus government forms and the policy maximizing the sum of utilities of the members of the government is implemented.","A central issue in the Political Economy literature is understanding how voters influence policy outcomes in a democracy. Voters select their political representatives through the electoral rule and these representatives, in turn, choose the policy outcome. When elections are held under plurality rule, the system implements the policy preferred by the winner. As rational voters anticipate this, the theoretical and empirical literature has typically assumed that voters are instrumentally motivated in such a context (Cox, 1997). In the proportional representation case, however, voting incentives are more complicated. This seems to be the main reason for the dearth of models that assume strategic voting under proportional representation. However, Cox (1997) also identifies the strategic incentives that voters face under proportional representation systems. More recently, Abramson et al. (2010) and Hobolt and Karp (2010) show that the amount of strategic voting under plurality and proportional system are quite similar.====But there is an additional layer of complexity. Typical parliamentary democracies are characterized by a legislative body, elected by proportional rule, and by an executive body, which derives its mandate from the legislature. The theoretical literature on legislative bargaining under proportional representation starts from Austen-Smith and Banks (1988) where a quite complex three-party model is analyzed leading to a plethora of equilibria. The existence of a multiplicity of government structures that arise in equilibrium is a common characteristic of these models. This can be seen also in Baron and Diermeier (2001) who put forward a protocol of legislative bargaining in a three-party system and show how the government coalition and the policy outcome may depend on who is selected as government formateur.====Thus, policy oriented voters have to anticipate how their vote affects the final policy outcome within such a complex institutional setup. There seems to be evidence that this is indeed the case. Bargsted and Kedar (2009) show empirically that the expectation about which parties will form the government coalition influences voters’ behavior. Duch et al. (2010) illustrate, using data of over 23 countries, that voters anticipate post-electoral bargaining outcomes and factor this expectation into their vote choice. Moreover, Kedar (2005) studies and tests empirically a model in which voters anticipate policy choices in parliamentary elections, hence displaying strategic behavior in this context.====We introduce a tractable proportional representation model that incorporates all of these main features. Voters vote taking into account which government will eventually form and which will be the policy implemented by such a government. For the voting stage, we build on De Sinopoli and Iannantuoni (2007). The authors study strategic voting under proportional rule and find that, essentially, only a two-party equilibrium exists, in which rational voters vote only for the two extreme parties. However, we depart from their model by allowing for uncertainty over the voters’ preferences, thus enriching the model in a more realistic manner. For the government formation process and final policy choice, we draw on Baron and Diermeier (2001) who study a three party proportional model with a post-electoral bargaining stage. A randomly selected formateur chooses a potential government coalition and makes its members a take-it-or-leave-it offer over both the policy outcome and the allocation of transfers. In equilibrium, the policy choice corresponds to the policy that maximizes the sum of utilities of the government members. The main difference in this stage of our model with Baron and Diermeier (2001) is that legislators’ preferences allow us to work with an arbitrary number of parties. Baron and Diermeier (2001) notice that increasing the number of parties in their bargaining protocol increases the number of possible coalitions and, therefore, also the complexity of the characterization of equilibria. In our model, legislators always prefer being in government to the status quo. This implies that consensus governments arise, and that there always is a unique equilibrium coalition.====We obtain the following results. Most voters vote only for two extreme parties at either side of the political spectrum. This is the most efficient manner in which a voter moves the final policy outcome towards her preferred policy as she anticipates that the final policy will be the average of the elected legislators’ ideal points. Thus, the composition of the parliament is dominated by members of these two extreme parties, nonetheless, any selected formateur selects a government coalition consisting of every member of the parliament. That is, a consensus government arises. Similar to Baron and Diermeier (2001), the implemented policy is the one that maximizes the sum of utilities of the members of the government.====Cho (2014) tackles a research question very close to ours, i.e., the theoretical analysis of the strategic behavior of policy oriented voters in proportional representation elections with a post-electoral bargaining stage. Contrary to our findings, proportional representation in his framework promotes the representation of small parties. Using the same bargaining protocol as Romer and Rosenthal (1978), Cho (2014) analyzes robust equilibria in which a voter’s choice must remain best response when there is a small probability that the proposer’s selection is proportional even when there is a majority party (the Romer and Rosenthal (1978) protocol prescribes that a majority party is the proposer with probability one). Under such equilibria, voters have an incentive to vote for minority parties. In a more recent paper, Troumpounis and Xefteris (2016) prove the existence of equilibria in which only two parties receive a positive fraction of votes under proportional rule in the presence of incomplete information when the number of voters follows the Poisson distribution. Despite tackling the same research question, their model differs substantially from ours. First, the model does not have a policy space because voters’ utility depends on the identity of the party in government. Therefore, a voter’s utility to government G is her average utility over the parties in government instead of her utility to the average policy. Moreover, the government formation process is based on the minimal winning coalition, different from ours. Nevertheless, Troumpounis and Xefteris (2016) obtain similar results based on their different underlying mechanism. In their three-party model, voters may have an incentive to vote for their second-best choice to affect the composition of the government, which may lead to two-party equilibria. Instead, our model generates two party equilibria because voters vote for the two extreme parties in an effort to move the average policy closer to their ideal point.====Studying strategic voting under proportional representation in our setting might shed light in various directions. First, it can help improving the understanding of how voters’ preferences are aggregated into political outcomes in parliamentary elections. Second, our analysis has implications on the number and nature of the parties that may arise in this context. Our results suggest that strategic behavior under proportional representation leads to the polarization of parties’ positions because extreme parties take most votes. The empirical political economy literature debates such a result since the seminal work of Cox (1997). Recently, Kedar (2005) studies a decision theoretical voting model and shows empirically that proportional representation elections favor voting for extreme parties. Interestingly, the mechanism at work is quite similar to ours: “voters compensate for the watering-down of their vote, often voting instrumentally for a party whose positions differ from their own”.====The paper is organized as follows. Section 2 presents the model. We solve the post-electoral bargaining in Section 3, the government coalition selection in Section 4, and the electoral stage in Section 5. Section 6 concludes the paper analyzing three examples.",Proportional representation with uncertainty,https://www.sciencedirect.com/science/article/pii/S0165489616301743,May 2019,2019,Research Article,250.0
"Kawanaka Susumu,Kamiyama Naoyuki","Graduate School of Mathematics, Kyushu University, Japan,Institute of Mathematics for Industry, Kyushu University, Japan,JST, PRESTO, Japan","Received 29 September 2018, Revised 22 February 2019, Accepted 25 February 2019, Available online 5 March 2019, Version of Record 15 March 2019.",https://doi.org/10.1016/j.mathsocsci.2019.02.003,Cited by (0),"-time algorithm, where ==== is the size of the ground set, ==== is the number of acceptable sets, and ==== is the maximum size of an equivalent class. In this paper, we propose an ","The two-sided matching market model introduced by Gale and Shapley (1962) is one of the most fundamental mathematical models for assignment problems. When we consider many-to-many matching markets, we frequently assume that the preference lists of agents are given in the form of choice functions. (If there exist ties in the preference lists, then they are given in the form of choice correspondences.) In matching models with choice functions, the property called substitutability plays an important role (see, e.g., Fleiner, 2003, Hatfield and Milgrom, 2005). For example, in several matching models, if choice functions are substitutable, then a stable matching always exists (see, e.g., Hatfield et al., 2012).====In this paper, we consider substitutability of choice functions from the algorithmic viewpoint. More specifically, we consider the problem of checking whether a given preference list (i.e., the choice function induced by it) is substitutable. This line of research was initiated by Hatfield et al. (2012), and they proposed an ====-time algorithm in the strict preference case, where ==== is the size of the ground set and ==== is the number of acceptable sets. Then Aziz et al. (2013) considered the weak preference case (i.e., the preference lists contain ties), and they proposed an ====-time algorithm, where ==== is the maximum size of an equivalent class.====In this paper, we propose an ====-time algorithm in the weak preference case. Our algorithm is based on a generalization of the characterization of substitutability of strict preferences given by Croitoru and Mehlhorn (2018).",An improved algorithm for testing substitutability of weak preferences,https://www.sciencedirect.com/science/article/pii/S0165489619300150,May 2019,2019,Research Article,251.0
"Bjorkman Beth,Gravelle Sean,Hodge Jonathan K.","Iowa State University, United States,University of Nebraska-Lincoln, United States,Grand Valley State University, United States","Received 5 January 2018, Revised 17 December 2018, Accepted 19 February 2019, Available online 28 February 2019, Version of Record 28 March 2019.",https://doi.org/10.1016/j.mathsocsci.2019.02.002,Cited by (1),"The notion of separability is important in a variety of fields, including economics, political science, computer science, and operations research. Separability formalizes the idea that a decision-maker’s preferred ordering of outcomes on some dimensions within a multidimensional alternative space may depend on the values of other dimensions. The character ","The notion of separability is important in a variety of fields, including economics, political science, computer science, and operations research. Separability formalizes the idea that a decision-maker’s preferred ordering of outcomes on some dimensions within a multidimensional alternative space may depend on the values of other dimensions. A much-studied example is that of multiple-question referendum elections, in which voters are required to cast simultaneous ballots on multiple questions or proposals. In this context, nonseparable voter preferences can lead to unsatisfactory or paradoxical election outcomes, such as a winning outcome that is the last choice of ==== voter (Brams et al., 1997, Lacy and Niou, 2000).====Bradley et al. (2005) showed that several important properties of separability depend on the topology of the underlying alternative spaces. In particular, two theorems of Gorman (1968) – that separability is preserved under most common set operations, and that all separable preferences are additive – hold for continuous alternative spaces but fail when the sets of alternatives for each dimension are discrete. Subsequent research has sought to better understand the structure of separability in discrete settings—particularly in the context of multidimensional binary alternative spaces like those that arise in the study of referendum elections.====Given a convenient representation of an agent’s preferences (we use ====), it is relatively straightforward to determine the sets of questions that are separable. The collection of all such sets is called the ==== of the given preferences. However, the corresponding inverse problem is more complex. In fact, there are no general methods for constructing preferences with a given character, and it is sometimes impossible to determine whether such preferences even exist. When they do, the associated character is said to be ====. Past research has focused on classifying and constructing admissible characters. Bradley et al. (2005) proved that closure under intersections is necessary for admissibility. Hodge and TerHaar (2008) determined all admissible characters for 4 or fewer dimensions; they also proved the existence of nontrivial inadmissible characters – that is, collections of sets that are closed under intersections and yet never arise as the character of any actual preferences – for question sets of size 4 or more. Most recently, Hodge et al. (2009) developed the technique of ==== to construct preferences with certain classes of characters.====In this paper, we investigate a new method that uses vertex–edge graphs to generate preferences. In Section 2, we introduce this method and apply it to the ====-dimensional hypercube graph, defining a class of preferences called ====. In Section 3, we prove results that relate the separability structures associated with cubic preferences to the algebraic structure of the hypercube graph. In Section 4, we present methods for constructing cubic preferences and prove results about the resulting characters. Importantly, we show that the characters associated with cubic preferences are distinct from those generated by preseparable extensions. We conclude in Section 5 with a brief discussion of the potential applications of our methods and some questions for further research.",Cubic preferences and the character admissibility problem,https://www.sciencedirect.com/science/article/pii/S0165489619300149,May 2019,2019,Research Article,252.0
Amorós Pablo,"Universidad de Málaga, Campus El Ejido, E-29013, Málaga, Spain","Received 8 November 2017, Revised 13 November 2018, Accepted 4 February 2019, Available online 14 February 2019, Version of Record 23 February 2019.",https://doi.org/10.1016/j.mathsocsci.2019.02.001,Cited by (1),"A possibly partial jury has to choose the winner of a competition. A deserving winner exists and her identity is common knowledge among the jurors, but it is not known by the planner. Jurors may be biased in favor (friend) or against (enemy) some contestants. We study the conditions based on the configuration of the jury such that it is possible to implement the deserving winner in ==== when we restrict ourselves to mechanisms that satisfy two conditions: (1) each juror only has to announce the contestant she thinks should win the competition, and (2) announcing the deserving winner is always an equilibrium. We refer to this notion as natural implementation. We show that knowledge of jurors’ friends or jurors’ enemies may help to naturally implement the deserving winner. In particular, our results suggest that knowledge of jurors’ friends may be more useful to the planner than knowledge of jurors’ enemies in terms of facilitating natural implementation.","Consider the hiring process in a department. The members of the department must choose whom to hire. Suppose that all members of the department know the best candidate (there is no private information). The optimal objective is to hire the best candidate available. However, some professors may be biased in favor/against some candidates, ====, some professors may not want to hire a candidate who is better at her job than they are, and others may want to hire a candidate who works in the same areas as them even if she is not the best candidate.====Many problems have a similar structure to that described above, which can be summarized as follows. A group of contestants are involved in a competition. A group of jurors must choose a winner among the contestants. All jurors know the best contestant: the “deserving winner”. The socially optimal choice rule (SOCR) aims to select the deserving winner regardless of who she is. However, jurors may be biased in favor/against some contestants. Biased jurors may try to favor/harm one contestant rather than another, regardless of who is the best.====Sometimes, it is possible to design mechanisms that neutralize the particular interests of partial jurors. It depends on the specific bias of the jury. Amorós (2013) provided a necessary condition for the implementation of the SOCR in any equilibrium concept: for each pair of contestants, at least one juror must be known to be impartial with respect to them (a juror is known to be impartial with respect to a pair of contestants if, whenever one of the two contestants is the deserving winner, the juror prefers that contestant to the other). We refer to this requirement as “minimal impartiality”. This condition is also sufficient for the Nash implementation. If minimal impartiality is satisfied, then the canonical mechanism for Nash implementation implements the SOCR. However, the canonical mechanism has been criticized as being unnatural and having excessively complex message spaces (see Jackson, 1992). The reason that this mechanism is abstract and unnatural is that it is designed to characterize what can be implemented so it must handle a large number of problems (see Serrano, 2004).====In this paper, we aim to implement the deserving winner focusing on a tight class of mechanisms that are “natural” on the type of problem at hand. First, we require that our mechanisms have straightforward message spaces. Some of the simplest mechanisms are those in which each juror only has to announce the contestant who she thinks should win the competition. We refer to these mechanisms as “straightforward” mechanisms. Secondly, because we are asking each juror to identify the deserving winner, then it seems natural to require that telling the truth (announcing the deserving winner) should always be an equilibrium. We say that the SOCR is naturally implementable if a straightforward mechanism exists for implementing the SOCR where telling the truth is always an equilibrium.====Natural implementation restricts the mechanisms that can be used. Not surprisingly, this restriction on a mechanism hinders implementation. We show that minimal impartiality is no longer sufficient for the natural implementation of the SOCR in Nash equilibrium. Thus, a natural question arises: what additional conditions must the jury satisfy in order to ensure that we can design a natural mechanism that induce the jurors to always choose the deserving winner? Sometimes the planner has more information about the jury and knows that some jurors would like to favor/harm some contestants. We say that a contestant is a known friend/enemy of a juror if this contestant is always her most/less preferred alternative, regardless of who is the deserving winner. Knowing that jurors have friends or enemies reduces the set of admissible states of the world. Our aim is to analyze how this reduction may facilitate the natural implementation of the SOCR.====First, we study the simple case in which there are three contestants and, for each pair of contestants, there is only one different juror who is known to be impartial with respect to them (so that minimal impartiality is fulfilled). This assumption allows us to identify each contestant with one juror that can be her friend or her enemy. Under this assumption, we wonder how many of the jurors would need to have known friends (respectively enemies) so that the SOCR can be naturally implemented in Nash equilibrium. We show that, while it is sufficient that one of the jurors has a known friend to naturally implement the SOCR in Nash equilibrium (Theorem 1 ), at least two jurors with known enemies are necessary to achieve that goal (Theorem 2). This is surprising because the reduction in the size of the set of admissible states due to the existence of one juror with a known enemy is equal to that obtained when we have one juror with a known friend. However, in the former case this reduction is sufficient to allow the natural implementation of the SOCR, whereas it is not in the latter case. From Theorem 1, Theorem 2 we conclude that, under our assumption, natural implementation is easier when jurors have known friends than when they have known enemies. Specifically, the minimum number of jurors with known friends required to naturally implement the deserving winner is less than the minimum number of jurors with known enemies required.====This asymmetry between the situations with friends and enemies seems to remain when there are more than three contestants. The analysis here is less direct, because different possibilities can fulfill minimal impartiality and, for many of them, there is no unequivocal identification of each contestant with a different juror who can be her friend or her enemy. In this case, we propose a straightforward mechanism and we investigate the direction in which the minimal impartiality requirement must be extended for this mechanism to naturally implement the SOCR. The mechanism comprises three simple rules: (i) if all jurors announce the same contestant then that contestant is chosen; (ii) if there is only one dissident, then we choose the contestant announced by the dissident only if she is known to be impartial between that contestant and the contestant announced by the others, and (iii) if more than two jurors disagree then we identify the contestants announced by the jurors with numbers to induce a “modulo game”. We show that having four jurors with known friends is sufficient for this mechanism to naturally implement the SOCR in Nash equilibrium (Theorem 3). However, even if all jurors have known enemies, the proposed mechanism fail to naturally implement the SOCR (Theorem 4). These results are in line with our intuition that natural implementation is easier when jurors have known friends than when they have known enemies.====Amorós (2016) analyzed the natural implementation of the SOCR in subgame perfect equilibrium with extensive form mechanisms for the case with three jurors and three contestants, thereby demonstrating that there is no asymmetry between the cases with friends and enemies. Thus, having one juror with a known friend or one juror with a known enemy is a sufficient condition for naturally implementing the SOCR in subgame perfect equilibrium when there are three jurors and three contestants. Amorós (2013) also provided necessary and sufficient conditions based on the configuration of the jury for the implementability of the SOCR, but the natural implementation was not considered. Amorós (2011) studied the particular case where each contestant has one friend and a natural extensive form mechanism was proposed to implement the SOCR in subgame perfect equilibrium. Moskalenko (2013) proposed an alternative to the previously described mechanism where each juror can veto a contestant.====Amorós (2009) analyzed a model where the jurors must choose a full ranking of the contestants instead of selecting one winner, where necessary and sufficient conditions were provided based on the jury for the Nash implementability of a rule that always selects the socially optimal ranking, although there was no consideration of the implementation using natural mechanisms (the mechanisms proposed were similar to those described by Maskin, 1999). Amorós et al. (2002) investigated the same model and analyzed implementation in dominant strategies and Nash equilibrium when each juror has one friend and is impartial with respect to the remaining contestants. Adachi (2014) also examined the problem of choosing the socially optimal ranking when jurors may have friends and a natural mechanism was proposed for implementation in subgame perfect equilibrium. Ng and Sun (2003) investigated the problem of excluding self-awarded marks when calculating the ranking when each of the contestants is biased in favor of herself.====Doğan (2013) studied a closely related model where a set of tasks must be allocated among a set of agents whose preferences over allocations may or may not be “responsive” to the optimal allocation (the notion of an agent being responsible is similar to that of a juror being impartial). It was shown that the optimal allocation can be implemented in Nash equilibrium if there are at least three responsible agents. The present paper is also related to previous investigations of the effects of honest agents on the general implementation problem (Matsushima, 2008, Dutta and Sen, 2012)====Holzman and Moulin (2013) studied the problem of choosing the winner of a competition when the jurors are the contestants themselves and each contestant preferences bear on whether or not she wins, and nothing else. They analyzed straightforward mechanisms where each contestant cannot nominate herself and such that a contestant message never influences whether she is chosen or not (they call these mechanisms impartial nomination rules). Because reporting one’s disinterested opinion never affect whether or not one is chosen, every strategy of every contestant is dominant, and so they ignore incentives and focus on the normative analysis of the mechanism. In the present paper, however, we want to implement the deserving winner and jurors’ incentives are crucial for that.====The problem of natural implementation has been identified and studied in several papers in the literature. The idea is to set intuitively appealing conditions that mechanisms should satisfy and to identify the class of social choice rules that are implementable by mechanisms satisfying these conditions. The suitability of the conditions on mechanisms depends greatly on the type of problems at hand (Jackson, 1992). For instance, in the economic environment, Sjöström (1996) proposed the use of a quantity mechanism: each participant announces just her consumption bundle as her strategy. Similarly, Saijo et al. (1996) require three conditions (forthrightness, feasibility, and best response) that a natural mechanism should satisfy in exchange economies. The definition of natural implementation proposed in the present paper is designed for the particular problem analyzed here: choosing the deserving winner of a competition.====The reminder of this paper is organized as follows. In Section 2 we explain our basic model, we state the necessary and sufficient conditions on the jury for Nash implementation, and we introduce the concept of natural implementation. In Section 3 we present the results on natural implementation when there are three contestants and jurors. In Section 4, we analyze the situation when there are more than three contestants and jurors. In Section 5, we give our conclusions. The proofs of some of the results are in appendices A, B, and C.",Choosing the winner of a competition using natural mechanisms: Conditions based on the jury,https://www.sciencedirect.com/science/article/pii/S0165489619300137,March 2019,2019,Research Article,253.0
"Bajoori Elnaz,Vermeulen Dries","University of Bath, Department of Economics, Bath, BA2 7AY, United Kingdom,Maastricht University, School of Business and Economics, Department of Quantitative Economics, P.O. Box 616, 6200 MD Maastricht, The Netherlands","Received 23 August 2018, Revised 23 January 2019, Accepted 24 January 2019, Available online 12 February 2019, Version of Record 7 March 2019.",https://doi.org/10.1016/j.mathsocsci.2019.01.003,Cited by (0),"In second-price auctions with interdependent values, bidders do not necessarily have ","In private value second-price auctions each bidder has a dominant strategy in which he bids his own valuation of the object. However, in second-price auctions with interdependent values bidders may not have dominant strategies while there may exist multiple equilibria in undominated strategies. For example  Birulin (2003) shows that any auction with an efficient ex-post equilibrium has a continuum of inefficient undominated ex-post equilibria. Therefore, a selection tool is needed in such Bayesian games to rule out the less intuitive equilibria. We develop a concept of trembling hand perfect equilibrium for Bayesian games with infinite type and action spaces, called distributional strictly perfect equilibrium (DSPE). In this equilibrium concept, players’ strategies are robust against any slight perturbations of their opponents’ strategies.====In finite normal form games, DSPE is equivalent to strictly perfect equilibrium by  Okada (1981),==== ==== as both definitions require robustness against arbitrary slight perturbations of strategies. It is known that strictly perfect equilibrium may not exist (see example 1.5.5 in Van Damme, 1991), therefore DSPE may not exist even in finite games. However, we can prove existence of a weaker notion of DSPE, which is called distributional perfect equilibrium, in Bayesian games for which the type space of each player is a separable metric space, the action space of each player is a compact metric space, and player types are drawn from a prior probability measure on the product of the type spaces that is absolutely continuous with respect to the product measure of its marginal probabilities.==== ==== ====We apply this notion of perfection to second price auctions with interdependent values in order to select among equilibria. In our auction model, there is one continuous and efficient equilibrium introduced by Milgrom (1981) and a set of inefficient ex-post equilibria introduced by Birulin (2003). All of these equilibria are undominated and there is no dominant strategy. Furthermore, all these equilibria all ex-post, so ex-post cannot be used as a selection criterion in this model. We prove that the efficient equilibrium due to Milgrom (1981) is DSPE, but the one’s constructed in Birulin (2003) are not. In other words, our results show that the efficient and continuous equilibrium is robust against arbitrary slight perturbation of strategies, while other discontinuous and inefficient equilibria are not. These discontinuous equilibria contain either overbidding or underbidding when compared with the efficient strategy profile by Milgrom (1981). We show that if each bidder believes that with a very small probability her opponent can bid according to the efficient bidding strategy, then overbidding or underbidding is not a best response.====Bajoori et al. (2016) define the notion of Perfect Bayesian Nash Equilibrium and apply this notion to an example in interdependent value auctions. In a Perfect Bayesian Nash Equilibrium the strategies are robust against some slight perturbations of strategies. This definition is weaker than the notion of DSPE, but it is not strong enough to exclude all the discontinuous and inefficient equilibria in a more general model of an interdependent value auction.====To the best of our knowledge there is not much research done on the theory of equilibrium selection in interdependent value auctions. However, there are some relatively recent studies on the equilibrium selection in common value auctions such as Parreiras (2006), Abraham et al. (2012),  Cheng and Tan (2008), Larson (2009), and  Liu (2014). In the most relevant study by  Liu (2014), they use noisy bids rather than strategy perturbations. In their model, bidders need to believe that any bid is possible for their opponent even if this strategy does not have full support. They show that the continuous set of equilibria introduced by  Milgrom (1981) are robust against such noisy bids, while the set of discontinuous equilibria (similar to the ones introduced by Birulin (2003)) are not. Our paper does not cover the case of common values, so that our results are disjoint from those in Liu (2014).====Another relevant strand of literature is by Bergemann and Morris (2005) on robust mechanism design. They show that, within the class of separable environments, interim implementability of a social choice correspondence for any given type space is equivalent to implementability in ex post equilibrium. Thus, in their approach, ex post equilibrium is used as the basic solution concept for robust implementation. However, as we already mentioned, it is well known in the literature on auction design that ex post equilibrium may often be inefficient, and the refinement we propose often rules out the inefficient equilibria. Thus, our approach may in the future have consequences for mechanism design as well.====To illustrate our results, we discuss the following example.====The paper is structured as follows. First, we provide some preliminary notions in Section 2. In Section 3 we introduce the concept of distributional strictly perfection. In Section 4, first we present our interdependent value auction model. Then, we have two subsections; one on selected equilibrium and the other on discarding equilibria.",Equilibrium selection in interdependent value auctions,https://www.sciencedirect.com/science/article/pii/S0165489619300125,March 2019,2019,Research Article,254.0
Marlats Chantal,"Université Paris II-Panthéon Assas, France","Received 19 October 2017, Revised 16 January 2019, Accepted 19 January 2019, Available online 7 February 2019, Version of Record 25 February 2019.",https://doi.org/10.1016/j.mathsocsci.2019.01.002,Cited by (1),This paper explores the robustness of predictions made in long but finitely repeated games. The robustness approach used in this paper is related to the idea that a modeler may not have absolute faith in his model: The ,None,Perturbed finitely repeated games,https://www.sciencedirect.com/science/article/pii/S0165489619300113,March 2019,2019,Research Article,255.0
"Abdou Joseph M.,Keiding Hans","Centre d’Economie de la Sorbonne, Université Paris 1, Panthéon-Sorbonne, 106-112 Boulevard de l’Hôpital, 75647 Paris Cedex 13, France,Department of Economics, University of Copenhagen, O. Farimagsgade 3, DK-1353, Copenhagen K, Denmark","Received 28 July 2018, Revised 4 January 2019, Accepted 6 January 2019, Available online 16 January 2019, Version of Record 29 January 2019.",https://doi.org/10.1016/j.mathsocsci.2019.01.001,Cited by (4),"In the present work, we consider a basic model of political structure, given through its agents or forces and the viable configurations of agents as collective bodies of decision making. When the set of all agents is not viable, a compromise must be searched for. We model a political structure as a ","Political activity can be viewed as an interaction between forces seeking to exercise power, to choose actions realizing political projects. Power is commonly exercised by collections of agents constituting only a subset, in some case a small subset, of all relevant agents, and such collections or coalitions of agents may be considered as representing the remaining agents, in the broadest possible sense of this term. Such coalitions will typically be faced with the problem of taking political action in many different situations, facing different agendas, where the members of the coalitions may have very different viewpoints on the desirability of the goals to be obtained by political action.====When forming a coalition for joint political action, it is obviously a prime concern that the coalition has sufficient power to carry out political action in all the reasonably expectable situations arising during its presumed time span of existence, meaning that breakdowns due to unsurmountable differences of opinion among the coalition members should be avoided.====So far, nothing has been said about the nature of the political decisions which are the final result of the formation of coalitions, which are entrusted with power. There is a considerable literature which considers various aspects of political decision making, showing that observable phenomena of politics can be explained as rational behavior in the given context of agenda, power and preferences. The seminal contribution by Dixit et al. (2000) shows how decisions having the nature of a compromise arise in contexts where power shifts over time between two political powers both seeking to maximize discounted future benefits in a problem of repeated division of a cake. A similar case of moderation in the political decision is presented by Aumann and Kurz (1977) in their famous model of taxation, where every majority can tax away all income of the complement. In both cases, the political agenda is specified in detail, so that specific solutions can be derived. Thus, the focus is on the given power structure and how it is exercised in a particular case. In Baron and Ferejohn (1989), Baron and Ferejohn show how the rules for setting the agenda may influence political decisions in legislative assemblies. Another aspect of decision making in legislatures was analyzed by Austen-Smith and Banks (1988), showing how decisions are influenced by the proximity of elections.====These remarkable results are obtained in a framework where the issues, and to some extent also the preferences of agents over these issues, are clearly specified. In the present work, we consider problems set in a much simpler and more abstract environment, since we are concerned only with viability of a political body, considered as a collection of agents, without committing ourselves to a detailed specification of what constitutes this viability. In this sense we deal with political problems occurring at the initial state of creating a viable government which will not fall apart when confronted with the political realities. In countries with a tradition of peaceful political decision making, there are commonly accepted mechanisms which prescribe a method for breaking deadlocks once they occur (for instance elections, referendum or justice ruling). Nevertheless, many political entities may experience a blocked governance period, while the current order fails to provide a solution.====This is a political crisis or a stalemate. It may occur in a context where a military conflict left the entity with a pre-state, pre-constitutional configuration, that is where a universally accepted rule does not prevail. Similarly, the threat of a violent action may impede a regular unfolding of the political process, or there may be a risk of accumulating discontent with the current establishment. What happens if a configuration composed by many agents or parties with incompatible political agendas lacks the institutional mechanism that enforces a settlement?====Modeling politics has a long history starting from early greek philosophers. Foundation of political order in a city or a state has been a fundamental theme in political philosophy. The object of political activity being to achieve coexistence, in a common space, of entities with conflicting wills, the notion of conflict is central in this thinking. One has to explain locally the emergence of an order among agents, and globally the coexistence of many such orders with different agendas. The difficulty (but also the interest) in modeling the notion of conflict lies in the fact that the latter is, by definition, a situation that erupts as a crisis of the current order and which therefore is unsolvable by existing institutions.====One of the main concepts in the analysis of conflict is the notion of an enemy, and it is often argued that the essential moment of politics is the dichotomy friend/enemy, cf. e.g. Schmitt (2008). This binary choice prevails in situations of disruption, when the political body is in danger, such as war or civil strife. The theory to be considered here does not follow the idea that politics is bipolar, instead it can be considered as the search for a viable situation when departing from a disrupted one.====The emergence of viable coalitions has been studied in many contributions to the literature, for example in models of coalition formation, cf. e.g. Pycia (2012) in a context of matching, where agents have preferences over coalitions to which they may belong, and Cechlárová and Romero-Medina (2001) where such preferences can be obtained by extension of preferences over individuals. In the broader context of game theory, coalition formation has been considered by many authors, e.g. Greenberg and Weber (1993), Bogomolnaia and Jackson (2002) and Banerjee et al. (2001).====In the present work, the approach is more abstract and therefore less dependent on specific assumptions. The basic notions are potential coalitions or configurations of agents, where agents may be individuals but may also be other entities, and we shall not assign preferences of any kind to agents, since we are concerned only with the combinatorial structure of the different coalitions and the possibilities of incompatibility. In this choice of basic framework, we follow a tradition in social choice theory, see e.g. Peleg (1984), Peleg and Peters (2010) and Peleg and Holzman (2017). We begin with a given configuration of forces together with a notion of viability. This can be seen as an abstract description of the situation that prevails in a country after a civil war or an invasion has destroyed the previous state. A less dramatic but formally similar situation prevails locally in political entities where elections or other rules result in a distribution of forces unable to coexist in a single government. Coexistence means here that the coalition would not disrupt due to incompatible individual differences, so that a viable order can be established.====The property of viability of certain collections of agents, interpreted as the ability to reach a common decision in a sufficiently large variety of political agendas, is formalized by the notion of a simplicial complex, where the simplexes are the permitted, or viable, collections. For the smooth working of a political system, it is desirable that the totality of agents constitutes a simplex, but this may not be the case, and the result is that crises may arise.====A fundamental notion in our approach to crisis resolution in this framework is ==== An agent can delegate power to another agent when the latter is at least as well situated in the viability distribution as the delegating agent. Friendly delegation by an agent is possible if any viable configuration containing the agent remains viable if the delegate joins the configuration. The idea is that no viable occasion is lost if the agent withdraws in favor of the delegate. From a formal point of view, the delegating agent is no longer at the forefront of the political scene and is removed as a point of the simplicial complex, and as a result the latter may become more tractable, indeed it may happen that delegation by several agents may transform the original complex into one for which the entire collection is viable, constituting a ====The possibility of such a reduction of the original framework to one where no fundamental conflict will arise is what in the present model constitutes a ==== carried out by reducing the set of politically active agents through delegation. The advantage of viewing this as a transformation of an original simplicial complex to another one with a simpler structure comes from the possibility of using results from algebraic topology. For the present case of a finite set of agents endowed with a structure of stable configurations the appropriate tool is homotopy theory of finite topological spaces as set forward by Barmak and Minian (2012), see also Barmak (2011). This makes it possible to identify characteristics of power structures for which compromises are possible, and more generally to trace the possibilities of reducing the inherent conflicts through delegation.====The delegation of power from one agent to another one selected as a representative may however not be enough to eradicate possibilities of conflict, and in such cases it may be necessary to consider repeated delegation, meaning that an agent, with whom power has been delegated from another agent, can perform further delegation to a third agent (who may then delegate once again). This type of iterated delegation means that the connection between the delegating agent and the final representative is less direct, but on the other hand it creates additional possibilities of reducing the inherent conflicts, possibly producing a final viable collection of representatives, here called a ==== Once again, algebraic topology is useful for characterizing the power structures for which such delegated compromises can arise.====The paper is organized as follows: In Section 2, we introduce the basic notion of a political structure, consisting of a set of agents or forces and a collection of configurations of these agents, together with some examples of political structures, and we define the central notion of delegation. In the following Section 3, we consider representations as the results of delegation, and we explain how notions and results from the topology of finite sets may be useful. The investigation of delegation and representation is followed up in Section 4, where we look at represented compromises, representations which are themselves viable. Then, in Section 5, we turn to what may be seen as a second level of delegation, allowing agents who receive delegation to delegate further in their turn. This gives rise to delegated compromises as results of subsequent delegations which are also viable, representing a compromise achieved through iterated delegation. We conclude Section 6 with some comments on the general approach as well as the need for further development of the theory.====The model is abstract and relies to some extent on homotopy theory for finite topological spaces. To avoid excessive formalism, we have put the proofs of results which pertain only to the formal apparatus in an Appendix at the end of the paper. For the results which involve notions of the model of political structures, we have chosen to give the proofs in the text so that the reader may follow the development of the central ideas. Some specific notation has been used, mostly explained in text. We use the notation ==== for the cardinality of a finite set. The sign ==== marks the end of a proof, and==== the end of an example.",A qualitative theory of conflict resolution and political compromise,https://www.sciencedirect.com/science/article/pii/S0165489619300083,March 2019,2019,Research Article,256.0
"Atay Ata,Núñez Marina","Institute of Economics, Hungarian Academy of Sciences, Budapest, Hungary,Department of Economic, Financial, and Actuarial Mathematics, and BEAT, University of Barcelona, Spain","Received 25 February 2018, Revised 12 November 2018, Accepted 30 December 2018, Available online 7 January 2019, Version of Record 17 January 2019.",https://doi.org/10.1016/j.mathsocsci.2018.12.002,Cited by (1),"We analyze the extent to which two known results of the relationship between the core and the stable sets for two-sided assignment games can be extended to three-sided assignment games. We find that the dominant diagonal property is necessary for the core to be a stable set and, likewise, sufficient when each sector of the three-sided market has two agents. Unlike the two-sided case, the union of the extended cores of all the ====-compatible subgames with respect to an optimal matching ==== may not be a von Neumann–Morgenstern stable set.","In this paper, we consider markets with three different sectors or sides. Coalitions of agents can only achieve a non-negative joint profit by means of triplets comprising one agent of each side. Then, a three-dimensional valuation matrix represents the joint profit of all these possible triplets. These markets, introduced by Kaneko and Wooders (1982), are a generalization of the two-sided assignment games first introduced by Shapley and Shubik (1972). In a similar vein, Stuart (1997) represents a supplier-firm-buyer situation using a three-sided assignment market.====In a two-sided assignment game, the two sectors are associated with a sector of buyers and a sector formed by sellers. Each seller has one unit of an indivisible good to sell and each buyer wants to buy at most one unit. The valuation matrix represents the joint profit obtained by each buyer–sellertransaction. From these valuations a coalitional game is obtained and the total profit under an optimal matching between buyers and sellers yields the worth of the grand coalition. A distribution of this worth such that each agent receives at least his/her individual coalition worth is called an ====. The best known solution concept for the coalitional game is the ====. Roughly speaking, a dominance relation is defined between imputations and the core is the set of undominated imputations.====Three-sided assignment markets appear naturally when a supplier (or middleman) is needed to match a buyer with a seller, or when each buyer needs to buy two complementary objects from two different types of seller to make a profit (for instance, each buyer needs to buy a computer as well as the services of an internet provider). A key difference between the two-sided and the three-sided assignment games is that while the core is always non-empty in the case of the former, it may be empty in that of the latter (Kaneko and Wooders, 1982). This is why we are interested in the studying of some other set-valued solution concepts, such as stable sets, for these games.====A ==== (von Neumann and Morgenstern, 1944) is a set of imputations that satisfies internal stability and external stability: ==== no imputation in the set is dominated by any other imputation in the set and ==== each imputation outside the set is dominated by some imputation in the set. It is known from Lucas (1968) that a game may have no stable set. Since the core always satisfies internal stability, it is included in any stable set; and if the core is externally stable, then it is the only stable set. Other notions of stability are analyzed in Roth (1976), Peris and Subiza (2013), and Han and van Deemen (2016).====The purpose of this paper is to analyze the extent to which existing results for two-sided markets can be extended to three-sided markets. In particular, we focus on two existing results of the relationship between the core and the stable sets. First, we study whether the dominant diagonal property is a necessary and sufficient condition for the core to be a stable set, as in two-sided markets (Solymosi and Raghavan, 2001). Second, we analyze whether the union of the extended cores of all ====-compatible subgames is a stable set of the three-sided assignment game, as in the two-sided markets (Núñez and Rafels, 2013).====In the case of two-sided assignment games, Solymosi and Raghavan (2001) show that the core of a two-sided assignment game is a stable set if and only if the valuation matrix has a dominant diagonal. Later, Núñez and Rafels (2013) prove the existence of a stable set for all two-sided assignment games. The stable set they introduce is the only one to exclude third-party payments with respect to an optimal matching ==== and it is defined through certain subgames, known as ====-compatible subgames.====In the present paper, we generalize the notion of the dominant diagonal to the three-sided case and prove that it is a necessary condition for the core of this game to be a stable set. We also show that for three-sided markets with only two agents on each side, the dominant diagonal property suffices to guarantee that the core is stable. It remains open as to whether it is also sufficient for arbitrary three-sided assignment markets. Furthermore, we extend the notion of ====-compatible subgames introduced by Núñez and Rafels (2013) to the three-sided case. Then, given a three-sided game and an optimal matching ====, we consider the set ==== formed by the union of the cores of all ====-compatible subgames. In contrast with the two-sided case, we show that ==== may not be a stable set. However, we prove that ==== is an abstract core if we restrict the set of feasible payoff vectors to those imputations that are compatible with ====. Moreover, ==== coincides with the usual core if and only if the valuation matrix has a dominant diagonal.====The rest of the paper is organized as follows. In Section 2 we outline the preliminaries on assignment games. Section 3 extends the notion of a dominant diagonal valuation matrix and studies its relationship with core stability. Finally, in Section 4, we extend the notion of ====-compatible subgames, and show that the union of their cores may not be a stable set but that it still satisfies some appealing property.",A note on the relationship between the core and stable sets in three-sided markets,https://www.sciencedirect.com/science/article/pii/S0165489619300010,March 2019,2019,Research Article,257.0
"Giansiracusa Noah,Ricciardi Cameron","Assistant Professor of Mathematics, Swarthmore College, United States,Undergraduate Mathematics and Economics Major, Swarthmore College, United States","Received 7 June 2018, Revised 22 December 2018, Accepted 23 December 2018, Available online 2 January 2019, Version of Record 17 January 2019.",https://doi.org/10.1016/j.mathsocsci.2018.12.001,Cited by (10),"We use the United States Supreme Court as an illuminative context in which to discuss three different spatial voting preference models: an instance of the widely used single-peaked preferences, and two models that are more novel in which vote outcomes have a strength in addition to a location. We introduce each model from a formal axiomatic perspective, briefly discuss practical motivation for each in terms of judicial behavior, prove mathematical relationships among the voting coalitions compatible with each model, and then study the two-dimensional setting by presenting computational tools for working with the models and by exploring these with judicial voting data from the Supreme Court.","A popular view among legal scholars, journalists, and amateur court-watchers is that the nine justices sitting on the bench of the U.S. Supreme Court are driven largely by political ideology (Segal and Spaeth, 2002). To quantify this perspective, one considers each justice as having an “ideal point” on a one-dimensional axis==== ==== and uses this layout in voting models to help understand the ways the justices vote, and in particular the ways the justices group themselves into a majority coalition and a minority coalition. While the majority/minority dichotomy masks the complexity of the concurring and dissenting opinions underlying each decision, it is a simplification that many scholars – ourselves included – are willing to make, so we shall assume that each case has two outcomes, affirm or reverse, and that each justice votes for exactly one of these.====Voting models often assume “single peaked” preferences, which here means the two potential outcomes of a case are placed along the same one-dimensional political axis as the justices’ ideal points, and if a justice is to the left of both outcomes or to the right of both then the justice votes for the closer of the two outcomes. If a justice lies between the two outcomes then the vote depends on the precise shape of the justice’s preference function; since this is difficult to estimate, a natural simplifying assumption is that all the justices use negated distance for their preference function, which means they just vote for the closest outcome to their ideal point on the political axis. The midpoint between the two case outcomes then serves as a dividing point so that all justices to the left of it vote one way and all justices to the right vote the other way. In particular, each majority coalition provided by this model is separated from the minority coalition by a point on the political axis. But how does one then make sense of scrambled situations like the 5-to-4vote in ==== (1965), where the minority coalition consisted of far left Douglas, far right Harlan, the right-leaning Stewart, and the centrist Black? (See Fig. 1.)====The dimensionality of the Supreme Court has been an active topic of exploration among scholars (Brazill and Grofman, 2002, Sirovich, 2003, Edelman, 2004, Lee et al., 2015), and indeed some of the majority/minority divisions that seem illogical from the political spectrum perspective naturally reveal themselves on a second dimension (Fischman and Jacobi, 2016). However, a two-dimensional binary outcome voting model based on Euclidean distance preference functions still only allows for votes in which the majority is separated from the minority by a line in the plane, and while many cases do nicely exhibit this structure, many yet do not (including, for instance, the scrambled 5–4 vote mentioned above). We introduce here two voting models, one which allows more geometrically flexible majority/minority divisions and one which allows the same divisions as the Euclidean distance model but arrives at these divisions in a different manner. These models might be novel, but both authors of this paper come from a mathematical – specifically, geometric – background, so we are less familiar with the voting theory literature; our emphasis then is not on originality, per se, of these two voting models but on comparing the coalitions provided by all three models in two dimensions and presenting methods from computational geometry that allow a practitioner to work with and explore them.====In Section 2 we introduce the three models from a formal mathematical perspective, provide a geometric interpretation, and briefly discuss some motivation for the models in the context of Supreme Court judicial voting behavior. We then prove a theorem relating the coalitions – which is to say, the majority/minority divisions – allowed by each model; this is done for a Euclidean space ==== of arbitrary dimension ====. The section concludes with an illustration of the models in two dimensions on a few case votes from the Supreme Court. In Section 3 we present computational methods for finding all the coalitions allowed by each model for a fixed configuration of voter ideal points, and for one of the models we also show how to estimate the locations of the two case outcomes based on the votes; this too is illustrated with Supreme Court cases. We also mention connections to some topics in discrete/computational geometry, such as ====-sets (Edelsbrunner and Welzl, 1986) and higher order Voronoi diagrams (Lee, 1982). We conclude in Section 4 with a few closing remarks.",Computational geometry and the U.S. Supreme Court,https://www.sciencedirect.com/science/article/pii/S0165489618300842,March 2019,2019,Research Article,258.0
"Bosi Stefano,Desmarchelier David","EPEE, Université Paris-Saclay, France,University of Lorraine, University of Strasbourg, AgroParisTech, CNRS, INRA, BETA, France","Received 4 June 2017, Revised 12 November 2018, Accepted 26 November 2018, Available online 3 December 2018, Version of Record 17 December 2018.",https://doi.org/10.1016/j.mathsocsci.2018.11.001,Cited by (9),"We provide ==== to detect local bifurcations of three and four-dimensional dynamical systems in continuous time. We characterize not only the bifurcations of ==== one but also those of ==== two. For the sake of completeness, we give also the non-degeneracy conditions for each bifurcation. The added value of our methodology rests on its generality. To illustrate the tractability of our approach, we provide two analytical applications of dimension three and four to environmental economics, complemented with numerical simulations.","In dynamic general equilibrium theory, the most popular model is Ramsey (1928). The core of this continuous-time model is a two-dimensional dynamical system. The Ramsey model is characterized by the saddle–path stability of a unique equilibrium. Many dynamic economic models are extensions of the Ramsey model. The introduction of market imperfections or agents’ heterogeneity often increases the dimension of the dynamical system and makes dynamics richer: the non-linearities associated to these imperfections change the stability properties of the steady state and generate more complex attractors such as limit cycles. For example, monetary extensions of the Ramsey model are three-dimensional while two-country general equilibrium models are often four-dimensional. In general, the introduction of an additional building block in some previous extension of the Ramsey model raises the dimension of the dynamical system and makes the economic analysis more difficult.====The stability change of a dynamical system corresponds to a change in some fundamental parameter through a critical value. When the parameter crosses a critical value, a bifurcation takes place. A bifurcation is said to be local when it arises in a neighborhood of an attractor (such as a steady state). A bifurcation generated by a single parameter is said of codimension one. A bifurcation generated by the joint change of two parameters is said of codimension two. In economics, the bifurcation analysis is relevant to reproduce the cycles observed in the time series, from a theoretical point of view. In a seminal contribution, Grandmont (1985) has pointed out that cycles induced by non-linearities through (local) bifurcations are pervasive phenomena that can arise even in the simplest economic model. More recently, Barnett and He (2010) have also highlighted that local bifurcations can appear in macroeconometric models under plausible calibrations.====Our paper addresses the methodological question of necessary and sufficient conditions for local bifurcations of higher-dimensional dynamical systems in continuous time. We provide a simple method to detect bifurcations of codimension one and two in the case of three and four-dimensional dynamical systems using the sum of minors of the Jacobian matrix.==== ====Mathematical textbooks on bifurcation theory provide general conditions on the eigenvalues of the Jacobian matrix to characterize the local bifurcations.==== ==== It is often difficult to compute analytically these eigenvalues. In 2D systems, either in discrete or continuous time, it is usual to study the minors of the Jacobian matrix instead of the eigenvalues to characterize the local bifurcations.==== ==== This methodology has been extended to higher-dimensional systems. In discrete time, Barinci and Drugeon (2017) have applied a geometrical method to characterize the local bifurcations of codimension one of 3D maps generalizing (Grandmont et al., 1998), while Kuznetsov and Sedova (2012) have given necessary and sufficient conditions on the minors of the Jacobian matrix to get local bifurcations of codimension one and two for 3D and 4D maps. Concerning the continuous time, Dockner and Feichtinger (1991) have provided conditions for a Hopf bifurcation to occur in the specific case of a 4D system resulting from a dynamic optimization with two state variables while a general characterization is provided by Asada and Yoshida (2003). More recently, Bosi and Desmarchelier (2017b) have found necessary and sufficient conditions under which codimension one bifurcations occur in 3D system. That is, to the best of our knowledge, there is no equivalent in the literature in continuous time with what Kuznetsov and Sedova (2012) have done in discrete time. The present paper aims at filling the gap, giving necessary and sufficient conditions, on the structure of the minors of the Jacobian matrix to obtain bifurcations of codimension one and two for systems of dimension three and four in continuous time.====Checking non-degeneracy conditions is also necessary to study correctly a bifurcation. While the computation of the eigenvalues refers to the first-order terms of the Taylor expansion, the genericity conditions involve second and third-order terms. Even if the added value of our paper is to provide a simple operational characterization of local bifurcations of codimension one and two to empower the reader interested in economic applications, for the sake of completeness, we give the non-degeneracy conditions for any bifurcation considered. From an analytical point of view, it is sometime difficult to verify these conditions, but standard bifurcation softwares such as MATCONT automatically check them.====As was the case for 2D system in continuous time, computing the minors of the Jacobian matrix of 3D and 4D systems turns out to be easier than calculating the eigenvalues. To illustrate our approach, we give two basic examples of environmental economies of dimension three and four respectively. To convince the reader, the analytical characterizations are complemented with numerical simulations based on the original non-linear systems.====The remainder of the paper is organized as follows. Section 2 characterizes the local bifurcations of three-dimensional dynamical system and provides an economic example with pollution, while Section 3 focuses instead on four-dimensional systems and gives an example with natural capital. Both the economic illustrations of our methodology are complemented with numerical simulations. All the proofs are gathered in the Appendix.",Local bifurcations of three and four-dimensional systems: A tractable characterization with economic applications,https://www.sciencedirect.com/science/article/pii/S0165489618300829,January 2019,2019,Research Article,259.0
"Goetz Renan,Hritonenko Natali,Xabadia Angels,Abdulai Awudu","Department of Economics, University of Girona, Carrer de la Universitat, 10, 17003, Girona, Spain,Dunham College of Business, Houston Baptist University, 7502 Fondren, Houston, TX 77074, USA,Department of Mathematics, Prairie View A&M University, Prairie View, TX 77446, USA,Department of Food Economics and Consumption Studies, Christian Albrechts University of Kiel, Johanna-Mestorf-Str. 5, 24118 Kiel, Germany","Received 23 October 2017, Revised 30 October 2018, Accepted 30 October 2018, Available online 15 November 2018, Version of Record 29 November 2018.",https://doi.org/10.1016/j.mathsocsci.2018.10.004,Cited by (2),"The owner of an asset often transfers the right to use or exploit that asset to an agent in exchange for a rent. A limited time of the license and the failure of the owner’s commitment to compensate the agent for any asset improvement are likely to lead to underinvestment (holdup). In this study, we analyze the optimal length a contract would need to have to maximize the owner’s income in the short- and long-run. We determine the design of a sequence of renegotiation-proof, overlapping, fixed time contracts that allows eliminating the hold-up problem. The obtained outcomes are tested and illustrated on a specific problem (land lease and soil quality). Numeric simulation demonstrates that the most severe version of the hold-up problem arises when the lease contract is not long enough for farmers to make any investment in the soil quality (less than 3 years on calibrated data).","Firms must decide which transactions will take place within the firm itself and which they will have to rely on the markets for. In other words, firms have to define their boundaries. Nobel prize winner Oliver Williamson (1979) considered transaction costs as a critical element in deciding where to draw the boundary between the firm and the market. A significant problem is that market partners often have to make a relationship-specific investment, which can only be recovered within the relationship. If one partner unilaterally decides to end the relationship, then market transactions may be very costly, since investments become sunk costs that cannot be recovered. As a remedy, one may suggest writing a complete contingent contract that safeguards the interests of both parties. However, as argued by Grossman and Hart (1986), the rationality of economic agents is bounded and, thus, prevents the agents from foreseeing all possible future contingencies. Moreover, certain future states cannot be included in a written contract because they cannot be verified by a third party (for example, intangible benefits such as the originality or trendiness of a product, reputation, friendly customer services, or improvements in human capital). Likewise, the non-investing partner has less incentive to renegotiate or accept any demands when the contract has ended if a third party (court) cannot verify the cooperative investment (Harstad, 2012, Segal and Whinston, 2012). The risk that the non-investing partner will take advantage of this situation can result in underinvestment by the investing partner and thus lead to the hold-up problem.====The existing literature on the hold-up problem has only considered time in a stylized manner. Normally, agents can invest in the first period and expect benefits in the second period. A standard model of a contract (Bolton and Dewatripont, 2005) assumes that each period is of a given and identical length, but its duration is not specified. Moreover, investments are frequently modeled not as a stock, but rather as a flow variable. However, real-world examples, like franchise systems or concessions, show that periods of investments and benefits are interrelated and that the dynamics of the investment behavior and realization of the benefits are more complex than portrayed by the standard model. Given this observation, our point of departure is that the agent’s investment behavior depends on the length of the period or duration of the contract, particularly if investment is not a one-time event, but rather a continuous process while the contract is in place. Moreover, the benefits of investments are often not immediate and can only be fully recovered over time. As an extension of the existing literature, our study focuses on the dynamic description of optimal investment behavior in order to determine the optimal design of contracts in terms of fixed-time duration and renewal in the presence of a hold-up problem.====There are many real-world examples that underline the importance of fixed-time contracts and stock variables in a production process. Within this context, we find landlords who do not cultivate their land themselves, but rather lease it out to agricultural producers in the form of a fixed rent or sharecropping contract over a fixed time horizon. Such leasing contracts affect about 40% of all cultivated land in Europe and 60% in North America (Food and Agricultural Organization, 2004). Farmers cultivating the land normally invest in the soil quality as this is important for agricultural productivity and farm output (Abdulai and Goetz, 2014). Another common example is the franchise agreement. The franchiser is the owner of the asset which is often in the form of a brand and its associated reputation. A fixed-time contract authorizes the franchisee to exploit the brand while the contract is in place. The reputation of the brand actively contributes to its sales, while investments by the franchisee, such as training and developing personnel, can help improve the brand’s reputation and increase sales. Another example from the private sector is the concept of concessions. The concessionaire usually signs a fixed-time contract that allows them to operate within the premises of the concession grantor. As in the case of the franchisee, the overall reputation of the premises and the concessionaire’s investments contribute to the success of the business. Examples include business concessions within sports or cultural venues, concessions to retailers to operate in department stores, or concessions to chefs to run restaurants within hotel premises. Within the public sector, we find private companies that enter into agreements with local, regional or national governments to operate public utilities related to water or energy supply, or to provide transport or sanitation services (Worldbank, 2011). A common feature of fixed-time contracts is that they give a company the right to exploit public infrastructure. To maintain and augment its returns, the company needs to invest in the infrastructure, as it forms an essential element in the provision of public services.====The aforementioned examples are all similar in that the owner of an asset offers an agent a fixed-time contract, allowing the agent right to exploit the asset. The asset itself is a building element for providing a service or producing goods. The duration of such contracts is generally limited and not open-ended. Investment by the agent improves the asset, while intensive production tends to degrade it. Open-end contracts can be found in industrial collaboration such as joint ventures, strategic alliances, or start-up businesses (Comino et al., 2010, López-Bayón and González-Díaz, 2010), while the fixed-time format is common for private and public contracts where an asset is involved. Fixed-time contracts frequently take the form of concessions, operating licenses or franchise agreements (Zylbersztajn and Lazzarini, 2005, Brickley et al., 2006).==== ====Our study focuses on the question of how an efficient long-run equilibrium can be achieved through a sequence of time-dependent contracts. These types of contracts often include an automatic renewal clause known as the Evergreen Clause. This allows an agreement to continue for a fixed time if the existing agreement is not renegotiated or properly cancelled with advance notice. Evergreen clauses can be found in both consumer and commercial contracts (Voorhees, 2016) and examples of such can be found in the agricultural sector (Goodhue et al., 2003) or forest sector (Townsend and Young, 2005) or in health care provision (Prives, 2013). If the time-dependent contract does not include an evergreen clause it is either in form of an open-end contract, i.e., no renewal but the duration of the contract is open, or in form of a fixed-time contract.====Guriev and Kvasov (2005) analyzed how the long-run equilibrium can be achieved by comparing fixed-time contracts and open-end contracts. As a result of their modeling approach, they find that fixed-time contracts cannot induce efficient investment and one has to resort to open-end contracts. In contrast, our study shows that under fairly general conditions even a sequence of renegotiation-proof fixed-time contracts can induce efficient investment. Moreover, fixed-time contracts seem to be employed more frequently than open-end contracts. If the long-run equilibrium can be achieved while the first contract is in place, then the sequence of identical fixed-time contract is able to replicate the first-best solution. However, if the long-run equilibrium cannot be achieved during this time, then it is optimal to either employ a sequence of non-identical contracts or offer no contract at all.","The dynamics of productive assets, contract duration and holdup",https://www.sciencedirect.com/science/article/pii/S0165489618300817,January 2019,2019,Research Article,260.0
"Cohen Chen,Levi Ofer,Sela Aner","Department of Economics, Ashkelon Academic College, Ashkelon, Israel,Department of Mathematics and Computer Science, The Open University of Israel, Israel,Department of Economics, Ben-Gurion University of the Negev, Israel","Received 8 January 2018, Revised 3 September 2018, Accepted 27 October 2018, Available online 13 November 2018, Version of Record 23 November 2018.",https://doi.org/10.1016/j.mathsocsci.2018.10.003,Cited by (1),"We study all-pay auctions with discrete strategy sets and analyze the equilibrium strategies when players have asymmetric values of winning as well as asymmetric effort constraints. We prove that for any number of players if one of them has the highest effort constraint then, independent of the players’ values of winning, he is the only player with a positive expected payoff. However, when two players have the same highest effort constraint then they do not necessarily have the highest expected payoffs.  By several examples we show a significant distinction between the equilibrium strategies of two players and a larger number of players, particularly when the player with the highest effort constraint is not unique.","The all-pay auction is one of the main contest forms in the literature on contest theory. In all-pay auctions, only the player with the highest effort receives the prize, but all players, including those who do not win the prize, incur costs as a result of their efforts. Most of the literature has focused on all-pay auctions with two players (see, for example, Hillman and Samet, 1987, Hillman and Riley, 1989, Baye et al., 1996). Che and Gale (1998) calculated the bidding equilibrium of the all-pay auction with two players having the same effort constraint, and Hart (2016) characterized the equilibrium strategies in the two-player all-pay auction with different effort constraints.==== ==== However, the analysis of the all-pay auction with more than two effort-constrained players has not as yet been done. Hence, the aim of this paper is to shed light on this contest when there are more than two players with asymmetric effort constraints, and particularly to show that the behavior of these players might be completely different than in the all-pay auction with only two players. We demonstrate by several examples that the behavior of the players in all-pay auctions with multiple players and asymmetric effort constraints breaks some well-known conventions concerning the all-pay auction with and without effort constraints. Moreover, these examples show that in a case with more than two effort-constrained players, the all-pay auction is not straightforward as in the case with two players since if there are more than two players it is hard to figure out what the players’ expected payoffs are.====Formally, we consider all-pay auctions with discrete strategy sets, namely, the players have effort constraints and finite strategy sets. The main difference between our model with discrete strategies and the standard all-pay auction with continuous strategies is that in the standard model the probability of a tie (the players exert the same effort) is zero, while in our model there is a positive probability for a tie (see Baye et al., 1994, Cohen and Sela, 2007, Dechenaux et al., 2006).====We first analyze the equilibrium strategies in the two-player all-pay auction. The equilibrium in this model with asymmetric effort constraints is not unique. Moreover, even when the smallest money unit converges to zero, the equilibrium in our model and the standard all-pay auction is not necessarily similar. However, independent of the size of the smallest money unit, we characterize common properties that provide a uniform framework to all the equilibrium points. In particular, we show that when the smallest money unit converges to zero in any equilibrium point, independent of the players’ values of winning, the expected payoff of the player with the lower effort constraint converges to zero, while the expected payoff of the player with the higher effort constraint converges to the difference of this player’s value of winning and his opponent’s effort constraint.====In contrast to the two-player all-pay auction, when there are more than two players with asymmetric effort constraints, all the players may be active where each of them has a completely different strategy as well as a different expected payoff and a different probability to win the contest. However, for each all-pay auction with multiple asymmetric effort-constrained players, by mathematical methods, we are able to numerically solve systems of non-linear equations and derive the equilibrium strategies . Using these equilibrium calculations we find that several well-known facts about the two-player all-pay auction with and without effort constraints no longer hold. For instance, we show by examples that in all-pay auctions with more than two effort-constrained players, a player may have a completely different expected payoff for different equilibrium strategies. We also show that given an all-pay auction, in one equilibrium player ==== is the only player with a positive expected payoff, and in another player ==== is the only player with a positive expected payoff.====In the all-pay auction with multiple asymmetric effort-constrained players it is not clear whether the players’ values of winning or their effort constraints have a higher effect on the players’ expected payoffs. Although the players’ equilibrium strategies are quite complex we provide a clear answer to this question by showing that when the smallest money unit converges to zero, independent of the players’ values of winning, the player with the highest effort constraint is the only one with a positive expected payoff. However, when players are weakly asymmetric such that more than one player has the highest effort constraint, there is a major difference between the all-pay auction with more than two players and that with either two symmetric or asymmetric players: In the two-player contest the values of the players do not have any effect on who the player with a positive expected payoff is but only on the level of his expected payoff. The parameters that affect which player has the higher expected payoff in the two-player contest are the effort constraints, in that the player with the higher effort constraint is the only one with a positive expected payoff. On the other hand, in the all-pay auction with more than two players the value of a player as well as his effort constraint will affect whether this player has a positive expected payoff or not. In other words, if players are weakly asymmetric and a player has a sufficiently high value of winning, even if he does not have the highest effort constraint, namely, there are at least two players with higher effort constraints, he might still have a positive expected payoff.====The rest of this paper is organized as follows: In Section 2 we introduce our model of the all-pay auction with discrete strategies. In Section 3 we analyze the all-pay auction with two asymmetric effort-constrained players. In Sections 3 Asymmetric two-player contests, 4 Asymmetric multi-player contests we analyze the all-pay auction with a larger number of effort-constrained players who are asymmetric and weakly asymmetric. Section 5 concludes. The proofs appear in the Appendix.",All-pay auctions with asymmetric effort constraints,https://www.sciencedirect.com/science/article/pii/S0165489618300805,January 2019,2019,Research Article,261.0
Reynès Frédéric,"Sciences Po OFCE — French Economic Observatory, France,NEO — Netherlands Economic Observatory, Netherlands,TNO — Netherlands Organization for Applied Scientific Research, Netherlands","Received 26 October 2017, Revised 13 April 2018, Accepted 22 October 2018, Available online 3 November 2018, Version of Record 20 November 2018.",https://doi.org/10.1016/j.mathsocsci.2018.10.002,Cited by (13),By defining the Variable Output ,"In their influential contribution to economic theory, Cobb and Douglas (1928) introduced the production function that was named after them. Since, the Cobb–Douglas (CD) function has been (and is still) abundantly used by economists because it has the advantage of algebraic tractability and of providing a fairly good approximation of the production process. Its main limitation is to impose an arbitrary level for substitution possibilities between inputs. To overcome this weakness, important efforts have been made to develop more general classes of production function with as a corollary a strong increase in complexity (for a survey see e.g. Mishra, 2010).====Arrow et al. (1961) introduced the Constant Elasticity of Substitution (CES) production function which has the advantage to be a generalization of the three main functions that were used previously: the linear function (for perfect substitutes), the Leontief function (for perfect complements) and the CD function, which assume respectively an infinite, a zero and a unit elasticity of substitution (ES) between production factors.====A limitation of the CES function is known as the impossibility theorem of Uzawa (1962) - McFadden (1963) according to which the generalization of the class of function proposed by Arrow et al. (1961) to more than two factors imposes a common ES between factors. To allow for different degrees of substitutability between inputs, Sato (1967) proposed the approach of nested CES functions which has proved very successful in general equilibrium modeling and econometric studies because of its algebraic tractability. The substitution between energy and other inputs is one of the main applications (e.g. Prywes, 1986, Van der Werf, 2008, Dissou et al., 2015). Although this method is flexible, substitution mechanisms remain constrained and the choice of the nest structure is often arbitrary.====To overcome this limit, several “flexible” production functions have been proposed such as the Generalized Leontief (GL) (Diewert, 1971) and the Transcendental Logarithmic (Translog) function (Christensen et al., 1973).==== ==== These are second order approximations of any arbitrary twice differentiable production functions.==== ==== They have the advantage not to impose any constraint on the value of the ES between different pairs of inputs but their use is much more complex. This at least partly explains their little success in general equilibrium modeling compared to the nested CES approach.==== ==== Two difficulties are particularly limiting:====Whereas the existing literature has attempted to overcome the weakness of the CD function by proposing more general but also more complex alternatives, we remain here in the tractable framework of the CD function and investigate the conditions under which it can be used as a flexible function. We use the fact that any homogeneous production function can be written under a specification that is very close to a CD function. We define this specification as the Variable Output Elasticities CD function because compared to the original CD function, the output elasticities are not necessarily constant. The Output Elasticity (OE) of a given input (e.g. labor, capital or energy) measures the percentage change of output induced by the percentage change of this input. The specification of the OE is importantly influenced by the assumptions regarding the level of ES between inputs (e.g. Ferguson, 1969, Charnes et al., 1976; or Kümmel et al., 1985, Kümmel et al., 2002). The OE is constant only if the ES between every input is equal to one which is the assumption of the original CD function. For any other configuration of ES, the OE varies and depends on the relative quantities of each input and on the ES between inputs. We use here this property to characterize a relatively general class of production functions that has the advantage to combine the linear tractability of the CD function with a high level of flexibility in terms of substitution possibilities between inputs. More specifically, we shall see that this approach has the following advantages:====Section 2 defines formally the Variable Output Elasticities CD (VOE-CD) function in the general case of ==== inputs and shows that the CD function can be seen as a flexible function generalizing any homogeneous function. Section 3 shows that the VOE-CD provides a generalization of the CES function where the ES between each pair of inputs are not necessarily the same. Section 4 derives the demand for inputs that minimizes the production costs in the case of a VOE-CD production function. Section 5 investigates the particular case of a nested CES function with 3 inputs (e.g. capital–labor–energy) and shows that its VOE-CD formulation allows for a straightforward analysis of the substitution properties of a system of nested CES functions. Section 6 concludes.",The Cobb–Douglas function as a flexible function: A new perspective on homogeneous functions through the lens of output elasticities,https://www.sciencedirect.com/science/article/pii/S0165489618300799,January 2019,2019,Research Article,262.0
Dur Umut Mert,"North Carolina State University, United States","Received 8 September 2014, Revised 13 July 2018, Accepted 26 August 2018, Available online 17 October 2018, Version of Record 25 June 2019.",https://doi.org/10.1016/j.mathsocsci.2018.08.004,Cited by (12),"Many school districts in the U.S. assign students to schools via the Boston mechanism. The Boston mechanism is not strategy-proof, and it is easy to manipulate. We slightly modify the Boston mechanism and show that the modified version outperforms the Boston mechanism in terms of strategy-proofness. In particular, the Boston mechanism is manipulable whenever the modified version is, but the modified version is not necessarily manipulable whenever the Boston mechanism is. We define a weaker form of consistency and characterize the modified Boston mechanism by this weaker form and a new axiom called respect of priority of the top-ranking students.","Beginning with Abdulkadiroğlu and Sönmez (2003), economists have pointed out the major deficiency of the Boston mechanism (BM): the BM creates incentives for students to misreport their true preferences.==== ====Abdulkadiroğlu and Sönmez (2003) propose two strategy-proof mechanisms==== ====and suggest that school districts adopt one. Thanks to the following experimental and theoretical papers focusing on this drawback of the BM, it was replaced with a strategy-proof mechanism in Boston. However instead of abandoning the BM, many school districts still use it.==== ====Moreover, the BM was first replaced with a strategy-proof mechanism in Seattle but was then adopted again in 2011 (Kojima and Ünver, 2014).====Given school districts’ tendency towards using the BM, a natural question is whether we can improve the performance of the BM with minor changes. In this paper, we show that with a slight modification it is possible to decrease the level of gaming under the BM. In Round ==== of the BM, the unassigned students apply to the ====th choice school in their preference list. This may cause students to apply to a school that has filled all of its available seats in the previous rounds, or a school that considers them unacceptable. Hence, these students waste their opportunity to be assigned in that round by applying to a school that will reject them independently of the students applying to that school in that round. To fix this point in each round we allow a student to apply to his most preferred school among the schools that consider him acceptable and have available seats.====The modified version of the BM inherits desired features of the BM. It is constrained Pareto efficient and satisfies a weaker form of consistency==== ====(Theorem 1). The modified version of the BM is not strategy-proof. However, by using the notion introduced by Pathak and Sönmez (2013), we show that the modified version performs better than the BM in terms of vulnerability to manipulation (Theorem 3). In particular, whenever a student can manipulate the modified BM then the BM can be manipulated by some students. Moreover, there exist problems in which the modified BM cannot be manipulated but the BM can. In addition, any Nash equilibrium outcome of the preference revelation game associated with the modified BM leads to a stable matching under the true preferences (Theorem 4). We demonstrate that, except for the DA mechanism, all other members of the application rejection mechanisms introduced by Chen and Kesten (2013) do not perform better than the modified BM in terms of immunity to manipulation (Proposition 3). This modification can be thought of as a design that improves upon the BM. Moreover, the modified BM performs better than the DA mechanism based on the number of students assigned to their reported first choice under naive submission (Proposition 1). In addition to the standard school choice model, we study an environment composed of sincere and sophisticated students. We show that all sophisticated students (weakly) prefer the Pareto dominant Nash equilibrium outcome of the preference revelation game induced by the BM to the one induced by the modified BM (Theorem 5). We compare the fraction of students who can manipulate the BM and the modified BM by running simulations under various scenarios. The simulation results support better performance of the modified version in terms of vulnerability to manipulation.====We characterize the modified BM by using two axioms: the weak consistency and a new axiom that we introduce in this paper (Theorem 2). We weaken the consistency axiom by only requiring the remaining students to be assigned to the same school whenever all the students assigned to their most preferred schools among the ones with available seats and considering them acceptable are removed along with their assignment. The weak consistency is satisfied by any consistent mechanism. Moreover, the DA mechanism, which does not satisfy consistency, is weakly consistent. The axiom that we introduce is similar to the main axioms used in the characterization of the BM by Kojima and Ünver (2014) and Afacan (2013). Kojima and Ünver (2014) and Afacan (2013) introduce ==== ==== ====and ==== ==== ====respectively. We say that a mechanism ==== if whenever student ==== is one of the top ==== ranked students in the priority order of school ==== among the acceptable students considering ==== as the most preferred school in the set of schools with at least one available seat, then ==== is assigned to ==== by the mechanism.==== ====Respect of the priority of top-ranking students is neither weaker nor stronger than the axioms introduced by Kojima and Ünver (2014) and Afacan (2013). For instance, the BM respects both preference rankings and priorities however it does not respect the priority of top-ranking students (Example 2). On the other hand, the modified BM respects the priority of top ranking students (Theorem 1) but it does not respect both preference rankings and priorities (Example 1).====Alcalde (1996) introduces a version of the BM called Now-or-Never mechanism in the marriage problem. Similarly to the modified BM introduced here, under the Now-or-Never mechanism, in each round each agent applies to the most preferred partner among the remaining ones. However, agents might still apply to partners who consider them unacceptable. Alcalde shows that the Now-or-Never mechanism can implement all the stable matchings in undominated Nash equilibria. This paper is different in that it focuses on the many to one matching problem and compares two mechanisms based on their vulnerability to manipulation and gives a characterization of the modified BM. In the school choice environment, Miralles (2008) introduced the generalized version of the Now-or-Never mechanism in the many to one school choice problem and named it as the Corrected Boston mechanism. By using simulations, Miralles (2008) shows that under the Corrected Boston mechanism the sincere students will be better protected against the strategic students and thus will be better off compared to the BM. In this paper, we use an environment of sincere and strategic agents to show that strategic students weakly prefer the Pareto dominant Nash equilibrium outcome of the BM to the Pareto dominant Nash equilibrium outcome of the modified BM (Theorem 5). Mennle and Seuken (2014) generalize the Now-or-Never mechanism in many to one matching problem without priorities and call it Adaptive Boston mechanism. They compare its properties with the BM. Unlike this paper, they do not provide a characterization or an equilibrium analysis in their paper. In a recent paper, Harless (2016) focuses on the generalized version of the Now-or-Never mechanism in the school choice setting and studies its solidarity and robustness properties of it. All these mechanisms were developed independently, but it is worth mentioning that the modified BM is less manipulable than the others (Proposition 2).====This paper relates to the literature comparing mechanisms based on their vulnerability to manipulations. The notion used to compare two mechanisms which are not strategy-proof is introduced by Pathak and Sönmez (2013). Chen and Kesten (2013) use this notion to compare the vulnerability to manipulation of a class of mechanisms including the BM and the DA. This paper also relates to the works on the axiomatic characterizations of matching mechanisms. By using respecting preference rankings, consistency, resource monotonicity,==== ====and an invariance property, Kojima and Ünver (2014) characterize the set of mechanisms coinciding with BM for some priority order. Afacan (2013) shows that BM is the unique individually rational mechanism that also respects both preference rankings and priorities. There are also papers characterizing other school choice mechanisms. Kojima and Manea (2010) and Morrill (2013a) characterize the DA mechanism. Abdulkadi̇roglu and Che (2010), Morrill (2013b), and Dur (2012) provide characterizations of the TTC mechanism.",The modified Boston mechanism,https://www.sciencedirect.com/science/article/pii/S0165489618300647,17 October 2018,2018,Research Article,263.0
Sato Norihisa,"Faculty of Economics, Nagoya Gakuin University, 1-25 Atsuta-Nishimachi, Atsuta, Nagoya, Aichi, 456-8612, Japan","Received 28 February 2018, Revised 6 August 2018, Accepted 1 October 2018, Available online 11 October 2018, Version of Record 22 January 2019.",https://doi.org/10.1016/j.mathsocsci.2018.10.001,Cited by (3),. We then establish a similar characterization in the case of fixed alternatives by introducing a stronger version of strategy-proofness. The latter result answers an open problem left in M. Vorsatz (2007).,"Approval voting is a voting method that allows voters to vote for as many alternatives as they wish. Since the seminal work of Brams and Fishburn (1978), the method has been studied by many researchers from various perspectives. This paper adds to the literature by exploring the possibility of axiomatic characterization of approval voting under certain settings. More specifically, we aim at answering an open question raised by Vorsatz (2007, Footnote 5). Assuming a variable set of alternatives and a variable electorate with dichotomous preferences,==== ====Vorsatz (2007, Theorem 1) showed that approval voting is the unique social choice rule that satisfies the following six axioms: ==== ==== ==== ==== ====. Although variable sets of alternatives and voters are reasonable assumptions,==== ====it was left open in Vorsatz (2007) whether they are crucial in his characterization. In other words, it is an open question whether or not a similar characterization can be obtained when the sets of alternatives and voters are given and fixed. Obviously, this question can be answered only by examining the case of fixed alternatives and voters.====As a first step to accomplish our objective, we consider the case in which the set of alternatives is variable while the set of voters is fixed. We show that in this setting, approval voting can be characterized by the first five axioms mentioned above, namely, ====, ====, ====, ====, and ====. This result includes Vorsatz’s characterization theorem as a corollary, and more importantly, shows ==== to be redundant in his characterization. In addition, our result can be seen as a counterpart of Corollary 2 of Fishburn (1978). He showed that when the set of alternatives is fixed while the set of voters is variable, approval voting can be characterized by ====, ====, ====, and ====, which is an axiom specific to models with a variable electorate.==== ====(See also Table 1)====Encouraged by the above result, we proceed to the case in which the set of alternatives and the set of voters are both fixed. More specifically, we consider whether or not the four axioms ====, ====, ====, and ==== characterize approval voting. A little surprisingly, the answer is in the negative, i.e., we can find another social choice function that satisfies the four axioms. However, a further observation on the manipulability of the function suggests that a more stringent version of strategy-proofness, which we call ==== or ====, be worth considering. Under ====, a voter may have incentive to falsify his or her preference over alternatives when he or she has a certain preference over possible selection outcomes, whereas, under ====, no voter has such an incentive regardless of what his or her preference over selection outcomes is. It then turns out that approval voting is the unique social choice function that satisfies ==== together with ====, ====, and ====. With this second characterization result, we conclude that the open problem in Vorsatz (2007) is positively answered.====The rest of this paper is organized as follows. In Section 2, we introduce notations and definitions. In Section 3, we characterize approval voting in the setting of a variable set of alternatives and a fixed electorate. We also show the redundancy of ==== in Vorsatz’s (2007) characterization theorem. In Section 4, we show that ====, ====, ====, and ==== do not necessarily characterize approval voting in the case of fixed alternatives and voters. Then, we introduce ====, and establish the second characterization result. Finally, the Appendix provides the proofs of the results.",Approval voting and fixed electorate with dichotomous preferences,https://www.sciencedirect.com/science/article/pii/S0165489618300702,January 2019,2019,Research Article,264.0
"Konno Tomohiko,Ioannides Yannis M.","National Institute of Information and Communications Technology, Japan,Department of Economics, Tufts University, Medford, MA 02155, USA","Received 8 April 2018, Revised 30 August 2018, Accepted 6 September 2018, Available online 10 October 2018, Version of Record 16 November 2018.",https://doi.org/10.1016/j.mathsocsci.2018.09.003,Cited by (1)," function of strategies of distant players, and the expected utility. We study how the probability of adopting a cooperative strategy in a prisoner’s dilemma game and the probability of adopting Pareto efficient strategies in a cooperation game are affected by changes in the parameter that expresses payoff-responsiveness.","Almost all economic and social activities involve spatial proximity. A retailer, a supermarket, a convenience store, a restaurant, ====, compete with their neighbors. A retailer engages in price competition with its neighbors. A restaurant decides whether or not to renovate taking account of the competition with its neighboring restaurants. One chooses whether to open a shop at a location or not, taking consideration of the competition with its neighbors if they exist. Spatial proximity is not limited to economic activities only. For example, patterns of diffusion and changes of languages and cultures can be also analyzed as spatial coordination games. Changes and mutual influences propagate spatially.====We analyze the decision problems involving interactions over space by investigating a spatial game in which players play games only with their neighbors and choose discrete strategies according to the logit response model Luce, 1959, Anderson et al., 1992, Train, 2003. The associated logit probabilities are endogenous responses that express that players are not perfectly rational. Their evaluation of the payoffs of their actions is subject to error.====The specific model that we will study in the present paper is a simple spatial logit response two-strategy game with general symmetric payoffs. Several previous authors have studied local interactions models of the logit type that we address in this paper. Several authors have drawn attention to the fact that it is mathematically equivalent to the Ising model, which was introduced by statistical physics Ising, 1925, Baxter, 1982, Brézin, 2010, Nishimori and Ortiz, 2011, and has been adopted by numerous applications Weidlich, 1972, Weidlich, 1991, Sznajd-Weron and Sznajd, 2000. Models relying on the machinery of random fields were first introduced by Föllmer, and then adopted by others Föllmer, 1974, Blume, 1993, Lux, 1995, Lux, 1997, Brock and Durlauf, 2001, Hautsch and Klotz, 2003, Bayer and Timmins, 2005, Ioannides, 2006, Zanella, 2007, Chang, 2007, Levy, 2008. However, Blume must be credited with recognizing that Gibbs distributions admit a (spatial) logit interpretation.====The progress to date has been incomplete. Many applications emphasize steady-state properties and rely on mean-field approximations or numerical simulations. See, e.g., a mean-field approximation of complex network structures in Konno (2011). The mean-field approximation does not account for the richness associated with the underlying spatial structure. Mean-field approximations simply fall short of the exact solution in closed-form proposed by the present paper. It is thus attractive to take full advantage of the techniques developed by statistical physics to obtain exact solutions. Although numerical simulations afford us some intuition, exact solutions help us to understand the models in full depth.====In the present paper, we adapt the model to the requirements of Glauber dynamics of the one-dimensional Ising model (Glauber, 1963) and derive the exact solution of the steady state employing statistical physics methods. We will derive exact solutions for the probabilities that a player chooses different strategies, the strategy correlation function ==== (the effect of a strategy chosen by a player on the strategies chosen by the players at a graph-distance ====), and the expected utility of the decision process of the typical player. We show how the probability of adopting a cooperative strategy in a prisoner’s dilemma game and the probability of adopting a Pareto efficient strategy in a cooperation game, respectively, change in response to changes in ==== the payoff-responsiveness parameter that reflects the dispersion of the underlying stochastic shocks that affect player valuations. We also derive the probabilities in the limit as ==== goes to infinity.====Having addressed the solution to spatial logit response games, we show that there exists a formal similarity with binary discrete choice models with social interactions. Taking off from Brock and Durlauf (2001), Ioannides, 2004, Ioannides, 2006 proposed an interactive discrete choice model, with agents’ acting under full information of other agents’ decisions.",The exact solution of spatial logit response games,https://www.sciencedirect.com/science/article/pii/S0165489618300684,January 2019,2019,Research Article,265.0
"Ding Fangli,Hong Shihuang,Jiao Zhenhua,Luo Xinggang","School of Science, Hangzhou Dianzi University, Hangzhou, 310018, China,School of Business, Shanghai University of International Business and Economics, Shanghai, 200433, China,Management School, Hangzhou Dianzi University, Hangzhou, 310018, China","Received 25 January 2018, Revised 11 June 2018, Accepted 18 July 2018, Available online 22 September 2018, Version of Record 22 January 2019.",https://doi.org/10.1016/j.mathsocsci.2018.07.002,Cited by (0),"In (Ju et al., 2018) Ju et al. investigate the responsiveness of affirmative action in school choice. For the minority reserves type affirmative action they introduce a new matching mechanism and claim that their mechanism is responsive to the minority reserves policy (Theorem 1 in Ju et al., 2018). However, we construct a counterexample, which shows that it is not the case.","School choice programs aim to give students the option to choose their school. At the same time, underrepresented students should be favored to close the opportunity gap. Currently, many school districts in the United States have affirmative action policies to favor minority students and help them attend more preferred schools.====The majority quota is a traditional type of affirmative action. This type of policy gives minority students higher chances to attend more preferred schools by ==== at schools. The majority quota affirmative action policy, however, may hurt all the minority students and result in avoidable inefficiency. To circumvent these shortcomings caused by majority quota, ==== propose another type of affirmative action called “minority reserves” affirmative action. This type of policy is to ====, and to require that a reserved seat at a school can be assigned to a majority student only if no minority student prefers that school to her assignment.====Since the affirmative action policies in school choice aim to improve the welfare of the minority students, a satisfactory assignment mechanism should have the following property: running such a mechanism, a stronger affirmative action policy should not hurt a minority student without benefiting anyone of the minority students. If an assignment mechanism satisfies this property, then we say that it is ====. Unfortunately, for the responsiveness of affirmative actions, much more impossibility results than positive results have been obtained in the literature.  ==== shows that neither DA (student-proposing deferred acceptance algorithm) nor TTC (top trading cycles algorithm) is responsive to the majority quota affirmative action. For the responsiveness of Boston mechanism to majority quota policy, ==== also obtain negative result. For the minority reserves affirmative action, ==== show that the Boston mechanism is responsive.  ==== introduce the deferred acceptance algorithm with minority reserves (DA====) for school choice with the minority reserves affirmative action. Although DA==== satisfies desirable efficiency property, it is not responsive to the minority reserves affirmative action. More recently, ==== propose an efficiency-improved deferred acceptance mechanism with minority reserves (EIDA====) and claim that EIDA==== is responsive to the minority reserves affirmative action. This note provides a counterexample to show that it is not the case.",Corrigendum to “Affirmative action in school choice: A new solution” [Math. Social Sci. 92 (2018) 1–9],https://www.sciencedirect.com/science/article/pii/S0165489618300532,January 2019,2019,Research Article,272.0
