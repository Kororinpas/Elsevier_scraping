name,institution,publish_date,doi,cite,abstract,introduction,Title,Url,Time,Year,Type,Unnamed: 0
Monroy-Gómez-Franco Luis,"Department of Economics, University of Massachusetts, Amherst, United States","Received 8 February 2023, Accepted 30 May 2023, Available online 3 June 2023, Version of Record 8 June 2023.",https://doi.org/10.1016/j.rie.2023.05.004,Cited by (0),"In this paper, I show that the decomposition of intergenerational persistence indicators into their structural and positional components offers a clearer understanding of the determinants of heterogeneity in subnational mobility rates. This constitutes a departure from the current consensus in estimating mobility rates at the subnational level in economics. Applying this approach to the Mexican case, I show no significant differences in positional mobility across the country's regions. This contrasts with the existing results, particularly regarding ==== in the country's southern region.","One of the most active areas of economics research on intergenerational social mobility is the study of intranational heterogeneity in the patterns and determinants of mobility. However, existing research==== has left aside the analysis of differences in positional mobility across a country's regions. In this paper, I contribute to filling this gap by providing the first set of strictly positional measures of regional intergenerational mobility for Mexico. In contrast with previous estimations of regional patterns of intergenerational mobility (Vélez-Grajales et al., 2018; Delajara and Graña, 2018; Monroy-Gómez-Franco and Vélez-Grajales, 2021; Delajara et al., 2022), I do not find differences in rank persistence and aggregate persistence measures for the Mexican regions. The reason for the difference in results is that previous estimates employed as reference group the national distribution, which leads to a mixing of the structural and positional mobility patterns.====Conceptually, positional intergenerational mobility refers to a change in the position occupied by a person in their cohort's distribution of the outcome of interest with respect to the one occupied by their parents in their cohort's distribution (Jäntti and Jenkins, 2015). This mobility concept is also referred to in the literature as “exchange”==== and “relative====” mobility. Another concept of mobility is that of structural mobility, which refers to changes in the shape of the distribution of the current cohort with respect to that of their ancestors. In other words, changes in the outcomes associated with each position across parents and children (Treiman, 1970; Markandya, 1982, 1984).====These two mobility concepts represent an analytical decomposition of the observed changes in economic resources between one generation and the previous one, as proposed by Treiman (1970) and shown by Markandya (1982, 1984). However, the mainstream economics framework for the analysis of economic mobility, represented by the works of Becker and Tomes (1979, 1986) and Becker et al. (2018), does not consider this decomposition to be relevant, as it assumes that only absolute changes in consumption matter for the personal welfare of a person, and positional concerns play no role in it. Although the empirical literature in some cases still focuses on the analytical distinction between each type of intergenerational mobility when analyzing mobility patterns====, such concerns have remained largely absent from the discussion regarding differences in mobility patterns within a country.====A case in point is the seminal work by Chetty et al. (2014, 2015). Chetty et al. (2014, 2015) introduced the use of nationally defined ranks as the dimension of analysis upon which the different regional patterns of intergenerational persistence are to be compared. Although originally motivated by the data restrictions faced by the authors, the approach is also consistent with a hybrid interpretation of intergenerational mobility in which the concepts of structural and positional mobility are mixed. On the one hand, changes in the position of a household from a particular region in the national distribution between two generations imply changes in the absolute level of resources accessible to that household, thus implying structural mobility. But on the other hand, it also captures persistence (or the lack thereof) at a specific part of the national distribution, thus implying positional mobility with respect to the national distribution but not necessarily the regional distribution.====The main limitation of this approach is that, by definition, it limits the possibility of identifying if the main driver of the observed intergenerational mobility patterns is structural or positional mobility. In this paper, I seek to contribute to the existing literature on regional differences in social mobility by showing that focusing on positional mobility can provide a deeper understanding of regional differences. This implies opening a research agenda that has not been explored in the literature while complementing the existing findings for the Mexican case.====The existing evidence on Mexico suggests that it is an interesting case of study to analyze if there are differences between the regional patterns observed using a hybrid mobility concept and those observed when a positional approach is taken. First, there is evidence of the regional or local character of the capacity of elites to modify the institutional order to guarantee their persistence at the top (Garfias, 2018; Albertus et al., 2016). Secondly, experimental evidence identifies the presence of positional concerns in the realms of income and education (Castilla, 2012; Esposito and Villaseñor, 2019). And thirdly, there is a substantive body of work on regional mobility patterns that relies on the hybrid concept of intergenerational mobility (Monroy-Gómez-Franco and Corak, 2019; Monroy-Gómez-Franco and Vélez-Grajales, 2021; Delajara et al., 2022; Delajara and Graña, 2018).====The rest of the paper proceeds as follows. In the following section, I discuss the importance of analyzing positional intergenerational mobility at the subnational scale, emphasizing how it complements the existing literature on regional differences in mobility. Next, I describe the different measures of intergenerational mobility used in the paper, emphasizing their positional interpretation. In the fourth section of the paper, I describe the main characteristics of the database employed, as well as the construction process of the outcome variable. The dataset can be considered an example of the type of data set employed for social mobility analysis in contexts where panel databases are non-existent or non-intergenerational. After that, I present the results for positional mobility across Mexican regions, showing how they complement the existing results and provide a better insight into the determinants of regional differences in mobility. Finally, I discuss some final remarks and future avenues of work.",The importance of positional mobility for regional comparisons,https://www.sciencedirect.com/science/article/pii/S1090944323000406,3 June 2023,2023,Research Article,0.0
Amedanou Yawovi Mawussé Isaac,"LASTA & Department of Economics, University of Rouen Normandie, Rouen, France","Received 4 March 2023, Accepted 12 May 2023, Available online 27 May 2023, Version of Record 7 June 2023.",https://doi.org/10.1016/j.rie.2023.05.003,Cited by (0),–====. The impact of PPP investments is significantly higher than that of pure public investments. The evidence also shows that the positive impact of PPP investments strengthens economic growth as the public debt grows to a point where there is no longer any significant pro growth impact.,"Number of countries are showing a growing interest in Public–Private Partnerships (PPP) contracts that involve collaboration between a government agency and a private entity that can be used to design, finance, build and operate projects, such as social and economic infrastructure. Financing development is a real challenge for developing countries. To face this, public entities are stepping up partnerships with private companies to finance infrastructure projects in the form of PPPs (Eggers and Startup, 2006) mostly in developing countries which have registered a significant increase in private sector participation in funding and building infrastructure since the 1990s (Iossa and Martimort, 2009). More broadly speaking, the use of PPPs provides a larger mobilization of financial resources towards the funding of public infrastructure, which are crucial links in the potential for long-term growth.====From an economic point of view, PPPs are justified by their ability to efficiently allocate risks between public and private partners, and hence provide incentives for better public infrastructure delivery. Additionally, PPP contracting can allow savings to be reallocated to long-term investments for the financing of the real economy. In the latter case, Arezki et al. (2016) confirms that matching long-term savings with bankable PPP projects will optimize resource allocation and stimulate economic growth. These arguments suggest that public–private co-investments constitute an alternative source of financing for economic growth. Guerguil and Keen (2014) and Abiad et al. (2014) made the remark that since the early 1990s in both middle and low income countries, PPPs have raised as an alternative source of financing to scale up public capital stock.====Economic theory recognizes the provision of public goods and services by government (Musgrave, 1959) given the inability of markets to provide public goods, internalize externalities and cover costs in cases where significant economies of scale exist. Such public goods and services cannot be provided by the market because private operators do not have the skills to exclude free riders or to charge users a competitive price. Thus traditionally, governments have built, maintained and rehabilitated physical infrastructure such as roads, ports and airports, as well as telecommunications and electricity networks, which are essential for most economic activities. For this purpose, governments finance economic and social infrastructure out of public investment by levying taxes, or when difficulties arise in increasing tax revenues, they resort to debt to support public investment and boost economic growth.====The needs for infrastructure in developing countries are currently, especially in emerging economies. Yet, developing countries face an unresolved challenge in infrastructure provision and its funding: large economic and social infrastructure needs, weak tax mobilization and a high level of public debt. It is clear from the foregoing that the latter two limit the sources of funding for public investment. Faced with such a complex equation, governments are increasingly turning to public–private partnerships, directing private sector funds towards the financing of public goods and services. It is worth noting that Hammami et al. (2006) have already drawn attention to the fact that PPPs tend to be more common in countries where governments suffer from heavy debt burdens. Moreover, Watts et al. (2000) have confirmed that the use of PPPs for funding social infrastructure projects will help reduce the overall level of government borrowing and help governments maintain and improve current service levels.====However, it is important to keep in perspective that PPPs do not always avoid long-term debt overhang. In general, PPPs allow governments to avoid or defer infrastructure spending without giving up their benefits. It may therefore be particularly attractive for governments that are limited in their current ability to finance infrastructure spending to use PPPs to bypass spending controls and move public investment off-budget and debt off the government’s balance sheet. But it consequently forces governments to face potentially high budgetary costs in the medium to long term so that in the long term the risk of over-indebtedness is not spared. And furthermore, where debt sustainability is not a concern, there are other considerations that need to be taken into account in order to ensure the efficiency of infrastructure services, including (i) the legal framework governing PPP contracts, (ii) the processes for selecting and implementing PPPs, and the role of the ministry of finance in this context, and (iii) the contractual obligations that underpin PPPs and that directly determine the fiscal risk incurred by the government (Akitoby et al., 2007). Moreover, governments should also strive for transparent tax accounting and full disclosure of all fiscal risks involved in PPPs.====Public–private partnerships (PPPs) constitute arrangements in which the private sector takes over infrastructure assets and services traditionally provided by governments. They are mostly contracted for a wide range of social and economic infrastructure projects including transportation infrastructure, telecommunication, water plants, financial support, innovative financing, general public services as well as hospitals, schools etc. So, should we still be asking whether PPP investment can be an alternative source of funding for productive governments expenditures? Obviously, in view of developing countries’ current fiscal constraints, one can only answer in the affirmative.====Economic growth is driven by investment and occurs when aggregate output increases, offering and expanding employment opportunities that allow for increased income and mobility for individual workers, thus improving the standard of living. Hence, one of the most important questions that can be asked is whether PPPs allow for a more efficient allocation of resources and lead to an increase in country’s productivity? The existing literature have yielded widely different estimates of the impact of infrastructure investment on economic growth (Estache and Garsous, 2012, Dintilhac et al., 2015, Arezki et al., 2016), particularly efficient transport infrastructures that enhance competitiveness and boost economic growth by raising the marginal product of labor and capital, thereby the overall efficiency of the productive mix (Aschauer, 1989) and strengthen the attractiveness of certain areas towards new production facilities, which are reflected in self-reinforcing growth processes (Messina, 2008). Even if empirical studies prove that the economy is positively boosted by PPP investments, the real question is: can PPP investment be a credible alternative funding instrument to traditional public investment?====While many theoretical arguments are put forward to support the potential economic benefits of PPPs contracts, often claimed to be far greater than those of traditional public goods provision, empirical evidence has very rarely compared the economic and social gains of funding through public or PPP investments. This paper draw on theoretical and empirical studies to looks at the policy implications of PPP’s impact on country’s economic growth, focusing on ==== Sub-Saharan African countries, most of which are facing funding deficits and growing needs for socio-economic infrastructures.====According to the results, the economic and social gains from infrastructure and services financed by PPP investments are higher than those from pure public investments. If the debt level is already high and reaches an unsustainable level, there is a diminishing effect of PPP investment on growth. As such, in order to meet the growing needs for infrastructure development, given their budgetary constraints, governments should engage in co-investment with the private sector. But beware, reforms aimed at making public debt levels sustainable would be a guarantee for positively higher returns from financing socio-economic infrastructure through public–private partnerships (PPPs).====Our paper contributes to the relevant literature on two strands. First, it looks at the impact of PPP investments on economic growth. A large part of the literature finds that the extent and type of PPP contracts are drivers of economic growth (Shediac et al., 2008, Zangoueinezhad and Azar, 2014, Oluwasanmi and Ogidi, 2014 and Mofokeng, 2019) even if some have assessed this effect when looking at the macro-economic benefits of PPPs (Checherita, 2009, Lee et al., 2018, Uddin and Akter, 2021). Others have rather shown that private sector participation accelerates growth through productivity gains, such as La Porta and Lopez-de Silanes, 1999, Trujillo et al., 2002 and Brown et al. (2006). Second, a large part of the literature, including the latest extensions of the neoclassical growth model as well as endogenous growth theories, has pointed to the role of public investment in economic growth (see, for example, Romer, 1986, Barro, 1991, Barro and Lee, 1993, Fischer, 1993). One view holds on the importance of public investment in long-term economic growth because it not only generates positive spillovers in the economy through physical infrastructure and services, but also attracts private investment, thereby enhancing economic growth (Arrow and Kurz, 1970, Barro, 1990 among others). In another segment of the literature, several other authors have argued that public investment does not necessarily have a pro-growth impact on the economy (Khan and Kemal, 1996, Devarajan et al., 1996, Ghani and Din, 2006) or even on the level of output per worker (Milbourne et al., 2003).====The paper is structured as follows. In Section 2 the theoretical framework is presented. Section 3 provides an empirical preliminary testing of the model. Section 4 discusses the empirical findings. And, the last section concludes and derives some economic policy implications.",Financing the economy in debt times: The crucial role of public–private partnerships,https://www.sciencedirect.com/science/article/pii/S109094432300025X,27 May 2023,2023,Research Article,1.0
Kerr Amanda,"Department of Economics, University of Maryland, MD, United States of America","Received 1 March 2023, Accepted 12 May 2023, Available online 21 May 2023, Version of Record 6 June 2023.",https://doi.org/10.1016/j.rie.2023.05.002,Cited by (0),"China’s one child policy stands among the most consequential actions ever taken by a government to regulate the basic structure and fundamental nature of the family unit. Scholars and policy analysts have long recognized its likely effects with respect to the aging of Chinese society. In recent years they have also become more aware of the implications of the gender imbalance the policies have produced, in particular as they pertain to the formation of marriages. This paper analyzes the selection of surplus men into marriage by means of a model that explicitly accounts for earnings and ","China’s one child policy ranks among the most intrusive public policy mandates in world history. Imposed in 1979 by the communist government for the purpose of reducing population growth, and aggressively enforced by a large state bureaucracy, the policy has produced unprecedented demographic imbalances that are likely to affect life in China for generations.====Although recent actions by the government have been implemented to relax the single-child restriction, its effects after four decades are evident in a rapidly aging population and a declining proportion of persons in prime working ages. These outcomes were widely anticipated in warnings by demographers and policy analysts at the time of the policy’s inception. What was less expected has been the extent of the increase in the male-to-female ratio of live births. In 1980, the gender ratio was ====. By 2005 it had grown to ==== (Wei and Zhang, 2009). Hudson and Den Boer (2005) attribute the increase to a cultural preference for sons, suggesting that it has been accomplished by means of gender-selective abortion or abandonment of female infants.====The gender imbalances become most evident when single children reach the age of marriage. Wei and Zhang (2009) estimate that by 2005, a quarter-century into the one child policy, approximately 40 million men were challenged in entering marriage due to a shortage of marriageable women; Hudson and Den Boer (2005) state the imbalance in terms of a shortage of brides, offering a similar estimate of approximately 40 million “missing women”. For perspective, this is slightly larger than the entire population of Poland.====The shortage of available brides has significant implications for the marriage market (see, for example, Howden and Zhou, 2014, Poston et al., 2011; and Zhang, 2017). Recent literature has started addressing the manner in which the market adapts to the shortage. Hudson and Den Boer (2005) and Eberstadt (2010) speculate that spouse selection will tend not to favor potential grooms characterized by low earnings or wealth, less education, and low skills. To date, however, the literature has not produced a formal study of the selection phenomenon.====The purpose of this paper is to address the selection of surplus men into marriage by means of a model that explicitly accounts for their earnings and wealth. Its central focus is the extent to which relatively scarce brides marry men with comparatively strong economic prospects in terms of earnings or wealth.====One econometric challenge is the potential for high-earning men to be selected into marriage in part on the basis of unmeasured characteristics that correlate both with potential earnings and their desirability as husbands. An empirical approach that fails to account for latent selection might be susceptible to bias in estimating the effect of measured earnings. The model used in this paper, to be described in Section 4, addresses this issue by means of an explicit parameter that is embedded in a joint model of earnings and the transition to marriage.====Results of this study, based on data from the China Health and Nutrition Survey, provide evidence that marriage in the age of the one child policy is indeed selective of men who are relatively high earners. This result is robust to a series of alternative specifications of the model.",A shortage of brides: China’s one child policy and transitions of men into marriage,https://www.sciencedirect.com/science/article/pii/S1090944323000248,21 May 2023,2023,Research Article,2.0
Lukyanov Georgy,"Ecole polytechnique, CREST, 5 av. Le Chatelier, 91120 Palaiseau, France","Received 14 October 2022, Accepted 12 May 2023, Available online 20 May 2023, Version of Record 31 May 2023.",https://doi.org/10.1016/j.rie.2023.05.001,Cited by (0),"This paper develops a uniform-quadratic cheap-talk setting of Crawford add Sobel (1982), in which the sender may be uninformed and cares about his reputation for competence (that is, for being informed). We establish the existence of a partition equilibrium with two messages and show how this equilibrium is affected when we change the exogenous parameters: the sender’s bias, the initial belief that the sender is competent and the sender’s reputational concerns. We also show that if the reputational concerns are high enough and the sender’s initial reputation is extremely low or extremely high, there exists a ==== equilibrium in which the competent sender perfectly reveals the state. Possible extensions of the setup are discussed. One possible application of our model might be the interaction between media provider and the public.","When people communicate some information, they often pursue the objectives that are somewhat different from the ones who hear this information and undertake the decision. For example, the media might be biased towards a particular candidate, and at the same time possess some valuable information that might influence the choice of voters.====In the classical strategic communication setting, the ==== between the sender and the receiver creates a tension which partly or even completely destroys valuable communication. As was shown in the seminal paper by Crawford and Sobel (1982), the set of messages that get transmitted is coarse: even in the most informative equilibrium, the sender is able to effectively send only a ==== set of messages, albeit the number of possible ==== (about which he has some information) is infinite.====However, in many circumstances, the sender might care not only about the action undertaken by the receiver but also about his perceived level of ====. If after the receiver has undertaken the action, the true state is realised, the receiver may assess, based on the sender’s message, whether the sender actually possessed the information in the first place.====In this paper, we take up the uniform-quadratic example that was introduced by Crawford and Sobel (1982), but suppose that the sender may be of two types: the ==== sender actually observes the state, whereas the ==== sender does not see anything. The sender then sends the message, the receiver observes it and undertakes an action, and at the end of the game, the receiver sees the state and updates her belief about the sender being competent.====We confine our attention on the particular ==== equilibrium, in which the informed sender reports whether the true state is relatively low or relatively high, while the uninformed sender is simply guessing. In the example suggested above, we may think of the media provider as the one having some knowledge as to whether the left-wing or the right-wing candidate’s platform appeals to the preferences of the median voter, and thus who is more likely to win the elections.====This equilibrium is characterised by two endogenous objects: the threshold type of the informed sender that switches from the low (“pessimistic”) to the high (“optimistic”) messages, and the equilibrium probability with which the uninformed sender type sends the pessimistic message.====First, we establish the existence of equilibrium in our model under an open set of parameters (Theorem 1). Second, we proceed to the comparative statics (Proposition 1, Proposition 2, Proposition 3). In particular, we show that the set of relatively “pessimistic” messages becomes larger (i) the greater is the sender’s bias, (ii) the larger is his initial reputation and (iii) the smaller are his reputation concerns. Likewise, we establish that in all these cases, whenever the set of pessimistic messages expands, the uninformed type starts to send them less frequently.====Then we show that when the reputation concerns are important, partition equilibria are ==== the only type of equilibria that are possible. Specifically, we show that in the limiting cases when the initial reputation of the sender is extremely high (so that the receiver is almost certain that the sender is informed) or extremely low (so that the receiver realises that facing an informed sender is almost an impossibility), there will be an equilibrium with ==== by the informed type; this is established in Theorem 2. This result has important normative implications, suggesting that the presence of a tiny fraction of uninformed experts might be beneficial for information transmission and potentially raise welfare.",Reputation for competence in a cheap-talk setting,https://www.sciencedirect.com/science/article/pii/S1090944323000236,20 May 2023,2023,Research Article,3.0
"de Jesus Diego Pitta,Besarria Cássio da Nóbrega","Department of Economics, Paraíba Federal University- UFPB, João Pessoa, Brazil","Received 29 January 2023, Accepted 14 March 2023, Available online 22 March 2023, Version of Record 6 April 2023.",https://doi.org/10.1016/j.rie.2023.03.001,Cited by (0),"The main motivation of this paper is to use machine learning techniques to build a new insolvency risk rating metric for banks traded on Brazilian stock exchange. Then, a set of prediction models will be used to project the risk rating of these institutions. Conventionally, the literature analyzes bank insolvency risk from accounting data and ","The financial fragility of a banking institution can have devastating effects on a country's economy and financial system. Currently, several types of forecasting models are employed with the objective of predicting banking risk, providing information to regulators so that they are able to take some decision in advance, avoiding or minimizing the negative effects on the rest of the financial system.====In this context, this work intends to build a new financial fragility risk rating metric for the main publicly traded banks at B3 (Brazilian Stock Exchange). It will create instruments that help to mitigate banking risk by increasing the sustainability of the financial system that can help predictability and thus anticipate eventual vulnerability situations. This new banking risk measure will be compared to the Z-score, which is a traditional and widely used metric in the literature. After the construction of the risk variable, via clustering techniques, it will be projected by a set of prediction models in order to find out which one offers the best accuracy. In addition, the study also seeks to verify whether the bank manager's sentiment is a relevant variable in predicting the new metric.====The importance of this work is linked to the fact that financial crises always have catastrophic consequences, especially the Subprime crisis, which began in 2008 with the bursting of the housing bubble. This crisis had multiple consequences on the global economy, showing, among other issues, that the financial problems of banking institutions go beyond social and economic problems and are able to affect agents around the world.====Given the strong adverse financial repercussions generated, bank fragility behaviors have become increasingly highlighted in the literature, since both investors and deposit owners tend to lose confidence in these defaulting institutions, which can contaminate the other banks present in the market and, in the long run, result in a banking crisis. The latter denotes even more severe consequences, ranging from the paralyzing of credit supply to companies and families to the flight of capital from the country (Barbosa, 2017).====According to Lepetit and Strobel (2013) the insolvency of a banking institution occurs when the losses incurred by this institution cannot be covered by its own resources. From this fact the literature has developed measures that seek to measure the risk of insolvency, among these, the CAMELS system and the Z-score stand out, in which the latter indicates how far the bank is from insolvency behavior. For more details, see: Suss and Treitel (2019), Vieira et al. (2020), Viswanathan et al. (2020).====In addition to measuring bank risk, the literature has also become concerned with predicting and anticipating the failure of these institutions. Numerous techniques have been developed over the years in an attempt to provide analysts and decision makers with effective methods of predicting bank failure risk based on various financial ratios and mathematical models, with these models including linear and logistic regressions splines multivariate adaptive regression, survival analysis, linear and quadratic programming, and multiple criteria programming, as seen in Karels and Prakash (1987), Ezzamel et al. (1987), and Ravi et al. (2008). According to Huang and Yen (2019) many of these techniques are typically based on the assumptions of linear separability and multivariate normality and, indeed, independence of the explanatory variables. However, these conditions are often violated in real-life situations.====With the expressive increase in the number of data and information some authors have started to employ machine learning (ML) techniques for banking risk prediction.==== According to Huang and Yen (2019) ML techniques have the ability to extract meaningful information from unstructured data, while dealing with nonlinearity effectively. However, the application of advanced ML techniques to financial forecasting is still a relatively new area for researchers to explore.====Paule-Vianez et al. (2019) state that the variables used in most studies to predict the bankruptcy risk of financial institutions have been the financial ratios, especially the ratios classified into capital, assets, management, results liquidity and sensitivity (CAMELS System) and some economic variables.==== Other authors have sought to include new variables in explaining bank insolvency risk. One of them is the sentiment that the manager of the banking institution conveys through quarterly reports and communications to the market. The idea is to capture, through textual sentiment, a direct relationship between insolvency risk and the pessimistic tone, seeking to increase the predictive ability of the models.====This discussion can be found in Gupta et al. (2016), in which these highlight that textual sentiment is able to predict the bankruptcy of publicly traded banks, further considering that optimistic textual tone in managers’ communication has greater predictive power for these institutions, identifying that insolvent banks exhibit more positive sentiment than their non-bankrupt peers.====The prediction literature has established the value of textual analysis, as well as a general methodology for converting text into quantitative scores that primarily assess the polarities of texts. According to Gentzkow et al. (2019), the information encoded in text is a rich complement to the more structured data types traditionally used in empirical research. In fact, in recent years, an intense use of textual data has occurred in different research areas.====Thus, this paper seeks to contribute to the literature described above in a few points, these being: first is the construction of a new metric of bank insolvency risk from cluster clustering through the k-means technique which consists of an unsupervised ML method that allows us to classify a database through clusters that minimize the squared error. This allows us to model insolvency risk rather than total bankruptcy. This approach has a number of key advantages, the most important being alignment with the practical needs of regulators who seek to intervene well in advance of failure, i.e., full bank failure.====Second, the paper contributes to the literature on bank failure risk by going beyond conventional modeling techniques, using methods from the ML literature alongside more traditional approaches. According to Suss and Treitel (2019) conventional approaches, such as logistic regression models, are unable to account for complex interactions and nonlinearities, and tend to perform worse than their more flexible machine learning counterparts. In this paper, we compare logistic regression, with five Bayesian ML models: Naive Bayes (NB), Random Forest (RF), AdaBoost, Support Vector Machines (SVM) and Decision Trees (DT).====The third is to verify whether the textual tone of banks’ quarterly reports is able to improve the accuracy in predicting bank failure risk. In addition, the present work makes use of a timevarying dictionary in the construction of the bank sentiment variable. So far in the bank risk prediction literature, the scarce papers that apply any bank sentiment variable use a fixed dictionary. Therefore, the use of a time-varying dictionary is something new in this discussion.====The results indicate that the bank risk classification, via the ==== algorithm, was able to classify 17% of the sample into the highest risk group (1), while 83% of the sample was in the lowest bankruptcy risk group (0). Using the Z-score metric, we found that 65% of the sample is in the low-risk group, and 35% of the sample is in the high-risk group. Thus, the ==== algorithm is more rigorous in classifying a bank in the high-risk category. Next we used the data already described to project the risk of bank insolvency. The results of this step showed that the decision tree model performed the best for the test sample. In addition, it was found that the inclusion of bank sentiment variables is able to improve the performance of prediction models, especially, when bank sentiment is constructed from a time-varying dictionary.====This paper is divided into four sections. The first is the introduction presented above. The second presents the methodology applied to construct the bank insolvency risk variable and the bank sentiment variables. In this section the prediction models are also shown. The third illustrates the main results obtained. Finally, the fourth indicates the final conclusions, discussing the main contributions and limitations of the paper.",Machine learning and sentiment analysis: Projecting bank insolvency risk,https://www.sciencedirect.com/science/article/pii/S1090944323000224,22 March 2023,2023,Research Article,4.0
"Watanabe Akane,Yakita Akira","Graduate School of Social Sciences, Nanzan University, Nagoya 466-8673, Japan,Faculty of Economics, Nanzan University, Nagoya 466-8673, Japan","Received 3 October 2022, Accepted 24 February 2023, Available online 1 March 2023, Version of Record 8 March 2023.",https://doi.org/10.1016/j.rie.2023.02.003,Cited by (0),This study explores the effects of monetary transaction costs on economic growth in a closed overlapping generations economy with cash-in-advance constraints. Working generations can also hold interest-bearing capital claims. Economic growth is powered by an engine of learning-by-doing and knowledge spillover among workers. The results indicate that reductions in transaction costs raise the balanced growth rate and possibly reduce long-term ,"KPMG (2016) reports that the transaction costs of using cash and checks amount to 0.52% of Singapore's GDP. Digital technology, such as central bank digital currencies, might reduce transaction costs (Pfister, 2019). This study uses Diamond's (1965) overlapping generations (OLG) model to examine the effects of the transaction costs of using cash balances in transactions on long-term economic growth. Meanwhile, cash-in-advance constraints are applied to purchases of a set of consumption goods.====Baumol (1952) formulates transaction costs as the costs of withdrawing cash balances, which is the sum of broker fees and a constant cost. Stockman (1981) develops an economic model with capital and with a cash-in-advance transaction constraint. Rotemberg (1984) considers lump-sum broker fees paid to banks as transaction costs in a neoclassical infinitely lived agent model. Crettez et al. (1999) classify the cash-in-advance constraint in OLG models into four types. Following these arguments, we assume that transaction costs are the costs of transactions between cash and real resources (i.e., consumable goods) to satisfy the cash-in-advance constraint in an endogenous growth model.====The salient features of our model setting are (1) endogenous economic growth driven by learning-by-doing and knowledge spillovers among workers (Grossman and Yanagawa, 1993), (2) cash-in-advance constraints encountered by young working individuals (classified as the usual type in Crettez et al. (1999)), (3) transaction costs of a linear function of the transacted amount of money (Baumol, 1952), and (4) real resource transaction costs (Rotemberg, 1984).==== The results show that reduced transaction cost raises the balanced growth rate. This result holds true not only for per transaction but also for lump-sum transaction costs.====Section 2 introduces a model with individuals, government, and production sectors. Section 3 considers balanced growth and presents an analysis of the effects of changes in transaction costs on growth rate. Finally, Section 4 concludes this paper.",Effects of monetary transactions costs on economic growth,https://www.sciencedirect.com/science/article/pii/S1090944323000194,1 March 2023,2023,Research Article,5.0
Rao T.V.S. Ramamohan,"Indian Institute of Technology, Kanpur, India","Received 25 August 2022, Accepted 28 January 2023, Available online 4 February 2023, Version of Record 15 February 2023.",https://doi.org/10.1016/j.rie.2023.01.010,Cited by (0),"Every firm in differentiated oligopoly offers a product that is different from that of rival firms. Similarly, in general, a firm interfaces with consumers and interacts with rival firms on the market. As a result, both the firm and consumers experience information asymmetry. In practice, a firm is a risk taker in its dealings with rival firms and is a risk averter in its interface with consumers. However, firms utilize intangible investments (non-price strategies) to convey the value of their product to consumers and stabilize their market share. Note that consumers are risk averse and ignore such attempts by a firm once they recognize the intrinsic value of the product. These two features explain the frequency and depth of the supply fluctuations that have not been acknowledged so far. This study offers a fundamental explanation of this phenomenon along with the steady state behavior in a synthetic manner.====“With uncertainty entirely absent, every individual being in possession of perfect knowledge of the situation, there would be no occasion for anything of the nature of responsible management or control of production activity.”- ====, p.267)","The following features are specific to a firm in differentiated oligopoly====.====The fundamental argument of this study is that market imperfection, information asymmetry, and the attitudes of firms and consumers generate supply side effects in imperfect markets====.====Against this backdrop the rest of the study is organized as follows. Section 2 offers a specification of the (p,S) locus and the behavior of the firm and consumers that generate cyclical effects. Section 3 outlines the influence of market imperfection in the money market. Section 4 presents a summary. The appendix proposes another method of developing the (p.S) relationship.","Information asymmetry, attitudes toward risk, and macroeconomic performance",https://www.sciencedirect.com/science/article/pii/S1090944323000054,4 February 2023,2023,Research Article,6.0
Wilson Matthew S.,"Economics Department, University of Richmond, United States","Received 17 December 2022, Accepted 28 January 2023, Available online 3 February 2023, Version of Record 11 February 2023.",https://doi.org/10.1016/j.rie.2023.01.007,Cited by (0),"During the Covid pandemic, people weighed the benefits of social contact against the risks to their health. Ideally, people would respond based on the true infection rate; this is the ====. However, there was high uncertainty, so perhaps people relied upon the ==== instead. I estimate revealed preferences for health and social contact at the county level and find evidence in favor of the heuristic model. This is important since many models of optimal policy assume that people respond to the true infection rate.","Since people value their health, an increase in the infection rate should cause social contact to fall. Surprisingly, the evidence fails to support this basic economic intuition. Instead, people adopted a heuristic due to uncertainty about the true infection rate. Perceived risk soared when WHO declared a pandemic. There is weaker evidence that the heuristic also depended on local confirmed cases. Heuristics have major implications for policy. If we incorrectly believe that behavior responds to the actual infection rate, then we will miscalculate people's response and the incentives required to achieve optimal contact.====I borrowed the SEIR model from epidemiology and endogenized social contact. I focus on Florida since it fully reopened in September 2020. Other states are unsuitable because there is confounding: how much social distancing is due to government policies and how much is due to people's preference for staying healthy? The Florida data shows how people navigate the tradeoff between health and social contact, revealing their preferences. In the rational model, behavior responds to true risk. However, in this case the revealed value of health is often insignificant and sometimes negative. The heuristic model produces more realistic results. Now the revealed value of health is positive and significant in every single county. Due to length, most discussion of optimal policy is reserved for a companion paper. Here I focus on rationality and heuristics.====This paper is not the first to endogenize social contact in an epidemiology model. Fenichel et al. (2011), Fenichel (2013), and Morin et al. (2013) did this years before the Covid pandemic. A number of recent papers followed (e.g.Ng, 2020, Farboodi et al., 2021, Bosi et al., 2021). To calculate optimal policy, they used the value of statistical life. Quaas et al. (2021) took a different approach, relying on survey data to calibrate the utility function parameters. However, in all these papers, contact depends on the true infection rate.====Heuristics have received little attention. Atkinson (2021) presents simulations where contact falls in response to daily deaths rather than the infection rate. Yan et al. (2021) show that social contact responds to confirmed cases. This can be construed as a heuristic, since confirmed cases underestimate infections. Kucirka et al. (2020) find that the standard PCR tests often lead to false negatives for pre-symptomatic patients. Many infections are mild or asymptomatic, so these patients never get tested. In the beginning of the pandemic, tests were scarce and many infections went undetected. I find that perceived risk may have depended on confirmed cases before WHO declared a pandemic, but not afterwards.====The word “heuristic” never appears in Papageorge et al. (2021). However, they demonstrate that people are not responding to the true infection rate. In their survey, subjects estimated the current local infection rate. The average answer was 24%! This is far too high. Perceived risk was very different from true risk, which indicates that people were using a poor heuristic. In the survey, reported behavior changes (e.g. masking) were linked to demographic factors such as income and political affiliation. I use cell phone location data instead, and while there is significant heterogeneity across counties, none of my demographic variables can explain it.====Many papers discuss pandemic fatigue (e.g. Yan et al. 2021, Caulkins et al. 2021, Atkinson 2021, Reicher and Drury 2021, Haktanir et al. 2022, Lilleholt et al. 2020). I capture this by modeling the value of health as a linear function of time. The coefficient on time should be negative; if people are becoming less worried about their health, then they will resume socializing. However, there is no evidence of pandemic fatigue in either the rational or the heuristic model. This is in contrast to Lilleholt et al. (2020) and Haktanir et al. (2022). In their surveys, subjects reported that they were being less cautious with their health. However, Reicher and Drury (2021) challenged this narrative, pointing to other surveys where large majorities continued to comply with the mandates. Most of this literature is based on surveys. Yan et al. (2021) works with mobility data and briefly mentions pandemic fatigue, but they do not test for it. Delussu et al. (2022) analyze Facebook and Google location data in Italy. They demonstrate that contact was rising over time, and that the increase was larger in regions with tighter restrictions. Their paper is lacking in control variables. Wilson (2021) emphasizes that the weather can also explain changes social contact. In addition, I also include controls for holidays, spring break, and the BLM protests. The next section discusses the controls in more detail.",Social contact in a pandemic: Rationality vs. heuristics,https://www.sciencedirect.com/science/article/pii/S1090944323000078,3 February 2023,2023,Research Article,7.0
"Sander Christian J.,Thiem Stefan","Kiel University, Department of Economics, Olshausenstr. 40, 24098 Kiel, Germany,University of Münster, Institute of Public Economics, Wilmergasse 6–8, 48143 Münster, Germany","Received 30 December 2022, Accepted 28 January 2023, Available online 2 February 2023, Version of Record 8 February 2023.",https://doi.org/10.1016/j.rie.2023.01.006,Cited by (0),"There is a lively debate on whether football fans should pay a security fee to ==== police activities. This paper investigates the price effect on the demand for tickets in a dynamic setting, by considering two subgroups of spectators, namely fans and hooligans. We analyze a situation in which the demand from each subgroup causes a negative social externality for members of the other group but a positive one for members of the same group. We show that charging a security fee may start a dynamic process, leading to fewer fans and more hooligans attending matches and thus, counterintuitively to even more violence. Therefore, the present study provides an argument to refrain from charging a security fee. As an alternative economic solution, we discuss the strategy of outpricing hooligans.","Hooliganism remains a major problem in sports, and particularly in football. In order to curb the violent behavior of hooligans on match days, governments spend vast amounts of money on police activities. Moser (2009) estimates these costs at around 2 Million Euros per year, alone for games hosted by FC Bayern Munich, a club of Germany’s first football league ====. There are now attempts by local governments to impose these costs to clubs in the form of a security fee (costs-by-cause principle).==== If the amount of the security fee depends on the number of spectators, the introduction of the fee will increase marginal costs. Therefore, we assume that clubs will increase ticket prices to recoup these costs. This paper investigates the effect of such a rise in ticket prices on the demand for tickets in a dynamic setting. In this context, the distinctive feature of football matches, that the size and composition of the audience creates an atmosphere which in itself is part of the consumption experience, is taken into account. Regular fans provide an atmosphere other fans enjoy. By contrast, hooligans engage primarily in competitive fights with each other. Furthermore, the attendance of one group constitutes a negative externality for non-group members. In other words, members of each subgroup prefer to keep to themselves. We show that a marginal increase in ticket prices may result in more hooligans attending in the long run and thus, counterintuitively to even more violence.====More specifically, we model the demand of the two subgroups of spectators as follows. Each spectator derives utility from watching the match (private consumption). Additionally, the total attendance of each subgroup during the event adds a second dimension to the consumption experience of each visitor (mob effect). Hence, the aggregate demand for tickets of each subgroup for the upcoming match not only depends on the price (negative) but also on the total attendance of the same group at the previous match (positive due to a bandwagon effect), and the total attendance of the other group at the previous match (negative due to what we call repel effect).====The dynamic structure of the model is as follows. First, an additional security fee on tickets is introduced, thus increasing the price of tickets. This lowers the demand from both subgroups. At the second stage, both fans and hooligans adjust their demand again, due to the reduced attendance at the previous match. There are two effects for each group; the first effect is a further reduction of demand, due to a decrease in attendance by members of the same group at the previous match. The second effect runs contrary to the first; a lower attendance by members of the other group increases the demand from both groups.====Our main result is that there exists a new steady state in which the attendance of hooligans is larger than at the steady state before the introduction of the security fee. The contrary is true for fans. At this new steady state, the level of violence is higher, as there are more hooligans which raises need for further police activities. An alternative strategy for clubs might be to raise the ticket price above the prohibitive price for hooligans. This “outpricing” practice might be an economic solution, despite of possible short-term losses for clubs.====Our paper is related to the literature on social externalities. Leibenstein (1950) analyzed nonfunctional demand for goods, for which the utility derived by the product is enhanced or decreased by others consuming it (bandwagon and snob effect). Becker (1991) shows that the bandwagon effect can lead to a positive relationship between price and aggregate demand. Both authors concentrate on the level of consumption and its effect on the value of the product. DeSerpa and Faith (1996) and Busch and Curry (2011) take into account that the willingness to pay may depend not only on the absolute level of consumption, but also on the characteristics of the customers. Other studies analyze situations in which the bandwagon and snob effects arise simultaneously. Amaldoss and Jain (2005), for example, model two different groups of customers: snobs and followers. The former desire exclusivity. Consequently, their utility from purchasing a product decreases as more people consume it, whereas the opposite is true for followers who want to conform. Kovács (2015) analyzes status goods, for which the strength of both the bandwagon and the snob effect changes, due to the diffusion of status goods among consumers.====In contrast to the previous literature, we analyze a situation in which the demand from each subgroup causes a negative social externality for members of the other group but, simultaneously, a positive one for members of the same group.====A relatively small body of the economics literature focuses on hooligan behavior and its effect on the attendance of other fans. Hooligans are described as committed football supporters (Armstrong and Giulianotti, 2002), but with a strong preference for violence (Poutvaara and Priks, 2009). Dobson and Goddard, 1996, Dobson and Goddard, 2011, Avgerinou and Giakoumatos (2011) and Jewell et al. (2014) provide empirical evidence that hooliganism causes a decline in the attendance of fans. On the other hand, the presence of (normal) fans bothers hooligans who are interested in competitive fights with people “regarded as worthy of confronting” (Spaaij, 2008). The psychological and sociological literature on football hooliganism provides a reason for our assumption that hooligans’ reaction to marginal price changes is relatively small. Kerr (1994) provides a theoretical model which explains how violent behavior at matches can develop into an addiction. Spaaij (2006) finds that violent behavior is a key aspect of building and confirming a tough masculine identity for hooligans.====Nyberg and Priks (2017) analyze the effect of an introduction of co-payments on private guards, police and unruly behavior for the Swedish football league. They find that clubs which were required to pay the full costs of policing increased private guards and suggest a reduction of unruly behavior. However, they do not study the effect of ticket prices on the composition of the audience. Instead, their paper focuses on payments for private guards and the problem of free-riding on police efforts.====To the best of our knowledge, no paper so far investigates the price effect on the demand for tickets in a dynamic setting by considering different spectator subgroups. The remainder of the paper is structured as follows, Section 2 lays out the model. In Section 3, we analyze the dynamics and steady states, before we study the practice of “outpricing” of hooligans in Section 4. Section 5 summarizes the analysis and derives policy implications.",Should football fans pay for security? Effects of a security fee,https://www.sciencedirect.com/science/article/pii/S109094432300008X,2 February 2023,2023,Research Article,8.0
Nakatani Ryota,"International Monetary Fund, 700 19th Street, NW, Washington, D.C. 20431, USA","Received 14 December 2022, Accepted 28 January 2023, Available online 1 February 2023, Version of Record 7 February 2023.",https://doi.org/10.1016/j.rie.2023.01.009,Cited by (0),"Does the maturity of debt matter for productivity? Using data on Italian firms from 1997 to 2015, we study the relationship among debt maturity, productivity, and firm characteristics. We find that productivity is positively associated with short-term debt and negatively associated with long-term debt. This result supports the hypothesis that the less intense monitoring of firm performance and fewer liquidation fears stemming from the long maturity of debt causes a moral hazard, while short-term debt serves as a disciplinary device to improve firm performance in the short run. This effect is evident in small- and medium-sized enterprises and old firms. In contrast, large firms can utilize long-term financing to improve productivity through long-term investments. Firms improve productivity by purchasing ==== financed by short-term debt.","How does debt maturity affect productivity? The literature only focuses on how debt maturity influences output (Jaramillo and Schiantarelli, 1997) and firm growth (Léon, 2020) but not productivity. For instance, Léon (2020) found that long-term debt does not stimulate the growth of small and young firms but that short-term debt spurs firm growth.====It would be interesting to explore how debt maturity influences firm productivity because the effects can be either positive or negative. On the one hand, a long debt maturity can avoid liquidity risk, allowing firms to focus on productivity-enhancing activities. On the other hand, a long maturity causes a moral hazard for firms due to less intense monitoring by creditors.====In theory, an optimal financing strategy is to match the maturity of liabilities and assets (Hart and Moore 1995). The implication of this theory is that companies use long-term debt to purchase tangible fixed assets and short-term debt to finance working capital or intangible assets. In the absence of long-term finance, which is often the case for small and medium-sized enterprises (SMEs), companies tend to favor investments in technologies with immediate payoffs because of liquidation fears. Diamond (1991) demonstrates that companies face liquidity or roll-over risk when they finance long-term investments using short-term debt, as creditors may refuse to roll over their credits. On the other hand, the agency theory by Jensen (1986) predicts that short-term debt may be a tool that disciplines managers by imposing frequent renegotiations.====The effects of debt maturity on productivity could differ between large companies and SMEs. This is because SMEs tend to face credit constraints for long-term financing due to insufficient eligible collateral. Therefore, we also study whether the effects of long- and short-term debt on firm productivity differ between SMEs and large companies.====We also analyze how firm age affects the influence of debt maturity on productivity. Older firms may face a moral hazard due to their long relationships with lenders. Alternatively, older firms can survive in the market as a result of disciplinary efforts to improve firm performance and productivity. Thus, the effects of firm age are ex ante unclear. We further study the effects of debt maturity in relation to intangible assets because these assets are likely to be purchased through short-term loans to quickly raise productivity.====This paper analyzes firm-level total factor productivity (TFP) dynamics from the viewpoint of corporate balance sheets to answer the research question. We use Italian firms’ data compiled in the Orbis database from 1997 to 2015 to analyze productivity dynamics. The detailed data and empirical methodology are explained in the following sections.",Debt maturity and firm productivity—The role of intangibles,https://www.sciencedirect.com/science/article/pii/S1090944323000066,1 February 2023,2023,Research Article,9.0
Afonso Oscar,"University of Porto, Faculty of Economics, and CEFUP, Portugal","Received 6 January 2023, Accepted 28 January 2023, Available online 1 February 2023, Version of Record 1 February 2023.",https://doi.org/10.1016/j.rie.2023.01.008,Cited by (0),"A detailed examination of wage data points to a wage polarization trend vis-à-vis the distribution of qualifications. Theoretically terms, this points to the need for modeling focused on the relevance of the direction of technological knowledge. To this end, we branched production into routine and non-routine tasks. In this way, the results produced positive relationships between the relative supply of skilled workers and the skill premium and between automation and wage polarization.","Since the 1980s, inequality has been steadily increasing in advanced economies, becoming a major social concern (e.g., Alvaredo et al., 2018). Initially, the analysis of this phenomenon focused on the sustained increase in the skill premium, measured by the wage differential between skilled and unskilled workers and within the Directed Technical Change (DTC) literature (e.g., Akerman et al., 2015, McAdam and Willman, 2018). In theoretical terms, this approach has led to the development of models that explain the wage differential due to the DTC mechanism: more favorable wages for skilled workers due to the strong increase in their demand motivated by the technological-knowledge bias in their favor (e.g., Bound and Johnson, 1992, Katz and Murphy, 1992, Juhn et al., 1993, Acemoglu, 1998, Acemoglu, 2002).====A finer analysis of the data also allowed for the observation of a wage polarization phenomenon, revealing that, in effect, the wage growth rate of workers at the extremes of the skill distribution outpaced that of other workers (e.g., Autor and Dorn, 2013). This observation contributed to a paradigm shift in the literature to focus on how wage inequality may be affected by task automation (e.g., Acemoglu and Autor, 2011).====Next, several authors find that medium-skilled workers are employed in more automatable routine tasks, while skilled and unskilled workers are employed in non-routine, respectively more abstract and more manual tasks (e.g. Acemoglu and Autor, 2011, Autor and Dorn, 2013). The latter, non-routine tasks are difficult to reduce to a specific set of instructions performed by machines. Indeed, abstract tasks, performed by skilled workers, require complex cognitive processes (e.g., scientists, managers, technicians, etc.), and manual tasks, performed by unskilled workers, require manual dexterity. It is in this context that one possible explanation for wage polarization is the automation of rote tasks. Indeed, the increase in the number of routine tasks performed by machines decreases the relative demand for middle-skilled workers (e.g., Acemoglu and Autor, 2011).====The paradigm shift has rendered the basic DTC framework inappropriate because, by considering only two skill sets (skilled and unskilled), it is unable to explain the wage polarization phenomenon (e.g., Acemoglu and Restrepo, 2018). Motivated by the paradigm shift that the finer analysis of the data suggests, in this paper, we extend the view of the DTC to make it compatible with the wage polarization phenomenon. In the new scenario, aggregate output is now produced by two sets of tasks, a continuous set of routine tasks and another set of continuous non-routine tasks. The non-routine tasks are differentiated between, on the one hand, tasks requiring abstract skills performed by skilled workers and, on the other hand, other tasks requiring physical dexterity and human interactions performed by unskilled workers.====According to the literature cited above, it is then considered that: (i) the set of routine tasks is performed by medium-skilled workers; and that (ii) the set of non-routine tasks is performed by both skilled and unskilled workers. All types of tasks are in turn complemented by specific quality-adjusted machines.====In parallel, it is noted that the literature has also been discussing and analyzing channels through which inflation affects or can affect the distribution of earnings (e.g., Erosa and Ventura, 2002, Doepke and Schneider, 2006, Albanesi, 2007, O’Farrell et al., 2016), suggesting that there is a strict link between inflation and wage inequality, which in this case is at the level of wage polarization and the skill premium. Given that the DTC literature originates from the endogenous growth literature and that the endogenous growth literature has emphatically addressed the relationship between inflation and economic growth (e.g. Cameron et al., 1996, Valdovinos, 2003, Benhabib and Spiegel, 2009, Chu and Cozzi, 2014, Chu et al., 2015, Chu et al., 2019), it becomes desirable that the now proposed extension to the DTC model to accommodate the polarization phenomenon, also contemplate the ability to explain the impact of inflation on wage inequality — either wage polarization or the skill premium. Thus, in addition to accounting for routine and non-routine tasks, the proposed extension of the DTC model also includes Cash-in-Advance (CIA) constraints on the economy’s R&D activities, assuming that these constraints are different for producing designs used by different types of workers — skilled, medium-skilled, and unskilled.====The CIA constraints reflect the need for producers/entrepreneurs to have liquidity in their activity, so they introduce the demand for money into the model and allow for the analysis of inflation effects. In particular, the introduction of CIA constraints on the execution of R&D activities is justified by the strong liquidity requirements of R&D investment (e.g., Brown and Petersen, 2009, Brown and Petersen, 2011, Brown and Petersen, 2015, Falato and Sim, 2014, Lyandres and Palazzo, 2016, Chu and Cozzi, 2014, Gil and Iglésias, 2020) and the nature of the assets held by R&D firms, which are essentially intangible and subject to asymmetric information (e.g., Alam and Walton, 1995, Zantout, 1997).====In this literature, CIA constraints are found to be similar across firms because the models consider only one type of work. Therefore, they are independent of workers’ qualifications. However, by differentiating workers, it makes sense to explore the differences since firms operating in routine production face more financial constraints than those operating in non-routine sectors, as is suggested by, among others, Beck and Demirguc-Kunt (2006), Hirsch-Kreinsen (2008), Czarnitzki and Hottenrott (2011), Popov (2013), Cao and Leung (2016), Ferrando and Ruggieri (2018), Frank and Yang (2019), Gómez (2018), and Feng et al. (2020). Moreover, in innovative economies, there is evidence that labor-intensive, skilled firms face fewer financial constraints (e.g., Matsuyama, 2007, Chaney, 2016). On the other hand, firms performing routine tasks compete with alternatives to relocating abroad through offshoring, international outsourcing or foreign direct investment and therefore face a higher risk of failure that leads them to face more financial constraints (higher risk-premium requirement).====In view of the above, the introduction of CIA constraints in the model considers the following facts: (i) firms producing designs used by medium-skilled workers in routine tasks are subject to more financial constraints than firms producing designs used in non-routine tasks; (ii) firms performing non-routine tasks face greater financial constraints in the production of designs used by unskilled workers than in the production of designs used by skilled workers.====In the global context of relevant inflation rates, the model provides important results at the steady-state level. First, it allows us to observe that there is a single balanced and stable growth path along which output, consumption, machine quality, and wages in each sector grow at the same rate. Second, by highlighting the existence of a positive relationship between inflation and the nominal interest rate implied by the Fischer equation, it allows us to state that an increase in the inflation rate (i) decreases the rate of economic growth, (ii) increases the skill premium, (iii) generates wage polarization and favors skilled workers in the routine sector.====The rest of this paper is structured as follows: Section 2 describes the model and the main static results at each point in time; Section 3 describes the general equilibrium conditions of the model, with particular emphasis on the steady state; Section 4 concludes.","Inflation, technological-knowledge bias, and wages",https://www.sciencedirect.com/science/article/pii/S1090944323000091,1 February 2023,2023,Research Article,10.0
Sanjaya Muhammad Ryan,"Department of Economics, Faculty of Economics and Business, Universitas Gadjah Mada, Jln Sosio Humaniora No. 1, Bulaksumur, Yogyakarta 55281, Indonesia","Received 17 January 2023, Accepted 28 January 2023, Available online 31 January 2023, Version of Record 2 February 2023.",https://doi.org/10.1016/j.rie.2023.01.005,Cited by (0),"Antisocial behavior experiments, both conducted in the laboratory or in the field, have become commonplace in the ==== literature. Such experiments found their relevance in the real world as people are not always selfish or nice to others, but they also sometimes behave spitefully. This paper focuses on payoff-destruction experiments conducted over the last two decades and synthesizes the findings. We are able to find 46 studies where we found inequity reduction and pure spite as the main motives for such behavior. This behavior can also be explained by conflict experience. We conclude with suggestions for future research.","More than a million people were violently killed from armed conflicts from 1989 to 2017 with Africa, Middle East and Asia as the main contributors of such violent (Allansson and Croicu, 2018; Gleditsch et al., 2002). While much research has been done at aggregated levels, there is still a growing debate on whether such conflict can be explained by looking at individual behavior. That is, whether an isolated behavior can transform a peaceful community into chaos and war?====In experimental economics literature, antisocial behavior experimental games are often used to elicit negative behavior that may ruin social cohesion. One family of such games is the payoff-destruction game. In this game players are given the opportunity to reduce (‘burn’) other participants’ money, either for free or at some cost. The payoff-destruction literature can generally be divided into those that tried to observe pure spite and those that are concerned with inequity aversion. For example, in the original money burning (MB) game some participants received a (randomly allocated) advantage over others to create a sense of unfairness (Zizzo and Oswald, 2001). Meanwhile, in the joy-of-destruction (JoD) game participants who received smaller amount of endowment—which was designed to be equal in expectation—did not destroy more of their co-participant's money (Abbink and Sadrieh, 2009).====This article explores the general findings from two decades of antisocial behavior experimental studies. In addition to generally accepted criteria of a standard economic experiments (incentivized, no deception), we use three criteria for the inclusion of experimental games that capture antisocial behavior: (i) the game must be strategic; (ii) at least one of the strategies must be a spiteful one; and (iii) punishment games, such as the public goods game with punishment, were excluded. From the first criterion, we exclude experiments that could observe antisocial behavior yet did not involve strategic decision making, such as in cheating and lying games (Cadsby et al., 2010; Fischbacher and Follmi-Heusi, 2013; Gravert, 2013) and allocation decisions that are used to classify types of preferences (Fehr et al., 2013; Kerschbamer, 2015). It also excludes games where players are making destruction decisions against money donated to charity (Takahashi and Tanaka, 2020).====The second criterion was mostly used to filter out actions that are rationally motivated by material benefit, such as taking money from others in the fragile public goods game (Hoyer et al., 2014), undervaluing competitors’ outcomes in the sabotage game (Carpenter et al., 2010; Habring and Irlenbusch, 2011) and ‘attacking’ other players in the anarchy and destruction games (Powell and Wilson, 2008; Scacco and Warren, 2018). Similarly with games where a player can only indirectly harm the other player (e.g., by making it more difficult for the other player to get money (Sadrieh and Schröder, 2017)).====The third criterion was selected because punishment games have been widely studied, such as ultimatum game (Oosterbeek et al., 2004) and public goods game (Thoni, 2014), where some experiments were conducted in the field, such as in India (Fehr et al., 2008) and dozens of societies around the globe (Herrmann et al., 2008). Punishments in these games are typically introduced as a norm-enforcing mechanism, which is arguably a different motivation than pure spite. Therefore, in the following part of this article, we focus on antisocial behavior in non-punishment games.====We managed to identify 46 antisocial behavior experimental papers (with a total of 88 treatments) that satisfy the three criteria. We are also able to identify factors that contribute to antisocial behavior. A simple regression using a subset of the sample that contains only MB and JoD games shows that money burning was more likely when it was conducted using non-students and when money burning is costly, although the effect of the latter disappears when region dummies are controlled. We also found that studies with a conflict background tend to generate higher burn rate.====It might be interesting to note that there are several antisocial behavior games that are part of market mechanisms (auctions). Morgan et al. (2003) developed a theoretical work based on four standard types of auctions, where the theory predicts the existence of spite motive, with people aggressively bidding in first-price auctions after knowing that their opponents are not machines. They also predict that aggressive subjects in first-price auctions should be more aggressive in second-price auctions. Empirically, the experiment is designed so that bidders know their own private valuation of the item and, in the complete information condition, know other bidders’ private valuations (and vice versa for the incomplete information condition) (Nishimura et al., 2011). In another study, Kimbrough and Reiss (2012) measured spite in a dynamic second price auction, and found that people are either always spiteful or not spiteful at all. While understanding spiteful auctions have their merits, the focus of this paper is payoff-destruction games that capture antisocial behavior.====The rest of this paper is organized as follows. The next section reviews the use of payoff-destruction games and the general experimental results. We then elaborate the various factors that have been found to explain antisocial behavior. Next, we review several findings from lab-in-the-field experiments where participants live or had conflict experience. We also present the findings from a simple regression using the data from antisocial behavior experiments in the next section. The final section summarizes the main findings of this paper.",Antisocial behavior in experiments: What have we learned from the past two decades?,https://www.sciencedirect.com/science/article/pii/S1090944323000108,31 January 2023,2023,Research Article,11.0
"Chang Yang-Ming,Raza Mian F.","Department of Economics, Kansas State University, 319 Waters Hall, Manhattan, KS 66506-4001, USA,Department of Economics, Harper College, J268 Building J, 1200W Algonquin Rd, Palatine, Illinois 60067, USA","Received 27 December 2021, Accepted 24 January 2023, Available online 30 January 2023, Version of Record 9 February 2023.",https://doi.org/10.1016/j.rie.2023.01.004,Cited by (0),"This paper examines which types of firms, from a developed country (DC) or a less developed country (LDC), tend to practice dumping, using a two-market equilibrium analysis of trade in similar products. Specifically, we present a vertical product differentiation model of duopolistic competition between a DC firm and an LDC firm under ","Unfair trade practices such as dumping products into the markets in developed countries continue to make business news headlines. In 2019, American Kitchen Cabinet Alliance petitioned the U.S. International Trade Commission (ITC) against cheap wooden cabinets and vanities imported from China. On April 6, 2020, the ITC documented that Chinese exporters dumped wooden cabinets and vanities of $4.4 billion worth in the U.S. market in 2018. The ITC's findings confirmed that the Chinese exporters’ unfair trade practices materially injured the U.S. domestic industry. The ITC imposed antidumping (AD) duties on seven Chinese wooden cabinets and vanities exporters/producers, ranging from 4.37% to 262.18% (US Department of Commerce 2020b). The use of AD duties by the U.S. government is not limited to products such as wooden cabinets and vanities. The U.S. government has been investigating whether aluminum foil from Armenia, Brazil, Oman, Russia, and Turkey is dumped in the U.S. market (US International Trade Commission 2021). Likewise, Wind Tower Trade Coalition filed a petition against India and Malaysia for dumping utility-scale wind towers at the alleged dumping margin of 54% for India and 93% for Malaysia. The U.S. government investigates whether these exports injured the U.S. industry by selling their product at less than fair value (US Department of Commerce, 2020a). These cases exemplify the practices of dumping products at lower prices in developed countries, where governments may impose antidumping duties as unfair trade remedies.====One crucial concern is whether World Trade Organization (World Trade Organization WTO, 2021a, World Trade Organization WTO, 2021b) does not see dumping as an unfair trade practice and thus does not necessarily support using antidumping duties. WTO provides member states a mechanism to deal with dumping practices known as the “Anti-Dumping Agreement.” GATT/WTO (Article 6) allows a member state to take action against dumping when there is material damage to its domestic industry due to a lower price of an imported product than its price in the exporting country's market. In addition to the WTO documentation, scholarly work widely recognizes that dumping is an unfair trade practice. Researchers cautiously remark that WTO does not necessarily prohibit dumping unless it causes a material injury to a trading partner. In this case, the trading partner's imposition of an AD policy makes dumping a costly choice.====From a global perspective, there are significant issues concerning (i) which types of firms, from developed countries (DCs) or less developed countries (LDCs), tend to practice dumping and (ii) whether DC or LDC governments are likely to launch antidumping investigations into imports. How is dumping related to trade in similar products? Given the growing concern over the large-scale dumping of cheap products to DCs, would LDCs be better off if their governments restrained exporters from practicing dumping? Are antidumping duties efficacious in protecting domestic firms and hence proven to be a welfare-improving policy for an importing country? How would the global dumping of lower-quality products affect the Pareto superiority of worldwide free trade? Can world welfare (defined by aggregating the social welfare of DC and LDC as trading partners) be higher under an AD policy than under free trade in the presence of dumping? This paper answers these questions with a two-market equilibrium analysis of international trade between DC and LDC in quality-differentiated products.====In retrospect, voluminous studies have contributed to our understanding of dumping and antidumping regulations.==== Recognizing the literature's contributions, we follow the GATT/WTO guidelines to derive the conditions under which dumping arises by directly comparing the equilibrium market prices of similar products sold by DC and LDC firms.==== The equilibrium price comparison for a firm's product sold in both DC and LDC markets helps identify the firm type (DC or LDC) that practices dumping. In characterizing international duopoly under free trade, we use a two-stage game where each competing firm endogenously determines its product quality level before engaging in price competition in the DC and LDC markets. We consider inter-country income differentials, which reflect different degrees of international market competition. Within the two-way trade in quality-differentiated products between DC and LDC firms,==== we show that the DC firm sells a higher-quality product due to its economic incentive to undertake R&D for quality improvements. In contrast, the LDC firm sells a lower-quality product with no interest in costly R&D for quality up-gradation. These analytical results are consistent with the empirical findings that, on average, a DC firm's strategic choice of product quality is relatively higher than that of an LDC firm (Amiti and Khandelwal, 2013). Moreover, our trade model in quality-differentiated products with inter-country income differentials reveals that the DC firm does not practice dumping under free trade, whereas the LDC firm dumps its lower-quality product into the DC market at a price lower than the LDC's local price.====The framework of trade in similar products (with quality differentiation) permits us to investigate how consumers and producers in two trading countries (DC and LDC) are affected by three different trade regimes: free trade, antidumping (AD), and price undertaking. Facing the LDC dumping of a lower-quality product in the DC market under free trade, the importing country's government (i.e., DC) responds by imposing AD duties. The analysis indicates that DC consumers and producers gain a higher surplus by consuming and producing more of the higher-quality good and less of the lower-quality good. As such, DC's overall welfare is highest under the AD regime. However, DC consumers and producers confront the highest economic costs when their government allows LDC dumping firms the option of price-undertakings. We find that the win-win-win equilibrium by imposing AD duties on LDC dumping may explain why under the auspices of antidumping regulations, the U.S. government favors the use of antidumping policies rather than the offering of price-undertakings to foreign dumpers.====As for the economic effects on LDC, we show that LDC consumers are better off when the DC government imposes AD duties on LDC dumping than when there is free trade. The economic intuition behind this result is as follows. Under an AD policy, gains in surplus to LDC consumers by consuming a higher-quality imported product outweigh the loss of consumer surplus resulting from buying the lower-quality domestic product at an increased price. However, the LDC firm enjoys the highest producer surplus by dumping its lower-quality product under free trade. Third, LDC's welfare is the highest when its exporter accepts a price-undertaking, but the welfare is the lowest when the exporter dumps its lower-quality product and pays AD fines.====From the perspective of world welfare, calculated by aggregating the welfare of trading partners (DC and LDC), we show that the trade damage measure of imposing AD duties is Pareto-improving compared to free trade (under which dumping takes place). This result provides a theoretical justification for AD duties in response to foreign dumping.====This paper contributes to the literature by identifying the firm type (DC or LDC) that tends to practice dumping, on the one hand, and showing whether DC or LDC governments are likely to launch AD actions against foreign dumpers, on the other. Prusa (2001) empirically documents that until the 1980s, approximately 95% of the AD disputes were initiated by DCs against imports from LDCs. Vandenbussche and Zanardi (2008) find that the later trend reveals that LDCs are also highly involved in AD actions. Bown (2011a, b) indicates that AD actions are generally concentrated across traditional users (DCs) and new AD users (LDCs).==== Interestingly, Bown (2013) further remarks that most of the new AD disputes launched by LDCs have targeted imports of cheaper products from other LDCs – the so-called “South-South protectionism.”==== Blonigen and Prusa (2016) document that, based on AD duties' size, DCs remain the most frequently observed AD policy users against the unfair practices of dumping firms from LDCs. Hansen and Neilsen (2009) show that product quality differences lead to higher-quality firms soliciting tariff protection. The authors remark that product differentiation makes AD policy more beneficial to those firms manufacturing and exporting higher quality (i.e., the developed world). Our analysis of LDC dumping deviates from the reciprocal dumping model of Brander and Krugman (1983) in two crucial respects. First, we examine two-way trade in quality-differentiated products, while Brander and Krugman examine two-way trade in identical products without quality differentiation. Second, we look at the DC-LDC trade when there are inter-country income differentials or different degrees of international market competition. Brander and Krugman analyze issues of trade between two DCs with identical economies. The contribution by Flam and Helpman (1987) is among the first to analyze north-south trade in vertically differentiated products. The authors focus on trade issues other than dumping.====Our study complements the contribution of Moraga-González and Viaene (2015) in examining trade between a DC and an LDC. The methodological differences and the economic implications between the two studies are as follows. We examine a fully covered market for products.==== Moranga-Gonzalez and Viaene analyze dumping issues for a partially covered market. We show that the DC firm produces a higher-quality product and does not practice dumping, but the LDC firm produces a lower-quality product and practices dumping in the DC market. As a consequence, dumping is a result of lower product quality. Moraga-González and Viaene (2015) show that DC firms practice dumping in LDC markets. The authors find that an LDC government opts for a price-undertaking policy toward a DC dumping firm and that the policy benefits the implementing country (i.e., LDC) but negatively affects global welfare. Our model shows that an antidumping policy is welfare-increasing to an implementing country (i.e., DC) and the world (measured by the aggregate welfare of the trading partners).====The remainder of the paper is organized as follows. Section 2 first lays out an analytical framework of trade in quality-differentiated products to analyze competition between a DC firm and an LDC firm in both of their markets. We consider three different trade regimes: free trade with the presence of dumping, an antidumping policy, and a price undertaking. Section 3 compares firm profits, consumer surplus, and social welfare in DC and LDC under alternative trade regimes. Section 4 contains concluding remarks.","Dumping, antidumping duties, and price undertakings",https://www.sciencedirect.com/science/article/pii/S1090944323000030,30 January 2023,2023,Research Article,12.0
"Cremaschini Alessandro,Maruotti Antonello","Dipartimento MeMoTEF, Università di Roma La Sapienza, Italy,Dipartimento di Giurisprudenza, Economia, Politica e Lingue Moderne, Libera Universita` Ss Maria Assunta, Italy,Department of Mathematics, University of Bergen, Norway,Invitalia SpA, Italy","Received 15 April 2022, Accepted 8 January 2023, Available online 12 January 2023, Version of Record 24 January 2023.",https://doi.org/10.1016/j.rie.2023.01.001,Cited by (0),"In this paper we apply a clustering procedure to detect trend changes in macroeconomic data, focusing on the GDP time series for the G-7 countries. A finite mixture of regression models is considered to show different patterns and changes in GDP slopes over time in the long-trend component. Two popular trend-cycle decompositions (i.e., Beveridge and Nelson Decomposition and Hodrick and Prescott filter) are considered in a preliminary step of the analysis to stress the differences between the two methods in terms of the inferred clustering, if any. This approach can be used also to detect structural breaks or change points and it is an alternative to existing approaches in a probabilistic framework. We also discuss international changes in the GDP distribution for the G-7 countries, highlighting similarities, e.g., in break dates, aiming at adding more insights on the ==== among countries. Our findings suggest that by looking at changes in slope over time a mixture of regression models is able to detect change points, also compared with alternative procedures.","Identifying structural breaks in macroeconomic data has attracted a wide number of researches (Stock, 1994; Bai, 1994, 1997; Bai and Perron, 1998, 2006, 2003; Koop and Potter, 2000; Clark and McCracken, 2005; Papailias and Dias, 2015; Wang and Tsay, 2019; Schweikert, 2022) particularly in the case of output, e.g., gross domestic products, series (Pritchett, 2000; Malik et al., 2022; Fokin et al., 2021). This is mainly because monetary and fiscal policies should be modified as the economic situation changes due to a structural break. The existing literature suggests that these trend breaks are fortunately quite uncommon (Perron and Wada, 2009, 2016). However, detecting the time of these breaks as they occur is controversial, and several approaches have been discussed in the literature (Hausmann et al., 2005; Perron and Qu, 2006; Jones and Olken, 2008; Kejriwal and Perron, 2010; Kar et al., 2013; Perron and Zhu, 2005; Papailias and Dias, 2015; Check and Piger, 2021), with a relevant debate of pros and cons of each.====If the breakdate is known a priori, classically Chow's test (Chow, 1960) for structural change is used (see also Andrews and Fair, 1988). Of course, all is about how confident we are about the guessed breakdate; otherwise, the test may be uninformative, as the breakdata is missed, or misleading, as the breakdate is endogenously correlated with the data.====In practice, thus, the breakdate is unknown and as such should be treated (Quandt, 1960; Bai and Perron, 1998). Here, following the idea of ==== i.e., a decrease in the trend function, introduced by Ben-David and Papell (1998), we consider a model-based approach able to identify these changes and provide a measure of uncertainty surrounding the reliability of the breaks.====There has been a great deal of interest in the shape and evolution of the cross-country distribution of GDP in recent years (see e.g., Russo and Foster- McGregor, 2022; Dungey et al., 2017, and references therein). We argue that such changes can be estimated and depicted with a certain probability by a change in the slope of the trend function in the GDP series. Accordingly, we introduce a data-driven approach, where the trend function is modelled in a finite mixture of regression model framework. The finite mixture literature is huge (for general introduction on mixture models, see McLachlan and Peel, 2000; Frühwirth-Schnatter, 2006; Everitt, 2013) and are commonly employed as a modeling tool to account for unobserved heterogeneity. To model heterogeneity in the time series features several approaches under the finite mixture framework are available in the literature (Gregoir and Lenglart, 2000; Alfo` et al., 2008; Pittau et al., 2010; Eirola and Lendasse, 2013; Compiani and Kitamura, 2016; Perron and Wada, 2016; Mazza and Punzo, 2017). The finite mixture models are convenient and flexible approaches to model observations when we face heterogeneous data and unknown shape and we can assume that this heterogeneity can be sufficiently described by introducing ==== sub-models. In our empirical context, the basic mixture model is extended allowing for different regressions in each sub-group of observations. This allows us to identify endogenous changes in the slope of the GDP levels, as a linear combination of two or more regression models, for the G-7 countries. The proposed approach is useful in many applications, every time we need to know when a structural change occurred. Treating the date of structural change – the break date – as unknown, the issues are how to estimate the break date and how to obtain uncertainty measures for the break date. The latter are easy to compute, as a by-product of the estimation step (see Section 2), and hence are very useful in applications, as they indicate the degree of estimation accuracy. Finite mixtures of regressions are a natural alternative whenever changes can be easily identified by changes of slopes in a growth model.====Of course, this is not the first attempt to answer to the question “when different trends can be estimated in the GDP series”: Bai and Perron (1998, 2003); Muggeo (2003) proposed different methods to check whether the change in slope occurs by estimating linear models with structural breaks or by segmented models, respectively. To further corroborate our results, we perform robustness checks and compare our results with those methods.====The methods are applied to estimate structural breaks in GDP trend series. The application of the finite mixture of regression is sound as most countries show a decline in the rate of growth over time (see Perron, 1989) with significant changes in the slope of the trend function identified by Perron and Yabu (2009). To wrap up, structural changes in the slope of the trend function might be a common feature affecting all real GDP series for the G-7 countries.====Finite mixture models with a fixed number of components are usually estimated with the expectation-maximization (EM) algorithm within a maximum likelihood framework (Dempster et al., 1977) and with MCMC sampling (Diebolt and Robert, 1994) within a Bayesian framework.====Our approach is convenient mostly for computational tractability without any preliminary assumption.====The plan of the paper is as follows: in Section 2 an introduction of the modeling framework along with the computational details, models selection criteria and a brief recalling of the two competing change point models are provided. In Section 3 series are presented in terms of the two decomposition procedures applied on the logarithmic transformation of the data, then the estimated clustering is discussed in terms of inferred sub-groups; a ==== on our findings is provided to compare them along with two existing change-points procedures; in Section 4 a wrap-up of our results is provided.",A finite mixture analysis of structural breaks in the G-7 gross domestic product series,https://www.sciencedirect.com/science/article/pii/S1090944323000017,12 January 2023,2023,Research Article,13.0
"Kohli Deepti,Mehra Meeta Keswani","Department of Economics, Swami Shraddhanand College, University of Delhi, Delhi 110007, India,Centre for International Trade and Development, School of International Studies, Jawaharlal Nehru University, New Delhi 110067, India","Received 28 October 2022, Accepted 8 January 2023, Available online 10 January 2023, Version of Record 16 January 2023.",https://doi.org/10.1016/j.rie.2023.01.002,Cited by (0),"This paper constructs a stylized model of election between two opportunistic candidates who can influence equilibrium policy platforms in exchange for monetary contributions provided by two distinct lobby groups. Two key features are embedded which give rise to a dual uncertainty in the model: the existence of partisan spread across voter groups as well as the embezzlement of campaign funds received by the electoral candidates from the interest groups. We derive and compare the equilibrium platforms of the two office-seeking candidates in three scenarios: none of the above uncertainties exist (benchmark case), only uncertainty about voters’ preferences exist (swing-voter case), and both the uncertainties exist (swing voters and lobby groups case). We find that an opportunistic candidate’s swing-voter ","Lobbying by special interest groups to influence the policy choices of electoral contenders through monetary donations is a well known aspect of the political economy literature (Austen-Smith, 1987, Baron, 1994, Grossman and Helpman, 1996, Besley and Coate, 2001, Grossman and Helpman, 2005 among others). The significance of campaign donations in altering electoral outcomes ultimately depends on how competently, or effectively a candidate utilizes campaign contribution money to garner votes from impressionable voters. For instance, if a political candidate is dishonest or corrupt, he or she may direct a part of the contributions towards either personal gain or political purposes, rather than for campaigning on the issue as intended by the interest groups. This is highly probable when a donor has no credible means of retaliation, or when the political party’s grip on power is uncertain. A real world example of political corruption can be observed in the U.S., where certain Congress members could use their campaign war chest to fund some of their personal uses. However, the 1979 Federal Election Campaign Act Amendments amounted to a change in an electoral contender’s degree of honesty (or dishonesty) by disallowing the practice of personal pocketing of campaign war chest (Groseclose and Krehbiel, 1994). Additionally, in Australia, former Victorian Liberal Party state director Damien Mantach was alleged to have embezzled around 1.5 million dollars of election campaign funds.==== Another practical example of a shift in the political corruption parameter could be the 2013 ==== case in the Indian Supreme Court, that disqualified convicted legislators as candidates.====There are a few studies in the existing literature that have analysed the link between interest groups and political corruption and its role in electoral competition models. For instance, Damania and Yalcin (2008) assert that greater electoral competition works towards reducing policy distortions, which however, encourages more intense lobbying, in turn, increasing the scope of corrupt behaviour. Le and Yalcin (2018) research the impact of lobby groups on electoral competition and equilibrium policy outcomes by utilizing a “money for policy favours” model of lobbying. They show that, in case of misappropriation of campaign funds, political parties that divert more funds for personal gain stand on more independent platforms and necessitate increased contributions from lobby groups. In a slightly different context, Wilson and Damania (2005) investigate the effect of corruption on environmental policy under varying degrees of political competition. They find that higher levels of political competition lead to stricter regulations and better environmental outcomes and that political competition has the capacity to decrease both grand and petty corruption, though this is not guaranteed.====In this paper, we focus on the combined lobbying and probabilistic voting model of Persson and Tabellini (2002) and extend it to capture the extent of corruption of electoral candidates in the form of misappropriation of campaign funds. The probabilistic setting and campaign fund embezzlement create a dual uncertainty within the model. The first is related to the random factors that can potentially affect voter’s decisions to vote for a certain candidate, which entails that electoral candidates in our model have incomplete information about voter’s preferences. The second uncertainty arises on part of lobby groups who are unsure whether an electoral candidate will honestly utilize their contributions to increase their chances of electoral success. We derive equilibrium policy positions of the two office-seeking candidates in the scenario where none of the above uncertainties exist (the benchmark case), where only uncertainty about voter’s preferences exist (swing-voter case), and where both these uncertainties exist (swing voters and lobby groups). This paper helps to fill an important literature gap by investigating the impact of such uncertainties both independently, and in interaction with other uncertainties on electoral competition and the policy-making process through an explicit comparison of the different policy equilibria derived across the three specifications of the model.====Our findings indicate that an opportunistic candidate’s tax platform in the swing-voter case is always lower than the tax platform of the same candidate in the benchmark case. This is because the presence of swing voters gives rise to an inherent candidate bias, which is exploited by an opportunist candidate through a deviation in the level of tax policy relative to the benchmark case. Additionally, our results indicate that the equilibrium tax platforms chosen by the electoral contenders in the presence of swing voters and opposing lobby groups will be greater (lower) than the level of tax chosen under the swing-voter case if the lobby group advocating a greater (lower) level of public good provision (and tax level) is sufficiently well-organized such that it outweighs the relative swing voter effect in that group.====Our comparative static results reveal that a lesser diversion of monetary funds by the electoral candidate induces the interest group to raise the level of donations to the candidate. This is because, lower embezzlement of campaign funds by a candidate provides greater marginal benefit to the donor interest group, which in turn incentivizes an increase in campaign donations to that electoral candidate. Next, a rise in the organizational ability of a lobby group results in lower financial resources being transferred to the electoral candidate in the form of donations for campaign expenditure, provided that the proportion of unorganized voters relative to the proportion of organized voters is smaller in comparison to the proportion of swing voters in that group. Furthermore, when an electoral candidate transitions from being highly corrupt to being relatively more honest, the equilibrium level of public good provision is found to adjust in conformity with the well-organized group’s economic preferences. Finally, a lower partisan bias within a group is found to persuade an electoral candidate to choose a tax platform closer to that group’s policy bliss point only if the strength of relative lobbying effect is weaker. Interestingly, if the relative organizational strength of an interest group is strong such that it outweighs the relative swing voter effect in that group, an unequivocal compensation of the loss in the candidate’s vote share from the first group via a gain in vote share from the remaining voter groups cannot be ruled out. In such a scenario, a rise in swing proportion within a voter group can induce electoral candidates to adjust their policy platforms in favour of the bliss point of the voter group having a greater inherent partisan bias.====The rest of the paper is organized as follows. Section 2 outlines the basic structure of the model and the stages of the game. Section 3 lays out the characterization of equilibrium for different specifications of the model. This is followed by a comparison of policy platforms in different equilibrium specifications of the model in Section 4. Section 5 deals with the comparative statics and provides intuitive explanation for the derived results. Finally, Section 6 concludes.","Impact of electoral competition, swing voters and interest groups on equilibrium policy platforms: Exploring the strategic forces at work",https://www.sciencedirect.com/science/article/pii/S1090944323000029,10 January 2023,2023,Research Article,14.0
Ferrari Alessandro,"European Central Bank, DG Monetary Policy, Monetary Analisys Division, Sonnemannstraße 20, 60314, Frankfurt am Main, Germany","Received 4 November 2022, Accepted 19 December 2022, Available online 29 December 2022, Version of Record 12 January 2023.",https://doi.org/10.1016/j.rie.2022.12.002,Cited by (0),"This paper studies the effect of deep recessions on intergenerational ==== by quantifying the welfare effects on households at different phases of the life cycle. Deep recessionary episodes are characterized by large declines in the prices of real and financial assets and in employment. The former levies high welfare costs on older households who own financial ====, the latter determines labour income losses and destroys the human capital of younger cohorts, lowering their productivity. The paper extends previous analyses in the literature by including permanent labour income losses in an OLG model calibrated to match the ====. The analysis shows that younger households lose more than double of all other living cohorts, as younger household become unemployed and experience a decline in their future income. The dynamics of households’ consumption and portfolio composition between 2007 and 2013 in the US are consistent with the predictions of the model.","Since the Great Financial Crisis, there has been an increasing debate on which segments of the population are most severely affected by deep recessions. The answer to this question is becoming more and more compelling also in light of the sharp contraction in economic activity induced by the outbreak of the Covid-19 pandemic. Indeed, the elevated public debt burden has reduced the fiscal policy space in many advanced countries and the available resources should be concentrated to alleviate the costs for those that suffer the most from the pandemic.====Individuals at the beginning of their lives do not have financial wealth but only “human wealth”, i.e. the present discounted value (PDV) of the stream of future labour incomes. As a consequence, they can be negatively affected by a fall in employment and the long-term effects associated to youth unemployment. On the other hand, older cohorts have accumulated assets throughout their lives but their human wealth is almost nil. They can be affected by the fall in asset prices. How these two channels affect different cohorts? Who are the “losers amongst the losers”? This paper studies the welfare effects of an aggregate shock on households at different life-cycle phases.====Kiyotaki et al. (2011) have highlighted that a recession creates a window of opportunity for young cohorts that can buy assets at a low price from older cohorts by leveraging with cheap credit and, in turn, they can enjoy a strong increase in their wealth in the future. Glover et al. (2020) use a calibrated OLG model to assess the intergenerational redistribution that took place during the Great Recession episode and found that higher labour income losses suffered by young generations would have been partially offset by “financial gains” while older households suffered the most from the recession. According to estimates in Glover et al. (2020) households, where the head is above 70 years old, experience significant losses from the Great Recession. By contrast, younger households who become active during a downturn suffer relatively less and, under some specific conditions, they may even enjoy net welfare gains if compared with those that become active in normal times.====Nonetheless, a crucial hypothesis for the above results is that the loss in labour income arising from the recession episode is temporary. In particular, they assume that unemployment impacts human wealth only through labour income missed during the recession and that it has no effect on the future stream of wages and employment. However, several empirical works (Ellwood, 1979; Jacobson et al., 1993; Bell and Blanchflower, 2011; Davis and von Wachter, 2017; Jarosch, 2014 among the others) have shown that earning losses from unemployment are persistent: a worker that suffers layoff and/or unemployment has, ====, lower labour income even after decades. Theory suggests that the “permanent scar” on the stream of future labour income can be explained by different factors, including less experience and on-the-job training, and loss in firm specific human capital caused by the displacement. The first channel is more relevant during severe economic downturns, like the Great Recession, due to higher duration of unemployment spell while the amplitude of the second effect is independent from the state of the economy.====This paper extends the previous analyses by taking into account the permanent losses in earnings arising from unemployment. Specifically, we extend the framework developed by Glover et al. (2020) to incorporate a labour market friction and human capital that cumulates through on-the-job experience. The labour provided by each cohort is determined exogenously: labour market is characterized by an entry friction that affects different cohorts in an heterogeneous manner and becomes less or more tight according to the state of the economy. Human capital affects risk-taking: a decrease in human capital by lowering the expected labour income in future periods makes the households poorer and therefore, with standard CRRA preferences, it increases the risk aversion of households.====The model is calibrated to match both macro and micro data. With respect to macro data the model matches the main empirical moments at the aggregate level observed during the Great Recession: the fall in GDP, the decrease in employment, the fall in the stock market. Long-term losses from unemployment for households of different age groups are calibrated using micro estimates from the empirical literature, and specifically those of Davis and von Wachter (2017). With these calibration the model exhibits a good fit of the fall in income of each cohort induced by the recession, a moment not explicitly targeted. The model is then used to compute the welfare loss of each cohort in terms of life cycle consumption equivalent compared with a counterfactual cohort that never experienced a recession.====The analysis reveals that households in their twenties are the most affected in a severe economic downturn as the Great Recession. Their welfare loss is more than double in magnitude than those of other living cohorts and amounts to around 23% of one-period consumption, that is the amount of consumption that they require over ten years (the length of our period in the model) to be indifferent with a counterfactual cohort that did not experienced the recession.====The quantitative model is then used to understand which channel is more relevant in explaining the difference in welfare losses among cohorts. The first channel involves a below-potential output for several years after the recession as a consequence of the loss in human capital experienced by living cohorts that reduces the amount of effective labour supply while they are active workers. Despite younger households are those that will spend most of their life-time in an economy with a lower output than the counterfactual, the quantitative analysis reveals that this channel plays a minor role in explaining their larger welfare loss. On the contrary, the effects of unemployment on labour income and especially on human wealth are the most important factors in explaining the difference in welfare loss among cohorts. In particular, as a consequence of the Great Recession, households’ in their 20 s experienced a loss in human wealth of 17%, a quarter higher than those of any other cohort in the model. These are the first-round losses, i.e. those arising from the partial-equilibrium effects. The second channel works through the behaviour of individuals: the loss in human wealth suffered by young households increases their degree of risk-aversion and , in turn, their purchases of risky assets and leverage with respect to a counterfactual cohort born in normal times. These results are opposed to those of Glover et al. (2020) on the Great Recession, which we show depend crucially on the absence of long-term effects from unemployment. Therefore gains identified by the theoretical work of Kiyotaki et al. (2011) fades away if permanent losses from unemployment are included into the model.====Having studied the working of the model and having assessed the channels of transmission of recessions, we test the validity of our results by comparing the predictions of the model for consumption and portfolio choices with actual data. Consumption data disaggregated by age groups from ==== (CE) and ==== (PSID) show a reduction in consumption share of households in their twenties between 2007 and 2010 of a magnitude in line with model predictions. With respect to portfolio choices, the model predicts that during a downturn young households, compared with their counterparts in normal times, have a lower share of risky asset and a lower leverage. Data from ==== (SCF) exhibits the pattern predicted by the model.====This work contributes to welfare analysis of the severe recessionary episodes as the Great Recession. With respect to the analysis of Glover et al. (2020), our contribution is to include permanent income losses from unemployment. This channel proves to be very relevant for a proper assessment of the welfare costs. The quantitative analysis shows that neglecting the permanent effects of unemployment leads to an underestimation of the losses of the younger cohorts. Our findings complement those in Hur (2018), who extends the framework of Glover et al. (2020) by adding intra-cohort heterogeneity, a dimension not considered here. Hur (2018) shows that during the Great Recession, a large fraction of young households, who tend to be more indebted and liquidity-constrained than other cohorts, could not take advantage of cheap funding and that the amount of risky assets in their portfolios did not increase. His model predicts a much larger loss for the younger, up to 8% of lifetime consumption or 33% in one period consumption (over a ten years period). These losses are additional to those estimated in this paper.====More generally, the paper relates to the studies on the welfare costs of an aggregate shock on different age groups. Doepke and Schneider (2006) study the redistributive effect of a positive inflation shock in a quantitative OLG model and find that while the redistribution of wealth from borrowers to lenders is zero-sum, gainers and losers are characterized by different responses in terms of consumption which do not offset. They also conclude that, on aggregate, positive inflation surprises increase the welfare of the economy: redistributive gains quantitatively offset the losses coming from monetary frictions estimated in other work (e.g. Lucas 2000).====Finally, the paper indirectly relates to the literature on the below potential growth after the Great Recession and the secular stagnation hypothesis (Hall, 2014, Ball, 2014, Fernald, 2015, Summers, 2014, Hansen, 1939, among the others). In our model, the permanent loss in human capital of working cohorts caused by the recession reduces potential output for some decades after the end of the recession.====The rest of the paper is structured as follows: in the next subsection, we look at previous empirical findings on the permanent effects of a crisis at aggregate and individual level and; in Section 2, we describe the model; in Section 3, we explain the solution method and the calibration; in Section 4, we present the welfare effects of the Great Recession and we provide a quantitative analysis of different channels at work; in Section 5 we test the model’s implications on micro data; finally, Section 6 contains the conclusion.",Losers amongst the Losers:: The welfare effects of the great recession across cohorts,https://www.sciencedirect.com/science/article/pii/S1090944322000680,29 December 2022,2022,Research Article,15.0
Abokyi Eric,"Department of Economics and Social Sciences, Universita‘ Politecnica Delle Marche, Italy","Received 24 June 2022, Accepted 24 November 2022, Available online 26 November 2022, Version of Record 5 December 2022.",https://doi.org/10.1016/j.rie.2022.11.002,Cited by (0)," have a wide range of benefits to households and the economy at large. Evidence show that women are the major recipients of remittances in developing countries, consequently this may have the potential of reducing the gender gap in economic outcomes. This study examines the impact of remittances on gender gap in ","This study seeks to investigate the impacts of remittances on the variation in financial inclusion and gender gap in developing countries. Table 1 shows the flow of remittances to different regions in the Low‐ and Middle‐Income countries (LMICs). The region with the highest flow of remittances is East Asia and Pacific, while Sub‐Saharan Africa region is last. The reported information shows that remittance inflows to the various regions, as well as the global level have generally shown an upward trend including the 2020 recession year except for the year 2016 where the inflows dropped slightly below the previous year's inflow. At the global level, the flow of remittances was estimated at $773 billion in 2021. This figure is forecasted to keep rising into the year 2022 and 2023. The continuous rise in the flow of remittances is forecasted to affect all the regions reported in Table 1, but East Asia and pacific region is projected to receive lower inflows than the 2020 figure of $137 billion dollars which was also a decline compared to the $148 billion dollars remittance received in 2019.====The inflow of remittances to LMICs as well as the global level was near record high growth in 2021, estimated at 605 billion USD (constituting 8.6% growth) and 773 billion USD (constituting 7.6% growth) respectively (World Bank, 2022). The high growth in remittances in 2021 is attributable to migrants sending more money to their families back home to cushion them from the hardship imposed by the COVID-19 pandemic. This is consistent with remittance literature which asserts that remittances can act as shock absorbers, increasing during periods of crises and reducing in good times (IFAD and World Bank; 2015; Amuedo-Dorantes, 2014; Yang, 2008). The fiscal stimulus programs implemented by many of the host countries, which led to an increase in economic activities and employment levels, in turn made it possible for migrants to send larger amounts of remittances back home in the year 2021.====While the global economy is struggling to escape the grip of the pandemic, it is still faced with the impact of the Russian invasion of Ukraine. The impact of the pandemic on the global economy is still significant, causing low growth in demand, supply shortages, and disruption in international trade. These devastating consequences have been heightened by the Ukraine crisis resulting in the unprecedented increase in commodity prices such as crude oil and natural gas, which started rising in the year 2021 and surged with the Russian invasion of Ukraine in 2022. The rise in the prices of crude oil, natural gas and food can be explained by the important role played by Russia and Ukraine in the supply of these commodities. According to FAO (2022), Russian and Ukraine are among the top producers of agricultural commodities globally. In the global energy market, Russia alone supplies 18% of coal, 20% of gas and 11% of crude oil (FAO, 2022). Countries that are more vulnerable to the economic consequences of the war are the import-dependent countries including the Sub-Saharan Africa, and the Middle East and North Africa regions (World Bank, 2022). Regions such as the Latin America and the Caribbean, East Asia and the Pacific, and South Asia are also exposed to the economic harmful consequences of the invasion but to a lesser extent. The projected increase in the prices of these commodities in the year 2022 and beyond will raise the cost of living which has implications for migrants who send remittances to support their families back home.====Remittances have a wide range of benefits to households and the economy at large. At the household level, remittances could be the lifeline for some poor households. According to IFAD and World Bank (2015), remittances have been referred to as the largest poverty reduction program in the world. Several studies have supported the poverty reduction benefit of remittances (Adam et al., 2008; Adams and Page, 2005; Gupta et al., 2009). Remittances can provide some financial stability to households living in rural areas with volatile income by smoothing their consumption and improving their welfare. In times of need or crisis, remittances tend to be a very reliable cushion for households and families. Remittances are usually timely and enough to meet the needs of the recipients. This can help in the timely payment of school fees, hospital bills, loans and investment. Despite all the benefits remittance receivers get from remittances, it also has some negative effects at the household level which include reduction in labour supply as recipients become dependent on it; conspicuous consumption among others (Amuedo-Dorantes, 2014).====At the macro level, there are a lot of benefits conferred on remittance receiving economies. According to Amuedo-Dorantes (2014), remittances provide improvement in credit worthiness, economic stability, increase the flow of investment to enhance economic growth and poverty reduction. Amuedo-Dorantes (2014) notes two important characteristics of remittance which are resilience and counter cyclical nature which enables it to ease economic instability. In times of crisis or natural disasters, the flow of remittances increases to meet the increased needs and reduces in the absence of these crisis (IFAD and World Bank, 2015; Yang, 2008). Bugamelli and Paterno (2006) notes that since remittances increase hard currency in the economies of receiving countries, it helps prevent the receiving economies from experiencing unforeseen current account reversals in periods of economic instability. They also noted that remittances attract new investments and improve the credit rating of remittance receiving economies. Notwithstanding the many benefits of remittances on receiving economies, some studies (IFAD and World Bank, 2015; Amuedo-Dorantes, 2014) have reported the tendency of remittances to cause appreciation of the currencies of receiving economies, making exports expensive and imports cheaper.====Some studies have provided link between remittances and financial inclusion (Gani, 2016; Aga and Peria, 2014; IFAD and world Bank, 2015). According to Ratha (2007), when households receive remittances, it augments their income and provide poor households with financial resources. Remittances create a starting point which allows the building of other inclusive and sustainable financial services (IFAD and World Bank, 2015). When individuals or households receive remittances, savings and other financial services could be accessed. This is because receiving remittances can create demand for a safe place to keep excess cash until it is needed, and the remittance receivers would usually turn to the banks for this service (Toxopeus and Lensink, 2008). The idle cash could be saved or invested until the time it is needed. Moreover, when remittances are received from a formal financial institution, the remittance receivers could be enlightened on the financial products that are available (Toxopeus and Lensink, 2008). Thus, while empirical studies on remittances and financial inclusion are scarce, the few available studies report a positive relationship or correlation between the two variables. Factors which promote financial inclusion in general may also impact the gaps in financial inclusion since it is usually the poor and the disadvantaged group who are excluded from accessing the services of the formal financial sector. Therefore, factors which ensure that more people have access to formal financial services may disproportionately benefit the poor and the vulnerable group and thus aid in bridging the gaps in financial inclusion. However, empirical studies that investigate the impact of remittances on variation in financial inclusion and gender gaps are almost non-existent. To have a proper appreciation of the variation in financial inclusion and gender gap situation in developing countries, more details about the problem is provided based on the Findex dataset.====The Findex 2017 survey shows that 69 percent of all adults aged 15years and above have an account as of 2017. This figure is an improvement over the previous surveys where 62 percent and 51 percent were recorded in 2014 and 2011 respectively. Meanwhile, account ownership in developing countries rose from 54 percent in 2014 to 63 percent in 2017. This leaves the number of unbanked adults globally at 1.7 billion as of 2017 compared to 2 billion in 2014, all of which live in the developing countries since ownership of account is almost universal in high income countries (Demirguc-Kunt et al., 2018). Among the unbanked adults, about half of the total live in just seven developing countries, namely, Pakistan, Nigeria, Mexico, Indonesia, India, China, and Bangladesh. In the absence of formal financial services, the unbanked adults may be forced to depend on informal mechanisms for savings and other financial services which may be expensive, insecure, and limited in scope.====While efforts geared towards financial inclusion have seen great strides, the gaps in financial inclusion continue to persist. Globally, the percentage of men with account ownership according to the Findex database was estimated at 72 percent while the percentage of women owning a bank account stood at 65 percent in 2017, thus showing a gender gap of 7 percentage points. This figure is higher for developing countries where gender gap is estimated at 9 percentage points since 2011 (Demirguc-Kunt et al., 2018). It is important to note that gender gap has remained unchanged at their current level since 2011. The size of gender gap in developing countries varies widely. While countries such as Turkey, Pakistan and Bangladesh have a very wide gender gap close to 30 percentage points, countries such as Cambodia and Myanmar do not have significant gender gap. For Argentina, Indonesia, and the Philippines, account ownership among women was higher than men (Demirguc-Kunt et al., 2018).====Apart from gender gap in account ownership, conspicuous gaps also exist in savings, debit and credit card ownership etc. At the global level, the Findex dataset shows that the percentage of men that saved in a financial institution was 24% versus 21% for women, and thus producing a gender gap of 3 percentage points in 2011. While the percentage of savings improved, the gender gap remained unchanged at 3 percentage points in 2014 but increased to 5 percentage points in 2017. In the case of developing countries, though the percentage of savings is lower than the global level, the pattern of change in the gender gap is identical to that of the global level as it also remained unchanged at 3 percentage points in 2011 and 2014 but increased to 6 percentage points in 2017. With regards to borrowing from financial institution (or using credit card), the gender gap was estimated at 3 percentage points at the global level for the years 2014 and 2017. For the same years, gender gap recorded for formal borrowings in developing countries was stable at 4 percentage points. Gender gap also exists in credit card ownership, which remained stable at 3 percentage points in 2011, 2014 and 2017 at the global level while for developing countries, gender gap in credit card ownership increased from 2 percentage points in 2011 to 3 percentage points in 2014 and 2017. For debit card ownership, the dataset shows relatively larger increases in gender gap in developing countries and the global level. At the global level the dataset shows a gender gap of 5 percentage points in 2011. This figure increased to 7 percentage points in 2014 and 9 percentage points in 2017. In the case of developing countries, gender gap in debit card ownership was 6 percentage points in 2011, which increased to 8 percentage points in 2014 and 10 percentage points in 2017.====Demirguc-Kunt et al. (2013) notes that financial services usage is significantly related to gender. In explaining factors contributing to the variation in access to financial services between men and women, they assert that gender norms and legal discrimination against women could account for the inequality in financial access faced by women. With regards to gender norms, practices such as early marriages and violence against women explain the variation in financial access while in the case of legal restrictions on women, factors such as restrictions in women's ability to work, be households’ heads, receive bequests, and decide the place to live, would reduce the likelihood that women would have access to financial services such as bank accounts, savings and borrowings.====Gender gap in access to financial services can result in gender gap in other economic outcomes. Meanwhile, empirical studies focusing solely on variation in financial inclusion are scarce since the few available studies mostly focus on financial inclusion and as rightly noted by Morsy and Youssef (2017), this situation could imply that some relevant aspects are neglected. This study departs from focusing on financial inclusion in general but rather on the variation and gender gap in financial inclusion and how these gaps are impacted by remittances in developing countries. Thus, the two main contributions to literature are the following:====To achieve these objectives, the study relied on fixed effects instrumental variable approach. The instruments for the study constitute the GDP per capita and the employment rate of the top 5 remittance sending countries. The findings of the study showed that remittances reduce gender gap in financial inclusion. However, in the case of the overall variation in financial inclusion, the study provided no evidence that it is impacted by remittances.====The rest of the study is organized as follows: Chapter two provides details of the methodology employed, while chapter three presents the results and discussion. In chapter four, the conclusion and policy implications are provided.",Effects of remittances on financial inclusion gender gap in developing countries,https://www.sciencedirect.com/science/article/pii/S1090944322000667,26 November 2022,2022,Research Article,16.0
Fujiwara Kenji,"School of Economics, Kwansei Gakuin University, Uegahara 1-1-155, Nishinomiya, Hyogo, 662-8501, Japan","Received 24 May 2022, Revised 19 September 2022, Accepted 8 November 2022, Available online 12 November 2022, Version of Record 28 May 2023.",https://doi.org/10.1016/j.rie.2022.11.001,Cited by (0)," location in a footloose capital model. We find that the home market effect is strongest in Cournot competition, second strongest in Bertrand competition, and weakest in monopolistic competition. And, we link the insights in industrial organization with our model of geography.","Trade cost has been decreasing steadily. For example, OECD (2018, p. 45) reports that the global transportation cost index fell from 100 in 1990 to 75 in 2015. According to the website of World Bank, the mean of the world tariff rate on all products fell by more than 50% during 1990-2017.==== In contrast to these well-established facts, the COVID-19 disruption and the resultant lockdown around the world have a big impact on the trade cost. As World Trade Organization (2020, p. 10) suggests, government responses to the pandemic can both raise and reduce trade costs in the future. Furthermore, the ongoing Russia-Ukraine war may raise trade costs because of the rise in energy costs. In sum, trade costs have been falling in the long-run, but may rise in the future, depending on the global response to the COVID-19 pandemic and Russia-Ukraine war. New Economic Geography (NEG) studies the impact of trade cost on the industry location.====Since the pioneering work of Krugman (1991), most NEG papers assume monopolistic competition.==== What implication is derived in other market structures? This question is important empirically as well as theoretically because evidence suggests that the share of exporters is small and oligopoly may better approximate current world trade.==== Indeed, recent literature on international trade has employed an oligopoly model.====We address the impact of the difference in market structure on the home market effect (HME) by developing a footloose capital (FC) model of NEG with Cournot, Bertrand or monopolistic competition. Here, the HME means that the country with a larger share of consumers (market size) involves a more than proportionate share of firms. Our main conclusion is that the HME is strongest in Cournot competition, second strongest in Bertrand competition, and weakest in monopolistic competition. In short, the difference in market structures has a quantitative difference significantly. After formally proving this result, we will argue that the insights in industrial organization are useful in interpreting it.====Comparison between Cournot and Bertrand competition has a long history in industrial organization. Singh and Vives, 1984, Vives, 1985 and Cheng (1985) show that when the number of firms is fixed, the equilibrium price is lower and welfare is higher in Bertrand competition than in Cournot competition. Cellini et al. (2004) and Mukherjee (2005) allow for free entry and prove that this result is reversed if goods are sufficiently differentiated. All of the above-mentioned papers exclude monopolistic competition. Adding monopolistic competition, Bertoletti and Etro, 2016, Parenti et al., 2017 and Chhy (2018) reach the same result that Cournot competition involves the largest number of varieties whereas monopolistic competition involves the smallest number. In particular, this paper is mostly inspired by Parenti et al. (2017). Focusing on a closed economy, Parenti et al. (2017) show the above ranking among the number of varieties. This paper is an attempt to extend their insight to an open economy.====This paper is organized as follows. Section 2 presents a model. Sections 3, 4 and 5 compute the Cournot, Bertrand and Chamberlin equilibrium, respectively. Section 6 proves the main result. Section 7 concludes. Appendices provide a few technical issues.",Market structure and industry location in a footloose capital model,https://www.sciencedirect.com/science/article/pii/S1090944322000576,12 November 2022,2022,Research Article,17.0
"Wu Chen,Nsiah Christian,Fayissa Bichaka","Department of Accounting, Economics, and Finance, Southeast Missouri State University, One University Plaza, Cape Girardeau, MO 63701 USA,Professor of Economics & Finance, School of Business, Baldwin Wallace University, 275 Eastland Road, Berea, OH 44017-2088 USA,Professor of Economics and Finance, Economics & Finance Department, Middle Tennessee State University, Murfreesboro, TN 37132 USA","Received 2 December 2021, Accepted 11 October 2022, Available online 19 October 2022, Version of Record 28 May 2023.",https://doi.org/10.1016/j.rie.2022.10.001,Cited by (0),"This study investigates the financial development and remittances nexus using a panel of 84 countries at all levels of income over the 1995 to 2018 period. We employ a dynamic commonly correlated effects estimator (DCCE) for a heterogeneous panel data model with weakly exogenous regressors to investigate the impact of financial development on remittances using indices that cover all aspects of the financial sector development (overall, institutional, and markets). The results indicate the importance of the development of the financial sector for ==== inflows, regardless of the motives. Our findings generally support the notion that improving the financial sector positively influences the magnitude of remittances for lower-income countries but not high-income countries. To attract more remittances, policymakers of low-income countries may implement policies that improve the development of the financial sector.","According to a recent report by the World Bank (2021), worldwide remittances registered a smaller than projected decline, thus, remaining resilient in 2020, despite the ravaging economic effects of the COVID-19 pandemic. The report further indicates that remittances to middle-income countries were just 1.6% below that of 2019, totaling $548 billion, thus, experiencing a lower decline (4.8%) than the 2008 global financial crisis. Furthermore, the reduction in remittances during the pandemic was much lower than its impact on other sources of foreign inflows of capital, including foreign direct investment (FDI).====The resilience of remittance inflows was also displayed during the 2008 global financial crisis. As depicted in Fig. 1, remittance inflows to low and middle-income countries remained stable during and after the 2008 global economic downturn. Thus, one can conclude that remittances are the most stable source of foreign capital inflows to many low and middle-income countries. Furthermore, the existing evidence suggests that remittances are becoming the largest source of external funds in these countries, surpassing foreign direct investment (FDI), export revenues, and foreign aid (Giuliano and Ruiz-Arranz, 2009), making them a critical financial resource, especially to low and middle-income countries. Consequently, remittances play an essential role in reducing poverty, increasing household consumption, and boosting investments that promote overall economic growth. Many governments, including China, India, El Salvador, and the Philippines, have thus, implemented policies to leverage migrant remittances for their national development. Since remittance inflows are increasingly becoming a vital external financial resource for economic catch-up, studies analyzing factors that promote or impede the inflows of remittances have become critical for policymakers, especially in low and middle-income countries.====Many factors may impact the magnitude and frequency of remittances. While several prior studies have analyzed the motives to remit, an analysis of factors such as the link between financial services development, macroeconomic conditions of host countriesis somewhat lacking or inconclusive (Akkoyunlu, 2013). The transmission channel of financial services development on remittances may be mainly through a well-functioning financial system that lowers the transaction costs and improves investment opportunities in the recipient countries.====Freund and Spatafora (2008) indicates that one of the main factors that may affect the frequency and magnitude of remittances is the state of the financial sector development of the home country and, to some extent, that of the host country. A well-functioning financial system can lower transaction costs and increase the volume of remittances (Aggarwal et al., 2011). Furthermore, the financial services development may also attract diaspora investments into the domestic economy to take advantage of improved returns and lowered risk (Benson, 2019). The progress in financial services may occur in the form of financial markets development and/or institutions. There still, however, no consensus ont the general effects of financial services development on remittances, possibly due to the different proxies used in the existing literature. Thus, it is essential to analyze the relationships between these subcategories of financial services progress and remittances to provide a more complete picture of the relationship between remittances and the financial sector development of the home countries.====While one may expect that the cost of remitting is inversely related to remittances, remitters tend to resort to unofficial routes to avoid paying high fees. The relationship between transaction costs and remittances may, however, suffer from reverse causality in cases where the magnitude of remittances leads to reductions in the overall average remittance costs. Thus, finding the right instrument for transaction costs and/or employing a model that deals with endogeneity is essential. Furthermore, the reliability of the estimated relationships and inferences drawn may be questionable due to data availability and reliability issues. Given the data availability issues, the projected inverse relationship between financial sector development and transaction costs, and the multi-effects of remittances, many studies adopt financial development as the determinant of remittances instead of transaction costs (Ezeoha, 2013; Mallick, 2017; Mookerjee and Roberts, 2011, and Singh et al. 2010). Most of these studies, however, only use a few indicators for financial development. For example, Olayungbo and Quadri (2019) employ a single proxy for financial development, such as the Broad Money Supply, which only captures a specific aspect of the financial sector. Such studies, thus, do not provide a complete picture of the relationship between financial development and remittances.====Freund and Spatafora (2008) and Fromentin (2017) state that the relationship between financial development and remittances could differ across different income levels, especially between high-income and low, or middle-income countries====. Similarly, Ezeoha (2013) concludes that financial development plays a more significant role in determining remittances for emerging markets than financially developed countries, thus, indicating that the relationship between financial development and remittances may depend on the level of the economic growth of the recipient country. Furthermore, it is well known that high-income countries are likely to have a relatively more developed financial system. This phenomenon appears to be confirmed by our data which show that the average ranking values of all the financial development indices are systematically higher in upper-middle and high-income countries.==== Thus, it is essential to investigate the differential effects of financial development on remittances across income groups. The findings from the income-group-based analyses can be crucial for countries (especially the low-income countries) to uncover the dominant motives for remittance inflows and, thereby, correctly formulating policies for the development of the financial sector and identifying the areas that demand the most needed improvement to facilitate remittance inflows for poverty reduction and overall economic growth.====Another critical issue in the panel framework, especially for a panel with many cross-sectional units (N), is the cross-sectional dependency, or common factors/common shocks. Cross-sectional dependence has become a crucial econometric issue due to omitted common effects, spatial effects, or interactions within socioeconomic networks such as globalization. Evidence shows that the conventional panel estimators, such as fixed or random-effects models are inconsistent in the presence of cross-sectional dependence and can result in a misleading inference (Chudik and Pesaran, 2015 a, b). To the best of our knowledge, very few studies test for the existence of cross-sectional dependence in their variables (see Fromentin, 2017). Still, none of the studies on the financial development's effects on remittances has incorporated crosssectional dependence into the estimation, rendering the conclusions questionable.====This study investigates the relationship between the financial sector development and remittances using more inclusive financial development indices with a data set covering 84 countries at all income levels. Financial development is a complex and multifaceted concept, and it is essential to include the impacts of as many measures and aspects as possible. Using a single proxy to capture effects of the financial development eliminates the potential to investigate the impact of subtle complexities and various aspects of financial development, including financial institutions and financial markets (Ito and Kawai, 2018). Thus, we employ the IMF's broader Financial Development Indicators created by Svirydzenka (2016) since they provide global indices for financial development comprised of three pillars: depth, access, and efficiency in financial markets and institutions. Additionally, we test for and correct the cross-sectional dependency by using the the dynamic common correlated effects mean group estimator (DCCEMG)) developed by Chudik and Pesaran (2015a) which can accommodate the cross-sectional dependence, heterogeneity, and weak exogeneity in regressors that are commonly observed in the panel data, thus, make our results more robust. We also investigate whether the financial development/remittance relationship depends on the home countries' income level. Implementing this exercise is essential because previous findings in the literature suggest that the level of financial development may rely on economic growth, where higher-income countries are likely to have relatively more developed financial systems. Furthermore, the diminishing marginal returns may render the impact of financial development to vary from one income group to another.====Our findings suggest a persistent dynamic effect in remittances and the primary motive for remittances is for altruistic purposes. Financial services development (IFD), their different components, and the home country's per capita GDP are essential determinants of remittance inflows. The low and lower-middle-income countries enjoy significant remittance-promoting benefits from the financial sector development. Countries in the upper-middle or high-income level no longer necessarily experience any increase in remittances by improving their financial sector.====The rest of this paper is organized as follows: Section 2 reviews the literature and provides our contribution to the literature. Section 3 describes the empirical methodology employed and the data used in the study. Section 4 presents and discusses the empirical results. The final section summarizes the findings, draws conclusions based on the results, and makes some policy inferences.",Analyzing the differential impacts of financial sector development on remittance inflows,https://www.sciencedirect.com/science/article/pii/S1090944322000540,19 October 2022,2022,Research Article,18.0
Muro Kazunobu,"Faculty of department of economics, Meiji Gakuin University, Shirokanedai 1-2-37, Minato-ku, 108-8636, Tokyo, Japan","Received 8 June 2022, Accepted 18 September 2022, Available online 23 September 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.09.004,Cited by (0),"We shed light on a nexus between fertility transitions and economic growth patterns. We construct a two-sector overlapping generations (OLG) model with endogenous fertility, physical capital, and human capital, where one sector produces goods, and another produces childcare services. If the elasticity of fertility for expenditure on childcare services is zero, the economy experiences ====, and fertility does not depend on physical and human capital, but it increases with parental child-rearing time. On the other hand, if elasticity is positive, the economy converges to a steady state, and the number of children becomes the homogenous function of degree of elasticity (less than one). In other words, fertility is the decreasing-return-to-scale function of physical and human capital. We show that a subsidy policy for education is more desirable than a subsidy for childcare services.","The purpose of this paper is to elucidate a relationship between fertility transition and economic growth patterns and examine implications of childcare services and private educational investment on fertility and economic growth. This is explored within the context of an economy in which physical and human capital accumulate over time, and the price of services in terms of goods is determined endogenously. In what kind of mechanisms are fertility, physical capital, and human capital determined endogenously in the macroeconomy? Some countries experience sustainable economic growth, whereas others reach a steady state as fertility decreases. Some epochs go through decreasing (or increasing) fertility, whereas others keep fertility constant roughly. What is a crucial factor for generating that difference?====Economic growth patterns are associated with demographic transitions. According to Galor (2005) and Galor and Weil (2000), the decline in fertility is a factor behind the transition from the post Malthusian growth regime to the modern growth regime. Bloom and Williamson (1998) observed rapid growth and fertility transition in the Asian economies in the postwar period. Madsen et al. (2020) shows that fertility transition has been a significant contributor to growth since the 1880s. The fertility transition took place in the West from the 1870s to the 1970s, which brings about the regime shift.====What is a crucial factor for generating the regime change? What does rust the engine of growth? Why does the turn occur from a negative correlation between fertility and female labor supply in 1970 to a positive correlation in 1990 in OECD countries? In a mature economy, what kinds of subsidy policies are effective in pushing up the fertility? In particular, which policies are desirable for subsiding childcare service and education? These are important problems in the modern macroeconomy.====Generally, an economy has evolved over time and shifted from goods to services, according to Kuznets facts. It is possible to reduce childcare time and increase working hours by using the childcare services provided in a market. Although Apps and Rees (2004) assume that childcare services are available to parents outside the home====, their model is what we call the partial equilibrium model. Here, the wage and interest rate are constant exogenously owing to the small open economy, and the relative price of childcare services in terms of goods is one; that is, the production possibility frontier (PPF) is linear. In their model, the dynamics of physical and human capital are ignored. On the other hand, we construct a two-sector overlapping generations model with endogenous fertility, physical capital, and human capital, where one sector produces goods, and another produces childcare services. Day (2012) assumes that childcare production is labor-intensive, we assume that the childcare serivces are produced not only by labor, but also by physical capital. This is because they are supplied by not only baby-sitters, but also day nurseries, preschools, and daycare centers, where these organized services use physical facilities as well as installations and equipment.====Muro (2022) constructs a two-sector overlapping generations model with endogenous fertility, physical capital, and childcare services, but without human capital. In his model, the elasticity of fertility related expenditures on services is crucial for determining labor participation and whether fertility converges to a steady state with monotone or oscillation. If capital intensity in the goods sector is greater than the products of elasticity and capital intensity in the service sector, then capital per capita and fertility converge to a steady state monotonically. Conversely, they converge to a steady state with oscillations, otherwise. In his model, endogenous growth can not occur. Incorporating human capital into the model by Muro (2022), we can elucidate the nexus between fertility transitions and economic growth patterns.====According to Apps and Rees (2004), the expenditure on childcare services in the market and the parental child-rearing time produce the number of children. In this fertility function, the elasticity of fertility with respect to the expenditure on childcare services is a crucial factor for whether or not the economy achieves endogenous growth. In this paper, we show that if this elasticity is zero, the economy experiences endogenous growth, and fertility does not depend on physical and human capital. On the other hand, if elasticity is positive, the economy converges to a steady state, and fertility depends on physical and human capital.====According to Muro (2022) which displays the total fertility rate (TFR) of seven countries: U.S.A, U.K., France, Japan, Italy, Germany, and Sweden, the TFR decreased sharply from 1960 to the 1980s, and remained roughly constant, except for insignificant fluctuations, from the 1980s to 2019. Especially, low fertility in Italy and Japan since the 1980s is remarkable. Here, we present the reasoning behind our hypothesis. We interpret that a decrease in the parental child-rearing time and an increase in working hours decrease the number of children in an era of high-speed economic growth, when the parents could not use very many childcare services. That is, we hypothesize that elasticity was zero from 1960 to the 1980s. We show that under zero elasticity, fertility does not depend on physical and human capital, but increases with parental child-rearing time====. In other words, this endogenous growth case leads to a negative relationship between fertility and working time. This theoretical result is consistent with Ahn and Mira (2002), who show that the correlation between TFR and the female labor participation rate in OECD countries was negative in the 1970s. This post Malthusian growth regime is result from a quality-quantity trade-off and a rise in the opportunity costs of having children. First, the impact of an increase in individual income on fertility is subject to the quality-quantity trade-off. An increase in income by households raises the indirect and the direct costs of children, because parents place more focus on children’s quality to raise the chances of their children by educational investment. It induces a substitution effect against the number of children in favour of the quality per child. Second, given the increase in income, households are encouraged to invest more time in labor market participation than in caring for children, which substitute work for children. The development of women’s employment, then becomes one of the most prominent factors explaining the decrease in fertility. An increase in the opportunity cost is an economic factor that strongly contributed to the sharp decrease in fertilily rates observed since the early 1970s when income was constantly increasing. Substitution dominates over the income effect when household income is limited and highly dependent on women’s earnings, with a consequent decrease in fertility.====On the other hand, creches and babysitters were included from 1980s. The feature of the modern growth regime is the service-oriented economy. The fall in high economic growth and the productivity slowdown is associated with shifts toward the service economy. Parents purchased childcare services and worked in the labor market by using the time released from childcare. Some of them worked in the service industry and produce services. Importantly, services can not be accumulated; a service-oriented economy does not promote accumulation of physical capital. Therefore, it forces the economy to converge to a steady state without sustaining growth. We hypothesize that the elasticity is positive from the 1980s to 2019 when convergence to the mature economy occurs. We show that under positive elasticity but (less than one), fertility depends on physical and human capital. Particularly, the number of children become the decreasing-return-to-scale function of physical and human capital. In other words, fertility is the homogeneous function of degree of elasticity (less than one). Therefore, even if physical capital and human capital increase, the number of children gradually diminishes. By using childcare services and working in the labor market, parents earn income and invest in education for children. Educational investment leads to human capital accumulation, which increases the number of children. Therefore, this case of non-sustainable growth leads to a positive relationship between fertility and working time. This theoretical result is consistent with Marinez and Iza (2004), who showed a positive relationship between TFR and female labor force participation since the 1980s.====We analyze the effects of subsidy policies on physical capital, human capital, and fertility, with market childcare services available for parents outside the home. We analyze three policies in the case where elasticity is positive and sustainable growth does not occur. First, we examine the effect of subsidy for childcare services, given zero subsidies for education. An increase in subsidy for childcare service decreases the number of children, physical and human capital, output, and education investment at the steady state. This does not affect the physical to human capital ratio, and increases the relative price of childcare services. This case corresponds to Baumol’s (1967) cost disease. Second, we examine the effect of subsidies for education, given zero subsidies for childcare services. An increase in subsidy for educational investment increases the number of children and human capital at the steady state. It decreases the relative service price, which increases childcare services, and the ratio of physical to human capital, which brings about economic transformation from physical to human capital [Galor and Moav (2004)]. Finally, we examine the effect of subsidies on education under a fixed tax rate. The higher the subsidy for education, the lower the subsidy for childcare services. An increase in education subsidies under a constant tax rate increases fertility. In summary, a subsidy policy for education is more desirable than a subsidy for childcare services.====Blundell et al. (2016) develop a dynamic life-cycle model of women’s labor supply, human capital formation (including both education choice and work experience), and savings. They examine the partial equilibrium model of households in detail, but does not consider the supply side of goods and services, the usage of childcare services, the price of services, and physical capital. Considering transformation from goods to service economy, we focus on economic growth and endogenous fertility in the general equilibrium model.====The remainder of this paper is organized as follows. Section 2 constructs a two-sector dynamic general equilibrium model with endogenous fertility. Section 3 considers the market equilibrium. Section 4 describes the proposed dynamic system. Section 5 analyzes the case in which the elasticity of fertility with respect to expenditure on childcare services is zero. The case gives rise to endogenous growth. Section 6 analyzes the case of positive elasticity. In this case, the economy converges to a steady state. The number of children depends on their physical and human capital. Section 7 compares the three subsidy policies. Finally, Section 8 concludes this paper.","Physical and human capital, fertility, and childcare services",https://www.sciencedirect.com/science/article/pii/S1090944322000527,23 September 2022,2022,Research Article,21.0
"Watanabe Masahide,Fujimi Toshio","Faculty of Economics, Ryukoku University, 67 Tsukamoto-cho, Fukakusa, Fuhimi-ku, Kyoto 612-8577, Japan,Disaster Prevention Research Institute, Kyoto University, Gokasho, Uji, Kyoto 611-0011, Japan","Received 19 May 2022, Accepted 18 September 2022, Available online 21 September 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.09.007,Cited by (0),"We estimate a smooth ambiguity preference function, wherein an individual faces multiple probability predictions of policy outcomes, and then empirically measure their willingness-to-pay for the policy, ambiguity attitude, and ambiguity premium. Climate change mitigation policy is used as the example. The estimation results reveal that most people have ambiguity-seeking attitudes, but that these attitudes are heterogeneous across individuals. People who are older, are university graduates, have higher income, or trust more in science show stronger ambiguity-seeking attitudes. Their willingness-to-pay can be underestimated if ambiguity is not considered. Moreover, individuals with stronger ambiguity-seeking attitudes support aggressive mitigation policies more strongly. Our estimation strategy is generally applicable to policy evaluations wherein policy outcomes are ambiguous.","Advances in science and greater data availability have enabled superior predictions of probability distributions for policy outcomes across disciplines. However, in some cases, no unique probability prediction has scientific consensus; this is largely because of the involvement of highly complex and nonlinear systems as well as insufficient data to generate relevant policy outcomes. Thus, multiple probability predictions exist (Been et al., 2014; Knutti et al., 2010; Meinshausen et al., 2009; Spiegelhalter, 2017; Spiegelhalter et al., 2011). For example, the effects of climate change mitigation policy are represented as multiple probabilities of below 2 °C of global warming relative to pre-industrial levels (Knutti et al., 2010; Meinshausen et al., 2009). The effects of smoke-free legislation, similarly, are characterized by multiple probabilities of preterm birth, low birth weight, and hospital attendance because of asthma (Been et al., 2014). This uncertainty about probability is called ==== (Etner et al., 2012; Trautmann and van de Kuilen, 2015).====To design successful policy under conditions of ambiguity, policymakers should consider ambiguity appropriately when predicting people's behaviors or calculating change in their welfare as a result of policy implementation. Such policy analyses require a structural model based on a decision model under ambiguity. More specifically, an empirically estimated decision model under ambiguity enables us to quantitatively conduct policy analyses such as counterfactual policy evaluation and measurement of willingness-to-pay (WTP) for policy changes under various conditions. Numerous studies since Ellsberg (1961) show that people's preferences are sensitive to ambiguity, and a variety of theoretical decision models under ambiguity exist to explain people's behaviors in cases where conventional decision models under uncertainty (====, the subjective expected utility model) cannot (for an in-depth review, ==== Camerer and Weber, 1992; Etner et al., 2012; Machina and Siniscalchi, 2014; Gilboa and Marinacci, 2016).====Our study presents an empirical strategy for estimating the smooth ambiguity model developed by Klibanoff et al. (2005) (hereafter, the KMM smooth ambiguity model) and conducts a quantitative policy analysis accordingly, under a realistic situation. The KMM smooth ambiguity model is a popular decision model under ambiguity. It has found expansive use across various fields of economics and has led to new insights in economic phenomena and new policy recommendations (Alary et al., 2013; Berger et al., 2013; Bleichrodt et al., 2019; Elabed and Carter, 2015; Gollier, 2011; Snow, 2011; Treich, 2010).====Most analyses assume that people have ambiguity-averse attitudes (====, Alary et al., 2013; Berger et al., 2013; Gollier, 2011; Snow, 2011; Treich, 2010). However, recent experimental studies show that attitudes toward ambiguity depend on the degree of likelihood of an uncertain event and the domain of the outcome (gain or loss domain); furthermore, these attitudes are heterogeneous across people (Abdellaoui et al., 2011; Baillon and Bleichrodt, 2015; Dimmock et al., 2015; Kocher et al., 2018; ==== Trautmann and van de Kuilen, 2015, for a recent review). The intensity of sensitivity toward ambiguity is also critical for policy analysis because, for example, even if people have an ambiguity-averse attitude, a very weak attitude may have a similarly weak effect on policy analysis (Cubitt et al., 2018). Policy analysis based on an incorrect assumption of ambiguity attitudes may then lead to erroneous policy recommendations (Kocher et al., 2018). In this light, it will be useful for policy analysis in conditions of ambiguity to empirically estimate the KMM smooth ambiguity model.====One challenge in empirically applying the KMM smooth ambiguity model to realistic policy analysis is in obtaining unobservable second-order probabilities. Cubitt et al. (2018) estimate second-order probabilities using the certainty equivalents obtained through choice experiments between certain and ambiguous options. Although their method is attractive because it can avoid explicitly dealing with individual perceptions of second-order probability, it does not fit the natural policy settings that are our focus. In a real-world policy situation, which is distinct from Ellsberg-type lottery choice experiments, choices between an ambiguous option and one that is certain are not natural, because there are few policies with certain outcomes. In addition, ambiguity attitudes obtained through choices between certain and ambiguous options may be different from those obtained through choices between options that are both ambiguous. Thus, we directly elicit second-order probabilities by expanding the graphics-aided web tool developed by Delavande and Rohwedder (2008), originally used for eliciting first-order subjective probability (==== Section 3 for more details on the web-based tool).====We take climate change mitigation policy as a realistic policy example. Climate change is a major global issue, and many scientific studies provide real ambiguous scientific probability predictions regarding climate change (Meinshausen et al., 2009). Using real ambiguous probability information in experiments is meaningful because, in many realistic situations, people have some probabilistic information on policy consequences rather than no information (Cubitt et al., 2018). Thus, using real ambiguous scientific probability predictions also allows us to understand individuals’ perceptions of ambiguous scientific probabilistic information.====We summarize our empirical strategy as follows. We implement a web-based survey to obtain the necessary data. In the survey, respondents are provided with 19 different real probabilistic predictions of global temperature rise exceeding pre-industrial levels by 2 °C or more by 2100, as (differentially) predicted by different scientific models (Meinshausen et al., 2009). These real, multiple scientific probability predictions are the source of the ambiguity in this study. We use a graphics-aided web tool to elicit the second-order probability, after which we conduct choice experiments in which respondents are asked to make a choice between two greenhouse gas (GHG) emissions targets that have ambiguous consequences. When respondents make the choice, they refer to their stated second-order probabilities. Using the elicited second-order probabilities and choice data, we estimate a structural model based on the KMM smooth ambiguity model, which includes parameters representing heterogenous ambiguity attitudes across people. Using the estimated model, we calculate people's WTP for a variety of climate change mitigation policies to reduce GHG emissions that change the degree of ambiguity in different ways based on different ambiguity attitudes. We also measure the ambiguity premium in order to examine the extent to which ambiguity affects WTP.====Our work is novel because, first, to the best of our knowledge, this study is the first to empirically estimate a structural model based on the KMM smooth ambiguity model and to quantitatively conduct policy analysis under a realistic ambiguity situation. Previous studies applying the KMM smooth ambiguity model to policy analyses are limited to theoretical studies; in contrast, this study uses real multiple probability predictions regarding climate change mitigation policies instead of Ellsberg's urn–type artificial events. This type of experiment is meaningful because situations in which people face multiple scientific probability predictions for policy outcomes are becoming more relevant to real policymaking. Our empirical strategy is applicable to other policy evaluations wherein policy outcomes are ambiguous.====Second, we measure ambiguity attitudes through choice experiments between two ambiguous options, the results of which may differ from those of previous studies that have measured them through choice experiments between risky and ambiguous options (Trautmann and van de Kuilen, 2015).====Third, we use a large sample size (==== = 1,768) that includes a variety of types of people instead of just students. Such studies are rare in the related literature (Dimmock et al., 2016). Our sample allows us to estimate the heterogeneity of ambiguity attitudes across subjects and examine its relationship to demographic variables. This objective is both important and understudied (Borghans et al., 2009; l'Haridon et al., 2018; Trautmann and van de Kuilen, 2015).====Through the empirical application of the KMM smooth ambiguity model to climate change mitigation policy, we obtain novel empirical findings and policy implications. Contrary to the assumption of ambiguity-averse attitudes often found in theoretical studies that apply the ambiguity model, our study finds that most people have ambiguity-seeking attitudes, which is consistent with recent experimental results that find ambiguity-seeking attitudes in the loss domain (==== Trautmann and van de Kuilen, 2015). However, these attitudes are heterogenous across people: Older people, women, university graduates, higher-income groups, and those who trust in science have stronger ambiguity-seeking attitudes. We also measure the WTP for climate change mitigation policies using real scientific probability predictions. The WTP for aggressive climate change mitigation policy will be underestimated if we do not consider ambiguity. In addition, our results indicate that providing ambiguity information to people might increase public support for aggressive policies.",Ambiguity of scientific probability predictions and willingness-to-pay for climate change mitigation policies,https://www.sciencedirect.com/science/article/pii/S1090944322000539,21 September 2022,2022,Research Article,22.0
"Agostino Mariarosaria,Errico Lucia,Rondinella Sandro,Trivieri Francesco","Department of Economics, Statistics and Finance ‘Giovanni Anania’, University of Calabria, Rende, Italy,Department of Economics and Statistics, University of Naples Federico II, Napoli, Italy","Received 10 June 2022, Accepted 1 September 2022, Available online 16 September 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.09.001,Cited by (0),"This work investigates the Bank of England's policy response to the London Financial Crisis of 1914, triggered by the outbreak of the Great War. By using daily data on discount operations drawn from the Bank of England's historical archive, we empirically test whether the Central Bank played the role of ","Central Banks (hereafter CBs) play a fundamental role in coping with financial crises (Schinasi, 2003; Selgin, 2010; Gorton and Ordoñez, 2014). Indeed, they may impose credit restrictions to preserve the stability of the financial system, but these could imply a failure of the whole banking system, gener- ating dramatic consequence for the entire economy (Calomiris and Mason, 2003; Richardson and Troost, 2009). The financial crisis of 2007–09 and the current COVID-19 recession have renewed interest for the famous Bagehot's rules (Gorton, 2008; Vives, 2008; Bignon et al., 2012; Bahaj and Reis, 2020; Rieder, 2020), which refer to a set of principles for successful lending of last resort oper- ations. These concepts have been summarized by scholars’ discussion in ====, the use of ==== only, and the reliance on ====, wording never used by Bagehot (Bignon et al., 2012). A lender of last resort (hereafter LLR) is defined as an institution that helps the financial system to overcome adverse conditions, such as an unforeseen shock that causes a massive increase in the de- mand for liquidity (Freixas et al., 2000). CBs – in the guise of LLR – should try to guarantee the ability of financial institutions to respect their obligations, preventing a collapse due to multiple bank failures caused by panic (Humprey, 1975; Reis, 2015). However, the role assumed by CBs, as well as their response to financial crises and the related ‘panic’ changed over time: the change towards a policy of LLR was very gradual, and a certain degree of credit rationing has always accompanied the Bagehot rules (Bignon et al., 2012; Jobst and Rieder, 2016).====The importance of the adoption of LLR operations has been highlighted during several historical events. As a matter of fact, Friedman and Schwartz (2008) argue that acting as a LLR could have avoided the sequence of bank failures during the Great Depression. Moreover, LLR operations have been implemented by the Federal Reserve to deal with historical crises such as the onset of the Second World War in 1939, the regime changes in government funding arrangements in 1958, and the Vi- etnam war in 1970 (Garbade, 2021). Although improved with financial and regulatory system reviews over the past century, LLR functions still affect central bank policymakers (Freixas, 1999).====Concerning the role historically played by the Bank of England (hereafter BoE or the Bank), the existing literature provides a puzzling picture. Indeed, according to a strand of the research, the BoE began to assume a role of LLR from 1870, ignoring the identity of borrowers if good collateral were brought in for discount (Fetter, 1965; Goodhart, 1988; Ogden, 2003). As reported by Capie (2002), the behaviour of the Bank can be explained by the ‘frosted‐glass discount window metaphor’, i.e. the Bank based its decisions on few simple rules, looking at good collaterals rather than observing the identity of the applicants. On the other hand, according to Flandreau and Ugolini (2011 and 2013) and Anson et al. (2019a), the Bank used to rigorously monitor the discounter identity to prevent moral hazard problems.====To contribute to this debate, our work empirically assesses the policy adopted by the BoE during the London Financial Crisis of 1914 focusing on a specific credit channel represented by the discount operations. To the best of our knowledge, this is the first empirical study which analyses the BoE's discount policy as a response to the financial turmoil triggered by the Great War. In doing so, we also aim to provide evidence on the role of the BoE as a lender of last resort.====Our empirical analysis, which takes advantage of granular data on daily discounts drawn from the BoE's historical archive, indicates that the BoE did not change its discount policy at the outbreak of the First World War. The evidence we provide suggests that – though the loan applicant identity rep- resented a determinant of Bank's lending decisions, as in Flandreau and Ugolini (2011 and 2013) and Anson et al. (2019a) – the Bank of England operated, according to the Bagehot rules, as a lender of last resort throughout 1914. Stated differently, our results suggest an improvement in risk management rather than credit rationing ==== Even though an increasing rejection rate is documented over the war-time, features related to the identity of the applicants – such as deep knowledge, better reputation, and higher quality of the bills submitted – seem to matter in reducing the probability of obtaining a rejection. As a result, engagement in screening and monitoring techniques may have represented a favouring condition – along with the consolidation of the Bagehot's idea – that has encouraged the gradual adoption of LLR policies over time. According to Bignon et al. (2012), deeper screening ac- tivities have represented a crucial factor in the development of modern lending of last resort. Indeed, the realization of LLR operations went hand in hand with the increasing quality of the London bill market, certified by prestigious merchant banks (Bignon et al., 2012). Therefore, reducing information asymmetries may have discouraged the adoption of credit rationing policies by offering a less severe constraint for central lending during crises (Anson et al., 2019a).====The remainder of this work is organised as follows: Section 2 explains the features of an LLR and describes policies based on credit rationing. Section 3 illustrates the historical context, focusing on the London money market. Section 4 describes the research setting and the data, while Section 5 illustrates the empirical analysis and the results. Section 6 concludes.",On the response to the financial crisis of 1914: The Bank of England's discount policy,https://www.sciencedirect.com/science/article/pii/S1090944322000473,16 September 2022,2022,Research Article,23.0
Boitier Vincent,"Le Mans University, France","Received 6 December 2021, Accepted 11 September 2022, Available online 14 September 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.09.002,Cited by (0),"Eaton et al. (2011) underline that firms with similar production costs, entry costs and demand export to different countries. In this theoretical article, I provide a rationale for this feature of the data. I demonstrate that similar firms exporting differently can be explained by a baseline trade-off between attractiveness and competition that is present in any model with ====. I then show that this trade-off also generates valuable theoretical features including distance-related mark-ups, third country effect and equivalence with random utility models.","In a seminal article, Eaton et al. (2011) highlight that there is a sizable amount of heterogeneity in terms of export destinations among firms. They document that market- and firm-specific heterogeneity in production costs, entry costs and idiosyncratic demand shocks account for a large, but incomplete, part of this diversity. A key consequence of this result is that, in the data, firms with similar production costs, entry costs and demand export to different countries: similar firms do export differently.====In this theoretical article, I show that similar firms exporting differently can be explained by a baseline trade-off between accessibility and competition that is present in any model with monopolistic competition.====To this end, I construct a simple model of export choice in which a fixed mass of homogeneous firms play the following two-stage game. They choose a unique destination to export to, and once the destination is set, they monopolistically compete with other exporters for this destination. In that context, I study the behavior of the economy in the short run and show that firms face a trade-off. This trade-off is summarized by the fact that, naturally, profits decrease in relation to the density of competitors (i.e., fear of competition) and in relation to the distance to the destination due to transport costs (i.e., accessibility). Hence, firms have an incentive to export to the most attractive economies to avoid paying high transport costs. However, they anticipate that these destinations are precisely those to which a high number of competitors will export. As a consequence, some firms are encouraged to export to less attractive countries to escape competition. Based on this effect, equilibrium is a situation in which a non-degenerated distribution of firms with respect to their destination choices balances the two opposite forces summarized above.====On top of its simplicity, the qualitative model has the appeal of deriving five predictions consistent with the data. First, I demonstrate that productive firms serve more distant countries and can have higher mark-ups. This means that, even if firms do not export to the same set of destinations; on average, firms with high productivity serve less attractive markets than firms with low productivity, which is line with Bernard and Jensen (1999) and Bernard et al. (2007). In the core of the article, I provide an intuition for this result. Second, as proved by Bellone et al. (2016), mark-ups are negatively correlated to competition degree and positively to distance. The model generates such an outcome. This is not explained by the standard quality differentiation argument as in Manova and Zhang (2012), but precisely because distance and competition degree are (endogenously) negatively correlated. Third, I demonstrate that firms that produce high-quality goods make more profits, have higher mark-ups and integrate into more distant markets than firms that specialize in low-quality goods. In so doing, I provide a theoretical understanding of the empirical findings emphasized by Martin and Mayneris (2015) and Fontagné and Hatte (2013). Forth, I assert that the presence of the escape-competition effect induces non-obvious implications of trade liberalization in terms of welfare. In particular, I show that the model displays trade externalities. I underscore that trade liberalization in any given region matters for more than that region because it prompts firms to export to that region and to desert others (i.e., a negative third country effect emerges as an outcome). Last, I demonstrate that the equilibrium of firms is equivalent to the equilibrium coming from a random utility model. This complements the recent findings of Behrens and Murata (2021).====This paper contributes to the trade literature. Originally, it is well acknowledged that the seminal framework of Melitz (2003) cannot reproduce the fact that firms can serve different sets of foreign markets. This is because the model of Melitz (2003) predicts a hierarchy of trade in the sense that any firm selling to the ====st most popular destination necessarily sells to the ====th most popular destination as well (see Eaton et al., 2011 for the notion of hierarchy of trade). Since Melitz (2003), the literature has improved the capacity of the model with firm heterogeneity to generate complex trade patterns. Many models now address the question why firms can serve different sets of foreign markets (see, among others, Blum, Claro, Horstmann, 2018, Chaney, 2014, Chaney, 2016, Eaton, Kortum, Kramarz, 2011, Manova, Yu, 2017, Song Kim, Osgood, 2019). However, the common feature of these settings is that they introduce additional dimensions of heterogeneity between firms. In the Eaton et al. (2011) model, the second dimension of heterogeneity that is added is market- and firm-specific heterogeneity in entry costs and idiosyncratic demand shocks. The model adds these factors by incorporating the Arkolakis (2010) formulation of market access. This extra element explains why firms make different choices regarding their decision to enter a given market: only the firm with the highest market-specific component of demand and/or the lowest market-specific component of fixed costs enters the country. Chaney (2014) also integrates a second dimension of heterogeneity between firms. He argues that firms can meet trading partners in two ways. On the one hand, they can meet trading partners by direct search, which is modeled as a geographically biased random search, while on the other hand, once a firm has acquired trading contacts in foreign locations, it can develop a new network from these locations: firms differ in their ability to develop a network of consumers in a given market. This new ingredient triggers the heterogeneity of export choices across firms: firms export to those markets where they are able to develop their network. More rapidly, Chaney (2016) assumes that firms face different liquidity constraints to finance entries. Manova and Yu (2017) suppose that firms can produce differentiated good by using inputs of different quality levels. Quality constitutes the new dimension of heterogeneity. Blum et al. (2018) survey the influence the new trade costs including the role of export and import intermediaries. Song Kim and Osgood (2019) underline the role of heterogeneity of trade politics across countries. In contrast to these contributions, I propose a new rationale without the need to add another heterogeneous dimension between firms. I highlight that the fact that similar firms can export differently is due to a baseline trade-off between attractiveness and competition that is present in any model with monopolistic competition.====This paper is also a complementary to models of pure price or wage dispersion. Burdett and Judd (1983) demonstrate that price dispersion can constitute a stable equilibrium even when firms and consumers are identical. Likewise, Burdett and Mortensen (1998) show that wage dispersion can emerge among homogeneous firms and workers. In both cases, firms reach the same profit level despite paying different wages or setting unequal prices. As in the article, this state of the world is explained by a trade-off from the point of view of the firms that encapsulates a competition effect.====The remainder of the paper is organized as follows. Section 2 presents the body of the export choice model. Section 3 gives some extensions of the baseline model. Section 4 provides the conclusions.",Why do similar firms export differently?,https://www.sciencedirect.com/science/article/pii/S1090944322000497,14 September 2022,2022,Research Article,24.0
Tsang Ming,"Department of Finance, College of Business Administration, University of Wisconsin-La Crosse, La Crosse, WI, United States","Received 16 June 2022, Accepted 11 September 2022, Available online 14 September 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.09.003,Cited by (0),"This study examines risk perception in an endogenous information setting, where information about an uncertain event can only be gathered if the uncertain event is chosen over all other alternatives. We conduct a laboratory experiment that employs a driving context, where participants are asked to make route choices over uncertain routes using a driving simulator. Based on the route choices participants make, their subjective belief of travel delay can be inferred and structured estimated. The results show that: 1) The average participants initially overestimates the risk of travel delay across high- and low-risk conditions. 2) In subsequent driving periods, only participants in the lowest risk condition express significant downward belief adjustment, resulting in their beliefs no longer being significantly different from the objective risk. This is consistent with the toll fee being the most elastic in the lowest risk condition, and the most inelastic in the higher risk conditions.","The study of risk perception in an endogenous information environment has not received much attention in previous economic studies. An endogenous information environment is where new information about an uncertain event can only be obtained if the decision maker chooses the uncertain event over all other alternatives, otherwise no new information is generated about the uncertain event. This is an information environment that commonly occurs in everyday decision making, such as when choosing from a number of routes to commute to work. Given that different routes have different risk of congestion, therefore, different risk of travel delay, a decision maker can only obtain information about a particular route if she decides to choose that route. For the routes that she does not choose, no new information will be generated about the unchosen routes.====Under this endogenous information setting, we conduct a laboratory experiment to examine how a participant's risk perception of travel delay changes across multiple subsequent rounds of driving. We implemented a range of congestion probabilities and ask: 1) Across multiple driving periods, will participants adjust their beliefs of travel delay in the direction of the true congestion probability? And does the path of adjustment differ depending on the underlying congestion probability? 2) Further, are participants able to form estimates of the risk of delay that vary with the underlying congestion probabilities?====To answer these questions, we recruited real-life commuters from the Atlanta and Orlando metropolitan areas and present them with a route choice task in a driving simulator. The field participants are required to make a binary choice between a route that has an uncertain level of congestion and one that has no risk of congestion. The participants are assigned monetary incentives for the value of making the drive, a fixed penalty for arriving late to the destination, and a toll charged on the non-congested route. Apart from some prior information about the frequency of congestion on the uncertain route, drivers only obtain additional information if they choose to drive it. Therefore, information feedback is endogenous. Furthermore, the experiment includes four probability-treatments that differ in the objective risk of congestion, which allows us to examine risk perception and adjustments across a range of objective probabilities.====The results of this study show that: across multiple driving periods, drivers in the lowest congestion risk condition are the only ones who express significant belief adjustment (for a route with an uncertain level of congestion), whereas no belief adjustments are shown for those in the higher risk conditions. One possible explanation is, when the initial belief of delay is high (as it is the case in the higher risk conditions), driving on the uncertain route and collecting information is perceived as riskier, thus participants are less willing to drive it, leading to limited or no belief updating for that route. This could mean that for sufficiently high-risk cases, belief adjustment will be very slow and possibly result in subjective risk deviating significantly from the objective risk.====The results of this study suggest that for drivers who hold a high initial belief of delay over a route they normally do not take, they could be reluctant to try it out even when conditions on their usual route become less favorable. Here, studying how drivers’ risk perceptions develop under an endogenous information environment could provide an explanation as to why many drivers enroute are reluctant to divert to an alternate route (Ziółkowski et al., 2021; Khoo and Ong, 2011), which is likely due to persistently false beliefs of congestion for the alternate route. In such a scenario, increasing the toll on the usual route (i.e., expressway) may have a limited effect in re-directing traffic flow.====The rest of the paper proceeds as follows. Section 2 reviews the relevant literature, Section 3 provides the experimental design, Section 4 discusses the underlying theory, and Section 5 provides the results. Section 6 concludes.",Risk perception in an endogenous information environment,https://www.sciencedirect.com/science/article/pii/S1090944322000485,14 September 2022,2022,Research Article,25.0
"Zoungrana Tibi Didier,Yerbanga Antoine,Ouoba Youmanli","Unit of Training and Research in Economics and Management, University Thomas SANKARA, Burkina Faso,Academic Institute of Initial and Continuing Training, University Thomas SANKARA, Burkina Faso","Received 21 February 2022, Accepted 7 August 2022, Available online 15 August 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.08.001,Cited by (1),"COVID-19 is a virus with a very fast spread rate in the world. Therefore, knowledge of factors that may explain such spread is paramount. The main objective of this research was to analyze the determinants of the virus spread worldwide. Unlike previous studies that were limited to traditional factors, this research extends the analysis to government measures (quarantine, containment, and response budget) against the spread of the virus. Thus, an econometric model relating the variable of interest to a number of variables was carried out using the Ordinary Least Squares (OLS) and the Two Steps Least Squares (2SLS) methods on a sample of 163 countries. The main findings indicate that economic factors such as the level of development, the degree of trade openness and the response budget to the COVID-19 pandemic, have a positive effect on the spread of the virus. With regard to social factors, the population density and confinement are major causes of the spread of the virus. Finally, temperature contributes to reduce the spread of the virus. These findings are robust to the estimation technique and to the measurement of the spread of the virus considered. In the light to these findings, implications for economic policies have been drawn.","Similarly to other diseases, the COVID-19 pandemic has a significant impact on economies. According to ====, the world has experienced several diseases related to coronavirus that have resulted in global health and economic damages. The best known are the Severe Acute Respiratory Syndrome (SARS) and the Middle East Respiratory Syndrome Coronavirus (MERS-CoV). SARS was a viral disease discovered in China in the late 2002.====During 2002-2003, 8,098 cases of SARS were identified, including 774 deaths. MERS was also a viral respiratory disease caused by a coronavirus that was found in camels in several countries. The first case was identified in Saudi Arabia in 2012 and spread to 27 other countries. Thus, 2,494 individuals have been infected with MER-CoVout of which 858 have died since 2012.====). Several scientific works have focused on the analysis of the economic impacts of this disease.====Although quantifying the effects of the virus is useful for economic debates, identifying factors that spread the virus is also relevant. Indeed, in controlling the spread of the virus, several States have invested in taking short and long run measures without any real scientific investigations on the relevance of those measures to limit the spread of the virus. The measures taken include quarantine, confinement====In line of Keynesian theory (====), a negative supply shock could in return lead to the spread of the virus given the reduction of economic means in fighting this pandemic. Moreover, this negative shock can be explained by the rationality of investors. In fact, limited by the unavailability of information associated with the evolution of the virus (Simon, 1997), they limit production decisions in a situation of uncertainty. This generalized pessimism from investors, described as ====, is also mentionedby ====.====Furthermore, in order to contain the negative supply shock related to COVID-19, governments have increased spending on social protection and taken measures to support the economy through businesses. Central Banks also intervened to support their respective economies. These measures have been effective in fighting against new infections, as in the case of Singapore and Hong Kong (Anderson ====, 2020). However, the effectiveness of the measures may not be the same and may also vary from one country to another. Several studies have shown that factors that may explain the spread of the virus are economic (====; Yun ====., 2020), social (====) and climatic (====; ====; ====).====, the COVID-19 pandemic requires sufficient public funding to ensure a comprehensive response. All countries have, at the onset of the pandemic, set up a COVID-19 Budget, financed by public and institutional resources and by private contributions. This required the reprioritizing public spending toward bolstering the economy and the health system, without knowing the effect of this response budget on the spread of COVID-19. Large sums have been injected against the spread of the virus and it is necessary to identify their effect on the spread of COVID-19.====The remaining part of the paper is organized around four sections. The first section addresses the review of literature on the socio-economic, governmental and climatic factors in the spread of virus. The second section outlines the research methodology. In the third section, the main findings are presented and analyzed. Finally, the last section concludes the research and provides implications for economic policies.",Socio-economic and environmental factors in the global spread of COVID-19 outbreak,https://www.sciencedirect.com/science/article/pii/S1090944322000369,15 August 2022,2022,Research Article,26.0
"Osman Syed Muhammad Ishraque,Islam Faridul,Sakib Nazmus","Data Analytics and Business Intelligence Program, Long Island University-Post, NY, USA,Department of Economics, Morgan State University, Baltimore, 10587, MD, USA,Lewis Honors College, University of Kentucky, 1120 University Dr., Lexington, 40526, Kentucky, USA","Received 30 March 2022, Accepted 7 August 2022, Available online 10 August 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.08.004,Cited by (0),"Does adopting social distancing policies amid a health crisis, e.g., COVID-19, hurt economies? Using a machine learning approach at the intermediate stage, we applied a generalized synthetic control method to answer this question. We utilize state policy response differences. Cross-validation, a machine learning approach, is used to produce the “counterfactual” for adopting states—how they “would have behaved” without lockdown orders. We categorize states with social distancing as the treatment group and those without as the control. We employ the state time-period for fixed effects, adjusting for selection bias and endogeneity. We find significant and intuitively explicable impacts on some states, such as West Virginia, but none at the aggregate level, suggesting that social distancing may not affect the entire economy. Our work implies a resilience index utilizing the magnitude and significance of the social distancing measures to rank the states' resilience. These findings help governments and businesses better prepare for shocks.","The onset of COVID-19 in 2020 triggered a global catastrophe on human life and business, not seen in generations. Several policies have been adopted to address the massive economic disruption and avoid a potential meltdown from spiraling. This research aims to assess the impact of one such measure, i.e., social distancing, for the US economy. The proposed policies quickly became political football, polarizing the entire nation. Several arguments have been made against this measure. To help readers understand the nature of the views expressed, we cite some of them here. They were quickly characterized as overreactions, reflective of panicked group thinking. As Yinon Weiss said, “The collective failure of every Western nation, except one, to question group think will surely be studied by economists, doctors, and psychologists for decades to come.”==== Rahn of Cato Institute stated, “Most of those in the 65-plus age who die are among the oldest, 80 and above, with underlying conditions, and who have a very short additional life expectancy. So, why .... Shut down the entire economy to lengthen the average lifespan of the oldest Americans by a few months at most?”==== Conservatives contend that lockdown measures are economically cost-inefficient.==== Others saw top-down measures as an infringement on first amendment rights—an abuse of power.==== They point out that this onesize-fits-all response not only dispossessed people of their free choices, but also intensified mistrust in a nation that is already deeply divided (J.D. Tuccille====). Some argue that lockdown/stay-at-home orders as a remedy have negative economic and social effects and are thus worse than the disease (C. Friedersdorf,==== Gov. K Ivey,==== and G Mullen.====) All of these views were considered strong. Curiously, these views were being ventilated at a time when things were really looking miserable for everyone and the US economy. In an effort to describe the situation, we briefly narrate the immediate impact of COVID-19 for readers to offer a background idea of what prompted policymakers to choose the social distancing measure. In the absence of any clear idea on what can be done to address the unknown COVID-19 and the associated crisis it brought about when we needed unity to face a national crisis of this magnitude. The COVID-19 infections and its rapid spread led to a 20% decline in global demand for agricultural commodities as early as March 20, 2020.==== It transpired that something had to be done. It appeared that the contagion was caused by close contact, implying a need to restrict movement. The initial economic toll was caused by a reduced demand for restaurant services. Many thought that government-mandated measures made matters worse, causing profound negative implications for the meat, dairy, and other perishable commodities market. While retaliatory economic statecraft between Russia and Saudi Arabia was blamed to be partially behind it, nonetheless, it was the biggest price drop in a single day on March 23, 2020 in nearly three decades, when Brent Crude dropped by 24%.==== The manufacturing sector was no exception. The British Plastics Federation (BPF) carefully examined the data to assess how the industry suffered from the COVID-19 outbreak. In a survey, they found that over the next two quarters, more than 80% of respondents anticipated a decrease in turnover of the implementation of the policy, and 98% expressed concern over the negative impact.==== In the US, the SP 500, Dow Jones Industrial Average, and NASDAQ all fell drastically and continued until the US government passed the Coronavirus Aid Act. Equity market indices rose by 7.3%, 7.73%, and 7.33%, respectively.==== Similar patterns were observed in the Asian and European markets. The 10-year US Treasury bond yields saw a sharp drop of 0.67%,==== To offset the negative effects of COVID-19 on markets, central banks across the world intervened with whatever means was considered feasible.==== Analysts compared such spending by the government to the post-Napoleonic, pre- and interwar era, where public liabilities rose exponentially.====Given that all the states employed varying levels of public safety response to face the raging pandemic, the lingering questions are: Did the early adopters sustain greater economic losses compared to those that did not? Did their economy perform better on average, relatively, as a result of their action? Our results suggest that despite a significant initial negative macroeconomic impact, it disappeared. Our study contributes to the literature in several ways, including methodological breakthroughs and innovative applications. Clearly, it is the first of its kind in the annals of economics and, in particular, in assessing the economic impact in the COVID-19 literature. Curiously, the merits of this methodology are still poorly understood in economic inquiry. Our approach is likely to lead to its further application to other economic topics and perhaps beyond. We implement the generalized synthetic control method (GSCM) to assess the impact of a shock, such as the lockdown order, on an economy, as in this case. As stated, the approach appears very broad in scope, with the potential for application to economics and other areas of inquiry. Its strength lies in the simplicity of its explanation of complex issues. However, it failed to find a hospitable home in economics, despite its ability to penetrate deeply into the domain of data analytics. We believe that this work will bring promises for potential spearheading a pathway for further application, even though it is not yet appreciated. From a policy perspective, the findings suggest that the measures failed to meet the desired efficacy overall, and policymakers are less likely to face hostility in pursuing social distancing, which might help to lessen controversy. When results from scientific studies suggest that a policy can be helpful in hindering the spread and assuaging adverse effects on an economy, it may be better appreciated by the general public. As for preparedness for future crises, the GSCM seems appropriate in this case because of its ability to help determine the influence of a shock such as the lockdown order—pre and post. Importantly, the GSCM uses a less restrictive set of assumptions compared to the traditional approach—the difference-indifference (DiD) method—thus results are expected to prove more reliable and robust. The policy has been applied broadly across the US, but the experience thus far points to limited success in some aspects; thus, more is needed to be done. We expect these findings to help avoid political polarization and divisiveness, a much-needed call for the day. As for household, business, and policymaking circles, they will be able to make decisions using educated and data-driven guesses of the “what if” kind of situation—the counterfactual scenario. It is fair to assume that states that adopted a lockdown/stay-at-home order (SHO) should have favorable economic outcomes. We take the position as valid to begin with, but there is more than one way of looking at it. States adopting the measure may have contained the spread of the virus and thus helped their economies indirectly relative to the control group, which might have been affected in a negative way. Once we estimate the impact on the economy and identify the factors, we expect to verify whether businesses that prioritized resources to face the crisis can help us to zoom in to the adopting states and rank them according to the severity of the impact. The remainder of this paper is organized as follows. Section 2 presents a literature review. Section 3 discusses the data sources and empirical strategy. Section 4 reports and discusses our results. Section 5 summarizes and concludes the paper with a note on the limitations of the study and directions for future research.",Economic resilience in times of public health shock: The case of the US states,https://www.sciencedirect.com/science/article/pii/S1090944322000382,10 August 2022,2022,Research Article,27.0
Ahiadorme Johnson Worlanyo,"Department of Economics, University of Verona, 37129 Verona, Italy","Received 30 March 2022, Accepted 7 August 2022, Available online 10 August 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.08.002,Cited by (0)," toward inclusive growth. The evidence from a large sample of countries shows that in both the short and long terms, low ","A key policy issue facing central banks today is the policy towards inclusive growth and welfare. This recognition is part of a broader, world-wide trend of visibly greater engagement of policy for welfare and equity purposes. Since the advent of the global financial crisis, some economists have pointed to the seemingly fragile nature of economic growth with some arguing that growth is generally non-inclusive and its benefits accrue mostly to the top of the income ladder. The Coronavirus pandemic and the not-too-distant financial crises have renewed keen interest in resilient and inclusive growth. The OECD estimates that amidst the Covid-19 pandemic, the global economy plummeted by about 4.5 percent in 2020 as the poor and the vulnerable felt greatly the brunt of the raging pandemic. Within the policy environment, the recovery plan includes an emphasis to kick start a new inclusive growth cycle. The focus of the discussion is shifting toward what can be done to stabilise employment, mitigate poverty and the rising inequality to foster greater inclusion. It is important to reflect on the outcome of policy choices of the past and shift the focus away from policy designs that target efficiency in isolation.====Policy institutions are faced with the duty to implement (monetary) policy consistent with achieving higher employment to address poverty, and rising inequality and attain inclusive growth. Jonathan D. Ostry of the IMF cautions that if they (policy institutions) fail, “progrowth reforms will lose political legitimacy, enabling destructive nationalist, nativist and protectionist forces to gain further traction and undermine sustainable growth”. The call is for inclusive growth rather than a straightforward economic growth strategy. This paper seeks to ascertain the inclusive growth impacts of macroeconomic stabilisation objectives of monetary policy. The research is motivated by the joint contribution of the ILO, UNCTAD, UNDESA, and WTO as part of the post-2015 development agenda, indicating that: “the broad objective of macroeconomic policy is to contribute to economic and social wellbeing in an equitable and sustainable manner”. In a World Bank contribution, Ferreira et al. (1999) emphasise that macroeconomic stabilization policies should achieve their macroeconomic objectives at the least cost to the most vulnerable. Notwithstanding the considerable differences amongst researchers, the general understanding is that economic policies that seek to stimulate growth must consider the implications for inequality and poverty, by emphasising equitable growth policies (Dagdeviren et al., 2001).====At least there is anecdotal evidence that sustained and inclusive growth critically depends on supportive macroeconomic policies (see, for example, Galli and von der Hoeven, 2001). The European Central Bank (ECB) emphasises the desire for macroeconomic stability in its strategy, believing that:====This paper offers a contribution to the inclusive growth literature within the context of a macroeconomic stabilisation policy. To contribute a different angle to the debate, we concentrate on one aspect of macroeconomic policy, namely monetary policy. This choice was not only dictated by concerns to keep the research focused but also by the persistent global application of monetary policy as an active instrument of economic policy. Monetary policy has assumed increasing importance across the globe as the last two decades have witnessed a plethora of monetary actions in both developing and developed economies of the world.====Inclusive growth parameters may generally not represent the mandate of central banks in the conduct of monetary policy. This observation provides the motivation to formulate inclusive growth as a function of the explicit macroeconomic stabilisation objectives of central banks; a process which would yield a reliable gauge for policy design towards “quality growth” as it would provide an indirect way to get at the realistic inclusive-growth implications of monetary policy stance. As the central bank's policy is implemented to achieve inter alia, stable prices and moderate economic growth,==== the study analyses the relationship between inclusive growth indicators and inflation and growth. Thus, the centrepiece of the analysis is a representation in which growth inclusiveness is a function of policy-related variables – a social welfare function that includes both inflation and economic growth. The study is consistent with the previous studies that tested the hypothesis according to which monetary policy indirectly impacts inequality. By this approach, the findings of this study can be extended to all countercyclical and stabilisation economic policies and perhaps, partly address the concerns regarding the coherence of policy packages in pursuit of the inclusive growth agenda.====This study seeks a data-driven answer to the realistic implications for the inclusive-growth impact of monetary policy via the mechanisms of price and growth stabilisation. This is crucial to provide robust empirical evidence on monetary policy design that is a win-win for inclusive growth. Monetary policy can contribute to employment stability since it seeks to stabilize the economy and minimize business fluctuations. Changes in monetary policy tend to influence poverty and income inequality via various transmission channels including interest rate, inflation, income, and asset prices channels. Thus, monetary policy tends to influence aggregate demand, growth and inflation and can affect the real economy and promote inclusive growth. Macroeconomic instability creates uncertainty, generates expectations of further instability, disrupts financial markets, and discourages physical and human capital investments. This retards growth and generally reduces average income to undermine inclusion.====This current research is not an attempt to exhaustively explain the drivers/policy packages to steer inclusive growth but to investigate whether the monetary policy environment matters for growth inclusiveness. We use data for 144 countries over the period 2000 to 2018 in a short and long-term analyses to test the hypothesis that a part of the variation in growth inclusiveness amongst countries can be explained by monetary policy-related variables. In terms of estimation technique, we utilise a system generalised method of moments (GMM) estimator in consideration of possible endogeneity and heteroscedasticity problems with the dynamic panel methodological approach. The estimation results show that macroeconomic stability is equalising and enhances the income of the poor to improve growth inclusiveness. These relationships are statistically significant in both the short and long terms and are quantitatively larger in the long term. These results suggest that monetary policy that controls inflation and output variability is likely to reduce poverty and income inequality and promote the inclusion agenda. Thus, monetary policy towards inclusive growth is most likely, sound monetary policy. The analysis also identifies investment in human development as a super pro inclusive growth strategy. The empirical findings indicate that even after controlling for the effect of economic growth, policies that lower inflation and promote human development can have a direct impact on the well-being of the poor and promote the inclusion agenda. This is however conditional on the initial inflation in the economy. In advanced economies where inflation rates are near zero, further disinflation induces huge unemployment costs, harms the income of the bottom of the distribution and hurts inclusion efforts. The indication from our analysis is that the twin objectives of macroeconomic stability and inclusive growth offer no trade-offs. Macroeconomic stabilisation policies should seek to achieve stable economic growth and complement this key objective with the need to stabilise prices and external balances to steer the economy towards sustainable inclusive growth. The evidence from our sample shows that greater inclusiveness depends on employment generation, distribution of income and poverty reduction.====Building on this section, the rest of the paper is outlined as follows: Section II presents the related literature; Section III discusses the data and the methods; Section IV presents the short and long-run analysis; Section V engages a threshold analysis to evaluate non-linear relations and Section VI gives the concluding remarks.",Monetary policy in search of macroeconomic stability and inclusive growth,https://www.sciencedirect.com/science/article/pii/S1090944322000370,10 August 2022,2022,Research Article,28.0
Ressin Marat,"York Entrepreneurship Development Institute; 907 Alness Street; Toronto, Ontario M3J 2J1 Canada","Received 29 October 2021, Accepted 7 August 2022, Available online 8 August 2022, Version of Record 25 November 2022.",https://doi.org/10.1016/j.rie.2022.08.003,Cited by (0),"The purpose of this study is to determine the relationships between the implementation of start-ups and the dynamics of the main characteristics of national economic growth. In developing the methodological design of the study, a quantitative approach was used, which allowed realizing the advantages of the integrated use of correlation and ====, analysis of trend models and general scientific methods of knowledge to analyze the time series model and prove the following hypotheses on the example of the economies of Canada, China, and South Korea. Н1: An increase in the number of start-ups has a positive impact on the sustainable development of the social sphere. Н2: More start-ups have a positive impact on the sustainable development of the economy. Н3: More start-ups have a positive impact on the sustainable development of the green society; Н4: The increase in the number of start-ups has a positive effect on the development of the institutional sphere. The study results substantiate a strong direct correlation between the implementation of start-ups and the achievement of the UN SDGs (Sustainable Development Goals), with a link proven not only for economic, but also for social, environmental, and institutional SDGs, which increases the importance of start-ups for achieving sustainable development in territories.====This study may be of interest to state and municipal officials in the implementation of measures to create a favorable startup ecosystem and to academic researchers, opening new avenues for future research.","The beginning of the third millennium was marked by significant global challenges for the world economy, provoking numerous crisis phenomena: from micro and meso crisis at the level of enterprises, industries, or regions to global economic crises of both endogenous (the Great Recession) and exogenous (economic crisis caused by the COVID-19 pandemic) nature. The influence of negative factors at the present stage of global economic development is so great that a considerable part of researchers call the modern period of economic development and, in fact, the global economy itself, turbulent (Kotler and Caslione, 2009). It was the turbulence of the economy that caused society's need for additional factors to ensure faster economic growth than the baseline indicators and, as a consequence, a quicker exit from the crisis - the so-called drivers of economic growth. In a turbulent economy, it is the drivers of economic growth that provide the fastest recovery of the economy in the post-crisis period, generating unique competitive advantages and providing additional opportunities for sustainable development. It makes sense that the search for economic growth drivers has been ongoing for the past decades, with researchers increasingly citing start-ups as drivers of economic growth, at the level of global (Berg et al., 2018), national, regional economies (Bowmaker-Falconer and Herrington, 2020; Korreck, 2019; Koster et al., 2012), and at the enterprise level (Gandhi et al., 2019).====The most recent global economic crisis, resulting from the impact of the COVID-19 pandemic and the quarantine restrictions imposed by virtually all governments to counter the pandemic, was, according to many researchers, the worst since the beginning of the 21st century, and damage from the COVID-19 pandemic in its first year reached figures comparable to estimates of damage from the Great Recession (Giroud and Ivarsson, 2020). In addition, the exogenous nature of the crisis critically limited the possibilities to minimize risks, which, in fact, put not only individual enterprises, but also entire industries on the verge of survival. In conditions of an exogenous crisis, it is extremely difficult to prevent negative consequences of the crisis, and the main advantages will be gained by those market players who will be able to ensure higher growth rates in the post-crisis period, which contributed to a sharp increase in social demand for research of economic growth drivers, including start-ups.",Start-ups as drivers of economic growth,https://www.sciencedirect.com/science/article/pii/S1090944322000357,8 August 2022,2022,Research Article,29.0
Funashima Yoshito,"Faculty of Economics, Tohoku Gakuin University, 1-3-1 Tsuchitoi, Aoba-ku, Sendai, Miyagi 980-8511, Japan","Received 28 November 2021, Accepted 26 July 2022, Available online 30 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.009,Cited by (0),"What is the optimal group size in the voluntary provision of public goods in a purely altruistic economy? The popular consensus on this fundamental question is that the free-rider problem worsens as the group size increases. This study provides a counterexample of the consensus by featuring plausible threshold preferences for certain typical public goods. Under these preferences, marginal utility hardly diminishes below a threshold level, but declines significantly in close proximity to the threshold and nearly drops to zero above the threshold. We find that threshold preferences significantly reduce inefficiency. We also show that if marginal costs increase, then the threshold preferences lead to a partly positive relationship between efficiency and group size, which allows us to detect the locally efficient group size. Moreover, the locally efficient group size is proportional to the slope of the marginal costs as well as the threshold of marginal utility.","The effect of group size in a voluntary economy on the degree of efficiency in the private provision of public goods is a fundamental question in public economics since Olson’s (1965) pioneering work. To date, many researchers have studied this issue in a purely altruistic economy.==== In simple homogeneous models with noncooperative contributors, theoretical studies reached a broad consensus that the deficit of the Nash equilibrium below the optimum provision of public goods increases as the group size increases. For example, Cornes and Sandler (1985) studied quasi-linear preferences and Mueller (2003), among others, studied Cobb–Douglas preferences.==== Isaac and Walker (1988) provide experimental support for the theoretical consensus.====Although we often take such consensus for granted, in this study, we show that, in particular cases, the issue is more nuanced than previously understood. Early theoretical works did not address the possible preference features of public goods sufficiently.==== Recognizing that there are no appropriate preferences for all types of public goods, it is even more important to investigate how (in)efficient resource allocation can arise from every conceivable angle. Specifically, we cannot rule out the a priori argument that the diminishing degree of marginal utility depends to a greater extent on the usual consumption level. As part of the possible preferences, our analysis considers the case of individuals with a threshold preference for public goods consumption. Under this threshold preference, marginal utility hardly diminishes below a minimum threshold, but declines significantly in close proximity to the threshold, and drops to almost zero above the threshold; that is, unlike the standard case, utility increases in an approximately linear fashion below the threshold and hardly increases above the threshold, as Fig. 1 shows. We refer to this as individuals’ threshold preference (of public goods).====The threshold preference is in fact considered plausible in some cases of well-recognized public goods such as volunteer work and clean environments. For example, one could assume a charity for natural disaster victims to be a social contribution issue. In this case, the marginal utility of supportive activities could potentially decrease slightly at insufficient levels of total contributions because the victims still tolerate the inconveniences of an unsettled lifestyle. Rather than assuming an immediate diminishing rate of marginal utility, it is more likely that it decreases substantially only after the total contributions sufficiently aid the victims. Once their standards of living are sufficiently restored, the marginal utility can become virtually zero.====As another example, consider garbage-strewn beaches and a local beach clean-up effort as an environmental issue. Regardless of how much waste some people collect, the marginal utility of an additional clean-up would remain high as long as the remaining waste is visible and thus spoils an intrinsically beautiful landscape. Only after the beaches are restored to a satisfactory level of cleanliness will the marginal utility finally begin to decrease noticeably. Eventually, little marginal utility is derived from removing the remaining limited and inconspicuous waste.====The plausible examples are not limited to the two examples above. Public transportation infrastructure is a good example. No one can use infrastructure such as roads and airports before construction is complete, and the status of whether it is under construction or completed is important. In other words, regardless of the stage of construction, contributions to construction yield high marginal utility; however, once contributions reach the completion level, further contributions become meaningless and yield very little marginal utility.====In contrast to the theoretical consensus described above, our analysis reveals that the provision of public goods in the Nash equilibrium can lead to sufficiently efficient outcomes. To this end, we consider a very simple and standard model in the literature that assumes an altruistic economy, except the utility function has three features: (a) marginal utility diminishes relatively slowly when public goods are below the threshold, (b) it declines relatively sharply only when public goods provision is in close proximity to a threshold value, and (c) marginal utility is almost zero when it exceeds the threshold. We emphasize that these three features are associated with relative changes in marginal utility and retain the standard assumptions; that is, positive marginal utility and the law of diminishing marginal utility. Thus, our analysis is consistent with the underlying framework in the altruism literature, but we nevertheless conclude that voluntary provision of public goods can lead to sufficiently efficient resource allocation.====In addition, we find that if the marginal cost of contributions from individuals increases, then the threshold preferences are contrary to the broad consensus that an increase in group size inevitably leads to lower efficiency. Although considerable existing literature assumes constant marginal cost, an increase in marginal cost seems rather natural in the provision of certain public goods. For example, if the provision of public goods involves specific physical tasks, then contributors gradually become fatigued as their contributions increase, and contributors eventually become exhausted. The aforementioned examples of volunteering activities and keeping the environment clean apply to this case.==== Based on this natural assumption, we demonstrate that threshold preferences lead to a partly positive relationship between efficiency and group size, and thus allow us to detect the locally efficient group size. Within the confines of the simple noncooperative behavior of contributors in a purely altruistic economy, this study is the first to reveal the locally efficient group sizes.====This study is at the crossroads of two lines of research. The first stream addresses the efficiency–group size nexus in the voluntary provision of public goods in a purely altruistic economy, with certain representative studies already mentioned above. Among others, Hayashi and Ohta (2007) deserve special mention as a significant precursor to this study. The authors embraced this idea by positing two assumptions: the increasing marginal cost of voluntary provision and the existence of a finite satiety point in utility. They consequently claim that inefficiency is alleviated as group size increases, and optimality is achieved when the group size approaches infinity. While their work overlaps with ours, there are notable differences in both the assumptions and findings. Specifically, we assume threshold preferences instead of a finite satiety point in utility and find substantial efficiency improvements when the group size is not infinite. The characteristics of threshold preferences are similar to the assumption of satiation in that the threshold level of provision could be interpreted as roughly corresponding to the satiation level. However, there are crucial differences between the two: in threshold preferences, marginal utility diminishes slowly below the threshold level and declines sharply in close proximity to the threshold level. In addition, as detailed below, we relax the assumption of increasing marginal cost, where the marginal cost must approach zero as the individuals’ contributions approach zero.====The second line of research is on the characterization of the types of public goods (e.g., pure and impure public goods). Traditionally, these types include congestible goods and local public goods. In this stream of research, this study is most closely related to the recent growing body of literature on threshold public goods (e.g., Cadsby and Maynes, 1999; Spencer et al., 2009; Brekke et al., 2017; Cartwright et al., 2019). While plenty of studies have adopted experimental approaches to investigate a public goods game, threshold public goods are characterized as consumable only when the total contributions surpass a minimum threshold (provision point). Thus, threshold public goods and present threshold preferences are analogous in that the total contributions have meaningful threshold values. However, they critically differ from each other; that is, in threshold public goods, if the total contributions exceed a critical level, then the goods are provided, whereas in threshold preferences, marginal utility is sizable only when the total contributions are below a critical level.====The remainder of this paper is organized as follows. In Section 2, we present the analytical framework. We formally define threshold preferences and describe their properties in Section 3. In Section 4, we examine the relationship between efficiency and group size analytically. In Section 5, we use a numerical analysis to provide further results that are analytically ambiguous. We discuss other possible threshold preferences in Section 6, and present our concluding remarks in Section 7.",Efficiency and group size in the voluntary provision of public goods with threshold preference,https://www.sciencedirect.com/science/article/pii/S109094432200031X,30 July 2022,2022,Research Article,30.0
"Thiombiano Noel,Ouedraogo Salifou,Moussa Abiboulaye","Department of Economics and Management, Thomas Sankara University, 12 PO Box 417 Ouagadougou 12, Burkina Faso,Department of Economic Policies and Internal Taxation, West African Economic and Monetary Union Commission, 01 PO Box 543 Ouagadougou 01, Burkina Faso","Received 9 December 2021, Accepted 26 July 2022, Available online 29 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.008,Cited by (1),None,"From 2015 to 2019, global economic growth was estimated to an annual average of 3.3% compared to 3.2% for African countries. In Sub-Saharan Africa, annual average economic growth during the same period was estimated to 2.8% due to the slowdown in world trade, the tightening of financing conditions and the strengthening of the US dollar. However, this growth fluctuated over the period. Indeed, it went from 3.4% in 2015 to 1.4% and 3.1%, respectively, in 2016 and 2019. Moreover, in the WAEMU, the average annual growth rate over the same period was 6.3% due to the dynamism of commercial activities and market services, agriculture, mining and construction activities.====Fluctuations in growth have a different explanation. Indeed, according to the proponents of the traditional cycles theory (Keynes, 1936; Samuelson, 1948), due to the imperfect functioning of the regulating mechanisms, in particular, the price adjustment problems or demand imbalance, the economy moves from one imbalance to another. Thus, for the latter, the economic cycles observed are disequilibrium cycles. However, for economists supporting the new classical cycles theory (Kydland and Prescott, 1982; Lucas, 1977), fluctuations represent economies’ reactions to exogenous shocks, in particular, monetary shocks and real shocks. Thus, for these economists, due to the fact that fluctuations are the consequences of the optimal reactions of agents, the observed cycles are equilibrium cycles.====Assessing the implementation of the multilateral surveillance reveals several postponements of convergence deadlines due to the inadequate macroeconomic performance of member states since the implementation of the CSGCP, with respect not only to the emergence of socio-political crises that some member states have faced, but also to difficulties related to its institutional framework. Thus, the Additional Act No. 01/2015/CCEG/WAEMU on the CSGCP was adopted in order to keep the multilateral surveillance, ensure the sustainability of public finances and ensure the compatibility of national budgetary policies with the common monetary policy. So, new convergence pact, which consolidates all the texts currently in force and dispersed in various texts that have modified the basic text, proposes new convergence indicators, made of five (5) convergence criteria, three (3) of which are primary criteria. The two (2) second-tier criteria are essentially intended to contribute to compliance with the key criterion. The convergence horizon, initially set for 31 December 2013, is extended to 31 December 2019. The key criterion of the new framework is the overall balance, including grants, which replaces the basic budget balance used until now.====The assessment of the implementation of the new criteria reveals that the inflation rate remains very satisfactory, with all countries having met the criterion during the recent period. The criterion on the ratio public debt to nominal GDP has been met by all countries in recent years, particularly following the relief obtained under the Heavily Indebted Poor Countries (HIPC) Initiative and the Multilateral Debt Relief Initiative (MDRI). However, member states have had difficulty meeting the key criterion of overall fiscal balance, including grants. Indeed, in the WAEMU, this ratio deteriorated over the period 2015–2018, rising from −2.3% in 2014 to −3.3% in 2018, with a low point of −3.6% in 2016 and 2017 before improving to −2.5% in 2019. Thus, between 2015 and 2019, the number of countries that met the key criterion was, respectively, three (3) in 2015, zero (0) in 2016, four (4) in 2017, three (3) in 2018 and six (6) in 2019. Consequently, by the end of December 2019, the assessment of the implementation of the Supplementary Act revealed that none of the member states has been able to sustainably meet all of the first- and second-tier criteria, and the most met criteria by the states remain the level of inflation and the debt ratio. Indeed, currently, five (5) Member States met all three (3) first-tier criteria against three (3) in 2018. These are Benin, Burkina Faso, Côte d'Ivoire, Mali and Togo. Thus, the conditions for convergence were only met in 2019====. However, the conditions for access==== to the stability phase, starting on 1 January 2020, have not been met.====Regarding the difficulties that countries have in meeting the convergence conditions, i.e., respecting the key convergence criterion related to the overall budget balance, including grants as a percentage of nominal GDP, and thus maintaining a genuine macroeconomic stability framework despite the high growth rates recorded in the Union between 2015 and 2019, it is necessary to examine the relationship between budgetary rules and economic growth in the WAEMU. This is motivated, on the one hand, by the fact that discretionary fiscal interventions can be pro-cyclical, i.e., when a country pursues expansionary fiscal policies (higher spending and/or lower taxes) during good economic times, and restrictive policies (lower spending and/or higher taxes) during recessions. On the other hand, WAEMU member states are, for example, assessed on the basis of compliance with the convergence criteria to determine the appropriateness of fiscal policy in the Union. For instance, an overall budget balance, including grants, in excess of 3% of GDP could be explained by an economic slowdown, a sharp increase on debt interest rates or the advent of a low inflation rate compared to the expected one.====Prior investigations have attempted to provide some answers, particularly Tanimoune et al. (2008), and Ary Tanimoune (2011), through the determination of fiscal adjustment mechanisms and the cyclicality of fiscal policies in the WAEMU. In addition, Wade (2015) sought to determine whether the key fiscal criterion (basic budget balance) used in the WAEMU before the adoption of the new criteria in 2015, has led to debt control and cyclical stabilization in order to achieve sustained growth in the union. Thus, given that almost all existing studies are based on the old convergence criteria, this research is in line with these previous studies but differs from them in that it takes into account the overall fiscal balance, including grants, as a key criterion in accordance with the Additional Act No. 01/2015/CCEG/WAEMU. This research also attempts to estimate budgetary reaction functions in order to identify the objectives of macroeconomic cycle stabilization and public debt stabilization in the WAEMU according to the state of the economies in the cycle.====The rest of this article is structured as follows. The first section analyses the fiscal impulse in the WAEMU, the second section presents the literature review, the third section presents the methodology and the fourth section presents the analysis and interpretation of the estimation results.",Fiscal policy rules and economic fluctuations in the countries of the West African Economic and Monetary Union (WAEMU),https://www.sciencedirect.com/science/article/pii/S1090944322000333,29 July 2022,2022,Research Article,31.0
"Mahata Sushobhan,Khan Rohan Kanti,Chaudhuri Sarbajit,Nag Ranjanendra Narayan","Department of Economics, University of Calcutta, Kolkata, India,Department of Economics, St. Xavier's College (Autonomous), Kolkata, India","Received 2 December 2021, Accepted 26 July 2022, Available online 28 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.010,Cited by (3),"COVID-19 has posed severe challenges not only to researchers in the field of medicines and natural sciences but also to policymakers. Almost all nations of the world lockdown have been chosen as an immediate response to this pandemic crisis. The labour market in developing economies continues to be gendered with gender-based wage differentials besides occupational segregation, women who are the marginalized section in the society, bear the brunt of the unprecedented COVID-19 lockdown. Against this backdrop, a multi-sectoral ==== model has been constructed with heterogeneity in migration (with and without family migration) that has been derived from the intra-household bargaining problem amongst unskilled families to analyse the gendered effect of the pandemic. Lockdown has been conceptualized as a restriction on the physical gathering of labour in the contact-intensive sectors. The results of the paper reflect internal contradictions of developing economies that have a conditional-conditioning relationship with an archaic structure.","). ==== found a large decline of about 60 per cent in household non-farm income and that labour was almost wiped out from the lockdown. Besides this, migration has been a major problem in the Indian labour market. India, with a migrant population of about 5.6 crores (Census, 2011), witnessed the worst circular/reverse migration of labour in its history since independence. Around 10.4 million to 10.5 million migrant labourers moved from urban to rural areas of origin in about 30 days of the first phase of lockdown (====; ====). The labour market in India continues to be gendered with gender-based wage differentials besides occupational segregation. Women who are the marginalized section in society, bear the brunt of the unprecedented COVID-19 lockdown more than men in terms of disproportionate fall in female labour force participation, higher male-female wage disparity and increased labour in unpaid domestic activities. ==== found that in India around 40 per cent of working women lost their jobs which accounts for 17 million women workers, within two months of the lockdown. This is because the share of women in the “contact-intensive sector” is relatively higher than men (====) and women are more often employed in the informal sector or on a casual basis without a job contract in the formal sector (====).==== Around 90 per cent of the female workers were engaged in the informal sector (====). It is pertinent to note that women workers who are at the bottom of the informal employment pyramid, account for half the total migrant in India. As per Census India, 2011 report, 67.93 per cent of total migrants are women and 32.07 per cent are men (====). Women were hard hit during the lockdown period while migrating back to their native places (====; ====, etc.). Using the data obtained from the Consumer Pyramids Household Surveys (CMIE-CPHS), ==== estimated a logit regression model that predicts that women were seven times more likely to lose work during lockdown compared to men, however, education shielded male workers from losing jobs. Besides this, during the unlock phases they are less likely to return to the urban sector to get back their employment. ==== estimated that the likelihood of females being employed was nine per cent lower than that for men, compared to April 2019. ==== used a difference-in-difference model which revealed a lowering of the gender gap in employment probabilities. This result is due to a lower probability of male employment which fell sharply rather than an increase in female employment. All these results commonly indicate that women in the unskilled labour market were hard hit by the lockdown measure compared to their men counterparts. ==== found that this was because women are overrepresented in most precarious jobs in the manufacturing, informal service, construction and agriculture sectors. These industries are mostly contact-intensive industries that were completely or partially restricted by the governments of several developing nations to restrict the spread of the virus. Lockdown had both direct and indirect effects on females’ burden of unpaid domestic work. In a sample survey of different urban centres in India, ==== found that 35 per cent and 25 per cent of women spent more than 28 h/week and 50 h/week on unpaid domestic chores during the lockdown than 21 per cent and 8.8 per cent before the lockdown. On the contrary, a 26.3 per cent decline had been observed for women spending up to 7 h/week. ==== also reported that based on employment status, unemployed women witnessed the highest increase in unpaid household work. The volatility of the foreign capital market during the lockdown phase had also impacted the gendered labour market through various secondary channels in terms of sectoral relocation of unskilled and skilled labour, variation in household income etc. All of this influenced female labour force participation. Thus, in our analysis, we seek to theoretically explore this mixed effect on female effort in domestic work and her participation in the labour market in the backdrop of the pandemic crisis.==== found that the strict lockdown measures and decline in mobility lead to a drop in private consumption by double-digit rates in India. The empirical analysis in the paper also revealed that an adverse supply shock led to a sharp decline in the growth of the manufacturing sector in India (−22.9 per cent), Pakistan (−16.4 per cent), Sri Lanka (−19.2 per cent) and Thailand (−13.2 per cent). ==== employed a similar demand-constrained Keynesian model and analysed the transaction cost induced supply and demand shock in terms of fall in consumption, reduced investment and disruption in the supply chain. The paper illustrated how a demand-constrained Keynesian equilibrium turns to an artificially generated supply-constrained equilibrium. ==== used a calibrated DSGE New Keynesian framework and modelled the pandemic as a large negative shock to the utility of consumption that leads to an adverse demand shock. ==== studied the pandemic as a negative shock to the growth rate in productivity and considered endogenous technological change and stagnation traps. The other macroeconomic concern that the literature points out is rising public debt in response to the pandemic crisis. ==== using a dependant economy model conceptualized lockdown in terms of technological regress, fall in consumption expenditure, and fall in investment expenditure and analysed the effect of an expansionary fiscal policy on debt dynamics. The paper obtained that this could add to the future burden of liabilities of the Government and lower the exchange rate.==== and ====. ==== analysed the aspect of restriction on international mobility of skilled labour and restriction on trade of exportable products for a small-open developing economy. It obtained that restriction on international mobility of labour raises unemployment of unskilled labour while the effect on trade restriction remains ambiguous. On the other hand, ==== analysed the efficacy of the rural employment generation programme in a less-developed economy to counter the burden of the reverse migrated labour in the backdrop of the pandemic crisis. It obtained that unproductive expenditure on public employment schemes (when no capital is used) could produce a counterproductive outcome in terms of lower social welfare, however, the use of capital that improves land productivity besides increasing employment could lead to a better outcome in terms of lower income-disparity and higher social welfare.====Let us now consider different established theories which dealt with household modelling and migration modelling. However, very little work has been done to integrate both of these. The motivation of the paper is to fill this gap in the literature in the context of COVID-19 in an otherwise general equilibrium model. There is two main strand of literature which dealt with the gendered aspect of household modelling, viz., the unitary household model and the intra-household bargaining model. ====, ====) is the pioneering work in the unitary household model. The limitations of the works in this strand are: household is assumed to be one, resources of household are pooled and common preferences of members are usually assumed. The other strand of the balance of power approach (or, intrahousehold bargaining) in which the solution to the household problem is obtained as a Nash bargaining outcome between the male and the female member (====; ====; ====; ====; ====).====On the other hand, there is much honing of the H-T model that has drawn attention to factors such as the existence of the informal urban sector (====; ====; ====, ====), relative deprivation (====), capital market imperfection (====), asymmetric information (====) and family migration (====; Anam and Chaing 2007). ==== pioneered the discussion on family migration in a formal setup. The analysis found that net family gain rather than personal gain actuates family migration. However, the limitation of Mincer's analysis is that migration involves the mobility of the entire family. This is not a true feature of migration in many developing countries, particularly, South Asian economies. In large numbers, individual members migrate leaving behind other members of the family. Families in such cases are multi-centred but united (====; ====). On the other hand, ====, ==== and ==== developed a multi-centred migration model where families allocate members in rural and urban regions so as to maximize expected family gain given other economic factors. The majority of these analyses considered heterogeneity of members based on productivity differences. Unfortunately, these strands of literature have brushed aside many complex gender issues.====The extant literature on COVID-19 has identified the facets of consequences caused by this pandemic. However, the post-COVID fall in female labour force participation, higher male-female wage disparity, reverse migration and greater involvement of women in unpaid domestic activities bear testimony to multiple dimensions of gender discrimination. There exists a dearth of any analytical literature dealing comprehensively with family-based migration decisions, pandemic crises and gender-based labour market imperfections. Notably, the literature on theoretical “general equilibrium analysis” in the context of the gendered aspect of the pandemic crisis is scanty. Against this backdrop, this paper contributes to the emerging literature on the incidence of COVID-19 in a gendered society that is not merely a mechanically biological entity but fundamentally socially nurtured. A micro-theoretic dualistic general equilibrium model has been constructed for a distortion-ridden developing economy in the presence of capital market imperfection, open unemployment of male and female unskilled labour and migration of unskilled labour with family and without family. The regional (rural-urban) migration equilibrium has been obtained as a solution to the intra-household bargaining problem between the male and female members of the family. Migration of the male member accompanied by the female member of the unskilled household is classified as “migration with family”, whereas, migration of the male member leaving behind the female member in the rural region is classified as “migration without family”. The distinguishing feature of the migration equilibrium is that it generalizes the usual ==== and ==== migration equilibrium in terms of family migration equilibrium.==== in their primary sample survey of migrant workers during the lockdown phase, found that about 63.3 per cent underwent loneliness, 58.2 per cent experienced frustration and tension and 51 per cent felt anxious (====). Living far away from their families and native place mounted fear and uncertainty amongst migrants over their regular expenses like food, clothing, medicines, job security and accommodations. They became also worried about their safe return to their native places. Thus, in our analysis, we capture this aspect of psychological distress amongst migrant workers separately for those who were living with or without their families. The effects are analysed on female labour force participation, her burden on unpaid domestic chores, gender-based wage disparity and unemployment. Probably lockdown was inevitable but it has multidimensional adverse effects. The results of the paper reveal the complexities of gender discrimination and migration dynamics. And accordingly, no facile conclusion is warranted.====The rest of the paper is organized as follows. The structure of the representative developing economy is described in ====. In ====, the migration equilibrium is derived from the representative household's optimizing behaviour. The general equilibrium implications of lockdown have been analysed in ====. Finally, ==== concludes the paper.====Mathematical Appendix====The proportionate change in the varibales/parameters is denoted by ‘^’, ==== and ==== denote the distributive share and physical share of the ====th factor in the ====th sector, respectively.====Taking total derivative of ====, ==== and ==== yields the following results====where, ==== is the elasticity of informal interest with respect to formal interest.====The following equations are obtained from differentiating ====, ==== and ====, respectively====where, ====.====From ==== we obtained output of sector 1 (====) indepndnent of the parameter====, thus,====The effect on male unemployment is obtained by taking total derivative of ==== that leads to the following result====where,====Differentiating ==== and ====, respectively and using equations (5), (A.4) and (A.6)==== where,====Taking total derivative of ==== and ==== and arranging in matrix form====Using Cramer's rule, the following results are obtained====where,====On taking total derivative of ==== the following value is obtained====where,====and ==== is the elasticity of demand for commodity ==== with respect to==== where ==== and ==== implies skilled labour income and capital income, respectively.====The primary effects of lockdown are obtained as follows.====It follows from equation (A.1), (A.2), (A.3), (A.7) and (A.10) that ====, ====,====, ==== and ====.====From (A.11) and (A.12) the following effects are obtained provided the sufficient condition that sector 2 is more capital-intensive than male labour compared to sector ====, i.e., ==== .====where, ==== .====From (A.13) the following result is obtained provided ==== and ====From (A.8) the primary effect on unemployment is obtained as ====. It follows from equation (A.1), (A.2), (A.3), (A.7) and (A.10) that ====.====From (A.11) and (A.12) and given the factor-intensity ranking, the following results are obtained such that ====Finally, from (A.13) it is obtained that====It follows from (A.13) that ====.====The effect on output is obtained as follows:","COVID-19 lockdown, family migration and unemployment in a gendered society",https://www.sciencedirect.com/science/article/pii/S1090944322000321,28 July 2022,2022,Research Article,32.0
"Singh Vikkram,Shirazi Homayoun,Turetken Jessica","Ted Rogers School of Management, Toronto Metropolitan University, 350 Victoria St., Toronto, ON M5B2K3, Canada,Department of Economics, Toronto Metropolitan University, 350 Victoria St., Toronto, ON M5B2K3, Canada","Received 7 May 2022, Accepted 26 July 2022, Available online 27 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.011,Cited by (4),"The study explores the effect of COVID-19 on labour market outcomes for women in the major urban areas in Canada. Using data from the Labour Force Statistics, we find the pandemic has had a disproportionately negative impact on the employment and income of women, worsening gender inequalities. Sectors more likely to employ women faced immense negative pressures, leading to dismal employment numbers. The effects of continued lockdowns and future potential inflation suggest that gender wage disparity continues to increase, worsening the economic health of women and making them even more vulnerable to future event risks.","The COVID-19 pandemic has profoundly impacted the global economy, displacing 20–25% of all jobs.====). A growing body of research indicates that the adverse effects were much more profound for frontline workers who were unlikely to transition to remote work along with those younger, women, unmarried, and with less education – leading to employment and wage disparities (====, ====; ====; ====). The pandemic severely affected specific sectors, such as accommodation, travel, and retail, which led to a profound reduction in employment along with an increase in wage disparity (====). Due to these industries' rigidity and inflexibility in providing meaningful, reliable, and consistent remote work opportunities, women are more likely to be unable to find consistent employment because of pandemic-related disruptions (====).====Furthermore, the impact of the pandemic was much more disproportionate on female employment, as they left the labour market in more significant numbers than men because of job losses and familial responsibilities (====; ====). These factors increased the employment gap and worsened gender pay equity==== with a further decline in average female wages than their male counterparts (====). Because of the evolving nature of the pandemic, there is a scarcity of studies that measure the pandemic's effect on labour market outcomes, particularly the gender inequities, which our study aims to fulfil. We use a logit model to estimate the effect of the pandemic on employment and labour wage outcomes in Canada. In particular, we answer the following research questions: ====The motivation to study the pandemic's disproportionate effects is due to the immense welfare implications, particularly on the most vulnerable segments of the population. In addition, the study provides several contributions. First, to our knowledge, this is the first study that incorporates gender dynamics in addressing labour market outcomes and thus fills a critical research gap. Second, the results can facilitate informed policymaking to address gender imbalances in pay equity and employment access as the economy and labour market emerge from the pandemic. The insights can spark strategic conversations about potential interventions needed to change the current fiscal revenue tools and policy frameworks in the post-pandemic era.====The rest of the paper is organized as follows: ==== focuses on reviewing relevant literature that shows the gap in research. ==== describes the methodology, including data collection and statistical models. ==== discusses the results, while ==== presents our conclusions.",COVID-19 and gender disparities: Labour market outcomes,https://www.sciencedirect.com/science/article/pii/S1090944322000345,27 July 2022,2022,Research Article,33.0
"khan Feroz Noushad,Hassan Gazi,Cameron Michael P.","School of Accounting, Finance and Economics, University of Waikato, New Zealand,School of Accounting, Finance and Economics, University of Waikato, And Te Ngira - Institute for Population Research, University of Waikato, New Zealand","Received 25 April 2022, Accepted 17 July 2022, Available online 21 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.007,Cited by (0),"As a prominent social media tool, Twitter enables prompt dissemination of financial news and information, which can have a substantial impact on investors’ perceptions and decision-making processes. The propagation of financial news and information through Twitter can either positively or negatively affect investors’ perceptions. As per network theory, the impact of information on one's perception and behavior is known as the network effect. Since Twitter is also a network, we tried to contribute more to this theory in this study by considering other factors that can have an impact on the perceptions of investors. We argue that the impact of financial information and news on investors’ perceptions is moderated by other factors such as connectivity, social ties, and network size of the network. To establish the links between them, we considered three key factors in investors’ networks: (1) network connectivity (network structure); (2) social ties circle (friends, family, colleagues); and (3) size of the network (number of contacts). The results of this study indicate that highly connected investors receive more information and hence, the impact of news is derived from the connectivity of investors within the network. The findings of the study also show that the social ties circle plays a crucial role in determining the impact of the news. The findings further indicate that the impact of news on investors’ perceptions also depends on the theme of the news.","Information is key to trading, which is why investors are determined to get their hands-on financial information in the stock market (Baker and Haslem, 1974; Ozsoylev, Walden, Yavuz, and Bildik, 2014; Pevzner et al., 2015). This is why investors use media networks to obtain information that can help them make informed decisions, including through social media networks (such as Twitter or Facebook) (Bartov et al., 2018; Joyce, 2013; Miller and Skinner, 2015; Siikanen et al., 2018) and mass media networks like newspapers or press releases (Feng and Seasholes, 2004; Yin and Tan, 2017). However, financial information and news (Lee, Hutton, and Shu, 2015; Miller and Skinner, 2015) shared through social networks are more influential and attract more people (Lee et al., 2015; Stieglitz and Dang-Xuan, 2013) because social media enables the distribution of information promptly. (Chen, Tsai, and Chen, 2016).====Social media is increasingly being utilized as an important source of communicating valuable financial information in financial markets (Lee et al., 2015) because social media can provide investors an opportunity to exchange financial information, and viewpoints about companies, and markets (Cade, 2018). Previous studies (e.g. Miller and Skinner, 2015) have shown that Twitter has become a prominent social media platform for investors to not only obtain financial information but also connect with their counterparts. That's why social media, particularly Twitter, is an ideal platform to share sentiments or opinions and information promptly concerning the stock market (Eli el at., 2017). Hence, such sharing of financial information on social media (particularly Twitter) has an impact on the perceptions and behavior of investors (Guggenmos and Bennett, 2017; Elliott et al., 2018).====From a network theory perspective, when one individual affects another on a network (Murendo et al., 2018; Sundararajan et al., 2013), or one buyer affects the decision of another buyer on the market (Leibenstein, 1950), this is known as a network effect. Such effects occur through communication or interaction in networks (Evans and Schmalensee, 2017; Khan, Mohaisen, and Trier 2020). Becker (1999) argued that when there is a network, there is a network effect. Although there is still little research on the social network effect, some researchers have suggested that the social network effect has three factors: (1) relationship or social circle of the individuals in the social network (Khan, Mohaisen and Trier 2020; Gregory, Lili, 201; M. Panzeri, 2012); network structure of individuals in the social network (Matthew, 2002; Katona et al., 2011; Craig, 2019); and size of the network (Katona et al., 2011; Craig, 2019).====A plethora of literature is available that discusses the impact of the social media propagated financial news on investor perceptions (Barber and Odean, 2008; Chung et al., 2018; Daley and Green, 2012; Liu and Li, 2019; Ozsoylev et al., 2014). However, the literature has not defined the extent to which the effect of financial news on investor perceptions is moderated by connectivity (network structure), network size, and the social ties of investors. We posit that the extent to which financial news will affect an investor's perception is dependent on the structure of the network, the size of the network, and the investor's social ties in the network. In simple words, the notion of this study is to quantify the moderating role of network structure, the size of the network, and social relationships on investors’ perceptions.",To what extent do network effects moderate the relationship between social media propagated news and investors’ perceptions?,https://www.sciencedirect.com/science/article/pii/S1090944322000308,21 July 2022,2022,Research Article,34.0
"Mota Paulo R.,Fernandes Abel L.C.","School of Economics and Business and CEF.UP, University of Porto, Portugal,School of Economics and Business and NIFIP, University of Porto, Portugal","Received 16 March 2022, Accepted 16 July 2022, Available online 20 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.006,Cited by (0),"-period centered moving average of inflation forecasts, which may capture the length of the medium-term horizon orientation of monetary policy. We have tested for different combinations of policy horizons, and forecast horizons, to achieve the best fit. Allowing ==== to vary from 1 year (corresponding the standard inflation targeting regime) to a normal length of an economic cycle, we keep the model sufficiently general, and we let the data speak for itself. We have found that the ECB is targeting price stability over a period in between four years and a half and five years, implying that the ECB is area1dy following, albeit implicitly, a hybrid approach to price stability in line with average inflation target. This result is robust when we control for the Central Bank desire to avoid excessive policy interest rate variability due to the presence of policy interest rate adjustment costs.","The legacy of the recent financial crisis aggravate by the Covid-19 pandemic, slower productivity growth, and adverse demographic factors have contributed to a sustainable decline in the natural rate of interest and brought to the table the problem of the restriction that effective lower bound (ELB) causes to conventional monetary policy.====In this context, and in addition to the application of non-conventional monetary policy measures,==== monetary policy regimes involving makeup strategies such as price-level targeting or average-inflation targeting are being discussed as a serious option by Central Banks.====A fundamental aspect of the ECB's monetary policy strategy is that it aims to pursue price stability over the medium term (that should extends beyond its projected time-lag). This reflects the idea that monetary policy should not attempt to fine-tune developments in inflation over short time horizons (see ECB, 2011, p. 68). The problem is that the above mention elements of the ECB's monetary policy strategy are rather ambiguous. First, although the literal interpretation of Article 127(1) of the Treaty on the Functioning of the European Union concerning price stability implies the adoption of a price-level targeting regime, the ECB'strategy as stated by its executive board members==== is consistent with the flexible forecast inflation targeting approach as defined, ====., by Svensson (1997). Second, the ECB has not defined the medium term with reference to a predetermined horizon, retaining some flexibility regarding the exact time frame.====Following the FED decision to change the framework for conducting monetary policy from an inflation targeting regime to an explicit flexible average-inflation targeting regime (see Powell, 2020),==== the ECB announced a revision of its monetary policy strategy on 8 July 2021. The ECB now admits that when specially forceful or persistent monetary support is needed, inflation could moderately and temporarily exceed its 2% target. This can be viewed as an approximation to the FED's new monetary policy strategy. Although it is worth mentioning that the ECB had not formally adopted an average-inflation targeting regime.====The ambiguity surrounding the ECB's monetary policy strategy was not resolved with the strategy review announced in July 2021. In particular doubts remain about whether the ECB will merely tolerate or actively seek for inflation to exceed the target for some time.====As the inflation targeting in practice should be set so that the target is reached in the medium term, our hypothesis is that the ECB was already following at the time of the announcement of the strategy review, albeit implicitly, an average-inflation targeting framework.==== A critical aspect of an average-inflation targeting regime is the size of the time window (or the policy horizon) used to calculate the average rate of inflation, since it determines the reaction of monetary policy to shocks (Akram, 2010, and Amano et al., 2020). Therefore, an obvious question is what is the width of the window used to calculate average inflation, which is associated to the medium term concept of price stability of the ECB? This is the objective of this paper. As the ECB is not explicit about its medium term orientation, and as the selection of the optimal horizon is model and context (shock) dependent, this is ultimately an empirical issue.====Having in mind that the ECB uses the main refinancing open market operations (MROs) interest rate (policy interest rate) as the primary monetary policy instrument, to quantify the ECB's policy horizon we have estimated a regression where the ECB policy interest rate is expressed as function of a ====-period centered moving average of inflation forecasts, which may capture the length of the medium-term horizon orientation of monetary policy, and also as a function of a forecast of the unemployment rate. We have tested for different combinations of policy horizons, and forecast horizons, and we have chosen the one that maximizes de ==== of the regression. Allowing ==== to vary from 1 year (corresponding the standard inflation targeting regime) to a normal length of an economic cycle, we keep the model sufficiently general, and we let the data speak for itself.====As a robustness check, we have also estimate the regression with the shadow short interest rate as the independent variable.==== This allows us to extend the analysis beyond March 2016 (when the MROs interest rate reached zero), and to study how the non-conventional monetary policy measures have respond.====As the ECB retains a flexible medium-term orientation, which may be defensible as the optimal policy horizon is contingent on the type and the size of the shocks that hit the Eurozone economy, our regression should not be viewed as a policy interest rate reaction function, nor as policy rule derived from a structural model of the economy (see Giannoni and Woodford, 2005, and Svensson and Woodford, 2005). Indeed, inflation-forecast targeting is not linked to a mechanical formula that makes monetary policy a function of the current value of a small set of variables as it is the case of Taylor type rules (Woodford, 2007). Besides, the ECB does not (see Constâncio, 2018), and should not (see Svensson, 2020), follow a specific Taylor rule type reaction function for its interest rate instrument. Therefore, to shed some light on the implicit monetary policy regime, we have estimated a sufficient general equation that simply captures how the policy interest rate ==== and on average have related with inflation and unemployment forecasts, ====., an implicit reaction function (see Svensson, 2003, p. 436).====The remainder of the paper is organized as follows. Section 2 deals with the differences and consequences of price-level targeting versus inflation targeting. Section 3 discusses the fundamentals of the Central Bank's policy interest rate persistence. Section 4 describes the details of the empirical strategy, and the data set. Section 5 presents the estimation results, and Section 6 concludes.",Is the ECB already following albeit implicitly an average inflation targeting strategy?,https://www.sciencedirect.com/science/article/pii/S1090944322000291,20 July 2022,2022,Research Article,35.0
"Tuan Truong Anh,Nam Pham Khanh,Loan Le Thanh","University of Labour and Social Affairs - Ho Chi Minh City campus, Ho Chi Minh City, Vietnam,School of Economics, University of Economics Ho Chi Minh City, Ho Chi Minh City, Vietnam","Received 9 February 2022, Accepted 15 July 2022, Available online 18 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.005,Cited by (0), should not be underestimated.,"Many governments in low-and middle-income countries (LMICs) have implemented national health insurance reforms in recent years in order to move toward universal health coverage (Kutzin, 2012; Lagomarsino et al., 2012). In accordance with this, numerous studies on the effects of health insurance policies have been conducted. In line with the purpose of health insurance, which is to increase access and use, improve health status, and mitigate the financial consequences of ill health (Escobar et al., 2010), previous impact evaluation studies have primarily focused on outcomes such as utilization of health services, financial protection, and health status. For example, Erlangga et al. (2019) conducted a systematic review of 68 studies, 40 of which looked at utilization, 46 at out-of-pocket health spending and catastrophic health spending, and 12 at health status. And, more recently, all studies in the systematic reviews by Docrat et al. (2020) and Zhang et al. (2020) focused solely on the effects of insurance on health care utilization.====Health insurance enrollment is viewed as an ex-ante measure for individuals to mitigate the effects of future risks (Jorgensen & Siegel, 2019). It could thus be argued that households' participation in health insurance affects their choices of financial services both before and after risks occur. On the one hand, when members of a household have health insurance, it is likely that their choices of other ex-ante risk management instruments, such as savings and investments, will be influenced to some extent. We can imagine two opposing ways in which insurance take-up would impact savings and investments; for example, households may decrease them because they perceive a lower need for future medical costs, or increase them because their current out-of-pocket payments on health care are reduced, allowing them to increase both other expenditures and savings at the same time (Kirdruang & Glewwe, 2018). On the other hand, when a household suffers an income shock, such as the death of an income-earning household member, demand for insurance and savings falls, while demand for ex-post instruments such as credit rises (Bendig et al., 2009). Therefore, it is conceivable that households’ insurance status have an impact on other financial choices, and as such, studying the impact of health insurance in LMICs will only provide an incomplete picture if it focuses solely on outcomes such as utilization of health services, financial protection, and health status and ignores this area.====In reality, a number of studies have been conducted to confirm the impact of health insurance on household financial tools, with a small part of them coming from LMICs. In the United States, Gallagher et al. (2020) investigated the impact of Medicaid, a subsidized health insurance program, on savings behavior. The findings revealed significant heterogeneity in savings responses to Medicaid across households. Households that are not in financial distress save less through Medicaid. Medicaid eligibility, on the other hand, increases refund savings rates among households experiencing financial hardship. Besides, Bornstein & Indarte (2020) estimated the causal effect of expanded insurance on household debt by utilizing Medicaid's staggered expansions, and shown that Medicaid increased credit card borrowing. In Thailand, Kirdruang & Glewwe (2018) finds no decline in total household savings following the implementation of the Universal Health Coverage Scheme; however, the considerable increase in consumption of durables suggests a change in households' consumption-smoothing patterns, such that they spend more on goods with installment plans and thus save less for precautionary purposes. Kumar (2019) proved the positive impact of health insurance on life insurance in Finland. Most notably, Farrell et al. (2016) adopted a multivariate model to investigate the role of financial self-efficacy in explaining women's personal finance behavior in Australia. A system of six equations of financial products, including private health insurance, investments, mortgages, savings accounts, credit cards, and loans, were used in the model. Although the primary focus of the research was not on the relationship between financial products, the correlations between the error terms for each pair of equations were positive and statistically significant, implying that health insurance and financial products are supplementary to some extent.====When assessing the impact of health insurance, one important issue that previous research has generally overlooked is the interconnectedness of financial services. That is, in estimation strategies for drawing causal effects of health insurance, separate equations were typically used for each service choice (Bornstein & Indarte, 2020; Gallagher et al., 2020; Kirdruang & Glewwe, 2018). In practice, households may use a variety of financial services, some of which correlate with one another (Farrell et al., 2016; Giesbert et al., 2011; Viganò & Castellani, 2020). The correlations occurs when there are unobserved household-specific characteristics that influence several financial service decisions but are difficult to capture with measurable proxies (Belderbos et al., 2004). Hence, in the presence of correlations, estimates from separate equations of financial service choices are inefficient (Belderbos et al., 2004; Greene, 2012); additionally, these estimates do not tell the whole story of how health insurance affects financial tools in households' risk management basket.====Controlling for endogeneity is a task that researchers investigating the impact of health insurance coverage on financial service choice have to perform. Estimates will be biased if there are unobserved characteristics that affect both health insurance status and household financial service choice. A variety of strategies can be employed to control such unobserved characteristics. For instance, Gallagher et al. (2020), and Bornstein & Indarte (2020) utilized two-stage least-squares to account for the unobserved heterogeneity that jointly determine health insurance and financial choice. Kirdruang & Glewwe (2018) used the difference-in-difference method to account for endogeneity caused by unobserved characteristics that could bias the causal effect. In this study, we use a recursive multivariate probit model to circumvent endogeneity issues caused by unobserved heterogeneity.====The primary goal of this research is to determine the impact of health insurance on the choice of household financial services in Vietnam, including private insurance, savings, investments, and credit. This paper contributes to the current limited literature in a number of ways. First, it investigates the impact of health insurance on household financial choices, an area that is still unclear in Vietnam and in LMICs in general. Second, unlike previous research that disregarded the correlations between financial services, this research captures potential jointly determining processes by incorporating health insurance, private insurance, savings, investments, and credit simultaneously into a multivariate probit model that allows error terms to correlate. This method is expected to produce more efficient estimates and to provide a more comprehensive picture of the health insurance impact. Third, to the best of our knowledge, this is the first research of its kind to use an recursive multivariate probit model to control for endogeneity in assessing the impact of health insurance on household financial decisions.====This paper is structured as follows. The next section displays and examines the data utilized in the analysis, as well as the variables' descriptive statistics. Section three discusses the empirical approach adopted. The fourth section provides results from the multivariate probit model. The fifth section is the robustness checks. The conclusion is the last section.",The impact of health insurance on households’ financial choices: Evidence from Vietnam,https://www.sciencedirect.com/science/article/pii/S1090944322000266,18 July 2022,2022,Research Article,36.0
"Olanubi Oluwanbepelumi Esther,Olanubi Sijuola Orioye","Department of Economics, University of Lagos, Akoka, Lagos State, Nigeria,The Nigerian Economic Summit Group, The Summit House, Ikoyi, Lagos, Nigeria","Received 17 February 2022, Revised 3 July 2022, Accepted 7 July 2022, Available online 11 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.004,Cited by (0),"This study examines the importance of incorporating public sector efficiency considerations in the design of a “COVID Fund” in the euro area, aimed at providing insurance for member states against common health shocks. To test our proposition, we examine the efficiency of government spending on health during periods of severe resource constraints, which mirrors what occurs during pandemics like COVID-19. Specifically, we considered 19 administrations in the euro area during the global financial crisis and euro area sovereign debt crisis that followed. The results support our proposition. First, they reveal the average efficiency for all 19 administrations to be 0.950, which implies that member countries had wasted about 5% of funds allocated to health during this period. This suggests the need for the supranational institution to first of all ensure improvements in the use of public funds allocated to health by national governments in order to prevent wastage of the financial aid transferred to them during pandemics. Also, two of the four administrations that adopted the Economic and Financial Adjustment Programme of the troika (Portugal and Greece) during the twin crisis were among the most efficient. This suggest that making conditionalities an integral part of the central coordination of health funds during pandemics will result in improvements in the efficiency of funds transferred to member states.","The novel coronavirus disease (COVID-19) which began as a health crisis but has since morphed into an economic crisis, with deleterious effects on economies across the globe, has exposed major flaws in the proposals advanced for a central insurance mechanism for the euro area. Proposals from academic institutions and policy circles before the virus outbreak had focused on initiatives such as unemployment or social benefit schemes designed to provide insurance against idiosyncratic shocks (see for instance ====; ==== & ====; ====; ====; ====; ==== and ====). These initiatives are however ill-suited to protect countries against common shocks, such as the health shock triggered by the coronavirus outbreak. To further complicate the situation, present institutional agreements at the European Union (EU) level were not also designed to deal with common shocks. According to ====Against this backdrop, a large number of proposals have emerged following the virus outbreak, proffering different proposals for the central coordination of policy measures in Europe. Based on these proposals, ====) and the SURE proposal of the European Commission can also help provide financial relief during pandemics. The SURE scheme for instance will provide as much as €100 billion in long-term loans to member countries to finance an important expenditure requirement of governments during a pandemic. It focuses specifically on providing limited funding for partial unemployment schemes with their accompanying expenses.====We contribute to the ongoing debate by making a case for incorporating public sector efficiency considerations in the design of a centrally coordinated health fund (specifically, the COVID Fund proposed by ====) in the euro area aimed at protecting member states against common health shocks. First, we argue that funds transferred from the COVID Fund or its variants will not have the desired effect in the receiving economies if governments are already inefficient in the use of resources allocated to health from their national budgets, as there will be large-scale wastage in the utilization of the funds transferred. ==== identified the importance of integrating public sector efficiency considerations in the design of a centralized insurance mechanism for the euro area, focusing on the unemployment insurance scheme. They argued that this consideration will ensure that the funds disbursed by the supranational institution are efficiently utilized by member states. In line with their study, we consider 19 administrations in the euro area that existed roughly during the global financial crisis and euro area sovereign debt crisis. This approach will help to determine how efficient governments are in the use of public funds allocated to health when confronted with severe resource constraints since pandemics such as the coronavirus are accompanied by severe constraints on government budgets.====Second, we argue that funds should not be transferred to countries from the COVID Fund on a “needs basis” as proposed by ====; ====). The reform objectives were to bring about changes in the conduct of fiscal policy by instituting measures to tame rising budget deficits in countries. It also included large scale reforms in the healthcare sector (====).====As a preview of our empirical findings, our results reveal the average efficiency for all 19 administrations to be 0.950, which implies that member countries of the euro area have wasted about 5% of funds allocated to healthcare during the period of the global financial crisis and sovereign debt crisis that followed. This suggests the need to incorporate public sector efficiency considerations in the design of a centrally coordinated health fund for the euro area in order to prevent the wastage of funds transferred from the supranational institution during a common health shock or pandemic. Also, two out of the four countries that adopted the EFAP of the troika were among the most efficient of the 19 member countries of the euro area. The Pedro Manuel administration of Portugal was ranked second with an efficiency score of 0.99 while the George A. Papandreou/Antonis Samaras of Greece came fifth with an efficiency score of 0.969. This suggest that making conditionalities an integral part of the central coordination of health funds during pandemics will result in improvements in the efficiency of public funds allocated to health by member states.====The rest of the paper is structured as follows. In ==== we present our methodology for the study. ==== presents and discuss the results of our efficiency analysis while ==== concludes the study.",Public sector efficiency in the design of a COVID fund for the euro area,https://www.sciencedirect.com/science/article/pii/S1090944322000187,11 July 2022,2022,Research Article,37.0
"Bossman Ahmed,Umar Zaghum,Agyei Samuel Kwaku,Junior Peterson Owusu","Department of Finance, University of Cape Coast, Cape Coast, Ghana,College of Business, Zayed University, P.O. Box 144534, Abu Dhabi, United Arab Emirates","Received 16 March 2022, Accepted 7 July 2022, Available online 9 July 2022, Version of Record 26 August 2022.",https://doi.org/10.1016/j.rie.2022.07.002,Cited by (16), Japan and the US; ==== Singapore and the US; ==== China and Canada; ,"The global economy's recovery from the COVID-19 pandemic – which comes along with panics, diverse recovery policies, and a distinct systemic risk – has been a slow one (Bossman, 2021; Bossman et al., 2022; Gubareva and Umar, 2020; Umar et al., 2021, 2021, 2021; Umar and Gubareva, 2021a, 2021b). The European Public Real Estate Association (EPRA), in their 2021 Global REIT Survey report, underscore the important role of a resilient economic sector towards recovery building, which is not far from REIT regimes owing to the ability of firms to convert distressed assets into income-producing and investment vehicles (EPRA, 2021).====With a remarkable annual growth averaged at 13% over the last two decades, the real estate market has piqued the interest of investors in the past two decades (Ryu et al., 2021). Investments in real estate are characterised by moderate returns, high liquidity, and low risk. For the US alone, growth in real estate capitalisation measured up to $1269.96 billion in September 2019 compared to $138.72 billion in December 2000, as reported by ERPA and the National Association of Real Estate Investment Trusts (Nareit). From the global real estate market, REIT earnings, measured as operational funds operations, recorded a 24.6% increment in 2021 (Nareit, 2022).====As argued by Ryu et al. (2021), the rapid and consistent growth in the size of the real estate sector raises the question of whether there is a prompt adjustment to new information by real estate prices and the chances for arbitrage. Following this, efficiency and dynamic interrelations of real estate markets have caught the attention of scholars and practitioners alike (Akinsomi, 2021; Antonakakis et al., 2015, 2016; Antonakakis and Floros, 2016; Chiang et al., 2016; D'Lima et al., 2022; Liow and Schindler, 2017; Ryu et al., 2021; Tiwari et al., 2020; Tsai, 2015; Zhang and Hansz, 2022; Umar and Olson, 2021).====The global financial crisis (GFC) left behind unprecedented turmoil in financial markets globally, which in turn, has led to amplified uncertainty in economic conditions and policy actions across the globe, leading to intense uncertainty of investment markets including real estate (Antonakakis and Floros, 2016; Zhao et al., 2021; Esparcia et al., 2022). Notwithstanding, the ramifications of the systemic risk occasioned by the recent COVID-19 pandemic is unique owing to the channel of effect (Quinsee, 2021). The GFC was an endogenous shock caused by the activities of market participants, lenders, and speculators; this led to an excessive accumulation of debt and risk-taking, which resulted in the credit bubble (Roy and Kemme, 2020). The financial market meltdown occasioned by the COVID-19 pandemic, contrariwise, is caused by external factors that directly affect the real economy (Bossman, 2021; Bossman et al., 2022; Quinsee, 2021; Aharon et al., 2022; Gubareva et al., 2021;Zaremba et al., 2021; Bossman et al., 2022a; Ali et al., 2022). Similarly, the recent Geopolitical induced uncertainty caused by Russia-Ukraine conflict has created new uncertainties in the financial markets (Umar et al., 2022a,b).====With the spontaneous emergence of new strands of the coronavirus, fear and panics surrounding healthcare delivery, retirement planning, working conditions, etc. cast doubts about the effectiveness of investment. In curbing the spread of the virus and its ramifications, various policy actions – such as lockdowns, restrictions on persons’ movements, etc. – are employed (Agyei et al., 2021; Owusu Junior et al., 2021). These measures exacerbate the uncertainty of investment markets including real estate (Akinsomi, 2021; Bonato and Pierdzioch, 2021; D'Lima et al., 2022). Furthermore, EPRA (2021) notes that, towards a new regime of the global real estate market, a new REIT legislation is to be soon rolled out. Intuitively, this would further intensify the uncertainty surrounding investment in real estate.====Despite these developments in the global economy and the real estate industry, there is no unified study, to the best of our knowledge, that quantifies the mutual information between policy uncertainty and real estate markets. This type of study would not only offer a more accurate and full view of the complicated and nuanced interactions between economic policy uncertainty and real estate markets, but it would also yield better policy insights. Three main issues are to be addressed: (====) whether there are differences in the way national real estate indices respond to information flow from global economic policy uncertainty; (====) whether there are diversification prospects for real estate investments amidst policy uncertainty; (====) what frequencies reveal viable diversification opportunities?====Responses to these questions are empirically examined by employing a unique decomposition technique, the “improved complete ensemble empirical mode decomposition” (ICEEMDAN), which does better than wavelet, the variational mode decomposition, and earlier versions of the EMD family. Due to the potency of the ICEEMDAN, it has been employed in recent works (Bossman, 2021; Bossman et al., 2022; Kou et al., 2020; Wu et al., 2020; Yang et al., 2018). Additionally, we resort to the Rényi entropy, the class of transfer entropy (TE) that attributes fitting weights to differentiate tailed distributions. Tailed distributions in financial data series are especially influential in stressed market periods. TE offers a novel approach for measuring causal relations that is resistant to spurious associations, captures model-free information flow measurement, and makes no assumption on linearity, making it an innovative technique for conventional causality frameworks like the Granger causality (Huynh et al., 2020). TE is also noted for measuring information flow between asset returns and has been widely used in the scientific literature. Recent finance literature accords to the relevance of this technique (Adam et al., 2021; Asafo-Adjei et al., 2021; Bossman, 2021; Bossman et al., 2022; Ferreira et al., 2021, 2022; Owusu Junior et al., 2021).====We contribute to the strands of literature that examine the interrelations between macroeconomic variables, policy uncertainty, and real estate investments, to offer empirical policy directions and risk management solutions. Our specific contributions are as follows. First, our findings provide insights into how policy uncertainty affects top real estate markets across the globe. Since real estate indices are investable, such insights are essential for risk management and policymaking. Second, we examine the information flow in a denoised transfer entropy paradigm, which caters for noisy observations and maintains true signals only. This has been ignored in the existing works on real estate and, hence, catering for the complex, non-linear, and asymmetric properties of financial time series communicates the uniqueness of our study. Third, we offer a quantification of the mutual information between different real estate markets and policy uncertainty across distinct timescales representing the short-, medium-, and long-term investment horizons. No existing study offers this empirical quantification of the information flow between the variables in a denoised transfer entropy framework whilst incorporating the broader real estate indices rather than a class of real estate investment like equity or mortgage real estate.====In a denoised transfer entropy paradigm, we find that the information flow between real estate markets and policy uncertainty reflects inefficiency (efficiency) in the short term (mid-to-long-terms). Thus, diversification opportunities are available between the US and Japan, and between China and either Canada or Hong Kong at lower timescales.====The rest of the paper is outlined as follows. Section 2 details a review of theoretical and empirical literature; Section 3 presents our methods and dataset; we discuss our results in Section 4 with their implications in Section 5 and conclude in Section 6.",A new ICEEMDAN-based transfer entropy quantifying information flow between real estate and policy uncertainty,https://www.sciencedirect.com/science/article/pii/S1090944322000199,9 July 2022,2022,Research Article,38.0
"Braut Beatrice,Migheli Matteo,Truant Elisa","Department of Economics and Statistics “Cognetti de Martiis”, University of Torino, lungo Dora Siena, 100 I-10153, Torino, TO, Italy,OEET, Collegio Carlo Alberto, piazza Arabarello, Torino, TO 8 I-10123, Italy,Department of Management, corso Unione Sovietica, University of Torino, 218bis I-10134, Torino, TO, Italy","Received 8 December 2021, Accepted 25 June 2022, Available online 2 July 2022, Version of Record 18 July 2022.",https://doi.org/10.1016/j.rie.2022.06.002,Cited by (0),"The lockdown imposed to limit the diffusion of COVID-19 in Italy affected the economic situation negatively. The income of many households decreased, and people were forced to stay home. Both these factors influenced food consumption: on the one hand less income means less money for purchases, on the other, the negative psychological impact of lesser income and the pandemic shifted the consumption towards alcohol and tobacco. Using survey data, this paper shows how the negative economic shock due to lockdown, together with the restrictions imposed by it, affected the consumption of food items in a region of Norther Italy.","The Covid-19 pandemic represents a challenge for all countries worldwide and can bring long-lasting alterations. Several changes had to be faced, ranging from the healthcare domain, until transportation and delivery, farming, food processing, food purchasing, as well as public services (====). Until a vaccine can guarantee extended coverage of the population, the only measures to mitigate the Covid-19 dramatic effects and reduce contagion relates to the physical distancing that breaks social and economic contacts. The measures enacted to limit contagions negatively affect both economy and society, with short, medium and long-term consequences. In this context, policy makers have the delicate task of balancing the positive health effects of the measures with the economic costs of the burdens imposed to households and firms (====).====In Italy the Covid-19 epidemic spread rapidly. During the first wave, Italy was the first Western Country to implement total lockdown from March 9th until May 4th 2020. Non-essential economic activities and services stopped working and schools were closed. Outdoor activities were allowed only for food purchasing, working (limited to specific activities) or health reasons. Furthermore, many households experienced severe financial downturn, due to reduction in income and employment. Together with mobility restrictions, these issues influenced the consumer's access to food (====; ====; ====). Consequently, the lockdown restrictions imposed by the Government significantly affected food supply chain management and consumption habits (====), fostering the emergence of new solutions for food provision. These have affected quantities and types of food purchased by the households (Cappeli and Cini, 2020; ====). In Italy, for the quarter February-April 2020, food sales increased by 6.1% in value, compared to the 2019 quarter, while non-food sales plunged by 52.2% (====). For the months of March and April 2020, ==== reports a rise in purchases of several food categories: + 145% flour, + 14% and + 25% pasta and rice respectively, +57% mozzarella and eggs, +31% cured meats, + 22 long-life milk (while fresh milk −5%), +78% oranges, +60% apples. A change in consumption patterns is therefore evident.====During extreme scenarios such as this, household consumers often try to mitigate the risk of food insecurity and the difficulties in food shopping (====; ====; ==== and ====). Indeed, since the beginning of the first wave of Covid-19 pandemic there was evidence of stockpiling, hoarding, and panic buying.====In spite of a growing literature addressed to different facets and consequences of COVID-19 pandemic, little is still known about how households have responded in terms of their spending (Baker et al., 2020), comparing behaviour before and during lockdown.====This research investigates how the dramatic changes in the daily lives of consumers caused by the Covid-19 pandemic and lockdown influenced food preferences at household level. The primary aim of the paper is to understand if any changes in household food dynamics occurred in terms of food purchasing and eating behaviour. Specifically, this paper examines food purchasing behaviors, focusing on a segment of Italian population: university students and their families. A questionnaire was sent to understand the food purchasing habits during the period of total lockdown that characterized the first wave of Covid-19 pandemic.====The paper is structured as follows: section two provides a brief literature review on food purchasing habits during Covid-19 pandemic. Section three describes the methodology employed, the hypotheses formulated and key data. Section four presents the results of the research. Section five provides the discussion and concludes, highlighting the main practical and theoretical implications.",Food consumption changes during 2020 lockdown in Italy,https://www.sciencedirect.com/science/article/pii/S109094432200014X,June 2022,2022,Research Article,39.0
Gbandi Tchapo,"Saint-Etienne School of Economics - GATE LSE / UJM,The World Bank Group","Received 22 December 2021, Accepted 25 June 2022, Available online 30 June 2022, Version of Record 18 July 2022.",https://doi.org/10.1016/j.rie.2022.06.003,Cited by (0),"This article provides the first assessment of the relationship between competition over water resources and water-related civil conflicts within countries. Pressure on water resources is a major concern and source of conflict. In recent decades, this pressure has been increasing due to rising water demand. This study examines the effect of three indicators of water demand that are likely to affect the occurrence of water-related civil conflicts: population density, ====, and income per capita. We rely on a linear probability model with fixed-effects on a sample of 144 countries between 1961 and 2018. The results reveal that, among the three factors, demographic pressure is the only positive and robust determinant of the occurrence of water disputes. Furthermore, while pressure on water resources appears to be a relevant channel mediating population density's effect on the probability of water conflicts, this density effect is mitigated when populations' access to water resources for domestic consumption increases. Finally, it appears that population density is particularly relevant in explaining conflicts where the main issue is the access or control of water resources and where water is used as a weapon.","Among the 21st century's many challenges, the preservation of water resources is often identified as a key issue, especially when considering the United Nations Sustainable Development Goals. Since water is a vital resource for human existence and the world economy, the mere possibility of losing access to it in either the short or long term can lead to tensions between individuals, communities, peoples and economic actors (farmers, herders, industries, etc.), or between populations and governments, or across countries. Currently, the increasing use of water resources is exacerbated by the weight of an ever-growing world population, the growth of the global middle class, and technological advances that facilitate water's exploitation and distribution. In addition to climate change,==== which negatively influences water resources, the above factors intensify freshwater consumption in many sectors, including agriculture and industry,==== thus generating considerable pressure on this valuable resource.====Water is used in a variety of daily contexts, including household consumption, agricultural and industrial production and electricity generation. For this reason, water-related concerns have become increasingly important in recent decades and are the subject of ongoing debates and initiatives at both the national and international levels. Water stress lies at the root of increasing poverty, inequality, social tensions, instability, and security risks (Intelligence Community Assessment, 2012). More than a third of the world's population will suffer from water stress or scarcity by 2025 (Hameeteman, 2013). According to the United Nations, water use has been growing globally at more than twice the rate of population growth in the past century, and an increasing number of regions are reaching the limit at which water services can be sustainably delivered.====Given these challenges, many initiatives have been implemented to manage water problems. For instance, in the 1990s, the Food and Agriculture Organization (FAO) proposed the International Action Programme on Water and Sustainable Agricultural Development (IAP-WASAD) to promote sustainable agricultural development. In 2003, the United Nations established a framework to coordinate all freshwater-related matters. In 2015, it introduced specific objectives under the Sustainable Development Goals, calling for initiatives to ensure the availability and sustainable management of water and sanitation worldwide. Prior to that, the Millennium Development Goals established some measures that aimed to halve the proportion of the population without sustainable access to safe drinking water and basic sanitation by 2015. Many other forms of cooperation and agreements concerning freshwater resources have been developed.====As water is a limited resource, it must be considered a matter of national security (Gleick, 1993). The world has always been characterized by an unequal distribution of water, but this disparity has been growing in recent years. As freshwater resources become scarcer and their access becomes more unequal, community grievances may arise. Furthermore, water (e.g. rivers, groundwater) ignores boundaries.==== Thus, the overexploitation of water resources by economic actors or populations can affect its availability to other people. Finally, water pollution can be detrimental to populations even if it occurs at a considerable distance since pollutants can easily spread.====Conflict over water resources has a long history. In India, there have long been tensions concerning the water distribution process, often leading to conflicts that are costly in terms of instability, human life and economic activity. Water companies' staff are frequently harassed by residents. In 2019, water access triggered disputes between herders and farmers in Mali, leading to almost 37 deaths. Actually, all goods depend on water as an input. Termed “water footprint” every good requires a certain quantity of water during its production and processing. Therefore, increasing demand for goods, due to growing population size, international trade and income level may worsen the pressure on water resources.====While many water-related conflicts are interstate matters, an increasing number of disputes is limited to the country level. Fig. 1 shows the strong growth in water-related conflicts within countries around the world. While the literature has extensively explored international bilateral water disputes (Cooley, 1984; Lonergan et al., 1994; Wolf, 1999; Kreamer, 2012), to the best of our knowledge, no empirical study has analyzed conflicts that are only national in scope. This study attempts to fill this gap by considering tensions that are either within communities or between communities and governments or economic actors. These tensions include conflicts over water triggered by individuals or a group of people that have led to injuries, deaths or threats of violence, including verbal threats, military manoeuvres, and shows of force.====Competition over water resources is the main source of conflict (Salem et al., 2018). These disputes, involve actors with different interests who are motivated by many factors (Fröhlich, 2012). This study focuses on three of these potential factors: population density, international trade and economic development.====The demand for water from an increasing population is one of the main factors at play (Gleick, 1993). Demographic pressure captures one part of the internal water demand. The higher the population density, the higher the water demand====, and the lower its accessibility, given that water is a limited resource. Water price can also influence the relationship between density and water conflicts. On one hand, low density may lead to low water prices due to low demand, decreasing the pressure on the resource and thus reducing conflicts, all else being equal. On the other hand, lower density may also mean high water prices if we consider that water infrastructure expenses can be very considerable. Thus, if public policies fail to provide water to people, lower population density will still lead to higher water prices and thus to an increase in disputes.====As for international trade, an increase in agricultural and industrial activities for the international market can exacerbate water scarcity in a country. Debaere (2014) highlights that water is a source of comparative advantage====. However, producing goods for international trade can pollute water resources. The use of fertilizers and pesticides, for example, spoils water resources and can generate frustration and conflict. This issue is exacerbated by the high competition in international markets, which can lead firms to introduce more technology into their production process to extract water. In the agricultural sector, irrigation devices are one example of this phenomenon. The same picture is observed in the mining sector. Mining projects frequently generate community opposition. Water overuse and pollution are perceived as two primary sources of conflict in this sector (Bajani et al., 2020). Salem et al. (2018) analyzed water problems in Peru from 2007 to 2016 and found that the quality and availability of water are among the causes of water disputes, particularly with mining companies. These issues may trigger reactions from the locals, leading to conflicts among individuals or against firms and governments.====Countries’ development levels may also drive the demand for water. People in rich countries typically use more water to meet their basic needs than those in poor countries, all else being equal. As a consequence, high water demand should thus lead to more conflicts. However, people in developed countries may be more aware of environmental issues (due to high level of education and awareness campaigns) and they may have access to the technological tools necessary for efficient water use. The quality of public policies in high-income countries may also lead to better water management.====Our investigation is based on a panel dataset of 144 countries spanning from 1961 to 2018. The estimation method relies on a linear probability model (LPM) with fixed-effects. In general, the empirical results reveal that, among the three indicators of water demand retained in this analysis, demographic pressure is the only one that significantly raises the occurrence of water disputes. Furthermore, the findings indicate that pressure on water resources is a relevant factor mediating and exacerbating population density effect on the probability of water conflicts. However, this density effect is mitigated when populations’ access to water for domestic consumption increases. Finally, it appears that population density is particularly relevant in explaining conflicts where the main issue is the access or control of water resources and where water is used as a weapon. Given that the growing pressures on water resources may worsen local and regional tensions, this study aims to help policymakers and development practitioners undertake effective actions to avoid future conflicts.====The rest of the article is organized as follows. The next section outlines the methodology and describes the data sources used for the analyses. The results are presented and discussed in section 3. Section 4 explores the mechanisms mediating the effect of the primary determinant variable on the occurrence of water conflict, and Section 5 provides more details on the relationship under investigation. The last section brings the article to a close.",This water is all ours: Water demand and civil conflicts,https://www.sciencedirect.com/science/article/pii/S1090944322000151,June 2022,2022,Research Article,40.0
"Masoud Najeb,Al-Utaibi Ghassan","School of Business Administration, Al Dar University College, Dubai, United Arab Emirates,School of Future Foresight and Planning, Rochester Institute of Technology, Dubai, United Arab Emirates","Received 11 February 2022, Accepted 3 July 2022, Available online 8 July 2022, Version of Record 18 July 2022.",https://doi.org/10.1016/j.rie.2022.07.001,Cited by (1)," financial reporting related to cybersecurity incidents. The association between the cybersecurity risk disclosure and subsequent reported financial deficiencies is positive and significant, providing some evidence for regulators that more firm-specific disclosure may provide increased audit quality, to which the auditor responds by increasing audit effort. The empirical findings suggest that firms with prior cybersecurity risk disclosures are more likely to experience financial reporting deficiencies. The results obtained are robust to a variety of sensitivity checks.","The American Institute of Certified Public Accountants (AICPA) (2018, p.1) states that “cybersecurity is one of the top issues on the minds of management and boards in nearly every firm in the world - large and small, public and private”. Therefore, it is extremely important that although firms are only required to provide qualitative descriptions and do not need to quantify the likelihood or impact of the disclosed risks, they should be responsible in what and how to disclose. More clearly, the AICPA (2018) highlights that cybersecurity is not just an information technology (IT) problem, but also one concerned with enterprise risk management and that requires a global solution. Realising this, the Securities and Exchange Commission (SEC) has issued comment letters to require more risk information from firms (Johnson, 2010), warning firms to “avoid risk factor disclosure that could apply to any issuer or any offering” (SEC, 2010). The Sarbanes-Oxley Act of 2002 (SOX) also emphasised the importance of the firms placing a focus on strengthening their financial reporting controls and protecting investors. In respect of IT, SOX requires firms to have policies and procedures that prevent, detect, and disclose cybersecurity risks and incidents that are considered likely to be materially significant.====Evidence from this study confirms the significance for financial reporting of assessing the implications of cybersecurity incidents, and thus, relates to future financial reporting deficiencies. Existing research on indicators of cybersecurity risks is limited and in particular, questions concerning how cybersecurity risk disclosures affect the valuation of the firm due to the change in risk perceptions, affect financial reporting deficiencies. Cybersecurity risk and incident disclosures by firms are understood as signs of internal control material weaknesses in financial reporting (Benaroch and Chernobai, 2017; Benaroch et al., 2012; Chernobai et al., 2011) and can therefore present significant risk factors to the quality of financial reporting seen in firms’ annual reports (Hogan and Wilkins, 2008; Lawrence et al., 2018). Moreover, cybersecurity risk incidents in major organisations can result in significant damage to breached firms in terms of remediation costs, fines, and reputation for years (Cavusoglu et al., 2004; Gordon et al., 2011; Rosati et al., 2019a).====However, the cost is even greater, according to research on cybersecurity for large firms since as noted by Adler (2011), the average cost of a data breach is $214 per customer record. Using that average cost to extrapolate Target's 2013 Black Friday breach of as many as 110 million customer accounts illustrates the potential costs facing firms. Consequently, breaches have been shown to negatively impact firm valuation (Chai et al., 2011; Gordon et al., 2010). Further, since COVID-19, the US FBI has reported a 300% increase in reported cybercrimes, 95% of which are due to human error, and which predict a total cost for global cybercrime as reaching $6 trillion by 2021 (Cybint, 2020). The Standing Advisory Group of the Public Company Accounting Oversight Board (PCAOB) has also discussed the potential implications of cybersecurity risk and cyber-attacks on financial reporting and auditing (PCAOB, 2014). In this connection, DeFond and Lennox (2017) document that when PCAOB inspectors report issues in auditors’ internal control audit deficiencies, the auditor should issue an adverse opinion on internal control over financial reporting financial reporting, since as noted by Lawrence et al. (2018), a weakness in one area is likely to affect another area. Because of this, many researchers have measured the extent of technical, operational, administrative, and architectural internal control weaknesses (Ashbaugh-Skaife et al., 2008; Doyle et al., 2007b), and IT control weaknesses on the overall financial performance of firms (Haislip et al., 2016; Kuhn and Morris, 2017; Klamm and Watson, 2009; Masli et al., 2010; Messier et al., 2004), since they could have substantial explanatory value when considering the negative implications for financial reporting quality.====In 2011, the SEC issued its first guidance on disclosure obligations related to cybersecurity risks and incidents to assist firms in assessing what, if any, disclosures they should make in this field (SEC, 2011). The guidance states that while no explicit disclosure requirements pertaining to cybersecurity risks and cyber incidents were being imposed, the SEC views the disclosure of cybersecurity risks and issues as consistent with sharing timely and accurate information to help inform investment decisions. Therefore, some have argued that the guidance is becoming a de facto ruling (Grant and Grant, 2014). Indeed, in 2018, the SEC updated and expanded upon its 2011 guidance to further aid firms in preparing disclosures about cybersecurity risks and incidents (SEC, 2018). Firms’ responses to the SEC's act of “encouraging” disclosure can be considered a form of regulatory compliance. Therefore, the updated guidance underscores that the securities laws’ anti-fraud provisions apply when firm directors and officers make “selective disclosures” of non-public information about cybersecurity risks or cyber incidents (SEC, 2018, p. 7). Thus, cybersecurity involves the protection of information that is assessed and transmitted via any computer network (Gordon and Loeb, 2006). There is also some evidence suggesting that cybersecurity breaches can result in a higher likelihood of financial restatements in the year of the breach (Lawrence et al., 2018).====This paper seeks to build on this nascent stream of academic research and aims to provide the relationship between cybersecurity risk disclosure and financial reporting deficiencies. It is important to appreciate that the information conveyed by cybersecurity risk disclosures can help investors evaluate a firm's financial reporting and provide regulators with information about whether additional legislative rules are necessary to encourage firms to disclose more detail about their cybersecurity risks. In line with this argument, previous research has identified cybersecurity incidents and examined a set of contingency factors such as types of breach (Gordon et al., 2011; Yayla and Hu, 2011), firm characteristics (Blankley et al., 2012; Ettredge and Richardson, 2003), and information disclosed through news articles (Wang et al., 2013b) and distribution channels (Benaroch et al., 2012) that could deepen or mitigate the market reaction. However, only a few studies consider cybersecurity disclosure. Gordon et al. (2010) found an exclusive focus on the value of cybersecurity disclosures as a means of predicting future cybersecurity incidents. They further assert that while more cybersecurity does not always benefit an organisation, cyber-attacks are one of the main risks that organisations must control (Amir et al., 2018).====Based on the above arguments, it is imperative to synthesize the previous literature related to cybersecurity risk disclosure and identify the empirical research streams of the articles under review. In this study, a sample of 27,548 cybersecurity disclosures affecting US listed firms from 2006 to 2016, as reported by the Privacy Rights Clearinghouse (PRC) is used. This sample comes from firms audited by the Big4 auditors to ensure comparable levels of audit quality and financial reporting (Blankley et al., 2012; Rosati et al., 2020; Rosati et al., 2019b), and matches breached firms with non-breached firms operating within the same firm industry, and with the nearest firm size (total assets). Analysis is performed via a difference-in-difference (DID) model with year and industry fixed effects (see Khurana et al., 2020; Rosati et al., 2020; Rosati et al., 2019b; Yu et al., 2020), which allows the researcher to assess the improvement in financial reporting quality resulting from a cybersecurity risk disclosure while also taking into account the staggered nature of such incidents. In assessing the impact of cybersecurity risk disclosure on the quality of financial reporting, it must be recognized that a firm with a clean financial reporting valuation that experiences a cybersecurity breach has weaknesses concerning unauthorised access to its financial reporting information, even though it has no reported weaknesses in control over financial reporting at the time of the report.====Furthermore, the presence of cybersecurity risk disclosure is seen to be associated with subsequent financial reporting deficiencies revealing this practice to result in more disclosures by firms not having material cybersecurity risks. The study finds a positive significant association between cybersecurity incidents and firm-specific financial reporting, thereby suggesting that more firm-specific disclosure may provide an increase in audit quality, to which the auditor responds by increasing audit effort. This is an important finding because it supports the view that, despite being a significant risk disclosure, information relating to cybersecurity breaches does not result in financial reporting deficiencies or audit failure. Rather, the study documents a positive association between the variables audit quality and cybersecurity incident. This result can be rationalized via an interpretation of the principles of dynamics (described by, for example, Li et al., 2016; Rosati et al., 2020; Rosati et al., 2019b). Further, the researcher argues that the increased audit effort to provide financial information results in a positive post-incident effect. A robust analysis is also made of the situation when controlling for firm and industry characteristics and other determinants of financial reporting risks.====This study contributes to the growing literature on the impact and consequences of cybersecurity risk disclosure on financial reporting deficiencies in firms. In this respect, it makes a contribution to the cybersecurity risk disclosure literature, since existing studies (Gordon et al., 2011; Li et al., 2018, 2016; Yayla and Hu, 2011) mostly focus on cybersecurity incidents. Specifically, the approach to data collection in the study enables analyses on a much larger scale to demonstrate that firms facing greater cybersecurity risks allocate a greater portion of their disclosures to this area in their financial reporting. Secondly, the study also contributes to the literature on financial reporting deficiencies, confirming that although cybersecurity incidents result in an increase in audit risk (Rosati et al., 2019b; Li et al., 2018, 2016), they do not prove to be detrimental to the quality of financial reporting. Specifically, the study brings forth evidence that greater disclosure may promote increased audit quality, since the auditors can decrease audit risk by increasing their audit effort. It also provides further evidence of the importance of the SEC in addressing cybersecurity risk disclosures in regulatory monitoring. Here the SEC has warned firms to “avoid generic risk factor disclosure that could apply to any firm,” the outcome being consistent with the SEC's intention. Thirdly, the results can also help the board of directors, executives, and policymakers to determine the benefits of cybersecurity risk disclosures and the negative consequences of financial reporting deficiencies. And the study provides further evidence of the importance of addressing sample bias and unobserved differences in treatment and control firms’ financial reporting. In addition, using word-term patterns helps to obtain a thorough understanding with respect to the consequences of cybersecurity risk disclosure that firms are most concerned about, those being the difference-in- difference approach to identify potential causal relationships that then allow for an estimation of the relationship between cybersecurity events and firms’ financial reporting.====The remainder of this paper is organised as follows. The next section reviews the relevant literature and leads to the hypothesis development. This is followed by an explanation of the research methodology including details of sample selection procedures. Next, the empirical results are presented and discussed. And finally, the paper is concluded with some final remarks and avenues for future research highlighted.",The determinants of cybersecurity risk disclosure in firms’ financial reporting: Empirical evidence,https://www.sciencedirect.com/science/article/pii/S1090944322000163,June 2022,2022,Research Article,41.0
"Aggarwal Divya,Kalia Deepali","Indian Institute of Management Ranchi, Ranchi, Jharkhand, India","Received 8 January 2022, Accepted 7 July 2022, Available online 9 July 2022, Version of Record 18 July 2022.",https://doi.org/10.1016/j.rie.2022.07.003,Cited by (0)," uncertainty (GPU) with PPI-P&C index. The time-frequency relation between the indices is examined using wavelet coherence analysis (WCA), whereas the casual dependency is examined using the non-parametric causality in quantiles (CIQ) approach and linear and non-linear Granger causality tests. WCA shows significant co-movement phases between CPU and PPI-P&C across time-frequency domain with CPU leading the PPI-P&C over a specific time interval. Results from CIQ give evidence of uncertainty indices having an asymmetric significant dependency relation with the PPI-P&C index. The results have implications for examining the impact of rising uncertainties on rising insurance costs for P&C insurance providers.","One might argue that the insurance industry evolves and emerging risks allow for product repricing which generates a sense of safety for insurance providers. However, insurance providers always strive to model premium calculations by factoring in potential uncertainties to maintain a sustainable business model. The physical damage stemming from the increased severity and frequency of natural catastrophes can make some risks uninsurable and even unaffordable for Property & Casualty (P&C) insurance companies. In its recent report, Mckinsey & Company highlighted the need for P&C insurance companies to relook at both their business model and investment strategies under reeling climate change risks (Grimaldi et al., 2020). Hence, underestimating climate change risks can become detrimental as these risks can lead to systemic nonlinear effects, which require a reassessment of the true nature of climate risks in risk models. Scholars have shown that increasing uncertainty due to economic policy uncertainty (EPU) and geopolitical uncertainty (GPU) has a time-varying adverse influence on both insurance providers and insurance consumers (Balcilar et al., 2020; Hemrit, 2021).====The objective of this study is to contribute towards the unresearched gap of exploring the impact of climate policy uncertainty (CPU) on P&C insurance premiums. This study also aims to explore the relation of other uncertainty of economic policy uncertainty (EPU) and geopolitical uncertainty (GPU) on P&C insurance premiums, to provide insights on the impact of myriad uncertainties on P&C insurance premiums. To represent P&C insurance premiums, the study has chosen the producer price index of P&C insurance premium (PPI-P&C). Hence, the study examines three broad research questions; firstly does PPI-P&C show co-movement with CPU; secondly does PPI-P&C show similar co-movements with other types of uncertainty related to EPU, and GPU and lastly is there a causal dependency between the uncertainty indices and PPI-P&C. We test the following hypotheses:====The co-movement among the underlying indices is examined using wavelet coherence analysis (WCA) following the approach of (Torrence and Gilbert, 1998). The causal dependence structure is examined using the non-parametric causality in quantiles (CIQ) methodology on lines of (Balcilar et al., 2017) and with the help of linear and non-linear granger causality analysis (Reboredo et al., 2017).====The results support significant co-movement between CPU and PPI-P&C with the former leading the latter at medium frequency intervals of 8-16 months across a certain period. Results from non-parametric CIQ show that all the news-based uncertainty indices have a causal impact on the mean and variance of PPI-P&C, between quantiles ranging from 0.2 to 0.8. Results of granger causality tests also show both linear and non – linear causal dependence of PPI-P&C on CPU, EPU and GPU. The study is organized as follows, with Section 2 presenting the methodology, followed by data and empirical findings in Sections 3 and 4 respectively. Section 5 presents the conclusion of the study.",Examining comovement and causality between producer price index for P&C insurance premium and uncertainty indices: Wavelet and non-parametric quantile causality approach,https://www.sciencedirect.com/science/article/pii/S1090944322000175,June 2022,2022,Research Article,42.0
Barro Robert J.,"Harvard University, MA 02138-3001, United States","Received 3 May 2022, Accepted 24 June 2022, Available online 25 June 2022, Version of Record 18 July 2022.",https://doi.org/10.1016/j.rie.2022.06.001,Cited by (2),"A key issue for the ongoing COVID-19 pandemic is whether non-pharmaceutical public-health interventions (NPIs) retard death rates. Good information about causal effects from NPIs comes from flu-related excess deaths in large U.S. cities during the second wave of the Great Influenza Pandemic, September 1918-February 1919. The measured NPIs are in three categories: school closings, prohibitions of public gatherings, and quarantine/isolation. Although an increase in NPIs flattened the curve in the sense of reducing the ratio of peak to overall flu-related excess death rates, the estimated effect on overall deaths is small and statistically insignificant. These findings differ from those associated with COVID-19 in the sense that facemask mandates and usage seem to reduce COVID-related cases.","Epidemiologists, notably ====; ====; and ====, have studied effects of non-pharmaceutical public-health interventions (NPIs) on flu-related excess deaths in large U.S. cities over the 24-week period corresponding to the peak of the Great Influenza Pandemic, September 1918-February 1919. The weekly data on flu-related excess death rates come from U.S. Census Bureau, ====, reproduced in ====, Appendix, Table B). Continuous weekly data over the study period are available for 45 of the 50 largest U.S. cities, where this group of 50 corresponds to a central-city population in the 1910 U.S. Census of at least 100,000.==== Monthly data on flu-related excess deaths are available for these cities back to 1910 (Collins, Appendix Table A).====The underlying information on NPIs in ==== comes from articles in two newspapers in each city, along with other sources. The main data were reported as number of days in which NPIs of each type were in effect, with a focus on a variable that considers the presence of any type of NPI. For example, when school closings and prohibitions of public gatherings prevail on the same day, the variable records two days’ worth of NPI.==== ==== raises objections to the NPI data collected for New York City, and these objections are discussed below. Business closings, emphasized in the context of the ongoing coronavirus pandemic, were not implemented in broad form during the Great Influenza, although staggering of business hours was common.==== Likely because of the absence of substantial business closings, ==== did not focus on this category of NPI. However, the variable for prohibitions of public gatherings includes closings of theatres, bars, entertainment and sporting events, and so on.====The present analysis considers two characteristics of each city's flu-related excess death rates: first, the cumulative flu-related excess death rate out of the total population over the full 24-week study period, and second, the ratio of the peak weekly excess death rate during the period to the overall death rate. (All death rates are expressed at annual rates.) A lower cumulative death rate is a reasonable gauge of the ultimate success of the NPIs. In contrast, a lower relative peak implies a smoother pattern, often described as a “flattening of the curve,” which can be desirable from the standpoint of easing burdens on the healthcare system, possibly leading thereby to fewer cumulative deaths. However, for a given overall excess death rate, if an NPI lowers the relative peak, the implication is that the intervention delays deaths but does not ultimately avoid them.====The epidemiologists used standard epidemiological models to study the dynamics of flu-related excess deaths during the Great Influenza Pandemic, as illustrated by ====. The present analysis focuses instead on two measures of overall outcomes—the cumulative flu-related excess death rate over the study period (which corresponds to the areas under the curves in ====) and the peak weekly excess death rate measured relative to the overall rate.====As already mentioned, there is concern that NPIs—measured, say, by length of time in force—and flu-related death rates are simultaneously determined at the level of cities. On the one hand, the basic hypothesis is that more NPIs reduce death rates. On the other hand, NPIs implemented by city governments are likely to respond to death rates in terms of numbers realized or anticipated. The implicit assumption in the statistical analysis by the epidemiologists is that NPIs are determined exogenously; that is, shifts in actual or anticipated flu-related excess death rates do not impact the chosen NPIs. Bootsma and Ferguson (p. 7592) recognize the endogeneity problem—“Causality will never be proven, because, unsurprisingly, control measures were nearly always introduced as case incidence was increasing and removed after it had peaked”—but did not deal with it. The present research attempts to account for the potential endogeneity of NPIs by employing an instrumental-variable approach.====Appendix ==== shows the data used for the 45 large U.S. cities in the sample. The variables include measures of death rates, measures of non-pharmaceutical interventions (NPIs), and other variables. ====.","Non-pharmaceutical interventions and mortality in U.S. cities during the great influenza pandemic, 1918–1919",https://www.sciencedirect.com/science/article/pii/S1090944322000138,25 June 2022,2022,Research Article,43.0
"d'Albis Hippolyte,Badji Ikpidi","Paris School of Economics and CNRS, Mail: 48 Boulevard Jourdan, Paris 75014, France,Economix, Université Paris Nanterre, Mail: Bâtiment G - Maurice Allais, 200 Avenue de la République, Nanterre 92001-CEDEX, France","Received 2 November 2021, Accepted 20 March 2022, Available online 26 March 2022, Version of Record 26 April 2022.",https://doi.org/10.1016/j.rie.2022.03.003,Cited by (0),"Intra-generational ==== focus on the distributions within age groups. On the basis of French household income surveys carried out from 1996 to 2014, the ==== and D9/D1 inter-decile ratio were calculated so as to evaluate intra-generational income inequality before and after redistribution by the ","In this article we examine developments in inequality from a generational perspective. We want to know whether young generations face greater inequality than their predecessors. Inequality can breed resentment and it is accepted that inequality, along with standard of living, is a major component of the well-being of population groups. Scholars are increasingly integrating this topic in composite indicators, for example Fleurbaey and Gaulier (2009) and Jones and Kleenow (2016). The novelty here is to consider that inequalities within age group -or generation- are those that matter for a given individual.====Generational analysis of inequality faces two difficulties. First, it is not possible simply to analyse the change over time of an indicator that gives a degree of inequality at a given date. It would be misleading to suppose that a given variation in inequality in recent years generates the same variation in inequality for recent generations, since the former is likely to affect all generations alive at the time. Similarly, no change in inequality does not necessarily imply that all generations are equal with respect to inequality. Our first challenge is therefore to attribute the relevant variations in inequality to each generation. Inequality that was now greater among the working population than the retired would not necessarily imply that the generation that is currently active will over its lifetime experience greater inequality that its predecessors. We need to track the inequality specific to each cohort over its life cycle. Where we have incomplete data, as is almost always the case, we have to use estimates.====These two difficulties are fairly standard in cohort analyses and can be found in estimates of changes in living standards from one generation to the next. The novel feature here is to apply these estimates to the topic of inequality. Our starting position is to use intra-generational inequality, i.e. the distribution of income within an age group. This demographic approach to inequality has been used in particular by Mather and Jarosz (2014) and Fisher et al. (2018). The main assumption in our article is that this is a most relevant inequality. This is based on the intuition that individuals compare themselves more with others of the same age than with those of other generations. Starting from this assumption, we can construct inequality indicators relating to certain parts of the life cycle of generations observed in surveys and thus distinguish cohort effects from age effects.====We have used the Tax Income Survey (Enquête Revenus Fiscaux – ERF) and Tax and Social Incomes Survey (Enquête Revenus Fiscaux et Sociaux – ERFS), conducted by the French National Institute of Statistics and Economics Studies (Insee) to estimate annual income distribution before and after the redistribution induced by taxation and welfare transfers for five-year age groups from 1996 to 2014. Gross income (before redistribution) is not directly recorded in the surveys and we reconstructed for each household employer and employee contributions for pensions and unemployment. Intra-generational inequality was then estimated using the Gini coefficient and the inter-decile ratio. We also constructed for each inequality indicator a variable of the difference between income inequality before and after redistribution in order to measure the effect of the French tax and welfare system on intra-generational inequality. Like Piketty et al. (2018) and Bozio et al. (2018), we have thus estimated the effect of redistribution on inequality, but focusing in our case on intra-generational inequality and its variation from one generation to the next.====We estimated Age-Cohort-Period models to evaluate the effects of age and generation on intra-generational inequality. To our knowledge, this method has not often been applied to inequality. The exception is Deaton and Paxson (1994b), who use it to test the prediction of the permanent income hypothesis that income variance increases with age. We find that intra-generational inequality fluctuates considerably over a lifetime. It is very high at age 55–59 and much lower both at the start of working life and in retirement, when inequality is measured by the inter-decile ratio. The tax and welfare system plays a major role for those under 50 and a lesser role later. Over their lifetime, households thus see income inequality in their age group first increase then decrease. Allowing for this age profile, it is possible to compare generations and evaluate whether recent generations live in a more unequal environment than their predecessors. Taking the top and bottom deciles of the distribution, we find that inequality in gross income increased sharply beginning with the cohorts born in the 1970s; however, the tax and welfare system corrected this, because we find no significant differences between generations in inequality of disposable income. This redistribution effect of the tax and welfare system parallels the findings of Bozio et al. (2018) for the French population as a whole and of d'Albis et al. (2019) for inequality between generations. Recent researches have shown that living standards in France are not falling from one generation to the next, and that there is consequently no basis to the idea of a “sacrificed generation” (d'Albis and Badji, 2017). Economic growth benefits everyone and younger generations enjoy on average higher living standards than in the past. Subsequently, however, it has been shown that this improvement does not apply to all sections of the population: in particular, the living standards of men without the ==== (secondary school leaving certificate) in recent generations are lower than for previous ones (d'Albis and Badji, 2021). We add to this line of research by directly analysing variations by generation in the traditional indicators of inequality such as the Gini coefficient and inter-decile ratio. In particular, we find that inequality as measured by the Gini coefficient increases steadily generation after generation and the tax and welfare system does not manage to counter that increase.====We present first the data used, the method for constructing our indicators of intra-generational inequality and some descriptive statistics. Then we describe in detail our econometric model and present the results of our estimates, before concluding.",Inequality within generation: Evidence from France,https://www.sciencedirect.com/science/article/pii/S1090944322000047,26 March 2022,2022,Research Article,44.0
Fillmore Ian,"Washington University in St. Louis, USA","Received 9 December 2021, Accepted 20 March 2022, Available online 31 March 2022, Version of Record 26 April 2022.",https://doi.org/10.1016/j.rie.2022.03.002,Cited by (0),"Many economists and policymakers implicitly assume that “previous, modest increases in the minimum wage” are informative about the effects of a $15 minimum. ==== predicts that the employment effects of the minimum wage should vary with the composition of affected occupations and industries. I find that a $15 minimum would affect a far broader set of occupations and industries than prior increases, calling into question whether we can extrapolate from past experience with the minimum wage. I find that the frontier of historical experience is a federal minimum between $9 and $11.","Recent proposals have advocated for raising the federal minimum wage from $7.25 to $15. While opponents have objected that this would significantly reduce employment and harm low-wage workers, advocates have argued that such concerns are overblown, pointing to papers finding small or zero effects of the minimum wage on employment.==== But, of necessity, estimates of the employment effects of the minimum wage are based on historical experience with prior increases in the minimum wage. For example, in a letter advocating for a $15 minimum, over one hundred economists, including several prominent economists, argue that====The Congressional Budget Office also relies on prior experience when asked to evaluate the likely effects of a $15 minimum. In its 2019 report “The Effects on Employment and Family Income of Increasing the Federal Minimum Wage” (Congressional Budget Office, 2019), CBO projects 1.3 million workers would become jobless in response to a $15 minimum. This projection was based on its own review of the literature on the employment effects of the minimum wage.====Thus, many economists and policymakers are implicitly assuming that “previous, modest increases in the minimum wage” are indeed informative about the effects of a 15 minimum. However, economic theory predicts that the employment effects of the minimum wage should vary with the composition of affected occupations and industries. I find that a $15 minimum would affect a far broader set of occupations and industries than prior increases, calling into question whether we can extrapolate the employment effects of a $15 minimum based on past experience.====Much of the debate over a $15 minimum has focused on its size. For instance, $15 is high relative to historical levels of the federal minimum wage (see Fig. 1). Between 1960 and 1980, the real minimum wage hovered around $10 per hour (in 2019 dollars). Then, beginning in the 1980s, it fell and has generally varied between $7 and $8. Thus, not only would a $15 federal minimum be far outside of the last 40 years of historical experience, it would considerably exceed its highest-ever level of $11.76, set in 1968. In percentage terms, the most recent increase in 2007 was also the largest ever—a 41 percent increase in the nominal minimum wage phased in over 3 years. The prior two increases were 27 percent, in the late 1980s, and 21 percent in the mid-1990s. In contrast, $15 would represent a 107 percent increase in the nominal minimum wage. However, focusing solely on the size of the minimum wage may provide perverse guidance to policy makers. Would raising the minimum wage by 41 percent always be ==== historical experience, even if the minimum had been raised by 41 percent just a few years earlier? Would raising the minimum above $11.76 always be ==== historical experience, even if productivity gains have moved the real wage distribution far to the right of where it was in 1968?====Economic theory predicts that the employment effects of the minimum wage should vary across labor markets. In some labor markets, like those for doctors or lawyers, a $15 minimum would not be binding and would have no effect. In other markets, a $15 minimum would be binding, but the employment effect would vary with several factors including the size of the increase relative to the current wage distribution, the supply and especially demand elasticities for labor,==== and the amount of monopsony power, if any, that employers possess. Thus, the estimates in the empirical literature are driven by how these factors happened to align in those labor markets that were affected by prior minimum wage increases. For example, since past increases primarily affected a small number of service industries, particularly fast food, we have little direct experience with the effect of the minimum wage in, say, manufacturing.====How can we determine whether a proposed minimum wage is “within historical experience?” I propose using data from the Occupational Employment and Wage Statistics (OEWS) program at the U.S. Bureau of Labor Statistics. This data provides annual wage percentiles for detailed occupation codes and industries, both nationally and at the state level. Since OEWS only goes as low as the 10th wage percentile, I consider an occupation to be “measurably affected” by a minimum wage increase if the proposed minimum exceeds the 10th percentile of that occupation’s wage distribution. And I define an occupation to be “strongly affected” if the proposed minimum exceeds the 25th percentile.==== The same applies for industries. If a proposed minimum wage increase affects similar occupations and industries as past increases did, then the proposal is within historical experience. If it does not, then it is outside historical experience.====I find that, by any measure, a $15 federal minimum wage is far outside historical experience. The most recent increase in 2007, the largest in history, strongly affected 25 occupations and two industries.==== By contrast, a $15 minimum in 2019 would strongly affect 245 occupations and 45 industries. Strongly affected occupations in 2006 can account for only 28.1 percent of employment in occupations that would be strongly affected by a $15 minimum in 2019. Similarly, strongly affected industries in 2006 can account for only 17.5 percent of employment in strongly affected industries in 2019. Thus, regardless of one’s views on the employment effects of past minimum wage increases, these effects cannot be directly applied to a $15 federal minimum.====Although a $15 minimum wage would be outside historical experience, smaller increases in the minimum wage would not be. For example, I find that a $9 minimum would be well within historical experience and that the frontier of historical experience falls between $9 and $11. It’s important to note that being within historical experience does not tell us anything directly about the employment effects of a proposed minimum wage. Rather, it merely indicates that prior experience is likely to be a reasonable guide. Thus, while my results say nothing directly about the employment effects of a $9 minimum, they do show that raising the federal minimum from $5.15 to $7.25 in 2007 affected a similar set of occupations and industries to those that would be affected by a $9 minimum in 2019. This suggests that the employment effects of the minimum wage hike in 2007, be they small or large, could reasonably be applied to predicting the employment effects of a $9 federal minimum in 2019.====The methodology in this paper offers policy makers, federal or state, a practical approach to assessing whether any proposed minimum wage increase is within historical experience. It may also be useful to state-level policymakers for deciding which minimum wage increases in other states would be a useful guide for a proposed increase in their own state. For example, if state A is dominated by the financial services industry and state B is dominated by manufacturing, a minimum wage increase in state A may provide a poor prediction for the effects of a similar increase in B. I also discuss several recently enacted increases in state-level minimum wages that are scheduled to phase in over the next several years. These scheduled increases offer researchers an opportunity to estimate the heterogeneous effects of the minimum wage by occupation and industry, which state policy makers can use to make better predictions about the effect of the minimum wage in their home states, based on their state’s mix of occupations and industries.",A $15 federal minimum wage is outside historical experience,https://www.sciencedirect.com/science/article/pii/S1090944322000059,March 2022,2022,Research Article,45.0
"Polyzos Efstathios,Kuck Simon,Abdulrahman Khadija","College of Business, Zayed University, Khalifa City, P.O. Box 144534, Abu Dhabi, United Arab Emirates,College of Business and Economics, United Arab Emirates University, P.O. Box 15551, Al Ain, United Arab Emirates","Received 2 November 2021, Accepted 8 March 2022, Available online 11 March 2022, Version of Record 26 April 2022.",https://doi.org/10.1016/j.rie.2022.03.001,Cited by (4),"This paper analyses the conditional effect of demographic change on economic development in the MENA region. We employ fixed-effects panel analysis on data from 19 countries in the region and demonstrate a negative impact of natural rents on the relationship between the working-age population and economic growth. Once the critical level of approximately 16% of resource rents (as share in total GDP) is reached, a one-unit increase in working-age population appears to harm economic growth. Further tests show that this finding is mainly driven by the negative effects of resource rents on female labor force participation. However, other drivers are a large public sector, low private sector development and inefficient ==== and issues such as the “Dutch disease”. The main finding remains after robustness checks in the form of controlling for competing hypotheses. Policy makers are advised to encourage economic diversification, female employment and private sector development.","This paper examines the relationship between demographic transition (proxied by working-age population) and economic growth, particularly under influence of natural resources in the MENA (Middle East and North Africa) region. It aims to calculate a threshold of resource rents after which a unit increase in working-age population harms economic growth. The data panel is compiled of data for 19 MENA countries covering the period from 1992 to 2014. After applying several different models, we observe a significantly negative effect in the relationship between the working-age population and resource rents. The analysis finds that the critical level of resource rents, as percentage of GPD, after which a working-age population harms economic growth is approximately 16 percent. This main finding remains robust for several tests, such as adding competing interaction terms. However, further analysis shows that one key driver for the negative effect of demographic transition in the MENA countries might be the comparably low female labor force participation (FLFP) in this region.====The relevant literature includes many papers on the relationship of natural resources, demographic transition and resource rents. The seminal paper by Ross (2008) discusses these points, with respect to primarily Muslim countries. Our work is focused on examining whether demographic change and FLFP are endogenous to oil rents, something which is implied by Ross. We believe that this represents a gap in the literature that is worth exploring further. Focusing on the MENA region in a panel study also adds to the discussion, since analyzing global samples can prove problematic in terms of bias induction. Despite the fact that MENA countries tend to have limited variation in terms of demographic changes and in FLPF, there are notable exceptions included in our sample (for example the United Arab Emirates and Qatar). This adds further appeal to performing a study focused on this region.====Our research aims to address issues in the future of the labor market in these countries, which is indeed a crucial question, especially considering the rapid demographic changes that are taking place. Policy makers, in resource-rich countries in particular, want to ensure that the labor market can supply enough job opportunities in order to reap the benefits of these changes. In addition, despite cultural traditions, which are also changing rapidly, most countries in the region try to encourage FLFP. These changes and their effects on economic growth need to be examined under the context of resource rents. Such changes can also add to the discussion on the reduction of poverty (Wietzke, 2020) and of intrahousehold resource allocation (Fuwa et al., 2006).====Our paper adds to the existing literature by linking resource abundance and demographic change and analyzing their combined impact on economic growth in the MENA region. Demographic change is proxied by total working-age population as suggested in the literature (Bjorvatn and Farzanegan, 2013). The MENA region is the best choice for this study, since it is characterized by both high resource rents and a strong demographic change. Panel data analysis is applied for on data for 19 countries for the period from 1992 to 2014, accounting for both year and country fixed effects.====Following the approach of Bjorvatn and Farzanegan (2015) will enable us to calculate a critical level of resource rents after which the impact of the working-age population growth on economic development turns from positive to negative. The inclusion of other relevant interaction terms as additional robustness-checks ensures that the tested channel of resource rents is the main driver of our results. Moreover, FLFP is an important factor since it is considerably influenced by the presence of resource rents in an economy. The MENA region has the lowest FLFP rates in the world (Fig. 1). Even though we can observe a moderate increase in labor force participation rates over time, the region is still lagging compared to the rest of the word. Therefore, we also analyze an additional model in which the overall working-age population was replaced by the female working-age population in order to examine the extent to which the results are driven by the absence of women in the labor force. Furthermore, we control for lagged independent variables in an additional robustness-check to reduce endogeneity that triggers reverse causality effects.====Our work contributes to the existing scholarly literature, which mainly focuses on the direct effects of demographic transition or natural rents on growth but not on the conditional impact of resource rents. Second, it is, to the best of our knowledge, the first study of this kind to focus strictly on the MENA region and to include resource rents in the discussion. Existing research is mainly based on global panel data analysis. Hence examining the MENA countries as a separate group can help yield more accurate results for this region, especially given its increasing importance in the world economy. Data show that the MENA region is characterized by the lowest FLFP rates among all regions. This might be specifically driven by the resource abundance of this region that negatively affects participation of women in the job market. Dutch disease is one of the factors that impacts FLFP, since it harms the tradable sector of resource rich economies that usually employs a high share of women.====The paper is structured as follows. Section 2 provides an overview of the relevant literature. Section 3 discusses the data and the methodological approach used. Section 4 presents the regression results by focusing on the interaction of resource rents and the working-age population. Section 5 concludes with our policy suggestions.",Demographic change and economic growth: The role of natural resources in the MENA region,https://www.sciencedirect.com/science/article/pii/S1090944322000035,11 March 2022,2022,Research Article,46.0
"Barro Robert J.,Ursúa José F.,Weng Joanna","Harvard University, United States,Dodge & Cox, United States,Dartmouth College, Tuck School of Business, United States","Received 13 January 2022, Accepted 17 January 2022, Available online 21 January 2022, Version of Record 26 April 2022.",https://doi.org/10.1016/j.rie.2022.01.001,Cited by (3),"Data for 48 countries during the Great Influenza Pandemic imply flu-related deaths in 1918–1920 of 40 million, 2.1 percent of world population, implying 160 million deaths when applied to current population. Regressions with annual information on flu deaths 1918–1920 and war deaths during WWI imply flu-generated economic declines for GDP and consumption in the typical country of 6 and 8 percent, respectively. Higher flu death rates also decreased realized real returns on stocks and, especially, on short-term government bills.",None,"Macroeconomics of the Great Influenza Pandemic, 1918–1920",https://www.sciencedirect.com/science/article/pii/S1090944322000011,21 January 2022,2022,Research Article,47.0
"Angelini Francesco,Castellani Massimiliano","Department of Statistical Sciences “Paolo Fortunati”, University of Bologna, Piazza Teatini, 10, Rimini 47921, Italy","Received 8 October 2018, Accepted 17 January 2022, Available online 20 January 2022, Version of Record 26 April 2022.",https://doi.org/10.1016/j.rie.2022.01.002,Cited by (2),"In this paper, we model private art market agents’ strategic interactions in presence of two types of asymmetric information, about artwork quality and buyer’s knowledge, assuming the seller does not know how informed is the buyer while the buyer does not know the quality of the artwork before purchase. If the seller can choose either a high or a low price and the buyer can signal his type to the seller, we identify the conditions for both equilibria with pooling buyer signalling strategy and with separating strategy, as well as conditions for equilibria where the seller fixes the price according to the actual quality and where he posts prices trying to take advantage of buyer’s limited information. Finally, we identify the condition for the emergence of a “counter-lemon” result, where low-quality artworks and uninformed collectors exit the market, suggesting that seller uncertainty does not directly benefit the buyers, but it can impact the quality traded in the market.","Among markets characterised by asymmetric information issues, there are surely art markets, where cultural goods are traded. These goods feature a hard-to-evaluate quality and a multi-dimensional value (McCain, 1980), made up of an economic value, strictly related to their prices, and a cultural value, which influences the former but can also be evaluated in a non-market setting (Angelini and Castellani, 2019). Price formation of cultural goods is then influenced by these characteristics and by the interaction between the agents operating in the art market and with other stakeholders interplay who can have an active role in the art world without being part of the trades (Angelini and Castellani, 2021).====Concerning pricing mechanisms, the existence of asymmetric information could generate misbehaviour and opportunistic choices in the hands of those who hold superior information, who are generally the sellers (Akerlof, 1970); in fact, collectors who are not knowledgeable of the market could end up with misattributed or fake artworks (Radermecker et al., 2021), paying for a higher-valued artwork, being unable to identify the actual quality.====In this framework, we want to study how asymmetric information framework interacts with price setting in the private art market, where galleries and art dealers mainly sell to private collectors.==== In this market, price is formed through “haggling” (Velthuis, 2011), i.e., a bargaining process on the discount of price between the buyer and the seller, following the dealer posting a price for an artwork. While the actual discount is private information and depends on the market power of the two parts, it is interesting to study the price formation considering the type of buyer the seller faces: in particular, it is likely that when a knowledgeable buyer is offered a low-quality artwork as if it were a high-quality one, he can recognize it after purchase and ask for reimbursement, as in an experience good market with guarantees, while when a less informed buyer wants to buy the same artwork, he has to rely on quality signals, but like in credence good markets, the quality is never actually known by the consumer (even after purchase).==== In this paper, using a standard framework in cultural economics, we consider two different types of collectors: “insider” collector to refer to an informed buyer and “outsider” collector to indicate a less informed collector.====Recently, cultural economics research focused on identifying a cultural good as either credence or experience good (or as other types of good), depending on the information available to the agents and the particular features of each cultural good.==== For example, both Blaug (2001) and Krueger (2005) state that cultural goods are experience goods, suggesting that “a temporal process of consumption” is needed to understand the quality of the good and know the utility derived from its consumption. Others scholars suggest that cultural goods are credence goods, meaning that their quality cannot be identified by the buyer but is known to the seller, or even “meta-credence” goods, for which their quality cannot be identified by neither the seller nor the buyer (Ekelund et al., 2020). For the contemporary art market, Zorloni (2013) introduces the concept of “trust good, whose quality is not assessable by the buyer neither before nor after purchase due to lack of technical and cultural knowledge”. In this vein, Bonus and Ronte (1997) define “cultural quality” which can be evaluated only thanks to a very specific type of cultural knowledge. Finally, Caves (2003) asserts that “in creative industries ====, and the core problem is one of symmetrical ignorance”, while Lupton (2005) posits the concept of “indeterminate good”, whose quality is uncertain for everyone, though for the artworks the artist alone knows the quality.====The salience of these types of goods can be observed both in the high-end and in the low-end art market, but the sales in the high-end market, such as fine arts and luxury goods, are mostly intermediated by auction houses, while the sales in the low-end, regarding for example ethnic and tribal art, prints and collectibles, etc., are mainly intermediated by galleries and art dealers. In this private information context characterised by asymmetric information, signals of quality become particularly important for the buyers (Candela, Castellani, Pattitoni, 2012, Wolinsky, 1983). Among these signals, the most important are the artwork price (Throsby, 2001), the artist’s talent and fame (Adler, 1985, Candela, Castellani, Pattitoni, Di Lascio, 2016, Rosen, 1981), and the artist human brand (Angelini et al., 2022, Hernando, Campo, 2017, Muñiz, Norris, Fine, 2014, Preece, Kerrigan, 2015, Schroeder, 2005, Zorloni, 2005). However, the use of these signals interacts with the pricing strategy of the sellers and with the choices of the collectors (Radermecker et al., 2021), which also depend on market power and on how innovative is the artist traded, on the knowledge the buyer has of the art market (Brito, Barros, 2005, Champarnaud, 2014, Di Gaetano, Mazza, Mignosa, 2019), and on the market channel in which the gallery is located (Angelini, Castellani, 2018, Cellini, Cuccia, 2014).====The co-existence of asymmetric information issues, agents’ strategic choices, and imperfect signals that we illustrated so far, are the features of a complex market where adverse selection and opportunistic behaviour are likely to emerge (Akerlof, 1970). In the case of asymmetric information, the role of signals on quality has been studied within the ethnic art market (Candela et al., 2012) and within adverse selection models (Palazzo, 2017), while Radermecker et al. (2021) studied the role of signals on price formation in presence of (possibly) misattributed and fake artworks. However, in the cultural economics literature there is no focus on the strategy a buyer can follow in disclosing fake information about himself, mimicking a higher knowledge of the market when facing the seller. This type of strategy is limited to 1-to-1 transactions where bargaining is (possibly) at work since an auction mechanism would get rid of any possible incentive for the buyer to misdisclose his status to the information holder.====In this paper, we develop an art market signalling model assuming that two types of collectors exist, an informed type (insider) and an uninformed type (outsider), and that the dealer cannot identify the type of buyer before trading. We assume that an additional (and usually observed in the art market) asymmetric information issue is present, namely the dealer knows the artwork quality before trading, while the insider collector does not before trading but can know it after purchase, while the outsider collector can never inspect the quality because is too costly. In our model, the buyer can signal his type to the seller, possibly misdisclosing it to take advantage of the seller’s uncertainty and possibly sustaining a cost. Depending on this strategic choice and on the price posted by the seller, we identify the condition for pooling and separating equilibria about the disclosure of the type of the buyer, showing that the introduction of uncertainty in the buyer’s type does not seem to benefit the outsider collectors. However, we find that a “counter-lemon” equilibrium exists, in which the low quality and the outsider collectors exit the market.====Our paper contributes to the literature on the role of information asymmetries in private art markets, taking into account the previously unexplored possibility of misdisclosure of the information held by the collector when interacting with a private art dealer. Our model also adds to the previous studies on the private art market and on the price formation within it (e.g., Benhamou, Moureau, Sagot-Duvauroux, 2002, Peterson, 1997, Schönfeld, Reinstaller, 2007, Velthuis, 2003) by considering how a new type of uncertainty can shape the pricing strategies of an art private dealer.====The remainder of the paper is organized as follows. In Section 2, we develop the model of pricing in presence of asymmetric information on both the seller and the buyer side. Section 3 discusses the results and Section 4 concludes.",Price and information disclosure in the private art market: A signalling game,https://www.sciencedirect.com/science/article/pii/S1090944322000023,20 January 2022,2022,Research Article,48.0
"Bhattacharya Sukanta,Dasgupta Aparajita,Mandal Kumarjit,Mukherjee Anirban","Department of Economics, University of Calcutta, 56A B. T. Road, Kolkata, West Bengal, 700050, India,Assistant Professor in Economics, Ashoka University K-32,NDSE Part 2, New Delhi 110049, India","Received 17 October 2021, Accepted 15 December 2021, Available online 24 December 2021, Version of Record 26 April 2022.",https://doi.org/10.1016/j.rie.2021.12.001,Cited by (0),"In this paper we examine the gender matching effects in learning outcomes by studying the sorting behaviour of teachers and students by gender across private and public schools.We develop a theoretical framework behind the selection mechanism process which is grounded in the ==== of systematic gender norms. Using contextual gender norms that are relevant to a developing country context, our theoretical model of matching behaviour predicts that the relative gain in learning outcomes is higher for female students under female teachers. We find support for our theoretical predictions when we test them using Young Lives Survey (YLS) data collected from Andhra Pradesh.","The literature on teacher-student gender matching that studies the effect of a student being matched with a teacher of same sex on the student’s performance largely attributes the positive effect on two possible channels – Pygmalion effect or role model effect. In both these mechanisms, the explanations are driven by cultural beliefs (either the teacher’s belief that a student of his/her same sex can do better or the student’s belief that he/she can be like his/her teacher) which presumably do not vary with other socio-economic parameters. The studies which aim to estimate the gender matching effect therefore, aim for scenarios where students and teachers are matched randomly so that the estimated effect of same-sex matching purely picks up the beliefs described earlier. However, for studies relying on observational data, it is important to understand selection mechanism that is prevalent in sorting of students and teachers across school type, that emanates from stark gender norms in the context of developing countries.====In this paper, we investigate this sorting behaviour in different types of schools and shed light on the potential explanation of the gender matching results.We model a selection mechanism driven by existing gender norms in India. The mechanism ensures that high quality female teachers and students self select themselves to private schools in urban location thereby creating a positive gender matching effect.====Before going into our framework, let us briefly review the existing literature on teacher-student gender matching. The existing literature, as we have mentioned before, are based on two theoretical conjecture – ==== (PE) and ==== (RME). The Pygmalion effect – named after the mythical Greek sculptor Pygmalion who fell in love with a statue he carved – conjectures that if a teacher expects high performance from a student, the student responds by putting up good performance, making it a self fulfilling prophecy. One of the pioneering studies that found support PE hypothesis was done by Rosenthal and Jacobson (1968). After this study was published a series of studies were done to re-examine the Pygmalion effect for students from different age and social strata. Despite the fact that non consensus emerged from these studies, PE remained one of the two most powerful hypothesis explaining gender matching effect (Braun, 1976).The other most popular candidate explanation for gender matching effect is the ==== where a student of certain race or sex idolizes a teacher from the same identity and gets inspired to perform better. The RME, however, is not limited to school performance and works for other decisions such as career choice as well (Almquist, Angrist, 1971, Basow, Howe, 1980).====While the existing empirical papers on teacher-student gender matching cite these two candidate explanations, we have not come across any paper that tests one channel against the other. As both the hypotheses are based on psychological factors, it is difficult to test for either of these using data which collects data on examination grades and other observable socio-economic parameters. Hence, the studies which tests for gender matching effect implicitly assume the existence of either (or both) of the channels and then look for the effect of a student being taught by a teacher of his/her same sex. From this perspective, the major empirical challenge in this literature is to solve the selection problem as students and teachers are not always randomly assigned to a class. Many studies have tried to exploit the longitudinal data structure to solve this problem (Dee, 2007). There is however, no clear consensus on the existence of student-teacher gender matching effects. While some studies such as Dee (2007) found positive gender matching effect for eighth grade students in the United States, some others such as Carrington et al. (2008) found little support for role model effect with eleven year old children enrolled in British schools. Conducting a study conducted from a multilevel perspective, Marsh et al. (2008) also did not find any positive effect of male teachers on male students in Australian schools. While these studies are mostly based on single country, Cho (2012) conducted a cross country study using data from OECD countries and did not find any effect of teacher-student gender matching. Similar results suggesting little or no support for gender interaction are reiterated in studies based in Ohio, United States (Price, 2010), Stockholm, Sweden (Holmlund and Sund, 2008) and Florida, United States (Egalite et al., 2015). Among these studies, Egalite et al. (2015) found mixed results – they found no effect of gender matching for the elementary school students but found some effect of modest magnitude for middle and high school students. In a qualitative study based on classroom observations and individual interviews of 7–8 year old children enrolled in British schools and their teachers, Francis et al. (2008) does not find any support for gender matching effect. While the above mentioned studies, based on developed countries, found little evidence in favour of gender matching, some other papers, mostly focusing on developing countries, find positive gender matching effect. Rawal and Kingdon (2010), for example, examining the role of identity matching on the lines of gender, caste and religion using a data set from India, found positive significant effect of matching along all the dimensions of identity including gender. In another study conducted with a sample of 8th graders in Chile, Paredes (2014) finds a positive effect of a match between female teachers and female students. However, they do not find any negative effect on male students who are matched with female teachers. A similar positive result of gender matching has also been found by Muralidharan and Sheth (2016) in an Indian sample and Lim and Meer (2017) in a Korean sample.====A cursory look at the discussion of the above literature reveals that the existence of gender matching effect can at best be called ambiguous. There is however, a pattern – in most of studies based on developed countries, the evidence of gender matching is weaker than their less developed counterpart. This could be the result of differential gender norms prevailing in developed and less developed countries. It is possible that the selection mechanisms that guide the teachers and students are very different in these two settings, which has implications on the extent of PE and RME in the resulting gender interactions. In our paper, we model such a mechanism based on gender norms prevailing in Indian society and test the implications of the model using a survey data.====In our theory, we argue that there are two types of norms that lead to gender based sorting mechanisms for both teachers and students. For the teachers, there exists an ideal location of residence where they want to stay because of availability of amenities such as good hospitals and good schools for their children. In India, urban centers typically host such amenities. The teachers reside in these places and travel to work places (i.e. schools). The real costs of traveling are higher for female teachers than their male counterpart as the gender norms prevailing in India require the women to take care of the household chores. Even in families where women’s work participation is encouraged, household chores remain the primary responsibility of the women members. Such a situation makes a women teacher prefer a work place near her home more. Therefore, we argue that women prefer nearby private schools than government schools where hiring is done centrally and upon recruitment, candidates may be posted in far away places. A good female teacher with the requisite qualifications therefore favor the nearby private schools with their decentralized hiring practice. From the student’s side, given the norm of patrilocal exogamy, where wives migrate to co-reside with their husbands’ kin, parents know that after their daughters’ marriage, only a fraction of their daughters’ future income will remain in the family. Thus, even when expected labour market return from private schooling is higher, incurring the higher cost of private schooling is justified only for the highest quality girls. The notion that patrilocal exogamy leads to worse educational outcome for women in India is also confirmed by Rammohan and Vu (2018). With these two mechanisms in place, private schools - specifically the urban ones - receive female teachers and female students of high quality making a strong, positive gender interaction effect. We test these theoretical hypotheses using Young Lives Survey data collected from Andhra Pradesh, India.====In our model, the positive effect of gender matching does not stem from any cultural belief about the quality of female teacher or student. Rather, it comes from a sorting mechanism emanating from deep rooted gender norms in the society. In essence, our story is similar to the one discussed by Munshi and Rosenzweig (2006) where an otherwise conservative, traditional gender norms led to a good outcome in which girls opt for English education and white collar jobs. In our study, we show that traditional norms lead to differential sorting behaviour which can shed light on the mechanism behind the gender matching effect. While we do not have any direct policy suggestion, we show that gender matching effect can be a result of a complex selection mechanism. Unlike experimental set up, the assignment of teachers and students in schools follow some selection mechanism and any effective policy for the female teachers or students must internalize the selection mechanism. Our paper, contributes in this area by detailing one such mechanism and providing empirical support for the same.====Besides the specific literature of gender matching effect, the paper is also related to the literature of the effect of teachers’ characteristics on students performance (Clotfelter, Ladd, Vigdor, 2006, Kane, Rockoff, Staiger, 2008, Metzler, Woessmann, 2012, Rockoff, 2004). This is also related to the papers on matching effect of other identity dimension such as race (Diamond, Randolph, Spillane, 2004, Eddy, Easton-Brooks, 2011, Egalite, Kisida, Winters, 2015, Rezai-Rashti, Martino, 2010). The paper is organized as follows – in Section 2 we present the theory and in Section 3, empirical evidence followed by the conclusion.",Identity and learning: A study on the effect of student-teacher gender matching on learning outcomes,https://www.sciencedirect.com/science/article/pii/S1090944321000521,24 December 2021,2021,Research Article,49.0
Banerjee Taposik,"Ambedkar University Delhi, India","Received 13 November 2021, Accepted 21 November 2021, Available online 24 November 2021, Version of Record 26 April 2022.",https://doi.org/10.1016/j.rie.2021.11.002,Cited by (0),The idea of equating the concept of ‘rationality’ with that of the ‘choice of best elements’ unnecessarily limits the sense and scope of ‘rationality’. The existing internal consistency conditions that are popularly used in the ==== theory to assess a choice function are insufficient to analyze several non-standard choice patterns. This paper makes a modest effort to address this limitation. The paper accepts a broader definition of rationality and characterizes choice behaviors where an individual chooses a second best element when available and chooses a best element only when a second best is not available.,"A choice function is essentially a description of choices made from different sets of alternatives in different situations either by an individual or by a collective entity. There is nothing inherent in such a description that would prompt us to believe that the choices made are rational. One needs to examine the choices carefully and determine whether this act of choice could indeed be consistent with the behaviour of a rational entity. In the literature of choice theory we find several axioms==== which are known as internal consistency conditions. These axioms, if satisfied by a choice function, would assure us that the choice behaviour is to some extent consistent; and we may say that such a choice function is rationalizable. Rationalizability of a choice function, that we just mentioned, would ordinarily mean that all the chosen elements are best elements according to some preference relation. In other words, a rationalizable choice function represents choices that could be made by a rational individual.====The idea of such rationality, however, is not unquestionable. Sen in his Presidential address of the Econometric Society in the year 1984 argued that consistency of choice cannot be judged in a context-independent way (Sen, 1993). In his own words, “there is no ‘internal’ way - internal to the choice function itself - of determining whether a particular behavior pattern is or is not consistent. The necessity of bringing in something outside choice behavior is the issue”. The so called internal consistency conditions are not capable of identifying consistent behavior whenever anything other than the best element is chosen. Following Sen’s argument Baigent and Gaertner (1996) came up with characterization of a choice function where the individual ordinarily chooses best elements, but chooses a second best element in case there is a unique best element in the set. Gaertner and Xu, 1999 characterized a choice behavior where the individual picks up the median element from a set of alternatives.====The present paper tries to characterize a choice behavior that follows the following norm: the individual ordinarily chooses a second-best element whenever available; if a second-best element is not available then best elements are picked up. A binary relation according to which such choices are made would be called a second-best element rationalization (alternatively we would use the term ====) of the choice function. The paper is divided into 4 sections. The second section contains definitions and notation. In Section 3 we discuss the characterization of a 2-rationalizable choice function with full domain by a reflexive, connected and acyclic binary relation. The characterization of a 2-rationalizable choice function with full domain by an ordering is discussed in Section 4.",Characterization of a second-best rationalizable choice function with full domain,https://www.sciencedirect.com/science/article/pii/S109094432100051X,24 November 2021,2021,Research Article,50.0
"Dragone Davide,Lambertini Luca,Palestini Arsen","Department of Economics, University of Bologna, Piazza Scaravilli 2, Bologna 40126, Italy,Department of Economics and Alma Climate Centre, University of Bologna, via San Giacomo 3, Bologna 40126, Italy,MEMOTEF, Sapienza University of Rome, Via del Castro Laurenziano 9, Rome 00161, Italy","Received 19 October 2021, Accepted 14 November 2021, Available online 18 November 2021, Version of Record 26 April 2022.",https://doi.org/10.1016/j.rie.2021.11.001,Cited by (1),"We revisit a well known differential ==== game with polluting emissions, to propose a version of the model in which environmental taxation is levied on emissions rather than the environmental damage. This allows to attain strong time consistency under open-loop information, and yields two main results which can be summarized as follows: (i) to attain a fully green technology in steady state, the regulator may equivalently adopt an appropriate tax rate (for any given number of firms) or regulate market access (for any given tax rate); (ii) if the environmental damage depends on emissions only (i.e., not on ==== regulation so as to create the industry structure most favourable to green innovation.","If one takes a quick look at the static models dealing with emission taxation in oligopoly (little matters whether these models include green R&D or not), it appears that usually environmental taxation is levied on per-firm emissions rather than on the resulting (aggregate) environmental damage. The opposite applies instead if one examines the corresponding literature using optimal control or differential game theory.====This poses a problem of consistency between the static and the dynamic approach to modelling the environmental impact of oligopolistic interaction on the environment and the related design of emission taxation. Moreover, judging on the basis of casual observation, the two approaches are not equally realistic. To begin with, although aggregate data on emissions may well be more readily and easily available than individual data at the single firm level, taxing a magnitude defined as the environmental damage amounts to using a quite elusive concept, as the environmental damage imputable to any single industry adds up to the cauldron of a global economic system generating global warming and related effects. Additionally, current rules (for instance, in the EU) require firms to explicitly declare the CO==== -equivalent emission rates of their products (e.g., cars), making these data accessible to the public and the authorities.====In view of these considerations, here we propose a differential Cournot game in which firms are being taxed in proportion to their individual emissions and react to the environmental tax rate by both modifying output levels and investing in R&D for green technologies.==== This setup allows us to obtain several results. The first is that - taxation being linear in each firm’s emission volume - the game at hand exhibits a linear state structure and therefore yields a subgame perfect equilibrium under open-loop information. The second result is that there exists a unique tax rate driving to zero the volume of emissions for any number of firms, or equivalently there exists a unique industry structure attaining the same outcome for any environmental tax rate. The third result is that - if the environmental damage is unaffected by industry output and the tax rate is optimally set - the aggregate R&D effort at the steady state equilibrium is non-monotone in the number of firms and has an inverted-U shape, i.e., there exists a unique industry structure that maximises the collective equilibrium investment in green technologies. The emergence of an analgous inverted-U shaped aggregate R&D curve has been illustrated by Feichtinger et al. (2016) using a differential game in which the public authority regulates market price (or tariff) in combination with an emission standard to which firms react by investing in green technologies over time. A static Cournot model also featuring an inverted-U curve, this time in presence of emission taxation, is in Lambertini et al. (2017). The present paper can be viewed as a properly dynamic representation of the latter model, illustrating additionally the possibility of reconstructing analogous results in a fully analytical way in more realistic setups properly accounting for the dynamics of global warming associated with firms’ and consumers’ intertemporal decisions. Moreover, the ensuing analysis shows that the appearance of a concave and single-peaked aggregate R&D curve is not necessarily associated with any form of price regulation.====The appearance of a concave and single-peaked relationship between innovation and market structure has a clearcut connection with an ongoing discussion in the theory and empirics of the economics of technical progress, which deserves to be illustrated in some more detail before delving into the analysis of our specific setup.====The acquired industrial organization approach to the bearings of market power on the size and pace of technical progress can be traced back to the indirect debate between Schumpeter, 1934, Schumpeter, 1942 and Arrow (1962) on the so-called Schumpeterian hypothesis, which, in a nutshell, says that one should expect to see an inverse relationship between innovation and market power or market structure. Irrespective of the nature of innovation (either for cost reductions or for the introduction of new products), a large theoretical literature attains either Schumpeterian or Arrovian conclusions (for exhaustive accounts, see Reinganum, Schmalensee, 1989, Tirole, 1988).==== That is, partial equilibrium theoretical IO models systematically predict a ==== relationship, in either direction.====The picture drastically changes as soon as one takes instead the standpoint of modern growth theory. In particular, Aghion et al. (2005) stress that empirical evidence shows a ==== relationship between industry concentration (or, the intensity of market competition) and aggregate R&D efforts: this takes the form of an ====, at odds with all existing theoretical IO models; in the same paper, the authors provide a model yielding indeed such a concave result, and fitting the data. A thorough discussion, accompanied by an exhaustive review of the related lively debate, can be found in Aghion, Akcigit, Howitt, 2013, Aghion, Bloom, Blundell, Griffith, Howitt, 2005.====One could say that the inverted-U emerging from data says that Arrow is right for small numbers, while Schumpeter is right thereafter. Alternatively, on the same basis one could also say that neither Arrow nor Schumpeter can match reality, if our interpretation of their respective views is that “competition (resp., monopoly) outperforms monopoly (resp., competition) along the R&D dimension”. Be that as it may, there arises the need of constructing models delivering a non-monotone relationship between some form of R&D (for process, product or environmental-friendly innovations) and the number of firms in the industry. This is particularly true for green R&D in view of its relevance in the framework of the Paris Agreement and, as we shall briefly illustrate at the end of the analysis of the model, calls for a joint design of environmental and competition policies (in particular, towards horizontal mergers).====The remainder of the paper is organised as follows. The setup is illustrated in Section 2. The equilibrium analysis and the main results are laid out in Section 3. Section 4 contains concluding remarks.","Emission taxation, green innovations and inverted-U aggregate R&D efforts in a linear state differential game",https://www.sciencedirect.com/science/article/pii/S1090944321000430,18 November 2021,2021,Research Article,51.0
Mantell Edmund H.,"Professor of Economics and Finance, Lubin School of Business, Pace University, New York, NY, United States","Received 29 August 2021, Accepted 8 October 2021, Available online 14 October 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.rie.2021.10.002,Cited by (0),"This paper analyses the theory of the optimal output decision for a firm whose policy is to post a non-negotiable price for a good or service in a concentrated market where the demand facing the firm is determined, in part, by a random variable. The theoretical findings are the opposite of those in competitive markets; Proposition 1 states that the optimal output of a risk-averse firm is expected to be larger than that of a risk-neutral firm if the expected payoff of its marginal profit is less than or equal to 1. Proposition 2 states that the optimal output of a risk-seeking firm is expected to be smaller than that of a risk-neutral firm if the expected payoff of its marginal profit is greater than 1.",None,The economics of posted prices in a concentrated market where demand is uncertain,https://www.sciencedirect.com/science/article/pii/S1090944321000429,14 October 2021,2021,Research Article,56.0
"Radzvilas Mantas,Karpus Jurgis","Department of Biomedical Sciences and Public Health, Marche Polytechnic University, Via Tronto 10/A, Torrette di Ancona 60126, Italy,Faculty of Philosophy, Philosophy of Science and the Study of Religion, LMU Munich, Geschwister-Scholl-Platz 1, Munich 80539, Germany,Faculty of Psychology and Educational Sciences, LMU Munich, Leopoldstrasse 13, Munich 80802, Germany","Received 10 September 2021, Accepted 29 September 2021, Available online 8 October 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.rie.2021.09.003,Cited by (0),The theory of team reasoning has been developed to resolve a long-lasting niggle in orthodox ,"Game theory is an abstract theoretical framework that provides an internally consistent set of formal principles and mathematical concepts to represent and study the behaviour of interacting agents. Due to its abstract nature and, consequently, extremely general scope, game theory has become an important analytical tool in many disciplines. In its normative applications, game theory is used by economists, mathematicians, psychologists, philosophers and, as of late, computer scientists to study the choices of perfectly rational decision-makers in idealized strategic situations, which may or may not represent real-world social encounters. The descriptive applications of game theory, on the other hand, are aimed at constructing formal models to capture the salient features of real-world social interactions in order to explain and predict people’s (or other interacting agents’) behaviour (Selten, 1988).====Despite its widespread use, the critics question both the normative appeal and the empirical status of game theory.==== Some point to its inability to single out intuitively “obvious” solutions in simple games and its consequent inability to predict people’s behaviour in them. By now a classic example is the Hi-Lo game, in which two players simultaneously and independently from each other choose one from a pair of actions: Hi or Lo. If both choose Hi, they get the payoff of 2 each. If both choose Lo, they get the payoff of 1 each. If one chooses Hi while the other chooses Lo, they both get 0. The game is shown in Fig. 1, where one player chooses between the two actions identified by rows and the other — by columns. The payoffs, in turn, represent the two players’ personal preferences over the possible outcomes of the game.====The standard game theoretic analysis assumes that players’ payoffs are common knowledge and that each player is rational and expresses a common belief in rationality. A rational player is defined as a decision-maker who always chooses a best response — an action that maximises her personal payoff given her probabilistic beliefs about other players’ action choices. For each player in the Hi-Lo game, Hi is a best response if she==== believes that the probability of the other player choosing Hi is at least ====, and Lo if that probability is at most ====. If both players choose actions that turn out to be best responses to each other, then players’ actions constitute a pure strategy Nash equilibrium — a combination of actions which, once the action of the other player is revealed, creates neither player an incentive to unilaterally change her choice. It is therefore an ex post rational solution of the game. The Hi-Lo game has two pure strategy Nash equilibria — ==== and ====.==== However, the standard analysis cannot conclude that rational players will necessarily end up making choices that constitute a Nash equilibrium in the Hi-Lo game: it makes no assumptions about players’ private beliefs about each other’s action choices. Common belief that each player will choose a best response is insufficient for either player to form a belief about the other player’s action choice. Thus, the standard analysis concludes that both Hi and Lo are ex ante rationally permissible choices and, consequently, that every combination of players’ actions in this game is rationalizable.====The critics find this result problematic for both normative and empirical reasons. The Nash equilibrium ==== is unambiguously the best outcome for both players in this game. Therefore, one may expect that many people would single out ==== as the only sensible solution of the Hi-Lo game and expect everyone to choose Hi. Results from experiments support this by revealing that over 90% of the time people opt for Hi.====The theory of team reasoning was developed to account for people’s reasoning and action choices in the Hi-Lo and related games — usually those, in which some outcomes are strictly Pareto superior to others.==== It suggests that certain contextual and/or structural features of games may prompt a player to shift from a mode of reasoning directed at identifying a best response to a mode of reasoning directed at identifying a combination of players’ actions that attains an outcome that best promotes their common interests, i.e., shared preferences, in a game.====Common interests of the interacting players can be operationalized in a number of ways, and we will discuss some of them later. It is, however, obvious that the outcome that best promotes the interacting players’ common interests in the Hi-Lo game is the Nash equilibrium ====. Consider, for example, a very simple understanding of players’ common interests from an individual player’s standpoint: “it is our common interest to attain an outcome that yields each of us the highest possible personal payoff”. A player who adopts the team mode of reasoning with this idea of common interests in mind will identify the outcome ==== as the unique solution of the game. The proponents of the theory of team reasoning suggest that this is exactly what happens when a player thinks about what she should do in the decision problem that she faces in this game.====Despite its intuitive appeal, the theory of team reasoning has so far received little attention from mainstream economists and game theorists. We identify two key reasons why that is so. First, team reasoning is usually construed as involving a shift from the “I” to the “We” mode of reasoning in a decision-maker’s deliberation about what to do. This shift is said to be associated with some form of transformation of the decision-maker’s agency. The exact nature of this transformation, however, is often left ambiguous. Under some interpretations of the theory, the transformation appears to necessitate the abandonment of ontological individualism — a fundamental assumption of mainstream economic theory that all social facts, including facts about groups, are exhaustively determined by facts about individuals.==== Under other interpretations of the theory, whether the transformation implies the abandonment of ontological individualism is, at best, unclear. This is problematic, since, if the theory abandons ontological individualism, it is unlikely that it will ever be accepted as a viable development of the theory of rational choice in games that remains to be a part of the broader edifice of neoclassical economics.====The second reason for being sceptical about the theory, is that it sometimes prescribes to the interacting decision-makers action choices that seemingly go against the satisfaction of their personal preferences over the possible outcomes of a game. This raises questions concerning the normative appeal of the theory of team reasoning in games: how can it be rational to choose an action that leads to the attainment of an outcome that is not the one that the decision-maker personally prefers the most? While addressing this second issue is crucial to determine the theory’s appeal from the normative point of view, its descriptive appeal in explaining and predicting decision-makers’ choices can nevertheless be accepted so long as the theory does not abandon ontological individualism in achieving its explanatory success. Therefore, our primary goal here is to address and alleviate the first of the two worries.====As a player switches from best response reasoning to team reasoning when she thinks about what she should do in a game, her understanding of what kind of agent she is — an independent individual player or a part of a collective — is seen to undergo a substantial change. This, in turn, prompted debates on how to account for shared or collective beliefs and intentions that a player comes to form when she engages in team reasoning (Hakli et al., 2010). Considering these debates, it is not unjustified for one to worry that the agency transformation in question might be a process that essentially turns the interacting players into a collective agent with a set of characteristics that cannot be accounted for by the characteristics of the individual players.==== We believe that this is one of the reasons why, for example, Ross (2019) distinguishes between “real” and “pseudo” game theory in his recent review of the theory of team reasoning.====In this paper, we will argue that the worry that the theory of team reasoning inevitably relies on the existence of collective agency is unnecessary. First we will argue that a player’s engagement in team reasoning does not necessitate a form of agency transformation that is not compatible with ontological individualism. We will then show that the core principles of the theory of team reasoning predate the development of the theory itself and are in fact implicitly assumed in some branches of orthodox game theory that are not considered problematic. Lastly, we will argue against the methodological approach that construes team reasoning as involving payoff transformations that represent the interacting players’ modified personal interests concerning the possible outcomes of games.",Team reasoning without a hive mind,https://www.sciencedirect.com/science/article/pii/S1090944321000405,8 October 2021,2021,Research Article,57.0
"Mercer Antonio Carlos,Póvoa Angela Cristiane Santos,Pech Wesley","Department of Business Administration Program, Pontifical Catholic University of Parana, Curitiba, PR, Brazil,Department of Economics, Finance and Marketing, Tennessee Technological University, Cookeville, United States","Received 3 August 2021, Accepted 23 September 2021, Available online 25 September 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.rie.2021.09.001,Cited by (0),"This paper experimentally investigated luck framing. Specifically, we analyzed the difference between being assigned an advantageous role in a distribution game and being assigned the same role while being explicitly told that you were lucky to be in that favorable position. We tested this difference by implementing a dictator game and a no-veto-cost ultimatum game. We observed that: a) dictators transferred larger amounts in the game when they were explicitly told they were lucky to be the dictator compared to dictators who did not receive this message, and b) responders in the no-veto-cost ultimatum game who were explicitly told they were lucky to be in that role were significantly less likely to reject a particular offer compared to responders in the game who did not receive this message. The combination of these results is consistent with the hypothesis that people are more likely to behave in a more prosocial and egalitarian manner when they are reminded that they are lucky to be in a particular position.","Random assignment is a standard methodological practice in experimental studies. Researchers apply it to ensure that different groups can be compared statistically without further addressing the most basic forms of endogeneity. However, in some decision-making experiments with random assignment, it is common for subjects to prefer to be in one particular experimental group than another, either because they are more likely to earn more money or because they can more easily achieve a superior non-monetary outcome. In these cases, randomly assigning subjects to different groups is equivalent to determining which subjects will be 'lucky' and which ones will not. In experimental economics, for example, subjects' roles in interactions such as the ultimatum game or the dictator game are usually randomly assigned by the experimenter (Weimann and Brosig-Koch, 2019); in the ultimatum game, subjects are segregated into Proposers and Responders, and in the Dictator Game they are divided into Dictators and Receivers. In both cases, subjects have a preference for playing the role of the first-mover. In general, subjects who receive this ""first-mover advantage"" in the game tend to demand a bigger share for themselves – in the dictator game, for example, the average amount sent to receivers is only about 20-25% of the initial endowment (Camerer, 2003; Forsythe et al., 1994). Although it might be harder in the case of the ultimatum game to determine which role carries the upper hand (since a rejection by the responder results in zero payoffs for both players) – the fact that offers closer to 50% of the endowment are rarely rejected, proposers tend to offer on average between 30% and 50% of the initial endowment, which causes them to leave the experiment with a larger share of the pie (Güth and Kocher, 2014). Therefore, it is rare for a responder in the Ultimatum Game or a receiver in the Dictator Game to end up with a larger monetary amount than first-movers in these two games. Because of these results and subjects' preferences, it can be argued that subjects who were selected to be the first-movers in these games were luckier than the ones assigned to be second-movers. In fact, when there is competition to decide who is going to be the first-movers, subjects tend to exert a significant amount of effort to get the chance to play that role in these two games (Hoffman et al., 1994; List and Cherry, 2000). Given the role of luck in these types of social interactions, our study investigated the following question: Do subjects behave differently depending on whether the role of luck is explicitly mentioned to them or not?====In our experimental design, we used framing effects to manipulate the perception of luck (Levin et al., 1998; Tversky and Kahneman, 1981). A large body of evidence shows that contextual changes (i.e., loaded or framed instructions) in economic experiments directly affect subjects' other-regarding preferences even if they do not provide additional information to subjects (Alekseev et al., 2017). For example, individuals tend to invest significantly more in a public goods game under the ""give"" frame compared to the ""take"" frame (Dufwenberg et al., 2011). We compared the outcomes between a situation in which a subjects' role was explicitly mentioned to the subject as being lucky and one in which there was no mention of this luck. However, the role of luck was obviously implicit in the experiment's instructions. In addition to using loaded instructions, we also used a coin-flip task to emphasize the randomness in role assignment. We conducted two experiments: one using the standard dictator game and another implementing a modified version of the ultimatum game. We found that subjects behave in a more prosocial manner, i.e., they made larger transfers in the dictator game and rejected fewer offers in the ultimatum game when the role of luck was explicitly mentioned to them.",The effect of luck framing on distributional preferences,https://www.sciencedirect.com/science/article/pii/S1090944321000387,25 September 2021,2021,Research Article,58.0
"Ball Christopher,French Jack","Quinnipiac University, 275 Mount Carmel Ave., Hamden, CT 06518 USA","Received 22 June 2021, Accepted 23 September 2021, Available online 25 September 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.rie.2021.09.002,Cited by (0),"This paper explores the stock market-GDP relationship from basic theory to simple empirics to better understand what stock market movements tell us about underlying GDP in real time. We present a simple theoretical model to make key relationships clear, then explore US GDP and US stock market (S&P 500) performance through a range of analytical tools from visual inspection to correlations, regressions, counting and extreme value calculations to a few illustrative narrative investigations. We find that the S&P 500 is weakly correlated with real GDP as well as with vintage GDP releases contemporaneous, but more strongly and statistically significantly with one lag as theory predicts. We also find that the S&P 500 is more closely related both contemporaneously and with a lag to final, revised GDP numbers - only known months later - than to vintage GDP estimates, suggesting that stock market trends are informative about true GDP.","Every day one sees headlines linking stock market rallies to GDP (Chen, Ibert, Vazquez-Grande, 2020, Serwer, Zahn, 2020). Basic economic theory suggests stock prices should indeed reflect underlying economic conditions, but to what extent and how closely is less clear. Most macroeconomic models specialize in focusing on specific aspects of financial markets like equity premia or other puzzles, add them as financial intermediaries matching savers and borrowers in aggregate or ignore them altogether. The finance literature is often more interested in aspects of GDP that might forecast stocks and stock market returns. Economists ought, however, to be able to explain with some confidence how stock markets are related to GDP in both theory and practice.====While not every up and down in the stock market signals a change in GDP, there are some casual observational reasons for thinking that a persistent rise or fall tells us something. Fig. 1 contains nearly all the elements we explore. To start, it seems clear that the S&P500====s movement (blue line) leads movements in real GDP (red line). Around March 2020 there is a decline prior to the first big GDP drop (April 2020, red line) and a rise between March and April 2020 prior to the subsequent GDP recovery (July 2020, red line).====The real question we have in mind, however, is when individuals in this economy knew what happened to GDP. Not only are US GDP estimates quarterly, but Real GDP is a calculated aggregate measure released quarterly in the United States in an admittedly “draft” or “vintage” form and then updated in subsequent quarters until we are relatively confident that it is accurately reflecting “true” GDP. In stable times those revisions converge on “truth” relatively quickly with small adjustments but in unstable times vintage and later revisions can diverge more and potentially take longer to revise accurately.====Conceptually then, the question of what stock markets as information aggregators tell us about GDP in real time is a natural one. This paper takes a first look. We review the major indices in the United States and look at their relationships to each other and to GDP over time. We then select a single index, the S&P 500, and explore how closely related to GDP it is. We view this as a first step in a longer research agenda that explores this relationship.",Exploring what stock markets tell us about GDP in theory and practice,https://www.sciencedirect.com/science/article/pii/S1090944321000399,25 September 2021,2021,Research Article,59.0
"Alfaro Martin,Lander David","University of Alberta, Department of Economics, 9-08 HM Tory Building, Edmonton, AB T6G 2H4, Canada,Peking University HSBC Business School, Shenzhen, 518055, China","Received 26 July 2021, Accepted 4 August 2021, Available online 12 August 2021, Version of Record 26 November 2021.",https://doi.org/10.1016/j.rie.2021.08.001,Cited by (1),"This paper studies a strategic-investment model under endogenous entry of followers. We extend the standard setting à la Etro by incorporating multiple heterogeneous leaders and demand-enhancing investments directly affecting competition. Our findings indicate that all leaders simultaneously restrict entry without harming each other. Moreover, while entry accommodation never arises, a wide range of strategies is consistent with aggressive behavior, including quality upgrades exclusively targeted to high-valuation consumers. By using the tools of aggregative games, we provide conditions over demand primitives to identify when a leader over-invests and whether it (i) raises or lowers its price, (ii) increases or decreases its revenue, and (iii) supplies greater or lower quantities. We illustrate the results by completely characterizing outcomes under the Logit and CES demands.","Scenarios where leading and following firms compete constitute one of the core topics in Industrial Organization. Broadly speaking, they can be separated into two approaches: Stackelberg-type models, where a leader directly commits to some price/quantity, and models of strategic investments, where a leader affects the nature of market competition through investments.====While these approaches have traditionally focused on cases with only one potential entrant, recent studies have analyzed the leader’s behavior in industries with free entry of followers (e.g., Etro, 2006, Etro, 2008, Anderson, Erkal, Piccinin, 2020). Their main conclusion is that accommodating entry is never optimal, since any attempt to soften competition is undermined by entry of additional followers. Consequently, the leader always behaves aggressively with the goal of limiting entry.====The scenarios considered in those papers are based on several simplifying assumptions. While they make it possible to clearly contrast outcomes relative to setups with one follower, they simultaneously limit the scope of industries that can be captured. In particular, there are two assumptions worth mentioning.====First, these studies consider industries with several followers, but only one leader. Yet there are numerous examples of industries with multiple leaders, such as Coca-Cola and PepsiCo in carbonated beverages, Adidas and Nike in sports apparel, or McDonald’s and Burger King in the fast-food industry. The existence of only one leader determines that restricting entry is always beneficial for this firm. However, the existence of several leaders behaving more aggressively could result in a Pareto-dominated situation, where each leader ends up with lower profits. This is in fact the conclusion obtained by Gilbert and Vives (1986) under a homogeneous-good industry with one potential entrant.====Second, these studies model investments as only impacting a firm’s own profits. While this is reasonable as a first approximation to model cost-reducing investments, it becomes narrow in scope for demand-enhancing investments. It determines that a firm cannot directly affect competition (and hence the rivals’ profits) by improving features of its good. Thus, it leaves out industries where competition occurs primarily through innovation, such as the cell phone industry with Apple and Samsung. These firms create a tougher competitive environment by constantly launching overhauled versions of their cell phones, even when their prices barely change.====This second point has strong implications for the outcomes that can be captured. In particular, it establishes that aggressive pricing is the only possible strategy to limit entry, which basically turns investments into commitment devices to reduce a firm’s own price. The fact that the same strategy is predicted irrespective of the investment considered could result in counterintuitive outcomes for some industries and types of investments. For instance, it determines that a leader always downgrades “quality” under demand-enhancing investments that increase a consumer’s willingness to pay, as Etro (2006) concludes.====In this paper, we revisit models of strategic investments under free entry of followers. Our framework departs from extant models in two respects. First, we focus on demand-enhancing investments that strengthen competition, and so directly affect the demand of ==== firms. Intuitively, this captures that firms compete for the same set of customers, which can be attracted by lowering prices or improving non-price features of goods. In addition, we account for multiple leaders. Moreover, we allow them to be heterogeneous to capture that each possibly has an asymmetric influence in industry conditions. Thus, for instance, we can accommodate cases like Coca-Cola and PepsiCo, where the former has traditionally been the firm with the greatest influence in the carbonated beverages industry.====Our results indicate that each leader strategically uses its investment to restrict entry of the least-profitable firms, without having other leaders as a target. Moreover, while the deployment of this strategy requires increasing competition, leaders do not inflict mutual damage; only potential entrants are affected. Consequently, leaders do not end up trapped in a Pareto-inferior situation and each earns greater profits.====We also show that strengthening competition can be achieved via a wide array of strategies, with radically different implications for market outcomes. Given the richness of possibilities, we identify conditions in terms of model primitives to characterize outcomes. Depending on the features of the industry analyzed, we show that a leader could downgrade quality and engage in aggressive pricing as in the standard endogenous-entry models. In other cases, a leader could restrict entry by upgrading quality and charging higher prices. In fact, we show that aggressive behavior is consistent with overhauling varieties to exclusively attract the highest-valuation consumers and charge them high prices. This could reduce total quantities and revenues of a leader; however, by making it more difficult for followers to attract the most lucrative consumers, market profitability reduces and entry is effectively limited.====In Section 3, we begin the analysis by describing the model setup. Our framework considers an industry with a horizontally differentiated good, where multiple heterogeneous leaders compete with an unbounded pool of followers governed by free-entry rules. Joint with prices, each firm makes investment decisions that enhance their own demand and involve sunk fixed costs, as in Sutton, 1991, Sutton, 1998.==== These investments could either increase or decrease the price elasticity of demand, and hence lower or raise a firm’s price.==== When in particular they increase prices, we refer to them as investments in “quality.”====As for demand, we suppose it depends on a firm’s own price and investments, along with a real-valued function that defines the competitive environment. Such a function depends on all firms’ prices and investments, reflecting that a firm can strengthen competition by either lowering its price or overhauling its good. Demands with such a functional form encompass augmented versions of the three most common cases utilized in the literature: the CES, Logit, and linear demand. Additionally, it covers other standard cases such as the translog and demands derived from an additively separable indirect utility. At a formal level, it determines that the game is aggregative.====In Section 4, we isolate the leaders’ strategic motives to invest by utilizing the standard two-stage approach by Fudenberg, Tirole, 1984, Fudenberg, Tirole, 1991. This requires comparing the outcomes between a simultaneous- and sequential-move game. In the latter, leaders make investments choices prior to the followers’ entry choices and the market stage. As for the simultaneous-move games, it constitutes a non-strategic benchmark where investments decisions are unobserved, and hence cannot be used strategically.====In Section 5, we establish the main results emerging from the comparison. First, we show each leader always chooses its investment to strengthen competition and limit entry. Furthermore, even when they all behave more aggressively simultaneously, leaders do not inflict mutual damage and each garners greater profits. The results hold irrespective of whether prices are strategic substitutes or complements, and independently of the nature of demand-enhancing investments (i.e., price decreasing or increasing). Moreover, they could entail deploying an under- or over-investment strategy relative to the simultaneous-move game, with different implications regarding prices, revenues, and quantities. Nonetheless, regardless of the situation considered, accommodating entry is never optimal by a similar reason as in Etro, 2006, Etro, 2008: positive profits of followers induce additional entry, thereby undermining any attempt to weaken competition.====Second, we delve into the implications of strategic behavior for market outcomes. Unlike the standard endogenous-entry models, the strategy deployed by each leader is now setup-specific and can result in different market outcomes. Thus, a leader could under- or over-invest to increase competition since investing triggers two effects on the competitive environment. First, it directly toughens competition by making a firm’s variety more appealing. Second, it concurrently affects a firm’s incentive to choose its price. Thus, depending on whether the firm is induced to decrease or increase its price, this effect could make competition tougher or softer.====In particular, over-investing always arises when demand-enhancing investments decrease prices, since in that case both the direct and indirect effects work in the same direction. Additionally, it also takes place when investments do not raise a leader’s price to such an extent that it decreases competition; remarkably, this outcome arises even under pronounced price effects that could end up reducing both the quantity and revenue of a leader.====Etro (2006) characterizes the results when a leader under-invests. This strategy generates similar outcomes in our framework, and therefore we focus on the implications of the over-investment case in Section 6. In contrast to the under-investment case, which always predicts aggressive pricing, over-investing is compatible with a multiplicity of outcomes. Due to this, we provide conditions in terms of demand primitives to identify when a leader over-invests and whether it (i) raises or lowers its price, (ii) increases or decreases its revenue, and (iii) supplies greater or lower quantities.====The results enable us to identify specific types of strategic behavior. In addition to aggressive pricing, two strategies to restrict entry are worth noting. The first involves a leader upgrading quality and charging higher prices, while simultaneously increasing its quantities sold and revenues. The second entails a pronounced upgrade in quality targeted to the group of consumers with the greatest willingness to pay. This provides a leader with incentives to substantially increase its price, thereby reducing its total quantities and revenues. Yet, this strategy would effectively reduce the market’s profitability and hence restrict entry, by making it more difficult for followers to attract the most lucrative consumers.====Finally, in Section 7, we apply our characterization of results. This aims to answer the question: what kind of strategic behavior are we predicting under standard demands? We illustrate this by analyzing two of the most common demands in applied work: quality-augmented variants of the Logit and CES. Under these demands, each leader always over-invests to limit entry, sets higher prices, and increases its sales. Moreover, each leader increases its quantities under the Logit demand, whereas a leader does it under the CES when its market share is not disproportionately large.====Also, while the multiplicity of outcomes in the general case precludes any conclusion regarding welfare, the Logit and CES have specific implications on this matter. They can be easily derived using the results of Anderson et al. (2020), given the particular way in which non-price features are embedded in these demands. Specifically, we show that consumer surplus does not vary since the opposing effects impacting it completely offset each other; on the contrary, total industry profits increase. Consequently, consumers are always better off when profits are passed back to them.",Restricting entry without aggressive pricing,https://www.sciencedirect.com/science/article/pii/S1090944321000314,12 August 2021,2021,Research Article,60.0
"Bo Hao,Galiani Sebastian","University of Maryland,University of Maryland and NBER","Received 12 June 2021, Accepted 15 June 2021, Available online 2 July 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.rie.2021.06.005,Cited by (3),"In designing any causal study, steps must be taken to address both internal and external threats to its validity. Researchers tend to focus primarily on dealing with threats to internal validity. However, once they have conducted an internally valid analysis, that analysis yields an established set of findings for the specific case in question. As for the future usefulness of that result, however, what matters is its degree of external validity. In this paper we provide a formal, general exploration of the question of external validity and propose a simple and generally applicable method for evaluating the external validity of randomized controlled trials. Although our method applies only to RCTs, the issue of external validity is general and not restricted to RCTs, as shown in our formal analysis.","In designing any causal study, steps must be taken to address both internal and external threats to its validity (see Campbell, 1957, and Cook and Campbell, 1979). Researchers tend to focus primarily on threats to internal validity, i.e., determining whether it is valid to infer that, within the context of a particular study, the differences in the dependent variables are caused by the differences in the relevant explanatory variables. External validity, on the other hand, concerns the extent to which a causal relationship holds over variations in persons, settings, and time. It is important to underscore the fact, however, at the outset that external validity does not extend to modifications in the treatment, although in practice, researchers often try to generalize their results by conflating the two levels of generalization into a question of external validity.====Randomized controlled trials solve the problem of selection bias in the identification of causal effects. Thus, theoretically, cause-effect constructs identified by means of randomized controlled trials are internally valid, that is, they permit the identification of causal effects for the population from which the random sample used in the estimation was drawn. The outcomes of such experiments are interesting in their own right, but researchers sometimes explicitly assume external validity (EV), i.e., that the internally valid estimates obtained for one population can be extrapolated to other populations. In fact, it is not uncommon that after researchers have established a cause-and-effect relationship in a specific population, they proceed to discuss its implications based on the assumption that this relationship is generally valid. In this paper, we formalize the concept of external validity and show that in general, it is unlikely that any given study will be externally valid in any general sense. This is one reason why Manski (2013) says that the current practice of policy analysis “hides uncertainty”.====Once researchers have conducted an internally valid analysis, that analysis yields an established set of findings for the specific case in question. As for the future usefulness of that result, however, what matters is its degree of EV. The most commonly held view in this regard is that the EV problem hinges on assumptions about the relationship between the population for which internally valid estimates have been obtained and another, different population. Apart from researchers who are focusing on EV in a specific context, many researches either ignore the EV problem altogether or approach it subjectively. In this paper, we provide a formal and general reflection on the EV problem and propose a simple and generally applicable method for evaluating the external validity of randomly controlled trials (RCTs).====In this paper we define external validity as the stability of the conditional distribution p(outcomes | treatment) across different populations. We then formalize the degree to which we can make judgments about a new population (density) generated as a subpopulation from an overarching population that also generates the “original” population for which there is an internally valid estimate. Without loss of generality, assume that we have data that allows estimation of the joint distribution p(outcomes, treatment). We then have p(outcomes, treatment) = p(outcomes | treatment) ==== p(treatment). We say that there is external validity if, for other data with a potentially different joint distribution of outcomes and treatment, the conditional distribution p(outcomes | treatment) stays the same.====Our definition of external validity is similar to that of Janzing, Peters, and Schölkopf (2017). Admittedly, this seems quite stringent. It might be thought that, even with a moderate change of p(outcome | treatment) across different populations, external validity could be maintained. But what exact degree of change in p(outcome | treatment) leads to EV or external invalidity cannot be precisely defined. We need an operationalizable definition of EV, so, in line with a small body of literature (Janzing, Peters, and Schölkopf, 2017), we err on the side of caution, although we admit that there are other ways of defining EV that provide interesting and important insights, e.g., Meager (2019).====Based on our theoretical framework, we then propose two alternative measures of external validity. To the best of our knowledge, we are the first to propose formal mathematical definitions of external validity and, on that basis and in the context of an RCT, to propose purely data-driven measures related to theoretical constructs. The measures of EV we propose in this paper can take advantage of multiple trials to evaluate the degree to which certain empirical conclusions are valid across different populations. Needless to say, ultimately, the external validity of all causal estimates is established by replication in other datasets (Angrist, 2004).====We would like to determine whether a given study or a given set of studies can be generalized to other populations in general.==== In order to do that, we propose a method that applies to RCTs, but it should be noted that the issue of external validity is general and not restricted to RCTs, as shown in our formal and general reflection below.====The rest of this paper is structured as follows. In Section 2, we provide a formal and general reflection on the EV problem. Based on the model described in that section, in Section 3 we propose a simple and generally applicable method for assessing the external validity of RCTs. Finally, we present final remarks.",Assessing external validity,https://www.sciencedirect.com/science/article/pii/S1090944321000296,2 July 2021,2021,Research Article,61.0
"Liu Han,Ackert Lucy F.,Chang Fang,Qi Li,Shi Yaojiang","Shaanxi Normal University, Center for Experimental Economics in Education, 620 West Chang'an Ave, Xi'an, Shaanxi, China, (86) 158-2900-9858,Department of Economics and Finance, Michael J. Coles College of Business, Kennesaw State University, 1000 Chastain Road, Kennesaw, Georgia 30144, (470) 578-6111,Shaanxi Normal University, Center for Experimental Economics in Education, 620 West Chang'an Ave, Xi'an, Shaanxi, China, (86)-134-8495-0704,Department of Economics and Business Management, Agnes Scott College, 141 E. College Avenue, Decatur, Georgia 30030, (404) 471-6556,Shaanxi Normal University, Center for Experimental Economics in Education, 620 West Chang'an Ave, Xi'an, Shaanxi, China, (86)-138-9283-3777","Received 21 May 2021, Accepted 15 June 2021, Available online 23 June 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.rie.2021.06.004,Cited by (2),"Social capital promotes cooperation between people and, in turn, economic growth and stability. Trust and trustworthiness are components of social capital that are associated with economic success. This paper provides insight into the impact of social division on cooperative behavior. We use the one-shot investment game to measure trust and reciprocity among inmates in a Chinese prison, which offers an institutional setting that allows us to examine how social interaction, or a lack thereof, fosters cooperation. Results show that the variation in social division through physical separation does not have a significant impact on cooperative behavior among inmates. However, inmates are more trusting than our benchmark group of university students even though inmates have faced significant life challenges. While social interaction fails to boost trust and reciprocity, childhood experience and family environment mold ====. In particular, reciprocity deteriorates for those who have migrant mothers.","Social capital increases human capital and, in turn, the growth and stability of a society (Gouldner, 1960; Coleman, 1988). According to Fukuyama (2001), social capital serves as an informal norm that promotes cooperation between people. While social capital takes many forms, trust and reciprocity are particularly important manifestations for policymakers who search for ways to promote economic progress. People who trust and prove to be worthy of that trust are valuable in business relationships because transactions costs are lower.====Social norms, including trust and reciprocity, vary across and within cultures depending on an individual's status within the society (Gouldner, 1960). High status people are viewed as more trustworthy. Sociologists argue that people view in-group members as more cooperative, whereas those in out-groups are viewed with suspicion (e.g., Campbell, 1967). Consistent with this view, Glaeser, Laibson, and Scheinkman (2000) report that trust and trustworthiness increase when people are socially closer. Social interactions lead to trusting behavior and serve as a learning channel for individuals to assess others’ trustworthiness. But, trust and reciprocity are multifaceted constructs, which is a challenge to our ability to isolate their underlying determinants. A large literature in economics, psychology, and sociology suggests that individual preferences are impacted by a multitude of factors, including the social environment and life experience (Malmendier and Nagel, 2011).====The goal of this paper is to study the impact of social division, or lack thereof, on cooperative behavior. Typically it is difficult for a researcher to control the social interactions among experimental participants in traditional lab or field settings. We examine trust and reciprocity with citizens who have been convicted of crime and were incarcerated in a Chinese prison. Due to the forced physical separation of inmates into groups by prison administrators, our design offers an institutional setting that allows us to examine the impact of various degrees of externally imposed social division on pro-social behavior.==== We also recruited a group of Chinese university students to provide a baseline for comparison as students are commonly the subject pool whose preferences are elicited in studies of social behavior (e.g., Berg, Dickhaut, and McCabe, 1995). In addition, we collect data on subjects’ risk preference, education, and social and family history to further understand the factors that mold social preferences.====Because of the physical separation of inmates into buildings and, within the buildings, cellblocks, we can examine the level of cooperation among these groups with externally imposed separation where social interaction ranges from none to complete. Prior research documents that in-group relationships are based on stereotypes of cooperativeness and lead to loyalty and favoritism, whereas out-group members are viewed as less honest and are, thus, untrustworthy (Campbell, 1967; Chen and Li, 2009). For example, recent research with an inmate group reports that inmates discriminate against individuals outside the prison system (Balafoutas, Garcia-Gallego, Georgantzis, Jaber-Lopez, and Mitrokostas, 2020). Importantly, psychologists report that interaction with people from other groups can effectively reduce intergroup bias (Gaertner, Dovidio, Anastasio, Bachman, and Rust, 1993).====Cooperative behavior across groups within a community promotes social capital, with trust being an important ingredient for economic success (Glaeser, Laibson, and Scheinkman, 2000). Interestingly, Falk and Zehnder (2007) report that individuals anticipate the trustworthiness of people in other groups and discriminate in accordance with their expectations. However, they also point out that research on trust and reciprocity across groups in other environments is incomplete. Individuals currently serving time in prison might seem to be a group likely to lack the trust of others. After all, they have been convicted of serious crimes. Much research documents the importance of trust in promoting positive societal outcomes, like respect and obedience to laws, efficacy of law enforcement officers, and growth of social capital (Flexon, Lurigio, and Greenleaf, 2009; Boateng, 2016; Intravia, Stewart, Warren, and Wolf, 2016; Lowrey, Maguire, and Bennett, 2016). Perhaps surprisingly, a recent study finds that in interactions with individuals from both inmate and general populations, convicted criminals were as prosocially motivated as non-criminal citizens (Birkeland, Cappelen, Sorensen, and Tungodden, 2014). Other recent research, though, reports that offenders under community supervision were less trusting and less trustworthy compared to people from the general population (Clark, Thorne, Hendricks, Sharp, Clark, and Cropsey, 2015). At the same time, these authors report that the offenders developed behavior consistent with trust and reciprocity once they learned they were in an environment in which cooperative behavior was rewarded. These results suggest that while a hostile environment discourages cooperation, prosocial behavior can be observed in other settings. This is further supported by the work of Trussell (2018) who studied former child soldiers in Liberia. Former child soldiers survived great tragedy but were as trusting as citizens who were not members of warring factions. To our knowledge, Guo, Liang, and Xiao (2020) is the only other study of social preferences among Chinese inmates.==== Though they note that some expect stigmatized groups to show in-group derogation, inmates in their study were willing to work harder if the benefit went to another inmate.====We examine trust and reciprocity using the one-shot investment game first proposed by Berg, Dickhaut, and McCabe (1995) and summarized as follows. Two players are anonymously paired and both are endowed with cash. The first player decides how much of an endowment to send to the second player, with the amount sent increased by a multiplier. The second player then chooses how much of the amount received (after the multiplier is applied) to return to the first player. The sub-game perfect equilibrium is for the first player to send nothing, but the experimental findings are not consistent with the prediction. Typically, positive amounts are sent and returned (e.g., Ackert, Church, Davis, 2011).====In this paper, we first consider the role of social division on trust and reciprocity among inmates. Interestingly, we find that the social division has no significant effect on trust or reciprocity and, even though they have lower social status, inmates are not less pro-social than students. As noted above, Trussell (2018) reports that former child soldiers who experienced significant personal tragedy trust as much as average citizens. In addition, Cadsby, Song, and Yang (2020) report that social preferences are similar across rural and urban Chinese children. However, while our results confirm that social division and social status do not predict pro-social behavior, our survey data shows that early life experience does (for both inmates and students). In particular, our subjects whose mother is absent due to migration for employment reasons are significantly less reciprocal. Other researchers also find that life experiences are significant in forming negative social preferences. For example, Kesternich, Smith, Winter, and Hörl (2018) report that children who experience hunger in wartime are less trusting as adults. Furthermore, divorce and parental absence may discourage trusting behavior as children grow into adulthood (King, 2004). In China, parental migration has increasingly received attention and left-behind children are of particular societal concern (e.g., Sudworth, 2016; Zhang, 2018). Children who are left behind are believed to suffer cognitively and experience poor life outcomes (Yu, Dai, Li, Wang, and Li, 2014; Murphy, Zhou, and Tao, 2016; Chang, Jiang, Loyalka, Chu, Shi, Osborn, and Rozelle, 2019; Liu, Chang, Corn, Zhang, and Shi, 2021). Our results support the view that concern among Chinese policymakers is warranted. While some consider the massive rural to urban migration a sign of economic progress, it is not without social cost.====The remainder of the paper is organized as follows. Section 2 reviews the research method, including the experimental design, procedures, and subject pools. Section 3 presents comparisons of trust and reciprocity across social division groups. Section 4 delves into the life experiences associated with greater trust and reciprocity. Section 5 summarizes our study and provides direction for future research.","Social division, trust, and reciprocity among Chinese inmates",https://www.sciencedirect.com/science/article/pii/S1090944321000284,23 June 2021,2021,Research Article,62.0
Milovanska-Farrington Stefani,"The University of Tampa, Department of Economics, and IZA Institute of Labor Economics, 401 W Kennedy Blvd, Sykes College of Business, , Tampa, FL 33606, United states","Received 15 April 2021, Accepted 17 June 2021, Available online 22 June 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.rie.2021.06.006,Cited by (5),"This article explores the impact of grandparents’ supervision time input relative to the effect of parents’ childcare provision on children's cognitive, social and behavioral development at an early age. We identify the effects of interest through panel data estimation methods. The findings provide evidence of complementarity between parental and grandparental involvement in the child-rearing process. Specifically, grandparental care has a stronger effect than parental intervention on the vocabulary skills of the child. However, parents’ time input in the child has a larger impact than does the supervision time investment of ==== on the socio-behavioral development and the picture similarities measure of cognitive ability of children between 3 and 6 years old.","Cognitive, social and behavioral development at an early age has an effect on later educational attainment, health, behavioral and socioeconomic outcomes of children. These different aspects of the maturity process can be influenced by all caregivers who supervise the child. Therefore, it is interesting to explore the relative importance of grandparents, other childcare providers and parents in the enhancement of early childhood outcomes for the following reasons. Grandparents can influence the early development of children through intergenerational transfer of experience, wisdom, knowledge and skills. They can help parents in the child-rearing process and might have more time, vigor and willingness to spend quality in addition to supervision time with the child. Grandparents can also directly affect early educational attainment of the child through helping him/ her learn letters and numbers, do homework, and develop practical skills which the child is likely to use later in life. However, this does not necessarily imply that grandparental care is sufficient for adequate child development. This paper addresses the question of whether grandparents and parents can be thought of as substitutes or complements in the development process of children, and quantifies the relative effect of parental and grandparental supervision time on child outcomes.====Previous literature focused on either the impact of grandparental provision of child care on grandparents, or the effect of grandparental resources, mainly material and financial, on the educational outcome of the grandchild. However, existing articles on the effect of downward transfers (i.e., transfers from grandparents to children) on child behavior and educational achievement are limited and inconclusive. We contribute to the existing literature by exploring the importance of raising grandchildren on their early-age development. We extend a previously developed model of skills and knowledge accumulation to take into account the supervision time investment in the child not only by parents but also by grandparents. We employ Scottish data in FE panel data regression analysis and seemingly unrelated regressions (SUR) in order to identify the effect of the number of hours of childcare provision by grandparents relative to that of parents on three measures of child development at an early age.====Our findings are indicative of a significant difference between the effect of grandparental childcare provision and parental time investment in the child on the social and behavioral outcomes of children under 6, as well as on their cognitive attainment. While parents’ supervision time has a larger impact on children's social and behavioral development than an additional hour spent with grandparents, the grandparents’ effect on children's vocabulary enhancement is larger than that of the parents. Transferring 10 h a week from the parents to the grandparents improves children's cognitive ability by 2.2%. These results are consistent with the findings of the psychology literature that not only parents but also other relatives and people children socialize with determine children's development at an early age (Harriss 2009).====Our findings imply a beneficial role of grandparents, and provide a strong argument in favor of policy considerations aimed to promote grandparental involvement in the child-rearing process in the first few years of life. Such policies include national insurance credit grants, financial allowances and paid leave, such as the ones recently implemented in the UK, Germany, Portugal and other European countries. In the context of Scotland and other countries in which childcare is not sufficient in some areas or cost-prohibitive country-wide, parents have to be aware of the consequences of employing a grandparent as a substitute for childcare or themselves.====The remainder of this paper is organized as follows. Section 2 summarizes the existing literature. Section 3 presents the empirical models, and explicates the identification strategies. Section 4 describes the data used in the empirical analysis of the paper. Section 5 presents the findings of this study. Finally, Section 6 discusses the policy implications, and Section 7 concludes the paper.",The Effect of Parental and Grandparental Supervision Time Investment on Children's Early-Age Development,https://www.sciencedirect.com/science/article/pii/S1090944321000302,22 June 2021,2021,Research Article,63.0
"Reiter-Gavish Liron,Qadan Mahmoud,Yagil Joseph","University of Haifa, Israel,University of Haifa, and Western Galilee College, Israel","Received 14 April 2021, Accepted 15 June 2021, Available online 18 June 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.rie.2021.06.003,Cited by (1),"Using data from about 290,000 household investment accounts, we provide a comprehensive analysis of the role of personal economic and demographic characteristics in determining the tendency to utilize financial advice. Our findings indicate that investors' sophistication level, captured using several proxies, is negatively correlated with the decision to follow the financial advice received. In addition, we find that individual differences such as age, gender and family status are strongly associated with the tendency to use the advice. The findings are robust under different distributions of the data. Finally, we also test how ==== uncertainty affects the tendency to utilize financial advice. Our results demonstrate that higher levels of financial uncertainty are associated with less use of financial advice.","It is well documented in the literature that professional money managers on average underperform the strategy of buying and holding market indices (e.g., Jensen, 1968; Gruber, 1996; French, 2008). Yet, many recent studies indicate that retail investors still hire financial advisors and money managers. One of the explanations justifying the demand for financial advice maintains that retail investors may be too hesitant to make risky investments on their own, and hence, hire financial advisors to help them invest (e.g., Bergstresser et al., 2009; Von Gaudecker, 2015). Indeed, advisors may improve investors’ trading skills in terms of diversification, timing and selection of assets. Surprisingly, however, the question of whether advisees do follow the advice given to them is still largely unanswered. In addition, only a few studies have considered the role of financial literacy when it comes to following financial advice. Furthermore, these studies yield mixed findings. Given this gap in the literature, this paper examines how the decision to follow investment advice is linked to investors' sophistication level and their demographic characteristics.====Since financial advice is a large industry worldwide and the role of advisors in guiding retail investors' investment decisions is clearly significant, we focus on those who use such advice. Specifically, we address the following questions. Who is more likely to follow the advisor's recommendation and trade according to the advice given? Are they elderly people, less experienced traders, or less sophisticated investors? Are they more likely to be men or women, home-biased investors, or highly educated investors? Does family status influence the tendency to use advice? Does the overall atmosphere of economic uncertainty correlate with utilizing financial advice? Motivated by the scarce literature on this issue, we attempt to answer these questions using a large and rich data set consisting of about 290,000 actual investment accounts of individuals and their trading activity in stocks.====Our study differs from previous studies in several ways. First, the vast majority of the research dealing with financial advice for individual investors considers either the performance of the investors' portfolio or deals with trust related issues that justify seeking financial advice (e.g., Gennaioli et al., 2015; Von Gaudecker, 2015). We extend this stream of research by controlling for investors’ economic and demographic factors, and assessing their role in investors' decisions to actually trade according to the advice received.====Second, we focus on retail investors' level of sophistication and attempt to assess the impact of this characteristic on their decision to follow the investment advice. As detailed in the next section, we capture investors' sophistication level using different proxies. These proxies include, for example, individuals who engage in complex trading, their occupational complexity and the home bias tendency of investors. In other words, while prior works on household finance capture sophistication using financial literacy, a variable evaluated using questionnaires (e.g., Lusardi and Mitchell, 2008), we propose expanding this framework by using real trading accounts with the proxies suggested above.====Third, our sample consists of actual real accounts, and, compared to prior studies, is very large in terms of the number of accounts, the period covered (which includes both stable and unstable economic conditions), and the accompanying economic and demographic characteristics.====One of the core hypotheses in this study postulates that more sophisticated individuals are less likely to utilize financial advice because of their increased ability to collect and process information (e.g., Campbell, 2006; Cole et al., 2014). In addition, an increase in financial literacy may lead to more confidence in one's own judgment (e.g., Bannier and Schwarz, 2018; Stolper, 2018). Indeed, we document that trading experience and occupational complexity are negatively correlated with utilizing financial advice, implying that more sophisticated investors (in the broad sense) tend to utilize the financial advice they receive less frequently.====In addition to sophistication, we also establish that personal demographic characteristics affect the tendency to utilize financial advice. To wit, we find that women are more likely than men to utilize financial advice, and that being married significantly increases the tendency to follow financial advice compared with being single. In addition, we find that the tendency to utilize financial advice is much greater for those who have been widowed than married investors. Furthermore, we find that men and women who have been widowed exhibit no difference in their tendency to utilize advice, indicating that being widowed may have a stronger impact on the use of financial advice than gender.====Finally, we test whether macroeconomic uncertainty is correlated with the use of financial advice. The first theory we consider posits that increased uncertainty may reflect an increase in investors' risk aversion, prompting investors to balance their portfolios by increasing the diversity and reducing the volatility of their portfolios (e.g., Qadan et al., 2019). Hence, individuals who are invested in portfolios and willing to buy or sell securities are expected to consult with advisors before acting, because professional financial advice may reduce the risks associated with uncertain decisions. The second theory, however, postulates that under increased risks, agents prefer to avoid advice, even though information may not have a cost (e.g., Karlsson et al., 2009). We find that periods associated with high levels of the VIX are associated with less likelihood of utilizing advice.====Given the increasing need for individuals to manage their own financial well-being and retirement wealth, determining how investors' personal characteristics and financial literacy relate to the demand for financial advice has received increasing attention from researchers and policymakers (e.g., Bhattacharya et al., 2012; Chalmers and Reuter, 2012; Calcagno and Monticone, 2015; Kramer, 2012; 2016). However, the research on whether advisees actually implement the advice given to them is still limited. Only a few studies have considered the role of financial literacy when it comes to following financial advice. Furthermore, these studies yield mixed findings and are unclear as to whether the advice serves as a complement to the investors’ financial knowledge or a substitute for it. Our starting premise is that while many individuals use advisors or other intermediaries, they may differ significantly in their decision to actually utilize the financial advice given to them. Hence, this paper focuses on investors' decisions to actually follow the advice they receive.====Our study may have implications for the advisory service industry provided by retail banks and money managers. For example, given our finding that less sophisticated investors are more likely to follow the financial advice they receive, and that investors avoid utilizing advice when economic uncertainty prevails, bank advisors should be more proactive with their clients. This recommendation is particularly important when helping less sophisticated investors improve the performance of their investments.====The remainder of this paper proceeds as follows. Section 2 describes the literature review and the hypotheses. Section 3 presents the methodology. Section 4 describes the data. Section 5 discusses the empirical findings, and Section 6 concludes with a brief summary and suggestions for future research.",Financial advice: Who Exactly Follows It?,https://www.sciencedirect.com/science/article/pii/S1090944321000272,18 June 2021,2021,Research Article,64.0
"Milovanska-Farrington Stefani,Farrington Stephen","University of Tampa, Department of Economics, and IZA Institute of Labor Economics, United States,University of Tampa, Department of Finance, United States","Received 14 March 2021, Accepted 5 June 2021, Available online 9 June 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.rie.2021.06.002,Cited by (1),We examine the effect of the marginal child and the total number of children on self-reported well-being as a proxy for happiness. Prior literature has not controlled for endogeneity. We propose an ,"Social scientists have been trying to identify the forces which affect individuals' subjective well-being. The presence and the number of children are among the determinants. Understanding the causal relationship between children and life satisfaction is important for at least three reasons. First, having children is a common phenomenon and choosing how many children to have, optimal family size, or whether to have an additional, marginal child, are typical family decisions. Second, happiness facilitates goals fulfillment, and contributes to better health, higher productivity and more stable relationships. Third, birth rates have been below replacement levels in most European countries for several decades. The low birth rates and the increase in life expectancy have led to aging of the population. The elderly dependents as a share of the entire population have been growing while the share of working age individuals has been declining, raising concerns about the future demographics, productivity and sustainability of the continent.====Governments have been exploring programs designed to reverse the trend of declining birth rates. However, well-being might not be optimized within a society comprised homogeneously of the archetypal two-child household. Rather, well-being might be optimized within a heterogeneous society comprised of a mixture of the voluntarily childless and of families which have more than two children. Societal and cultural pressures and/or government programs based on a misconception of the distribution of well-being and aimed at encouraging families to have more (or fewer) children when they might have chosen to have fewer (or more) may be causing a decrease in overall life satisfaction. Correcting current misconceptions about happiness through raising awareness and the rapid dissemination of the findings of a distribution of well-being that contains two distinct peaks, or bimodal distribution, might change the course of action chosen by policy decision makers. It may also change the way that individuals perceive and prejudge each other in the society.====We test the hypothesis that parents are happier than their childless counterparts and examine the effect of the number of children on life satisfaction. There are fewer articles which examine this effect as compared to studies which explore other factors associated with happiness. In addition, prior evidence of the effect of having children on happiness is inconclusive.====Previous literature has also examined the effect of ====We contribute to the rapidly growing body of the happiness literature in the following ways. First, we explore the effect of a relatively less commonly discussed determinant of life satisfaction. Namely, we examine whether the presence and the number of children have a significant effect on well-being. Second, we overcome endogeneity within the context of family size and happiness by offering the duration of the maternity leave in the country of residence as an ==== for the presence and the number of children. We also propose alternative instruments. The non-linear marginal effects presented here provide a sufficient explanation as to why prior literature was divided in their findings.====Our results reject the hypothesis that having a first child increases the degree of life satisfaction, and thus confirm prior evidence of a ""parenting happiness gap"" found in the sociology literature. However, we also find evidence that higher-order children have a significant, positive impact on happiness levels, and a larger number of children improves happiness at an increasing rate. This is consistent with a recent psychological study conducted by Dr. Harman at the Edith Cowan University in Australia.====These results suggest that for those people who choose to have children rather than remain voluntarily childless, a larger number of children is likely to make them better off. From a policy perspective, this implies that awareness programs which disseminate the idea that even though having a first child creates a happiness gap, more children improve well-being, are likely to provide an incentive to families with one child to plan higher-order children. In addition, governmental programs whose aim is to increase the ==== might be more successful and might further improve well-being of individuals if they target families which already have a child rather than all people of child-bearing age. That is, it might be more cost effective in terms of additional children per policy dollar spent and in terms of improved societal well-being per dollar spent to encourage families who already have children to have more rather than to encourage childless families to have their first child. In other words, societal happiness can be improved by a shift from more families having one child to some families having more than one while others have none. Furthermore, medical literature has previously found that happiness contributes to higher productivity and lower risk of cardiovascular diseases and depression. Therefore, policies which promote well-being through the channel of fertility by reducing the costs of raising an additional child, might indirectly improve health and labor market outcomes thorough the effect of having more children on subjective well-being (SWB).====The remainder of this paper is structured as follows. ==== briefly summarizes the current state of the happiness literature. We specify a model in ====, discuss the data and present summary statistics in ====, and provide results in ====. In ====, we discuss the possible channels which explain our results and the policy implications of the study. ==== concludes the paper and outlines limitations of the current study and areas of future research.",More and none? Children and parental well-being: A bimodal outcome from an instrumental variable approach,https://www.sciencedirect.com/science/article/pii/S1090944321000260,9 June 2021,2021,Research Article,65.0
"Zanola Roberto,Vecco Marilena,Jones Andrew","University of Eastern Piedmont Amedeo Avogadro - Alessandria Campus: Universita degli Studi del Piemonte Orientale Amedeo Avogadro Sede di Alessandria, Alessandria, Italy","Received 2 May 2021, Accepted 5 June 2021, Available online 9 June 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.rie.2021.06.001,Cited by (1),"A crucial point in any sale is the choice of the market where to sell. This issue is much more important in the case of the artworks, where there is evidence that arbitrage does not necessarily equalise prices of comparable items across different cities of sale. Are these price differences due to the specific characteristics of items sold in different places or do they capture the idiosyncratic nature of the markets? In order to answer to this question, we apply the unconditional Recentered Influence Function (RIF) regression method to a sample of Picasso paintings sold worldwide during the period 2000-2019. Specifically, we compare percentile price differences between New York City, which is known for its status as a world art city and the Rest of World. Overall, results illustrate that the law of one price fails with Picasso's ‘blue chips’, his most expensive artworks. However, after the 2008-09 financial crisis the art market became more efficient and the idiosyncratic nature of New York's art market faded.","The Law of One Price (LOP) implies that the price of a single identical (or near-perfect substitute) commodity is the same at any given time and location if transaction costs are null or regulatory barriers are absent. If the LOP holds, there should be no profitable arbitrage, while persistent differences in price levels may constitute arbitrage opportunities.====In general, arbitrage is commonplace in financial markets due to a lack of transportation costs. Efficient markets rarely exhibit short disruptions of the LOP, while inefficient markets are characterised by a number of infractions, whose detection is particularly challenging in non-standard investment markets (fine wines, antiques, vintage cars, etc.). In fact, arbitrage is easier in standard financial markets since there are no transportation costs and transactions occur almost instantaneously (Lamont and Thaler, 2003). By contrast, non-standard financial assets sold worldwide are uncommon and usually have different characteristics (Worthington and Higgs, 2004). This ‘singularity’ is exacerbated in the case of items sold at art auctions, which are characterised by multidimensionality, uncertainty, and incommensurability (Karpik, 2010).====The literature investigating the LOP in the art (auction) market is limited, and mainly based on repeat sales indexes. Pesando (1993) analysed the LOP in the art market and used repeat sales of prints sold at Christie's and Sotheby's in both the US and UK. He found systematic price differences between both auction houses and markets. Czujack's (1997) findings were comparable and showed that Picasso's paintings were sold at higher prices in New York than in London. Mei and Moses (2002) revealed the existence of undetected arbitrage opportunities across different auction houses. Later, Pesando, and Shum (2007) revisited the results of Pesando (1993) and used the sales of alternative copies of the same print sold worldwide at Christie's and Sotheby's between 1977-2004. They found that prices were 9 percent less on average at Christie's than at Sotheby's during the period 1977-1992. By contrast, prices were 12 percent higher on average at Christie's than Sotheby's during the period 1993-2004. Additionally, prices were higher in New York than in all of Europe across the entire period. Liu (2015) used the sale history of Andy Warhol's ==== print series to highlight the role of what she called “auction experience” in determining a clear violation of the LOP. However, location was not taken into account in these studies. Recently, Etro and Stepanova (2021) jointly analysed transaction costs and new information on the value of artworks, avoiding the possibility of spurious relations. They concluded that there were no significant differences in returns for either auction houses or sales location, suggesting that the efficiency hypothesis cannot be rejected.====In contrast to these contributions, which use repeat sales (RS) to test the existence of LOP, our paper uses single sale observations to investigate the existence of the LOP. This allows us to overcome some of the limits of the RS approach. In fact, RS reduces datasets to a small number of observations since high value art objects are traded infrequently (Goetzmann and Spiegel, 1997). In addition, most RS studies might be biased due to disproportionately representation of famous artists, or geographical sale provenances, etc. (Guerzoni, 1995). Finally, resales can be difficult to identify, potentially creating inappropriate links between two sales (Graddy et al., 2012).====This study investigates the existence of the LOP in the arts specifically focusing on New York (NY),==== which is known for its status as a world art city. We analyse price differences between NY and the Rest of the World (RoW), decomposing them into differences which are due to different distribution of characteristics of artworks sold at NY or in the RoW or to the differences in the effects these characteristics exert on prices across markets. Based on a sample of Picasso paintings sold worldwide at auction in the period 2000-2019, we apply the unconditional Recentered Influence Function (RIF) regression method (Firpo et al., 2009) to decompose the differences across percentiles in the distribution of returns between NY, and the RoW. The main findings suggest that overall, the LOP fails with Picasso's ‘blue chip’ artworks, while it holds with Picasso's lower price paintings. However, after the 2008-2009 financial crisis, we show that the art market became more efficient and the idiosyncratic nature of NY disappeared. The choice to use NY is due to its importance for trading art worldwide (Deloitte, 2019).====The main contributions of the paper to the existing literature are twofold. Firstly, different from previous studies, the use of the hedonic sample rather than RS to address the LOP enlarges allows us to keep as much available information as possible to carry out the analysis. Secondly, despite the role of NY in the art markets being well known (Power, 2008), we suggest a unique way to use the RIF-decomposition results to highlight the idiosyncratic nature of it in the art markets. This approach could be applied to analyse other non-standard investment markets.====The remainder of this paper is organised as follows. Section 2 surveys the specific role of NY in the arts. Section 3 briefly illustrates the unconditional RIF-decomposition method, while Section 4 describes the sample and the variables selected for this study. Section 5 shows the empirical findings. Section 6 discusses the results and concludes by providing the main implications.",A place for everything and everything in its place: New York's role in the art market,https://www.sciencedirect.com/science/article/pii/S1090944321000259,9 June 2021,2021,Research Article,66.0
"Antunes António,Ercolani Valerio","Banco de Portugal and NOVA SBE,Bank of Italy","Received 7 May 2021, Accepted 7 May 2021, Available online 12 May 2021, Version of Record 27 August 2021.",https://doi.org/10.1016/j.rie.2021.05.001,Cited by (0),", inducing a fall of approximately 30 basis points in the return to assets and a moderate increase in the wage rate. The implied output loss amounts to roughly ","There is a large empirical literature studying the effects of individual health dynamics on earnings for both industrialized and developing countries; Currie and Madrian (1999) provide a careful review. In general, the research supports the view that poor health has a negative and non-negligible effect on earnings, through either changes in wages or in hours worked. As a consequence of this, health has an impact on individual and, hence, on aggregate measures of productivity. For example, Barro and Sala-i-Martin (1995) document a positive relationship between the life expectancy at birth and the yearly GDP growth rate per capita for a large panel of countries.====We take the above evidence for granted (although we perform further empirical investigations) and go beyond in that we aim at measuring the general equilibrium effects stemming from those changes in individual health that have the following characteristic: they directly affect the earnings of a large share of workers. As an illustrative example, consider many workers that are hit by a health shock (e.g., an influenza or a pneumonia). They can become less productive or work less hours for a certain period, experiencing a fall in their earnings. These effects directly impact on prices, e.g., the average return of capital will be lower in the economy, ceteris paribus, thereby discouraging saving. What is the magnitude of the change in (factor) prices? What is the magnitude of the change in savings and in output? Answers to this kind of questions are the main objective of the present work.====In order to perform this measurement exercise we proceed in four main steps. As a first step, we develop a general equilibrium model with workers and capital accumulation, characterized by two salient features. First, a worker’s health affects her own earnings, i.e., a negative health shock immediately deteriorates the worker’s health status and this provokes a fall in earnings in the current period. Second, the health status also has an endogenous component, i.e., workers can act against its deterioration (and thus against the fall in earnings) by investing in health. The latter means that individuals can either buy directly health-related goods (i.e., out-of pocket expenditures) or subscribe a private health insurance. Further, a distinctive feature of health is that it directly affects one’s lifespan; we hence allow for the health status to impact on the mortality rate through a probability of not being alive in the following period. With this modeling we want to capture the unavoidability of certain health shocks by imposing that the individual suffers immediately the effects of a negative health shock. However—provided she is still alive in the following period—she can act in order to improve her health status in the future. In order to focus solely on wage earners, we assume that there is a probability of the agent retiring, a period which we do not model. The model also encompasses standard idiosyncratic productivity shocks. Within this framework agents are heterogeneous in several aspects, i.e., wealth, health status, productivity, survival rate, and insurance protection; hence, large and unanticipated health shocks could potentially affect various groups of individuals differently.====As a second step, we calibrate our model for the U.S. economy paying particular attention to measuring the direct impact of health status on earnings. To this effect, we use the Medical Expenditure Panel Survey (MEPS) data. We label this calibrated model as our “benchmark” economy.====As a third step, we study the response of our economy to a health shock. As a candidate for this shock we refer to one of the worst health catastrophes that ever occurred in the U.S., the 1918 influenza pandemic.==== This choice has several motivations. First, this influenza largely impacted on earnings because, unlike the typical pandemic, it mainly affected and killed healthy adults 20 to 50 years old. Second, this event had the characteristics of a typical shock because, as we report in Section 4.1, it was exogenous to the agents’ economic characteristics and largely unexpected. Third, understanding the potential economic consequences of an influenza pandemic occurring today (e.g., avian influenza) is a first-order concern for international institutions, economists and politicians all over the world (see Garrett, 2008). For example, the U.S. Centers for Disease Control and Prevention estimate that an influenza pandemic would cause approximately 200,000 deaths in U.S. and an initial drop of GDP of 1.5 percentage points, with even higher long-run losses. Notice, furthermore, that due to the H5N1 (a subtype of the influenza A virus) threat, on November 2005 the Bush Administration disclosed the National Strategy for Pandemic Influenza, sponsored by a request to the Congress for around 7 billion dollars to begin implementing the plan.====As a final step, we study the general equilibrium effects of changes in the individual health that have a permanent character. Instead of focusing on a health shock that, though large, has a transitory character, we perform a comparative steady-state analysis. More precisely, we compare our steady-state benchmark economy with two other economies. In the first one—the “no-risk” economy—health risk does not exist, i.e., we set the probability of health shocks hitting workers to zero. In the second economy—the “policy” economy—a policy that forcibly heals agents in the period after the occurrence of the health shock, at the cost of a higher taxation for everyone, is implemented. This set of exercises can be useful to understand the potential effects of those reforms that directly affect the level of health status and risk of wage earners, e.g., a reform of the health insurance or of the pension system.====We obtain the following main results. First, we estimate that the influenza pandemic generates, on impact, a drop in the efficient labor by roughly ====, which induces a fall of approximately 30 basis points in the return to assets and a moderate increase in the wage rate. The implied output loss amounts to more than ====. These effects are persistent, especially for prices and consumption. The knowledge by the public about the consequences of the influenza pandemic (e.g., the increase in the mortality rate) plays a mild role in shaping the dynamics of savings and consumption. Regarding the comparative steady-state analysis, we document that general equilibrium effects, in particular prices effects, play an important role, both qualitatively and quantitatively. For example, considering the policy economy, if prices are not allowed to adjust, the changes in capital, consumption and output with respect to the benchmark economy are ====, ==== and ====, respectively. Instead, if the adjustment in prices is allowed for, these numbers become ====, ==== and ====, respectively.====To sum up, the empirical fact that health directly affects earnings has important general equilibrium consequences, especially on prices. That is, the correct evaluation of the effects of those health shocks or reforms that influence the earning dynamics needs to be performed in a general equilibrium framework. If we omit the general equilibrium forces, both the effects on prices and the reactions of the economy to these price changes will be missed.====Our work is related to a number of studies in the literature. Among others, Palumbo (1999) and De Nardi et al. (2010) study, within a partial equilibrium framework, the importance of health risk for the savings decisions of elderly people. Similarly, Kopecky and Koreshkova (2013) quantify savings for several old age health expenses within a life-cycle model.==== Unlike them, we focus on the general equilibrium effects of health dynamics through its influence on earnings. Our paper is obviously related to the stream of the literature on dynamic general equilibrium models with heterogeneous agents and incomplete markets (e.g. Aiyagari, 1994, Huggett, 1993). Within this class of models, Attanasio et al. (2010) build a model where, among other features, health status affects individual productivity and use it to evaluate the effects of different financing schemes for Medicare. Our objective is different from theirs, and, importantly, we allow for an endogenous component of health.==== Other works by Jeske and Kitao (2009), Feng (2012) and Pashchenko and Porapakkarm (2013) focus on the welfare effects associated to different health insurance policies and reforms for the U.S. Instead, we concentrate on the general equilibrium effects generated by both a large and transitory health shock and permanent changes in health risk. Some of the models proposed in the literature (such as, for instance, De Nardi et al., 2010; Finkelstein et al., 2013; Palumbo, 1999) allow for the health status to directly influence the marginal utility of consumption. Though this choice seems reasonable and in some cases supported by the data, the focus of our paper is on studying the effects of health changes through earnings. In our model health affects preferences only indirectly through discounting, and this is due to varying survival probabilities. There are a few papers that, despite the scarcity of data during the 1910s, try to study empirically the economic effects of the 1918 influenza pandemic. For example, Brainerd and Siegler (2003) examine the effects of the influenza on subsequent economic growth using data from a sample of U.S. states. Unlike them, we are interested in studying the general equilibrium effects of an influenza pandemic that occurs in modern times.====The paper has the following structure. Section 2 describes the model. Section 3 presents the calibration, with particular focus on the estimation of the impact of health status on earnings. Section 4 presents the simulation exercises. Section 5 concludes.",Health and Earnings: a General Equilibrium Evaluation,https://www.sciencedirect.com/science/article/pii/S1090944321000181,12 May 2021,2021,Research Article,67.0
"Chareyron Sylvain,Chung Amélie,Domingues Patrick","University Paris Est Créteil (UPEC), ERUDITE (EA 437), UPEC, UGE, TEPP (FR 3435), 61 Av. du Général de Gaulle 94010 Créteil,University of New Caledonia, LARJE, 145 Av. James Cook 98851 Nouméa,University Paris Est Créteil (UPEC), ERUDITE (EA 437), 61 Av. du Général de Gaulle 94010 Créteil","Received 23 February 2021, Accepted 19 April 2021, Available online 1 May 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.rie.2021.04.002,Cited by (0),"This paper explores the relationship between ethnic diversity and adolescent educational outcomes at a very fine level. The French Labor Force Survey is used to obtain information on neighborhood composition and we use repetition of the school year at age 15 to 16 as an indicator of school underperformance. We use the method of Bayer, Ross, and Topa (2008) to take account of endogeneity. Although high ethnic diversity of the residential area and educational success seem to be negatively related at first sight, we show that the causal relationship between the two disappears when all confounding factors are properly controlled for. This result is robust for alternative definitions of origin.","In some developed countries, people from the ethnic majority tend to be reluctant to live in ethnically diverse neighborhoods and may even choose to move out of neighborhoods that they consider have become overly diverse (Card et al., 2008). This reaction tends to reinforce the dynamics of spatial segregation and its associated adverse effects (Boozer ==== 1992; Grogger, 1996). It may be explained by an aversion to ethnic diversity (Alesina and La Ferrara, 2005) but also by the fear of people from the main ethnic group that their children will suffer from poor educational conditions. This thinking may come from first sight observations, as can be illustrated in the French educational system where a ==== analysis would tend to conclude that more ethnically diverse neighborhoods are associated with a higher school failure rate. Fig. 1 plots the correlation between ethnic diversity and repetition of an academic year in France. The Fig. shows a positive correlation between diversity and educational delay. However, this apparent negative relationship between high ethnic diversity and educational success may be biased by many confounding factors, since ethnically diverse neighborhoods tend to be associated with other factors that reduce educational success. In this paper, we are trying to delve deeper into this apparent relationship and disentangle the connections between ethnic composition of the residential area and school performance.====A substantial literature has built up in evaluating the economic and social implications of ethnic diversity in both developing and developed countries. For instance, the canonical paper by Easterly and Levine (1997) pointed out that ethnic diversity is an explanatory factor in weak economic growth performance. More recently, Awaworyi Churchill et al. (2018), based on an analysis of macroeconomic data, found that greater ethnic diversity is associated with increased gender gaps. It also occurs alongside lower participation in community life (Alesina and La Ferrara, 2005), and is found to have a negative effect on social solidarity and the level of inter-racial trust (Putnam, 2007). Diversity is also associated with negative business performances (O'Reilly et al., 1998) as well as lower spending on productive public goods (Alesina et al., 1999; Algan et al., 2016). In addition, some studies have shown that expenditure on public goods and services are lower in more fragmented areas, particularly expenditure on public education (Poterba, 1997; Goldin and Katz, 1999; Vigdor, 2004).====Scholars addressing the effects of ethnic composition on educational indicators have more mixed findings. In some cases, a high proportion of non-natives has a partially negative effect on test scores (Hoxby, 2000; Hanushek et al., 2009). Others find that the proportion of ethnic minorities does not have any significant effect on performance (Card and Rothstein, 2007; Angrist and Lang, 2004). While still others, such as Maestri (2017), find that ethnic diversity has a positive effect on the test score of minority students. This lack of consensus is also reflected in studies on the effect of racial segregation and isolation in the US context. The Coleman (1968) report offered evidence that ethnic diversity undermined academic achievement, showing that the performance of black students declined in schools with a high proportion of black students, and Hanushek (1972) found that a higher concentration of blacks affects both blacks and whites. More recently, Kain and O'Brien (2000) find that black pupils gain more value than white pupils when they move to higher quality schools in more mixed neighborhoods, while Rivkin (2000) finds no evidence of a positive effect on academic attainment for blacks who are more exposed to whites. Hanushek ==== (2009) underline that the contradictory findings in the literature dealing with ethnic composition stem from a difficulty with identifying the causal effect of peer characteristics.====Several mechanisms have been proposed to explain why ethnic diversity can affect academic performance in a positive or negative way (Dronkers and van der Velden, 2013). For instance, Lazear (1999) suggests that ethnic diversity boosts language acquisition, especially in a school context where minorities have more incentives to espouse the majority culture. Ottaviano and Peri (2006) pursue the idea advanced by Lazear (1998), according to which ethnic diversity stimulates students' curiosity. The idea that diversity is enriching to a society comes from the fact that different ethnic groups bring different skills which can mutually enhance each other and contribute to productivity. Negative effects may arise from the fact that a more heterogeneous student population may inhibit a teacher from specializing her or his teaching, thus reducing the effectiveness of teaching.====These seminal papers have focused on ethnic diversity at the school or the classroom levels; but we find few studies in the economic literature on the links between the ethnic heterogeneity of a residential area and school achievement. Furthermore, although 9.2% of the French population were born abroad and 11% are individuals born in France with at least one immigrant parent,==== little work has been done on the economic impacts of ethnic diversity in this country.==== Consequently, lack of proper identification of the real causes and effects is problematic in the French context because it can lead to a belief in and a reliance on apparent relationships that may be biased by many confounding factors.====Therefore, we are using French data to evaluate the potential effect of the ethnic composition of an individual's residential area on his school performance. The aim is to determine whether ethnic heterogeneity is a contributory factor to educational underachievement. To this end, we follow Goux and Maurin (2005) who chose the repetition of the school year between the ages of 15 and 16 as an indicator of school underperformance. We measure ethnic heterogeneity at a very fine level so that we can obtain a credible measure of the exogenous effect of ethnic diversity on educational success. Following the example of Hémet and Malgouyres (2018), we define ethnic origin using three methods of categorization: (i) nationality; (ii) country of birth; (iii) origin of parents. In the next section we focus on the various definitions and categorizations of ethnicity used in the literature. Finally, we apply the reasoning of Bayer et al. (2008) to the endogeneity of ethnic diversity at the neighborhood level. Although a superficial analysis of the results shows that ethnic diversity and educational success are negatively related, this causal connection disappears when all confounding factors are properly controlled for, regardless of the definition of ethnic origin used.====The paper is organized as follows. Section 2 focuses on the plurality of approaches to ethnic diversity and its endogeneity in the economic literature. It surveys and assesses the literature for the positive and negative effects of ethnic diversity on economic policies and outcomes. Data treatment and variable construction are described in Section 3. Section 4 outlines the empirical strategy. The results are presented in Section 5 and the robustness checks in Section 6. Some concluding elements are presented in Section 7.",Ethnic diversity and educational success: Evidence from France,https://www.sciencedirect.com/science/article/pii/S109094432100017X,1 May 2021,2021,Research Article,68.0
Colacicco Rudy,"Department of Economics, Finance and Accounting, Maynooth University, Ireland,EPPA SA, Place du Luxembourg 2, 1050 Bruxelles, Belgium","Received 26 February 2021, Revised 2 April 2021, Accepted 3 April 2021, Available online 15 April 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.rie.2021.04.001,Cited by (5),"I build a two-country general oligopolistic equilibrium model, in which sectors differ in emissions and technologies, and pollution can be transboundary. I derive the optimal bilateral ","The environment has increasingly become a key topic for policymakers. In fact, they increasingly care not just about pollution generated within their own economy but also about the transboundary effects of pollution produced elsewhere in the world. Environmental policy is able to change the balance of competition in many sectors and, in turn, the demands for production factors. Given the wide empirical evidence on the oligopolistic nature of many sectors of several economies [e.g., 22], it is interesting and useful to analyze the environmental policy in a full-fledged multi-sector general equilibrium model of international trade with imperfect (strategic) competition. A theoretical framework to consider environmental policy in general equilibrium can generate relevant findings for a better policy implementation.====The adoption of oligopolistic competition to analyze environmental issues is well-established in literature. For example, the first studies of strategic environmental policies date back to the first half of the 1990s [e.g., [3], [8], [18], [28], [32]].==== These earlier studies have looked at cases in which policymakers can credibly pre-commit and unilaterally (non-cooperatively) choose the optimal environmental policy for ==== to help domestic firms in competing with foreign ones. Moreover, this literature is in partial equilibrium and does not consider feedback from factor markets.==== This fact is probably due to the dearth of full-fledged general equilibrium frameworks in which to embed oligopolistic competition in a theoretically consistent way.==== Since then a theoretically consistent approach to embed oligopoly in general equilibrium has been developed. This approach relies on the concept of general oligopolistic equilibrium (hereafter GOLE) as developed by Neary [23], [24], in which the economy is assumed to be composed of a continuum of oligopolistic sectors linked through factor markets.====In this paper, I study how environmental and trade policies interact. To do so, I adopt the GOLE approach. The only study that also adopts the GOLE approach to study environmental issues is the one of Richter [30]. Nevertheless, he focuses on the optimal environmental policy for single sectors. In a framework with segmented markets and perceived environmental damage as a linear function of the total pollution, Richter replicates a partial equilibrium analysis, and studies how partial trade liberalisation affects environmental policies in general equilibrium. He assumes that policymakers have perfect information on single markets and set environmental policies (i.e., emission taxes) at the sector level, to maximize sectoral welfare, which is given by the standard form as derived in partial equilibrium.==== This means that policymakers have not a global knowledge of the economy in which they operate (e.g., how national income and factor rewards are affected by their policies).====Dixit and Grossman [9] highlighted the empirical difficulty in obtaining the necessary information for targeting sectors.==== So far, the GOLE literature has disregarded a more direct role of policymakers, and how they interact to set their policies for the economy as a whole. An exception is provided by Palokangas [27], who focuses on interacting policymakers to analyze how wages are endogenously determined. However, unlike the present paper, he does not look at policies to affect the economy as a whole.====I do not consider optimal policy decisions at the sector level. Instead, in the model I present here, policymakers can levy only emission taxes to affect their economies as a whole, assuming that these policies are uniform across sectors (viz., no sector targeting). I consider that the perceived environmental damage can be either a linear function of the total pollution, as in Richter [30], or a convex (quadratic) one. In addition, unlike Richter [30] who assumes that all sectors have the same emission rate and use the same technology, I allow sectors to differ both in emission rates and technologies. These features cannot, of course, be addressed in a partial equilibrium (single-sector) analysis. I am interested in deriving the optimal environmental policy for two countries as a whole (i.e., cooperative equilibrium), because it is much more difficult to reach an efficient policy with non-cooperative countries and transboundary pollution. For the rest, I adopt the simplest possible theoretical set-up to have closed-form analytical solutions and to convey insights.====The paper continues as follows. The next section presents a general equilibrium model with many segmented markets in which firms strategically compete and generate pollution, which can be transboundary. I begin with the simplest framework to lay the groundwork for the other sections: the optimal environmental policy is derived for the case in which the perceived environmental damage is a linear function of the total pollution and sectors use the same technology but differ in emission rates. In Section 3, the analysis is extended to consider technologically heterogeneous sectors. In Section 4, I consider the alternative assumption concerning the perceived environmental damage as a convex (quadratic) function of total pollution. Section 5 concludes.","Environment, imperfect competition, and trade: Insights for optimal policy in general equilibrium",https://www.sciencedirect.com/science/article/pii/S1090944321000168,15 April 2021,2021,Research Article,69.0
Tomat Gian Maria,"Bank of Italy: Banca d’Italia, Regional Economic Research Division, Via XX Settembre 97/E, 0187, Rome, Italy","Received 15 February 2021, Revised 30 March 2021, Accepted 30 March 2021, Available online 6 April 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.rie.2021.03.003,Cited by (0),"The expectations hypothesis contends that long rates should equal expected forthcoming average short rates. The spread between long and short rates should therefore forecast changes in short rates. In addition, forward rates should anticipate future spot rates. We present ","Every day financial and nonfinancial operators take decisions, which involve transactions due to occur at different points in the future. The decisions might regard exchanges of commodities or services. The traded goods might be used in production activities or in consumption. The current and expected future levels of interest rates are important elements for the agents choices on the timing of the transactions. Moreover, according to the type of deal the relevant interest rates could have different horizons. The term structure of interest rates, or yield curve, is the relation between interest rates with different time horizons prevailing in any given period.====There is overwhelming evidence, that usually long rates are greater than short rates. We find ourselves at present in a rather exceptional circumstance, as the possibility of the occurrence of a yield curve inversion has been recently followed with considerable concern. The yield curve is inverted, when short rates are greater than long rates. Shiller (2017) has recently described the importance of narratives, as powerful contrivances shaping our understanding of the world. The yield curve inversion records world wide provide one of such tales, as it has been observed how this event usually precedes an economic recession.====Following established financial theory, long rates should in any given time period reflect expectations regarding forthcoming short rates. The long rate is greater than the short rate, when short rates are expected to increase. Conversely, the long rate is lower than the short rate, when short rates are expected to decline. A relevant issue concerns the relation between nominal and real interest rates, since the difference between nominal and real interest rates provides a measure of expected inflation.====In the present work we use data regarding the term structure of interest rates in the Euro area, for the period running from September 2004 to August 2018, released by the European Central Bank (ECB). We study the relation between long and short rates and between forward and spot rates. We have evidence of a negative correlation between the term spread and subsequent spot rate changes.====This pattern confirms much former econometric evidence. In a recent study Greenwood and Shleifer (2014) for instance examine a number of different survey based measures of stock market return expectations. They find, that stock return expectations are mostly positively correlated with past stock market returns and negatively correlated with future returns. Moreover, the dividend/price ratio and other valuation indicators positively forecast stock market returns, even though forecasting regressions have low predictive power.====The results in the present work, show that interest rates are persistent. Financial shocks have long lasting effects on the yield curve and investors exhibit overconfidence over the pricing of debt securities in the Euro area. Moreover, we do not find evidence of predictive power of interest rates for the level of inflation. In our sample, the relation between interest rates at different time horizons and forthcoming inflation is mostly non significant.====Section 2 introduces the expectations theory of the term structure. Section 3 describes the data. Section 4 presents the results of a regression analysis of the relation between long and short rates. Section 5 reviews regression evidence on the relation between spot and forward rates. Section 7 analyses the predictive properties of the spread between long and short rates for future interest rates, using a vector autoregression modelling framework. Section 8 studies the forecasting power of nominal spot rates for the level of inflation. Concluding remarks are in Section 8.","Term Spreads, Forward Rates and Yield Curve Forecasts",https://www.sciencedirect.com/science/article/pii/S1090944321000156,6 April 2021,2021,Research Article,70.0
"Garciga Christian,Verbrugge Randal","Federal Reserve Bank of Cleveland, Cleveland, Ohio, USA,Federal Reserve Bank of Cleveland and NBER/CRIW, 1455 E 6th Street, Cleveland, OH 44114 USA","Received 21 January 2021, Accepted 1 March 2021, Available online 8 March 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.rie.2021.03.001,Cited by (3),"Most consistent estimators are prone to total breakdown in the presence of a handful of unusual data points (UDPs). This compromises inference. Robust estimation is a (seldom-used) solution; but methods commonly-used in applied research have severe drawbacks. In this paper, building upon methods that are relatively unknown outside of the robust statistics literature, we provide an enhanced tool for robust estimates of mean and covariance, useful both for robust estimation and for detection of unusual data points. It is relatively fast and useful for large data sets. We also provide a new robust cluster method, an input to our broader method, but also useful for standalone UDP detection or cluster analysis. We provide a comparative study of numerous methods that is not available in the current literature. Testing indicates that our method performs at par with, and often better than, two of the currently best available methods. We also demonstrate that the issues we discuss are not merely hypothetical, by applying our tools to real world data, and to re-examine two prominent economic studies. Our methods reveal that their central results are driven by a set of unusual points.","This paper provides new tools to assist applied economists in detecting and addressing unusual data points (UDPs) that threaten inference. We think economists will want to know if – as was true for prominent studies such as Fama and French (1992, 1993) and Card and Krueger (1994) – key results in their paper are driven by unusual data points.====Many commonly-used “consistent” estimators – i.e., estimators that are consistent for a particular model – are what Müller (2007) terms “highly fragile”: small contaminations can make the estimator converge in probability to something arbitrarily divergent. For instance, the classical estimator of the covariance matrix is ====: sufficient corruption of a ==== data point – a negligible percentage of a large sample – can cause the estimator to break down, i.e., to be driven arbitrarily far from the true population covariance matrix. Standard techniques such as multivariate regression or principal components are based upon multivariate means, covariance matrices, and least-squares fitting, all of which can be profoundly influenced by even a few unusual data points (UDPs).==== Such phenomena do ==== disappear asymptotically (consider, for example, contamination by a small amount of data drawn from a standard Cauchy distribution).====Is this just an interesting pathology that, given typical practice in economics, is rarely a threat to inference?====We suspect that the answer is no. It is true that in a sufficiently large sample, randomly-distributed contamination usually does little to compromise inference. And it is true that data points that are extreme along one dimension are easy to detect and address. Often such points are removed via trimming (e.g., removing observations with annual income in excess of $1,000,000 or less than $10) or dummy variables (e.g., US GDP in 2020Q2 and Q3). Occasionally, researchers use traditional outlier-detection techniques such as studentized residuals or Cook's ==== to identify problematic data points.====But these methods are far from foolproof. Traditional outlier-detection techniques such as Cook's ==== can easily fail in multivariate data, as we demonstrate below. Multivariate outliers may not be extreme along a single dimension and can be extremely challenging to identify, particularly in large data sets… and they can easily undermine inference. This is because UDPs may be distributed in such a way that they disguise each other, a phenomenon known as ====.==== Inference is compromised, and yet there is no indication that anything has gone wrong.====In the regression context, one might think that a ready solution is to use robust regression, in order to assess whether findings from conventional analysis are reliable. This often works; but in this paper we provide more dependable methods, necessary because most commonly-used robust regression techniques are less robust than they appear. For instance, perhaps the most commonly-used robust regression method in economics, median regression, can fail with a single outlier: a single outlying (====) pair can force ==== quantile-regression hyperplanes to pass through it.==== Other estimators, such as the least trimmed squares (LTS) estimator (Rousseeuw, 1984, 1985) and estimators based upon the minimum covariance determinant estimator (MCD) (Rousseeuw, 1984), are very robust – but impractical to compute except in small data sets. And their so-called “practical” implementations, such as “fast-MCD,” do not necessarily share the good properties of the original estimators, because they rely on basic resampling (numerous initial random samples), studied in Hawkins and Olive (2002). In order to succeed, these approaches require that one of the initial random samples is comprised ==== of clean data, which in some cases is extremely unlikely (see Appendix A.2 for further discussion). This deficiency is on display in our simulation study below.====Fortunately, there now exist better methods that address the aforementioned deficiencies. This paper offers an enhancement of two efficient MCD estimators, RMVN and detMCD, that do not rely upon basic resampling. The outputs are robust estimates of the mean and covariance. Equipped with these robust estimates, it is straightforward to conduct subsequent inference, such as multivariate regression (see Appendix A.3). Further, these estimates allow the straightforward detection of UDPs, based upon (now robustly-estimated) Mahalanobis distances. The analysis of UDPs can lead to a deeper understanding about the data-generating process (see also the discussion in Knez and Ready, 1997); and identification of such data is sometimes the point of an analysis (e.g., detecting counterfeits or fraud, identifying superstars). Indeed, as Janson and Verbrugge (2020) discuss in more detail, tools like these can play a critical role in discovering heretofore unknown unobserved variables (or multiple data-generating mechanisms) in the data.====Our chief contribution is to extend previous work in the MCD branch of the robust statistics literature by integrating insights from a separate branch of the literature, clustering. In particular, we develop a novel robust clustering technique that may be of independent interest, but which we offer primarily as an augmentation to the aforementioned methods. This augmentation is a notable advance. UDPs that are distributed completely at random represent the least troublesome type of contamination; but contamination that is systematic, i.e. contamination consisting of ==== of aberrant points, can be both far more problematic and far more challenging to detect. This is due to masking, discussed above. On the face of it, cluster analysis would seem well-suited to address this challenge, since its entire aim is to find partitions that best approximate the data.====As is common in this branch of the literature, we offer no asymptotic theory for our extension, and assess the relative practical (small-sample) performance of our contributions using artificial data that is “contaminated” in various ways, and on the basis of classic real-world data sets (where the “answer” is “known”). This comparison study indicates that our methods operate on par with, and in some cases dominate, their predecessors. Finally, we offer a demonstration that these tools can yield consequential results: in prominent economic studies we investigate (one of these in detail), UDPs drive the main results (and for most of the data, the opposite conclusions hold).====Thus we provide applied researchers with powerful new tools for understanding the data they are confronted with. We believe that methods like ours should be routinely used as part of a scientific (hands-off) robustness test of results – and may also be useful in machine learning applications, where contamination is a severe threat to inference. If evidence for UDPs is uncovered, a researcher then faces the important – though perhaps challenging – decision about what to do next. With knowledge of the set of UDPs in hand, she must determine the best course of action: further study to determine if there is an omitted categorical variable; developing or altering existing theory to take into account the now-richer understanding of the data; winsorizing, data cleaning, or simply focusing on one part of the sample, such as the majority; or perhaps something else.",Robust covariance matrix estimation and identification of unusual data points: New tools,https://www.sciencedirect.com/science/article/pii/S1090944321000053,8 March 2021,2021,Research Article,71.0
"Contensou François,Vranceanu Radu","ESSEC Business School and THEMA, 1 Av. Bernard Hirsch, Cergy 95021, France","Received 4 January 2021, Revised 5 February 2021, Accepted 3 March 2021, Available online 8 March 2021, Version of Record 1 June 2021.",https://doi.org/10.1016/j.rie.2021.03.002,Cited by (1),In the ,"According to the standard labour market theory, wage rates tend to reflect the marginal productivity of labour services. Consequently, wage rates should coincide for workers performing the same task with the same level of ability. Labour market observations however, in many instances, tend to challenge this view. Despite convergence in educational attainments and other relevant explanatory variables, unexplained differences still exist between the full-time wages of men and women, particularly at the top of the wage distribution (Blau, Kahn, 2017, Cahuc, Carcillo, Zylberberg, 2014, Gunderson, 1989, Neumark, 2018, OECD).====To explain the differences between the neoclassical expected equality and observed facts, economists have often resorted to the notion of discrimination (Becker, 1957). Labor market discrimination has been defined as “a situation in which persons who provide labor market services and who are equally productive in a psychical or material sense are treated unequally in a way that is related to an observable characteristic such as race, ethnicity, or gender” (Altonji and Blank, 1999). It is well known, however, that some observed differences in wages obtained by equally competent workers may be explained without resorting to discrimination. The theory of compensating differentials, originating in Adam Smith’s celebrated seminal works, argues that wage gaps can be grounded in differences in preferences for job attributes (Rosen, 1986; Thaler and Rosen, 1976).==== Some other explanations resort to a productivity argument: in her presidential address to the 126th Annual Meeting of the American Economic Association, Goldin (2014) pointed out that in some qualified occupations (legal services, business, and finance), longer hours provided by men in general, are rewarded at a higher wage rate.==== Goldin explains this outcome based on productivity increasing with hours, as firms and their clients value “temporal flexibility”: on-site presence, intensive client contact, face-to-face time, etc. Workers who work longer hours and accept fragmented schedules could be more valuable to a firm since they can exchange information with their peers more efficiently. If men are more available than women in such jobs, then a gender wage gap could result.====This paper provides a model of wage and hours differentiation consistent with the above-mentioned observations that is completely free of productivity differences or discriminatory practices as defined by the labor market literature: in our model, all workers have equal access to all contracts proposed by the employer. The fundamental source of heterogeneity in the model is workers’ disutility from work. For the sake of parsimony, we assume that workers are of only two types, and workers of the first type always demand less compensation for any amount of working time than workers of the second type. The other basic assumption of this model is that workers of the first type are in limited supply.====The principal seeks to hire workers for a predetermined number of working hours, as required to achieve a production target. The goal of the firm is to minimize the total cost of labor. If workers of both types were available in any quantity, cost minimization would generally cause labour demand to concentrate on the “less expensive” type. The more demanding workers being crowded out, they do not appear in the data. Our analysis focuses on the nontrivial, and most plausible situation, where less demanding workers are in scarce supply and the demand for hours is great enough to prompt the employer to hire both types of workers.====When contract discrimination is not possible, hiring workers of the more demanding type creates an externality influencing the contract offered to the less demanding type, through an incentive compatibility constraint that ensures the efficient self-selection of workers. The determination of optimal contracts builds on standard principles of contract theory (inter alia, Bolton, Dewatripont, 2005, Laffont, Martimort, 2009, Salanié, 2005), to which we add, as an original theoretical contribution, an endogenous determination of the proportion of worker types. Despite the relatively complex structure of the problem, the solution is fully characterized for a quadratic compensation function.====A compensation function defines the level of consumption required by a worker to fulfill his/her participation constraint. Our analysis reveals that ordering worker types by non-crossing compensation functions is not sufficient to predict the ordering of optimal working times, compensations and implicit hourly wages. In particular, the model points out the crucial role played by the specific assumption concerning the sensibility of the worker compensation differential with respect to working time itself.====The analysis also reveals that the employer minimizes the total labour cost by offering contracts generally exhibiting different (implicit) hourly wages. We refer to these wage differences as forms of “pseudo-discrimination”, since these wage rate differences between workers doing the same job with the same level of skills exist, but are not explained by incorrectly perceived productivity differences or by a biased objective function of the employer. In particular, the model reveals the possibility of a paradoxical situation in which less demanding workers are granted a higher wage rate, in sharp contrast to the case in which discrimination is possible.====Finally, in formalizing and interpreting the labour cost function, the model explains the demand for more exacting workers and predicts local discontinuity. For a threshold of needed working services, the cost minimizing policy switches from employing less demanding workers only, to the adoption of a mixed labour force, including a minimum number (quantum) of the more demanding type. This setting is compatible with some forms of type-specific mass redundancy in the case of an economic slowdown.====Our analysis can shed light on the topic of gender discrimination if consumption/leisure preferences are specific to the gender of the employee. Because women still have the responsibility of caring about their children in many countries, they might obtain more utility from out-of-job hours (Cain, 1986, OECD). Women can therefore be represented as the group of more demanding workers. Men would then represent the group of less demanding workers. If “less expensive” male labour is in short supply, the model shows possible gender wage differentials in the absence of any biased information about productivity or a nonconventional objective of the employer involving contract discrimination. As such, the tools used to study hours/wage offers are quite standard (contract theory); however, including the demanded hours constraint in the optimization problem and solving it explicitly with quadratic compensation functions can be seen as a contribution to this literature.====The paper is organized as follows: In Section 2, we introduce our main assumptions and define the mixed employment regime. In Sections 3, 4 and 5 the model is analyzed in three increasingly detailed sets of assumptions. Section 3 analyses the cost minimization problem for the general case. In Section 4, we provide a more precise definition of optimal contracts when the compensation differential is a linearly increasing function of working hours. In Section 5 more precision is achieved by adopting a quadratic compensation function, and numerical simulations are used to support the analytical results. The minimum number of type 2 employed workers is endogenous. Section 6 concludes.",Working time and wage rate differences: Revisiting the role of preferences and labor scarcity,https://www.sciencedirect.com/science/article/pii/S1090944321000065,8 March 2021,2021,Research Article,72.0
Pandey Siddhi Gyan,"O. P. Jindal Global University, Sonipat, Haryana 131001, India","Received 8 December 2020, Accepted 4 January 2021, Available online 8 January 2021, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2021.01.003,Cited by (0),"This paper proposes a model of formation of signed networks that comprise both positive (friendly/cooperative) and negative (antagonistic) social ties between players who differ intrinsically in strength. Friendships/alliances serve to increase one’s power over their intrinsic strength; in the determination of power it matters not ==== allies one has, but "," or the structures of social and economic relations between individuals influence outcomes for interacting individuals in a wide range of social contexts. Given the crucial role that networks play in determining social outcomes, game theoretic analysis of strategic network formation has generated a rich literature over the last few decades. With its foundation in the works of Roger B. Myerson, who first modelled network formation explicitly in the context of cooperative games (see Myerson (1977)), this literature now spans network formation models in a variety of social and economic contexts==== While much of it focuses on social ties of cooperation and friendship, i.e. ties that yield benefits to the individuals that are involved in them by virtue of providing access to information, opportunities, resources, or other sources of utility, a part of the literature has also explored ====.====Signed networks are social networks that simultaneously incorporate relationships of friendship and cooperation (positive ties) and relationships of antagonism, envy or non-cooperation (negative ties). Rooted in the area of social psychology, the literature on signed networks is heavily influenced by Heider (1946) contribution to theory of ====, which states that individuals who share a positive relationship with each other will often match their attitudes towards third parties. This idea gave rise to the theory of ====, developed in the works of Harary and Cartwright (1956), which is a theory about the structure of signed networks. In particular, structurally balanced signed networks must compose of triads that either consist of three positive links or one positive and two negative links. This implies that the global configuration of structurally balanced signed networks is such that either all individuals are friends or there exist two distinct sets, where individuals in the same set are positively tied and individuals in different sets sustain negatively tied. A weaker version of structural balance (which allows for triads with three negative ties) implies that graphs may consist of multiple cliques. Much of the signed networks literature builds on and verifies, in different contexts, the theory of structural balance==== While there exists some literature that examines the network implications of ==== in the context of signed networks, these models often ==== the principle of balance as a behavioural attribute of individuals, and do not provide micro-foundations to the process of signed network formation in terms of any other dynamics==== One exception is the model of signed network formation proposed by Hiller (2017), which in fact provides game theoretic micro-foundations for (weak) structural balance. Hiller models strategic formation of signed networks in the contexts of bullying where a) mutual consent is needed to form a tie of friendship, while antagonism can be established unilaterally, and b) incentives for ganging up prevail because the aggregate of one’s friendships determine their ==== in the network, and individuals extract payoffs from less powerful enemies in the network. Hiller’s model is most appropriate for situations where one’s power depends on ====, and hence people have incentives to gang up (form alliances with equally powerful individuals) against less powerful individuals. When modelled in this manner, every ==== network necessarily comprises cliques of ==== wherein every clique comprises only ties of friendship (the size of the clique determines coercive power of every member of the clique), everyone offers positive links to every member of a bigger clique and negative links to every member of a smaller clique; this is in accordance with weak structural balance.====This paper proposes a model of signed network formation for contexts of surplus extraction where one’s power of surplus extraction depends not on the aggregate of one’s alliances, but on the strongest of one’s allies. This model, a variation of Hiller’s model of signed network formation, is motivated by the need to investigate the structure of relationships between individuals who differ in intrinsic strengths and who interact in a context where ====. These are contexts where it is not the aggregate of one’s alliances but only specific alliances that matter because power is determined by endorsement from or association with a stronger player. The network is formed through the following mechanism: Each player chooses between offering either a link of friendship (a positive link) or of antagonism (a negative link) to every other individual. A ==== of friendship or an alliance requires both players to extend positive links to each other==== However, antagonistic ties may be unilaterally formed. More specifically, if one of the player extends a positive link while the other extends a negative link, the resulting tie is one of ====, or a ====. If both players offer negative links to each other, it results in a tie of ====. Both of these ties are antagonistic in nature. The population contains players that are heterogeneous in their intrinsic strengths. Utility is derived by coercing ==== from other individuals who are less powerful (in terms of ====) and are connected to one via a ====. The coercive power of a player is determined endogenously, and equals the intrinsic strength of the intrinsically strongest person in one’s neighbourhood of friends (this includes the player herself). Further, while unilateral antagonism is not costly, conflict necessarily is costly for both players involved. The nature of conflict is qualitatively different from a coercive tie; in the latter - regardless of which of the two parties initiates hostility - the antagonistic relationship established between the two players results in the more powerful player coercing a payoff from the less powerful player. In conflict, however, possibilities of such surplus extraction are assumed to be lost as a result of mutual aggression and hostility. Thus no payoffs are exchanged in conflict. The cost of conflict, borne by both players, may be interpreted as the cost of material damage, damage to property/life and livelihood caused by acts of mutual aggression and hostility. For simplicity it is assumed to be independent of one’s intrinsic strength. Finally, Hiller’s use of the contest success function to determine the magnitude of payoff exchanged across any coercive tie is retained here, as is the reliance on Nash equilibrium as a notion of stability. A strategy profile underlying a network is a Nash equilibrium if and only if no individual player has an incentive, in the form of higher utility, to unilaterally deviate to a different strategy than the one they’re playing in the strategy profile in question.====The first three propositions pertaining to equilibrium analysis establish some necessary conditions on Nash equilibria. While ties of conflict are completely ruled out ====. In equilibria where ties are established such that players acquire unequal coercive powers, players with unequal coercive powers necessarily share ties of coercion in equilibrium. In particular, every player offers a positive link to more powerful players==== and a negative link to less powerful players than herself. This implies that in any equilibrium where players acquire unequal coercive powers, the population must be partitioned into distinct sets based on coercive power, with equally powerful players belonging to the same set, players across sets necessarily engaged in coercive ties and antagonism sustained by the more powerful players. Equally powerful players within a set may or may not be friends, but conflict is ruled out within equally powerful players as well. Further, all payoffs exchanged across coercive ties are capped, in magnitude, by the cost of conflict.====That equally powerful players may not be friends leads to possibilities of violation of weak structural balance in equilibria. That the payoff exchanged between any two players in equilibrium must not exceed the cost of conflict has important consequences for the existence and nature of Nash equilibria under various ranges of cost of conflict. After exploring necessary conditions for equilibrium, the nature of equilibria are explored for all possible conditions on cost of conflict. We find that when the cost of conflict exceeds every possible payoff that may potentially be exchanged across coercive ties in the population (these payoffs depend on the distribution of intrinsic strengths in the population), all strategy profiles where there is no conflict and players offer positive links to intrinsically stronger and negative links to intrinsically weaker players are Nash equilibria. However, a lower cost of conflict restricts the presence of coercive ties in equilibria. Because in equilibrium the payoffs exchanged across coercive ties must not exceed the cost of conflict, equilibrium networks compatible with a very low cost of conflict must have little (if any) antagonism. In fact, a Nash equilibrium profile doesn’t exist under the specific conditions where a) cost of conflict is less than every possible payoff that may potentially be exchanged (given the distribution of intrinsic strengths in population), and b) there exists a uniquely intrinsically strongest player in the population. However, as long as there is no uniquely intrinsically strongest player in the population, the network where all players share positive ties with each other is a Nash equilibrium regardless of the cost of conflict.====The remaining paper proceeds as follows. The model is set up in the next section, following which results on equilibrium are presented with proofs and discussed in section three. The fourth section concludes the paper.",A model of signed network formation with heterogeneous players,https://www.sciencedirect.com/science/article/pii/S109094432100003X,8 January 2021,2021,Research Article,73.0
"Takahashi Harutaka,Le Riche Antoine","Graduate School of Economics, Kobe University, Japan,Meiji Gakuin University, Japan,School of Economics, Sichuan University, China","Received 2 November 2020, Accepted 2 January 2021, Available online 7 January 2021, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2021.01.002,Cited by (1),"Reports of the literature documenting the declining labor share of income have increased greatly in the past few years, which is opposed to one of the famous “Kaldor's stylized facts” of growth. The declining labor income share has been observed since the 1980s in a number of countries, and especially in the United States. Recent studies have revealed the following five major driving forces of the declining labor share: (i) supercycles and boom-busts, (ii) rising and faster depreciation, (iii) superstar effects and consolidation, (iv) capital substitution and automation, and (v) globalization and labor bargaining power. We set up a two-sector optimal growth model with the R&D intermediate sectors producing ====. By integrating driving factors (ii) through (iv) above into the model, we demonstrate the long-run decline of the aggregated labor income share.","Studies conducted in the past few years have increasingly augmented the literature documenting the declining labor share of income, which is opposed to one of the “stylized facts” of growth reported by Kaldor (1961). The labor income share decline has been observed since the 1980s in many countries, and especially in the United States (see for instance Elsby et al. (2013) and Karabarbounis and Neiman (2014)). This is a very important topic since it could generate a decrease in the aggregate demand as the decline in labor share entails a slowing income growth and a loss of the consumer purchasing power. Recently, McKinsey Global Institute Discussion Paper (2019) has surveyed studies of the literature examining factors driving the labor share decline, with categorization of the main driving factors as explained below.====A noteworthy point is that, except for reports published by Koh et al. (2015), Barkai (2016), and Lawrence (2015), many reports have pointed out multiple factors driving the labor share decline. The main driving factors are not unique: they are multiple and are yet inconclusive.====Furthermore, the same McKinzey discussion paper (2019) has also presented re-examination of the five driving factors for the US economy based on the OECD STAN database from a macro–micro perspective. By ranking the five leading forces that have driven the recent capital share increase instead of those of the labor share decline, the report has indirectly described the main causes of the decline in labor share, as summarized in Table 1.====Although cyclical factors are the major driving forces, growth theory clarifies that leading driving factors (ii), (iii), and (iv) in the table are important. In fact, those factors jointly explain 56% of the decline in labor share. Factor (ii) was examined by Koh and Santaeulalia-Llopis (2016), who concluded that because of the transition to more intangible capital, especially in IPP capital intensive economy, rising IPP depreciation and net IPP income have emerged. Factor (iii) is studied by Autor et al. (2017) among others. Technology and market conditions have facilitated the emergence of “superstar” firms with very high profit and a very low labor share. Factor (iv) is particularly examined by Karababounis and Neiman (2014). Decreased relative prices of capital goods because of information technology (IT) and automation have induced firms to shift away from labor to capital.====Based on the discussion presented above, we set up a two-sector-consumption goods and capital goods sectors-optimal growth model with intermediate goods sectors. Each sector's intermediate goods are produced by application of labor and tangible capital goods by Cobb–Douglas technologies with learning-by-doing technical progress. In contrast to tangible capital, assuming that intermediate goods become obsolete instantaneously and that their depreciation rate is therefore 100%, one might regard intermediate goods as intangible IPP capital. Corrado et al. (2009) point out that treating intangible capital as an intermediate good omits the technology for producing knowledge. Our model setup avoids this problem. Furthermore, each final goods sector produces final goods with the sector's IPP capital and labor using Cobb–Douglas production technologies. Intangible capital captures the various investments firms can make that do not form traditional, measurable capital. Examples are research and development that affect what we might otherwise consider to be the productivity of a firm or a product; advertising and marketing that build brand value or raise awareness of a product; or investments in firm culture and organization capital. An important feature of intangible capital, which is the focus in this paper, is that it is difficult to undo or to sell. Intangible capital becomes a part of the underlying productivity of a firm or product====. Thus driving factor (ii) has been integrated into the model successfully. Furthermore driving factors (iii) and (iv) can also be integrated into the model as follows: By combining the intermediate sector with the final goods sector, the model can be recast as a standard two-sector optimal growth model with sector-specific total factor productivity (TFP) growth. This characterization is consistent with the recent findings of Swiecki (2017) and Van Neuss (2018) on economic structural change, where they conclude that sector-dependent technological change is the most important mechanism overall.====Contrasted to the standard two-sector model with TFP growth studied by Takahashi (2017), the TFP growth rate is endogenously determined here. In Takahashi (2017), where a two-sector optimal growth model with a sector-specific TFP is set up and under the Cobb–Douglas technologies, it is demonstrated that each sector's optimal path converges to a sector-specific optimal steady state path. This property also holds here under the condition that the integrated consumption goods sector is more capital intensive than the capital goods sector. Given these circumstances, an empirical fact exhibits that the sector with a higher TFP growth rate indicates a lower labor income share. We observe the similar relation among other OECD countries. Combining these, along the optimal steady state, the value-added of the consumption goods sector eventually dominates that of the capital goods sector. Consequently, the aggregated labor income share declines.====Guillo et al. (2011) set up a similar two-sector aggregated growth model to ours and mainly studied the economic structural change. Our model, however, shows a sharp contrast to theirs in the following three points. 1) They assumed that the household's preference was non-homothetic and economy was composed of overlapping generations of individuals. Here, we assume a dynasty-type household who maximize his/her life time aggregated utility expressed by a standard representative household's utility function. 2) Each goods is produced by the aggregated Cobb-Douglas technology with a same share parameter, but a different TFP growth across sectors. In contrast, our production technology is of the Cobb-Douglas type, with different share parameters and TFP growth rates across sectors. 3) They solve the model numerically with calibration methods, while we solve the model analytically.====The paper is organized as follows: The next section presents the model and related assumptions. In Section 3, each sector's R&D process is solved explicitly. As described in Section 4, using the production possibility frontier, we integrate the model into a standard two-sector optimal growth model and solve it. In Section 5, the existence and uniqueness of the steady state are proved. In Section 6, saddle-point stability is presented. Section 7 explains the aggregated labor share decline. Section 8 concludes the paper.",A dynamic theory of the declining aggregated labor income share: Intangible capital vs. tangible capital,https://www.sciencedirect.com/science/article/pii/S1090944321000028,7 January 2021,2021,Research Article,74.0
"Postlewaite Andrew,Schmeidler David","Tel-Aviv University, Tel-Aviv, Israel and HEC, Paris, France,University of Pennsylvania, Philadelphia, PA, USA,Tel-Aviv University, Tel-Aviv, Israel","Received 21 November 2020, Revised 26 November 2020, Accepted 2 January 2021, Available online 6 January 2021, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2021.01.001,Cited by (1),", the consumer problem is NP-Hard, and it appears unlikely that it can be optimally solved by a human. Two implications of this observation are that (i) households may imitate each other’s choices; (ii) households may adopt heuristics that give rise to the phenomenon of mental accounting.","Economists seem to be in agreement about two basic facts regarding neoclassical consumer theory. The first is that the depiction of the consumer as maximizing a utility function given a budget constraint is a very insightful tool. The second is that this model is probably a poor description of the mental process that consumers go through while making their consumption decisions at the level of specific products.====The first point calls for little elaboration. The neoclassical model of consumer choice is extremely powerful and elegant. It lies at the heart and is probably the origin of “rational choice theory”, which has been applied to a variety of fields within and beyond economics. Importantly, utility maximization, as a behavioral model, does not assume that a mental process of maximization actually takes place. And thus, while many writers have commented on the fact that a literal interpretation of the theory does not appear very plausible, economic theory generally adopts the “as if” interpretation of constrained utility maximization, thus rendering the second point largely irrelevant.====However, recent literature in psychology, decision theory, and economics is replete with behavioral counter-examples to the utility maximization paradigm, showing that the theory has many failures even when interpreted as a mere description of behavior. These include direct violations of explicit axioms such as transitivity or Independence of Irrelevant Alternatives, as well as examples that violate implicit assumptions, such as the independence of reference points or the fungibility of money (see Kahneman, Tversky, 1979, Kahneman, Tversky, 1984, Thaler, 1980, Thaler, 1985, and others).====In this paper we point at a stylized fact that is also incompatible with the classical model of consumer behavior. We provide an example that suggests that consumers are not aware of all the possible bundles they may choose among. We argue that this limited awareness cannot be dismissed as a mere mistake, due to the vast set of possibilities. In order to support this argument we discuss formal complexity results, which suggest that this type of “bounded rationality” is inherent to consumers’ decisions in an affluent society. We then introduce two implications of these results.",The complexity of the consumer problem,https://www.sciencedirect.com/science/article/pii/S1090944321000016,6 January 2021,2021,Research Article,75.0
"Yang Guanyi,Casner Ben","Economics Department, St. Lawrence University United States,Federal Trade Commission, United States","Received 18 October 2020, Revised 13 November 2020, Accepted 25 November 2020, Available online 3 December 2020, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2020.11.004,Cited by (0),"Studies on structural education choice models are often inconsistent in choosing whether and how to include a disutility of education, especially in an environment with risk and ","Many structural models include a utility cost when an agent attends school, largely for the purpose of matching observed patterns in education choices.==== The theoretical implications and consequent implicit structural limitations imposed by introducing a disutility of education term to agents’ decision problems, are largely unknown, especially in a framework with uncertainty and inequality.==== We aim to fill this gap by creating a simple two-period decision model to investigate how the disutility term links wealth, risk, and education choices.====The primary message of this paper is that including disutility of education in a dynamic model breaks the direct association between financial return and utility return to education, allowing risk and precautionary saving motive to impact education choice. Education serves as a saving/insurance device that transfers current wealth into human capital for future earnings. A heightened future consumption risk further increases the expected consumption ==== in addition to any increase in lifetime income. If going to school yields a greater financial return than other saving vehicles, an increase in consumption risk implies an increase in the utility return to education ====. The agent is more likely to go to school. Without a disutility term, this increase in the utility return of education does not matter because the agent will just choose whichever education option maximizes her lifetime budget constraint.====We investigate two forces affecting education decisions in an environment of uncertainty: risk aversion reduces the appeal of the option with higher relative risk, while a rise in precautionary saving increases the importance of a higher financial return. If, for example, the variance of consumption is higher after schooling instead of working in the first period, schooling will be less attractive than if it had the same risk as working. However, if both schooling and working experience an increase in risk, but to different degrees, agents will increase their precautionary saving differently for the different options. Focusing on the case where schooling has a positive financial return, if working becomes riskier relative to schooling , then both forces compound and the utility return to education increases further. If the relative risk of schooling increases, but only moderately, the precautionary saving motive dominates the effects of risk aversion and attending school still becomes more appealing despite the increase in relative risk. Even if the increase in risk after schooling is sufficiently large so that the risk aversion factor is dominant, its influence will be partially counteracted by the precautionary saving motive.====Our results build on Belley and Lochner (2007), who show that concave consumption utility implies that the utility returns to education decrease in wealth. The absolute difference between consumption with and without education is constant, but the proportional difference in consumption is smaller with higher base wealth. Consequently, holding human capital constant, a wealthier individual who faces disutility of education will be less inclined to go to school than one currently holding no assets because of the smaller increase in utility. Similarly, if education provides positive utility then a wealthy agent will be more likely to attend school than one with no initial endowment if doing so provides a negative financial return.====The above results demonstrate that adding a disutility term in a structural education model embeds these connections between wealth, risk, and education into any counterfactual simulations. Causal inference requires an identification of which results are assumed and which come from the data. The literature is often less than totally cautious in considering the inclusion and estimation of the disutility term. If disutility exists in the data, omitting the disutility term in a model may underestimate the impact of wealth and risk on education choices.==== Similarly, projects which include a disutility term but who do not discipline the term with wealth and risk related data create an inconsistency between the assumptions used to estimate the disutility term and the implicit assumptions in the model used to analyze its implications.====In addition to their methodological significance, the above connections from risk and wealth to human capital investment through disutility provide an alternative explanation for intermittent schooling choices documented by the recent empirical literature (e.g. Arcidiacono, Aucejo, Maurel, Ransom, 2016, Dynarski, 1999, Jepsen, Montgomery, 2012, Johnson, 2013, Light, 1995, Light, 1995, Monks, 1997, Seftor, Turner, 2002, Yang). Changing wealth/human capital conditions, the relative disutility of schooling, and relative risk between work and education can induce agents to enter or exit school. Our results can also be extended to a variety of choices. For example, in college major choices, a larger financial return is required to induce an agent to choose a major with higher risk or larger disutility. This adds a new angle to the literature on the Roy Model of college major choice by suggesting sorting based on differential risks and disutilities across choices and agents as well as financial return.",How much does schooling disutility matter?,https://www.sciencedirect.com/science/article/pii/S1090944320303926,3 December 2020,2020,Research Article,76.0
Castelluccio Marco,"Department of Economics, University College London, 30 Gordon Street, London WC1H 0AX, United Kingdom","Received 2 October 2020, Accepted 25 November 2020, Available online 28 November 2020, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2020.11.002,Cited by (1),"This work studies the actual degree of progressivity in the Italian tax and transfer system and examines possible reforms towards the optimum. It analyzes the distribution of personal income and effective tax rates across the Italian population, computing income and tax liabilities from survey data, and studies the optimal level of progressivity. To this end, it uses a model developed in Heathcote et al. (2017) with heterogeneous agents where skill investment and labor supply are endogenous and the government provides a public good under a balanced budget. All the main tradeoffs that shape optimal progressivity appear: the presence of ","What is the actual level of progressivity in the Italian personal income tax (PIT) system? How can policymakers reform the system towards the optimum? These are two important questions that are frequently debated in the Italian academic and public spheres. Insights into these two questions, however, hold relevance beyond the Italian perspective. Italy represents a particularly interesting case because of some of its features: tax rates on labor income are among the highest in the EU, the ratio between tax revenues and GDP is well above the average for the OECD countries (43% versus 34%), some income categories within the PIT framework are subject to substitute taxes which are proportional and therefore decrease the actual level of progressivity, and estimates of tax evasion suggest that 2% of the Italian GDP is constituted by missing PIT revenues. Despite these inefficiencies, Italy is not considered to be a country with high income inequality.====This research has two main goals. Firstly, it aims to characterize the distribution of several relevant variables, such as before-tax and after-tax income, and provide estimates of effective tax functions. These tax functions describe in a simplified way the Italian PIT system while preserving the heterogeneity observed in data, and are thus valuable tools for working with macroeconomic models with heterogeneous agents. This work gauges the impact of the Italian tax system on income inequality and shows the difference between statutory and effective tax rates. Second, it determines the optimal degree of progressivity for the Italian economy and identifies the different forces that shape it. To this end, it employs an analytically tractable equilibrium model with heterogeneous agents who endogenously choose their skill investment and labor supply. Since some of the assumptions in this work are quite heavy, the reader should take the results with a grain of salt. The results are informative in that they provide a reliable theoretical framework for studying the determinants of optimal progressivity, but they should not be taken at face value.====The data used for this research comprises survey data from two main sources: the European Union Statistics on Income and Living Conditions (EU-SILC), conducted by ISTAT, and the Survey on Household Income and Wealth (SHIW), conducted by the Bank of Italy.====The key takeaways from the descriptive part of this work are the following. First, income is unevenly distributed across Italy; living standards are higher in the Northern and Central areas as compared to the Southern area and the Islands. Second, income inequality is present both at the individual level and at the household level. Thus, the Italian PIT helps in redistributing income since the Gini coefficient of disposable income is six percentage points lower than that of gross income. Third, tax liabilities are heterogeneously distributed. Indeed, the vast majority of individuals at the very bottom of the income distribution do not face positive tax liabilities and among those who do face positive tax rates the average effective tax rate is below 10%. In contrast, effective marginal tax rates increase steeply around the middle of the income distribution and stay approximately flat at the top.====Concerning the normative part of the analysis, the model, developed in Heathcote et al. (2017), features all the main trade-offs of progressive taxation. In particular, optimal progressivity is shaped by the counteracting forces of skill investment and labor supply, which are distorted by progressivity, labor market uncertainty and redistribution, which are addressed by a progressive tax system, and private risk sharing which, being incomplete, requires government intervention. The presence of public goods increases the social cost of a progressive system. The Italian tax and transfer PIT system is proxied by a two-parameter functional form firstly introduced by Feldstein (1969) and Benabou (2002). The resulting actual progressivity measured using Italian data is 19%, slightly higher than the US estimate and considerably higher than the optimal degree predicted by the model, which is 7.1% under the baseline specification. The social welfare gains derived from adopting the optimal level of progressivity are equivalent to almost 1% of lifetime consumption. This reform can be approximated by a flat tax at 29% with a standard deduction of € 7,600. Positive progressivity is still embedded in this system thanks to the standard deduction, while labor supply and skill investment distortions diminish. Obviously, such a reform would have some social costs. Although each individual would be subject to a lower average tax rate, the flat tax would benefit the rich substantially more than middle-income individuals.====This research contributes to two main literatures. First, it contributes to a growing literature on the effects of PIT on the distribution of individual and household income and the differences between statutory and effective tax rates. García-Miralles et al. (2019) and Guner et al. (2014) provide parametric estimates of the tax functions using data from the US and Spain, while Curci et al. (2017) and Di Caro (2018) conduct studies on the Italian PIT using data sources different from EU-SILC. Second, this work also contributes to the literature investigating the determinants of optimal tax progressivity. Apart from Heathcote, Storesletten, Violante, 2014, Heathcote, Storesletten, Violante, 2017, to which this work is deeply indebted as it inherits the analytical framework, Krueger and Ludwig (2013) and Guvenen et al. (2013) investigate the distortionary effects of progressivity on labor supply and skill investment. Conesa and Krueger (2006) develop an environment which is comparable to the one used here but that cannot be studied in closed form. Others, without restricting the functional form of taxes, in the spirit of Mirrlees (1971), focused on human capital accumulation (Stantcheva (2017)), labor productivity shocks (Farhi and Werning (2013); Golosov et al. (2016)) and imperfect substitutability across skills (Rothschild and Scheuer (2013)).====This work is organized as follows. Section 2 provides a concise description of the Italian tax code and Section 3 describes the datasets used in this work, together with their limitations and restrictions. Section 4 analyzes the individual and household income distribution and effective tax rates. Section 5 briefly describes the model and its main trade-offs. Section 6 presents the quantitative results of the model, showing the optimal degree of progressivity predicted for the Italian economy. Finally, Section 7 concludes.",Flat tax? Maybe not a bad idea after all,https://www.sciencedirect.com/science/article/pii/S1090944320303902,28 November 2020,2020,Research Article,77.0
"Lambertini Luca,Marattin Luigi","Department of Economics, University of Bologna, via S. Giacomo 1, Bologna 40126, Italy","Received 7 October 2020, Revised 13 November 2020, Accepted 25 November 2020, Available online 27 November 2020, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2020.11.003,Cited by (0),"We revisit the discussion about the relationship between price’s cyclical features, implicit collusion and the demand level in an ==== supergame where a positive shock may hit demand and disrupt collusion. The novel feature of our model consists in characterising the post-shock noncooperative price and comparing it against the cartel price played in the last period of the collusive path, to single out the conditions for procyclicality to arise both in the short and in the long-run. This poses an issue in terms of an antitrust agency’s ability to draw well defined conclusions on the firms’ behaviour after the occurrence of the shock, with particular reference for the litigation phase after a cartel breakdown.","Rotemberg and Saloner (1986), R&S henceforth) argue that oligopolies are likely to behave more competitively when demand rises, especially under price competition and product homogeneity. Hence, pricing behaviour exhibits a countercyclical pattern.==== We modify their framework in three directions: (i) the presence of product differentiation;==== (ii) the possibility for firms to collude, after the occurrence of the demand shock, on virtually any price between the monopoly price and the Nash equilibrium one; and (iii) the role of the state of demand before the shock hits the industry, and the size of the shock itself. We show that in this framework the traditional R&S result no longer holds, and procyclical pricing may emerge depending on the state of demand before the shock hits, and the size of the demand shock. We do so under Bertrand and Cournot behaviour, comparing the collusive price under two alternative demand states (“low” and “high”) with the Nash price charged after the occurrence of a positive demand shock affecting the high demand state and triggering the output’s cyclical movement. This specific aspect of our model relies on Tirole (1988) exposition of R&S and has the same flavour of an analogous assumption in Spiegel et al. (2014).====Our results show that in such a framework the traditional countercyclical result may indeed flip over depending on the interplay between the size of the shock, the demand level observed in the last collusive period and the market variable being set by firms. More precisely, the larger the shock, the higher the tendency towards the emergence of a procyclical pattern. This happens because the natural countercyclical tendency due to competitive behaviour is offset by the larger market size, thereby increasing the Nash equilibrium price. At the same time, as the difference between the pre-shock demand states shrinks, countercyclicality arises, as the model takes an increasingly R&S-like flavour.====In this respect, a few additional remarks are in order. As noted by Tirole (1988) p. 250), if one compares monopoly price in the low demand state with the Nash price in the high state, it is indeed possible that the price is higher during booms, whereby what R&S indeed refer to is the underlying amount of collusion (or lack thereof). Consequently, the same is necessarily true for infinitely many partially collusive prices. Without explicitly modelling partial collusion, our results imply that, if post-shock noncooperative pricing is procyclical, then by continuity any post-shock collusive price which could be sustained by firms is necessarily procyclical as well. In such a case, the disruption of collusion gives rise to a new price path whose procyclical pattern may make it very difficult (if not altogether impossible) for an antitrust agency to detect whether firms are colluding anew in high demand states or not, as the increase in price might be simply due to the shock. This has some implications about the interpretation of price increases during litigation, as a way of inducing underestimation of damages (cf. Harrington, 2004).====The remainder of paper is structured as follows. Section 2 lays out the model. Sections 3 and 4 illustrate the Bertrand and Cournot supergames, respectively. Section 5 summarizes our results by carrying out a comprehensive analysis of price cyclicality in the post-shock period only, while Section 6 looks at the long-run cyclical properties of pricing. Section 7 concludes.",On prices’ cyclical behaviour in oligopolistic markets,https://www.sciencedirect.com/science/article/pii/S1090944320303914,27 November 2020,2020,Research Article,78.0
"Chantreuil Frédéric,Fourrey Kévin,Lebon Isabelle,Rebière Thérèse","University of New Caledonia, LARJE, Ateliers DEG, BP R4 98851 Nouméa Cedex, New Caledonia, and TEPP-CNRS, France,CEET-CNAM, 29 promenade Michel Simon, 93160 Noisy-le-Grand, LIRSA and TEPP-CNRS, France,CREM-CNRS, University of Caen Normandy, MRSH, Esplanade de la Paix, 14032 Caen Cedex, and TEPP-CNRS, France,Lirsa-CNAM, IZA, Ceet and TEPP-CNRS, Department of Economics, 40 rue des Jeûneurs, Case 1D2P30, Paris 75002, France","Received 15 September 2020, Revised 31 October 2020, Accepted 9 November 2020, Available online 10 November 2020, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2020.11.001,Cited by (1),"This paper studies individual characteristics of earnings inequality within the population of blacks and whites in the United States over the period 2005–2017. Beyond education and age serving as a proxy for professional experience, applying a new Shapley income decomposition methodology enables us to isolate and measure two discriminative factors in earnings differences: race and gender. We show that these two factors explain a significant share of total earnings inequality, as defined by the ====, for all the geographical administrative divisions used. Whatever the division, the share of earnings inequality associated with gender greatly exceeds that of race. While gender earnings inequality has fallen over time, inequality associated with race has tended to increase since 2010 and is stronger in the Southeast of the country.","Race and gender inequality is a hot topic in the United States. A nationwide poll conducted by CNN and the Kaiser Family Foundation in 2015 found that 49% of US citizens think racism is “a big problem” in society today, whereas the figure was “only” 28% in 2011. Also in 2015, a Gallup’s Minority Rights and Relations survey showed 67% of Americans supported affirmative action for women while slightly fewer, 58%, for minorities. These issues are not new. Title VII of the Civil Rights Act of 1964 prohibits employment discrimination based on race, color, religion, sex and national origin. In 1965, President Johnson signed an executive order promoting affirmative action that ensured that all applicants and employees would be considered with no distinction of race, creed, color or national origin. Since then, these measures have been strongly criticized by conservatives, and several states have opted to take action against positive discrimination policies in public institutions. The importance that these issues have maintained in public debates underlines the need for convenient indicators that are easy to use by public policy makers.====Existing income decomposition literature most often focuses on decomposing the evolution of income inequality over time or the difference in inequality between two groups (see Fortin et al., 2011 for a literature review or Montes-Rojas et al., 2017 for a recent application). Our contribution, on the other hand, proposes a practical Shapley decomposition tool that is able to decompose the inequality of income distribution by individual characteristics (hereinafter referred to as attributes). Our methodology has the advantage of not requiring choosing a modality of reference for categorical variables (e.g. level of education, gender or race) known to affect the results (see Oaxaca and Ransom, 1999 for a discussion). Moreover, contrary to the distribution decomposition by income sources proposed by Lerman and Yitzhaki (1985), our methodology is suitable for a decomposition by attributes and is applicable to a wide range of inequality indexes: Gini, Theil, Atkinson, etc. Finally, our decomposition does not stand on parametric models and their related hypotheses (as it is the case in Björklund et al., 2012). To the best of our knowledge, there is no other decomposition instrument in the literature for carrying out inequality index decomposition by attributes that presently provides all of these advantages. Our contribution is an application of this new Shapley income decomposition methodology in which we concentrate on the discriminatory part of inequalities observed through two subjective factors in income differences - race and gender - that contribute to the inequality in earnings (wages and self-employment revenues), as defined by the Gini index within the population of Blacks and Whites taken together for each of the geographical administrative divisions of the United States over the period 2005–2017.====Literature on racial and gender inequality often concentrate on the income gap between groups, ==== comparing different ethnic groups or women versus men. From this point of view, Sites and Parks (2011), and Couch and Daly (2002) show that racial income inequalities in the United States were significantly reduced following the passage of the Civil Rights Act of 1964 and other measures aimed at reducing labor market discrimination, but have not changed significantly since 1974, with the Black–White wage gap remaining around 30% to the late 1980s, while a new convergence was observed in the 1990s. More recently, the median annual income of a family in 2014 was $76,658 for whites and $43,151 for blacks, the second-largest ethnic minority after Hispanics (Economic Report of the President, 2016). As a matter of allocation, decades of sociological research has shown that Black–White inequality in local areas is greater where the Black population is relatively large (see, for instance, Huffman and Cohen, 2004). Using the decomposition techniques of Couch and Daly (2002); Juhn et al. (1991) found that greater occupational diversity among Black workers partly explained the reduction of the racial income gap during the 1990s. Convergence is partly due to equalization in the attainment of education and experience and also to the distribution of employment across industries and occupations, rather than being a purely ethnic matter. Sections of the literature explain part of the recently observed income inequality by the reduction in unionization rates. Blacks’ private-sector unionization rates surpassed those of White workers for decades (see Jones and Schmitt, 2014). Using data from the CPS (1973–2007), Rosenfeld and Kleykamp (2012) showed that African-Americans join unions for protection against discriminatory treatment in nonunion sectors. Private-sector union decline has exacerbated Black–White wage inequality (see also Rosenfeld, 2014), especially among female workers. The gender wage gap has been intensively investigated for decades, but still remains an area of active research (see Blau and Kahn, 2017). While the long-term trend shows a significant reduction in the wage gap, convergence has been slower and uneven since the 1990s. Income decomposition methods show that the gender distribution with respect to occupation and industry is the main factor explaining this gap nowadays. Still, gender differences in work interruptions and working times are significant sources of income gaps.====Our contribution differs from the previous literature by concentrating on racial and gender inequality as defined by the contribution of race or gender to overall inequality. In doing so, applying decomposition methods to measure earnings inequality seem to provide an attractive framework for the appraisal of inequality associated to these two characteristics. Among income decomposition methods, those inspired by the Shapley value seem particularly interesting since they provide an explanation of income inequality by determining the contribution of various income sources (==== labor and capital incomes), or the contribution of different sub-populations to overall inequality.==== The Shapley decomposition methods developed so far cannot, however, be used to estimate the share of overall wage inequality that is due to a characteristic such as race or gender. In fact, if the two sub-populations are made up of Blacks and Whites respectively, the results of the decomposition will give the contribution of Blacks to overall income inequality on the one hand and the contribution of Whites to overall inequality on the other, ==== inequality observed within each sub-population. But this contribution does not reflect inequality between sub-populations, ==== between Blacks and Whites. Hence, this decomposition framework does not provide for the determination of the contribution of race to income inequality. In order to make up for this drawback of the “classical” Shapley decomposition rule, Chantreuil and Lebon (2015) extended this framework to a third dimension, namely the decomposition of income inequality by attributes. Defining the earnings received by an individual as the sum of several elements, each element representing the part of earnings resulting from each individual’s attribute, the Shapley decomposition rule then offers a simple way to determine the contribution of each of individuals’ attributes to overall earnings inequality. Here, the order of attributes matters. Our contribution adds to this latter literature as we do not pre-assume the attributes’ ordering.====The Shapley decomposition method enables us to distinguish inequality arising from several attributes, some of which are linked to intrinsic individual productivity such as the length of professional experience - for which age is a proxy - and education level, from inequality linked to discriminative factors such as race and gender. Focusing on the latter, we find that the share of earnings inequality attributable to racial affiliation is about 1% to 4%, depending on the 9 designated divisions of the United States Census Bureau, while the contribution of gender to the total observed inequality in the Black and White populations taken together is much larger, at 9% to 13%. Time comparison shows that the contribution of race to earnings inequality has tended to increase in all administrative divisions over the past decade, whereas that of gender tends to have been decreasing recently.====This paper is organized as follows: Section 2 presents the data and empirical evidence on earnings inequality in the United States over the period 2005–2017, focusing on gender and race. Section 3 outlines the Shapley decomposition methodology according to conditional decomposition. Section 4 analyzes the results. Section 5 empirically discusses the incidence of the ordering of attributes. Finally, Section 6 contains some concluding comments.",Magnitude and evolution of gender and race contributions to earnings inequality across US regions,https://www.sciencedirect.com/science/article/pii/S1090944320303835,10 November 2020,2020,Research Article,79.0
Oyèkọ́lá Ọláyínká,"INTO University of Exeter, Stocker Road, Exeter, EX4 4PY, UK","Received 24 July 2020, Accepted 27 October 2020, Available online 1 November 2020, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2020.10.010,Cited by (2),"Can historical exposures of non-European countries to European migrants explain part of their current health outcomes? We find that higher European share of the colonial population robustly raised life expectancy and reduced both fertility and infant mortality rates of present-day population in these former colonies. Specifically, after controlling for other plausible determinants, our baseline results imply that, on average, countries at the 95th percentile of the European share of the colonial population, compared to those at the 5th percentile, live 17 years longer, have 1 less child, and experience 54 fewer infant deaths per 1000 live births. A causal interpretation is given to these results by considering various identification strategies. Overall, our results indicate that health fortunes around the world, on average, improved because of European colonial settlers and that differences in the current levels of health performance can be traced back to differential levels of European colonial settlements, where countries that experienced higher influx of colonial Europeans have better health prosperity nowadays than countries with lower inflow of colonial Europeans. A puzzlement arises, however, as countries with no colonial European settlements have outperformed countries with low colonial European settlements. Thus, explaining this phenomenon and exploring how historical migration holds such an enduring influence on the health of nations today opens up an important avenue for future research.","Was colonisation a blessing or a curse? In a recent paper, Easterly and Levine (2016), henceforth EL, have documented factors, such as local population density in CE 1500, latitude, unfavourable disease environment, and indigenous mortality, which helped in shaping regions that were attractive to European settlers during the colonial epoch. They presented evidence that current income per capita of these countries still register the direct effects of the European share of the colonial population. To do this, EL constructed a new database on the European share of the colonial population for each year, starting in 1540, by combining data from various historical sources on European migratory successes to non-European countries.====Using this data, they find that the economic development of countries with a sizeable historic European share of the population is higher by about 40 per cent than it would have been otherwise. This result is strikingly large and appears to lend support to a corollary of the modernisation hypothesis (Lipset 1959), which claims that colonisation has boosted fortunes of the former colonies by integrating them into global economic, educational, socio-political, and technological systems. Moreover, EL submits that subsequent benefits from these exposures far outweigh the detrimental effects of limited European presence, leading to more extractive policies, that accompany colonial activities in some locations, as documented in the existing literature (e.g., Engerman and Sokoloff 1997, 2002; Acemoglu et al., 2001, 2002; Nunn 2008; Bruhn and Gallego 2012).====As income is only a facet of development, and conveys a people's living standards and welfare levels only in part, we focus on health performance, which is useful in moving away from the unbridled emphasis on income and in shining more light on another dimension of development: human biological well-being.==== Our perspective is similar to that expressed by the United Nations Development Programme, when they wrote in the first Human Development Report (HDR 1990, p. 9) that: “People are the real wealth of a nation. The basic objective of development is to create an enabling environment for people to enjoy long, healthy and creative lives… [as] income is not the sum total of human life.”====It is also constructive to point out that our focus on health should not be seen as an assault on the relevance of wealth, rather we contend that it is useful to investigate more closely one of the “ends” for which the “means” are sought. On this, Cutler et al. (2006, p. 97) argue that: “The pleasures of life are worth nothing if one is not alive to experience them,” and Buchanan and Ellis (1955) also took this view as they correlated post-war economic outcomes and indicators of health performance—which include life expectancy, infant mortality, and food energy intake (i.e., calories consumed per person, per day).====Following this tradition, we ask different but complementary questions to EL: Was the health of non-European countries determined during their historical encounters with the European invaders? Is there a role for the share of Europeans in a population during colonisation to affect the health outcomes of nations today? Why, for example, do the Canadians, and the Japanese live longer, have fewer children and witness more newly-born babies survive till adulthood, while citizens of Ghana and Sierra Leone die younger, have more children, and are accustomed to being bystanders as their infants flatline before their first birthdays? If a country's position on the comparative health development spectrum depends on the extent of European colonial settlement, which colonised countries can be grateful they were?====We find that the European share of the colonial population is strongly positively related to life expectancy, whereas it is highly negatively associated with both fertility and infant mortality rates. Specifically, the direct effect of European share of the colonial population in the baseline regressions, after controlling for British legal origin, year of independence, and ethnic diversity, reveals that: (i) increasing the European share of the colonial population from the 5th percentile, a value of 0.001 for a country like Ghana, to the 95th percentile, a value of 0.659 for a country like New Zealand, is associated with (a) a 21% jump in life expectancy, (b) a 0.4% reduction in fertility rate, and (c) a 73% slump in infant mortality rate; (ii) increasing Ghana's European share of the colonial population to match the frontier value of 0.905 (computed for Canada) implies that the former's (a) life expectancy would rise by 17 to 75 years, (b) fertility rate would drop by 3 to 2 children, and (c) infant mortality rate will fall by 54 to 10 deaths per 1000 live births; and (iii) looking at the two continents with the least (Africa) and most (Oceania) European share of the colonial population, our finding indicates that 23% of the disparities in current life expectancy, 34% of the disparities in current fertility rate, and 29% of the disparities in current infant mortality rate between these two areas can be explained by their European share of the colonial population.====Moreover, noticing that Asia performed much better on all counts of health outcomes, though having on average European share of the colonial population far less than that in Africa, we divide countries in our sample into those with high, moderate, low, or no European share of the colonial population, and ask whether the magnitude of the European share of the colonial population matters for it to be rewarding or harmful. We find that: (i) countries with high European share of the colonial population enjoy greater life expectancy today than countries with moderate, low, and no European share of the colonial population, of 9.22%, 19.5%, and 13.1% respectively; and (ii) countries with low European share of the colonial population have fared worse than countries with no European share of the colonial population, having lower life expectancy, of 6.4%, higher fertility rate, of 14.7%, and a greater infant mortality rate, of 24.5%. This finding, thus, supports the view that distinctions in the forms of distributions of European share of the colonial population at the onset have had an enduring effect that is still detectable in the contemporary dispensation of health measures across the non-European countries.====Although the estimates indicate substantial economic consequences, we may not conclusively interpret them as evidence that European share of the colonial population causes higher life expectancy, lower fertility rate, and reduced the infant mortality rate. A number of reasons can be provided for this: (i) it is plausible that some of the non-European countries were already on the path to the levels of health outcomes that we see today prior to European infiltration, such that dissimilarities in a variety of observed and unobserved country characteristics drive the differences in health development; (ii) measurement errors may also be biasing our estimates; and (iii) including outlying observations may importantly influence our results. We affirm that our results are robust to controlling for an array of observable country characteristics, assessing the bias from unobservable country heterogeneity, omitting outliers, employing an alternative estimation technique, and using different dates to measure health outcomes and European share of the colonial population.====In seeking answers to the above queries, we believe that our paper contributes to two streams of economic literature. First, our paper is related to the stream that endeavours to assimilate the influences of historical and prehistorical factors in causing differences in the levels of current socio-economic development (Diamond 1997; Engerman and Sokoloff 1997; 2002; La Porta et al. 1997, 1998; Acemoglu et al., 2001; 2002; Galor and Moav 2007).==== The work of Galor and Moav (2007), with which our paper accords well, for example, advances an evolutionary theory that socio-economic and environmental changes taking place in the time elapsed between the Mesolithic and the Copper periods impacted on the nature of the environmental hazards confronted by the human population, and that this episodic experience activated an evolutionary process that has had a significant long-term effect on contemporary variations in life expectancy. They then provide empirical analysis showing that a substantial portion of the current differences in longevity across countries (1.6 to 1.9 years, after controlling for national geographical and continental characteristics, as well as income, education, and health expenditure per capita) can be traced back to differences in the time passed since the ancestors of the population in a country underwent the Neolithic revolution.====Our paper adds to this theoretical stance by investigating a different source of historical persistence affecting the health of nations today. In particular, we study whether there is (or not) a marked positive and consistently robust interconnectedness between European colonial settlers and present-day health outcomes. For this purpose, we combine EL's dataset on European share of the colonial population with three standard cross-nationally-available measures of a country's health status (life expectancy, fertility rate, and infant mortality rate) and explore what the long-term effects of Europeans selecting to settle in other regions of the world, starting around 500 years ago, are on the host countries’ health outcomes today.====Additionally, our research fits with the economic literature on integration. This stream of research has conventionally focussed on integration through contemporary trade openness (or migration) and its effects on, for example, economic performance (e.g., Grossman and Helpman 1991; Frankel and Romer 1999; Greenaway et al., 2002; Dollar and Kraay 2003; Ortega and Peri 2014; Oyekola 2020). In a parallel stream, which has grown in intensity in more recent times, interest has been focused on other forms of integration across both time and space and their impacts on economic progress, offering a new breed of fundamental determinants of economic development (e.g., Comin and Hobijn 2004; Hibbs and Olsson 2004; Giavazzi and Tabellini 2005; Spolaore and Wacziarg 2009; Cervellati et al., 2018; Özak 2018). EL, Acemoglu et al. (2001, 2002), and Putterman and Weil (2010) mirror brightest the spirit of our work.====The rest of the paper is structured as follows. In Section 2, we review some background literature and place our paper within the existing literature. Section 3 describes the data and offers preliminary correlations between the European share of the colonial population and our indicators of health development. In Section 4, we present our main results and provide estimates from several extensions and sensitivity checks. Finally, we offer concluding remarks in Section 5, including a suggestive agenda for future research.",Where do people live longer?,https://www.sciencedirect.com/science/article/pii/S1090944320303811,1 November 2020,2020,Research Article,80.0
Furno Marilena,"Università degli Studi di Napoli Federico II, Italy","Received 3 September 2020, Accepted 27 October 2020, Available online 31 October 2020, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2020.10.009,Cited by (0),"The synthetic control approach to analyze policy impact is here complemented to compute i) multivalued ==== and ii) treatment effect evaluated at different levels of the distribution of the treated, at the various quantiles. Confining the analysis to binary treatment and/or to average values alone can be very misleading since treatments can be multivalued and they can have different impact away from the mean, in the tails. Two case studies and a set of simulations allow to investigate the behavior of the proposed approach.","Comparative case study analyzes the impact of policy intervention. Generally, the treatment effect is measured by comparing the outcome of the treated versus the control group (untreated) in a binary case, i.e. comparing units in the presence versus unit in the absence of treatment, and their difference, the treatment effect, is usually computed at its average value. This study discusses the generalization of the treatment effect analysis to i) multivalued rather than simple binary treatments and ii) its implementation not only at the mean, as it is generally done, but also at the lower and higher levels of the distribution of the treated, the quantiles. Confining the analysis to average values, indeed, can be misleading, since policy effectiveness may vary in the tails, for low/high values of the outcome. Multiplicity of treatment conditions allows to analyze, for instance, the case of different lengths in a training program, or different types of policy interventions (financial support, investments, etc.,) or in clinical trial the effect of different drugs or diverse drug doses.====In general, comparative case studies involve some ambiguity in the selection of the comparison units, often chosen implementing subjective measures of affinity between treated and control. This subjectivity induces uncertainty about the ability of the control group to reproduce the counterfactual outcome of the treated units, i.e. the value the treated units would assume in absence of the policy measure. The treatment effect is the difference between the observed treated and the counterfactual. The latter is not observed and must be approximated. To this end Abadie and Gardeazabal (2003) and Abadie et al. (2010, 2015) define the synthetic control approach. Their method considers data-driven procedures to select the control units to compute the counterfactual, thus reducing the discretionality involved in the comparison of observations and counterfactuals. The approach computes affinity between the affected and unaffected units by means of quantifiable characteristics.====The idea behind synthetic control is that a combination of control units to approximate the no-treatment outcome for the treated, the counterfactual, provides a better comparison than a single unit. This approach moves away from using a single control unit or a simple average of control units and uses a weighted average of the selected set of controls. By selecting a weighted average of the control units to compute the counterfactual, it shows the contribution of each control unit and the similarities between the treated and the control.====In what follows this approach is defined within the quantile regression framework, to measure treatment effect away from the mean. A case study shows that the treatment effect does indeed change at the quantiles. Academic schools have a positive impact of on students’ performance that declines across quartiles, and low scoring students show the greatest improvement. Vice versa, vocational schools have a negative impact of on students’ proficiency. The damage is graver in science for the best students, while it decreases across quartiles in math and reading, thus causing the greatest harm for low scoring students in these two fields.====The simulations show that the synthetic control at the quantiles provides good results in the presence of outliers, kurtosis and serial correlation. Vice versa, it loses efficiency in the presence of heteroskedasticity and skewness.====Section 2 reviews the standard synthetic control approach while Section 3 moves the focus toward the tails, looking at its computation at the quantiles. Section 4 investigates how to analyze a multivalued treatment. A case study, simulations and the conclusions complete the analysis.",The synthetic control approach: Multivalued treatments at the quantiles,https://www.sciencedirect.com/science/article/pii/S109094432030380X,31 October 2020,2020,Research Article,81.0
Martínez-Sánchez Francisco,"Departamento de Métodos Cuantitativos para la Economía y la Empresa, Universidad de Murcia, Murcia 30100, Spain","Received 27 September 2020, Accepted 27 October 2020, Available online 29 October 2020, Version of Record 13 March 2021.",https://doi.org/10.1016/j.rie.2020.10.008,Cited by (0),"In a model à la ==== in which consumers are loss-averse, I check the robustness of the result obtained by ","Behavioral economics has shown that humans are averse to losses (Tversky and Kahneman, 1991). This means that the pain of a loss is greater than the pleasure of a gain of equal size. This discovery has been incorporated into recent models that analyze the pricing strategies of firms.==== However, the seminal paper that analyzes the choice of a price or a quantity contract, Singh and Vives (1984), shows that the price contract is a dominant strategy for each firm if the goods are complements; otherwise, the dominant strategy is the quantity contract.==== Moreover, in a model à la (Mussa and Rosen, 1978), Tanaka (2001) shows that the quantity contract is a dominant strategy for each firm. To my knowledge, there are no papers that analyze how the loss aversion of consumers affects competition on quantities. The objective of this paper is to analyze whether the quantity contract remains a dominant strategy in a vertical differentiation model in which consumers are loss averse.====Heidhues and Köszegi (2008b) find that consumers’ loss aversion increases the intensity of competition in a horizontal product differentiation model. Karle and Peitz (2014) modify that model and find that loss aversion in price is procompetitive, while loss aversion in taste is anticompetitive. These papers consider that the reference point arises endogenously, but I assume that it is determined exogenously as in Zhou (2011), who finds the same results as Karle and Peitz (2014).====Recent papers have developed monopoly models with vertically differentiated products and loss-averse consumers. In that framework, Carbajal and Ely (2016) study optimal price discrimination when consumers have reference-dependent preferences for the quality of the product. They find that optimal price discrimination may show efficiency gains relative to second-best contracts without loss aversion. Hahn et al. (2018) consider that consumers have reference-dependent preferences for the quality and price of the product. They show that offering menus with a small number of bundles is consistent with profit-maximizing firms that deal with loss-averse consumers. Courty and Nasiry (2018) apply loss aversion within a class of products of the same quality but not across quality classes. They show that uniform pricing can be optimal across quality classes up to a quality threshold. Finally, Martínez-Sánchez, 2020 analyzes how the loss aversion of consumers affects the strategies of the government and the incumbent for preventing commercial piracy. He finds that those models that do not take into account the loss aversion of consumers overestimate the government’s effort to deter piracy, but underestimate the incumbent’s effort.====In this paper, I study the choice of a strategic variable (price or quantity) in a duopoly model of vertical product differentiation in which consumers are loss-averse. I show that Cournot is the outcome in equilibrium, and that loss aversion in general intensifies competition.====The rest of the paper is organized as follows: Section 2 describes the model formally. Section 3 presents the equilibrium. Finally, Section 4 concludes.",Price versus quantity in a duopoly of vertical differentiation with loss-averse consumers,https://www.sciencedirect.com/science/article/pii/S1090944320303793,29 October 2020,2020,Research Article,82.0
"Zhang Jiarui,Xu Xiaonian","Department of Finance, Accounting, and Economics, Nottingham University Business School China, Trent Building 362, 199 Taikang East Road, Ningbo 315100, China,Department of Economics and Decision Science, China Europe International Business School, Shanghai, China","Received 11 July 2020, Accepted 12 October 2020, Available online 22 October 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.10.001,Cited by (1),This paper presents a ,"What caused the financial crisis of 2008? There are two schools of thought. One, represented by the erstwhile Chairman of the Federal Reserve, Ben Bernanke, blames financial deregulation and the consequent excessive risk taking in the form of financial innovations (Bernanke, 2010). The other, advanced by John Taylor (Taylor, 2008, 2012) and others, holds monetary policy as the primary contributing factor for the unprecedented U.S. housing boom that eventually led to the crisis. Bernanke defends the Fed's policy prior to the crisis as nothing unusual, whereas Taylor notes considerable deviations from the dictates of the Taylor rule (Taylor, 1993) between 2001 and 2005.====Dokko et al. (2009) provide the most comprehensive defense of the Fed's monetary policy to date. The authors run simulations of the Federal Reserve Board/U.S. (FRB/U.S.) model with actual federal fund rates as the inputs. They compare the simulation results with those when the interest rates are set according to the Taylor rule. The level of housing market activities (measured by residential investment/GDP) under actual policy rates is only slightly higher than that under the Taylor rule (see Edge et al. (2009) for the FBR/EDO model and similar conclusions). They also find an insignificant correlation between monetary policy and housing price. Based on these findings, the authors suggest that the major causes of the housing boom should be found elsewhere, for example, in unconventional financial products and financial deregulation.====However, both FRB/U.S. and FRB/EDO models feature no financial frictions. By contrast, we develop a model that explicitly features household mortgage borrowing and re-investigate the relative importance of the two sets of factors, that is, monetary policy and financial market conditions, in fostering the housing boom.====Many features of our model draw from the financial accelerator models of BGG (1999) and Carlstrom and Fuerst (2000), and housing model of Iacoviello and Neri (2010). We differ from the latter in that we do not rely on an exogenous borrowing constraint but rather derive it endogenously to capture the mortgage delinquencies. We differ from the traditional financial accelerator model in that the borrowers in our model are risk-averse households rather than risk neutral entrepreneurs, as we focus on the U.S. residential housing boom.====Our model features two types of households, Savers and Borrowers. Using houses as collateral, borrowers make one-period mortgages and use their own net worth to finance their consumption and acquisition of houses. A representative bank draws the saving household's deposits and pays the household a risk-free interest rate (the benchmark or policy rate) set by the central bank. It then makes a mortgage loan to every borrower according to an otherwise classic mortgage contract that specifies the loan quantity and interest rate, which are determined by the borrower's net worth, among other things.====Our model simulations show that persistently low interest rates ==== can produce housing price boom of the scale comparable to that in the U.S. prior to the 2008 crisis. When the central bank cuts the benchmark interest rate, it affects the housing price through several channels. As savings with lower interest rate on deposits become less attractive, the saving household spends more on housing service and consumer goods, and the house price goes up immediately. In response to the rise in demand for consumer goods, the firm will increase production by employing more labor. Real wage rises, and it will lead to even greater demand for houses. Besides this traditional transmission mechanism of monetary policy, asset price is further reinforced by credit expansion of the bank. Borrowers respond to lower interest rates of mortgages by leveraging up. More importantly, with the windfall in net worth due to the appreciation of house value, borrowers have more to put up as collateral, and hence, they are able to borrow a greater amount. Meanwhile, as the expected default rate falls with improved investment returns in a bull market, the bank will offer more favorable terms, such as lower interest rates and less collateral, or equivalently, higher loan-to-value ratio (LTV). Monetary shocks are thus enlarged by the positive feedback between housing price and bank credit via the financial accelerator. As a result, a significant housing price boom emerges in our model.====We calibrate our model with the U.S. data and feed actual Federal Fund rates into the model for simulations. The model generates a housing price trajectory that mimics the movement of the Case–Shiller index fairly well. In contrast, when the monetary policy follows the Taylor rule, no dramatic asset price inflation can be observed.====We also conduct experiments to gage the significance of financial innovations, as compared to monetary policy, in creating housing boom. Massive use of mortgage-backed securities (henceforth, MBS) turn illiquid mortgages into liquid assets and allow banks to expand credit (Salmon 2010; U.S. Financial Crisis Inquiry Report 2011; Loutskina, 2011; Justiniano et al., 2015). The improvement in banks’ liquidity is captured in our model by an increase in supply of loanable funds for the bank or a positive shock to the loan-to-deposit ratio (henceforth, LTD ratio). To quantify the liquidity shocks, we divide the actual value of MBS issuance by U.S. household savings for the period 1997 to 2008. The extra liquidity provided by MBS was around 4% of household savings before 2001. It rose sharply in 2001q3 and continued the upward trend through 2003q3 to reach a peak of 14% (see Fig. 6 in Section 4). We model the liquidity effect of MBS to enlarge the household saving pool accordingly.====Model simulations under the MBS-induced liquidity shocks yield a housing price boom substantially smaller than that under the monetary shocks. The intuitions behind this result are not difficult to understand. The additional liquidity available does not affect the general taste of both borrowers and savers. Monetary easing, on the other hand, not only affects banks’ willingness to lend, but also affects the households’ willingness to borrow. A cut in benchmark policy rate directly lowers the interest rates on mortgage, and it also causes the household to substitute savings for housing and consumption, which adds to the upward pressure on property price. This substitution effect does not exist when MBS are used.====Having argued so, we warn against rushing to conclusions on the relative importance of monetary policy and MBS in creating asset bubbles. Restricted by the simple structure, we cannot include all effects of the two sets of factors into our model. For instance, MBS and other financial products may change risk preferences of market participants (Bies, 2004; Mian and Sufi, 2009; Nadauld and Sherlund, 2009; Schwartz, 2009). This issue is dealt with primitively in this paper by varying the Loan-to-Value (LTV) ratio exogenously. We treat more risk taking as equivalent to a higher LTV ratio of the bank or lower requirement for down payment (Duca et al., 2010). In doing so, we replace individual loan contracts in the baseline model with an aggregate and exogenous LTV ratio similar to Kiyotaki and Moore (1997), Iacoviello (2005), and Iacoviello and Neri (2010). The actual LTV ratio of the U.S. banking system was, by and large, stable between 1998 and 2001, with an average of around 0.87. It began to rise in 2002 and reached a peak of 0.97 in 2006. Had this ratio been applied to mortgages, it would mean nearly zero down payment, a practice often seen in the pre-crisis years. We plug the time series of real LTV ratio into our model and run simulations with interest rates set in line with the Taylor rule. The increases in the LTV ratio turn out to have no material impacts on housing price. The main reason lies on the demand side, where the Borrowers’ willingness to borrow is self-restrained because getting deeper into debt today would undermine their position to borrow in the near future.====We provide empirical and anecdotal evidence in Section 5 of this paper to support our theoretical results. Following a study of the Euro zone, we run a regression analysis for 11 Asian economies for the period 1990 to 1996, and we obtain a significant positive correlation between property price inflation and policy rate deviations from the Taylor rule. Housing booms in these economies have little to do with sophisticated financial products, and thus, they can be attributed mainly to monetary policies.====When interpreting the U.S. data, we refer to the literature to explore possible causalities between monetary policy and changes in the behavior of financial market players. The long period of low policy rate was associated with bank asset expansion and shifting toward riskier assets (Rajan, 2005, 2010; Ziadeh-Mikati, 2013). The Fed's own bulletin in 2010 admitted that, “Greenspan slashed interest rates and kept them too low for too long. Banks and shadow banks leveraged themselves to the hilt, loaning out money as if risk had been banished.” Loans were granted to riskier borrowers (subprime mortgages) at interest rates lower than what they could afford (McDonald and Stokes, 2013). Easy credit fueled the housing boom, and rising prices led to even looser standards of lending (Krugman 2009). Consistent with these arguments, MBS and sub-prime MBS data anomalies in the U.S. occurred ==== the Fed's interest rate policy began to diverge from the Taylor rule in the early 2000s. We are inclined to explaining, at least partially, these unusual phenomena as rational responses of financial institutions to monetary policies (See Zhang and Xu (2019) for a theoretical investigation of this argument).====The remainder of this paper is organized as follows. Section 2 briefly reviews the literature on the debate about this topic. Section 3 describes and calibrates the general equilibrium model in details. Section 4 reports simulation results, based on which we discuss the relative importance of monetary policy and financial innovations. Section 5 presents some empirical and anecdotal evidence to aid our theoretical analysis of the previous section. Section 6 concludes the paper.",The effects of the monetary policy on the U.S. housing boom from 2001 to 2006,https://www.sciencedirect.com/science/article/pii/S1090944320303720,22 October 2020,2020,Research Article,90.0
Yang Mingyi,"Washington State University, United States","Received 21 August 2020, Accepted 12 October 2020, Available online 20 October 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.10.006,Cited by (0),This paper is motivated by the fact that the ,"In literature (see King, Plosser, Stock, and Watson (1991), Prescott (1986), and Hansen (1997)), the standard approach of decomposing TFP into stochastic trend and cyclical component is as follows:====where ====denotes TFP at time ====, the innovations, ====, are assumed to be independent and identically distributed with a mean of 0 and a variance of ====. The parameter ====represents the average rate of growth in TFP; ====represents deviations of actual growth of TFP from this average. Obviously, ==== is nonstationary because it follows a random walk with drift. However, the cyclical component of ====, ====, is stationary. Therefore, in real business cycle model, we usually use the cyclical component of TFP, ====, instead of the non-detrended TFP, ====. The method shown above can be rewritten as====Thus, ====can be estimated as====and ====thus can be estimated as====From the expression above and using the US aggregate yearly TFP data over 1977 to 2007 from EU KLEMS, we can calculate the standard deviation of ==== as====However, this paper argues that the standard deviation of US yearly cyclical TFP of 0.012 is not even close to the true value. In fact, 0.012 is very close to the standard deviation of US quarterly cyclical TFP, not the yearly cyclical TFP. Moreover, based on Prescott (1986), the standard deviation of US yearly cyclical TFP is nearly 5 times as large as the standard deviation of US quarterly cyclical TFP. As a result, the estimated standard deviation of US yearly cyclical TFP should be at least much larger than 0.012, considering the fact that the data I use in this paper is from a different time period than what Prescott (1986) uses. Therefore, the standard approach of decomposing TFP into stochastic trend and cyclical component cannot provide us a correct way to measure the standard deviation of the cyclical TFP. This paper aims to solve this problem by introducing us to use geometric Brownian motion (GBM) to characterize aggregate TFP in continuous time and then transforming the GBM characterization of TFP with estimated parameters to a process of random walk with drift, so that both stochastic trend and cyclical component of TFP can be obtained in the continuous-time form. Using this new approach, we will find that the standard deviation of cyclical TFP is much closer to the real-world data. Moreover, based on the new approach, we will also find that the stochastic trend of TFP can be decomposed into three parts: an initial value, a deterministic trend, and a term involved with Weiner process.====The paper is organized as follows: First, I solve for the theoretical closed-form expression of aggregate TFP shock using GBM characterization. Second, I use real-world data to estimate the drift and diffusion coefficients in the geometric Brownian motion. More importantly, this paper develops a method of estimation of the drift and diffusion coefficients in the geometric Brownian motion that is consistent with the basic idea of rational expectations. Then, I transform the GBM characterization of the aggregate TFP in continuous time to a process of random walk with drift and use the drift estimate and the lagged TFP together as the stochastic trend and stochastic error term as the cyclical component of TFP. We will have two findings: the first one is the standard deviation of cyclical TFP derived from the new approach is much closer to the real-world data; and the second one is the stochastic trend of TFP can be decomposed into three parts: an initial value, a deterministic trend, and a term involved with Weiner process. Finally, to see if the remeasured stochastic trends can improve the capability of real business cycle model to mimic real-world economic fluctuations, I apply the remeasured stationary cyclical component of TFP to a rational expectations model and then recalculate the business cycle statistics generated by the model and compare them with the business cycle statistics generated by the real-world data.",Remeasuring and decomposing stochastic trends in business cycles,https://www.sciencedirect.com/science/article/pii/S109094432030377X,20 October 2020,2020,Research Article,91.0
Toshimitsu Tsuyoshi,"School of Economics, Kwansei Gakuin University, Japan","Received 20 August 2020, Accepted 12 October 2020, Available online 20 October 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.10.005,Cited by (0),"We examine whether an incumbent monopolist has an incentive to invite a new entry. In particular, we demonstrate the condition of a profit-raising entry effect in the presence of network externalities. Here the incumbent monopolist grants a free ==== license for a perfectly compatible product for a new firm when it can choose the level of compatibility.","The orthodox economics literatures relies on the assumption that increased levels of competition, i.e., an increase in the number of new firms entering a market, leads to lower prices and reduced profits but improved social welfare. Most commonly, in a standard Cournot–Nash oligopoly model, an increase in the number of firms leads to a reduction in an individual firm's output and profit, and in industry profits.==== Thus, the typical design of competition policy is to reduce market concentration and ensure appropriate levels of competition.====Conversely, incumbent firms should create barriers that make it difficult for new firms to enter the market to maintain above-normal levels of profits. If a patent license is a barrier that protects the profits of incumbent firms, it is important to consider whether in a network product market, where a winner with an advanced technology can take all, the winner (i.e., an incumbent monopoly holding the patent) has an incentive to invite new entries by granting a free or low patent license. That is, we should consider the research question presented by Economides (1996, p. 211) that “… the incentive of an exclusive holder of a technology to share it with competitors in a market with network externalities.” Economides (1996) uses this to draw on a standard Stackelberg game where one leader and many followers compete on quantities and not prices in a homogeneous product market.====Assuming a unit linear city model with network externalities, Kim (2001) later considers the cases of an incumbent monopoly and Bertrand duopolistic competition in a post-entry market. Comparing the profits between the monopoly and duopoly, Kim (2001) demonstrates whether the incumbent monopoly invites new entry or not. In particular, if the network externalities are strong and/or the products less differentiated, the monopoly profit exceeds the duopoly profit. Thus, the incumbent monopoly has an incentive to deter entry. This is because the incumbent monopoly fully captures the market via the strong network externalities, such that the new entrant does not contribute to network expansion.====Alternatively, if the network externalities are weak and/or the products are highly differentiated, the duopoly profit exceeds the monopoly profit. Thus, the incumbent monopoly has an incentive to invite new entry. In this situation, the incumbent monopoly partially captures the market and, thus, the network size expands sufficiently with a new entrant without serious price competition, largely because the post-entry market will be a local monopoly under weak network externalities.====In a related study to this analysis, Fanti and Buccella (2017) investigate entry effects in a network product market and demonstrate that a profit-raising entry for an incumbent takes place when the network effect is of a sufficiently high level. However, their model depends on a specific setting in which firms behave in line with coperate social responsibility (CSR), implying that firms maximize a combination of profits and consumer surplus.====Similar to Fanti and Buccella (2017), by assuming a quasi-linear utility function with network externalities, we reconsider the Economides’ problem by comparing the profits of an incumbent monopoly in the pre-entry market and a duopoly in the post-entry market with both price and quantity competition. We demonstrate that an incumbent monopolist has an incentive to invite a new entry if the degree of network externalities is sufficiently large.",Does an incumbent monopolist have an incentive to invite new entry through granting a free patent license?,https://www.sciencedirect.com/science/article/pii/S1090944320303768,20 October 2020,2020,Research Article,92.0
"Predelus Wilner,Amine Samir","Université du Québec en Outaouais, 283 boul. Alexandre-Taché, C.P. 1250, succursale Hull, Gatineau, Québec J8 × 3 × 7, Canada","Received 18 August 2020, Accepted 12 October 2020, Available online 20 October 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.10.004,Cited by (1),"Beyond the maximum insurable income, the size of the shortfall increases with the worker's income, leaving unemployed workers with little or no room to maintain a decent life while meeting all their other obligations. This situation is even more perilous in an indebted society where the debt per income ratio hovers at around 175%. This paper contributes to the literature by identifying the key socioeconomic and demographic indicators that dictate the insolvency choice (bankruptcy or consumer proposal) of Canadian insolvent employment insurance recipients. Using Canadian data, we show that low employment insurance leads debtors to eat up their asset and incur more debt, which ultimately make bankruptcy more appealing to them than proposal.","Consumers who lose their job often find themselves struggling to meet their financial obligations, as their very source of income dries out. In Canada, employment insurance, which provides an income replacement for workers who involuntarily lost their job, covers only 55% of the lost income with a weekly cap that stands at $562 in 2019 (Canada Employment Insurance Commission, 2019). In the best-case scenario, workers who earn up to $53,100 per year technically find themselves with a 45% shortfall in a weekly gross income that, oftentimes, was already too low to cover regular expenses (Blain, 2015). Beyond the maximum insurable income, the size of the shortfall increases with the worker's income, leaving unemployed workers with little or no room to maintain a decent life while meeting all their other obligations. This situation is even more perilous in an indebted society where the debt per income ratio (including mortgage debt) hovers at around 175% (Mian et al., 2017; Hastings and Saris, 2012; Vroman and Woodbury, 2014).====Indeed, it is argued that the performance of the labour market has a definitive and direct impact on insolvency filings (Fay et al., 2002; Fisher, 2005). Domowitz and Sartain (1999) and Sullivan et al. (2000) observe that a very large proportion (over two-thirds) of those filing for bankruptcy in the US had recently experienced a job disruption. While Liscow (2016) argues that efficient insolvency laws can help reduce the need for government expenditures on social insurance payments by preserving jobs when unemployment rate is high. On the contrary, Sheppard (1984), Domowitz and Eovaldi (1993) and Buckley and Brinig (1998) suggest that transfer programs encourage insolvency. For their part, Arthreya (2003) and Fisher (2005) observe that government transfer programs, like Employment Insurance, reduce the probability of insolvency. Arthreya (2003) further observes that not only low employment insurance replacement ratios lowers welfare, but also reducing the replacement ratio increases bankruptcy rates.====As part of the Canadian insolvency system, the Bankruptcy and Insolvency Act (BIA) proposes mainly two options to assist insolvent debtors: bankruptcy and consumer proposal. During a bankruptcy, the insolvent debtor turns over all their non-exempt property rights to the Licensed Insolvency Trustee (LIT) who sells them and distributes the proceeds between the creditors. Once the debtor makes an assignment into bankruptcy, creditors have the legal obligation to stop all collection activities, unless they obtain a court decision allowing the collection. As opposed to bankruptcy, a consumer proposal is an agreement reached under the BIA between a debtor and their creditors that authorize the former to modify the terms and the amount of their payments. Insolvent debtors who file a consumer proposal are allowed to keep their entire asset against a repayment undertaken, which cannot be established on a period longer that is than five years or sixty months.====In terms of risk and cost sharing, Predelus and Amine (2019) stress that, insolvency, as a public insurance, is financed solely by debtors who manage to repay their debts, for creditors need to charge borrowers with higher interest rates to make provisions for defaulters. Citing the fact that the historic dividend pay to creditors is, on average, higher in proposal than bankruptcy, they further hypothesize that creditors would be more incline to charge higher interest rate and fees if they expect defaulters to file more bankruptcies than proposals. Therefore, the insolvency choice of employment insurance recipients becomes a pivotal element to understand the depth of borrowers’ contribution to the financing of social insurance programs. Since Predelus and Amine (2019) observe that debtors with higher income and/or wealth tend to file a proposal, it is not clear what insolvent debtors can do with a source of income that puts everybody on the same footing and covers only a fraction of their employment income. Thus, hypothesize that debtor's background undoubtedly plays an important role in defining their choice in insolvency.====This paper aims to analyse the behaviour of insolvent employment insurance recipients, and it contributes to the literature by identifying the key socioeconomic and demographic indicators that dictate the insolvency choice of this group of debtors. We show that low employment insurance leads debtors to eat up their asset and incur more debt, which ultimately make bankruptcy more appealing to them than proposal.====The rest of the paper is organized as follows. Section 2 looks at the model and explores the data used. Section 3 provides the empirical results and Section 4 concludes.",How employment insurance recipients make decision about insolvency?,https://www.sciencedirect.com/science/article/pii/S1090944320303756,20 October 2020,2020,Research Article,93.0
"Ogawa Kazuhito,Kawamura Tetsuya,Matsushita Keiichiro","Faculty of Sociology, Kansai University,Faculty of Economics and Business Management, Tezukayama University, Japan,Faculty of Economics, Kansai University,Research Institute for Socionetwork Strategies, Kansai University","Received 17 July 2020, Accepted 12 October 2020, Available online 17 October 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.10.002,Cited by (2),"We investigate how cognitive ability and age affect giving behavior in single-blind dictator game experiments with a sample comprising 514 non-student participants in Japan. First, we found a negative correlation between cognitive ability and giving behavior. Focusing on dictators with medium or low cognitive ability, the higher the cognitive ability, the lower are both giving probability and amount. Such a negative correlation was also found for dictators with high cognitive ability, but there was a jump increase in giving probability between the medium and high cognitive ability groups. Regarding the age effect, giving probability and amount of older dictators (====40 years) were higher than those of younger dictators (20–29 years).","Recently, experimental and survey studies show that cognitive ability influences economic decision-making. Here cognitive ability is “the capacity to think logically, analyze and solve novel problems, independent of background knowledge” (Mullainathan and Shafir, 2013, p.48). This ability is known to influence single-agent decision-making such as with respect to risk preference (Dohmen et al., 2010) and time preference (Benjamin et al., 2013, Shamosh and Gray, 2008).====Those with high cognitive ability tend to behave close to the prediction of subgame perfect equilibrium in games with a single subgame perfect equilibrium [e.g., the ====beauty game (Brañas Garza et al., 2012) and the responder behavior with the strategy method in the ultimatum game (Kawamura and Ogawa, 2019)]. For a game with multiple subgame perfect equilibria, they tend to behave cooperatively in repeated prisoner’s dilemma games (Burks et al., 2009, Proto et al., 2015).====Among survey studies, Thoma et al. (2015) investigated the relationship between cognitive reflection test (Frederick, 2005, hereafter, CRT) scores of financial traders and the avoidance of making decisions using heuristics. de Oliveira Leite et al. (2020) found a positive correlation between CRT scores and the wage level of accountants in Brazil. Thus, there is no doubt that the impact of cognitive ability on economic decision-making is growing in importance.====In this study, we examine whether cognitive ability affects giving in dictator game (DG) experiments (Engel, 2011, Forsythe et al., 1994, Kahneman et al., 1986) and, if so, what that effect is. To measure cognitive ability, we used one of the non-verbal estimation methods of fluid intelligence, Raven’s Progressive Matrices test (Raven, 2000, Raven, 1936), which is the most often used test in economics (Hanaki et al., 2016, Kawamura and Ogawa, 2019, Proto et al., 2015).====A small number of studies have examined how cognitive ability affects giving behavior====.Ben-Ner et al. (2004), Yamagishi et al. (2016), and Inaba et al. (2018) found negative correlations between cognitive ability and giving amount====. Chen et al. (2013) found that this relationship depended on the type of cognitive ability. However, Brandstätter and Güth, (2002) found little evidence for such a relationship. Thus, results on the relationship between cognitive ability and giving amount are inconclusive.====To examine the effect of cognitive ability, we require a wide variety of data. For this reason, we employed non-student adults for the experiment. Employing non-student adult participants raises the need to consider the effect of age ==== This variable is less important in the case of student participants, but needs to be taken into account in the case of adult participants of varying ages. Because cognitive ability is known to be lower on average at older ages  (for example, see Babcock, 2002), it is necessary to investigate giving behavior considering the relationship between age and cognitive ability.====Various surveys have shown that people tend to become generous as they age. For example, the frequency with which Japanese people donate rises as they age, with the giving amount of those in their 50s or 60s higher than that of those in their 20s and 30s (Japan fundraising association, 2018). Indeed, the amount donated to charities by the over-60s in the United Kingdom is twice that by the under-30s (Charities Aid Foundation, 2012).====However, the experimental results (Beadle et al., 2013, Kettner and Waichman, 2016, Rieger and Mata, 2013, Roalf et al., 2011) are inconclusive. As we will discuss in more detail in Section 2, some studies support that the elderly give more money than the young do, whereas others do not support this.====In consideration of the above, we consider the effect of cognitive ability and age simultaneously and investigate their effects on giving behavior in DG experiments. To our knowledge, only Cappelen et al. (2016) have previously investigated the effects of both cognitive ability and age on giving behavior. However, their study differs from ours in that they focused on the impact of cognitive ability on the response time of the dictator in deciding the giving amount.====Let us preview our experimental results briefly. First, focusing on the giving behavior of dictators with low or medium cognitive ability, we found a negative correlation between ability and giving behavior; that is, the higher the ability, the smaller are both the giving probability and the giving amount. Additional analysis focusing on all observations, despite an overall negative correlation, an upward jump in giving probability was observed between the medium and high cognitive ability groups. That is, the giving probability of dictators with high cognitive ability is larger than that of dictators with medium cognitive ability. Regarding the effect of age, the giving probability and amount of an older dictator (40–79 years old for giving probability and ====40 years old for giving amount) are higher than those of a younger dictator (those in their 20s).====Our study contributes to the literature as follows. First, employing non-student adults from the general population as participants provides information about the effects of income, age, gender, and cognitive ability. Especially, a wide range of cognitive ability is required to investigate the effect of cognitive ability on giving behavior. This information can facilitate understanding giving behavior in more depth. However, employing non-student adult participants raises some hurdles regarding, e.g., recruiting them, the appropriate payment scheme (i.e., sufficient payment), and teaching the elderly to use a computer mouse. Second, few experimental studies have considered the effects of age and cognitive ability simultaneously in DG games. Thus, investigating the influence of age and cognitive ability on giving behavior also fosters a deeper understanding of giving behavior.====The remainder of this paper is organized as follows. Section 2 presents the hypotheses. Section 3 explains the experimental design and procedure. Section 4 shows the experimental results. Section 5 discusses the results and Section 6 concludes.",Effects of cognitive ability and age on giving in dictator game experiments,https://www.sciencedirect.com/science/article/pii/S1090944320303732,17 October 2020,2020,Research Article,94.0
Urzúa Carlos M.,"Tecnológico de Monterrey, Revolución 756, 03700 Benito Juárez, Mexico City, Mexico","Received 19 July 2020, Accepted 12 October 2020, Available online 16 October 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.10.003,Cited by (0),"By means of duality theory, this paper generalizes the Balassa-Samuelson model as is used to explain the Penn effect; namely, the fact that national ==== tend to rise with per capita national incomes. The generalization made in this paper allows for any technological progress that is Hicks-neutral, Solow-neutral, Harrod-neutral, or any mixture of them. The implications of the enlarged models include, among others, the Balassa-Samuelson scenario, as well as a capital-intensity scenario that resembles Bhagwati’s. Those hypotheses emerge simultaneously when technical changes are, both, Hicksian and Solovian. The paper also presents an alternative model that is used to explain the apparent breakdown of the Penn effect in the case of the lowest-income countries.","National price levels tend to rise with per-capita national incomes. This empirical regularity, coined by Samuelson (1994, p. 201) as the “Penn effect,” has been repeatedly found over the years. Kravis et al. (1978) elaborated one of the earliest comprehensive studies in that regard, and Inklaar and Timmer (2014) one of the latest. Aside from rare exceptions, such as the paper by Hassan (2016) in which it is claimed that such a relationship breaks down in the case of the lowest-income countries, the Penn effect is a widely accepted economic phenomenon that has had a lasting impact on empirical work.====Its most well-known consequence is that the estimates of real incomes in rich and poor countries would be biased toward the first group if only the market exchange rates were used for comparisons. A related implication is that any model of a country’s long-run real exchange rate has to take into account that effect, together with other explanatory variables, since changes in relative national incomes across countries may be accompanied by changes in relative national price levels.====In order to account for the Penn effect is necessary to specify “sufficient scenarios” for it, to use another happy expression in Samuelson (1994, p. 205). One of those scenarios was provided independently by Balassa (1964) and Samuelson (1964). Using a Ricardian framework, they proposed the following hypothesis: (a) Technological progress is typically more dynamic in the industries producing tradeable goods than in the ones dedicated to nontradeables; (b) the real prices of tradeables tend to be similar worldwide; (c) the prices of nontradeables tend to be lower in countries that are less efficient producing tradeables; and (d), as a consequence, the most efficient countries in the tradeables sector should have the highest cost of living as well. Interestingly enough, and unknown at that time to both authors, Harrod (1933) had made a similar argument three decades earlier. The only noticeable difference was that, as opposed to the Balassa and Samuelson (B-S) hypothesis, Harrod’s argument never considered the possibility of technological progress.====The B-S hypothesis is often called the “Harrod-Balassa-Samuelson effect.” But in what follows the words “hypothesis” or “scenario” will be preferred to “effect”. Furthermore, it will be shown in this paper that the dynamic version of the B-S hypothesis requires Hicks-neutral (sector-neutral) technological progress. Thus, Harrod’s name will be reserved here for the case of Harrod-neutral (labor-augmenting) technological progress.====The B-S hypothesis was challenged two decades later, when a quite different model appeared. Using a Heckscher-Ohlin framework, Bhagwati (1984) showed that if a labor-abundant country is endowed with a sufficiently small capital stock, and the nontradeables sector is labor-intensive, then the price of nontradeables relative to the price of tradeables has to be low as well. As the economy evolves, capital deepening makes the country less labor-abundant, making the return on labor, and hence the cost of living, to increase.====The Bhagwati hypothesis throws light on the Penn effect, but it has never been fashionable in the empirical literature. For the case of developing countries this might be due to lack of data, while for developed countries some have argued that there is not enough variation across relative factor endowments (see, for example, De Gregorio et al., 1994). On the other hand, the B-S hypothesis proved to be popular from the beginning. In their thorough survey, Tica and Družić (2006) found that it had been tested at least 58 times using data from 142 countries, during the 1964–2004 period alone.====Most of those studies, however, were not made using the B-S model, but rather a simplified version of it, known as the B-S growth model. Section 2 reviews it. The exposition leaves untouched the common assumption of Cobb-Douglas production functions that exhibit disembodied technological progress (Hicks-neutrality). But such a basic assumption is, of course, unwarranted. Section 3 uses duality theory to show that the model can be readily extended to include more general production functions. These are all the neoclassical production functions that exhibit, simultaneously or not, Hicks-neutral, Solow-neutral and Harrod-neutral technological progress. By doing so, it is shown that a number of different scenarios for the Penn effect can be generated. The cases include, in particular, the Balassa-Samuelson hypothesis, as well as a capital-intensity hypothesis that has points in common with Bhagwati’s. The paper also presents a short-run version of the model to explain the apparent breakdown of the Penn effect in the case of the lowest-income countries. Section 4 draws the conclusions.",The Balassa-Samuelson and the capital-intensity hypotheses in a nutshell,https://www.sciencedirect.com/science/article/pii/S1090944320303744,16 October 2020,2020,Research Article,95.0
Alfaro Martín,"University of Alberta, Department of Economics. 9-08 HM Tory Building, Edmonton, AB T6G 2H4, Canada","Received 28 August 2020, Accepted 12 October 2020, Available online 16 October 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.10.007,Cited by (3),"This paper provides some stylized facts about market structure in Denmark, a country exhibiting high rates of exports and imports as is common in small developed economies. Utilizing disaggregated data at the firm-product level for manufacturing ","One common feature of small countries that distinguishes them from large economies is their heavy dependence on international trade (Benito, Larimo, Narula, Pedersen, 2002, Van Den Bulcke, Verbeke, 2001, Hannerz, Gingrich, 2017). Regarding exports, this is a consequence of the restricted size of their home market, which makes firms commonly sell abroad to increase their production scale and, hence, operate efficiently. As for imports, their limited variety of resources and market size create hurdles to supply the whole diversity of goods that consumers demand. Thus, these countries end up importing more intensively and exposing local firms to tougher competition.====Given these distinctive features of a small open economy, what kind of market structures arise? In this paper, we provide some evidence on the matter by identifying stylized facts for Denmark. Our findings highlight the ubiquity of industries with coexistence of domestic leaders and numerous negligible firms. For these industries, we identify empirical features of their firms and present a model that highlights the incentives of leaders to behave strategically against the negligible firms.====In Section 2, we identify patterns regarding the market structure of Danish manufacturing industries. Our findings are based on highly disaggregated data regarding Danish manufacturing, which primarily come from two sources: Danish Prodcom statistics and international transactions collected by Danish customs. These datasets are part of the country’s official statistics and cover almost all the international transactions and more than 90% of total production by industry.====Two features make these datasets particularly suitable for the analysis of market structure. First, the information is presented at the firm-product level and disaggregated at the 8-digit product level, making it possible to allocate each firm-product to an appropriately defined industry. In this way, we avoid issues arising with typical balance-sheet data declared for tax purposes, where a firm’s total revenue is usually allocated to the specific industry that generates the greatest bulk of it.====Second, the information on international transactions is also presented at the firm-product level and encompasses imports by both manufacturing and non-manufacturing firms. This becomes particularly relevant for small economies like Denmark, where import competition is pronounced, since the importance of a domestic firm within an industry can only be measured once that imports are accounted for.====By obtaining the domestic market share of each domestic firm in terms of expenditures (i.e., domestic sales plus imports), our findings are as follows. First, around half of the industries have numerous Danish firms with negligible domestic market shares operating. Throughout the paper, we refer to these firms as domestic non-leaders (DNLs). Remarkably, when we measure the importance of these industries by total manufacturing expenditure, we find that they determine the majority of it. Specifically, more than 80% of total expenditure is covered by them.====In addition, we show that DNLs are not the only type of firms operating in these industries. Rather, they commonly coexist with a handful of firms that accrue great domestic market share, which we refer to as domestic leaders (DLs). Specifically, among industries with numerous DNLs, more than 85% of them have at least one DL and they represent more than 80% of the total manufacturing expenditure.====Delving into properties of these industries, we analyze how DLs compare with DNLs across several features. The results point out that DLs have greater labor productivity (i.e., revenue per employee), are more capital intensive, and pay higher wages. Additionally, they are more likely to engage in exporting and importing activities, although a different picture emerges when we measure their degree of internationalization through domestic intensity (i.e., a firm’s domestic sales relative to the sales at home plus exports): conditional on exporting, DLs tend to display greater domestic intensity than DNLs. The result becomes relevant in light of the mixed empirical evidence on this matter==== and the debate on whether a firm’s position at its home market predicts a specific type of export behavior (for classical studies regarding this, see Mascarenhas, 1986, Bonaccorsi, 1992, Porter, 1998).====Based on the ubiquity of these types of industries, in Section 3 we investigate the implications that such a market structure might entail. In particular, the asymmetry of these firms in terms of size and position at home can lead DLs to make strategic moves against DNLs to gain a better position domestically (see Kwoka and White 2001 and D’Aveni 2002 for several examples of such behavior).====To explore this matter, we present an open-economy model where DLs are represented as in monopolistic competition and embed a set of non-negligible heterogeneous firms to capture the presence of DLs. Moreover, we suppose that DLs engage in strategic moves and make investment decisions to gain a better position locally. To isolate the strategic motive to invest by these DLs, we utilize the two-stages approach by Fudenberg and Tirole (1984). Thus, we consider a scenario where DLs decide on domestic investments prior to both entry of DNLs and the market stage. Then, we compare the outcomes of this scenario with a non-strategic benchmark where domestic investments are not observed by rival firms.====The results of the model indicate that DLs act more aggressively through overinvesting. The goal is to deter entry of DNLs and capture domestic sales that, otherwise, would go to DNLs. A corollary of this is that, even when we assume competition à la Bertrand, DLs never accommodate entry. The result clearly contrasts with what occurs in models with one incumbent and one potential entrant à la Fudenberg and Tirole (1984). This occurs because, in our model, DNLs are governed by free-entry rules. Thus, as in Etro, 2006, Etro, 2008, accommodating strategies are always unprofitable since they end up inducing additional entry and, hence, turning futile the attempt of softening competition.====To assess the potential impact of this type of behavior, in Section 4 we calibrate the model and perform some numerical exercises. We find that, on average, each DL could increase its domestic revenue by around 36%. In addition, the domestic market share of each would become greater by around 2.5 percentage points and its domestic intensity higher by around 5 percentage points. Nonetheless, the results exhibit great heterogeneity across and within industries.====Our paper is related to different strands of the literature. First, it touches upon studies that characterize markets in open economies. In particular, it is related to studies that have identified a coexistence of large and small firms. At the country level, this has been documented in Axtell (2001) for the USA and Fujiwara et al. (2004) for several European countries; at the industry level, a similar pattern has been shown by Bronnenberg et al. (2009) and Hottman et al. (2016) for various industries in the USA and Gaubert and Itskhoki (2018) for French manufacturing.====We contribute to the literature in several respects. First, the evidence that we provide covers all the industries belonging to manufacturing. Thus, in contrast to studies that analyze some specific industries, we can measure their relative importance with respect to the whole manufacturing sector. Specifically, this allows us to conclude that industries with leaders and negligible firms coexisting cover the great bulk of total manufacturing expenditure. Second, our results are based on highly disaggregated information at the firm-product level and including imports. Thus, we improve upon results utilizing balance-sheet data, which allocates a firm’s total revenue to the main activity of the firm without splitting it into the different industries from which this is generated. Finally, we focus on the case of Denmark, which is a small economy exhibiting high rates of exports and imports.====Furthermore, our paper is related to studies that search for stylized facts of firms through the use of microdata. Regarding this, there is a vast literature initiated by Bernard and Jensen (1995) on the differences between exporters and firms that exclusively serve domestic markets. See in particular Eaton et al. (2004) for France, Mayer and Ottaviano (2008) for several European countries, and Bernard et al. (2012) for the USA. Furthermore, there is a growing literature on the so-called “superstar firms”, including Freund and Pierola (2015), Autor et al. (2017), Gutiérrez and Philippon (2017), and De Loecker and Eeckhout (2018). While these papers define large firms according to a firm’s exports or total revenue, we deal with large firms defined by their domestic market shares and, hence, their position at the home market.",Market structures in small open economies: Evidence from Denmark,https://www.sciencedirect.com/science/article/pii/S1090944320303781,16 October 2020,2020,Research Article,96.0
Shy Oz,"Research Department, Federal Reserve Bank of Atlanta, 1000 Peachtree St. NE, Atlanta, GA 30309, United States","Received 9 July 2020, Accepted 6 September 2020, Available online 2 October 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.09.001,Cited by (8),"Low-income consumers are not only constrained with spending, but also with the type and variety of payment methods available to them. Using a representative sample of the U.S. adult population, this short article analyzes the low possession (adoption) of credit and debit cards among low-income consumers who are also unbanked. Using a random utility model, I simulate the potential ","Payments for purchases of goods and services and bill payments are made with the use of payment instruments such as cash, checks, credit cards, debit cards, prepaid cards, and account-to-account money transfers. However, as shown in this article, low-income consumers tend to have a lower variety of payment methods, and a smaller fraction of them own a bank account relative to high-income consumers. Therefore, low-income consumers are not only constrained with spending, but also with the type and variety of payment methods available to them.====This short article has two goals: First, to investigate the strong correlation between household income and the ownership of credit cards, debit cards, and bank accounts. Second, to analyze some policy proposals that have been discussed in the literature regarding the provision of unsubsidized and subsidized debit cards to unbanked consumers who rely mainly on cash to make payments. I also analyze the effect of subsidizing the cost of prepaid cards.====The motivation behind these policies stems from the fact that innovations in payment technologies do not always apply to some segments of the population. In particular the introduction of cashless stores and mobile apps cannot benefit consumers who do not have credit and debit cards. That also apply to online shopping which requires the use of credit and debit cards.====Both goals are accomplished with the use of a representative sample of the U.S. adult population. This sample contains information on bank account ownership, availability of credit and debit cards, individual assessments of each payment instrument, household income, and other demographic variables. In addition, the sample records actual payments made by the respondents including payment dollar amount, payment method, and merchant type.====The policy analysis is accomplished with the use of simulations based on utility estimates generated by a random utility model. Note that the utility derived from paying with a certain payment instrument is different from the utility derived from consuming the product or service for which the payment is made. It is the utility (or disutility) derived from the process of having to pay with a particular payment instrument. This utility is computed by regressing individuals’ payment choice at the point of sale on individuals’ assessments of cost, security, and convenience of each payment instrument. The simulations of policy options recompute the utility after unsubsidized and subsidized debit cards are added to the set of available payment methods of unbanked consumers, and also under the hypothetical policy of providing a cost subsidy to prepaid card users. Simulation results show the relative improvement in consumer welfare among all three policy options.====In the literature, several papers experimented with measuring the welfare consequences of adding/subtracting a payment instrument to/from the set of existing payment methods. Alvarez and Argente (2019) estimate the utility loss from only accepting cards for Uber rides, based on data from Mexico. Briglevics and Schuh (2020) use data from the 2012 Diary and Survey of Consumer Payment Choice to quantify utility losses with removing payment choices for U.S. consumers. Shy (2020) analyzes the consequences of stores becoming cashless. Huynh et al. (2020) and Wakamori and Welte (2017) use data from Canada where access to bank accounts is nearly universal. Huynh et al. (2020) introduce a central bank digital currency in which they assume to combine the most favorable features of cash, debit card, and credit card and compute the utility gains for consumers. Wakamori and Welte (2017) construct a policy experiment where all merchants accept all payment methods, based on a supposition that the government regulates card merchant fees to a very low level.====This article is organized as follows. Section 2 characterizes banked and unbanked consumers’ possession of credit or debit cards belonging to different household income groups. Section 3 develops the method for measuring the per-payment benefit and cost of using each payment method. Section 4 provides simulations of the welfare effects associated with several policy proposals directed toward unbanked consumers that have been discussed in the literature. Section 5 provides a discussion of unbanked consumers.",Low-income consumers and payment choice,https://www.sciencedirect.com/science/article/pii/S1090944320303719,2 October 2020,2020,Research Article,97.0
Levenko Natalia,"Department of Economics and Finance, Tallinn University of Technology, Akadeemia tee 3-482, 12618 Tallinn, Estonia,Eesti Pank, Estonia","Received 5 July 2020, Accepted 2 August 2020, Available online 6 August 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.08.001,Cited by (1),The European Survey of Professional Forecasters (SPF) is a dataset that is widely used to derive measures of forecast uncertainty. Participants in the SPF provide not only point estimates but also density forecasts for key ,"Uncertainty is now being incorporated extensively into economic research and is particularly relevant in analysis of recessions and post-recession periods, when economic agents are typically more uncertain about the future than they are in normal times. A large body of literature has documented the adverse effects that different types of macroeconomic uncertainty may have on the real economy. Uncertainty may affect the economy through numerous channels, such as precautionary saving by households, or firms taking a wait-and-see attitude to investment (Bloom et al., 2007; Stokey, 2016; Levenko, 2020). In a more general perspective, uncertainty may be associated with a contraction in economic activity depressing economic growth, creating financial distortions and slowing post-crisis recovery (Gilchrist et al., 2014; Baker et al., 2016; Basu and Bundick, 2017; Cesa-Bianchi et al., 2018; Bloom et al., 2018).====A widely-used source of data on the uncertainty of the macroeconomic forecasts in the euro area is the European Survey of Professional Forecasters (SPF) conducted by the ECB. Participants in the SPF provide point forecasts of key economic indicators, which are the inflation rate, real output growth and the unemployment rate, together with probability distributions for the forecast variables. The micro-level data of the individual forecasts can then be combined and aggregate measures of uncertainty can be computed from the survey data.====The main focus of this paper is on the quality of the mean individual variance of forecasts, which is a widely used measure of survey uncertainty.==== Mean individual variance is defined as the average of the variances of the individual density forecasts. The variance of an individual density forecast is supposed to be a direct indicator of how an individual forecaster perceives the uncertainty about the variable being forecast. The larger the variance is, the more uncertain the forecaster is, and the other way round, so the mean individual variance is often considered a direct measure of uncertainty (Zarnowitz and Lambros, 1987; Giordani and Söderlind, 2003; Boero et al., 2008; Abel et al., 2016).====An alternative measure of uncertainty that can be calculated using the same dataset is the cross-sectional variance of the point estimates, which is often labelled forecast disagreement. Both measures of uncertainty peaked during the Great Recession, but while the disagreement returned to its pre-crisis level immediately after the recession was over, mean individual variance has remained elevated since the crisis. This shift in the mean individual variance is puzzling since measures of uncertainty are typically countercyclical, meaning that economic agents are on average more uncertain about the future in bad times than they are in good times, as has been documented in many empirical studies (Abel et al. 2016; Binder, 2017; or Cesa-Bianchi et al., 2018 to name just a few).====It can be argued that there is no puzzle about the persistence of increased mean individual variance since the level of uncertainty was fairly high not only during the recession but also in the years after the crisis, which saw the sovereign debt crisis in the euro area, the unexpected Brexit vote followed by government crises in the UK, the European migrant crisis, international trade conflicts, and so on. In fact the correlation of mean individual variance with the Economic Policy Uncertainty (EPU) index is quite high.====Even so, this paper offers an alternative explanation for the elevated uncertainty, measured as mean individual variance, and argues that the upward shift cannot be justified solely by the crisis itself or by other economic and political fluctuations in the years since the crisis. The paper provides evidence that the changes in the mean individual variance should in large part be attributed to changes in the modelling preferences and habits of forecasters, and that the evolution of these preferences is a process that is exogenous to the economic and political instabilities in Europe following the crisis. Besides, this paper illustrates how modelling preferences can introduce additional noise into individual variance.====A key to understanding changes in the mean individual variance is the rounding behaviour of survey participants, meaning that forecasters may or may not round the probabilities they assign to the different outcomes of the variables being forecast. Rounding behaviour is likely to be rooted in the modelling preferences of forecasters, such as whether they use a modelling approach or a judgement-based approach. It seems reasonable to assume that density forecasts produced with econometric models would generally be non-rounded while judgement-based forecasts would be rounded. It is shown in Section 3.4 that the share of non-rounded responses in the survey is related to developments in the computer software market, implying that there is a growing tendency for researchers and other professionals to use more sophisticated software and modelling methods.====Rounding itself has a marginal positive effect on the variance of forecasts that can be neglected because of its magnitude.==== What is important is that when a forecaster rounds the probabilities, the bins at the tails of the probability distribution disappear by becoming nulled, which drives the variance downwards. It is shown by means of simulations in Section 2.2 that due to the different number of bins in rounded and non-rounded density forecasts the difference in variance can be very significant. These findings are perfectly in line with the data. The average number of bins used by rounders is considerably smaller than the number of bins used by those forecasters who do not round their forecasts (Glas and Hartman, 2018).====Rounding is believed to be directly related to the perception of uncertainty. Section 2 discusses how rounding is typically a strong sign of uncertainty from a survey participant (Krifka, 2002). Thus, if rounding and the subsequent reduction in the forecast variance are a product of forecasters perceiving greater uncertainty, then there is problem, since the inferences made by the survey users are the opposite to what is really being perceived by the forecasters. This is of great concern as around three fifths of the short-term and medium-term forecasts examined in this paper use a model-based approach with judgemental adjustment (Meyler and Rubene, 2009; ECB, 2014; ECB, 2019).====Various attributes of the SPF forecasts are examined in Glas and Hartmann (2018), who document that more responses were not rounded after the Great Recession. They state that this increase in the share of survey participants who do not round could be one possible explanation for the increased uncertainty after the crisis, but they still state that mean individual variance is a valid proxy for measuring uncertainty.====While Glas and Hartmann (2018) mostly use statistical and descriptive methods to examine the attributes of the SPF forecasts, this paper applies a more formal econometric approach in the form of smooth transition regression analysis, which to the best of my knowledge has never previously been applied to the SPF data. Apart from its technically not trivial method, the main contribution of this paper is its finding that the shift in the level of uncertainty after the Great Recession as measured by the individual variance can be attributed to the changes that have occurred in the modelling preferences of forecasters since the mid-2000s. Another contribution of the paper is that it shows by means of simulations that the rounding of density forecasts drives variance down by reducing the number of bins. These findings are relevant for applied empirical research.====The rest of the paper is organised as follows. Section 2 briefly discusses issues around the rounding behaviour of forecasters and runs simulations of rounding applied to density forecasts; Section 3 describes the data, estimation methods, and results; and finally, Section 4 concludes.",Rounding bias in forecast uncertainty,https://www.sciencedirect.com/science/article/pii/S1090944320302994,6 August 2020,2020,Research Article,98.0
"Dong Quan,Chang Yang-Ming","School of Economics and Management, South China Normal University, Higher Education Mega Center, Guangzhou 510006, China,Department of Economics, Kansas State University, 319 Waters Hall, Manhattan, KS 66506-4001, USA","Received 16 May 2020, Accepted 30 July 2020, Available online 31 July 2020, Version of Record 3 September 2020.",https://doi.org/10.1016/j.rie.2020.07.005,Cited by (5),"This paper analyzes two pollution control instruments, uniform ==== and absolute standards, when polluting firms engage in partial ownership arrangements (POAs). Specifically, we examine the case of a bilateral POA between competing firms in which both hold equity shares on each other's profits as silent investments. We show that taxes and standards are equally efficacious in affecting the firms' output decisions and pollution emissions. Compared to the social planner's solution, a bilateral POA results in suboptimal outcomes with lower industrial output and consumer surplus. Firm profits are higher and environmental quality improves (since emissions decline), but social welfare decreases. We compare the equilibrium results associated with two different types of POAs (bilateral vs. unilateral), and examine their differences in welfare implications for the choice of policy options between taxes and standards.","How would the choice of environmental policy instruments between uniform emission taxes and absolute emission standards be affected by different types of partial ownership arrangements (POAs) among polluting firms? It is becoming more prevalent that competing firms in an industry hold equity shares on rivals' profits as silent investments. Such cross-ownership arrangements of firms' holding partial equity stakes are increasingly common in industries, such as automobiles (Alley, 1997; Ono et al., 2004), airlines (Clayton and Jorgensen, 2005), electricity power (Amundsen and Bergman, 2002), and steel (Gilo et al., 2006). Considerable studies have contributed to our understanding of the economic effects of bilateral POAs by rival firms in oligopoly (e.g., Flath, 1992; Malueg, 1992; Reitman, 1994; Gilo et al., 2006; Clayton and Jorgensen, 2005; Dietzenbacher and Temurshoev, 2008; Fanti, 2013, 2016; Lopez and Vives, 2019). Analyzing the case of symmetric Cournot duopoly, Reitman (1994) shows that both competing firms have incentives to acquire a passive stake in each other's profits.====Considering the frequently observed bilateral POAs (in the form of cross-shareholding agreements) by firms in industries, we analyze the choice of policy options in environmental regulation. We concentrate our analysis on two pollution control instruments: uniform taxes and absolute standards. Explicitly, we incorporate bilateral POAs into a stylized duopoly model of competition between firms that produce a homogenous good while generating pollution. We show that taxes and standards are equally efficient in affecting the firms' output decisions and pollution emissions. Using the social planner's solution as a reference base, we find that a bilateral POA results in a suboptimal equilibrium with lower total industry output and consumer surplus. Despite that firms make higher profits and there is environmental quality improvement (since emissions are lower), overall welfare decreases. A bilateral POA thus generates conflict between welfare maximization and environmental quality.====Our study is an extension of Bárcena-Ruiz and Campo (2017). They have a model with a unilateral POA whereby only one firm holds an equity share on its competitor's profit.==== We take that model and extend it by considering the case of bilateral POA. Compared to the equilibrium when a POA is unilateral, how would a bilateral POA affect the policy choice between an emission tax and an emission standard? From the social welfare perspective, it is well-known that bilateral POAs affect market competition negatively (e.g., Reynolds and Snapp, 1986; Farrell and Shapiro, 1990; Reitman, 1994). One policy issue concerns how differences between unilateral and bilateral participation in ownership arrangements affect the choice of environmental policies. In this paper, we provide answers to this question. We show that the choice of policies for imperfectly competitive polluting industries cannot be isolated from the ownership structure with rival firms acquiring each other's profit in the form of cross-shareholding arrangements, which affect production and hence emission decisions of the polluting firms.====An earlier contribution by Bárcena-Ruiz and Campo (2012) is the first to stress the importance of considering bilateral POAs in analyzing the design and choice of environmental policy with imperfectly competitive polluting industries.==== The authors focus their analysis on strategic environmental taxes in international competition.====The present study focuses on domestic competition and finds that comparison in the relative efficiency between emission taxes and absolute standards depends crucially on whether a POA is bilateral or unilateral. For a POA being unilateral, the case analyzed by Bárcena-Ruiz and Campo (2017), social welfare is higher with an emission tax (standard) when the equity share on a rival firm's profit low (high). However, for a POA being bilateral, we show that taxes and standards are equally efficacious. We further compare differences in policy implications between the two types of POAs for environmental regulations. We find that the socially optimal emission standard is lower, the abated pollution emissions are less, the industrial output is lower, the regulated firms are better off, but consumers are worse off when a POA is bilateral than when it is unilateral. Nevertheless, environmental quality is relatively higher whereas social welfare is relatively lower under a bilateral POA. The economic intuitions behind these results are as follows. A bilateral POA generates a relatively higher degree of inter-firm output coordination causing the industry output to be relatively lower. That is, there is a greater dampening of market competition when a POA is bilateral. Thus, different types of POAs (bilateral or unilateral) play a role in affecting the choice of policy options between taxes and standards. When comparing differences between bilateral and unilateral POAs, we also discuss how different values of equity shares affect social welfare and environmental quality.====We organize the remainder of the paper as follows. In Section 2, we present a duopoly model of competition between two polluting firms engaging in a bilateral POA. We first solve for the social planner's welfare maximization problem (as a benchmark), and then evaluate the alternative equilibrium outcomes with uniform taxes and absolute standards. In Section 3, we conduct a comparison between bilateral and unilateral POAs, and discuss their differences in implications for policy choice between taxes and standards. Section 4 examines how equity shares affect social welfare and the environment. Section 5 concludes.",Emission taxes vs. environmental standards under partial ownership arrangements,https://www.sciencedirect.com/science/article/pii/S1090944320302155,31 July 2020,2020,Research Article,99.0
"Giraldo Iader,Jaramillo Fernando","Latin American Reserve Fund (FLAR), Calle 84A #12-18, Bogotá, Colombia,Economics Department, Universidad del Rosario, Cl 12c #4-69, Bogotá Colombia","Received 11 June 2020, Accepted 27 July 2020, Available online 30 July 2020, Version of Record 3 September 2020.",https://doi.org/10.1016/j.rie.2020.07.004,Cited by (0)," is strictly preferred to trade. Thus, international trade does not necessarily imply greater welfare, as is the typical result in a static context under CES preferences.","International trade not only involves an exchange of goods but also implies a constant flow of information, values, and behaviours, among other factors, all of which affect agents’ preferences. In addition to economic issues, international trade has direct implications for the preferences and tastes of agents. However, the extant literature on the effects of international trade focuses on welfare and economic growth, leaving aside the impacts of international trade on the preferences of agents. Indeed, it assumes exogenous preferences.====The objective of the present paper is to analyse the implications of international trade for the evolution of agents’ preferences. To this purpose, we focus on the following questions. What are the impacts of international trade on the preferences of agents? Will preferences or cultural values converge towards homogeneous patterns? These questions are important when analysing the discussion surrounding the implications of trade for the preferences of agents and the consequences for economic performance.====The new perspective that we propose is based on the existence of external habit formation in the consumption of differentiated goods,==== which depends on the historical consumption of this type of good in the trading countries. Other individuals’ consumption levels affect an agent’s utility. Therefore, under autarky, agents take as their reference the consumption patterns of domestic individuals. After trade, each agent compares his consumption with both national and foreign consumption. The reference points of consumption for the agents are dynamic and determined by the levels of consumption of differentiated goods in both countries; this represents a clear interaction between taste channels and agent preferences after trade. Moreover, the levels of consumption of differentiated goods also determine the effects of trade on economic growth and, consequently, future consumption levels. The effects of international trade on preferences and economic growth are jointly determined.====The present article shows that consumption converges towards a homogeneous pattern between countries after trade. Nevertheless, productivity and per capita income might either converge or diverge. The combination of convergence in consumption habits and convergence or divergence in countries’ incomes after trade produces different scenarios for welfare levels, some of which indicate that autarky is strictly preferred to trade. Welfare performs better in scenarios with convergence in countries’ incomes and performs worst under trade rather than under autarky in some divergent income scenarios. This last point is true because agents in the poorer country exhibit the same consumption patterns as the richer trading partner, but they do not have enough income to satisfy their consumption preferences.====An emerging economic literature on the implications of international trade for the preferences of agents has developed around the relationship between globalization and cultural diversity. The implications of trade for cultural values, consumption traditions and preferences have been studied using different models that reveal the interactions that occur between cultures after trade and their implications for the native cultures of each country. A recent survey of this literature can be found in Bisin and Verdier (2014).====Despite the findings reported in the literature on the topic, no current papers use the habit-formation mechanism to evaluate the impact of international trade on agent preferences. For example, Francois and Van Ypersele (2002), Bala and Van Long (2005), Janeba (2004) and Rauch and Trindade (2009) treat cultural diversity as an exogenous, static process. Other papers develop an endogenous, dynamic cultural diversity process similar to the process analysed in the present paper but with other specific mechanisms. Olivier et al. (2008) model cultural identity as a positive externality under perfect competition. Finally, Maystre et al. (2014) present a model in which trade integration leads to cultural convergence through the cultural socialization of parents under a monopolistic competition scenario.====Maystre et al. (2014) provide empirical evidence that trade affects agent preferences by identifying a link between trade and convergence values. They use the World Values Survey dataset to develop an index of cultural distance and derive two stylized facts: “bilateral cultural distances exhibit significant time variation and that time variation in bilateral cultural distances is correlated with time variation in trade in (differentiated) goods” (Maystre et al., 2014).====The models of habit formation in consumption have largely been developed from the literatures on asset pricing, structural change, and the growth and distribution of wealth. The seminal papers of Abel (1990), Constantinides (1990), and Campbell and Cochrane (1999) show how models with habit formation fit the data better than models with fixed preferences. However, the former specification has not yet been used to study the impact of international trade on consumer behaviour.====The idea of using the habit formation channel in the present paper comes from evidence that utility or happiness depends on relative income, as shown by Clark and Oswald (1996) and Alesina et al. (2004). ‘The accepted standard of expenditure in the community largely determines what a person’s standard of living will be’ (Pigou, 1903). External habit formation in consumption suggests that agents appreciate not only consumption ==== but also the relative position that it affords them with respect to the consumption of other agents. In an international trade context, agents compare their consumption not only with that of domestic agents but also with agents in the trading partner country. This is because trade is more than an exchange of goods; it is also an exchange of behaviours, tastes, traditions, information, and publicity, among other factors (Bisin and Verdier, 2014).====International trade allows for cultural interchange, which affects agent behaviours. Related to the literature on habit formation, international trade generates a new reference consumption level to be reached, and consequently, the utility levels of agents depend on the possibility of achieving the reference consumption level and achieving special status within society, which provides high levels of utility.====The results of the model presented in this paper are consistent with evidence on the correlation between trade and convergence in cultural values. Moreover, the results show that the impacts of international trade on preferences and economic variables are codetermined and that the welfare impact of trade might be positive or negative relative to that of autarky.====This article proceeds as follows. After this introduction, the second section describes the fundamentals of the model, the third section presents the habit specification in a closed economy, and the fourth section presents the specification in an open economy. The fifth section reports on a simulation of the model with all dynamic effects, the sixth section presents the implications that trade has for welfare, and the seventh section concludes.",International trade and “Catching up with the Joneses”: Are the consumption patterns convergent?,https://www.sciencedirect.com/science/article/pii/S1090944320302611,30 July 2020,2020,Research Article,100.0
Batzilis Dimitris,"Department of Accounting, Economics and Finance, The American College of Greece - ACG-DEREE, Gravias 6, 15342 Agia Paraskevi, Greece","Received 16 May 2020, Accepted 21 July 2020, Available online 25 July 2020, Version of Record 3 September 2020.",https://doi.org/10.1016/j.rie.2020.07.001,Cited by (1),"I study the allocation of spending, and the impact of politics on regional growth in Greece, using a large dataset that covers the period between 1959 and 2010. I find that electoral districts that vote for the majority party and are represented by majority MPs receive more spending from the central government, and grow faster. Districts do not seem to enjoy any additional benefits when their elected representatives also occupy positions in the executive branch of the government.","Using a large dataset on government finance, regional development indices, and demographics that spans the period from 1959 to 2010 (inclusive), I analyze the impact of politics on the allocation of government spending, and on regional growth in Greece. Because of the large time dimension of the panel dataset, and the multiple changes of government during the period I study, it is possible to identify the effect of alignment with the party in government, while using fixed effects to control for time-invariant region-specific factors. Furthermore, the algorithm for the allocation of parliamentary seats allows the separation of the effect of voting for the party that wins the elections, from the effect of being represented by MPs who belong in the majority party. Also, I use data on cabinet composition to test for the effects of regional representation in the executive branch on the allocation of the spoils. That is, I separate the effect of voting for the “right” party, from the effect of being represented by the “right” people.====I find that districts that are politically leaning towards the winner of the elections tend to receive more spending from the central government. A 10% increase in the percentage of votes for the party that won the last elections is associated with a 1.3% increase in spending, when I do not control for the number of the district's MPs who belong to the majority party. Similarly, an extra seat for the majority party results to a 1% increase in spending, when I do not control for the percentage of votes for the majority party. When both the electoral support and parliamentary representation variables are added together to the model, neither of them is separately significant, although together they retain their significance. So, even though there is evidence that political alignment matters, the precise channel is unclear. Finally, representation in the executive does not increase the central government's outlays.====Growth rates are also higher in regions that supported electorally the party in power. In particular, a 10% increase in the percentage of votes for the first party is associated with a 0.95% increase in the growth rate, and an extra seat in the majority party is associated with 0.7% higher growth rates. When both the voting and representation variables are included in the regression, the effects decrease to 0.67% and 0.5% respectively, but they remain statistically significant. So, in the case of growth, it seems that there is truth in both explanations. Again, representation in the executive branch does not make a difference.====Researchers have examined several different channels through which politics can influence the distribution of central government outlays across regions. Levitt and Snyder (1995) find a strong role for parties in the allocation of spending, and in particular they provide evidence that districts that voted for Democrats received more funds in the 1984–1990 period. Albouy (2013) finds that the quantity of federal funding U.S. states receive depends on the extent to which they are represented by congressmen in the majority party, and that the type of funding depends on the congressmen's party allegiance. Another large part of the literature, which starts with Ferejohn (1974), has provided evidence that committee members are influential at 3 directing spending towards their own districts (See also Hird, 1991; Alvarez and Saving, 1997; Lee, 2000; Knight, 2005). Other work has focused on the power of the executive to influence spending patterns: Berry et al. (2010), for example, find that federal spending flows more to districts when they are represented by politicians from the president's party. The evidence on the effects of politics on growth is more mixed. Levitt and Poterba (1999) find that seniority of representatives is positively associated with economic growth but not with government spending. On the other hand, Cohen et al. (2010) find that federal spending instrumented by changes in congressional committee leadership leads to lower investment and employment.====Psycharis (2008) and Monastiriotis and Psycharis (2009) analyze the patterns of spending distribution in Greece between 1975 and 2005, and document large changes across different political periods. Lambrinidis et al. (2005) study the allocation of infrastructure investment spending in the 1982–1994 period, and do not find an effect of vote percentage for the majority on infrastructure investment. Rodriguez-Pose et al. (2012) find a positive association between growth and the difference in voting percentage between the winner of the last elections and the opposition. The current paper expands on past efforts by i) Examining a bigger time span, that includes the years before the dictatorship (1967–1974), starting in 1959. The larger sample increases the identifying variation, because it includes more government changes. ii) Using a richer set of political variables, including information on representation in the parliament and the cabinet. iii) Adding in the analysis a rich set of demographic controls, based on the census results. iv) Analyzing the total of public expenses that can be disaggregated at the regional level, instead of only the investment spending.",The political determinants of government spending allocation and growth,https://www.sciencedirect.com/science/article/pii/S1090944320302131,25 July 2020,2020,Research Article,101.0
Banerjee Subrato,"Australia India Institute, University of Melbourne, Australia,Centre for Behavioural Economics, Technology and Society (BEST), Queensland University of Technology, Brisbane, Australia","Received 24 May 2020, Accepted 22 July 2020, Available online 24 July 2020, Version of Record 3 September 2020.",https://doi.org/10.1016/j.rie.2020.07.002,Cited by (1),Limited use has been made of power analyses in ,"Experiments in social sciences often witness control groups with invariant outcomes. Proposers in ultimatum games (UG),==== for example, are frequently known to offer 50% of the pie to the responders in experimental set-ups (examples are cited in Camerer, 2003; and Bardsley et al., 2009), contrary to what non-cooperative game theory would suggest (Hoffman et al., 1994 offers a possible explanation). In fact, according to Henrich and Henrich (2007), “The UG has been tested in more diverse cultural settings than any other experiment and is probably second overall as the “most run” experiment ... and ... general observations can be made about the results. First, UG behavior is extremely homogeneous ... proposers make ... modal offers at 50%”. Even in the area of cooperative game theory, experiments focused on testing Nash’s (1950) bargaining axioms observed the 50-50 split in face to face negotiations between ==== the subjects in the control group (Nydegger and Owen, 1975).====Suppose we have (say monetary) resources to fund 180 subjects for an experiment on ultimatum bargaining involving four treatment groups. The baseline treatment is the standard UG. The question we are interested in, is ‘====’==== Suppose we did that with (say) 44 subjects in the control group. Given the experimental evidence so far, we would expect 22 offers, either equal to, or very close to 50%. These 22 observations, however, will only verify what has been observed in previous experiments on ultimatum games. Interestingly, we can be sufficiently confident about the mean of 50% offers, with just 18 subjects (nine pairs) in the control group. Doing this will leave us with 162 individuals for the remaining three treatment groups (that is, with (say) 54 subjects for each group). The reality of experimental research is that, the central message often comes from the observations made in the treatment group(s). Therefore, it will benefit to allocate fewer subjects to the control group since we learn more from the treatments. I specifically derive the following expression for the minimum number ====, of subject-pairs we need to allocate to the control group of an ultimatum game (or any two-person bargaining game in general).====In the above expression, ==== and ==== are respectively the values that we do not want our probabilities of type I and type II errors to exceed; ==== and ==== ( > ====, without loss of generality) are the population mean-values assumed respectively under the null and the alternate hypotheses (associated with ==== densities ==== (with the parameter vector ====) and ==== (with the parameter vector ====), respectively); and ==== is the (pre-observed) variance of the outcome variable ==== of interest in the control group. Our decision function, for some critical value ====, takes the following simple form====I will demonstrate that the expression in (1) above, is in fact, a general solution for a family of density pairs ==== and ====, that have identical variance==== and are permissible for our testing procedure associated with ==== above.==== We do not ignore that with observations more than ==== above, one is more confident of the assumed mean under the null hypothesis - but we are not sure if so much confidence is desirable particularly when resources for research are limited. Allotting more than 18 subjects to the control group of an ultimatum game is like using two cells of a donor for the purposes of cloning which always requires only one - the additional cell has no more information to contribute.",Sample sizes in experimental games,https://www.sciencedirect.com/science/article/pii/S1090944320302295,24 July 2020,2020,Research Article,102.0
Ji Silvia,"Loyola Marymount University, 817 Westmount Drive, West Hollywood, CA 90069, United States","Received 28 May 2020, Accepted 22 July 2020, Available online 24 July 2020, Version of Record 3 September 2020.",https://doi.org/10.1016/j.rie.2020.07.003,Cited by (2),"This study will examine the introduction, development and popularity of flower still lives as well as of Dutch horticulture, along with Holland’s early flower trade and its effect on Tulipmania, which took place during the Baroque era in Europe from 1636 to 1637. The highest-ranked genres, including history paintings, portraits, and altarpieces were slowly replaced by formerly less-regarded genres, such as genre paintings, landscape paintings, and still-lives found a new audience in seventeenth-century Holland. Artists such as Ambrosius Bosschaert and Jan Brueghel the Elder specialized in flower still lifes, depicting elegant vases holding large varieties of perfect flowers. Focusing on Ambrosius Bosschaert’s ====, created in 1614 using oil on copper, this paper will examine the relationship between the popularity of horticulture and flower still lives in regard to their effects on the contemporary Dutch economy. First, the painting's formal elements will be analyzed, followed by a brief biography of Ambrosius Bosschaert, specifically discussing the influences on his art and his role as a pioneer in the genre of flower still lives. It will also study the interdisciplinary relationship between Art and Economics and implement past problems to identify similar market issues in the 21st century.",None,The true value of flowers and their effect on the Dutch economy. An interdisciplinary relationship between Art and Economics,https://www.sciencedirect.com/science/article/pii/S1090944320302325,24 July 2020,2020,Research Article,103.0
Aurazo Jose,"Central Reserve Bank of Peru, Lima, Peru","Received 5 April 2020, Accepted 2 June 2020, Available online 13 June 2020, Version of Record 1 December 2020.",https://doi.org/10.1016/j.rie.2020.06.002,Cited by (0),"Merchant internalization has been used to explain why merchants may accept high fees to accept card payments. However, merchants seem to be more resistant in some economic activities or countries; in particular, when the ","Over the last decade of the twentieth century, it was common to say that merchants accept payment cards only if the merchant discount (the fee that the retailer pays to the acquirer in each card transaction) is not higher than the net convenience cost of cash; i.e. the cost savings from accepting card instead of cash (Baxter, 1983). However, this proposition did not help to understand the “must-take card” phenomenon in which retailers may accept high merchant fees, even if they are above their net convenience cost of cash, in order to attract new consumers (Rochet, Tirole, 2011, Vickers).====The “must-take card” phenomenon or merchant internalization refers to the merchants’ ability to increase the retail price in order to obtain the net cardholder benefit of paying by card instead of cash, without losing market share (Rochet, Tirole, 2002, Rochet, Tirole, 2011).==== As a result of the merchant internalization, merchants are less resistant to accept high fees, and thus induces a card network to set inefficiently high merchant fees (Rochet and Tirole, 2002). The merchant internalization holds under different competition models: monopoly retailing, perfect retailing competition and Hotelling-Salop-Lerner differentiated products competition (Rochet, Tirole, 2002, Rochet, Tirole, 2011); the Perlof-Salop competition model (Ding and Wright, 2017) and the general discrete choice model and the representative consumer model (Ding, 2014).====However, there are some situations where the merchant internalization does not hold. For instance, if consumers are not aware of whether merchants accept cards, or if the retail prices are capped (Ding, 2014). This paper explores another reason where the merchant internalization cannot hold: tax evasion through cash payments.====According to Madzharova (2014) and Immordino and Russo (2018), cash payments are strongly associated with VAT evasion since they are not traceable and eases the under-reporting and its usage in illegal activities. In addition, Medina and Schneider (2018) show that in those countries with a high degree of shadow economy, cash is the most used payment instrument. Therefore, this feature of cash payments associated with tax evasion should be included into the analysis of the card industry; in particular, when the shadow economy is sizeable.====Aurazo and Vasquez (2020) include the government in the model setup of Rochet and Tirole (2011) to take tax evasion into consideration. In their model, merchants benefit from tax evasion through cash payments since they do not provide a receipt when the consumer pays by cash instead of card, and thus, they have an additional benefit per cash payment which reduces the net convenience cost of cash. The authors, however, focus on the role of tax evasion in the tourist test (also called the merchant indifferent test)==== rather than in the merchant internalization condition. This paper aims to fill gaps in this analysis.====We develop two cases for tax evasion. In the exogenous case, merchants benefit from tax evasion since it reduces the net convenience cost of cash while in the strategic case, merchants can increase the retail price by evading taxes in cash payments because the cardholder values the receipt received in card payments.====The rest of the paper is structured as follows. Section 2 describes a standard model of payment card platform to explain the merchant internalization condition. Section 3 incorporates tax evasion in the model setup when consumers do not ask for the receipt in cash payments or it is determined by some country variable (exogenous tax evasion). Section 4 explores tax evasion for strategic reasons since the net cardholder benefit per card payment is affected by the merchants’ decision to provide receipts in cash payments. Section 5 summarizes the key insights.",Merchant internalization and tax evasion,https://www.sciencedirect.com/science/article/pii/S1090944320301472,13 June 2020,2020,Research Article,104.0
Olszewski Wojciech,"Department of Economics, Northwestern University, Evanston, IL 60208, Unites states","Received 1 April 2020, Accepted 2 June 2020, Available online 7 June 2020, Version of Record 3 September 2020.",https://doi.org/10.1016/j.rie.2020.06.001,Cited by (2),"We propose a model in which researchers maximize the number of times they are cited in later papers. The equilibrium is inefficient, because researchers distort their effort toward writing on popular topics. This inefficiency is affected by various factors, policies and customs. We explored the effect of a variety of such factors. In particular, we argue that the inefficiency is likely to be higher in disciplines (areas of research) in which talent is uniform across topics rather than more topic specific. We also determine conditions under which assigning a higher weight to citations in papers published in higher-ranked journals enhances efficiency.","Citations have always played an important role in making assessments in academia. Papers with an extremely high number of citations have always been admired in some ways, and researchers have used citation frequency in their evaluations of other researchers. In economics, there seems to have been a recent surge of interest in using citations to assess researchers, academic journals, and various research institutions. Academic journals have increased their emphasis on citation indices. References to citations have become more common in peer evaluations. This trend may be related to researchers becoming more specialized, and finding it more difficult to evaluate research which does not belong to their research area, or it may simply be related to a need to search for more objective evaluation measures. In addition, Google Scholar delivered an easily available, and quickly updatable citation count.====Of course, the use of citations for assessing researchers provides them with strategic incentives for seeking projects that generate citations. The existing research on citations does not explore such strategic incentives, but is focused instead on measurement issues. Various citation indexes have been proposed as measures of scientific influence, among which the h-index (Hirsch, 2005) is probably the index most commonly referred to. In a recent article, (Perry and Reny, 2016) propose their own index, and provide a summary of the literature on citation indexes. Other papers address the question of whether citation indexes can provide a “compelling” assessment of scientists and academic departments (see Ellison 2010 and Ellison, 2012), or how to use indirect citations to obtain a more accurate assessment (see Chung et al., 2017).====We are broadly interested in how the importance of citations in evaluating researchers affects research. In the present model, we explore the following basic trade-off in the context of citations: Researchers choose topics on which they write their papers. Researchers face individual incentives coming from a higher number of future citations, because this enables them to compare favorably to other researchers. This makes their incentives misaligned with what is socially optimal. So, they are willing to sacrifice some social value to write papers that they expect to be cited more frequently. This creates a spillover effect, and more researchers with misaligned incentives sacrifice even more social value to write papers on popular topics. The resulting outcome is socially inefficient. The size of this inefficiency depends on various factors, such as the relative strength of the incentives coming from citations compared to the incentives coming from social value, the distribution of researchers’ talent, or how prone a field is to trends. We provide comparative statics with respect to such factors.====All our results follow from the simple trade-off between enduring quality (which we view as a synonym for social value) and strategic citation-seeking behavior caused by misaligned incentives. We believe that this trade-off should be present in any model of citation-driven research. We elaborate on this point in Section 3.1, after we present our model. However, a plethora of other motives—such as the desire to pioneer research in unexplored areas, or motives not related directly to citation counts—may be important for answering particular questions. In that light, many of our results are best viewed as thought-provoking, rather than as intended to offer definite answers to the questions they address. Despite this caveat, our basic model provides numerous insights regarding practical issues.====One can, for example, wonder whether assigning a higher weight to citations in papers published in higher-ranked journals would enhance efficiency. Our model suggests that it would do so in fields in which higher-quality researchers execute ideas which they find more socially valuable, and lower-quality researchers “shop” for projects that will generate citations. The reason is that the spillover effect is weaker when citations are weighted, because the average quality of researchers seeking citations is lower than the average quality of all researchers. In addition, the researchers who shop for citations publish (on average) in lower-ranked journals.====We also show that, perhaps surprisingly, higher-quality papers are not necessarily cited more frequently than others, and we argue that disciplines (areas of research) in which talent is uniform across topics rather than more topic specific tend to be less efficient.====Another set of potentially interesting questions concerns research dynamics. For example, one may wonder how the flow of papers on various topics would respond to a revival of interest in certain areas. We show that researchers motivated by citations switch to writing on topics in which breakthroughs are likely to occur (e.g., due to new techniques or technology, or the anticipated availability of new data sets), and they do so as soon as they anticipate such breakthroughs. Paradoxically, the inflow of papers on such topics at the time a breakthrough ==== to be anticipated may exceed the inflow of papers on such topics at the time the breakthrough actually occurs. Our model suggests that this paradox is likely to be observed when researchers need not sacrifice much quality to write papers that they expect to be cited more frequently. Intuitively, the anticipated inflow of papers on some topic in some period ==== provides incentives to researchers living in period ==== for writing on this topic. And if there are many researchers affected by these incentives, researchers living in period ==== are provided even stronger incentives for writing on this topic. As a result, stronger incentives are transmitted to earlier periods.====Our model also supports the view that assigning lower weights to citations in papers that appear shortly after a cited paper, and higher weights to citations in papers that appear a longer time after the given paper.====Finally, we emphasize that the researchers in our model seek a higher number of future citations, rather than maximizing the value of some more-involved citation indices. Replacing simple citation counts with such indices is likely to reduce the inefficiency described in our paper, and our results support a number of conjectures along these lines. However, such indices seem less useful, and less used, for many practical decisions regarding researchers.====The rest of the paper is organized as follows. We first present our basic model in Section 2, and discuss its equilibria in Section 3. (Some of this discussion is relegated to the Appendix.) This is followed by the results on comparative statics. The most substantive part of our analysis is contained in Sections 5 and 6, in which we apply our model to answer some specific questions of practical importance. We conclude, discuss some extensions, and suggest some additional, potentially interesting questions in Section 7.",A theory of citations,https://www.sciencedirect.com/science/article/pii/S109094432030140X,7 June 2020,2020,Research Article,105.0
"Figal Garone Lucas,López Villalba Paula A.,Maffioli Alessandro,Ruzzier Christian A.","Strategy and Development Effectiveness Department, IDB Invest, Buenos Aires, Argentina, and Department of Economics, Universidad de San Andrés, Buenos Aires, Argentina,Department of Economics, Universidad de San Andrés, Buenos Aires, Argentina,Strategy and Development Effectiveness Department, IDB Invest, Washington D.C., USA","Received 22 February 2020, Revised 27 April 2020, Accepted 29 April 2020, Available online 8 May 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.rie.2020.04.004,Cited by (5),"While the accumulation of factors of production, both physical and human capital, has helped Latin America and the Caribbean (LAC) to narrow the income gap with developed economies, aggregate productivity is still relatively low. Although there are numerous determinants of aggregate productivity, it is largely based on the underlying productivity of all firms in the economy. Using firm-level data from several waves of the World Bank Enterprise Survey and Chile's National Manufacturing Survey, we explore the ==== percentile firm; (ii) productivity differences persist over time – regressing a firm's current productivity on its one-year lagged productivity yields an autoregressive coefficient of around 0.9; and (iii) most of the growth in aggregate productivity comes from improvements in the productivity of existing firms.","The field of economics heavily emphasizes productivity. Indeed, a great many books and papers on productivity begin by quoting Paul Krugman's well-known axiom: “Productivity isn't everything, but in the long run it is almost everything.” This focus is largely because aggregate productivity is believed to explain cross-country differences in per capita income, economic growth, and, ultimately, standards of living. Nickell (1996: 725) goes so far as to proclaim that “productivity growth […] is the cause of the ‘wealth of nations.’” As populations age and countries level out in terms of educational attainments and labor force participation, productivity is likely to be the main driver of future growth and prosperity (OECD, 2015).====There are numerous determinants of aggregate productivity, but the increasing availability of firm-level data is allowing researchers to focus more and more on firm-level productivity as a key driver of aggregate productivity. After all, aggregate productivity must be, to a large extent, the result of the underlying productivity of all firms in the economy.====A firm's productivity is positively correlated with its profits (Foster et al., 2008; Chandra et al., 2016), size and growth (Balk, 2001; Wagner, 2002; Koellinger, 2008; Harrison et al., 2013), and survival rate (Aw et al., 2001; Fariñas and Ruano, 2005; Syverson, 2011). More productive firms have also been found to have higher chances of entering export markets (Bernard et al., 2003; Melitz, 2003; Bernard and Jensen, 2004; Cassiman et al., 2010; Melitz and Redding, 2015) and receiving foreign direct investment (Kimura and Kiyota, 2006; Arnold and Hussinger, 2010; Borin and Mancini, 2016).====In line with this evidence, the literature on the relationship between firm-level productivity and aggregate productivity tends to find that within-firm resource reallocation produces between-firms resource reallocations, which in turn lead to changes in aggregate productivity (Foster et al., 2001; Hsieh and Klenow, 2009; Brandt et al., 2012; Petrin and Levinsohn, 2012). Bloom et al. (2010) and Restuccia and Rogerson (2017) further confirm this finding and point out that cross-country productivity differences can be explained in part by differences in within-firm resource ====allocation. Ultimately, increases in firm-level productivity have been related to improvements in countries’ living standards, measured using GDP per capita (Acemoglu et al., 2006; Restuccia and Rogerson, 2008; Bartelsman et al., 2013), wages (Van Biesebroeck, 2011; Bartelsman et al., 2015; Konings and Vanormelingen, 2015), or employment (Hall et al., 2008; Harrison et al., 2014; Dachs and Peters, 2014).====Large and persistent differences in firm productivity within narrowly defined industries have been widely documented (see the surveys by Bartelsman and Doms, 2000; and Syverson, 2011). Gibbons and Henderson (2013) refer to these as persistent performance differences (PPDs) among seemingly similar enterprises (SSEs). The existence of such discrepancies implies there may be much to be done to positively affect aggregate productivity through firm-level productivity.====In this paper, we document the existence of PPDs among SSEs in Latin America and the Caribbean (LAC) – the ==== question on productivity dispersion – using novel empirical evidence from firms in the region based on the World Bank Enterprise Survey (WBES).==== We find significant performance differences within two-digit industries in the manufacturing sector: firms in the 90th percentile of the productivity distribution produce almost seven times as much output with the same measured inputs as firms in the 10th percentile. These productivity differences persist over time, although dispersion seems to have reduced over time: the average 90–10 total factor productivity (TFP) ratios are 8.09, 6.09 and 5.05 for 2006, 2009–10, and 2016–17. These figures are similar to those found in previous studies in developing countries – even though our results could be picking up intrinsic differences in the industries that make up the large two-digit aggregates – but much lower than corresponding figures for developed countries.====Using a narrower (four-digit) definition of industry and data from Chile, we find that productivity differences are somewhat smaller in Chilean manufacturing: the average 90–10 TFP ratio is almost four. The figures change little if we aggregate industries up to two digits, suggesting that aggregation might be explaining only a very small part of the productivity difference, which would reinforce our findings for the LAC region. Performance differences in Chile are also persistent: regressing a producer's current TFP on its one-year-lagged TFP yields an autoregressive coefficient of around 0.9 for every four-digit industry. Using a decomposition of aggregate productivity, we show that most of its growth comes from improvements in the productivity of existing firms, and some from the entry and exit of firms. Little of its growth appears to be due to reallocations of output between firms (i.e., more productive firms growing faster than less productive ones).====The rest of the paper is organized as follows. Section 2 measures performance differences at the firm level in LAC using the WBES. Section 3 reinforces the argument of performance differences and characterizes its persistence and dynamics for the case of Chile using longitudinal firm-level data. To the best of our knowledge, this is the first paper that examines productivity dispersion for the LAC region using firm-level data. Until now, most of the studies have focused on developed countries (e.g., Disney et al., 2003; Syverson, 2004; Ábrahám and White. 2006; Ito and Lechevalier, 2009). Section 4 contains concluding remarks.",Firm-level productivity in Latin America and the Caribbean,https://www.sciencedirect.com/science/article/pii/S1090944320300697,8 May 2020,2020,Research Article,106.0
Gao Yijin,"Iowa State University, United States","Received 20 March 2020, Accepted 18 April 2020, Available online 1 May 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.rie.2020.04.003,Cited by (0),"Learning is the basic thing for a person’s growth, whenever in school or on the work. Also the economic growth of a country depends its technology and science. The knowledge diffusion in a firm dynamic system is important. Some former models have been given by Fernando E. Alvarez, Francisco J. Buera and Robert E. Lucas, Jr’s paper: “Models of Idea Flows” (Alvarez et al., 2008). Our model based on the earlier work but extend the definition of spread direction based on Fourier’s law, in which case can also explain the asymptotic equilibrium state. Solving the partial differential equation helps us to learn the solution well. A simulation is given using finite difference method to sketch the curve moving, also comparing with the realistic economic curves and they are fit. It can well explain the economic stable in the closed system of a single country and a “huge” change under some special outside “source” power.","Learning is the basic thing for a person’s growth, whenever in school or on the work. Also the economic growth of a country depends its technology and science. The knowledge diffusion in a firm dynamic system is important. Some former models have been given before. In Fernando E. Alvarez, Francisco J. Buera and Robert E. Lucas, Jr’s paper: “Models of Idea Flows” (Alvarez et al., 2008). They give a summary for different kinds of mathematical models. And in Francisco J.Buera and his co-authors’ papers: “Idea Flows and Economics Growth” (Francisco and Robert, 2018) and “The Global Diffusion of Ideas” (Buera and Oberfield, 2020), they provide a tractbale theory of innovation and technology diffusion to explore the role of international trade in the process of development. Provide some conditions under which each country’s equilibrium frontier of knowledge converges to a Frechet distribution and derive a systems of differential equations describing the evolution. And recently many contributions with respect to the models have been made by Luttmer (2018), SeHyoun Ahn, Greg Kaplan, Benjamin Moll, Thomas Winberry and Christian Wolf’s paper: When Inequality Matters for Macro and Macro Matters for Inequality” (Ahn et al., 2017). Santiago Caicedo: Note on Idea Diffusion Models with Cohort Structures. Caicedo (2018). There are also many applications based on the technology diffusion model: Jesse Perla and Christopher Tonetti’s paper: Equilibruim Imitation and Growth (Perla and Tonetti, 2014), they prove the growth rate depends on characteristics of the productivity distribution, with a thicker-tailed distribution leading to more growth. “Innovation and Growth with Financial and other, Frictions” by Chiu et al. (2017), they study the generation of ideas in a model of endogenous growth, where productivity increases with innovation and where exchange of ideas allows those with comparative advantages to implement them. Also input-out architecture theory is given by Oberfield (2018), developing a theory in which the network structure of production-who buys inputs from whom-forms endogenously.====In our model, we extended the definition of spread direction (the space is continuous, from discrete to spatial diffusion), but different from Diego A. Comin, Mikhail Dmitriev and Esteban Rossi-Hansberg: “The Spatial Diffusion of Technology” (Comin et al., 2012), they study technology diffusion across countries and over time empirically. We construct the partial differential model based on Fourier’s law and we find that inside the country, the knowledge diffusion (study) can be stable in a finite time and the poverty can be less. After the equations built, I will give the exact solution theoretically with and without external sources. And there are some important theorems given (Maximum Principle, Compare Principle). So the difference between different countries is fixed at first, determined by the boundary initial conditions. However, if there exists important “external power” (policy, war, help from other countries), the balance will be destroyed (In the final part of simulation Section 5, I will take China and South Korea as two examples).====The left parts of the paper is designed as following: In Section 2, I may review the former famous models in idea flow theoretical including: (1) Poisson Arrival Model; (2) Continuous Arrival Model; (3) Multiple Effects of meeting models; (4) Production decision Model (involve people making choices). In Section 3, I will give our model based on fourier’s law, also can be seen as an extension of Production decision model. In Section 4 is the results of the equation, it is divided into two special cases: with or without external sources. And also from some partial differential equation knowledge is applied into these models. Except the theory analysis on equation itself, we also need to give the simulation based on finite difference method since of explicit solution exists, it is Section 5. And last Section 6 is the conclusion and discussion.",Knowledge Diffusion and economic Growth based on Fourier’s law,https://www.sciencedirect.com/science/article/pii/S1090944320301174,1 May 2020,2020,Research Article,107.0
"Javed Rashid,Mughal Mazhar","Pau Business School, France,CATT, University of Pau and Pays de l'Adour, France","Received 5 February 2020, Accepted 18 April 2020, Available online 30 April 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.rie.2020.04.001,Cited by (6),"A potential manifestation of son preference prevalent in Asia is gender-specific birth-spacing. The time couples wait before moving on to subsequent pregnancy remains short as long as desired number of sons are not born, leading to higher demand on the mother's body and greater health risks for mother and child. In this study, we examine this phenomenon using three representative surveys of Pakistani households and duration model estimators. We find strong evidence for differential behavior at early parities. Women whose first or second children are sons have significantly longer subsequent birth intervals compared with women with no sons. Birth-spacing differs substantially by parity and number of children. The association seems to have undergone little significant change over the past two decades. Besides, the likelihood of risky births (i.e. those occuring less than 24 or 18 months from the previous birth) is higher among women without one or more sons.","“The harvest is so ripe, yet why are daughters still born?” (A proverb from the Indian subcontinent)====Sen (1990) famously pointed out that there were more than a hundred million missing girls in Asia due to the prevalent son-preferring attitudes. In societies where the sex-selection methods are unavailable or less accessible, or are not considered socially acceptable, parent's planned fertility remains incomplete until and unless the desired number of sons is achieved. One potential demographic consequence of this disproportionate desire for sons appears in the form of shortened birth spacing. Couples with no sons at earlier parities may end up abbreviating the interval to the next birth in search of male offspring (Milazzo, 2012). This can have adverse effects on the mother's and children's health outcomes. There is a higher risk of maternal depletion, pregnancy-related complications and maternal mortality. Children born after short preceding intervals face increased odds of both neonatal and under-five mortality, even though the impact may only appear in high parity births (Kozuki and Walker, 2013). In their analysis of demographic data from 77 countries, Molitoris et al. (2019) showed that intervals shorter than 36 months substantially increase the probability of infant death.====The interval-shortening aspect of son preference is of particular significance to countries such as Pakistan where sex ratios are skewed and maternal and child mortality remain high. Son preference mainly manifests itself through larger family size. Besides, differential birth stopping is widely practised, resulting in high sex ratios at last birth (Javed and Mughal, 2018; Zaidi and Morgan, 2016).====In this study, we investigate how preference for sons affects birth-to-birth intervals among Pakistani women. We carry out a comprehensive examination of changes in birth spacing with respect to various aspects of son preference using data from three representative household surveys and employing a set of parametric, semi- and non-parametric methods. We analyze parity-wise effects of observed preference for sons on subsequent birth spacing and the differential impact of the number of sons born to a woman at a given parity. In addition, we gage the effect of the sex of the eldest child on the waiting time to the subsequent birth. The use of three rounds of the survey covering fertility history spanning two decades helps us understand the variation in the relationship that might occur over time. During the examined period (1991 – 2013), fertility rates in Pakistan fell and contraceptive prevalence picked up.==== We investigate whether disproportionate preference for male offspring increases the probability of risky births (those less than 24 or 18 months from the previous birth). Finally, we explore the individual and household characteristics of women who show sex-selective interval shortening behavior. We find strong evidence for a son-preferring behavior at early parities. The association seems to have undergone little significant change over the past two decades. Besides, the likelihood of risky births is higher among women without one or more sons.====The remaining content of the paper is organized as follows: Section 2 briefly reviews relevant literature followed by an overview of the son preference and spacing situation in Pakistan in Section 3. Section 4 describes the datasets used and discusses the empirical methodology and the models employed. Findings are presented in Section 5 followed by robustness measures in Section 6. Section 7 concludes and discusses possible implications of the findings.",Preference for boys and length of birth intervals in Pakistan,https://www.sciencedirect.com/science/article/pii/S1090944320300478,30 April 2020,2020,Research Article,108.0
"Malinowski Mariusz,Jabłońska-Porzuczek Lidia","Poznań University of Life Sciences, Faculty of Economics and Social Sciences, Department of Economics, Poland","Received 23 February 2020, Accepted 18 April 2020, Available online 28 April 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.rie.2020.04.002,Cited by (2),"The purpose of this paper is to present the evolution of female employment rates between 2013 and 2017 in selected Central and Eastern European countries (Poland, Czech Republic, Slovakia, Slovenia, Lithuania, Latvia, Estonia, Romania, Bulgaria, Croatia and Hungary), and to analyze the interregional relationships between female activity and education levels. Empirical data at a national level was analyzed with the metrics of ","The increase in female activity has become one of the main objectives of the labor market policy. The growing interest in the economic activity of this social group is the consequence of a demographic slowdown and of related changes in the workforce structure. Potential labor resources decrease because older age groups are not replaced by younger ones who start their professional activity. These problems have become a major challenge faced by many European Union countries. European Union authorities implemented incentive measures to increase employment in certain social groups, especially women and the elderly in order to prevent the decline in labor resources. The establishment of the Lisbon Strategy was a milestone in improving the condition of the labor market. The Strategy detailed the objectives for employment (including employment quality), labor productivity and social cohesion to be met by 2010. However, most EU countries failed in their implementation efforts, and therefore a new strategy (European Commisson, 2010) was adopted in 2010. One of the goals set out therein is to increase the employment ratio. EU countries are to implement optimum solutions in the field of social policy, labor market policy and family policy in order to make this happen.====The labor market must be researched on and analyzed with the primary focus on the economic activity of selected social groups, including women, in order to validate the existing solutions. Note that the labor market is specific in that it witnesses the economic and social consequences of diverse processes taking place in the economy. This is because the labor market is strongly related to other fields of socioeconomic activity which have an effect on both the supply of and demand for labor. The determinants of labor supply include population growth and migration, and workforce activation and deactivation, whereas demand for labor is driven by such factors as the economic growth rate, structural transformation in the economy and internal changes in enterprises. The purpose of this publication is to present the evolution of the employment rate for women between 2013 and 2017 in selected Central and Eastern European countries, and to analyze the interregional relationships between the levels of female activity and education (based on 2016 statistical data).====Multidimensional statistical analysis methods were used to quantify the activity and determine the education level of women in selected regions of the European Union. This is because it is necessary to compare many research objects described with a broad set of variables in such an analysis. Therefore, the development level of the phenomenon concerned is difficult to express with one measurable feature. This study focuses on regions in selected EU countries who joined the Community in 2004 (Czech Republic, Estonia, Lithuania, Latvia, Poland, Slovakia, Slovenia and Hungary) or later (Bulgaria and Romania in 2007 and Croatia in 2013) and who share a border. Therefore, Cyprus and Malta are excluded from the analysis. This group was selected largely because of the availability of up-to-date comparable statistical data. Another reason for choosing these countries was their economic and cultural similarity.====Empirical data for the countries selected was analyzed with measures of descriptive statistics, demographic measures (including the fertility rate, demographic dependency index, and life expectancy), and indices of the condition of the labor market (including the economic activity rate, employment rate, unemployment rate and EPL). An analysis of spatial autocorrelation was carried out to determine the strength of spatial relationships between different regions in terms of female activity and education levels. Also, the dependence between synthetic indicators of female activity and education levels was empirically analyzed based on spatial regression. The analysis was based on Eurostat and OECD data.",Female activity and education levels in selected European Union countries,https://www.sciencedirect.com/science/article/pii/S1090944320300703,28 April 2020,2020,Research Article,109.0
"Nguyen Tien Van,Khieu Hoang","Banking Academy, Vietnam,Vietnam International Arbitration Center, Vietnam,Johannes Gutenberg University Mainz, Germany","Received 31 January 2020, Accepted 27 February 2020, Available online 3 March 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.rie.2020.02.004,Cited by (3)," is higher than a positive threshold. Second, the tax rate is lower than a cap which rises in ==== but decreases in labour income heterogeneity.","[Motivation] There has been a strong consensus that wealth inequality is rising in countries where the data are available. An indirect evidence is the success of the book ==== by Thomas Piketty. In his book, Piketty (2014) proposes a global wealth tax to reduce wealth inequality. In particular, he proposes a tax on all types of assets (i.e. wealth or capital) while the currently existing capital taxes in reality are mostly based on real property. Under these existing taxes systems, he argues that heavily indebted households are taxed in the same way as households with no debt and that financial assets are ignored. He also argues that a wealth tax lowers the net rate of return on wealth and therefore reduces the gap between the (net) rate of return and the growth rate, ==== which is shown to be a central determinant of long run wealth inequality (Piketty and Zucman, 2015). Mankiw (2015), however, is concerned about consumption inequality rather than wealth inequality. In his view, the wealth tax “may look attractive” to reducing consumption inequality and “should be as large as it can be”. Nevertheless, he proposes a progressive consumption tax as “a better way to persue equality”.====[The open questions] We ask under which conditions a wealth tax reduces long run wealth inequality. We also ask whether the wealth tax reduces consumption inequality in the long run.====[The objectives] It is the objective of this paper to provide a framework that allows for studying how a wealth tax is related to wealth inequality and consumption inequality. We aim at formulating clear predictions about conditions under which consumption and wealth inequality rises.====[The setup] We consider a small open economy model. There are free capital flows and free trade, but labour is immobile. The rate of return on capital (wealth) is exogenously given. Firms employ capital and labour to produce a final good. Households supply labour inelastically and accumulate wealth. There is ex-ante heterogeneity in initial wealth and initial labour income. Specifically, households draw their initial levels of wealth and labour income from some given distribution before becoming economically active. We assume there is no ex-post uncertainty. We therefore study a deterministic optimal consumption-saving problem conditional on initial wealth and initial labour income. We use the coefficient of variation to measure inequality.====[Contributions] While the existing literature mostly discusses effects of top income tax rates on top wealth holders, our paper investigates how a wealth tax affects the entire distributions of wealth and consumption. The contributions of this paper are twofold. First, the paper shows that taxing wealth does not always reduce long run wealth inequality. We find that an increase in the wealth tax causes long run wealth inequality to decrease under three conditions. The first condition is consumption growth exceeds output growth, which is empirically plausible====. The second condition requires that the difference between the rate of return on wealth and the growth rate, ==== is sufficiently larger than a positive threshold. The third condition stipulates that the tax rate must be less than a cap which increases in ==== but decreases in the degree of heterogeneity in labour income. The tax cap is equal to ==== in the absence of labour income heterogeneity.====Second, our paper shows that a wealth tax has identical effects on wealth and consumption inequality in the long run. This is because consumption inequality and wealth inequality measured the coefficient of variation converge to an identical function of the wealth tax in the long run. This implies that a wealth tax appears to be a promising policy that corrects both the wealth distribution and the consumption distribution in the long run.====[Related literature] Our contributions are related to the literature on the effects of tax policy on inequality. Becker and Tomes (1979) investigate the effect of ==== in an OLG model with two-period lives and find that taxation may have unintended consequences for inequality. Castaneda et al. (2003) and Cagetti and De Nardi (2007) also find that eliminating ==== only produces very small or even perverse effects on inequality. Laitner (2001) constructs a standard two-period OLG model with intergenerational altruism and finds that ==== can reduce wealth inequality. The similarity of these papers is that they consider estate and bequest taxes.====Recent literature has paid great attention to the effects of taxes on inequality at the top. Benhabib et al. (2011) show that reducing estate taxes or capital income taxes can significantly increase wealth inequality in the top tail of the wealth distribution. This result holds only if idiosyncratic rates of return across generations are a main driver of wealth inequality. Constructing a stationary Bewley model with investment risk, Aoki and Nirei (2016) find that a decrease in top tax rates can explain the increasing concentration of wealth at the top. Aoki and Nirei (2017) study how changes in tax rates can account for the evolution of top incomes in the US and find that the tax cut can explain the increasing trend of the top 1 percent income share in recent decades. Kaymak and Poschke (2016) quantify the effects of tax reduction for top income groups and find that changes in taxes and transfers account for approximately half of the increase in wealth concentration in the US over the last 50 years. Along similar lines, Hubmer et al. (2018) examine a number of plausible sources of rising wealth inequality in the US over the last 40 years and find that the main driver of increasing wealth inequality since the late 1970s is the significant drop in tax progressivity.====The existing literature neither examines distributional effects of a wealth tax nor analyzes its effects on the entire distribution of wealth and consumption. Our paper fills these two gaps.====[Structure of the paper] In the next section we present a standard small open economy model in which wealth is globally taxed. Section 3 describes the distributions of wealth and consumption and the measure of inequality. section 4 presents our findings and section 5 concludes.",Does a global wealth tax reduce inequality? When Piketty meets Mankiw,https://www.sciencedirect.com/science/article/pii/S1090944320300387,3 March 2020,2020,Research Article,110.0
Miles William,"Department of Economics, Wichita State University, Wichita, KS 67260-0078, USA","Received 4 February 2020, Accepted 28 February 2020, Available online 29 February 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.rie.2020.02.005,Cited by (1),"Previous studies on regional convergence in the US have employed varied methodologies and yielded different conclusions. Some authors report evidence that convergence has grown stronger in recent decades, while others find recent years have seen an end to convergence. We test for convergence with a method that allows for structural change. We find, first, that there is little overall evidence for convergence within the US. Results also indicate a series of positive breaks in the 1980s, indicating movement away from convergence was pronounced during this decade. Diverging housing costs may have played a role in limiting convergence.","Convergence of per-capita income within the regions of a country, such as the United States, is a phenomenon basic economic theory and intuition would lead observers to expect. Workers in low-wage regions have an incentive to move to high-wage areas, which raises wages in the former and lowers them in the latter, helping incomes move closer together. Similarly, firms have an incentive to move plants, equipment and other capital to low cost regions. This expected convergence should increase national income, as resources move to where they are most productive.====At the same time, the empirical evidence on regional convergence has been mixed. Numerous studies for the United States, using different methodologies, have yielded conflicting results. It is possible that factors such as high housing costs in wealthy regions or agglomeration economies may inhibit or even reverse a tendency towards convergence.====Another question raised by recent research has concerned whether, to the extent convergence has occurred, the process has speeded up or slowed in recent decades. Some authors-Holmes, Otero and Panagiotidis (2014) present evidence indicating that convergence has gathered strength in later years. Others-Ganong and Shoag (2017)-have findings suggestive of an end to convergence since around 1980.====In this paper we propose to study both whether US regional per-capita income is convergent overall and whether such convergence may have increased or decreased in recent years. We do so with Pesaran's (2007) pairwise approach. This method avoids some of the empirical pitfalls encountered in previous studies, such as having to choose a base region. The pairwise approach has been employed in other papers to investigate convergence of incomes across countries.====We then employ, as part of the pairwise method, the Lee-Strazicich unit root test. This test allows for endogenous breaks while also testing for whether a given pair of regions has a stationary (convergent) income differential.====Our results indicate that the different US regional incomes are not convergent, contrary to many previous studies. The break analysis does indicate there was some movement in the direction of convergence from the late 1960s to the early 1980s. However, the early-to-mid 1980s exhibited a pronounced movement away from convergence. Finally, the Mideast and New England east coast regions display a particular lack of convergence with the rest of the US.====This paper proceeds as follows. The next section describes the previous literature. The third explains the methodology and data. The fourth describes our results, and the fifth concludes.",Regional convergence-and divergence-in the US,https://www.sciencedirect.com/science/article/pii/S1090944320300442,29 February 2020,2020,Research Article,111.0
"Leite Rodrigo de Oliveira,Cardoso Ricardo Lopes,Jelihovschi Ana Paula Gomes,Civitarese Jamil","COPPEAD Graduate School of Business, Federal University of Rio de Janeiro, Rua Pascoal Lemme 355 Office 423, Rio de Janeiro 21941-918, RJ, Brazil,Brazilian School of Public and Business Administration, Fundação Getulio Vargas–FGV-EBAPE, Rua Jornalista Orlando Dantas, 30, sala 206, Botafogo, Rio de Janeiro 22231-010, RJ, Brazil","Received 22 January 2020, Accepted 19 February 2020, Available online 26 February 2020, Version of Record 18 March 2020.",https://doi.org/10.1016/j.rie.2020.02.002,Cited by (4),"Using a sample of 12,982 Brazilian professional ","Studies on cognition and impulsivity show that thinking quickly may lead educated people to make mistakes on simple reasoning decisions (Frederick, 2005; Kahneman, 2011). Moreover, a lower cognitive ability (impulsiveness) is associated with lower performance in minimizing losses and maximizing gains (Cáceres and San Martín, 2017), and impulsive people who act quickly but without planning may tend to make disadvantageous choices compared to nonimpulsive people (Jelihovschi et al., 2018). Impulsiveness may also lead to negative consequences, such as substance abuse (Loree et al., 2015) and obesity (Fields et al., 2013). Moreover, it significantly impacts educational performance (Diamantopoulou et al., 2007) and employment behaviors (Everton et al., 2005). Despite the impairments caused by impulsiveness in daily life, Dickman (1990) suggests that impulsivity may also be functional, as acting with little forethought may be appropriate in some contexts—e.g., sports (Lage et al., 2011).====In economics, finance, and accounting studies, reflectivity used to be analyzed as an explanatory or control variable to rational decision-making, such as in pricing assets (Thoma et al., 2015), auditing financial statements (Viator et al., 2014), or interpreting financial information (Cardoso et al., 2018). Regarding individual characteristics besides cognitive reflection ability, there is evidence that personality traits affect employees’ compensation (Barrick and Mount, 1991); however, whether the market compensates for reflectivitiveness is currently unclear.====Reflectiveness is a broad category “encompassing a wide range of abilities such as reasoning, problem-solving, and abstract thinking” (Peng, 2019). Moreover, cognitive ability also is shown to influence economic decision making (Kawamura and Ogawa, 2019). Reflectiveness is also known as a Type 2 reasoning, while the impulsiveness is known as Type 1 (Kahneman, 2011). While the Type 1 reasoning is effortless it can be highly biased, however while Type 2 is regarded as producing better judgment and decision making, it is cognitive costly.====Thus, although “Type 1 responses are sometimes of high quality”, they can “lead to low quality decision making’’ (Griffith, Kadous, and Young, 2016). Therefore, it is expected that the job market should recognize a higher cognitive ability trait as desirable, and compensate for it in the form of higher wages. In this paper we test this hypothesis using data from the 2012 Survey of Accountants conducted by the Brazilian Accounting Association.====Results indeed show that a higher cognitive ability is associated with higher job compensation in the form of wages. We then proceed to estimate a number of robustness tests, such as estimating the effect with more consistent estimators (Ordinal Logit and Ordinal Probit instead of OLS), and we strengthen the causality claim of our results with a propensity score matching which reproduces the results. Additionaly, we replicated the results using data collected in 2017, five years after the first data collection.====Hence, this paper fills the literature gap by showing that the job market indeed recognizes cognitive reflection ability as a positive and valuable trait, thus showing the rationality of the job market. Moreover, we also show the importance of building one's cognitive ability in order to improve one's potential value for the job market.====The paper is structured as follows: Section 2 describes the data used in this study. Section 3 presents the results, including robustness estimations such as propensity-score matching and an alternative proxy for impulsiveness, and details replication of the results conducted five years after the main collection. Section 4 concludes.",Job market compensation for cognitive reflection ability,https://www.sciencedirect.com/science/article/pii/S1090944320300259,26 February 2020,2020,Research Article,112.0
"Milani Fabio,Rajbhandari Ashish","Department of Economics, 3151 Social Science Plaza, University of California, Irvine, CA 92697-5100 United States","Received 23 January 2020, Accepted 20 February 2020, Available online 22 February 2020, Version of Record 30 May 2020.",https://doi.org/10.1016/j.rie.2020.02.003,Cited by (6),"This paper exploits information from the term structure of survey expectations to identify news shocks in a ==== with rational expectations.====We estimate a structural business-cycle model with price and wage stickiness. We allow for both unanticipated and anticipated components (“news”) in each structural disturbance: neutral and investment-specific technology shocks, government spending shocks, risk premium, price and wage markup shocks, and ==== shocks.====The news series thus obtained largely differ from their counterparts that are estimated using only data on realized variables. Moreover, the results suggest that the identified news shocks explain a sizable portion of aggregate fluctuations. News about investment-specific technology and risk premium shocks play the largest role, followed by news about labor supply (wage markup) and ====.","The key role of expectations in driving or amplifying aggregate economic fluctuations was recognized a long time ago. Pigou (1927) pointed to excesses of optimism and pessimism by businessmen as causes of fluctuations in economic activity. Keynes (1936) attributed a large portion of fluctuations to the action of investors’ animal spirits. A renowned survey of business cycle theories written in the 1930s by Haberler (1937) also assigned a pivotal role to expectations, including discussions of how expectations may represent significant sources of shocks to the economy.====With the rational expectations revolution in the 1970s, however, the function of expectations in macroeconomic models has changed. Expectations still remain key in the propagation of macroeconomic shocks. But under the assumption of rational expectations, expectations generally no longer constitute autonomous sources of fluctuations.==== Expectational errors can be expressed as unique functions of structural innovations. The majority of macroeconomic models with rational expectations, therefore, abstracts from expectation shocks that cannot be explicitly reconducted to fundamentals. The most popular contemporaneous theories of the business cycle imply that fluctuations are driven by unanticipated fundamental shocks, most often to technology (Hicks-neutral or investment-specific) or to demand conditions (such as preference shocks that affect consumers’s utility, exogenous shifts in government spending, and so forth).====Theories of expectations-driven business cycles, however, have attracted much renewed attention recently. On the theoretical side, Beaudry and Portier (2006) and Jaimovich and Rebelo (2009) present models in which news about future technology shocks is a primary source of business cycle fluctuations, leading to comovement in output, consumption, investment, and labor hours. These theories imply that news about the future is able to generate realistic boom-bust cycles even if no change in technology materializes ex-post.====Recently, the interest has turned toward evaluating empirically theories based on news and quantifying the contribution of news to aggregate fluctuations. Beaudry and Portier (2006) are the first to provide favorable empirical evidence in the context of structural VARs. They show that a shock that doesn’t affect technology in the short-run, but that is correlated with technology in the long-run, accounts for a large share of fluctuations. Given its properties, the shock can be interpreted as reflecting news about future technology. Beaudry and Lucke (2010) find similar evidence using more comprehensive VAR and VECM specifications, including a variety of identified shocks.====Another strategy to investigate the importance of news consists of utilizing fully-fledged structural models as opposed to atheoretical VARs. Schmitt-Grohé and Uribe (2012) estimate a DSGE model with flexible prices, which incorporates news about future neutral and investment-specific technology, preference, government spending, and wage mark-up shocks, and conclude that news accounts for roughly half of output movements. Other papers, however, follow similar strategies to estimate DSGE models that are extended to include sticky prices, sticky wages, and a larger menu of structural disturbances (e.g., Fujiwara, Hirose, Shintani, 2011, Khan, Tsoukalas, 2012), but find only a modest role for news.====The wide range of results is not necessarily surprising. The identification of what should be defined as news from macroeconomic data is complicated. The structural shocks that enter business cycle models are already unobserved to the econometrician. When news is added, both the unanticipated and the anticipated (the news) components in the structural shocks are treated as unobserved and need to be inferred from a typically limited set of macroeconomic time series. The separation of the two components rests on the property that news affects future expectations of the structural shocks, which in turn affect consumption, investment, price setting, and other optimizing decisions, while unanticipated components do not influence future forecasts.====Empirical papers on news, however, typically do not have available or do not employ information on private sector’s anticipations. VAR studies use stock prices as a proxy forward-looking variable that is meant to capture news about future technology. Other forward-looking variables have also been used (e.g., consumer confidence, slope of the term structure) with mixed conclusions. DSGE models, instead, have lagged behind in the use of similar forward-looking variables (with stock prices being a partial exception, since they are occasionally used in robustness check exercises as an additional observable).==== This paper aims to advance the empirical literature on the importance of news in business cycles by exploiting the extensive, but underused, information contained on the available observed expectations data. We exploit the term structure of expectations, obtained from the Survey of Professional Forecasters, in the estimation of a DSGE model, while retaining the conventional assumption of rational expectations. Observed expectations provide additional key information that can constrain the computation of rational expectations through additional measurement equations that are appended to the model, and that can help the econometrician disentangle unanticipated shocks and news over the business cycle.====We estimate a popular DSGE model with sticky prices and wages, based on Smets and Wouters (2007), using full-information Bayesian methods. We exploit expectations at the one, two, three, four, and five-quarter-ahead horizons on output, consumption, investment, government spending, inflation, and interest rates, to inform the extraction of news shocks. Given our focus on the identification of news over the sample, we find it worthwhile using real-time data for our macroeconomic series of interest in the estimation. We show, however, that the conclusions are robust to the use of revised, current-vintage, data series.====In terms of methodological choices, we believe that an advantage of our approach is that it can fully retain the assumption of rational expectations, yet it forces expectations to be consistent with the available observed expectation series. Even under the assumption of rational expectations, expectations-driven business cycles may arise here because of the existence of news. News about future shocks, and subsequent revisions in those news, can constitute a source of aggregate fluctuations and create additional volatility in the economic system.====In addition, the use of a structural theory-based model, rather than a VAR, is motivated, among other things, by the well-known invertibility problem that affects VARs when anticipations are present (e.g., Leeper et al., 2008). Leeper and Walker discuss how the different information sets available to the agents in the economy and to the econometrician estimating the VAR, which exist when anticipations are an important component of the data, prevent econometricians from correctly identifying the structural shocks, and consequently lead to misleading impulse responses and variance decomposition shares.====In our empirical analysis, we compare the news shocks and their importance for business cycles with those estimated without using any information from expectations. We also re-estimate the model without news and with revised, rather than real-time, data to check the contribution of each modeling and estimation element to the final results.====When the model is estimated omitting data on expectations, it is unclear whether news shocks actually play a major role in the economy. First, the posterior means of the standard deviations of news shocks move closer to zero if compared with the corresponding prior means. The vast majority of the 95% credible sets for the news parameters contain the value of zero, which would indicate that the specific news is empirically unimportant. The main finding, however, is that, when expectations data are not used in the estimation, several parameters related to news shocks are very weakly identified or non-identified. In many cases, the priors are not really updated, as the posterior distributions for the news standard deviations overlap with the priors, or, if not overlapping, the two distributions closely resemble each other.====When the model is re-estimated exploiting data on observed expectations, the identification of news substantially improves. The posterior distributions for the news coefficients now typically fall further from the priors, and become narrower around their means. Moreover, the data often suggest values for the standard deviations of news that are significantly higher than prior means; in most cases, the credible sets are in strictly positive range.====In the baseline estimation, the empirical results indicate (unanticipated) investment-specific technology shocks as the main drivers of business cycles, a finding that is in line with recent evidence by Justiniano, Primiceri, and Tambalotti (2010), among others. Such shocks explain between 30 and 40% of real GDP growth (forecast error) variance. But news shocks are also important: the fraction of aggregate economic fluctuations that can be attributed to news also falls between 30 and 40%. News about the investment-specific technology shock at short-term horizons accounts for the largest share; short-term news about monetary policy and longer-horizon news about the risk-premium and wage markup shocks also have nontrivial roles.====The inclusion of expectations and news in the estimation also leads to changes in the posterior estimates for coefficients that are unrelated to news. The degree of real frictions, such as habit formation in consumption and investment adjustment costs, substantially falls. The degree of nominal frictions, such as rigidity in wages and prices, and indexation to past inflation, are also reduced. Therefore, the evidence suggests that news and subjective expectations work to create persistence in the system, so that the role of some popular frictions is diminished.==== The paper mainly aims to add to the emerging literature focused on testing the empirical importance of news over the business cycle. While the previously-discussed results by Beaudry and Portier (2006) and Beaudry and Lucke (2009) suggest a major role for news in VAR models, others (e.g., Forni et al., 2014, using a factor-augmented VAR) disagree. Theoretical work and the early empirical papers have mostly focused on news about technology. Schmitt-Grohé and Uribe (2012) estimate a RBC-type model and allow for news in a wider range of disturbances. Fujiwara et al. (2011), Khan and Tsoukalas (2012), estimate DSGE models with New Keynesian features similar to the one we use here. Again, there is contrasting evidence. Schmitt-Grohé and Uribe (2012) uncover a significant role of news over the business cycle. Fujiwara et al. (2011) and Khan and Tsoukalas (2012), on the other hand, find only limited contributions. Milani and Treadwell (2012) consider news regarding future monetary policy choices, possibly indicating central bank announcements or simply private sector’s attempts at anticipations, and show that anticipated monetary policy innovations play a larger role over the business cycle than monetary policy surprises.====Within the literature on estimated DSGE models with news, this paper has also points of contact with Avdjiev (2016), who suggests using stock prices in the estimation of DSGE models with news to better capture forward-looking information. Our paper differs, because we use an extensive set of expectations, directly regarding most variables that enter the model. Avdjiev studies the effects of adding stock prices in different estimated specifications of a flexible price model, while we consider a possibly more conventional sticky-price sticky-wage model of the U.S. economy. Moreover, our use of expectations about a large set of macroeconomic variables, rather than a stock price index as a single forward-looking variable, has the advantage of shielding us from the well known difficulty of general equilibrium models to simultaneously explain the real and financial sides of the economy. The results in the two papers, however, can usefully complement each other.====The paper most closely related to ours is Hirose and Kurozumi (2019). They estimate a small-scale three-equation New Keynesian model using forecasts’ data. We focus on the larger-scale Smets and Wouters’ business cycle model of the U.S. economy and we exploit a much larger set of expectations series, which allow us to better extract and disentangle news about technology, risk-premia, markup-shocks, and so forth.====In terms of methodology, the paper shows how the inclusion of expectations data can be useful to prevent rational expectations from falling too far from the available observations on macroeconomic expectations. The approach used here, therefore, is not restricted to applications focused on news, but it can be generally exploited in the estimation of any DSGE model, with or without rational expectations.====There is a long history of interest in the use of survey data on expectations (as exemplified, for example, by the survey by Pesaran and Weale (2006)). But their use in the estimation of DSGE models has started only more recently. Negro and Eusepi (2011) question whether typical models with rational expectations can match the dynamics of observed inflation expectations. Ormeno (2011) uses inflation expectations data in the estimation of a model with learning. Milani (2011) uses data on observed output, inflation, and interest rate expectations in a model with learning, showing that identified expectation shocks account for roughly half of U.S. business cycle fluctuations; Milani (2017) extends the analysis to a medium-scale model. This paper, instead, uses a much larger set of expectations data than those precursors, and its novelty lies in exploiting them to instruct the extraction of news.","Observed expectations, news shocks, and the business cycle",https://www.sciencedirect.com/science/article/pii/S1090944320300260,22 February 2020,2020,Research Article,113.0
"Izaguirre Alejandro,Di Capua Laura","Universidad de San Andrés, Buenos Aires, Argentina,Facultad de Ciencias Económicas y Estadística, Universidad Nacional de Rosario, Argentina,Instituto de Investigaciones Económicas, Universidad Nacional de Rosario, Argentina","Received 20 August 2019, Accepted 12 February 2020, Available online 13 February 2020, Version of Record 18 March 2020.",https://doi.org/10.1016/j.rie.2020.02.001,Cited by (6),"This paper assesses peer group influence on academic performance of primary school students in Latin America and the Caribbean. Based on TERCE data set, we investigate ==== in mathematics, language and sciences tests outcomes among sixth grade students. We apply a social interaction model which allows to identify endogenous and exogenous peer effects while controlling for group-level fixed effects. We explore some heterogeneities related to the school type (private, public or rural). The estimates suggest the existence of endogenous peer effects but their magnitude and significance depend on subject and school type.","Social scientists have long been interested in peer effects because of their far reaching implications at the individual and collective level. These non-market interactions represent how an individual’s decision or outcome is directly influenced by his peer’s outcome or characteristics. Social interactions play an important role in determining behavioral and economic outcomes regarding diverse aspects of life, such as criminal activity, use of public services, labor markets outcomes, etc.====Among the various spheres in which peer effects may manifest themselves, the school context is especially important considering the vital role educational attainments have on future living conditions of individuals. Human capital accumulation has intertemporal repercussions given the proven relationship between years of schooling and labor incomes (Mincer, 1974). The analysis of peer effects in education has received considerable attention, notably since the publication of the Coleman report (Coleman et al., 1966). A common hypothesis is that student outcomes are higher in the presence of favorable peer groups, conditional on individual characteristics and family background (McEwan, 2003).====Evaluating peer effects in academic achievements is important for parents, teachers and schools; but crucially from a public policy perspective. A major question in the economic literature is whether or not interactions among students lead to large social multipliers (Epple and Romano, 1996). Depending on the nature of peer effects, there may be social gains from their existence (Hoxby, 2000).====In this paper we analyse the possible existence of peer effects in educational achievements among sixth grade students participating in the Third Regional Comparative and Explanatory Study (TERCE) conducted by United Nations Educational, Scientific and Cultural Organization (UNESCO). Since this survey focuses on primary school students, TERCE data provides a unique opportunity to explore peer effects in education in its early stages, a phase in which public policy can make a difference to improve social equity through the education system.====One important difficulty in dealing with peer effects is that they are hard to identify with observational data since it is not easy to distinguish between the impacts that actually result from social interactions from the choices of with whom to interact with==== and the existence of a common environment among group members (Manski, 1993). Recent developments in network literature allow to study outcomes of social interactions taking into consideration the problems caused by endogenous association of members within a group and confounding factors (Bramoullé, Djebbari, Fortin, 2009, Goldsmith-Pinkham, Imbens, 2013, Hsieh, Lee, 2016, Lee, 2007, Moffitt, et al., 2001).====With this research we expect to contribute to the recent empirical literature on peer effects in education. Besides, this paper should specifically add to the scarce existing evidence on the magnitude and characteristics of peer effects in education in Latin America and the Caribbean. The article will explore personal, family and contextual factors associated with mathematics, language and sciences learning achievements for sixth grade students of the fifteen countries participating in TERCE. We also explore some heterogeneities in results depending on whether the school is urban public, urban private or rural.====The paper is organized as follows. Section 2 reviews the existing literature on peer effects in education. Section 3 presents the methodological approach and econometric model used for estimations. Section 4 describes the data and variables used in the analysis, and explains how we deal with missing observations. Finally, Section 5 shows estimation results while conclusions are provided in Section 6.",Exploring peer effects in education in Latin America and the Caribbean,https://www.sciencedirect.com/science/article/pii/S1090944319303242,13 February 2020,2020,Research Article,114.0
"Sayed Adham,Peng Bin","School of Economics, Huazhong University of Science and Technology, Wuhan, China","Received 2 November 2019, Accepted 15 December 2019, Available online 16 December 2019, Version of Record 18 March 2020.",https://doi.org/10.1016/j.rie.2019.12.001,Cited by (7),This paper examines the shape of the curve that represents the long-run relationship between income ,"Kuznets (1955) published his research about the relationship between income inequality and economic growth. Although this debate has been raging since the eighteenth century, Kuznets's research was based on data that were completely unavailable to his predecessors (Piketty, 2014). Based on this data, Kuznets concluded that the relationship between income inequality and economic growth took the form of Inverted-U. The curve, which became known later on as the Kuznets curve, shows that in the early stages of growth and at the beginning of the transition of labor from the agricultural sector to other sectors (and from rural to urban), inequality rates would rise, reaching their maximum; then they would gradually decline to their lowest possible levels when the final stages of growth are reached (Fig. 1).====Twenty years later, Robinson (1976) confirmed Kuznets's theory by constructing a mathematical model that represents it. Based on the Kuznets model, he divided the economy into two sectors, the agricultural sector, and the non-agricultural sector.==== Eq. (4) gives the shape of the curve, which will be Inverted-U with ==== < 0. Robinson explains how we get the Kuznets curve through this equation, where “As ==== increases, inequality first increases, reaches a maximum, then decreases-precisely the U hypothesis.”====Later, based on Kuznets U-hypothesis and the model by Robinson, many theoretical and empirical studies used the quadratic function (Ram, 1991; Barro, 2000; Zhan, 2016) to represent the relationship between income inequality (most studies use Gini index) and economic growth (GDP per capita) or income (per capita income). Dawson (1997) used time-series data to test this relationship using two models, quadratic,==== and semi-log quadratic.==== He finds that ==== and ==== are positive, and ==== and ==== are negative, and these follow the U-hypothesis and support Kuznets's result. Using panel data and quadric function, Desbordes and Verardi (2012) found observational evidence of a Kuznets curve.====Unlike these studies, List and Gallet (1999) used a cubic function, to represent the relationship between inequality and economic growth. Nevertheless, their results were not different from those who preceded them and they found that the countries are distributed over three regions on the Kuznets curve. For example, the United States is in the second positively sloped region of the Kuznets curve. In this sense, List and Gallet (1999) considered that what we saw after the end of the Kuznets curve in the early 1980s is the beginning of a new Kuznets curve, which contradicts Kuznets's theory that in the advanced stages of growth there will be no income inequality.====Milanovic (2012,2013) found an important result when studying global inequality, however, similar to what List and Gallet (1999) had reached, he used a new description of the Kuznets curve which is “Kuznets waves”, and what we have seen since the 1980s is the second Kuznets’ waves. For Milanovic, the factors that jump-started the new wave are namely economic policies especially in the rich countries, technological change, and the slumped of unionization and globalization. He also reached a new curve without naming it, when he represented the relationship between real income and the percentile of the global income distribution. In the same framework, Lakner and Milanovic (2015) conclude that the global growth incidence curve has a distinct supine S shape, which became later known as the “Elephant curve”==== (Alvaredo et al., 2018; Chancel and Gethin, 2017). In this curve, Milanovic (2016a) show that the winners from “high globalization” are the global top 1% (super-rich) and the middle classes in emerging Asia (especially China). On the other hand, the middle class in the developed world are the big losers (Fig. 2).====Piketty and Saez (2003) were indicated that the change in the path of inequality was not based on Kuznets's interpretation. They concluded that inequality in the United States has been rising since the seventies of the last century and that the curve takes the form of “U” not “U-invert.” They were able to get to this conclusion using data covering the period from 1913 to 1998. This data shows that the period between 1915 and 1948 was clearly consistent with the Kuznets curve especially for the USA (Fig. 1). But the decline of income inequality between 1939 and 1948 was not because of “a quiet process of inter-sectorial mobility”, as Kuznets (1955) described it, the main reason is that this period is the period of world wars and Great depression. The period between 1951 and 1970 was a period where inequality has stabilized at relatively low levels (Fig. 3).====Therefore, the decline in inequality between 1937 and 1944 and then the stability of inequality since 1950 till 1970s can be explained based on these factors: the first and second world war destroyed assets, large taxes on the rich to finance the conflict, the emergence of socialist movements and trade unions, the massive scale-up of public education, and the greater participation of women in the workforce (Milanovic, 2016a). But since 1970, these rates have been rising continuously, contradicting Kuznets's conclusions about stabilizing these ratios at low levels after the ups and downs (Fig. 4).====Piketty and Saez (2014) asked this question, do the balancing forces of growth, competition, and technological progress leading in later stages of development to reduced inequality, as Simon Kuznets thought in the 20th century?====They conclude that Kuznets's theory cannot apply to what happened in the first half of the twentieth century, they consider that the real cause is the two world wars and the crisis caused by the Great Depression, as well as taxes and Financial policies adopted after World War II. Therefore, they conclude that: Inequality does not follow a deterministic process. In a sense, Kuznets was wrong. There are powerful forces pushing alternately in the direction of rising or shrinking inequality. Which one dominates depends on the institutions and policies that societies choose to adopt.====Piketty (2014) concludes that the driving forces of distribution can be divided into two forces, convergence forces and divergence forces. The forces of convergence, summarized by Piketty are, the spread of knowledge, investing in training and skills. He also considers that the law of supply and demand always tends toward convergence, as well as capital mobility and labor.====In the same framework, Milanovic also presents the forces that have a significant effect on the path of income inequality (up and down). He divided those forces into “benign forces” and “malign forces.” The benign forces are technological change, globalization, social transfers, demographics, economic policies, trade unions, taxation, and education. And the “malign forces include war especially the first and second world wars, epidemics, state breakdown (Milanovic, 2016a, 2016b).====In this paper, we will look at the shape of the curve drawn by income inequality in relation to economic development in the long term, and provide an explanation of the reasons that led to this path. Therefore, we will examine the Kuznets curve in the long term in four developed countries based on the available data, using a panel data model.====In contrast to most previous studies, we will, first, not use the Gini index as a measure of inequality, but the top 10% income share instead, because the data related to this indicator (Gini) are for very short periods of time, but we want a long period data, and the recent studies on which we are relying on, produced a new indicator is almost the same index used by Kuznets in his study in 1955 . Second, most of the studies that looked at the shape of the curve were based on limited data. In our study, however, we relied on panel data in four developed countries over the period 1915 – 2014 (100 years), which helped to determine the shape of the curve more precisely. Third, based on Bai (2009) The analysis is carried using a panel data technique, fixed effects, random effects, and interactive fixed effects, that take into account common shocks (politic and economic crises, war…).====Our results confirm that the Kuznets curve no longer explains the path of inequality. The rise and fall of inequality are not what we have seen in the last 100 years. There are many factors that played a major role in the decline of inequality in the period studied by Kuznets (1913–1948) and the most important factors were the two world wars and the Great Depression (Piketty, 2005; Piketty and Saez, 2014; T. 2003).====Although many countries have moved to advanced stages of growth, inequality rates have risen again, recording high levels, and most studies predict that over the next 30 years there will be no room for these percentages to decline (Alvaredo et al., 2018, 2017). The curve closest to present this path is the “N-shaped” curve (Fig. 5), where the results show that the inequality rises, then declines and then rises again.====The rest of the paper is organized as follows: Section 2 presents the data set used in this paper and the limitations. Section 3 introduces the methodology and the empirical model. Section 4 presents and discusses the empirical results. Section 5 concludes with summary comments.",The income inequality curve in the last 100 years: What happened to the Inverted-U?,https://www.sciencedirect.com/science/article/pii/S1090944319304181,16 December 2019,2019,Research Article,115.0
"Hooper Peter,Mishkin Frederic S.,Sufi Amir","Economic Research, Deutsche Bank Securities, Inc, United States,Graduate School of Business, Columbia University, United States,National Bureau of Economic Research, United States,Booth School of Business, University of Chicago, United States","Received 29 October 2019, Accepted 6 November 2019, Available online 27 November 2019, Version of Record 18 March 2020.",https://doi.org/10.1016/j.rie.2019.11.004,Cited by (30),"As the US labor market has tightened beyond full employment with relatively little evidence of ==== since the 1980s. We also review the experience of the 1960s, the last time ==== became unanchored, and observe both parallels and differences with today. Our analysis suggests that reports of the death of the Phillips curve may be greatly exaggerated.","Starting in March of 2017, the unemployment rate has fallen below both the Congressional Budget Office's and the FOMC participants’ current estimates (as of the October 2019) of the natural rate of unemployment (4.6% and 4.2%, respectively). Since March of 2018, the unemployment rate has fallen below 4%, levels that were last seen in the late 1960s. The most recent median projections of FOMC participants (September 2019) for the unemployment rate are 3.7% in 2019, 3.7% for 2020, 3.8% for 2021 and 3.9% for 2022. FOMC projections indicate that the economy for the next several years will be operating in a high-pressure environment, where labor markets are tight and the unemployment rate remains below the natural rate of unemployment.====Monetary policy is also currently accommodative. The federal funds rate target range has bseen lowered to 1.5 to 1.75%, which is a real interest rate that is around −0.5% to −0.25%, given that inflation expectations for the PCE deflator are close to the Federal Reserve's inflation target of 2%. This real federal funds rate is very low by historical standards. The current federal funds rate target is also below current FOMC participants’ median estimate of the “neutral” or long-run federal funds rate of 2.5%.====Important to the Federal Reserve's decisions about the future path of the federal funds rate is whether the Committee's projection of a high-pressure economy for some time to come will lead to inflationary pressures that would require significant increases in the federal funds rate. Phillips curves estimated with data prior to 1990 suggest that a high-pressure economy in which the unemployment rate remains below the natural rate for long periods of time can lead to an acceleration of inflation. Indeed, this is what happened in the 1960s, when a high-pressure economy led to inflation accelerating to above 5% by the end of the decade. The Great Inflation period then ensued, which was only terminated by very high federal funds rates under the Volcker Fed.====Despite the projection of a high-pressure economy, the FOMC pivoted from raising the federal funds target to lowering it by 75 basis points (0.75 percentage points) in recent meetings. The Fed's shift reflects a combination of concerns about global risks and an absence of worries about an inflation overshoot. Inflation expectations, gleaned from market measures, various surveys, and the FOMC's own projections, remain subdued.====So far an unemployment rate below the natural rate of unemployment rate for the last several years has been associated with only a modest increase in inflation, which has continued to fall short of the Fed's 2% inflation objective. This result is consistent with a Phillips curve that is very flat: in other words, the responsiveness of inflation to the unemployment gap—the difference between the unemployment rate and the natural rate of unemployment—is very weak. Fig. 1.1 provides estimates of the slope of the Phillips curve, that is, the coefficient on the unemployment gap, from rolling regressions for core PCE. (The rolling regressions use 20-year windows, with the date in the figure indicating the end of the window. Details of this Phillips curve analysis is provided in Section 2.)====We see that starting with 20-year sample periods that would have begun by the early 1980s, the slope of the Phillips curve began to recede precipitously, and since the early 2000s it has been much closer to zero. This finding has often been characterized by saying that the Phillips curve is dead, or close to it, i.e., the unemployment gap tells us very little about what will happen to inflation. Thus running a high-pressure economy presents very little risk of accelerating inflation, and this provides an important rationale for having lowered the federal funds rate.====In this paper, we examine whether this sanguine view about the prospects for inflation when there is a high-pressure economy is justified. We look at why the Phillips curve has been so flat and whether there is a danger that it may not continue to be flat in the future, suggesting that there is a risk that the current high-pressure economy could lead to accelerating inflation. In other words we explore whether the Phillips curve is truly dead, or is just hibernating.====We conduct empirical analysis in the next four sections to examine whether the Phillips curve is alive and if there is inflation risk should we turn back toward a high-pressure economy.====Section 2 examines the macro, time-series data for both price and wage inflation going back as far as the early 1950s, with a particular focus on the finding that the Phillips curve has flattened since the late 1980s. It also examines in some detail the question of whether the Phillips curve is nonlinear with a slope that steepens appreciably when the unemployment rate is well below the natural rate of unemployment. The analysis yields significant evidence that such nonlinearity does exist, though less so in more recent years in the price Phillips curve. The analysis also reveals a significant difference that has emerged between the price-Phillips curve and the wage-Phillips curve in recent decades, with the wage-Phillips curve having flattened significantly less and retained greater nonlinearity.====The time-series data since the 1980s have very few episodes of the unemployment rate being well below the natural rate of unemployment. It is therefore possible that the inability to find much slope, at least in the price-Phillips curve, in the more recent sample using macro, time-series data occurs because there is insufficient variation in the data. We thus turn in Section 3 to estimate Phillips curves using data from states and Metropolitan Statistical Areas (MSAs), which have much greater variation. We do find that steeper and non-linear Phillips curves do appear in more recent years in the state and MSA data.====One of the prominent explanations for a flattening of the Phillips curve is the anchoring of inflation expectations in recent decades. However, there is a possibility that a high-pressure economy could lead to a shift from a regime of low inflation with stable expectations to one of significantly rising inflation. The last time this happened, with the Phillips curve coming out of hibernation at the national level, was in the 1960s. In Section 4, we take a closer look at the 1960s and consider some of the key parallels and differences with the current economic and policy environment.====Section 5 then turns to a more forward-looking assessment of what the various linear and nonlinear models of price and wage inflation have to say about the prospects for inflation over the next several years, under alternative scenarios, including both the FOMC's projections for unemployment and an even tighter labor market.====Finally Section 6 presents our conclusions.",Prospects for inflation in a high pressure economy: Is the Phillips curve dead or is it just hibernating?,https://www.sciencedirect.com/science/article/pii/S1090944319304132,27 November 2019,2019,Research Article,116.0
Peng Hui-Chun,"Department of Public Finance, National Taipei University, No. 151, University Rd., Sanxia Dist., New Taipei City, 237, Taiwan","Received 3 September 2019, Accepted 6 November 2019, Available online 7 November 2019, Version of Record 18 March 2020.",https://doi.org/10.1016/j.rie.2019.11.003,Cited by (3),"This paper conducts laboratory experiments to investigate the effect of cognitive ability on individual charitable behavior under two theoretically equivalent subsidy schemes, matching and rebate subsidy schemes. Experimental results show that whether the subject has high or low cognitive ability, his/her private donation under matching subsidies is significantly higher than that under the theoretically equivalent rebate subsidies. However, an individual with higher cognitive ability is more likely to attain the theoretical equivalence.","Individual voluntary donation has played an important role in supporting charitable organizations. To encourage private donors to contribute higher amounts, experimental economists investigate which mechanisms or fundraising institutions can more effectively attain this goal. One of the important mechanisms is the subsidy scheme. Among different subsidy schemes, matching and rebate subsidies are used mostly to encourage private donations to charities worldwide.====Matching subsidy is a subsidy scheme wherein if an individual donates 1 dollar to a particular charity, the government will ensure matching the individual donations at a pre-announced rate. For instance, assume that the matching rate is 100%; in that case, when an individual donates 1 dollar to a charity, the government also donates the same amount to that specific charity. The charity thus receives 2 dollars in total. On the other hand, the rebate subsidy is a subsidy scheme wherein when an individual donates 1 dollar to a particular charity, the government refunds a pre-announced portion of donation back to this donor. For example, assume that the rebate rate is 50%; in that case, when an individual donates 2 dollars to a charity, the government refunds 1 dollar back to the donor; however, the charity still receives a 2-dollar donation in total.====On the basis of the two aforementioned examples, it can be shown that given the matching rate, ====, and the rebate rate, ====, matching and rebate subsidy schemes theoretically generate an identical predictable private donation when ====. Several studies have used experiments to evaluate the effect of different subsidy schemes on charitable donations. All of these experimental studies find that instead of supporting the theoretical prediction, the individual donation is significantly higher under the matching subsidy than under the theoretically equivalent rebate subsidy. These existing studies propose different explanations for this observation.====Eckel and Grossman (2003) is the earliest experimental study that examines the effects of different subsidies on charitable giving. The researchers discover a higher contribution under the matching subsidy and explained that subjects may have viewed the act of contributing with matching subsidy in a more favorable context than with rebate subsidy. Further, Eckel and Grossman (2017) conduct a field experiment and confirm the laboratory finding that matching subsidies are more effective than rebate subsidies and that the subsidy mechanism increases the total amount of giving to charities. By employing Eckel and Grossman (2003) experimental method, Davis et al. (2005) find that many subjects use a simple, fixed constant contribution rule. Moreover, the constant contribution rule increases the total charitable giving under matching subsidy than under rebate subsidy.====Eckel and Grossman (2006a) argue that a subject makes a series of allocation decisions with matching and rebate subsidies at the same time may be confused. To minimize the confusion relating to matching and rebate subsidies, in the experiment, each subject only makes the allocation decision either under matching subsidy or rebate subsidy. The results reveal that although the individual net donation with matching subsidy is still significantly higher than with rebate subsidy, the difference in the individual net donation for these two subsidies is lesser.====Further, Eckel and Grossman (2006b) try to analyze whether donors are rebate-averse. Subjects first make a choice to participate in a dictator game involving either a 50% rebate or a 1-for-1 match. Then, they make their allocation decisions. As a result, no significant difference in the choice of subsidy mechanisms prevails; however, in accordance with the earlier studies, they still find that individual giving under the matching subsidy is significantly greater than that under the theoretical comparable rebate subsidy.====Davis (2006) examines whether the higher donation under the matching subsidy is attributable to an isolation effect, which means that when given a multidimensional problem, individuals tend to disaggregate dimensions of the problem and focus on only those components that they control most directly or that affect them most directly. His experimental results reveal that the isolation effect, rather than a preference for matching subsidies, drives higher individual donations under matching subsidies than under theoretically equivalent rebate subsidies.====Furthermore, cognitive ability is generally defined as an extensive category, encompassing a wide range of abilities such as reasoning, problem-solving, and abstract thinking (Gottfredson, 1997). Studies of cognitive abilities as a measurable attribute with considerable inter-individual variability have received considerable attention. Many researchers have found a significant relationship between an individual’s cognitive ability and his/her economic behavior. For example, Frederick (1995) and Benjamin et al. (2013) find that individuals with higher cognitive ability could be more patient. Frederick (1995) and Dohmen et al. (2010) reveal that individuals with higher cognitive ability behave in a less risk-averse manner. Corgnet et al. (2016) deduce a positive relationship between cognitive ability and trust. Lohse (2016) find that subjects who have higher cognitive ability contribute more in a one-shot public goods game. In experimental ultimatum games, Kawamura and Ogawa (2019) find that the higher the cognitive ability, the larger the amount a sender offers, but the smaller the offer a responder accepts. Brañas Garza et al. (2012) and Gill and Prowse (2016) exhibit that participants with higher cognitive ability are capable of deeper reasoning in an experimental p-beauty contest game. In addition, Benito-Ostolaza et al. (2016) find that the participants with higher cognitive ability play more strategically.====In the related experimental studies that investigated individual donation decisions with subsidy schemes, subjects usually answer a series of donation decisions with different subsidy schemes and/or various subsidy rates in a short experimental period. In addition, the subjects make their decisions only once in the one-shot game setting. On the basis of these observations, this paper proposes that to realize the theoretical equivalence of individual donation under a pair of comparable matching and rebate subsidy schemes, participants will need to reflect on the process quickly and execute computational tasks competently, that is, participants should have higher cognitive ability.====This paper measures cognitive ability by employing Frederick (1995) cognitive reflection test (CRT). The CRT is a suitable measure of cognitive ability for this study, as it simultaneously captures the ability to engage in reflective processes and execute computational tasks. Several experimental studies have employed the CRT to measure cognitive ability because of its short duration and its unprecedented success in predicting economic behaviors.====Currently, no study has analyzed the effect of cognitive ability on individual donation decisions under different subsidy schemes. This paper complements the existing literature by examining the relationship between individual donation decisions under matching and rebate subsidy schemes and cognitive abilities measured by the CRT. This paper finds that whether the subject has high or low cognitive ability, his/her private donation under matching subsidies is significantly higher than that under the theoretically equivalent rebate subsidies. However, an individual with higher cognitive ability is more likely to attain the theoretical equivalence.====The remainder of this paper is organized as follows. Section 2 explains the experimental design and procedures, and Section 3 presents experimental results. Section 4 concludes the paper.",Effect of cognitive ability on matching and rebate subsidies,https://www.sciencedirect.com/science/article/pii/S109094431930345X,7 November 2019,2019,Research Article,117.0
"Delbono Flavio,Lambertini Luca","Department of Economics, University of Bologna, Italy,Piazza Scaravilli 2, Bologna 40126, Italy,Strada Maggiore 45, Bologna 40125, Italy","Received 4 November 2019, Accepted 6 November 2019, Available online 6 November 2019, Version of Record 18 March 2020.",https://doi.org/10.1016/j.rie.2019.11.002,Cited by (1),"We show that managerial delegation based upon comparative performance may generate collusive outcomes observationally equivalent to those typically associated with repeated games or cross ownership. This happens when rivals’ profits are positively weighted in the managerial incentive scheme. We also identify the level of time discounting at which a repeated game based upon Nash reversion would achieve the same degree of collusion. Accordingly, such managerial contracts should attract the attention of antitrust authorities.","Since the pioneering contributions by Vickers (1985) and Fershtman (1985), the literature on strategic delegation has been growing significantly and various types of managerial incentives have been put forward. We may roughly group such incentives into three types, depending on whether, in addition to its own profits, a firm’s objective function includes also output (Vickers, 1985, Fershtman, 1985) or revenue (Fershtman, Judd, 1987, Sklivas, 1987), market share (Jansen, van Lier, van Witteloostuijn, 2007, Jansen, van Lier, van Witteloostuijn, 2009, Ritz, 2008) or the rival’s profits (Salas Fumas, 1992; Lundgren (1996), Aggarwal and Samwick, 1999; Miller and Pazgal, 2001).====There exists a strand of literature focussing on the emergence of implicit collusion among managerial firms in repeated games (Reitman, 1993, Spagnolo, 2000, Spagnolo, 2005, Lambertini, Trombetta, 2002). Moreover, the possibility for cross-ownership to generate collusive outcomes has also been investigated in detail (Reynolds, Snapp, 1986, Malueg, 1992, Reitman, 1994, Gilo, Moshe, Spiegel, 2006).====What we want to illustrate is indeed that another route potentially replicating collusion is represented by the use of delegation contracts based on comparative performance as in Salas Fumas (1992), Lundgren (1996), Aggarwal and Samwick (1999) and Miller and Pazgal (2001). This is the case whenever the weight attached in managerial contracts to rival firms’ profits is positive, which obtains at equilibrium under Bertrand competition in a market for substitute goods. The source of this result is the following. As we know from Miller and Pazgal (2001), the adoption of incentive based upon comparative performance yields a unique subgame perfect equilibrium irrespective of the specific market variables being set by managers. Moreover, the resulting profits are somewhere between those associated with the Bertrand and Cournot equilibria played by entrepreneurial firms. Hence, moving from the pure Bertrand outcome to the managerialised one is equivalent to partially colluding in prices, while it is procompetitive if the departure point is Cournot. This partially collusive outcome also replicates that engendered by systematic cross-ownership by the same amount in the entire industry.====In the remainder of the paper, we briefly reconstruct the basic result in Miller and Pazgal (2001) and then, using the folk theorem based on grim trigger strategies (Friedman, 1971), illustrate the tacitly collusive supergame reproducing the same result. Finally, at the empirical level a few relevant facts highlighted by Aggarwal and Samwick (1999) are worth recollecting. According to their data, a positive weight is attached to rivals’ profits also in industries where capacity constraints bite (which typically fall under the Cournot label), in contrast with the theoretical prediction. In summary, all of this should draw the antitrust agencies’ attention to industries in which comparative performance is a key component of managerial incentives, as what follows shows that this could be a relatively simple way of implementing collusion without explicit agreements.",On the collusive nature of managerial contracts based on comparative performance,https://www.sciencedirect.com/science/article/pii/S1090944319304284,6 November 2019,2019,Research Article,118.0
"Faralla Valeria,Borà Guido,Innocenti Alessandro,Novarese Marco","Dipartimento di Giurisprudenza e Scienze Politiche, Economiche e Sociali, Università del Piemonte Orientale, Palazzo Borsalino, Via Cavour 84, 15121 Alessandria, Italy,Dipartimento di Scienze Sociali, Politiche e Cognitive, Università di Siena, Complesso S. Niccolò, Via Roma 56, 53100 Siena, Italy,LabSi (Laboratory of Experimental Economics of the University of Siena), Università di Siena, Complesso S. Niccolò, Via Roma 56, 53100 Siena, Italy","Received 28 August 2019, Accepted 4 November 2019, Available online 5 November 2019, Version of Record 18 March 2020.",https://doi.org/10.1016/j.rie.2019.11.001,Cited by (1),"We assess in the laboratory the effect of promises on group decision-making. The gift-exchange game provides the testing ground for our experiment. When the game is played between groups, inter-group cooperation and reciprocity represent a condition for increasing total earnings as a measure of social efficiency. Our findings show that promises have positive effect on aggregate payoffs and that effect is reinforced when a group member is randomly selected as proposer to set forth an effort/transfer level that other group members can approve or reject. Promises and proposers elicit social conformity leading groups to exhibit more desirable social behavior.","First proposed by Akerlof (1982), the gift-exchange game was extensively investigated in the field of social preferences to assess the determinants of reciprocity and fairness. The game simulates an employment contract in a principal-agent relationship, in which the principal chooses how much to transfer to the agent. Then, the agent selects a costly effort level that determines the overall return. Since agents’ effort is not completely enforceable (and indeed the effort is not commonly explicitly included as a clause in work contracts), one would expect that they would maximize their own utility and conform to rational behavior. Laboratory experiments, however, have consistently found that agents do not necessarily act as selfish decision-makers but rather tend to increase their effort levels according to the amount offered by principals. More interestingly, agents acting as second movers do not necessarily minimize their costs as predicted by rational money-maximization but reciprocate the gift received from first-mover principals (Fehr et al., 1993). This in turn increases overall rather than individual earnings. As a result, the gift-exchange game allows testing how reciprocity can implement efficient outcomes in incomplete contracts.====Despite these findings were confirmed in a variety of experiments both in the lab and in the field (Abeler et al., 2010; Bellemare and Shearer, 2009; Brown et al., 2004; Charness and Kuhn, 2011; Fehr et al., 1997, 2007), little attention has been paid to situations in which the game is played by groups for which rational behavior and selfish decision-making seems to be reinforced (Bornstein and Yaniv, 1998; Charness and Sutter, 2012; Kugler et al., 2012) with possible implications for the reciprocity (individual) result. To date, Kocher and Sutter (2007) compared individual and group behaviour in the gift-exchange game by introducing communication within groups. More specifically, they showed that face-to-face communication induces efficient decisions with respect to both individuals and groups with computer-mediated communication. More recently, Brady and Wu (2010) extended this result by testing different patterns of communication and showed that even small changes in decision procedures have a significant impact on aggregate profits. They showed that group members’ behaviour depends not only on communication patterns but also on their role within the group, which is a key factor in differentiating group member's behaviour. Those having more authority in group decisions are found to identify more strongly with the group. These findings corroborate the fact that group behavior depends on a variety of factors, especially decision procedures (Cason and Mui, 1997; Charness and Jackson, 2009; Charness et al., 2007, 2010; Cooper and Kagel, 2005; Fahr and Irlenbusch, 2011; Feri et al., 2010; Fisher, 2017; Kocher and Sutter, 2005; Luhan et al., 2009; Rockenbach et al., 2007; Super et al., 2016; Zhang and Casari, 2012). Accordingly, in this study, we aim to continue with this line of research by using promises as decision-making procedure in a gift-exchange game played by groups. As a decision tool, promises have been shown to enhance efficiency in individual gift-exchange game by Charness et al. (2013). They showed that workers’ contract proposals including a promise of effort level increased aggregate payoffs. This finding was obtained without any communication apart from the suggested transfer or effort and was explained as a result of moral burden to honor non-binding promises. To interpret such evidence, Dufwenberg and Gneezy (2000), Charness and Dufwenberg (2006) and Battigalli and Dufwenberg (2007) postulated the presence of individual guilt aversion, whereas Ellingsen and Johannesson (2004) that of preferences for consistency (“keeping one's word”) and Vanberg (2008) for promise keeping per se. In other works (Bracht and Feltovich, 2009; Charness and Dufwenberg, 2010), bare agents’ promises were shown to have minor impact on efficiency.====To examine the role of promises in group gift-exchange game, we used a classical one-shot version of the game in which, however, agents make a non-binding promises before principals’ decisions. In other words, the agent group is allowed to promise an effort level before the principal group chooses the transfer. This is the only difference with respect to the game, as described earlier. Please note that we excluded any other form of direct communication within or between groups which can agree on their decisions by unanimity rule. In addition, we compare two decision procedures. In the baseline treatment, all members are asked to submit their own choice, while in the proposer treatment, which is a modified version of the Baron and Ferejohn bargaining game (Baron and Ferejohn, 1989), one randomly chosen group member acts as proposer and other members are asked to vote his/her proposal. The comparison between the two treatments investigates the robustness of intra-group agreement with respect to elements that are considered relevant in bargaining models (Banks and Duggan, 2000; Miller and Vanberg, 2015; Song 2009) and also highlights social conformity mechanisms. In particular, social conformity is supposed to be stronger with the proposer, who does not bear full responsibility for the decision but can influence other group members (Miller and Vanberg, 2015) by providing an anchor to which they can adhere (Cason and Mui, 1997). This hypothesis is in line with psychological literature on imitative behavior and norm compliance, as assumed by social comparison theory (Festinger, 1954) in which individuals would like to present themselves in a socially desirable way (Brown, 1986; Levinger and Schneider, 1969; Myers et al., 1980). Conformity can be also attributed to biological and evolutionary causes (Claidière and Whiten, 2012) that play a key role in determining group behavior in different context (e.g., Bardsley and Sausgruber, 2005; Fehr et al., 1998).",Promises in group decision making,https://www.sciencedirect.com/science/article/pii/S1090944319303394,5 November 2019,2019,Research Article,119.0
Gunadi Christian,"Department of Economics, UC, Riverside, CA 92521, USA","Received 18 October 2019, Accepted 18 October 2019, Available online 22 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.rie.2019.10.004,Cited by (3),"Does slavery play a role in explaining why some areas are more prone to hate crimes? Using county-level data on slavery in 1860, I find evidence that U.S. counties with a higher share of slaves in the population more than 150 years ago are more likely to observe hate crime incidents today. One percentage point increase in the share of slaves in the population in 1860 is associated with 0.018 more hate crime incidents per 100,000 population directed at blacks today. Additionally, there is evidence that slavery is associated with more hate crime incidents directed towards Jews and LGBT population. This result supports previous studies which find persistence in cultural norms and racial attitudes.","Hate crimes incidents have been rising in recent years. Although overall crime has been declining across major U.S. cities, hate crime incidents rose to a decade high of 2009 in 2018, a 50% increase from the decade low of 1332 in 2013 (CSHE, 2019). The rising trend is not only observed in the United States. In 2018, anti-semitic incidents rose considerably in Germany, Canada, and France (CSHE, 2019). Since crime imposes economic costs, it is important to examine factors that may contribute to the rise of hate crimes.====In this paper, I examine the role of slavery in explaining the present-day hate crime incidents in the United States. Using county-level data on slavery in 1860, I find evidence that prevalence of slavery is a significant predictor of hate crime incidents today. One percentage point increase in the share of slaves in the population in 1860 is associated with 0.018 more hate crime incidents per 100,000 population directed at blacks. The magnitude of this estimate is economically meaningful. Evaluated at the mean, it corresponds to a 5.8% increase in hate crime incidents directed at blacks. Furthermore, there is evidence that the prevalence of slavery in 1860 is statistically significantly associated with more hate crime incidents directed toward Jews and LGBT population.====The findings of this paper contribute to studies that examine the persistence of cultural norms and racial attitudes. One such work is Voigtländer and Voth (2012), which found that localities with a history of pogroms against Jews were more likely to exhibit violence towards Jews during the Nazi regime in Germany. Another work by Adena et al. (2015) discovered that the exposure to anti-Semitic propaganda had its most effective effect in areas where anti-Semitism was historically high. On female labor force participation, Alesina et al. (2013) found that the descendants of societies that traditionally practiced plough agriculture, which gives a comparative advantage to males for agricultural tasks, have a lower rate of female participation in the workforce.====This paper is also related to studies that examine the long-term effect of slavery on contemporary socio-economic outcomes. For example, the work by Engerman and Sokoloff (2002) argued that the reliance on slave labor resulted in extreme economic inequality, which in turn hamper economic growth. Testing the hypothesis proposed by Engerman and Sokoloff (2002), Nunn (2008) found that slavery is negatively correlated with economic development. However, the author did not find evidence that this relationship works through slavery’s effect on economic inequality as argued by Engerman and Sokoloff (2002). Investigating the effect of slavery on inequality across U.S. counties, Bertocchi and Dimico (2014) found that the proportion of slaves in the population in 1860 is associated with higher inequality across races today. The author showed that the observed effect comes from unequal educational attainment between blacks and whites due to slavery. On political attitudes, Acharya et al. (2016) found that whites who currently live in counties that had a high prevalence of slavery in 1860 are more likely to express racial resentment toward blacks.====A closely related study to this paper is the work by Gouda and Rigterink (2017), which examines the effect of slavery on contemporary violent crime. The authors found that although slavery is a strong predictor of violent crime rate today, the evidence on the relationship between slavery and violent hate crime directed towards blacks is inconclusive. I complement the work of Gouda and Rigterink (2017) in the following ways. First, I do not limit my analysis only to violent hate crime. This is important since the majority of hate crime incidents are non-violent in the form of vandalism or intimidation (Table 1).==== Additionally, since slavery may promote a culture that is hostile against minorities in general, I also examine whether slavery is associated with hate crime directed towards minorities other than blacks. Second, to address the potential bias from measurement error and other unobserved factors, I use soil suitability for cotton as an instrument. The instrument used by Gouda and Rigterink (2017), temperature suitability for malaria, seems unlikely to meet exclusion restriction for a valid instrument since crime rates have been found to be sensitive to changes in temperature (Field, 1992, Harp, Karnauskas, 2018, Heilmann, Kahn, 2019). It is possible that an area with suitable temperature for malaria is also an area with a climate that is conducive to commit a crime. Finally, I examine the effect of slavery in the period characterizes by the rise in hate crimes (2015–2017).====The rest of the paper is constructed as follows. Section 2 describes the data and empirical methodology used in the analysis. Section 3 documents the results of the analysis. Section 4 concludes.",The legacy of slavery on hate crime in the United States,https://www.sciencedirect.com/science/article/pii/S1090944319303941,22 October 2019,2019,Research Article,124.0
Yakita Akira,"Faculty of Economics, Nanzan University, 18 Yamasato-cho, Showa-ku, Nagoya 466-8673, Japan","Received 16 September 2019, Accepted 11 October 2019, Available online 16 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.rie.2019.10.002,Cited by (0),"We examine the optimality of public long-term care policy, incorporating an exchange game between elderly parents and adult children and transfer-seeking competition among siblings, instead of children's ","Increases in longevity tend to increase demand for elderly long-term care because healthy life duration might not increase at the same pace (Mayhew, 2011). Especially, elderly people 80 years old and older are more likely to fall into dependency. Recent declines in family solidarity induced by factors such as children's mobility and increased female labor force participation put more pressure on families to care for their dependent parents (e.g., Cremer et al., 2013, 2017). Mounting demand for elderly care requires that markets and the state share burdens of providing elderly long-term care with families.====Most reports of the literature related to optimal long-term care policy are based on altruism models.==== Cremer et al. (2013, 2017), among others, analyze optimal long-term care policy when both children's altruism toward the parents and the event of parents’ falling into dependency are uncertain. They consider topping-up and opting-out schemes and compare them in consideration of the extent to which children are altruistic toward the parents and whether the long-term care insurance is actuarially fair, or not.====As described in this paper, we examine the optimality of public long-term care policy using an intergenerational exchange game between elderly parents and adult children and a transfer-seeking competition game among siblings (Bernheim et al., 1985; Chang and Weisman, 1995; Yakita, 2018) instead of children's altruism.==== Although few parents plan to leave a larger share of their bequest to children who provide more care in Western Europe countries, more parents in economically developing countries of Asia are willing to have children for their own support when they become elderly (Leroux and Pestieau, 2014). Kagitcibasi (1982) reports that in Asian countries such as Indonesia, the Philippines, Thailand, and Turkey the proportions of parents who consider old age security as a reason for having children were about 80% and higher, although those in the U.S. and Germany were only about 8%.==== Given that population aging is proceeding in these Asian developing countries, the long-term care policy should be analyzed in intergenerational exchange models. This is the aim of this study.====We consider a long-term care policy that covers a generation of elderly parents without public intergenerational income transfers.==== Children might also provide attention and care, but only when parents fall into dependency, in exchange for transfers from them. For analytical purposes, we rule out any intergenerational altruism.====Results demonstrate that whether the long-term care policy is socially necessary or not depends on how efficiently government can provide long-term care. A higher tax can support more public long-term care. It also decreases parental bequests, inducing children to provide less family care in exchange for transfers. If children's wages are sufficiently low, then the competition for transfers among children becomes severe. Children tend to provide more family care per unit of bequests. Therefore, the net effect of taxation on the sum of public and family elderly care is likely to be small. In this case, a long-term care policy might be socially undesirable. This is the case if the child wage rate is sufficiently low to provide adequate care for obtaining transfers. By contrast, if the children's wages are sufficiently high to seek little in transfers from parents, then the effect of taxation on family elderly care provision is small; also, the net effect of taxation on total (public and family) long-term care provision is great. Therefore, the public long-term care provision policy is socially desirable.====The next section introduces a model of a two-stage game between parents and children. The model includes many families, each of which has parents and transfer-seeking children. Section 3 presents an examination of the optimality of actuarially fair public long-term-care insurance. The final section concludes this paper.",Optimal long-term care policy in an intergenerational exchange setting,https://www.sciencedirect.com/science/article/pii/S1090944319303552,16 October 2019,2019,Research Article,125.0
"Afawubo Komivi,Noglo Yawo Agbényégan","CEREFIGE EA3942-University of Lorraine-France, 54037 Nancy Cedex, France,CRESE EA3190-University of Bourgogne Franche-Comté, F-25000 Besançon, France,Paris West University Nanterre La Défense, 92001, Nanterre Cedex, France,Research Observatory : CAP-Afriques, University of Québec in Montréal (UQAM), Montréal (Québec) H3C 3P8, Canada,INSEEC MSc & MBA Institute, 75019, Paris, France","Received 16 August 2019, Accepted 10 October 2019, Available online 12 October 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.rie.2019.10.001,Cited by (12),"This study examines the impact of remittance inflows on deforestation in developing countries. We also investigate the role of institutional quality in enhancing remittances’ effect in reducing deforestation. Our results suggest that overall remittances reduce deforestation. We show that remittances’ reduction effect on deforestation is greater in middle-income countries than in low-income countries. Considering institutional quality, our findings suggest that, for the entire sample, and in low- and middle-income countries, control of corruption, political stability, government effectiveness and rule of law act to reduce deforestation. Moreover, institutional quality enhances the impact of remittances on reducing deforestation in the entire sample and in middle-income countries. In contrast, in low-income countries, institutional quality does not complement remittances to reduce deforestation.====These results imply that, to reduce deforestation rates, the focus should not only be on economic development, but to an even greater extent, on institutional quality.","Deforestation is currently one of the key environmental issues in the context of climate change and loss of biodiversity. Hansen et al. (2013) define forest loss as a stand-replacement disturbance or the complete removal of tree cover canopy at the Landsat pixel scale. The same authors define forest gain as the inverse of loss, or the establishment of tree canopy from a no-forest state.====Several recent studies have provided information about the evolution of deforestation. Due to different data sources, these studies lead to different assessments of global forest resources. According to the last Forest Resource Assessment (FRA) by FAO (2015), deforestation has slowed down: the net annual rate of loss slowed from −0.18% in the 1990s to −0.08% between 2010 and 2015. This decreasing trend is at odds with Kim et al. (2015), who show that deforestation increased by 62% in the 2000s compared to the previous decade. Kim et al. (2015) findings use data similar to the dataset employed by Hansen et al. (2013) and also are based on land cover imagery processing.====Economists have long studied the determinants of deforestation. Responsible factors include economic development (Culas, 2012; Chiu, 2012), institutional factors (Culas, 2007; Galinato and Galinato, 2013), plantations (Heilmayr, 2014), and agricultural activity (Barbier, 2004).====To the best of our knowledge, studies linking migrants’ remittances and deforestation are still rare, but some research has provided interesting lessons. According Lopez-Feldman and Chavez (2017), theoretically, the impact that remittances have on local environment is ambiguous. Indeed, for these authors, as remittances alleviate poverty, they might allow households to use market goods instead of extracting natural resources locally (e.g., gas instead of firewood) but they could also increase the demand for goods that put more pressure on the local environment (e.g., locally raised meat). Thus remittances could decrease or increase the pressure on natural resources. As for Hecht et al. (2006), if remittances could help to preserve or regenerate forest areas, they could also produce the opposite effect depending on the context. According to Duval and Wolff (2009), transfers of funds by migrants significantly influence the process of deforestation in developing countries.====This article aims at providing a new study of the impact of international remittances on deforestation in developing countries. This research is interesting for several reasons. First, the issue of the influence of migrants’ financial transfers continues to play an important role in economic debates. For developing countries, these transfers represent a source of external financing twice as high as the development aid they receive, and equivalent to almost two-thirds of their foreign direct investment (Ratha et al., 2005). Also, the graphs in Appendices 1 and 2 indicate a significant increase in remittances from 1995 to 2014 in low- and middle-income countries. In low-income countries (Appendix 1), although net official development assistance (ODA) is higher than remittances, the latter have increased since 1995 (1.29% of GDP) to 4.86% of GDP in 2014. As regards middle-income countries (Appendix 2), remittances are higher than net ODA and, after a peak reaching 1.79% of GDP in 2003 and 1.71% of GDP in 2007, sharply decreased to 1.43% of GDP in 2014. Secondly, forests play an essential role in the subsistence of rural poor people in developing countries, as they are also a source of food (meat, vegetables, fruit, etc.). The World Bank estimates that more than one billion people live in and around forests (World Bank, 2006). In addition, the World Bank estimates that more than 2 billion people depend on wood for their energy needs, mainly for cooking and heating (World Bank, 2006).====Moreover, what makes this research unique is that it extends the global study by Duval and Wolff (2009). This is because it uses more recent data from 1996 to 2014, while the Duval and Wolff (2009) study was based on old data (between 1990 and 2005). In addition, for the first time in the economic literature, our study also involves a decomposition of low- and middle-income countries.====The aim of our research is to explore whether remittances could improve people's living conditions and therefore divert them from deforestation activities. Comparing remittance levels between low-income and middle-income countries (see Appendices 1 and 2), we find that the level of remittances in low-income countries is much higher than that of middle-income countries in our study period. Therefore, we hypothesize that low-income countries will benefit more from remittances (Appendix 1 vs. Appendix 2), which could improve people's living conditions and therefore reduce the burden of deforestation. On the other hand, middle-income countries with better living conditions will receive fewer remittances (Appendix 1 vs. Appendix 2) so that ultimately remittances will not be able to hugely and significantly reduce deforestation.====Finally, we introduce an innovation into our approach that no longer exists in any literature, i.e. the influence of institutional quality. This new approach is intended to clarify for the first time whether or not institutional quality mitigates the influence of international remittances on deforestation. As a result of this study, we can understand in a more comprehensive way, both globally and specifically (income levels), how migrants’ financial transfers impact deforestation in developing countries in the case of institutional involvement.====The present work seeks to help fill the gap left in the literature by attempting to answer the following questions: Could low-income countries with higher remittance inflows make a greater contribution to reducing deforestation than middle-income countries with lower remittance inflows? What is the role of institutional quality in this relationship?====We carried out a panel analysis on 106 countries for the period 1995–2014, using remittances and the standard explanatory variables used in the literature. We estimate three econometric models namely OLS FE, OLS RE and GMM, on the overall sample and on the decomposition into low- and middle- income countries. Our results suggest that overall remittances reduce deforestation in developing countries and also in low- and middle-income countries taken separately. In contrast with our hypothesis, the effect on reducing deforestation is more acute in middle-income countries than in low-income countries. This could be explained by the fact that remittances, although high in low-income countries, do not significantly reduce the extreme poverty in these regions and consequently only slightly reduce deforestation compared to middle-income countries.====The interaction effects between remittances and institutional quality (control of corruption, political stability, effectiveness and the rule of law) are also addressed. For the entire sample, the results show that the control of corruption and government effectiveness accentuate remittances’ negative impact on deforestation. Considering low-income countries, the cross effects between remittances and the four institutional quality variables are not significant in any of the models. Hence, the institutional environment does not significantly influence remittances’ impact on deforestation. As regards middle-income countries, the interaction impact of remittances and the four institutional quality variables reduces deforestation. Thus, the institutional framework of middle-income countries further increases the advantageous effect of remittances to combat deforestation. Thus, if remittances can have an impact on reducing deforestation, institutional quality is paramount in order to permit these financial flows to be effective. Control variables, such as GDP per capita, square GDP per capita, GDP per capita growth, population density, openness, and terms of trade, also provide interesting results that will be analyzed.====The paper is organized as follows. After presenting a literature review of different determinants of deforestation, we describe the empirical strategy and the data. Then, we provide the results of the impact of international remittances on deforestation and the influence of institutional quality. The paper ends with some concluding remarks.",Remittances and deforestation in developing countries: Is institutional quality paramount?,https://www.sciencedirect.com/science/article/pii/S1090944319303199,12 October 2019,2019,Research Article,126.0
"Giannelli Gianna Claudia,Rapallini Chiara","Department of Economics and Management, University of Florence, Italy,IZA, Germany,CHILD, Italy","Received 1 August 2019, Accepted 22 August 2019, Available online 30 August 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.rie.2019.08.003,Cited by (4),"We investigate the relationship between math attitude and students’ math scores using data obtained from PISA 2012 and a 2SLS model. Math attitude is approximated by an indicator that takes into account intangible (parental attitude, student instrumental motivation and student math anxiety) and tangible (parental help in math homework) factors. The presence of one family member in a math-related career is our ","School performances in science and math have attracted an increasing attention among scholars because of the worldwide emphasis on their importance for technological development and global economic competition (Tucker-Drob et al., 2014). According to the OECD, an improvement of one-half standard deviation in mathematics and science performance at the individual level implies, by historical experience, an increase in the annual growth rate of GDP per capita of 0.87% (OECD, 2010). In terms of individual outcomes, Joensen, Nielsen, 2015, Joensen, Nielsen, 2009 show that there is a causal relationship between Math and earnings. Other evidence shows that Science Technology Engineering or Math (STEM) majors have the highest returns, while Arts/Humanities majors have the lowest (Webber, 2014). Coherently, having a math-related career increases the likelihood of getting a better job, as shown by suggestive evidence we have drawn from PISA data. Fig. 1 plots the kernel density estimates of the two digit international socio-economic index of occupational status (ISEI) of parents who either have or do not have a career that requires studying math at a university level. Higher levels of the index indicate higher levels of occupational status. The kernel density estimate is markedly right skewed for parents who do not have a math-related career, while for parents who have a math related career the distribution is more uniform.====Recent economics literature studies the determinants of educational achievements taking into account the effects of both the ==== and ==== components of family background (Björklund and Salvanes, 2011). Primary ==== factors are parental education, family income and parental help in homework. ==== components of the family background are inherited traits, beliefs and cultural values. In particular, intelligence and personality - respectively referred to as ==== and ==== skills - are traits that are shown to be relevant for educational outcomes (Rustichini, Iacono, McGue, 2017, Krapohl, Rimfeld, Shakeshaft, Trzaskowski, McMillan, Pingault, Asbury, Harlaar, Kovas, Dale, et al., 2014, Heckman, Kautz, 2012). Moreover, parents transmit different beliefs and values to their children (Bisin and Verdier, 2001), including the ability to delay gratification and exert self-control, that have been shown to differ across cultures and explain school outcomes (Figlio et al., 2016).====In the psychology literature, numerous studies have investigated how the dimensions of parenting are linked to the academic efforts, performances and occupational aspirations of students using both survey data and field experiments. These studies hypothesize that parents aim to transmit their values and beliefs to children through parental behavior. According to this view, in conversations with their children, parents may assert that studying math is relevant for the future and might encourage their children to put more effort into the study of math (Harackiewicz et al., 2012). Another way of transmitting this belief rests on the hypothesis that children internalize values through a positive identification with one’s parents, i.e., one’s parent is perceived as a positive role model (Jodl et al., 2001).====The multifaceted nature of math attitude should be considered when analysing its effect on student performance. Math attitude has been defined as the cluster of beliefs and affective orientations related to math, such as math self-concepts, and attributions and expectations for success and failure, math anxiety and math gender stereotypes (Gunderson et al., 2012). Parents and teachers are both considered the primary means for the intergenerational transmission of all these aspects, and the attention of scholars has primarily been devoted to the gender gap (Thompson, 2017, Gunderson, Ramirez, Levine, Beilock, 2012). In the vast literature on the math gender gap, scholars generally agree that environmental factors are crucial in the development of gender-math attitudes and that the lower performance of girls is linked to a lack of confidence, which can be measured by means of questions evaluating the self-efficacy, self-concept and anxiety of students when they approach the subject (OECD, 2015, Saarela, Karkkainen, 2014, Guiso, Monte, Sapienza, Zingales, 2008). Independent from the sex of the student, not only the students’ math anxiety but also that of the adults have received increasing attention in this debate. Gunderson et al. (2012) and Casad et al. (2015) have shown that adults’ own math anxieties and their beliefs that math ability is a stable trait may have significant impacts on children’s development of math attitude. Furthermore, some evidence from randomized experiments shows that short numerical problems delivered through an iPad application significantly increases children’s math achievement across the school year compared to a control group, especially for children whose parents are anxious about math (Berkowitz et al., 2015).====In this paper, we investigate the mechanism through which having parents who work in a math-related career contributes to explaining children’s math performance by affecting both ==== and ==== factors. The former are parental attitude, children’s instrumental motivation and anxiety toward math, and the latter is the parental help in math homework. Our working hypothesis is that parents, being aware that math skills are an advantage in the labour market –as Fig. 1 suggests–, may ease their children’s approach to math through at least four channels. First, parents who are in a math-related career in their conversations may assert that math is important for the future of their children in terms of placement in the job market. In this case, the belief about the relevance of math is not necessarily shared by their children. Second, parents who are in a math-related career might succeed in transmitting this belief, so that the children - if asked - would assert that math is an instrument to find a good job. In this case, if the student believes that making an effort in mathematics is worth it because it will help in her future work, the positive effect on her score might be even larger. Third, the fact that parents might appear to be more self-confident and relaxed about math when working in a math-related career, might help reduce math anxiety in their children. Actually, there are at least two reasons explaining this phenomenon: the first is that the parents themselves are less anxious, thus transmitting a positive feeling in approaching math; second, the positive identification with one or both parents can reduce the anxiety of the child. In all these cases, these beliefs and feelings might encourage children to study the subject. Fourth, when parents possess a high level of math skills, they might be more capable of helping their children with their math homework. In this case, the effect on math scores would be also conveyed through this tangible channel. In this study we measure math attitude with an indicator that takes into account all these factors.====From a methodological perspective, studies on the relationship between math attitude and children’s school achievements may suffer from an endogeneity problem because the former can be influenced by the latter. In other words, parents could claim that math is important for the future of their children merely because their children have high/low scores in this subject. The same problem emerges when studying the relationship between student attitudes and their performance. For example, students may declare that math will help them find a good job in the future because they are influenced by their scores. Likewise, student anxiety may be affected by their scores. For the same reason, parental help in math may be endogenous. To address this endogeneity issue, the instrument we adopt for our identification strategy is whether at least one of the student’s family members is in a math-related career. We expect this variable to be exogenous, because parents’ career is likely to be independent from children’s scores, and discuss the instrument validity.====Our study contributes to the empirical literature of the economics of education by investigating the link between the labor market and the educational system through the role played by the family. Actually, the empirical research investigating the effect of parental job on children is mainly focused on the direct causal impact. For example, Rege et al. (2011) study the effect of the parental job loss on children’s school outcomes, and they find that the negative effect is neither related to loss of income, nor to shift in maternal time towards employment or other ==== factors. They conclude that the effect of fathers’ job loss on children’s school outcomes can be explained by mental distress experienced by the fathers, thus suggesting to investigate the role played by attitudes and behaviors. In a more general perspective, the performance of an educational system may be supported by parents’ beliefs and behaviors: if math skills are demanded by the labor market, this can generate a virtuous circle. In other words, the school outcomes of children can improve because of parental awareness of the importance of math. On the contrary, if math skills are not demanded by the economy, parents may be unaware of the importance of math, and this may have a negative effect on the performance of the school system.====The data we use are obtained from the Programme for International Student Assessment (PISA) 2012, which measures the cognitive achievement of 15 year olds specifically targeting mathematical skills, with several sections dedicated to this topic.====The structure of the paper is the following. Section II overviews the background literature. Sections III presents the empirical strategy. Section IV describes the data, the sample and the variables. Section V presents and discusses the results, and Section VI concludes.",Parental occupation and children’s school outcomes in math.,https://www.sciencedirect.com/science/article/pii/S1090944319302923,30 August 2019,2019,Research Article,127.0
Turdaliev Nurlan,"Department of Economics, University of Windsor, 401 Sunset Avenue, Windsor, Ontario N9B 3P4, Canada","Received 24 July 2019, Accepted 22 August 2019, Available online 24 August 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.rie.2019.08.002,Cited by (0),"Adding heterogeneity to an otherwise simple model results in a deviation from the Friedman rule. We show that a central bank concerned with inequality delivers an outcome below the Pareto frontier. Our results may shed light as to why central banks around the world do not follow the Friedman rule and instead deliver positive inflation rates. On the other hand, the calibrated model indicates that the implied optimal inflation rates are much higher than those observed in the data. One possible interpretation of our results is to question the recent wisdom of thinking of inequality as part of central banks’ concerns.","The issue of inequality, after being mostly ignored for a long time, is becoming a booming area of study (see Alvaredo, Atkinson, Morelli, 2018, Krueger, Mitman, Perri, 2016, Mian, Sufi, 2016, Piketty, 2014). Many authors have started investigating the effect of monetary policy on inequality (Auclert, 2019, Bernanke, 2015, Coibion, Gorodnichenko, Kueng, Silvia, 2017, Domanski, Scatigna, Zabai, 2016, Turdaliev, 2018). While this effect is found to be significant, this paper argues that monetary policy is not an adequate tool to address this issue. We build a simple model with heterogeneous population and demonstrate that a central bank that cares equally about each agent in the economy will choose a higher level of inflation than efficiency dictates. A Pareto efficient allocation is obtained when the Friedman rule is followed. However, such an allocation is viewed by the central bank concerned about inequality as suboptimal, and its desire to correct it leads to a deviation from the Pareto frontier.====Let us provide some details of our results. While the Friedman rule has been shown to be optimal in many environments, it has not been implemented as a goal or rule by any central bank. Furthermore, many countries have adopted inflation targets at relatively low levels. Schmitt-Grohé and Uribe (2010) report that attempts to attribute this discrepancy to incomplete tax systems, downward rigidity of prices, or the zero lower bound on nominal interest rates do not seem to succeed much and propose to explore heterogeneity. And this is what our paper does. We demonstrate that in our simple environment with heterogeneity in productivity of workers, (i) Pareto optimality requires adherence to the Friedman rule; and (ii) social welfare calculated by putting an equal weight on each agent in the economy is not maximized when following the Friedman rule. We show that within a certain range inflation facilitates redistribution from the high-productivity workers to the low-productivity ones. To the extent that inflation taxes consumption via the cash-in-advance constraint, it hurts more high-productivity workers: they have to carry larger money holdings to finance consumption purchases.====To introduce inequality considerations to the goals of monetary policy, the central bank is assumed to be aware of the impact of its actions on inequality, to care about this impact and maximize a weighted average of utilities of heterogeneous households. We prove the existence of a (more “conservative”) central bank that puts a much higher weight on the utility of the high-productivity workers and delivers the Friedman-rule allocation. One possible interpretation of our results is that in a heterogeneous world, if inequality considerations are a concern to the monetary authority, there exists a natural tendency for central banks to choose a higher level of inflation than efficiency requires. With this interpretation on hand, we conclude that perhaps inequality should not be part of central banks’ concerns and instead be left to other branches of government such as the fiscal authority.====We calibrate the model and compute optimal inflation rates for a group of OECD countries by splitting the population to top 10% and the remaining 90%. We also find the implied weight the central bank has to put on welfare of the high-productivity workers in order to deliver the inflation rate observed in the data. In addition, we perform a sensitivity analysis. Our findings indicate that for a majority of countries the optimal inflation rates are significantly higher than those in the data. We conclude that these central banks have been “conservative”, i.e. biased toward the more productive part of the population.====We are not the first to point out a positive relationship between inflation and inequality. Let us divide the relevant literature on monetary policy in the context of heterogeneity into two main streams. In the first one, government uses both monetary and fiscal policy to achieve a desired outcome. Menna and Tirelli (2017) reconsider the received wisdom that the inflation tax burden falls largely on the poor. In an environment with an incomplete tax system they show that a combination of a lower income tax and higher inflation reduces inequality, with optimal inflation rate being well above the typical inflation targets in the developed economies. Higher inflation allows to tax consumption out of monopoly profits, which are not taxed. Simulations of their calibrated model demonstrates that social optimum features a higher steady-state inflation rate than that obtained in the representative agent environment, a result similar to our theoretical findings.====The second strand of the literature with heterogeneous agents, to which this paper belongs, considers monetary policy alone. Lahiri and Magnani (2012) explore the relationship between skill heterogeneity and long-run inflation. They find an empirical positive correlation between worker skills heterogeneity and inflation. And they build a model that features this relationship. Using a model with a representative household consisting of two heterogeneous workers they find that increased heterogeneity leads to possible preference for higher inflation. And while their results stem from numerical experiments, ours are obtained analytically.====Palivos (2004) considers an overlapping generations model with high-altruistic and low-altruistic agents (measured by how much they care about their offspring) and two assets, capital and money. Monetary policy exhibits a redistributive effect, and the optimal rate of monetary expansion is higher than that implied by the Friedman rule. The result is driven by this feature of the model: if the population consisted of low-altruism agents only, the optimal money growth rate would be higher than that consistent with the Friedman rule. In our model, however, violation of the Friedman rule arises only when heterogeneity is introduced: the Friedman rule does hold in the cases when the population consists entirely of high-skill agents or entirely of low-skill ones.====While the above papers assume inherent heterogeneity among households, there is an alternative modelling approach that assumes ex-ante identical agents with heterogeneity arising due to idiosyncratic shocks. Dolmas et al. (2000) find an empirical evidence of a positive relationship between inequality and inflation and build an overlapping-generations political-economic model that generates this relationship. The key mechanism in their model is lump-sum transfers from government financed by money creation; these transfers are intragenerational. In our model, monetary transfers facilitate redistribution across different agents because these transfers favor the poor.====And finally, some authors explore the short-term consequences of monetary policy on inequality. Gornemann et al. (2016) build a rich environment based on the New Keynesian framework where heterogeneity with respect to employment status arises due to the search and matching friction. They find a substantial redistributional effect of monetary policy, and, in particular, that a negative effect of contractionary monetary policy is larger if heterogeneity is taken into account. In a related study, Coibion et al. (2017) find that contractionary monetary policy in the U.S. systematically increases inequality. One of the mechanisms in work in their paper is that transfers are a relatively larger proportion of income of poor households, and that after contractionary shocks, real wages rise relative to transfers.====The remaining of the paper is organized as follows. Section 2 presents the model. Section 3 characterizes Ramsey equilibrium for various central bank preferences. Section 4 discusses our calibration and the numerical results. Section 5 summarizes and concludes. Proofs are in the Appendix.","Monetary policy, heterogeneous population and inflation",https://www.sciencedirect.com/science/article/pii/S1090944319302728,24 August 2019,2019,Research Article,128.0
"Krčál Ondřej,Staněk Rostislav,Slanicay Martin","Faculty of Economics and Administration, Masaryk University, Lipová 41a, 602 00 Brno, Czech Republic","Received 24 July 2019, Accepted 22 August 2019, Available online 22 August 2019, Version of Record 26 November 2019.",https://doi.org/10.1016/j.rie.2019.08.001,Cited by (0),"A large body of evidence supports a negative association between risk aversion of workers and the level of risk they face in their occupations. This relationship could be explained by the self-selection of workers into jobs according to their risk preferences or by the effect on risk attitudes of occupations in which people face or witness dangerous situations. We use incentivized experiments to measure risk preferences among three different groups: experienced firefighters, novice firefighters, and students. We find that experienced firefighters are less risk-averse than novice firefighters, and these in turn are less risk-averse than students. The effects remain significant even after controlling for other relevant differences between these groups. Our findings suggest that the observed relationship between risk aversion and high-risk occupations is not only a result of self-selection but also of people’s preferences being shaped by their work lives.","People base their career choices on their skills and training, and also on their preferences. Risk preferences are important in this respect as different occupations are associated with varying levels of risk. A large body of evidence indicates a negative association between the risk aversion of workers and the level of risk they face in their work lives. Using standard experimental lottery measures of risk aversion on US data, Barsky et al. (1997), Ahn (2010) and Brown et al. (2006) find that self-employed people are more risk-seeking than employees. Cramer et al. (2002), Hartog et al. (2002), Masclet et al. (2009), Brown et al. (2011), and Skriabikova et al. (2014) report similar findings for European labor markets. Using investment data for 400,000 individuals in Norway, Hvide and Panos (2014) find that common stock investors are around 50% more likely to subsequently start up a firm. Bonin et al. (2007) use data from representative sample of the German population to show that self-assessment of risk attitudes is correlated with cross-sectional variations in earnings. Di Mauro and Musumeci (2011) find similar results for a sample of 258 Italians aged between 25 and 40. If risk preferences are set and do not change during a worker’s life, these findings can be explained by the self-selection of workers into jobs according to their risk attitudes.====Yet a growing literature documents that risk preferences may change if people are exposed to one-off traumatic experiences, such as natural catastrophes or wars (Bucciol, Zarri, 2013, Cameron, Shah, 2015, Cassar, Healy, von Kessler, 2017, Eckel, El-Gamal, Wilson, 2009, Hanaoka, Shigeoka, Watanabe, 2016, Kim, Lee, 2014, Malmendier, Tate, Yan, 2011, Shupp, Loveridge, Skidmore, Lim, Rogers, 2017, Voors, Nillesen, Verwimp, Bulte, Lensink, Van Soest, 2012), if they live under specific conditions for a long period of time, for example if they live in disaster areas despite not having been directly affected by any disaster (Bchir, Willinger, 2013, Cassar, Healy, von Kessler, 2017, Shupp, Loveridge, Skidmore, Lim, Rogers, 2017), if they have lower wealth or income (Guiso, Paiella, 2008, Tanaka, Camerer, Nguyen, 2010), or if they have lived through an era of low stock market returns (Ampudia, Ehrmann, 2017, Malmendier, Nagel, 2011, Malmendier, Tate, Yan, 2011). The direction of the effect is not clear. Some studies indicate that exposure increases risk aversion while other studies find that it reduces risk aversion. In any case, the available evidence raises the possibility that risk preferences could be changed by work involving risk or danger.====If the exposure to risk on the job affects risk aversion, the explanation of the relationship between risk preferences and riskiness of work would need to be revised, and the revision would depend on the direction of the effect. If exposure to risk at work reduces risk aversion, the role of self-selection in the observed correlation between risk attitudes and on-the-job risks might be lower than previously observed or even nonexistent. If, on the other hand, the exposure to work-related danger increases risk aversion, the role attributed to self-selection might be even larger than what is observed in the data.====This is relevant for at least two reasons. The first reason is related to the role of self-selection in labor markets. One outcome of self-selection into jobs is increased efficiency (Bonin et al., 2007).==== If people are free to choose the occupation that best fits their preferences and their preferences differ substantially, the result is a more efficient allocation in the labor market; in contrast, if people are more similar at the beginning and their differences are deepened during their time at work, self-selection is less beneficial than might have been concluded from the available evidence. The second reason is that it is important to know whether jobs involving danger impact preferences. If exposure to dangerous situations makes soldiers or firefighters too risk-seeking, these workers might make choices that could put other people in danger, especially if they are in charge.==== In addition to that, changes in preferences might spill over into the private lives of these workers. People in high-risk jobs might require more intensive psychological care to prevent any adverse effects on their lives outside of work.====In this paper, we use a lab-in-the field experiment with firefighters to assess the role of self-selection and long-term job exposure to risk in risk preferences. Firefighting is a suitable occupation for this analysis because firefighters face a non-negligible chance of being injured during their career. They are also more likely to witness traffic accidents or fire-related injuries and deaths on a regular basis (for more details see Section 2). Moreover, since Czech firefighters have a low turnover rate, they are ideal for measuring the effect of long-term employment on preferences. Apart from the high status that Czech firefighters enjoy, the main reason for the low turnover is that they are entitled to special pension conditions if they serve for at least 15 years.====To measure risk aversion, we use the bomb risk elicitation task (BRET) by Crosetto and Filippin (2013). We selected this task for its intuitiveness, which makes it especially suitable for field experiments. To test the effects of long-term job exposure, we compare the risk aversion of experienced firefighters to that of novice firefighters who are in their initial training at the corps. The role of self-selection was estimated by comparing novice firefighters and students. To ensure that our results are sound, we control for several factors that are known to be correlated with risk aversion. We find evidence of the job-exposure effect: experienced firefighters are less risk-averse than firefighters in their initial training. We also find that novice firefighters are less risk-averse than their student peers.====The rest of this paper proceeds as follows. Section 2 provides information about firefighters in the Czech Republic. Section 3 describes the experimental design and procedure in detail. Section 4 presents our data and results. Section 5 provides a discussion of our findings.",Made for the job or by the job? A lab-in-the-field experiment with firefighters,https://www.sciencedirect.com/science/article/pii/S1090944319302704,22 August 2019,2019,Research Article,129.0
Guha Brishti,"Centre for International Trade and Development, School of International Studies, Jawaharlal Nehru University, New Delhi 110067, India","Received 23 June 2019, Accepted 24 July 2019, Available online 24 July 2019, Version of Record 5 September 2019.",https://doi.org/10.1016/j.rie.2019.07.006,Cited by (3),"If two players playing a Rubinstein alternating offers game are highly malicious (getting a high utility from “malice” in every period when the other player does not obtain a share in a fixed pie), and highly patient, no equilibrium with an agreement exists and players choose perpetual disagreement. This does not change if the players are subjected to a known deadline after which the pie will be appropriated by outside agencies or disappear: perpetual disagreement is still the only outcome. If in addition players are required to pay endogenously determined fines if they fail to reach agreement, players with discount factors in a certain range do reach agreement, but only at the deadline. However, infinitely patient players would never reach agreement for any feasible level of one-time fines. The result contrasts with spiteful or envious preferences. Our results highlight a novel reason for failure to resolve property disputes.","A player is malicious if, quite apart from utility from gaining his share in the pie, he also experiences a utility from malice in any period in which the bargaining opponent does not get a share of the pie. Thus, malice provides a reason to like periods of disagreement and delay, as the opponent does not obtain any part of the pie until an agreement is actually reached. Impatience works against this, making players dislike delay.====The current paper focuses on the case where players in a Rubinstein bargaining game are both highly malicious and extremely patient. I show that the combination of high malice and patience acts as a very strong force. Specifically, the only possible outcome is perpetual disagreement. More surprisingly, if a finite deadline is imposed on the players, after which the pie will disappear, players remain unaffected, and again, perpetual disagreement results. If, in addition to appropriating the pie after a deadline, an external body (say the government) also required the players to pay one-time fines for failing to reach an agreement, then appropriate fines can induce agreement at the deadline, but only if the discount factor is below a ceiling strictly less than one. Thus, as the discount factor tends to one, perpetual disagreement will occur for any feasible level of one-time fines. For the range of discount factors in which fines work, agreement cannot be reached in any period before the deadline, and each player's share depends positively on patience and own malice and negatively on the other player's malice. The external body may be more successful in inducing agreement at the deadline if it can credibly threaten to extract fines from the players in perpetuity if they fail to reach an agreement by the deadline.====These results provide one possible explanation for the phenomenon of long-running, often unresolved, property disputes in many countries. Consider a few examples. The longest running property dispute in India dates from 1878, when India was still a British colony, and has yet to be resolved. It involves a dispute over two acres of land in Doshipura, Varanasi, between two rival sects, Shias and Sunnis. Although, after multiple failed negotiations and attempts to resolve the matter legally, the Supreme Court did deliver a verdict on the case in 1981 – more than a century after the dispute arose - its ruling could not be carried out due to a fear of triggering off violence between the sects. Attempts to resolve the matter through settlement are still being (unsuccessfully) carried out.==== Similarly, three generations of a litigating family passed away without their intra-family property dispute being resolved.==== Nearly two-thirds of long pending cases in Indian courts involve property disputes. Moreover, sometimes governments are in a position where a huge amount of contested land that could have been put to state use is left vacant for many years – possibly decades, as in the Banjara hills case,==== where 140 acres worth of prime property has remained unused for decades, locked in a dispute between private claimants, each claiming that their ancestors had inherited it from the Nizam of Hyderabad (the erstwhile ruler of a princely state). In another case, a state government took matters into its own hands and declared ownership over 636 acres of land in Faridkot, Punjab, which multiple claimants had been unsuccessfully disputing over since 1973.==== This partially motivates my investigation of whether a policy whereby the government would announce a deadline by which it would appropriate land that had been left vacant due to long-running unresolved disputes, would impact whether agreement is, actually, reached.====The failure to resolve property disputes cannot be attributed solely to an inefficient legal system, as in most cases, rival claimants attempt to arrive at a private settlement precisely because of the dilatory legal system. Moreover, they are free to settle the matter privately at any juncture in the litigation process. Their failure to do so thus reflects bargaining failure.====I now briefly talk about the related literature. The experimental literature shows that malice – broadly defined as utility from causing harm to others without any corresponding monetary benefit to oneself – motivates many people. This literature includes, among others, Beckman et al., 2002, Bosman and van Winden, 2002, Bosman et al., 2006, Albert and Mertins, 2008, Zizzo and Oswald, 2001, Abbink and Sadrieh, 2008, and Abbink and Herrmann (2011). Malice also matters in the courtroom – as discussed in Guha (2016), which develops a model of malicious litigation, legal systems through the ages have always provided for the possibility that a plaintiff may accuse a defendant out of malicious motives, wanting to cause financial and reputational damage.====Guha (2018) introduces malice into the Rubinstein bargaining game, focusing on players who are impatient and not too malicious. In this event, there is a unique subgame perfect equilibrium, where immediate agreement occurs, and the split is more equal than the traditional Rubinstein split. Other negative motives that have been discussed in the literature on Rubinstein bargaining include spite (Montero, 2008) and envy (Kohler, 2013). While I explain the difference between these motives in a discussion section, a major difference in results is that, for both spiteful and envious preferences, the equilibrium involves immediate agreement even for infinitely patient players, unlike the perpetual disagreement outcome for highly patient players (definitely including those who are infinitely patient) in my paper. The intuition for this is provided in the discussion section.====Among the papers on bargaining that do not incorporate negative behavioural factors, relatively few discuss the possibility of perpetual disagreement. (I do not discuss the ones that involve imperfect information or imperfect divisibility of the pie here, due to space constraints, and since they are relatively irrelevant to my paper). Cai (2000) discusses hold-up in a multilateral bargaining setup, where a buyer bargains sequentially with a large number of sellers, and shows that if the number of sellers is high enough, perpetual disagreement may result. My paper, in contrast, has bilateral bargaining, specifically, Rubinstein alternating offers bargaining. Anderlini and Felli (2001) discuss a model where each of two players has to pay participation costs to participate in every round of Rubinstein bargaining. They show that there is always an equilibrium where the players do not pay the participation costs, and perpetually disagree. However, if players are patient enough, there is also an equilibrium where agreement is reached in finite time, while if they are relatively impatient, perpetual disagreement is the unique outcome. This is completely different from my results – I show that for sufficiently ==== players, perpetual disagreement is the unique outcome, while as shown in Guha (2018), immediate agreement is the unique outcome for players whose patience is below this limit. The intuition for this difference between our results is also briefly discussed in the discussion section.====Next, I discuss a few papers on Rubinstein bargaining that consider some form of deadline. Fershtman and Seidmann (1993) discuss a deadline after which the pie disappears. They show that if players are committed not to accept any offer less than an offer that they have previously rejected, then – subject to the proposer's identity in the final period being random – the players reach agreement at the deadline, but not before. In my paper, in contrast (which incorporates malice, and does not assume a commitment to reject previously rejected offers), I find that imposing a deadline after which the pie is appropriated is insufficient to generate agreement, even at the deadline. (Additional incentives, such as fines for disagreement, are necessary, and even these may not work on infinitely patient players.) A different sort of deadline is considered in papers that discuss Rubinstein bargaining with the possibility of arbitration. An arbitrator does not make the pie disappear, but imposes a division if players fail to reach an agreement beforehand. These papers include Manzini and Mariotti (2001) (in which each player has veto power over whether an arbitrator is called in), Ponsati and Sakovics (1998) (in which either player can call in an arbitrator whenever he wishes), and Rong (2012) (in which neither player has veto power over the arbitrator coming in after a fixed time). These papers show that equilibrium with delay can obtain under certain conditions, while in others, immediate agreement occurs: sometimes, the traditional Rubinstein outcome is unaffected. Of these, the one most similar to mine is Rong (2012) in the sense that the arbitrator automatically comes in after a sufficient period of disagreement (rather than the players choosing to call him in) while I also consider the possibility of the government appropriating the pie after a known and finite deadline. However, the fundamental difference is that while in mine, the whole pie will vanish after the deadline, in Rong's paper, the arbitrator will simply impose a split (based on the players’ final offers). None of these papers discusses perpetual disagreement as an outcome (or incorporates malice).====The rest of the paper is organized as follows. Section 2 provides the model and the results. Section 3 contains an intuitive discussion, and Section 4 concludes.",Malice and patience in Rubinstein bargaining,https://www.sciencedirect.com/science/article/pii/S1090944319302248,24 July 2019,2019,Research Article,130.0
Milovanska-Farrington Stefani,"University of Tampa, Sykes College of Business, Department of Economics, 401 W Kennedy Blvd, Tampa, FL 33606, United States","Received 11 June 2019, Accepted 23 July 2019, Available online 24 July 2019, Version of Record 5 September 2019.",https://doi.org/10.1016/j.rie.2019.07.005,Cited by (3),"Governments have recently attempted to reverse the below-replacement fertility rates in Europe by reducing child-rearing costs through child benefits, grants and paid leaves. This article examines the causal effect of family allowances on the likelihood of having another child, and on the extensive and intensive margins of labor supply. Evidence from Switzerland suggests that higher child benefits incentivize parents to have more children but do not affect their employment choice. The effect is larger for low-income families. These findings imply that policies aimed at improving the economic well-being of families are likely to increase fertility rates without distorting labor market outcomes.","Child allowances, or benefits regularly granted to families to partially compensate the expenses associated with child-rearing and to facilitate the balance between work and family responsibilities, can be an important source of income support, especially for low-income families. Because family benefits reduce the financial burden of raising a child, they are expected to have a positive impact on fertility. That is why many countries provide financial assistance to families with children, especially in European countries where the recent decline in the fertility rate contributes to an aging population and raises concerns about future labor supply and sustainability. However, due to the costs such policies impose on local governments, it is useful to evaluate the potential impact of the implementation of programs providing family grants.====The effect of such policies on fertility, parents’ labor supply, child outcomes and maternal mental health has been studied by prior authors in the context of Canada, Hungary, Norway, Spain, and Australia. This article extends previous literature in several ways. First, we explore the effect of family welfare support through child allowances on the likelihood of having another child, the extensive margin of labor supply measured by the dichotomous outcome of being employed or not, and the intensive margin of labor supply captured by the hours worked by the parents. More generally, we investigate whether the benefits received by Swiss families have an impact on family decision-making.====As a measure of the actual amount of allowances families receive we employ the benefits reported by respondents in the Swiss Household Panel survey. However, there is a discrepancy between the reported and the actual amount of assistance, attributable to one of the following reasons: respondents' misunderstanding of the survey question asking them to report the benefits they receive, reporting additional benefits provided by some but not all employers at their discretion, or misreporting due to imperfect information or inability to distinguish between different income sources when salary and child allowances are received simultaneously.====In either case, a mismatch between the reported benefits and the actual amount received indicates that the reported allowances are a noisy measure of the actual ones. Using an imperfect measure of the actual assistance creates an attenuation bias in the OLS estimates of the effect of the allowances on the outcomes of interest. In order to avert this bias, we adopt an instrumental variable approach. Specifically, we use the eligibility amount of benefits as an instrument for the reported allowance.==== The proposed instrument has not been used by previous authors in the context of Switzerland.====Finally, in addition to examining the effect of child allowances on fertility and labor market outcomes, we also study a second, related but distinct matter. Specifically, we identify the causal impact of the introduction of the Federal Family Allowance Act (FamZG) which imposed a universal floor on child benefits common to all 26 cantons in Switzerland.====The findings indicate that an increase in the family support by 1% improves the likelihood of having another child by 0.01%, but does not affect labor market outcomes. Moreover, the analysis of the effect of the introduction of a minimum amount of child allowances all cantons are obliged to pay indicates that a floor on benefits leads to a significant increase in the likelihood of having a child by 4.3% in affected cantons relative to unaffected ones while there is no significant difference between the differential effect of the policy on labor supply in the treated cantons and the controls.====From a policy perspective, these results imply that providing financial support to families to lower the cost of raising children is likely to positively affect fertility without having an adverse effect on the labor market in countries where federal decision-makers aim to improve fertility rates. In addition, such a reform would have a greater positive impact if targeted at families at the lower tail of the household income distribution.====The remainder of this paper is structured as follows. Section 2 reviews prior literature, and provides an overview of the federal law about child allowances in Switzerland. Section 3 presents the identification strategy. In Section 4, we describe the data used for the empirical analysis. Section 5 contains a summary of the findings. In Section 6, we examine the causal effect of the imposition of a universal floor on benefits in all cantons in Switzerland. Section 7 discusses a potential harvesting effect and the policy implications of the study, and Section 8 concludes the paper.",The effect of family welfare support on the likelihood of having another child and parents’ labor supply,https://www.sciencedirect.com/science/article/pii/S1090944319302108,24 July 2019,2019,Research Article,131.0
Fall Moussa K.,"Talence, KEDGE Bs, Bordeaux, France","Received 12 July 2019, Accepted 23 July 2019, Available online 24 July 2019, Version of Record 5 September 2019.",https://doi.org/10.1016/j.rie.2019.07.004,Cited by (1),"In this paper, we investigate the extent to which real appreciation of the Chinese currency contributed in a meaningful way to the drop of its trade surpluses during the ==== subsequent to the financial and economic crises beginning in 2007. Chinese currency appreciated 14.75% in real terms during April 2008 and December 2011. The beginning of 2008 witnessed the most significant part of this real appreciation, after then the appreciations slowed through the crisis and recovery and has included intervals of real depreciation. Using data on exports and imports for Foreign owned firms in China and Chinese owned firms, disaggregated for 29 provinces, spanning the period 2007–2012, we find significant impact which differs from regions also. The results are robust to including a common factor and when compared with impact in the pre-crisis period, before 2006.","By end 2004 China's trade surpluses took an unprecedented turn upward reaching 10% of its GDP in 2008 even though they declined subsequently. With corresponding rapid accumulation of international currency reserves, particularly in US dollars, low rates of economic growth outside China, there has been considerable pressure put on China to appreciate the RMB, with associated resistance by the Chinese authorities. In the aftermath of the 2008 financial and economic crises, China recorded its first trade deficit (7.2 billion US dollars) in March 2010 since its WTO (World Trade Organization) membership. This drop is also attributable to the rising of Chinas’ imports, which increased to 85.5% in January 2010, in part reflecting a pickup in China's domestic consumption. Indeed, a radical adjustment of existing RMB exchange rates in that period appeared to be viewed in a number of western countries as not just a compensatory measure, but almost a panacea for responding to certain economic woes elsewhere, where the latter have been exacerbated by the subprime and Euro zone financial crises. A fundamental issue related to that economic and financial landscape is the extent to which China's trade imbalances responded in a meaningful way to RMB appreciation during 2007–2012.====Some researchers have tried to attach some quantitative importance to the impact of an appreciation of the Chinese RMB on China's trade imbalances. Notably recent approaches have sought to control for the central role of China in not only Asian, but also overall international production networks, where a crucial distinction arises between Chinese exports and production entailing imported components, or transformed production - mostly from elsewhere in Asia - and exports based on value added which is generated within China's internal economy. Multinational enterprises account for by far the lion's share of such transformed production and exports, amounting, as suggested by Ma et al. (2010) to fully approximately 80 percent of such activities. This issue has been central to a number of other recent contributions, considering the trade sensitivity of Chinese exports and imports to exchange rate changes. These include Cheung et al., 2012, Dees, 2001, Aziz and Li, 2007, Cheung et al., 2009, Marquez and Schindler, 2007, Thorbecke and Smith (2010), as well as Garcia-Herrero and Koivu (2009). Among earlier studies focusing on this question can be noted that by Cerra and Dayal-Gulati (1999) which estimated aggregate export and import price elasticities, over the period 1983 to 1997, to be highly inelastic with values, respectively, of −0.3 and 0.7. Simulations by Bénassy-Quéré and Lahreche-Revil (2003) of the trade effects of a depreciation of the RMB found that whereas exports between China and OECD countries increase, there was an associated reduction of imports coming from emerging economies in Asia, under the assumption that exchange rates within Asia remained constant. Further simulations by Kamada and Takagawa (2005), in this case for the effects of a 10% appreciation of the RMB, suggested only a limited impact on China's exports, while imports were weakly stimulated. Yue and Hua (2002), along with Eckauss (2004), based on periods prior to China's entry into the World Trade Organization, broadly confirmed the results of the foregoing studies; namely, that an appreciation of the RMB can produce a fall in Chinese exports, while Voon et al. (2006), using sectoral data covering the period 1978 through 1998, also found similar negative effects. Nonetheless, while studies relying on recent data corroborate the foregoing findings regarding the likely impact of a RMB appreciation on Chinese exports, research, including that of Garcia-Herrero and Koivu (2009), as well as Marquez and Shindler (2007), identified a somewhat surprising result, that such an appreciation can actually produce a negative effect also on imports, where these studies examine China's trade performance from the perspective of global trade shares. More recent studies seek to control for the rapid changing structure of the Chinese economy by using highly disaggregated data, and suggest the need for future studies to do so in order to obtain more precise estimates. Moreover, studies relying on more recent data corroborate the forgoing findings regarding the likely impact of an RMB appreciation on Chinese exports, with a sharp rise in elasticities associated with WTO entry. Cheung et al. (2012), which use disaggregated data, show more robust estimates, but the only way in which they allow for WTO effects is through a step dummy. Girardin and Owen (2014) identify a clear regime changes after the WTO entry in a regime switching analysis of the impact of a real RMB revaluation on the Chinese trade imbalances. An IMF working paper on the Chinese trade imbalances by Ahuja et al. (2012) investigates the dynamics behind the drop of the China's trade surpluses. They conclude that, growing domestic investment; worsening terms of trade, weakening external demand and the real effective exchange rate appreciation explain a large share of the post crisis decline in the Chinese current account surplus.====The novel perspective offered by the present research is an assessment of the extent to which the real revaluation of the RMB contributed in a meaningful way to the shrinking of China's trade surpluses during 2007–2012. Following that objective, we conduct for two separate periods (1996–2006 and 2007–2012) an analysis of determinants of China's trade, adding China's nominal effective exchange rate as new determinant. In the latter regard, to the extent that capital controls in China and the non-convertibility of the RMB have introduced important distortions in the Chinese currency's nominal rate, which estimates and simulations of the effects of a nominal exchange rate appreciation need to take explicitly into account. The disaggregated trade data allows us to show the different impacts on foreign owned firms and local producer firms, and at the same time, show the different elasticities between the coastal and the interior provinces. Notably, in this regard, it is likely that both trade price and income elasticities may differ substantially for coastal regions, relative to those in the interior, in light of the quite different historical evolution of their degrees of openness to international trade and foreign direct investment. Given our short sample of panel data characterising the periods of interest and a relatively larger number of individuals, we use the dynamic GMM method put forth by Arellano and Bond (1991). The empirical analysis includes estimates of regional effective exchange rates at a regional level, as well as statistics for regional GDP.====Between 1996–2006, a real appreciation of the Chinese currency has negative impact on exports for both firm types, the impact is significant for both of them. The impact of the nominal exchange rate on exports is higher in absolute value for both firm types, and shows a positive coefficient for imports of both firm types contrary to the real exchange rate impact. This somewhat surprising results, however, has been identified in certain other recent studies, and can be explained conceptually on the basis of vertical linkages involving international production networks, notably between China, Southeast Asian countries and Japan. The impact of world GDP is significant also for the two types of firm. For the sample of interest 2007–2012, impact on exports and imports for both firm types are higher in absolute value and are strongly significant (when the common factor is added for the local firms). The impact of the nominal effective exchange rate is positive and strongly significant on imports for both firm types, contrary to the situation, the perverse effect disappears also here.====The outline of the rest of this paper is structured as follows. Section 2 offers an overview of salient features characterizing economic activity and trade at a regional level in China. Section 3 describes the data and presents the methodology. While Section 4 draws the principal findings, Section 5 gives the concluding remarks.",To what extent real exchange rate appreciation contributed to the shrinking of China's trade surpluses following the global financial crises?,https://www.sciencedirect.com/science/article/pii/S1090944319302510,24 July 2019,2019,Research Article,132.0
"Ghosh Papiya,Kundu Rajendra P.","Indian Institute of Foreign Trade, India,Jawaharlal Nehru University, India","Received 11 April 2019, Accepted 21 July 2019, Available online 23 July 2019, Version of Record 5 September 2019.",https://doi.org/10.1016/j.rie.2019.07.003,Cited by (2),This paper presents a model of public good provision in networks where the level of public good that an agent has access to depends on the maximum of contributions by the agents in her neighbourhood. Our analysis shows that the game always has an equilibrium in pure strategies and all equilibria are specialised and stable. Welfare analysis shows that formation of new links in a prevailing network may not always be useful.,"This paper investigates how prevailing networks in a society can affect individual incentives to invest in activities which are likely to create non-excludable benefits along direct (and sometimes indirect) social links. Taking innovation as a motivating example, it may be useful to begin by noting that typically innovations are gradual and technology progresses incrementally. A firm has to exert effort for bringing about innovation, with higher effort yielding better progress although at a greater cost. But once they happen some innovations assume the nature of a local public good. With various firms innovating simultaneously, once a firm exerts high effort lower efforts by its peers are rendered unproductive. Thus it is the maximum of the efforts exerted by a firm and its peers that remains relevant in determining the state of the technology available in an industry at a certain point of time. This idea motivates a model of public good provision where the level of public good depends on the maximum of contributions by the agents in a neighbourhood.==== To this end, we model a typical agent with a payoff function that allows for dependence of her benefit on the contribution of other agents; the structure of the benefit reflecting zero marginal return from one’s contribution if it is lower than someone else’s contribution in her neighbourhood. This benefit function has a direct bearing on the “exploitation hypothesis” by Olson and Zeckhauser (1966) according to which the defence preparedness of a group of nations who form an alliance to confront a missile attack or to deter proliferation of nuclear weapons is essentially determined by the greatest effort by a single nation (possibly the richest) in the group.==== Our model is also relevant in contexts like information gathering by individuals wherein people often turn to those close ties for reliable information who are known to have invested substantially higher than others in acquiring those information making the marginal contribution of others in her network zero. Lastly, our model finds an application in the context of measurement of effective literacy of a household or neighbourhood. It was in fact argued by Basu and Foster (1998) and later substantiated by Basu et al. (2002) that literacy often assumes the nature of a local public good with externality spillovers, particularly in a relatively poor rural community and the benefits from literacy that accrues to an individual is determined by the level of literacy of the most literate person in the neighbourhood. This might have an impact on the level of productivity of the individuals and hence on the decisions regarding investment in education.====This paper situates itself in a growing body of literature on games in networks, an exhaustive survey of which is provided in Jackson (2008), Goyal (2007) and Jackson and Zenou (2015). An important strand of the literature to which this paper contributes analyses how the structure of a given network shapes the equilibrium outcomes of a game. Most of the literature however focuses its attention on games of strategic complementarity, like in Morris (2000), Jackson (2008), Ballester et al. (2006) and Corbo et al. (2007), because they are well-behaved in a variety of ways and games of strategic substitutes in two-action space, as in Jackson (2008) and Galeotti et al. (2010). The analysis of games of strategic substitutes with continuous action space is somewhat limited because the equilibrium in such cases, if they exist, are difficult to find. While Bramoulle and Kranton (2007) was the first paper to provide such an analysis for undirected networks in the context of public good provision, López-Pintado (2013) extends it for directed networks. Bramoulle et al. (2014) further explores how network link patterns affect the equilibrium through strategic interaction for a broader class of games. While the settings analysed by them find many useful applications, there is still a wide variety of other settings, with agents choosing their actions from a continuum, exploring which may be worthwhile.====Our model, aimed at analysing the setting discussed at the outset, is a simple adaptation of the model of public goods in Bramoulle and Kranton (2007) but our point of departure is the way in which we incorporate the spillover of the benefits of contribution within a network. We propose a non-cooperative simultaneous-move game in which given the network structure within which an individual is embedded she decides how much effort to put towards public good provision when the maximum of the efforts of her own self and her neighbours determine the level of public good she has access to.==== Our model predicts the possible equilibria of the above game and examines their stability and welfare properties. We observe that this slight modification of the model of Bramoulle and Kranton (2007) introduces substantive changes in the conclusions of the analysis.====The main insights from this paper are: First, in general there are multiple equilibria. However, all networks give rise to only specialised equilibrium profiles. Second, all such equilibria are stable. Thus the stability analysis does not help us in improving the predictive power of the model. Third, new links may at times reduce social welfare.====The rest of the paper is organised as follows. Section 2 presents the model, Section 3 discusses the results, Section 4 deals with some extensions of our basic model and Section 5 concludes.",Best-shot network games with continuous action space,https://www.sciencedirect.com/science/article/pii/S1090944319301188,23 July 2019,2019,Research Article,133.0
Gong Yaxian,"School of Economics, Central University of Finance and Economics, Beijing 100081, China","Received 24 March 2019, Accepted 21 July 2019, Available online 22 July 2019, Version of Record 5 September 2019.",https://doi.org/10.1016/j.rie.2019.07.002,Cited by (1)," when goods are substitutes; (iv) the ==== with sufficiently small degree of complements. Moreover, we show that the No Revealing case in which the probability of information disclosure is zero will never show up.","Many articles have discussed about the information disclosure issue in the oligopoly market in which one firm observes the signal about the demand and she can decide the disclosure rule to share this private information to her competitor. However, all the existing literature assume that the firm will commit to some disclosure rule before she has observed the signal (Gal-Or, 1985, Gal-Or, 1985, 1986; Vives, 1984) and they obtain that if the goods are substitutes, (not) to disclose any information is a dominant strategy for each firm in Bertrand (Cournot)competition. If the goods are complements, the results are reversed. However, in some cases, the firm in the oligopoly market can decide her disclosure rule after she has observed some signal, which means that the commitment to the disclosure rule before observing the private information cannot hold. As these cases play their crucial roles in the information disclosure and oligopoly literature but have not been paid enough attention to, this paper attempts to fill this gap. Moreover, contrary to the assumption made by the existing literature that the information quality of the firm is exogenously given, we attempt to endogenize the information quality in this paper and we can find that even without any cost to raise the information quality, the firm will optimally choose some intermediate value of the information quality under certain conditions.====In this paper, we build a model with three dates (t=0,1,2) in which one firm, called as firm 1, has the opportunity to obtain the information concerning with the demand and then decides how to disclose her private information to her competitor in the oligopoly market. Specifically, at t=0, firm 1 can choose her information quality flexibly which is measured by the probability to obtain the information about the demand. At t=1, after firm 1 has observed her private information, she can decide whether to disclose it to her competitor called as firm 2. Then at t=2, these two firms compete in the Cournot or Bertrand framework.====We find several insightful results. Firstly, in the Cournot (Bertrand)competition, when the goods are substitutes (complements) and the degree of substitution (complement) is small enough, the information quality chosen is the maximal one. However, if the degree of substitution (complement) is large enough, the information quality is some intermediate value. This means that even rasing the information quality generates no cost, firm 1 will not choose the maximal value of the information quality. The intuition is that when the firm 2 did not receive any information and the information quality of firm 1 is large, implying that the firm 1 has a greater probability to observe the information but chooses not to disclose it, this will induce the firm 2 to choose the action which is harmful to the firm 1's profits. Then firm 1 will trade off this negative effect of the higher information quality and the benefits driven from the flexibility in disclosing information when the information quality is great to decide her optimal information quality choice. Secondly, we show that the Fully Revealing case where the probability of information disclosure is equal to one will occur in Cournot (Bertrand) competition if the degree of substitution (complements) is small enough and the No Revealing case where the probability of information disclosure is equal to zero will never show up, which is contrary to the conclusion in the previous literature. Thus our framework demonstrates the importance of the timing to observe the information and decide the disclosure rule.",Information quality choice and information disclosure in oligopoly,https://www.sciencedirect.com/science/article/pii/S1090944319301048,22 July 2019,2019,Research Article,134.0
"Lipshits Rachel,Barel-Shaked Sagit,Ben-Zion Uri","Department of Industrial Engineering and Management, ORT Braude College of Engineering, Snunit 51 St., P.O.Box 78, Karmiel 2161002, Israel,Department of Economics, Western Galilee college, P.O. Box 2125, Akko 24121, Israel,Ben-Gurion University of the Negev, Beer Sheva, 8410501, Israel","Received 3 September 2018, Accepted 14 July 2019, Available online 17 July 2019, Version of Record 5 September 2019.",https://doi.org/10.1016/j.rie.2019.07.001,Cited by (1),"We analyze ==== literacy by insights from behavioral economics, while incorporating individual differences in gender, cognitive ability and academic institution. Our sample consists of economic students from two academic institutions in Israel. For statistical analysis, we used Generalized Estimating Equations (GEE). Our main finding is that high-level male students who are prone toward mental accounting have very accurate expectations of inflation, interest rate and unemployment, i.e. they are highly macroeconomic literate. Yet, we found no indication that rational thinkers are more literate than others.","Individuals’ perceptions and expectations concerning inflation, interest rate and unemployment may affect economy-wide outcomes. Better understanding of these perceptions can help economists and central bankers to improve their forecasts. Previous studies show errors and misconceptions in those macro principles (Burke and Manz, 2014). Insights from behavioral economics explain real world behavior that is sometimes consistent with the macroeconomic theoretical framework and in other times, is not. Prospect theory is one such explanation, which refers to cases in which individuals do not maximize expected utility as suggested by rational thinkers (individuals who employ norms and rules of economics, mathematics or statistics theories). Essentially, this theory states that individuals make decisions based on evaluation of losses and gains relative to a specific reference point, rather than calculating expected utility (Kahneman and Tversky, 1979). Loss aversion is one of the underlying assumptions of the prospect theory, and thus was used in this study as indication of this theory. Imagine, for example, you just received 1000 NIS and you have two alternatives: an immediate and certain loss of 100 NIS, or, an even chance, based on the toss of a fair coin, between losing 200 NIS and losing nothing. A rational thinker would choose the first alternative: the expected utility. Choosing the other alternative implies risk taking in losses (loss aversion), neglecting the reference point (1000 NIS), and referring to the dilemma as a choice between two losses (Bar-Hillel and Procaccia, 2011).====Following the vast literature in the field of behavioral economics, the concept of mental accounting is presented by many as a substitute to the standard economy theory of the consumer (the rational thinker). “Mental accounting is the set of cognitive operations used by individuals and households to organize, evaluate, and keep track of financial activities” (Thaler, 1999).====Consider, for example, the two following scenarios: (1) You paid 50 NIS for a theater ticket and lost it. Would you pay another 50 NIS for a new ticket? (2) You lost a 50 NIS bill that was saved for theater ticket purchase. Would you still pay 50 NIS for a ticket? Mental accounting would lead you to avoid paying for another ticket (1), yet you would be willing to purchase a ticket when losing money (2). Unlike losing money, buying a second ticket is perceived as increasing the total cost of the play (Kahneman and Tversky, 1984). This psychological activity surely differs from what we would expect based on an economic theoretical framework.====Our study presents an attempt to analyze macroeconomic literacy using insights from behavioral economics. The last 50 years of research on decision-making and choice offer some conventions such as the one mentioned by Kahneman (2002) in his Nobel Prize Lecture: “…many intelligent people …gave the wrong answer”, or by Stanovich (2016), who pointed out: “Frederick (2005) found that large numbers of highly select university students at MIT, Princeton, and Harvard were cognitive misers…”. Literature shows that highly qualified people, when confronted with decision-making situations, often go against the standard theory. Studies suggest that instead of using economic norms, people tend to rely on thoughts that come automatically and immediately to their mind. These thoughts are called “intuitions” or “heuristics”, and they sometimes contradict concepts in economic theory. Much of this research was carried out by Kahneman and Tversky which is known as the “Heuristics-and-Biases” approach. The Heuristics-and-Biases tasks are considered operational measures of rational thought in cognitive science (Stanovich, 2016). An issue of importance for economic literacy in general, and particularly macroeconomic literacy, is adding this behavioral perspective when discussing the conditions under which people are more literate. In this paper, we offer some ways of implementing these perspectives.====In practice, we examine whether irrational thinkers are as macroeconomic literate as rational thinkers. We use the examples of loss aversion and mental accounting as indicators for irrational thinking. The choice to focus on two decision-making situations is meant to mimic judgments made in routine situations (Kahneman and Frederick, 2005).====We incorporated individual differences in gender, cognitive ability and academic institution into the analysis. For statistical analysis, we used Generalized Estimating Equations (GEE) introduced by Zeger and Liang (1986). The GEE method is often used to analyze possible correlated response data, mainly where responses are binary (Hanley et al., 2003). The GEE model is specified in Section 3.",Empirical study relating macroeconomic literacy and rational thinking,https://www.sciencedirect.com/science/article/pii/S1090944318302825,17 July 2019,2019,Research Article,135.0
Garivaltis Alex,"Northern Illinois University 514 Zulauf Hall, DeKalb IL 60115 United States","Received 3 March 2019, Accepted 7 April 2019, Available online 13 April 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.rie.2019.04.006,Cited by (3),"This paper supplies two possible resolutions of Fortune’s (2000) margin-loan pricing puzzle. Fortune (2000) noted that the margin loan ==== charged by stock brokers are very high in relation to the actual (low) credit risk and the cost of funds. If we live in the Black–Scholes world, the brokers are presumably making arbitrage profits by shorting dynamically precise amounts of their clients’ portfolios.====First, we extend Fortune’s (2000) application of Merton’s (1974) no-arbitrage approach to allow for brokers that can only revise their hedges finitely many times during the term of the loan. We show that extremely small differences in the revision frequency can easily explain the observed variation in margin loan pricing. In fact, ==== serve to explain ==== of the currently observed heterogeneity.==== where ==== is the cost of funds, ==== is the compound-annual growth rate of the S&P 500 index, and ==== is the volatility.","Anyone who has used a significant amount of margin debt is well aware that stock brokers in the United States differ widely in the interest rates they charge their clients on such loans. In addition to this heterogeneity, (Fortune, 2000) noted the puzzling fact that margin rates are very high in relation to the actual (low) credit risk and the cost of funds. For one thing, U.S. law caps the initial loan-to-value ratio on such debt at 50%. For another, U.S. brokerage customers who use margin debt will (in the aggregate) hold very liquid, high-quality collateral (e.g. the market portfolio). Even if market fluctuations cause some accounts to have negative equity, in practice customers will often respond to margin calls by depositing additional funds into the account. The broker’s low risk is underscored by the fact that there is an organized market for funding for such loans. As of this writing (September 2018) the “broker call rate” is ==== a mere 86 basis points above the 5-year Treasury yield.====Not only is the credit risk low, but certainly brokers also have the wherewithal to hedge some or all of it away, albeit at the cost of lower expected profits. In the Black and Scholes (1973) world, assuming that the client’s portfolio follows a geometric Brownian motion, the broker could eliminate risk by shorting a precise, continuously revised amount of the client’s holdings. The cost of delta-hedging would eat into the net interest margin, but the broker would be guaranteed a riskless profit without using any of his own capital. (Fortune, 2000) showed that, even assuming that stocks follow jump diffusions with very high volatility, Merton (1974) no-arbitrage analysis fails to rationalize the observed margin rates.====In the Black–Scholes world, if the broker can continuously monitor the client’s portfolio for solvency, then the no-arbitrage axiom implies that the price of margin debt must equal the cost of funds. Because of the continuous sample paths, there is no default risk, as the broker can liquidate an account the instant its equity equals zero (or some other threshold). For example, Interactive Brokers (which offers the lowest available margin rates) behaves in just this way. Thus, a risk premium ==== > ==== obtains only if the portfolio is unmonitored over some fixed loan term, ====. Outside of major panics, the practical default risk comes from leveraged portfolios that are held overnight or over the weekend. Hence, a reasonable value of ==== should not exceed three calendar days.====Section 2 extends (Fortune, 2000) analysis to model brokers that can only revise their hedges finitely many times over the loan term. Instead of exact, continuous hedging, the broker is now assumed to super-hedge (or super-replicate) his liability on a “filled-in” binomial lattice at ==== discrete points in time. This framework accommodates very general price dynamics, as the gross-return ==== may be distributed over [====] in any manner whatsoever. A super-hedge, as defined by Bensaid et al. (1992), is a trading strategy, together with an initial deposit of money, that guarantees to make no loss for all possible market behavior. The super-hedging cost (or super-hedging price) of a contingent claim is the lowest possible monetary deposit that (together with some special trading strategy) produces cash flows that dominate the derivative payoff. In our particular problem, as ==== increases, the broker is able to lower the rate it charges on margin loans, while still being able to guarantee no loss. As ==== → ∞, the broker’s short position converges to the one specified by the Black–Scholes (Δ-hedging) strategy, and the broker’s margin rate converges to the rate studied by (Fortune, 2000). We show that for a loan term of ==== days, the only distinguishing feature of the lowest-cost broker is that it is able to revise its short position an additional four times. The difference between 15 and 19 revisions is enough to explain all the heterogeneity observed among U.S. investment brokerages.====Section 3 studies the optimal behavior of a broker whose customers are continuous-time Kelly gamblers (Luenberger, 1998). By contrast to the Markowitz (1952) mean-variance theory of investing, a Kelly (1956) gambler eschews the tangency portfolio (of maximum Sharpe ratio) in exchange for an asymptotically dominant trading strategy that has the maximum expected continuously-compounded (read: logarithmic) growth rate. It is well known Breiman (1961) that in the long-run, with probability approaching 1, the Kelly rule outperforms any “essentially different” strategy by an exponential factor. A Kelly gambler who uses leverage will happen to maintain a certain (fixed) loan-to-value ratio at all times, which ratio depends on the quality of the available investment opportunities. Since the Kelly gamblers at a given brokerage will hold asymptotically 100% of the wealth, they will also owe 100% of all margin debt in the limit. Thus, we assume that the Kelly gambler’s broker acts as a monopolist over margin loans in the context of a permanent, infinite-horizon interaction. The broker solves a general stochastic control problem that yields simple and pleasant formulas for the optimal interest rate and the net interest margin.",Two resolutions of the margin loan pricing puzzle,https://www.sciencedirect.com/science/article/pii/S1090944319300663,13 April 2019,2019,Research Article,136.0
"Chen Yanan,Kelly Kyle A.","Department of Economics and Finance, West Chester University, 700 South High Street, West Chester, PA 19383, USA","Received 20 March 2019, Accepted 7 April 2019, Available online 8 April 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.rie.2019.04.005,Cited by (6),"This paper examines the effects of the Great Recession on the gender difference in hourly wage and the rate of return to schooling in the United States. Using data from American Community Survey 2000–2015, we find that the male-female difference in hourly wage declined during and after the recession. The Great Recession decreased earnings for both men and women, especially for those with more education. We also find there is a significant gender difference in the effects of the Great Recession on the returns to schooling. The Great Recession increased the rate of return to schooling for both men and women, and the female-male difference in the returns to schooling decreased by 0.4 percentage points in the post-recession period. The change of the gender difference in the returns to schooling can be explained by the wage structure change for men and women over the recession.","The Great Recession was the most severe downturn in the postwar U.S. economy (Elsby et al., 2010). Fig. 1 plots the monthly unemployment rate for the U.S. The unemployment rate was 5.0% at the start of the recession in December 2007 and peaked at 10.0% in October 2009. One feature of the Great Recession and subsequent recovery that differed with previous recessions was the increase in the duration of unemployment. Fig. 2 plots the monthly median unemployment duration. In December 2007, the median unemployment duration was 8.4 weeks. This sharply increased over the next two and a half years, reaching a peak of 25.2 weeks in June 2010. The duration remains higher today than in pre-recession levels.====This paper explores the effects of the Great Recession on the male-female wage gap. As suggested in Reder (1973) and Oi (1962), unskilled workers should be more subject to cyclical economic conditions than skilled workers. The wage differential between skilled and unskilled workers is expected to be greater during economic expansions and smaller during economic recessions. Historically, men earned more than women. In the long term, previous studies show that the gender wage gap increased in the 1950s and 1960s, and declined after 1970 due to the radically changed in women's education and experience (for example, Blau and Kahn, 1973, Mulligan and Rubinstein, 2008). In the short term, the gender wage gap may vary depending on cyclical economic conditions. For example, some industries with a relative high male-female ratio, such as manufacturing and construction, are more sensitive to the change of the economics conditions and may have greater fluctuations in employment and wages during business cycles, leading to changes in the male-female wage differential.====A second topic we examine is the effects of the Great Recession on the rate of return to schooling between men and women in the United States, and if a difference exists between the two. The rate of return to schooling measures the returns that individuals receive from investing in human capital. It can be empirically estimated as the percentage increase in earnings by one more year of schooling based on Mincer's earnings equation (Mincer, 1974). The economic conditions during a recession, such as local unemployment, may affect returns to schooling through the wage structure. Whether or not individuals fare better during economic downturns depends on the relative change in labor earnings between skilled and unskilled workers. For example, an increase in the unemployment rate may slow the growth of earnings for the individuals with higher education by a greater percentage than for those with fewer years of schooling. If unemployment rate increases during the recessions, the rate of return to schooling would fall, ceteris paribus. Historically, women had a higher rate of return to schooling than men (Polachek, 2008, Dougherty, 2005).====The gender difference in the returns to schooling may also vary during the business cycle. If there is a difference in the effects of economic conditions on returns to schooling between men and women, we would observe a significant change in the gender difference in the returns to schooling before and after the Great Recession.====Several empirical studies examine the rate of return to schooling and economic conditions. For example, Kniesner et al., 1978, Kniesner et al., 1980) use data from NLS 1967 and 1970 and find the relative rate of return to schooling for young whites to young blacks is affected by unemployment. During recessions, blacks fare relatively worse than whites. The findings are consistent in Psacharopoulos et al. (1996), who show the returns to education are positively related to economic conditions in Mexico. The returns are depressed during an economic recession and rise again as economic growth resumes, and remain high even after a significant expansion of the educational system. According to McGuinness, McGinnity and O'Connell (2009), in Ireland, the returns to education for men remain stable and wage inequality fell during the period 1994–2000, while the overall wage inequality fell for women in the period 1997–2001.====More recent studies look specifically at the effects of the Great Recession on earnings. Chen and Kelly (2017a) compare the rate of return to schooling between men and women for Pennsylvania. They find that the rate of return to schooling increased for men both during and after the Great Recession, but decline for women. Chen and Kelly (2017b) look at differences in both wages and the rate of return to schooling between blacks and whites in New York. They find both a positive white-black wage gap as well as a positive white-black earnings gap. In addition, these gaps increased both during and after the Great Recession. Belfield (2015) analyzes returns to schooling for young workers in Arkansas. He finds that returns to schooling increased in the quarters following the Great Recession. Liu et al. (2014) look at cohorts who entered a North Carolina Community college in 2002–03. They find the returns to community college attainment in North Carolina were positive for both men and women and for both associate degrees and bachelor's degrees and were consistent both before and after the Great Recession.====We explore the gender difference in the earnings and returns schooling by estimating a basic Mincer earnings function. Our data is from the American Community Survey (ACS). Our sample contains the information of individuals from 2000–2017 for the entire U.S. It allows us to compare the hourly wage and the rate of return to schooling during and after the Great Recession with what existed prior to the recession. We find that the Great Recession decreased the hourly wage for both men and women. The male-female gap in hourly wage significantly declined in the post-recession period. We also find that the rate of return to schooling increased for both men and women during the recession and recovery period and that a significant gender difference exists. The female-male difference in the returns to schooling is observed to be smaller in the post-Great Recession period.====The remainder of the paper is organized as follows. Section 2 presents the data and empirical strategy. Section 3 examines the effects of the Great Recession on the earnings and the rate of return to schooling for men and women, as well as the male-female difference in the effects of the Great Recession on the earnings and the returns to schooling. In Section 4 we provide possible explanations for the change in the gender difference in hourly wage and returns to schooling during the recession and recovery period. We conclude in Section 5 by summarizing the findings from the paper.",The gender difference in wages and the returns to schooling over the great recession in the U.S.,https://www.sciencedirect.com/science/article/pii/S1090944319300973,8 April 2019,2019,Research Article,137.0
"Brunette Marielle,Jacob Julien","Université de Lorraine, Université de Strasbourg, AgroParisTech, CNRS, INRA, BETA, 54000, Nancy, France,Université de Lorraine, Université de Strasbourg, CNRS, BETA, 54000, Nancy, France","Received 7 March 2019, Accepted 5 April 2019, Available online 6 April 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.rie.2019.04.004,Cited by (10),"We characterize the individual's attitude towards risk, prudence and temperance in the gain and loss domains. We analyze the links between the three features of preferences for a given domain and between domains for each feature of preferences. Consequently, the reflection effect, the mixed risk aversion and the risk apportionment, are key concepts of our study. We also display some determinants for risk aversion, prudence and temperance in each domain. To do this, we conducted a lab experiment with students eliciting risk aversion, prudence and temperance in the two domains, and collected information about each subject's characteristics.","Risk aversion, prudence and temperance are the main drivers of an individual's decisions under risk. Indeed, risk aversion increases insurance demand (Mossin, 1968) as well as prevention level (Ehrlich and Becker, 1972, Dionne and Eeckhoudt, 1985), and leads to a higher degree of diversification for portfolio choices (Bertrand and Prigent, 2010). In the same vein, prudence is a key determinant of most of the risky decisions, in particular, prevention (Eeckhoudt and Gollier, 2005, Dionne and Li, 2011, Krieger and Mayrhofer, 2017). Prudence tends to reduce prevention because prudent agents favor the accumulation of wealth to face risk and are then reluctant to spend money ex ante (Dionne and Li, 2011). Another example is the contribution of Eeckhoudt and Kimball (1992) that shows that decreasing absolute risk aversion and decreasing absolute prudence are necessary to ensure that a background risk will raise the optimal insurance demand against the main risk. Finally, temperance also plays a significant role on prevention. Courbage and Rey (2006) show that in the case of an increase in downside second period background risk, only temperate individuals will increase prevention. However, as indicated by Crainich et al. (2011), while risk aversion and prudence are widely used, temperance is still little known at this time and tends to meet with a certain degree of skepticism.====Eeckhoudt and Schlesinger (2006) proposed to define prudence and temperance with regard to the notion of risk apportionment. In this case, risk aversion is the equivalent of risk apportionment of order 2, prudence the equivalent of risk apportionment of order 3 and temperance corresponds to risk apportionment of order 4. More intuitively, a prudent individual prefers to combine a good event with a bad event and a temperate one prefers to disaggregate harm. These behavioral definitions lead to the emergence of an experimental literature that aims to characterize these individuals’ preferences, mainly in the gain domain. In this literature, the commonly accepted conclusion is that individuals tend to be risk-averse, prudent and temperate, satisfying risk apportionment of orders 2, 3 and 4 (Deck and Schlesinger, 2010, Ebert and Wiesen, 2011, Ebert and Wiesen, 2014, Noussair et al., 2014, Baillon et al., 2017). This type of behavior is characterized as mixed risk aversion by Caballé and Pomansky (1995). The interest for risk lovers appeared with Crainich et al. (2013) who define a mixed risk lover as an individual who is a risk lover, prudent and intemperate. These two types of patterns have been experimentally identified by Deck and Schlesinger (2014) and Haering et al. (2017).====The robustness of risk apportionment has been tested in various experimental settings. For example, some papers deal with the presentation format of the lottery by comparing risk apportionment tasks where choices are presented as compound lotteries with tasks where the choice is presented in a reduced form (Deck and Schlesinger, 2010, Haering et al., 2017). Haering et al. (2017) observed that subjects are generally significantly more prudent and more temperate when the lotteries are displayed in compound form rather than in reduced form.====The robustness of risk apportionment has also been tested with regard to the stake size and geographical locations by Haering et al. (2017). They show that mixed risk aversion characterizes most of the subjects in China, Germany and the USA, and that a non-negligible share of the sample is composed with mixed risk lovers. They also observe that these behaviors are quite insensitive to a tenfold increase in the payoffs of the lotteries.====However, to our knowledge, risk apportionment has been very little tested in the loss domain (see Maier and Rüger, 2012 for an exception), while most of the decisions under risk imply loss. In addition, a general trend is that individuals consider that losses are more painful than gains of equal size are pleasurable (Baler, 2012), so that we can expect different individual behaviors under gain and loss. For risk aversion, Kahneman and Tversky (1979) show that a risk-averse individual in the gain domain should be a risk lover in the loss domain. This reflection effect is a well-known trend in experiments (Kühberger et al., 1999). Consequently, it seems legitimate to wonder if this type of reversal of preferences also characterizes prudence and temperance.====In this paper, we propose to elicit individuals’ risk aversion, prudence and temperance both in the gain and loss domains. We characterize the individuals’ preferences through a lab experiment with students. We implement the recent procedure proposed by Noussair et al. (2014) to elicit risk aversion, prudence and temperance in the gain domain, and we propose an extension of it to the loss domain. We also analyze the links between preferences in a given domain (for instance, risk aversion, prudence and temperance in the loss domain) and between the two domains (gain and loss) for each feature of preferences. Finally, we display some individuals’ determinants for each type of preference both under gain and loss.====Our results show that participants are mostly risk-averse, prudent and temperate in the gain domain (i.e., mixed risk aversion), while they are mostly risk-averse, imprudent and temperate in the loss domain. We also observed that risk aversion in the gain and loss domains was positively and significantly correlated. The same result applies for prudence and temperance. We also showed that behaviors in terms of risk aversion, prudence and temperance were all bilaterally correlated in the gain and loss domains, except for risk aversion and temperance in the gain domain. Finally, we found that the determinants of an individual's preferences are different depending on the domains and features of preferences.====The rest of the paper is organized as follows. Section 2 presents the theoretical background of our experimental approach and Section 3 presents the experimental procedure. Section 4 presents the results, while Section 5 proposes to discuss them in more detail. Finally, Section 6 provides a conclusion.","Risk aversion, prudence and temperance: An experiment in gain and loss",https://www.sciencedirect.com/science/article/pii/S1090944319300778,6 April 2019,2019,Research Article,138.0
"Bernier Maxence,Plouffe Michael","University College London, United Kingdom","Received 28 March 2019, Accepted 2 April 2019, Available online 4 April 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.rie.2019.04.003,Cited by (31),"We leverage a ‘catch-all’ measure of financial innovation—research and development spending in the financial sector—to assess the net relationship between financial innovation and economic growth and evaluate the influence of macroprudential policy on this relationship. Using a panel of 23 countries over the period of 1996–2014, our results demonstrate a net-positive relationship between financial innovation and gross capital formation. We find no evidence of a net-negative impact of financial innovation on economic growth, challenging the popular and political stigma surrounding financial innovation. We also find little robust evidence of macroprudential policy influencing the relationship between financial innovation and economic growth. Our results support a functional approach to the regulation of financial innovation, which improves the intermediation process, leading to increased capital formation.","The use of financial innovation has surged over the past decade (Segoviano et al., 2013). For instance, collaterized debt obligations, a type of financial innovation, grew from $20bn in 2004 to $1.8T in 2007 (Barnett-Hart, 2009). Heralded as the cause of the Great Financial Crisis (2007–2008), financial innovation has became popularly perceived as risk-inducing instruments that with no positive impact on economic growth (Dwyer, 2011).====In response, policy-makers have increasingly relied on macroprudential policy tools to limit systemic risk induced by financial innovation (Calomiris, 2009). Surprisingly, there remains a lack of consensus on both (1) the impact of financial innovation on economic growth, and (2) whether macroprudential policies improve or worsen the economic effects of financial innovation (Bertay et al., 2017, Beck et al., 2016). Financial innovation is a field of substantial growth, high scrutiny, and increasing regulation, yet with no academic agreement on either its macroeconomic effects or the proper policy approach.====This study examines two key questions:====Leveraging panel data of 23 economies between 1996 and 2014, we use a novel proxy for financial innovation - research and development spending in the financial sector (ANBERD) - and a novel index of macroprudential policy (Cerruti et al., 2015). Our results are robust, demonstrating that: ","Financial innovation, economic growth, and the consequences of macroprudential policies",https://www.sciencedirect.com/science/article/pii/S1090944319301097,4 April 2019,2019,Research Article,139.0
Gebauer Markus,"International College of Economics and Finance, Higher School of Economics, Russia","Received 21 January 2019, Revised 5 February 2019, Accepted 2 April 2019, Available online 4 April 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.rie.2019.04.001,Cited by (1),"This paper investigates on a theoretical level the underlying causes of recent trends in decision of firms to hire temporary and permanent labour when workers and firms meet through a frictional directed search technology. Temporary workers differ from permanent workers in that they have a lower bargaining weight but look for a permanent job while on the temporary job. The findings are that permanent arrangements are more prevalent the more productive the aggregate production function is, i.e. also in the less productive phases. More efficient matching has an inverse U shaped impact, it first increases the prevalence of temporary arrangements and then decreases it. Bargaining weights have an ambiguous impact.","Temporary labour agreements have become more and more commonplace in Western labour markets. For instance in Spain the proportion of temporary workers in the entire workforce increased from roughly 15% in the mit 1980s to a peak of 34% in 2006 and a still substantial 24% in 2014. In US the total number of those employed by THS (Temporary Help Services)-agencies more than double from 1.2 million to 2.6 million between 1990 and 2006. Similar development is observed in other industrialized countries (OECD).====The causes of these increases are often seen to be the lifting of regulations pertaining to the allowed use of temporary arrangements. In Spain this story most certainly rings true since the increase in the use of temps clearly coincided with the government lifting the restrictions in 1984 (Guell and Petrongolo, 2007). However, this view may be too euro-centric. In the United States and Japan for instance the regulation concerning such arrangements has traditionally been relatively weak to begin with and there, the increase too came in the 1980’s and 1990’s. While it is true that in the US for example the decline of the at-will doctrine contributed to the rise of temporary employment, authors like Autor (2003) conclude it is not the entire story. It seems, the workings of labour markets and to the demand structure contributed to the rise of temporary work.====This paper is aimed at modelling labour markets with permanent and temporary contracts and identifying sources for an increased use of temporary workers ==== than a regime switch away from prohibited towards legal temporary contracts. To this end it employs a variant of the canonical search and matching Diamond–Mortensen–Pissarides Framework (DMP hereafter) (Pissarides, 2000). Multiple workers firms are made possible by a decreasing returns to scale production function and wages determined by generalised Nash bargaining as in Stole and Zwiebel (1996). The findings are:====This paper builds upon the work on matching models of and Pissarides (2000) as well as Alvarez, Veracierto, 1998, Alvarez, Veracierto, 2000, Alvarez, Veracierto, 2006 and Kaas and Kircher (2015). It is a directed search model in the spirit of Moen (1997). The models that are most closely related to the one in this paper are Elsby and Michaels (2008), Acemoglu and Hawkins (2010) and House and Zhang (2012). The wage determination mechanism is inspired by Stole and Zwiebel (1996).====Section 2 presents the model, Section 3 explores the properties of the steady state and the conditions under which an equilibrium with permanent contracts arises, Section 4 will provide some comparative statics and link them to some of the empirical evidence. Section 5 will conclude and examine possible future avenues of research.",Permanent and temporary workers in a matching framework,https://www.sciencedirect.com/science/article/pii/S1090944319300225,4 April 2019,2019,Research Article,140.0
Migheli Matteo,"Department of Economics and Statistics ""Cognetti de Martiis"", University of Torino, Lungo Dora Siena, 100, 10153 Turin, TO, Italy","Received 6 December 2018, Accepted 2 April 2019, Available online 3 April 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.rie.2019.04.002,Cited by (1),"Several selection processes use multistage tournaments to choose the best candidates. The theoretical models predict that tournaments are efficient in selecting the best candidates, as they stimulate the best to perform relatively better than their opponents. Empirical tests are difficult, as data on the agents involved in these selections are scarce. Exploiting data from field tournaments, the World Swimming Championships, I show that two- and three-stage tournaments are effective for stimulating performance and selecting the best contestants; results indicate that the winners are the players who are most able to increase their relative performance from one stage to the next.","“Rewards” are allocated to workers, students, athletes, political leaders, etc. following comparative evaluations of their performances in the relevant domains. These comparative procedures select the winners of promotions, medals, elections, prizes, scholarships, and admission to prestigious colleges, and are crucial in the formation of leadership in almost all the spheres of social life (politics, business, etc.). In various domains of activity, sequential tournaments are the most commonly used process of selection (Baker et al., 1988, Cable and Murray, 1999, Bognanno, 2001, Gunderson, 2001). In particular, elimination contests are widely used in the labour market, in political competitions, in science contests, for choosing CEOs, and for determining workers’ remuneration (Main et al., 1993, Moldovanu and Sela, 2006, Kale et al., 2009, Zhang and Wang, 2009, Connelly et al., 2014, Waldman, 2013). Job performance is a major variable used in evaluating workers for promotions (Fairburn and Malcomson, 1994, Treble et al., 2001), and the prospect of a promotion is a major incentive for workers to put effort into their job (Nguyen et al., 2015). Some authors (such as Schöttner and Thiele, 2010) have even highlighted that individual performance pay may be less effective in promoting workers’ efforts than promotion-based incentives. However, promotions also “contain” a prize in terms of salary increase (van Herpen et al., 2006), but Waldman (2013) has shown that the two components of the prize (money and glory) cannot be (easily) disentangled.====According to Hvide (2002), and following Holmstrom (1979), “The main theoretical rationale for rewarding relative performance [… is that] an optimal compensation contract conditions rewards on any variable that is (incrementally) informative about work intensity (effort).”==== Indeed, in the real world, tournaments allow the observation of the relative performance of the contestants, in order to select the best; they promote the survival of the fittest and save sampling costs by early elimination of weaker contenders (Rosen, 1986). In particular, firms generally award promotions to employees based on their relative performance in the workplace. Indeed, the maximum effort that an individual is able to exert is generally his or her private information and is hardly observable; winning a competition means overtaking the other contestants, which does not necessarily require the exertion of the maximum effort. Because of this, Lazear and Rosen (1981) noted that relative performances are more informative than absolute results. The extant theoretical and empirical literature on tournaments==== suggests that players’ effort depends on the presence of multiple prizes, on how their value is differentiated, on the degree of homogeneity among the contestants, and on the information disclosed in the intermediate stages of the tournament. In tournaments involving elimination, where prizes are awarded only in the last stage, high performances in intermediate stages are necessary to pass one stage and to compete in the next. On the one hand, since effort is energy consuming, the subjects have an incentive to save their energy for the last stage (when prizes are awarded); on the other hand, if they save too much energy (i.e. do not exert enough effort) in a stage and do not access the last one, all the effort is wasted. According to many models casts some doubts, the contestants should aim just to pass from one stage to the next, and then to use all their remaining energy to win the tournament. In other words, it is unclear how people allocate their energy between the different stages of a tournament. The intermediate results, anyway, represent a signal of the contestants’ ability to both the competitors and the principal; the latter bases the decision on whether to promote an agent to the next stage on this information. Therefore, the question is: How reliable are these intermediate signals in representing the subject's ability? In other words: Are they good predictors of the real ability of the subject (i.e. of the ability to win the competition in the last stage)? The answer to this question is important in terms of the efficiency of sequential tournaments in selecting the best competitors. Indeed, if this scheme were not able to incentivise the best to make enough effort to pass the intermediate stages, it would fail in individuating and promoting the best. As Connelly et al. (2014) highlighted, thirty years after the first seminal study on tournaments as incentives for workers, the question whether they are effective or not is still open. My paper aims to help answer this question, by examining the data from a real sports tournament in two and three stages. The results show that tournaments may not always be effective to select the best.",Competing for promotion: Are “THE BEST” always the best?,https://www.sciencedirect.com/science/article/pii/S1090944318304058,3 April 2019,2019,Research Article,141.0
"Salari Mahmoud,Javid Roxana J.","Economics Department, Loyola Marymount University, Los Angeles, CA 90045, USA,Department of Economics, California State University Long Beach, Long Beach, CA 90840, USA,Department of Engineering Technology, Savannah State University, Savannah, GA 31404, USA","Received 31 January 2019, Accepted 28 March 2019, Available online 28 March 2019, Version of Record 17 May 2019.",https://doi.org/10.1016/j.rie.2019.03.002,Cited by (3),Female labor force participation (FLFP) and household ,"Economic outcomes have experienced substantial changes over the past decades in many countries. Two notable changes including Female Labor Force Participation (FLFP) and household wealth in most countries, specifically in the U.S. FLFP is the main factor in the production function and economic outcome (Cavalcanti and Tavares, 2008, Cubas, 2016), while housing values show wealth of households (Lusardi and Mitchell, 2007, Skinner, 1989). Most homeowners believe their house could be the most important consumption good and asset at the same time in their portfolio (Flavin and Yamashita, 2002). Also, housing wealth could be the predominant non-pension investment vehicle for saving the money (Begley and Chan, 2018). Thus, understanding the main variables that impact on FLFP and housing values are important for most economists and policymakers. FLFP and housing prices change over past decades simultaneously as seen in Fig. 1. This figure shows the relationship between FLFP and Housing Price Index (HPI) for the U.S. states in 2017. This trend shows positive correlation exists between FLFP and housing prices, which indicates housing values relate to FLFP. Thus, increasing women decisions to work may resulted to increase housing prices as well.====This study investigates to the direction of causation between housing prices and FLFP using the state-level data in the U.S. There are three main possible scenarios for this observation. First scenario, high house prices encourage women to work and contribute their income regarding their households’ expenditures, so more women tend to work to have their own desire houses, so housing prices affect on FLFP. Second scenario, those families who have two earners could pay more to their houses, so this resulted to bid up the price of desire houses which raising the relative price of housing. This scenario indicates that increasing FLFP impacts positively on housing prices. Third scenario, there is an omitted variable which has correlation with both housing price and FLFP and there is no causation exist between housing market and labor market (Johnson, 2014). To overcome to omitted variable issue, this study suggests to run different specific estimation models to validate the results.====Finding the relationship between FLFP and housing values based on the first theory (scenario) has been considered by several studies. Generally, studies that employ first theory, housing values affect on women decisions to work, suggest that policy makers can estimate women's decisions to work due to their housing values. Holtz-Eakin et al. (1993) examine the effect of receiving inheritance on the labor force participation behavior in the U.S. They conclude that large inheritances decrease a person's labor force participation and the likelihood of an individual decreases his/her labor force participation depends on the size of the received inheritance (Douglas Holtz-Eakin, 1993). Henley (2004) investigates an individual's hours of work resulted from amount of housing wealth capital gain using British panel data. He concludes that housing gains reduce working hours for both men and women in response asymmetric whereas men appear to increase hours in response to housing losses, while women reduce hours in response to housing gains (Henley, 2004). Black et al. (2014) indicate that FLFP of married women are negatively affected by commuting time, so metropolitan areas with larger community time had experienced slower growth in FLFP (Black et al., 2014). Fu et al. (2016) employs China household survey data to estimate the effect of changes in housing values on labor force participation. They show that a 100,000 yuan increase in housing values results to a 1.37 percent decrease in FLFP (Fu et al., 2016). Additionally, Zhao and Burge (2017) investigate the relationship between housing values, property taxes, and labor supply. They show changes in housing values impact on labor supply as well as changes in financial assets and work in the same direction (Zhao and Burge, 2017).====Second theory suggests more women into labor market resulted raising household incomes and setting off a bidding war for their housings that causes increasing housing prices. Fortin (1995) employs empirical evidence to shows mortgage commitments impact on the decisions of married women regarding the job. She concluded that if outstanding mortgage amount of the average household decreases, the FLFP would be decrease respectively (Nicole M. Fortin, 1995). Green and Hendershott (2001) find a positive relationship between a country's unemployment rate and the rate of home-ownership (Bloze and Skak, 2016, Green and Hendershott, 2001). Haurin et al. (1996) show the wealth accumulation of American youth related to their housing choices and indicate women working hours are positively related to houses prices (Haurin et al., 1996). Jacob and Ludwig (2012) employs data from a randomized housing voucher in Chicago to examine the effect of housing assistance on labor supply. They show that housing vouchers reduce labor force participation and they find no evidence of housing-specific mechanism to promote work (Jacob and Ludwig, 2012). Johnson (2014) indicates that house prices are unlikely to raise FLFP, while it may affect on earnings. Likewise, an instrument for married women's labor supply reveals no casual effect of two earner households on housing values (Johnson, 2014). The large numbers of female who motivated to work within a concentrated geographic area provide more positive growth in externalities, which may cause increase house values. Households whose both husband and wife have a college education have been increasing and they tend to locate in larger metropolitan area to find their proper jobs (Costa and Kahn, 2000).====Housing is the main component of household's wealth and long-term investment in most countries, particularly in the U.S. Moreover, those households whose women work may have more opportunity to buy more expensive houses. In the United States, homeowners have equity more than 57 percent of the total value of household real estate (“Board of Governors of the Federal Reserve System (US), Households; Owners’ Equity in Real Estate as a Percentage of Household Real Estate, Level [HOEREPHRE], retrieved from FRED,” 2018). United States has been experiencing an extensive housing price appreciation recently. The FLFP and housing values differ from state to state because every state has different characteristics, socio-demographics, and regulations. Furthermore, many states have a higher population and income rather than some countries. Thus, understanding the main factors that impact on FLFP and housing values and find the relationship between these variables are important for policy makers.====To make clarification about this relationship between FLFP and housing values, this study uses FLFP and Housing Price Index (HPI) data and shows the trend in Fig. 2. This figure shows how FLFP and HPI move from 1976 to 2017. Fig. 2 indicates that FLFP and HPI move together while FLFP is superior in the direction. Remarkably, this pattern shows that FLFP may impact on HPI since it moves superior compared to the HPI. Thus, this study suggests that using FLFP as an explanatory variable to estimate HPI rather than dependent variable since FLFP may affect on HPI.====This study differs from previous from two main aspects. First, this study uses macro level data (state-level) rather than micro level data (individual) to estimate the effect of FLFP on housing values in the U.S. This is the first study using macro level data in the U.S. to estimate the effect of FLFP on housing values at the state-level. Second, this study uses static and dynamic estimation models and various robustness tests to address possible measurement errors including causation issues.====The rest of the paper is organized as follows: Section 2 provides datasets; Section 3 describes methodology approaches for both static and dynamic estimation models; Section 4 shows the empirical results; Section 5 presents the conclusion.",How does female labor force participation impact on housing values?,https://www.sciencedirect.com/science/article/pii/S1090944319300353,28 March 2019,2019,Research Article,142.0
Liu Kerry,"The China Studies Center, University of Sydney, Australia","Received 30 January 2019, Accepted 21 February 2019, Available online 21 February 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.02.002,Cited by (14),"Based on quarterly data between Q4, 2008–Q4, 2017, this study examines the interest rates pass-through from policy rate to lending rates, and more broadly, the determinants of lending rates in China. The results show that while there is a certain level of pass-through from money market rates to lending rates, and the interest rates pass-through has improved after the ==== liberalization completion on October 2015, this pass-through is also negatively affected by the asset quality of commercial banks and shadow banking activities as well. This study also finds that the macroeconomic condition also affects the lending rates.","Starting from April 2018, the People's Bank of China (China's central bank, PBC) has performed 5 reserve requirements ratio cuts as of 31 January 2019. Together with other unconventional monetary policy tools such as the targeted Midterm Lending Facility operations, the PBC has been trying to provide enough liquidities to the financial system hoping that these liquidities will finally be supportive of the real economy. However, the monetary transmission has been a big challenge. As stated during the second meeting of the Financial Stability and Development Committee under the State Council (China's cabinet) held in August 2018, the establishment of a more effective monetary transmission system has become a key task (Xinhua News, 2018). The Head of Monetary Policy Department at the PBC also admitted that China's monetary transmission is not very effective (Sun, 2019).====This study will address this issue by specifically focusing on the interest rate pass-through channel. The structure of this paper is as follows. Section 2 reviews relevant literatures. Section 3 introduces the data sets. Section 4 performs the empirical analysis. Section 5 concludes this paper.",The determinants of China's lending rates and interest rates pass-through: A cointegration analysis,https://www.sciencedirect.com/science/article/pii/S1090944319300328,March 2019,2019,Research Article,143.0
Dotti Valerio,"Department of Economics, Washington University, St. Louis, One Brookings Drive, Campus Box 1208, MO 63130, United States","Received 5 February 2019, Accepted 5 February 2019, Available online 6 February 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.02.001,Cited by (4),I study the relationship between income ,"What is the effect of an exogenous increase in income inequality on the level of public intervention in public education in a democratic country? Does such effect mitigate income inequality of future generations? This paper attempts to provide a theoretical framework to answer these questions. The relationship between the degree of governmental intervention in the provision of good and services and the features of the population in democratic political systems has been a major topic of research in Political Economy. Traditional models typically imply a positive relationship between the size of the intervention and income inequality (Meltzer and Richard, 1981). The reason is that such policies tend to have redistributive effects====, thus an increase in the public provision favors the relatively low income part of the voting population. This has important consequence in a voting model because of two factors. First, an increase in income inequality is associated with an increase in the share and the political power of the relatively low income voters. Second, traditional models do not allow voters to access to other redistributive policies such as lump-sum grants because of technical constraints. Thus, unsurprisingly, such models usually imply a positive relationship between income inequality and the size of any kind of policy with redistributive effects. Empirical evidence suggests that this relationship may hold true only for certain kinds of policies, for instance public education, but it may not hold true for other policies with redistributive effects such as social security and public health. In this paper I attempt to disentangle voters’ preferences for redistribution from their demand for public education by allowing them to choose both the size of in-cash redistribution -through a flexible tax system- and the quality of public education. This implies that the policy space is multidimensional. In this setting, the specific features of the public provision of education play an important role in determining the relationship between the size of public intervention and the degree of income inequality. Specifically, the presence of private alternatives to public education and the possibility of ==== of the public sector are crucial in shaping the results. Because of these reasons, both a multidimensional policy space and the possibility of ==== are essential features of this analysis. Unfortunately, both such modeling choices are source of well-know problems of existence of a voting equilibrium in the traditional deterministic Downsian framework. Thus, I adopt a Probabilistic Voting framework that allows one to tackle both issues, and I use it to study voters’ behavior in a model of parental investment in education. I find that public intervention in education may be affected by income inequality not because of its redistributive effects, but because of the peculiar way in which the provision is delivered. I derive analytical conditions for a positive relationship between income inequality and quality of the publicly provided education. I find that the sign of this relationship is positive if the expected marginal returns to public education are decreasing in parental income. This is consistent with recent empirical evidence, and can be due to credit constraints that induce relative low income parents to underinvest in their children. Moreover, I show that if this condition is met, then an increase in the quality of public education reduces income inequality in the next generation.====The paper is structured as follows. In Section 2, I describe the findings of the empirical literature about the relationship between public provision of education and income inequality and how the theoretical literature has tackled this question. In Section 3, I present the voting model and the methodology I propose to study the sign of the relationship between the equilibrium level of public provision of a good of interest and the degree of income inequality in the population of voters. In Section 4, I apply these results to a model of parental investment in education in order to provide an answer to the main question of the paper. In Section 5, I compare the predictions in Section 4 with the one that the same framework would deliver for other kinds of publicly provided goods such as pure public goods and health insurance. Section 6 concludes highlighting the achievements and the limitations of this analysis.",The political economy of public education,https://www.sciencedirect.com/science/article/pii/S1090944319300419,March 2019,2019,Research Article,144.0
Park Hyeon,"Department of Economics and Finance, Manhattan College, 4513 Manhattan College Pkwy, Riverdale, NY 10471, United States","Received 15 January 2018, Revised 14 November 2018, Accepted 26 January 2019, Available online 30 January 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.01.004,Cited by (0),"I develop an intertemporal choice model for rational deviators whose preferences depend not only on their actual consumption but also on comparison to their beliefs about the ====. The standard decision maker is loss averse with respect to this belief-dependent reference point. When psychologically weighted loss aversion is low, a decision maker deviates from the standard intertemporal choice behavior and over-consumption, as well as the alternative possibility of under-consumption can be rationalized. When the decision maker has time-varying degrees of loss aversion, he re-optimizes the consumption plan through adjusted beliefs as subsequent selves realize that past decision for the present period is no longer optimal. In the dynamic model, I solve for consistent intertemporal optimization rules by which a dynamic deviator should meet rational intertemporal consistency at each point in time. Finally, I demonstrate that the dynamic reference dependent model can solve a puzzling feature in lifecycle consumption data.","In this paper, I develop a dynamic model for individuals who exhibit reference-dependent preference and loss aversion following the framework of Kőszegi and Rabin (Kőszegi, Rabin, 2006, Kőszegi, Rabin, 2009). Reference dependence of utility has been widely confirmed in lab experiments: it helps understand why standard economic theory cannot explain such findings as the failure of the independence axiom in expected utility. As Rabin (2000) demonstrates, reference dependence is an important factor in explaining people’s attitude toward risk. It also explains a variety of field data (====: Humphreys and Zhou (2015); ==== Herweg and Schmidt (2014); ====: Bernasconi et al. (2014); ====: Farber (2008), Fehr and Goette (2007); ====: Genesove and Mayer (2001); ====: Barberis et al. (2006), Karlsson et al. (2009); ====: Sydnor (2010); ==== Gunther and Maier (2014)).====The choice of utility for a reference point results from the recent development in reference-dependent preference models. A key issue in these models is what determines the reference point of a model, because as Pesendorfer (2006) expressed, the reference point can be anything and may be arbitrarily selected by researchers. Quite often, the reference point is assumed to be the current status, such as current consumption, position, or endowment. Providing one way to solve this problem, Kőszegi and Rabin (2006) propose a model of reference-dependent preferences where the reference point is the individual’s rational expectation formed in the recent past about the relevant outcomes. Kőszegi and Rabin provide a solution concept, by which the reference point is ==== determined as a function of the decision maker’s beliefs on the available strategies combined with his planned action for each strategy. In another work, Kőszegi and Rabin (2009) propose a dynamic reference-dependent model where they specifically introduce the equilibrium concept by which the decision maker should meet rational consistency: the decision maker chooses the plan for state-contingent behavior that is consistent in each period with his future behavior, and that maximizes his expected reference dependent utility going forward.====Based on this solution concept, I develop an intertemporal choice model with reference-dependent preference. I specifically examine the model’s dynamic implication for an individual who is in a specific intertemporal wealth position with respect to lifecycle income and saving/debt position. Given any information about the future income stream, the decision maker forms rational beliefs on what should be the ex ante optimal consumption among all the available consumption-saving strategies over time. It depends on its consistency whether or not this plan is actually from the expectation that the individual is sure to follow ex post. This is because the decision maker only forms expectations that he will be fulfilling ex post. In other words, a decision maker’s intertemporal choice must be credible.====A reference-dependent decision maker derives utility from comparison to the reference status, i.e. the utility from the ex ante optimal consumption: it may be a gain to the reference point or a loss to it. A loss is assumed to be more important to the decision maker than a gain of the same magnitude, a property known as ====. In a two-period consumption-savings model, the decision maker feels a “contemporaneous gain” utility if his current consumption is more than the ex ante optimal solution. As a result, his consumption will be lowered next period and this yields a “prospective loss” utility relative to the reference point. If the contemporaneous gain utility is greater than the prospective loss utility, he then chooses not to follow the ex ante optimal committed plan but to deviate for more consumption. However, if the prospective loss utility is greater than the contemporaneous gain utility, then he sticks to the ex ante optimal consumption. The decision maker’s greater concern for the loss (high loss aversion) deters him from over-consuming.====This analysis can be applied the other way around: another decision maker may have a “contemporaneous loss” utility if she consumes less and saves more than the ex ante optimal solution. As a result, her consumption is elevated for the next period and this gives her a “prospective gain” utility. If her contemporaneous loss feeling is greater than the prospective gain utility, then she does not reduce her consumption but follows the committed plan. If, however, her prospective gain utility is more than the contemporaneous loss utility, then she is willing to reduce her consumption and save more. In this case, the high loss aversion of the decision maker deters her from under-consuming. By both cases, it is clear that the weight on prospective/contemporaneous loss utility relative to gain is crucial. When the decision maker’s loss aversion is high, the agent does not deviate from the ex ante optimal solution, which is equivalent to the outcome of the standard model. However, if the decision maker cares more about the gain due to low loss aversion, then deviation is desirable. The alternative consumption plans which fulfill the agent’s ==== of either over-consumption or under-consumption must satisfy the consistency condition so that the ==== consumption should maximize the ex post utility among all the feasible and consistent strategies to consider.====Constructing a multi-period dynamic model of intertemporal choice with this preference involves how to construct the multi-period reference points from overlapping layers of belief formation, as well as how to define an economic model that conveys the idea of “intended consumption” described above. These issues occur because the framework of the model is absolutely dependent on description, and admits many different formats for a model. To solve the first issue, I restrict the dimension of commodity space to one so that the agent of an economy has only the consumption space from which both the consumption utility and the gain–loss utility are derived. For the second issue, I focus mainly on the consistent consumption plans of two typical types of decision makers, assuming the agent is either an over-consumer or a natural born saver, but not both, if the agent’s loss aversion is low. The agent in the model is assumed to keep one’s natural type of spending, although one can change the degree of loss aversion over time within the same type of spending.====Next I construct intertemporal models of decision making, first in two periods, then three periods, and finally ==== periods, based on the approach of developing the consistent consumption strategy for any committed plans. Unlike in the two-period model, in three or more period models, a decision maker can have several alternative intended plans. For example, a deviator intends to consume more than the ex ante optimal for the first two consecutive periods, and as a result has to accept a very low consumption amount for the last period. Alternatively, he may want to consume more only in the first period. The two intended plans yield two different solutions, and the agent should select the most desirable strategy between them. In a later section, I compute the utility for the best consumption plan that a rational deviator employs because it gives him the highest utility among all the available consistent consumption profiles. In fact, it turns out that a plan that keeps over- or under-consumption up to the middle point of the planning horizon gives the highest utility for a deviator (Section 2). This property implies that the consumption smoothing rule is beneficial even to the deviators.====Based on this preliminary work, I analyze the model of a dynamic decision maker for the case where the decision maker may or may not change his mind over time. When the agent’s preference (loss aversion, in particular) stays constant, then because his expectation is met, there are no gain–loss utilities in the subsequent periods. However, if the agent’s preference does change, then depending on how the new reference points are formed, there are different types of gain–loss utilities following his intended plan in subsequent periods. I provide optimality conditions, as well as closed form solutions, for each of the alternative paths that the decision maker may choose.====Lastly I propose a self-corrective ==== (SPRP) in the dynamic model and explore the choice dynamics when the decision maker changes his mind over time due to time-varying degrees of loss aversion. The decision maker rebalances his consumption based on the adjusted belief regarding the ex ante optimal consumption through the SPRP for his remaining life. Because the SPRP is constructed based on current wealth position, whenever the decision maker changes his mind, he solves a new maximization problem relative to the updated reference point. The resulting consumption profile reflects his current financial situation. In this paper I demonstrate that age-related loss aversion which varies over time can solve a puzzling feature in lifecycle consumption data.",Inter-temporal choices with temporal reference dependence,https://www.sciencedirect.com/science/article/pii/S1090944318300127,March 2019,2019,Research Article,145.0
"Milovanska-Farrington Stefani,Farrington Stephen","The University of Tampa, Department of Economics, 401 W Kennedy Blvd, Tampa, FL 33606, United States,University of South Florida, Department of Finance, 4202 E Fowler Ave, Tampa, FL 33620, United States","Received 20 January 2019, Accepted 26 January 2019, Available online 30 January 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.01.006,Cited by (0),"The high cost of obstetric care is a common reason for late or no detection of preventable preterm labor, serious complications, long-term disabilities, and neonatal and maternal death. To resolve the debate of whether affordable, high-quality healthcare can be used as a policy tool to incentivize timely and more frequent checkups during and after pregnancy, we examine the causal effect of the Obstetric Care State Certificate (OSCS) program, which fully covered the costs of all antenatal services, consultations, medical exams, delivery and postnatal care in Armenia after 2008, on utilization of prenatal and postnatal care in the country. Evidence suggests that the reform had a significant, positive effect on care utilization during and after pregnancy, with the largest effects being elicited in the subsamples of women at high risk pregnancies, and mothers who have had a miscarriage or an abortion.","Antenatal and postnatal care are important for maintaining good health, identifying, preventing and reducing the risks of complications or decline of the health of the baby and the mother. Adequate antenatal care diminishes the risk of low birth weight 3 times, increases the chances of survival of the infant 5 times, and lowers the risk of pregnancy complications, stillbirth, preterm labor and the complications associated with preterm labor, the latter of which is the culprit for the death of nearly one million newborns every year (Office of Women's Health 2018, Litch et al., 2016). That is why the World Health Organization recommends that mothers receive at least 8 prenatal visits. Statistics, however, show that more than 36% of pregnant women receive fewer than 4 antenatal checkups globally.====Insufficient or absent postnatal care is also common and potentially hazardous. In 2013 alone, 2.8 million infants died in the first month of life (WHO, 2015). Some of these cases could have been prevented with timely and ample medical intervention. Its lack often leads to long-term disability of the mother or the child, or poor development of the newborn (Warren et al., 2006).====Despite the statistics and the benefits of both prenatal and postnatal care which tend to be well-documented in the medical literature, it is often neglected due to consumer ignorance, lack of outreach, difficulty in identifying complications, or cost of care. The latter is the subject of this article. The potential adverse consequences of insufficient care during and after pregnancy demonstrate the importance of examining whether affordability promotes more adequate birthing care. To study this question we investigate the effect of the introduction of the Obstetric Care State Certificate (OCSC) program which fully covered the costs of all birthing (i.e., antenatal and postnatal) services, treatment of complications during pregnancy, delivery, consultations with specialists, medical procedures, checkups and laboratory tests in Armenia after 2008 on utilization of these aforementioned health services.====The empirical evidence of the effect of programs promoting cost-effective and quality health care on health outcomes and utilization is inconclusive. Previous studies suggest either a positive, or no significant impact, and are based primarily on data from the US. We contribute to the literature by examining how the OCSC program affected birthing services, including prenatal care, delivery and postnatal care. We also consider the sensitivity of our results to control for mothers' type by risk group, presence of previous abortions and miscarriages. This study is innovative in that it examines the case of a country which has not been the subject of research on medical obstetric care utilization and health outcomes, and evaluates the impact of a reform which has not been previously studied. This research also focuses on pregnancy and childbirth which is not as commonly investigated in the literature as other health related procedures and outcomes.====The results suggest a significant, beneficial effect of more affordable and high-quality care on the number of prenatal visits, the incidence of visiting a healthcare facility for an antenatal medical exam and the likelihood of undergoing a professional checkup after birth. Mothers at high risk due to more advanced age were the main driver of all positive results although all mothers regardless of risk group were positively affected. In addition, the reform had no statistically significant effect on routine procedures such as blood and urine testing during pregnancy. It also did not affect the incidence of a C-section which is associated with a higher risk of maternal mortality and morbidity as compared with vaginal delivery (Zandvakili et al., 2017).====From a policy perspective, our findings imply that programs which reduce the cost of care while enhancing its quality are likely to promote higher demand for care of pregnant women and their newborns. As recommended by the World Health Organization, this is essential for early detection, treatment and prevention of complications during pregnancy and serious neonatal and maternal conditions after birth. Therefore our study provides a step towards the evaluation of the effect of programs similar to the OCSC system and the proposal of policies whose aim is to improve maternal and infant health.====The remainder of this paper is structured as follows. Section 2 briefly reviews the most relevant aspects of the literature and summarizes the nature of the OCSC program. We specify the model in Section 3, discuss the data and present summary statistics in Section 4, and provide results in Section 5. In Section 6, we discuss the policy implications of the study. Section 7 concludes the paper.",The effect of the cost of obstetric care on antenatal and postnatal healthcare utilization: Evidence from Armenia,https://www.sciencedirect.com/science/article/pii/S1090944319300195,March 2019,2019,Research Article,146.0
"Cabral René,Castellanos-Sosa Francisco A.","EGADE Business School, Tecnológico de Monterrey, Ave. Rufino Tamayo, San Pedro Garza García, NL C.P. 66269, México,Department of Economics, University of Texas at Austin, 2225 Speedway, Austin, TX 78712, United States","Received 10 December 2018, Accepted 25 January 2019, Available online 30 January 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.01.003,Cited by (21),"This paper examines the effects of the 2008 financial crisis on economic growth and convergence across European countries from 1973 to 2012. Employing cross-sectional and dynamic panel data techniques, the results show that the global financial crisis has brought a greater absolute convergence rate rather than divergence, affected richer members more heavily and, presumably, allowed less developed members to recover more quickly. We find evidence that creating the European Union has contributed toward economic growth and convergence; meanwhile, no similar evidence is found concerning the European Monetary Union. Moreover, we present evidence that both the average output per capita and the rate of convergence during the financial crisis fell around 7%.","Ten years ago, the global financial crisis triggered the deepest recession in the EU's history. Five years after the event, by the end of 2012, the size of the EU's GDP was still smaller than that registered at the end of 2007. There is no doubt that the recession deeply affected the EU's members, but did so differently, depending on their level of development and the extent of their economic integration. Across the ten newest EU's member states, the average GDP 10 newest EU member states, average GDP growth rate fell 5.68%, from 5.86% to 0.18%, in the five post-crisis years (2008–2012) relative to the five preceding years (2003–2007).==== Meanwhile, the 15 original members’ average growth rate fell 3.38%, from 2.83% to −0.55%. For the same periods, the 17 members of the Economic and Monetary Union (EMU) observed a fall of 3.9%, from 3.54% to −0.367%; while non-EMU members had a fall of 5.14%, from 5.12% to −0.024%.==== Following these dissimilar impacts on economic growth, this paper asks: How has the recent financial crisis affected income convergence in the EU?====While the five convergence criteria established by the EU (price stability, long-term interest rate, exchange rate stability, and public deficits) do not address, for obvious reasons, income convergence, this is a desirable and expected outcome from taking part in the union for any of its members, especially the less well-off. Indeed, convergence plays a central role in the EU integration debate. Undeniably, new members expect to gain from accession by boosting economic growth and converging toward EU income levels in the long-run.====Fig. 1 shows one of the most basic notions of convergence. It plots the average GDP per capita growth rate, measured as log difference, against the log of the initial GDP per capita for the 15 former EU members under different time periods. On panel (a) for the full sample period 1973–2012, we observe a positive slope in the linearly fitted regression, which suggests income divergence. On panels (b) to (e), we split the sample into different sub-periods (1973–1989, 1990–1998, 1999–2007, and 1999–2012). In nearly all cases, except for the 1990s, the positive slopes in the regression joining the scattered dots indicate that GDP per capita was diverging rather than converging across the EU. Intuitively, those countries observing the higher (lower) GDP levels also experience larger (smaller) GDP per capita growth rate.====By the 1990s, a mild convergence pattern could be seen with a slightly negative, nearly flat line in panel (c). However, this trend changes again in the most recent years of the sample, particularly following the financial crisis. Panels (d) and (e) compare the periods 1999–2007 and 1999–2012. In both sample periods, we observe a positive slope (i.e., income divergence), particularly in panel (e), when we include the post-crisis period in the analysis. In line with what could be expected, the divergence pattern rose significantly across the EU's 15 oldest members following the financial crisis. In contrast, analyzing the 10 newest EU members, we observe a convergence pattern, which is reinforced (i.e., a steeper slope) after the financial crisis. Panels (d’) and (e’) in Fig. 2 show how strong the convergence is among these countries. Following the financial crisis, these countries seem to be accelerating their income convergence, which is presumably due to their links with other EU members.====The empirical literature so far suggests that a convergence process seems to be taking place in the EU in later years (see, for instance, Crespo-Cuaresma et al., 2008, and Monfort, 2008). This convergence process is observed in panels (d’’) and (e’’) in Fig. 3 when analyzing all 25 countries together. As we have previously described, this positive assesment seems to be mainly driven by the recently incorporated members of the EU. In fact, panel (e’’) shows how the convergence pattern is slightly more pronounced when the post-crisis period is incorporated.====Existing empirical evidence is, however, not free from controversy and seems to be dependent on factors such as the period under investigation, the number of countries considered in the analysis, the size of the regions or territories observed, and econometric problems such as heterogeneity and endogeneity. There is empirical evidence of convergence before the 1980s (see, for instance, Armstrong, 1995, and Fagerberg and Verspagen, 1996), divergence in the 1980s (see, for instance, Bianco et al., 1997, and Button and Pentecost, 1995), and convergence again since the 1990s (see, for instance, Cavenaile and Dubois, 2011, and Eckey et al., 2009). In this same vein, Barry (2003) identifies three eras with different patterns of convergence for cohesion countries. First, a period of convergence from 1960 to 1973 in which Greece, Spain, Portugal, and Ireland were converging to the living standards of EU-15. A second period, from 1974 to 1986, was characterized by divergence and, third, a post-1986 period in which convergence was resumed. In all the studies listed above, the number of members considered also differs depending on the period in which the assessment was conducted.====The size of the regions under study seems to be another controversial issue. Petrakos and Artelaris (2009) suggest that when regions are appropriately weighted by their size, evidence of divergence rather than convergence is observed across the period 1990–2000. There is also disagreement about the proper territorial size at which researchers should be examining convergence. Boldrin et al. (2001) suggest that while the analysis of NUTS 2 and 3 has been the norm, many NUTS 2 and nearly all NUTS 3 regions are neither large enough nor reasonably heterogeneous in their endowment of factors to be treated as independent economic areas. Similar critics for NUTS 2 and 3 desegregation are found in Davies and Hallet (2002). They suggest that “in statistical terms, levels of disparities almost invariably increase at more disaggregated levels. It is thus far from evident which spatial level is the most appropriate for analyzing real convergence” (page 10).====As first pointed out by Caselli et al. (1996), many economic growth studies are affected by the presence of endogenous regressors and heterogeneity problems that are not correctly handled. These problems have also been observed in the literature about convergence in Europe. Examining the impact of European structural funds, Dall'erba and Le Gallo (2008) give an account of the theoretical arguments to consider explanatory variables as endogenous and of the problems encountered when dealing with such inconsistency. In terms of heterogeneity, Battisti and Di Vaio (2008) suggest that an absolute convergence test is plausible when the objective of the study is within-country because “regions share a common steady-state, being affected by similar saving rates, preferences, governmental policies, property rights, infrastructures, and so on”; however, that is unrealistic while testing between-convergence because countries might show different steady states. They suggest that empirical models that do not account for heterogeneity and spatial effects may present severe misspecification problems.====This paper revisits the convergence debate in Europe, paying special attention to the effects of the recent financial crisis of 2008. Employing a sample comprising 25 former European members over the period 1973–2012, we find that rather than leading to divergence, the financial crisis reduced the convergence rate across the EU. We argue that while the financial crisis was undoubtedly harmful to growth in Europe, it reduced the gap in output per capita of the EU's members. This is explained mainly by the richer members —which are principally the 15 oldest EU members— who were not just the most affected but also the ones that recovered less rapidly. Our results show that the recent incorporation of countries to the EU seems to have contributed toward absolute and conditional convergence across all its members. We also find no evidence that the EMU has contributed toward convergence or divergence. In this sense, our findings are akin to the differentiated effect proposed by Baumol (1986) and Borsi and Metiu (2015).====This paper contributes to the literature in at least three important ways. First, as far as we know, this is one of the few studies that have tried to look at the effects of the recent financial crisis on income convergence by contrasting the effects of the 15 oldest EU members and the newest 10 members (EU 25). A recent paper by Archibugi and Filippetti (2011) studying the innovative performance of EU members suggests that despite the convergence on innovative potential they observe over the period 2004–2008, disparities and divergence in innovative capabilities are expected following the financial crisis, and that “these might lead to divergence also in income and well-being.” We therefore assess the effects of the financial crisis from a broader perspective. Second, our paper revisits the convergence debate in the EU taking into account some of the econometric weakness encountered in previous studies such as heterogeneity and endogeneity problems. Finally, by looking at the national level (NUTS 0), rather than small regional territories, we are able to follow the debate at the more macro level at which the financial crisis has been examined since its occurrence.====The rest of this paper proceeds as follows. Section 2 describes the dataset and presents some descriptive statistics of the main variables used in the paper. Section 3 introduces the empirical models employed to test for absolute and conditional convergence. Section 4 discusses the findings from both the cross-section and panel data estimations. Finally, Section 5 provides some concluding remarks and future research considerations.",Europe's income convergence and the latest global financial crisis,https://www.sciencedirect.com/science/article/pii/S1090944318304125,March 2019,2019,Research Article,147.0
"Kawamura Tetsuya,Ogawa Kazuhito","Department of Management, Japan University of Economics, 24-5 Sakuragaoka, Shibuya-ku, Tokyo 150-0031, Japan,Faculty of Sociology and Center for Experimental Economics, Kansai University, 3-3-35, Yamate-cho, Suita, Osaka 564-8680, Japan","Received 16 March 2018, Revised 21 October 2018, Accepted 26 January 2019, Available online 29 January 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.01.005,Cited by (7),"Recent researches have shed light on the effect of cognitive ability on economic decision-making. By measuring cognitive ability applying Raven's progressive matrix test, we obtain two significant results that this effect affects decision-making in two types of experimental ultimatum games. First, the higher the cognitive ability, the larger the amount a sender offers when the offer is smaller than or equal to the half split. Second, the higher the responders’ cognitive ability, the smaller the offer they accept, when they accept it or not with the strategy method. This study not only finds new factors that affect decision-making in experimental ultimatum games, but also provides more evidences that cognitive ability influences economic decision-making.","Given the significant relationship between the cognitive ability of a person and his/her economic behavior (e.g., Dohmen et al., 2010 [for cognitive ability and risk aversion]; Gill and Prowse, 2016 [for cognitive ability and reasoning]), cognitive ability has a certain influence on economic behavior. We focus here on a type of cognitive ability based on non-verbal estimation of fluid intelligence, which is “the capacity to think logically, analyze and solve novel problems, independent of background knowledge” (Mullainathan and Shafir, 2013, p. 48). We measure this ability applying Raven's Advanced Progressive Matrices (Raven's test hereafter; Raven, 1936).====In particular, we show how cognitive ability affects behavior in experimental ultimatum games, or UGs (Güth et al., 1982). A UG consists of two stages with a sender and a responder. In the first stage, a sender receives a certain amount of money from an experimenter, and decides the amount to offer to the responder. In the second stage, the responder observes the offer, and decides whether to accept it or not. In a subgame perfect equilibrium, “a sender offers the smallest offer and a responder accepts it.” However, the average offer is 40–50% of the endowment and an unfair offer is likely to be rejected (Camerer, 2003). One reasonable interpretation for offering 40–50% of the endowment is the argument of “inequality aversion” (Fehr and Schmidt, 1999).====Thus far, only Brandstätter and Güth (2002) have focused on the effect of cognitive ability in UGs, finding that intelligence has no significant effect on bargaining behavior, although some studies report that cognitive ability affects economic behavior; participants of an experimental p-beauty contest game with higher cognitive ability are capable of deeper reasoning (Brañas-Garza et al., 2012, Gill and Prowse, 2016). Under strategic uncertainty, those with high cognitive ability alter their behavior depending on the opponent type (human or robot), whereas those with low cognitive ability do not (Hanaki et al., 2016). Benito-Ostolaza et al. (2016) found that participants with higher cognitive ability play more strategically. However, it remains ambiguous to what extent the cognitive ability, measured by the Raven's test score, explains human behavior in UG experiments. For example, whose behavior—a sender's or responder's—is affected by cognitive ability? Is the effect of cognitive ability on human behavior different between the hot and cold treatments (Oxoby and McLeish, 2004)? To fill the gap in our knowledge on this topic, we employ non-student participants to collect a wide variety of data on cognitive ability.====Because non-student participants are employed, the age of participants varies widely. Therefore, we also investigate the effect of aging on behavior. Bailey et al. (2013) found that aging increases the offer amount, while Beadle et al. (2012) showed no relationship between aging and this amount. On responder behavior, Bellemare et al. (2011) found that older participants are more likely to reject an unfair offer than are younger participants. Harlé and Sanfey (2012) showed that as a person ages, he/she rejects even slightly unfair offers (about 30% of the endowment). However, Roalf et al. (2012) found that aging does not significantly affect the acceptance rate. Finally, Bailey et al. (2013) found that young participants reject more unfair offers proposed by young relative to older adults.====This article is structured as follows. In Section 2, the hypotheses, experimental design, and procedure are described. The experimental results and statistical analyses are shown in Section 3. The results are discussed in Section 4. Section 5 concludes the article.",Cognitive ability and human behavior in experimental ultimatum games,https://www.sciencedirect.com/science/article/pii/S1090944318300905,March 2019,2019,Research Article,148.0
Mutascu Mihai,"ESCE International Business School Paris, 10 Sexius Michel, 75015 Paris, France","Received 30 November 2018, Revised 27 December 2018, Accepted 26 January 2019, Available online 29 January 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.01.007,Cited by (3),"The paper explores the co-movement between unemployment and ==== rates in US by using a battery of wavelet tools. The dataset covers the period 1945Q1–2017Q4, having quarterly frequency.====The main findings reveal a not stable Phillips curve in US, depending on economic context, seasonality and time-persistence. The Phillips curve is not validated during economic turbulences, while it works over expansion economic periods. Even so, the trade-off between unemployment and ==== is unstable under seasonal growth components and time-persistence, running from short- to medium-term. No link between unemployment and inflation is found in the long-term.","Phillips curve (PC) has fascinated during the last century not only researchers, but also profanes, the relationship between unemployment and inflation provoking huge debates especially among economists all over the world. PC claims a trade-off between unemployment and inflation rates.====The seminal work related to PC is dated before the Great Depression and belong to Fisher (1926), who demonstrates that inflation positively drives unemployment. Many years ago, in his remarkable paper, Phillips (1958) documents this link, called later PC, by observing two very important theoretical aspects. The first aspect is the nonlinearity existed between unemployment rate and nominal wages with negative sign, while the second one claims a strong trade-off between rate of change of unemployment and rate of change of wages, namely the ‘rate of change’ effect.====Therefore, strong criticisms of PC come from Phelps (1967) and Friedman (1968) by introducing the concept of natural unemployment, and (Lucas, 1972, Lucas, 1973), with his rational expectations. In contrast, Sargent, 1982, Sargent, 1999) shows that expected inflation is driven by its single lag rather than distributed ones, while Gordon (1984) entered the concept of policy responses to supply shocks, focusing on oil price.====An extended review of PC literature, especially from historical point of view, offers Gordon (2011). Recently, many works cover both theoretical and empirical sides, from the flattening PC of Blanchard et al. (2015) to the revised global slack variable in PC of Béreau et al. (2018). Having deep implications for PC, noteworthy also is the Okun's law (Okun, 1962), which connects the unemployment with output. According to this economic law, there is a negative correlation between unemployment and potential output.====The case of US deserves a special attention. Besides the fact that PC has US ‘origins’, this country experienced both negative and positive links between unemployment and inflation rates, as Fig. 1 reveals.====The Fig. 1 clearly illustrates that from 1948 to 1969 there is a strong negative co-movement between unemployment and inflation rates, validating the existence of PC, with low level of inflation and high unemployment. Two short economic recessions marked the period: August 1957–April 1958, and April 1960–February 1961. The second sequence covers the period 1970–1987, with strong economic disturbances caused by recessions (i.e. recession from December 1969–November 1970) and oil crises (i.e. 1973 and 1979). Herein, the unemployment is positively correlated with inflation, both of them increasing. Therefore, no PC evidence for this period. From 1980 to 1985, the inflation significantly reduced, stimulating the unemployment.====Finally, starting with ‘90, the PC's trade-off is generally again revealed as followed monetary policies relaxed the inflation pressure but stimulating unemployment. For a short episode, over 2005–2008, during the last economic crisis, the unemployment has the same sign as inflation.====On this ground, the paper analyses the PC in US by using the wavelet. The dataset has quarterly frequency, covering the period 1948Q1–2017Q4. Although the PC is validated during the periods with economic expansion, the main result shows that it is not stable from seasonal and time-persistence point of view.====Several papers follow the time-frequency methods to investigate the PC, almost all considering discrete and maximal overlap discrete wavelet transformations (Gallegati et al., 2006, Gallegati et al., 2011, Pimentel, 2013). Our approach – continuous wavelet transformation – is superior to discrete wavelet transformations and classical time-domain methods. Mutascu (2018, p. 444) shows that such a tool: ”(1) offers short-, medium- and long-run frameworks; (2) details the interaction between variables across different frequencies over time; and (3) shows the lead-lag and cyclical vs. countercyclical status of the nexus.”====The contribution of paper for literature is threefold. First, to the best of our knowledge, the paper is one of the first studies that evidences a not stable PC in US by using a time-frequency approach.====Second, the analysis reveals both negative and positive co-movements between unemployment and inflation by highlighting a seasonal interaction with translation effect, running from short- to medium-term. Third, also as novelty, for both signs of correlation, the study shows no interaction in the long-term, validating the short- and medium-term PC approaches.====The rest of the paper is organized as follows: Section 2 presents the data and methodology, while Section 3 reveals the empirical results. Finally, Section 4 concludes.",Phillips curve in US: New insights in time and frequency,https://www.sciencedirect.com/science/article/pii/S1090944318304010,March 2019,2019,Research Article,149.0
Aggarwal Divya,"XLRI, B 47, GMP Residence, XLRI, CH Area East, Jamshedpur, Jharkhand 831001, India","Received 2 January 2019, Accepted 25 January 2019, Available online 29 January 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.01.002,Cited by (35)," have become a fad among investors despite of the ambiguity surrounding on its nature and characteristics. This study aims to contribute to the existing literature of examining bitcoin returns under a financial asset purview. Through multiple robust tests, the ==== of daily bitcoin returns is analyzed for the time frame of July 2010 till March 2018. Strong evidence of market inefficiency characterized by absence of random walk model is found. The market inefficiency was found attributable to the presence of asymmetric volatility clustering. More studies are needed to examine the temporal dynamics of bitcoin returns.","Alternative financial assets in the form of digital currency are creating paradigm shifts for conducting financial transactions. Cryptocurrencies like bitcoins have become a fad but whether the fad will be short-lived or will transform the financial industry is yet to be seen. Industry experts are debating with diverging views on the pros and cons of cryptocurrency (Hileman and Rauchs, 2017). For academic researchers, this has become an important and active area of research to understand the characteristics of alternative financial assets like cryptocurrency. Today, bitcoins have become the most popular cryptocurrency globally. Table 1 shows the market characteristics of bitcoins and how it has swayed the world since its launch in 2009. It has created a disruption in the financial services space and is emerging as a threat to the strengths and abilities of money (Dyhrberg, 2016). Many investors have entered the bitcoin market to invest in the asset without being aware of its characteristics and market efficiency.====This study focuses on the very core question of finance, which lies at the heart of traditional finance scholars. Traditional finance theorist Eugene Fama (1970) stated that markets are fully efficient and follow a random walk model. He believed that the future prices cannot be predicted as markets prices already incorporate all information by being efficient. Hence, the popular term of random walk model i.e. markets follow a random walk path and the future path cannot be predicted. Since then, numerous studies have been done to examine whether markets or assets follow a random walk model. However, majority of the studies assumed the time series to have a linear stochastic process. Studies by Hseih (1991), Broock et al. (1996), and (Lim and Brooks, 2011) urged to incorporate nonlinear stochastic process in determining random walk model. Also new developments in examining random walk model by incorporating structural breaks in the time series data has also been done.====The objective of this study is twofold. Firstly, to widen the scope of market efficiency and random walk model studies by analyzing alternate financial assets like bitcoins. Secondly, to move beyond conventional random walk model tests of unit roots by detecting presence of structural breaks and nonlinear dependence in the series. By analyzing the specific type of nonlinear dependence and impact of volatility clustering, a more robust approach of examining market efficiency can be taken.====The study aims to contribute to the existing literature in many ways. Firstly, very few countable studies are done on examining market efficiency of bitcoins. This study aims to be a first systematic study to examine the daily returns characteristics of bitcoins. Secondly by performing multiple tests it ensures robust examination of returns. Thirdly, examining nonlinear models of conditional volatility persistence will enhance the current literature on volatility effects in markets.====The findings of the study reveal that bitcoin returns do not follow a random walk model and hence are characterized with market inefficiency. This can assist investors and policy makers to examine the return dynamics of bitcoins and be cautious of its highly volatile nature. The volatility clustering tests confirm the presence of conditional volatility in the returns with persistence of both large and small deviations.====The paper is structured as follows. Section 2 briefly talks about few studies on bitcoins. Section 3 mentions the data and methodology adopted followed by results and discussion in Section 4. The study is concluded by Sections 5 and 6 with conclusion and implications respectively.",Do bitcoins follow a random walk model?,https://www.sciencedirect.com/science/article/pii/S1090944319300018,March 2019,2019,Research Article,150.0
"Rapallini Chiara,Rustichini Aldo","Department of Economics and Management, Universita’ di Firenze, Via delle Pandette 9, Firenze, Italy,Department of Economics, University of Minnesota, 1925 4th Street South 4-101, Hanson Hall, Minneapolis, MN, USA","Received 12 December 2018, Revised 20 December 2018, Accepted 11 January 2019, Available online 15 January 2019, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2019.01.001,Cited by (1),"In a multi-ethnic society, friendship among children might be expected to be overwhelmingly shaped by ethnicity and cultural heritage. Using an original panel data-set of classmate networks in multi-ethnic primary schools near Florence, Italy, (==== children in 2nd and 5th grade), we show instead that cognitive skills and personality traits matter as much as ethnicity in shaping friendships, thus playing the role of ====. We test whether friends affect a child’s personality more than the other way round: to do this, we estimate peer effects. We only find non-significant effect of peers on math grades and a measure of intelligence (KBIT). For personality traits, peer effects are significant only for Extraversion. These findings are crucial for design of immigration policies: rather than emphasizing differences among ethnic groups, a farsighted policy could try to point these ==== among individuals.","School environment has always been of great interest to scholars and policymakers, because the social dynamics that develop at school are crucial to understanding those of society as a whole, and possibly lay the foundation for a stronger cohesion. In particular, social interactions in classrooms have been widely explored to test the homophily hypothesis, i.e. the idea that relations between similar people are established at a higher rate than those among dissimilar people. In this framework, the role played by ethnicity in network formation at school is usually investigated ignoring traits like intelligence or personality. These studies proceed under the assumption that the degree of race homophily in school networks can be taken as a proxy for that prevailing in society (Baerveldt, Van Duijn, Vermeij, Van Hemert, 2004, Currarini, Jackson, Pin, 2010, Currarini, Matheson, Vega-Redondo, 2016, Feddes, Noack, Rutland, 2009, Hallinan, Smith, 1989, Hallinan, Teixeira, 1987). When longitudinal data are available, changes in the degree of race homophily are seen as providing a test of Allport (1954) contact theory, which predicts that experience with cross-ethnic relationships may improve positive out-group attitudes among individuals (Baerveldt, Van Duijn, Vermeij, Van Hemert, 2004, Feddes, Noack, Rutland, 2009).====Here we choose a different conceptual path, and we test the hypothesis that bonds among individuals grow not exclusively out of a common ethnic background, but also out of common cognitive and non-cognitive skills of individuals. As far as we know, this hypothesis has not yet been tested. So far the predictive power of the ethnic group in the sorting process among individuals has been tested in isolation, with the exception of a few cases where the sex of individuals is considered, but never taking into account their personality or cognitive skills. On the contrary, the role played either by personality and intelligence in a wide range of outcomes has been recently highlighted (Almlund, Duckworth, Heckman, Kautz, 2011, Borghans, Duckworth, Heckman, Ter Weel, 2008).====The test of the predictive power of different traits in the sorting process among individuals requires us to address the concern for potential endogeneity of traits: your friends might affect who you are, more than who you are determines your friends. This peer effect has been largely investigated especially in the case of academic outcomes (Foster, 2006, Mayer, Puller, 2008, Sacerdote, 2001). There are at least three reasons why identification of peer effects is particularly challenging (Sacerdote, 2001). The first is the fact that a child’s score affects his peers mean score and vice versa: this is the ==== (Manski, 1993). Second, children are selected into a peer group based, in part, on their unobservable characteristics. Third, a child’s score is affected either by the children’s scores or the peers average characteristics. Separating the role played by the children’s scores from that of the peers’ average characteristic is difficult since peer background itself affects peer outcome. The latter source of endogeneity is cause of particular concern because the selection by schools of students according to immigration background, or socio-economic status, is surely quite common. Class formation, actually, is not a random process, because parents choose the school, and also because of the rules that principals and teachers’ boards have to follow; the latter in particular is directed by legislation (e.g. Angrist, Lang, 2004, Ballatore, Fort, Ichino, 2018, Hoxby, 2000).====Children in our sample are part of Italian school system: in this case the selection of the school by parents is less important than the role played by schools in the formation of classes. In fact, principals and teachers usually define class composition in terms of the sex, immigration background, and the skills and the disabilities of the students. In any case, independently of whether it is decided by parents or schools, the selection of classmates is always present and it should be taken into account when estimating peer effects. In addition, homophily rules the way in which students of the same class select into peer groups. The endogenous peer group formation may hamper policies that manipulate peer groups to reach some desired social outcomes, as for example that of exposing low ability students to high ability classmates (Carrell et al., 2013). In summary, whether the similarity between peers is due to causation, selection or a mixture of both is an important and still open issue (Barnes, Beaver, Young, TenEyck, 2014, Boucher, 2016).====The primary aim of this study is to investigate the relative importance of sex, ethnicity, personality traits and cognitive skills in predicting relationships among classmates. The second purpose is to measure the significance and size of peer effects on possibly endogenous traits, i.e. cognitive skills and personality traits. For our analysis we use longitudinal observations of a balanced panel with 396 children. This is a sample from multi-ethnic classes in primary schools in Italy for which we have gathered data on three networks, ==== and ====, described below, together with a complete profile of the individual characteristics of the sample, i.e. personality, cognitive skills, sex, ethnic origin and family background.====Briefly, our analysis will develop as follows. We first examine the motivation behind each kind of relationship, computing simple indices summarizing the basic characteristics of each network and of the relation between them. Then we develop a simple theoretical model of network formation to study relationships among classmates. The structural model we derive allows us to take into account the role played by the class composition in the probability of a relationship. Third, we use regression analysis to identify the factors predicting whether a link exists, using a logit model. Fourth, we estimate a linear regression model with child fixed effects to quantify how much the characteristics of peers affect those of a child.","Personality and cognitive skills in network of friends, for multi-ethnic schools",https://www.sciencedirect.com/science/article/pii/S1090944318304101,March 2019,2019,Research Article,151.0
"Barthel Anne-Christine,Hoffmann Eric,Monaco Andrew","West Texas A&M University, 2501 4th Ave, Canyon, TX 79016, USA,University of Puget Sound, 1500 N Warner St, Tacoma, WA 98416, USA","Received 11 September 2018, Revised 16 September 2018, Accepted 20 October 2018, Available online 2 November 2018, Version of Record 14 March 2019.",https://doi.org/10.1016/j.rie.2018.10.002,Cited by (1),"This paper experimentally compares the impact of the presence of strategic substitutes (GSS) and complements (GSC) on players’ ability to successfully play equilibrium strategies. By exploiting a simple property of the ordering on strategy spaces, our design allows us to isolate these effects by avoiding other confounding factors that are present in more complex settings, such as market games. We find that the presence of strategic complementarities significantly improves the rate of Nash play, but that this effect is driven mainly by early rounds of play. This suggests that GSS may be more difficult to learn initially, but that given sufficient time, the theoretically supported globally stable equilibrium offers a good prediction in both settings. We also show that increasing the degree of substitutability or complementarity does not significantly improve the rate of Nash play in either setting, which builds on the findings of previous studies.","Games of strategic complements (GSC) and substitutes (GSS) have garnered much attention both in recent theoretical as well as empirical studies, as they provide a tractable framework for studying a wide range of strategic scenarios by exploiting simple monotonic properties of an agent’s best response. GSC describe scenarios where agents find it optimal to choose a higher action in response to opponents choosing higher actions, such as in Bertrand price competition. On the other hand, GSS describe scenarios where agents find it optimal to choose a lower action in response to opponents choosing higher actions, such as in Cournot quantity competition. Theoretical work by Milgrom and Roberts (1990), Roy and Sabarwal (2012), and Barthel and Hoffmann (2017) shows that in both GSC and GSS, any unique, dominance solvable equilibrium is also globally stable.==== However, theory has little to offer concerning the question as to whether games exhibiting strategic complementarities as opposed to substitutes improve or hinder a player’s ability to adapt to the environment at hand and come to play the equilibrium strategy, or whether a higher degree of strategic complements or strategic substitutes have similar effects on performance. This paper fills a gap in the literature by testing in a laboratory setting whether the ordinal incentives present in GSC hinder or help promote the ability of participants to play the Nash strategies relative to those present in GSS. We also examine whether an increase in the magnitude of such incentives has similar effects.====Understanding these effects can be of great practical importance. For example, Mathevet (2010) gives conditions under which a social choice function can be implemented by a mechanism exhibiting strategic complements, including examples such as auctions, public goods provision, and the design of toll systems to minimize traffic congestion, among others. It is therefore of interest to know whether designing such mechanisms improves the ability of agents to achieve the socially desirable outcome, and if performance improves as the amount of complementarity present in the mechanism increases. Alternatively, suppose we are interested in the adjustment towards a new market equilibrium in Bertrand markets relative to Cournot markets following a shock to market demand which alters the slope of firms’ best responses. It is then of interest to understand how much of this difference in behavior is driven solely by the effect of introducing a comparable increase of complementarities and substitutability into the respective environments, and how much of it can be attributed to other market factors.====This paper exploits a simple property of the ordering on strategies in order to isolate the effects of strategic substitutes and complements in an experimental setting and study whether these effects impact the ability of players to behave according to the theoretical Nash prediction. The notion of GSC and GSS depends on the underlying order of each player’s strategy space, or which actions represent “high” and “low” actions, so that when an opponent takes a “higher” action, a player in a GSC has a larger incentive to take a “higher” action, whereas a player in a GSS has a larger incentive to take a “lower” action. The presence of strategic complements can then be thought of as an ordinal coordination effect, whereas the presence of strategic substitutes can be thought of as an ordinal anti-coordination effect. Through a careful design, we employ a simple two player, three action game with the property that simply reversing the underlying order on Player 1’s set of strategies transforms the game from a GSC to a GSS. This simple reversal of the high and low actions allows us to hold constant many other relative factors and isolate the effects of ordinal coordination on equilibrium play. In particular:====Properties 1 and 2 allow us to ensure that the strategic incentives between the GSC and GSS treatments are equivalent for corresponding strategies and, consequently, hold constant the “slopes” of players’ best responses. Property 3 ensures that corresponding strategies yield the same expected payoffs, which eliminates any bias that may induce players to choose strategies closer to equilibrium in initial rounds of play. We are also interested in studying whether or not a higher degree of complementarity or substitutability in a game affects performance. To this end, we conduct a laboratory experiment on 4 treatments, where players are randomly matched and play at least 20 rounds of each treatment. The treatments are labeled GSC Low (GSCL), GSC High (GSCH), GSS Low (GSSL), and GSC High (GSCH), where the Low and High designations indicate a low or high degree of complementarity or substitutability present in the respective game. In specific, we use a single parameter in order to vary the desire to ordinally coordinate or anti-coordinate in both GSC and GSS, and therefore investigate whether the presence of “more complements” or “more substitutes” increases equilibrium performance in both strategic settings, respectively.====This work is most closely related to Davis (2011) and Chen and Gazzale (2004). Davis (2011) experimentally studies the relative convergence of Bertrand (where strategic complements are present) and Cournot (where strategic substitutes are present) differentiated-product oligopolies using a 2 × 2 design, where the degree of product substitutability ==== ∈ [0, 1) is varied in both market structures. Specifically, higher levels of ==== correspond to a higher degree of product substitutability, so that products are perfect substitutes when ==== is near 1. Davis (2011) finds that in general, Bertrand markets converge faster than Cournot markets at both high and low values of product substitutability. Also, at high levels of product substitutability, Bertrand markets converge more completely, with no significant effects in Cournot markets.====Chen and Gazzale (2004) consider experimentally the role of strategic complementarities within the context of a compensation mechanism. Specifically, they consider whether convergence to equilibrium improves as parameters become closer to making the game a GSC, and also, within the range of GSC specifications, whether a higher degree of complementarities further improves convergence. Indeed, they find that games near the GSC threshold converge significantly better than those far from the threshold. However, they do not find significant differences in convergence for stronger and weaker degrees of strategic complements beyond the threshold.====Other related works include Potters and Suetens (2009), who consider whether coordination on a Pareto optimal solution (as opposed to a distinct Nash equilibrium) is more prevalent in a GSC or GSS. Indeed, they find that there is significantly more cooperation when the game is one of GSC than when it is one of GSS. However, they do not consider how these results change as the magnitude of payoff differences change, as in Chen and Gazzale (2004). Fehr and Tyran (2008) study the effects of the presence of boundedly rational agents on the convergence to a new equilibrium in a price setting game after a monetary shock. They conclude that after a shock, converge occurs faster in the presence of strategic substitutes than under strategic complements. One interesting explanation they offer is that the pre-shock equilibrium price induces a strong anchoring effect in the complements treatment, and that the cost of error is larger in the substitutes treatment, leading to more rational play.====While the results mentioned above are very interesting in contexts such as market outcomes,==== the properties of our simple specifications constitute an improvement when trying to isolate the pure strategic effects of complements and substitutes. For instance, a straightforward calculation in Davis (2011) shows that equivalent changes in ==== substitutability results in relatively more ==== substitutability in the Cournot game than strategic complementarity in the Bertrand game.==== Therefore, if product substitutability is of interest, a direct comparison of the effects of strategic substitutes and complements is lost. Furthermore, as Davis (2011) points out, given an interior equilibrium in a Cournot market, which is a GSS, firms have an incentive to respond to deviations from equilibrium by opponents in one direction by deviating in the opposite direction, which promotes convergence by dampening the aggregate deviation. Notice that this observation is the product of strategic substitutes coupled with features inherent to market games, namely an interior equilibrium and aggregative payoffs. In light of the results of Potters and Suetens (2009), it is then of interest to know whether it is the strategic substitutes present in Cournot markets which promotes equilibrium as opposed to cooperative play, or if cooperation truly is easier in games of GSC such as Bertrand markets.====Our findings can be summarized as follows: regardless of low or high treatment, subjects play Nash equilibrium in GSC at a rate that is significantly higher than in GSS. However, this difference is driven mainly by the initial learning process (first 10 rounds of play), as players are becoming accustomed to the strategic environment at hand. This suggests that, eventually, the theoretical Nash prediction in both GSC and GSS is a good indicator of eventual play, but that GSS environments may be more challenging for individuals to understand initially. Surprisingly, we find no significant difference between high and low treatment either in GSC or GSS. This result is consistent with the findings of Chen and Gazzale (2004) in the case of GSC, and further extends these findings into the case of GSS.====The remainder of this paper is organized as follows: Section 2 presents the basic definitions of monotone games, and the specific models under consideration including a comparison of them are presented in Sections 2.1 and 2.2. Section 3 explains the experimental design and procedures, while Section 4 presents our findings. Section 5 concludes by summarizing our findings, and discusses possible limitations and considerations for future research.",Coordination and learning in games with strategic substitutes and complements,https://www.sciencedirect.com/science/article/pii/S1090944318302874,March 2019,2019,Research Article,152.0
